[{"id": "20150149478", "patent_code": "10348661", "patent_name": "Unified server for managing a heterogeneous mix of devices", "year": "2019", "inventor_and_country_data": " Inventors: \nKrishna; Vikas (San Jose, CA), Bailloeul; Timothee (Sunnyvale, CA)  ", "description": "BACKGROUND\n 1.  Field of the Invention\n The techniques disclosed herein relate to a system and method for integrating one or more resource servers for one or more types of devices.  In particular, the techniques relate to a unified server for managing a heterogeneous mix of devices.\n 2.  Description of the Background Art\n Consumers have a variety of consumer electronic devices to choose from.  A problem arises, however, because many of these devices have different user interfaces and users find it confusing and difficult to adjust.  For example, devices can have\ndifferent usage paradigms, ranging from credentials for individual logins to group logins and different portals to access device usage sessions, documents, or content in general.  In addition, each device has a different application programming interface\n(API) for both the device itself and its backend resulting in a need to learn different APIs and API styles for each device to be able to employ them in different solutions.\n It is also difficult to update the devices' backends because each device has a different backend or server that the customer has to install and manage if hosted privately, or interface with if used publicly.  Since each device has a different\nbackend, the user has to update each device backend separately.  Furthermore, the data about each device resides in a separate system resulting in device data fragmentation and eliminating the opportunity to correlate device data.\nSUMMARY\n The techniques disclosed herein overcome the deficiencies of the prior art with a system for integrating one or more resource servers for one or more types of devices.  In one embodiment, the system includes an enterprise service bus and network\nappliance as a service (NAaaS) application services.  The enterprise service bus receives a query from a first device for one or more types of media related to the query, to transmit the request to NAaaS application services, to receive the determination\nof one or more resource servers that are associated with keywords, to generate a request for media for the one or more resource servers, to determine a first communication protocol of the request and one or more types of second communication protocols of\nthe one or more resource servers, to translate the request from the first communication protocol into each one of the one or more types of second communication protocols, to determine a first message format of the request and one or more types of second\nmessage formats of the one or more resource servers, to transform the request from the first message format into each one of the one or more types of second message formats, to retrieve the one or more types of media matching the translated request from\nthe one or more resource servers and to send the one or more types of media to the first device.  The NAaaS application services are configured to receive the query from the enterprise service bus, to identify the keywords in a global database index that\nmatch the query, to determine the one or more resource servers that store media associated with the keywords and to send the determination of the one or more resource servers to the enterprise service bus.\n Other aspects include corresponding methods, systems, apparatuses, and computer program products for these and other innovative aspects.\n The system advantageously creates a facility for federated search for accessing information from all the resource servers for one or more types of devices in a network.  The features and advantages described herein are not all-inclusive and many\nadditional features and advantages will be apparent in view of the figures and description.  Moreover, it should be noted that the language used in the specification has been principally selected for readability and instructional purposes, and not to\nlimit the scope of the subject matter disclosed herein. BRIEF DESCRIPTION OF THE DRAWINGS\n The techniques introduced herein are illustrated by way of example, and not by way of limitation in the figures of the accompanying drawings in which like reference numerals are used to refer to similar elements.\n FIG. 1A is a high-level block diagram illustrating one embodiment of a system for integrating one or more resource servers for one or more types of devices.\n FIG. 1B is a high-level block diagram illustrating another embodiment of a system for integrating one or more resource servers for one or more types of devices.\n FIG. 2A is a block diagram illustrating one embodiment of an enterprise service bus.\n FIG. 2B is a block diagram illustrating one embodiment of Network Appliance as a Service application services.\n FIG. 3 is a graphic representation of an embodiment of a user interface for displaying a webpage providing an overview of services for a user.\n FIG. 4 is a graphic representation of an embodiment of a user interface for selecting a media type for viewing on a collaborative computing device.\n FIG. 5 is a graphic representation of an embodiment of a user interface for selecting a location to send a media for display.\n FIG. 6 is a graphic representation of an embodiment of a user interface for displaying a list of devices at a location or within a set distance of a user device.\n FIG. 7 is a graphic representation of an embodiment of a user interface for displaying a list of commands to send to a selected collaborative computing device from a user device 102.\n FIG. 8 is a graphic representation of an embodiment of a user interface for displaying a list of options to select a source for displaying a document on a collaborative computing device.\n FIG. 9 is a graphic representation of an embodiment of a user interface for displaying a list of media.\n FIG. 10 is another graphic representation of an embodiment of a user interface for displaying a list of media.\n FIG. 11 is a graphic representation of an embodiment of a user interface for creating an event in an electronic calendar for a user.\n FIG. 12 is a graphic representation of an embodiment of a user interface for selecting media to upload for one or more devices reserved for a calendar event.\n FIG. 13 is a graphic representation of an embodiment of a user interface for providing an overview of the created event in the calendar.\n FIG. 14 is a graphic representation of an embodiment of a user interface for specifying configuration settings of a collaborative computing device and for turning them on or off.\n FIG. 15 is a flow diagram of one embodiment of a method for managing communication between one or more types of devices.\n FIG. 16A is a flow diagram of one embodiment of a method for controlling interoperability between different types of devices.\n FIG. 16B is a flow diagram of another embodiment of a method for controlling interoperability between different types of devices.\n FIG. 17 is a flow diagram of one embodiment of a method for generating a bill based on a cost determined for each transaction performed by a device.\n FIG. 18 is a flow diagram of one embodiment of a method for compiling a global database index for one or more resource servers.\n FIG. 19 is a flow diagram of one embodiment of a method for translating a request for one or more resource servers.\n FIG. 20 is a flow diagram of one embodiment of a method for controlling one or more devices reserved for a calendar event.\n FIG. 21 is a flow diagram of another embodiment of a method for controlling one or more devices reserved for a calendar event.\n FIG. 22 is a flow diagram of one embodiment of a method for generating one or more notifications for a calendar event.\nDETAILED DESCRIPTION\n A system and method for integrating one or more resource servers for one or more types of devices is disclosed.  In the following description, for purposes of explanation, numerous specific details are set forth in order to provide a thorough\nunderstanding of the techniques disclosed herein.  It will be apparent, however, to one skilled in the art that the techniques can be practiced without these specific details.  In other instances, structures and devices are shown in block diagram form in\norder to avoid obscuring the techniques.  For example, the techniques are described in one embodiment below with reference to user devices such as a smart phone and particular software and hardware.  However, the description applies to any type of\ncomputing device that can receive data and commands, and any peripheral devices providing services.\n Reference in the specification to \"one embodiment\" or \"an embodiment\" means that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment.  The appearances of the\nphrase \"in one embodiment\" in various places in the specification are not necessarily all referring to the same embodiment.\n Some portions of the detailed descriptions that follow are presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory.  These algorithmic descriptions and representations are the means used\nby those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art.  An algorithm is here, and generally, conceived to be a self-consistent sequence of steps leading to a desired result.  The\nsteps are those requiring physical manipulations of physical quantities.  Usually, though not necessarily, these quantities take the form of electrical or magnetic signals capable of being stored, transferred, combined, compared, and otherwise\nmanipulated.  It has proven convenient at times, principally for reasons of common usage, to refer to these signals as bits, values, elements, symbols, characters, terms, numbers or the like.\n It should be borne in mind, however, that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities.  Unless specifically stated otherwise as\napparent from the following discussion, it is appreciated that throughout the description, discussions utilizing terms such as \"processing\" or \"computing\" or \"calculating\" or \"determining\" or \"displaying\" or the like, refer to the action and processes of\na computer system, or similar electronic computing device, that manipulates and transforms data represented as physical (electronic) quantities within the computer system's registers and memories into other data similarly represented as physical\nquantities within the computer system memories or registers or other such information storage, transmission or display devices.\n An apparatus for performing the techniques introduced herein is also described.  This apparatus may be specially constructed for the required purposes, or it may comprise a general-purpose computer selectively activated or reconfigured by a\ncomputer program stored in the computer.  Such a computer program may be stored in a computer readable storage medium, such as, but is not limited to, any type of disk including floppy disks, optical disks, CD-ROMs, and magnetic disks, read-only memories\n(ROMs), random access memories (RAMs), EPROMs, EEPROMs, magnetic or optical cards, flash memories including USB keys with non-volatile memory or any type of media suitable for storing electronic instructions, each coupled to a computer system bus.\n Some embodiments can take the form of an entirely hardware embodiment, an entirely software embodiment or an embodiment containing both hardware and software elements.  A preferred embodiment is implemented in software, which includes but is not\nlimited to firmware, resident software, microcode, etc.\n Furthermore, some embodiments can take the form of a computer program product accessible from a computer-usable or computer-readable medium providing program code for use by or in connection with a computer or any instruction execution system. \nFor the purposes of this description, a computer-usable or computer readable medium can be any apparatus that can contain, store, communicate, propagate, or transport the program for use by or in connection with the instruction execution system,\napparatus, or device.\n A data processing system suitable for storing and/or executing program code will include at least one processor coupled directly or indirectly to memory elements through a system bus.  The memory elements can include local memory employed during\nactual execution of the program code, bulk storage, and cache memories which provide temporary storage of at least some program code in order to reduce the number of times code must be retrieved from bulk storage during execution.  Input/output or I/O\ndevices (including but not limited to keyboards, displays, pointing devices, etc.) can be coupled to the system either directly or through intervening I/O controllers.\n Network adapters may also be coupled to the system to enable the data processing system to become coupled to other data processing systems or remote printers or storage devices through intervening private or public networks.  Modems, cable modem\nand Ethernet cards are just a few of the currently available types of network adapters.\n Finally, the algorithms and displays presented herein are not inherently related to any particular computer or other apparatus.  Various general-purpose systems may be used with programs in accordance with the teachings herein, or it may prove\nconvenient to construct more specialized apparatus to perform the required method steps.  The required structure for a variety of these systems will appear from the description below.  In addition, the specification is not described with reference to any\nparticular programming language.  It will be appreciated that a variety of programming languages may be used to implement the teachings of the various embodiments as described herein.\n System Overview\n FIG. 1A illustrates a high-level block diagram of a system 100 for implementing a service oriented architecture that supports complex event processing and business activity monitoring are described below.  The illustrated embodiment of the\nsystem 100 comprises: user devices 102a-102n, collaborative computing devices 103a-103n, a network 104, a web server 120, a Network Appliance As A Service (NAaaS) middleware server 101 and a NAaaS application server 123.  In FIG. 1A and the remaining\nfigures, a letter after a reference number, for example, \"102a\" is a reference to the element having that particular reference number.  A reference number in the text without a following letter, for example \"102,\" is a general reference to any or all\ninstances of the element bearing that reference number.\n The network 104 is a conventional type, wired or wireless, and may have any number of configurations such as a star configuration, token ring configuration or other configurations known to those skilled in the art.  Furthermore, the network 104\nmay comprise a local area network (LAN), a wide area network (WAN) (e.g., the Internet), and/or any other interconnected data path across which multiple devices may communicate.  In yet another embodiment, the network 104 may be a peer-to-peer network. \nThe network 104 may also be coupled to or includes portions of a telecommunications network for sending data in a variety of different communication protocols.  In yet another embodiment, the network 104 includes Bluetooth communication networks or a\ncellular communications network for sending and receiving data such as via short messaging service (SMS), multimedia messaging service (MMS), hypertext transfer protocol (HTTP), direct data connection, WAP, email, etc. While only one network 104 is\ncoupled to the plurality of user devices 102a-102n, the plurality of collaborative computing devices 103a-103n, the web server 120, the NAaaS application server 123 and the NAaaS middleware server 101, in practice any number of networks 104 can be\nconnected to the entities.\n The user devices 102a-102n are devices associated with a particular user.  For example, a company provides its employees with a mobile device or a laptop.  The user devices 102a-102n are each coupled to the network 104 via signal lines 112a-112n\nrespectively.  The user device 102 is any computing device including a memory, a processor and a communication capability.  For example, the user device 102 can be a tablet computer, a personal digital assistant, a smart phone, a feature phone, etc. The\nuser devices 102 can communicate with the network 104 wirelessly or through wired connectivity.  The user devices 102 include one or more user applications (not shown) that generate messages to be processed by the enterprise service bus 107.\n The user device 102 is adapted for sending and receiving data to and from the NAaaS middleware server 101.  For example, the user device 102 sends a command to project an image of a presentation program document on at least one of the plurality\nof collaborative computing devices 103a-103n to the NAaaS middleware server 101.  The user device 102 includes a display for viewing information provided by the enterprise service bus 107.  For example, the user device 102 receives graphical data from\nthe NAaaS middleware server 101 for listing the plurality of collaborative computing devices 103a-103n for display on the user device 102.\n The user device 102 determines its location so that the user device 102 can interact with other user devices 102 or collaborative computing devices 103 via the NAaaS middleware 101.  The user device 102 determines its location information by\nusing global positioning system (GPS) circuitry included within the device itself to determine its location.  For determining the user device's 102 location indoors, the user device 102 employs radio frequency, ultra-sound signal or invisible light\ncommunication.  For example, the user device 102 determines its location through wireless access points based on measuring the intensity of received signals.  The user device 102 accesses a database including pairs of media access control (MAC) addresses\nand locations over the Internet.  To determine a location, the user device 102 retrieves the location corresponding to the access point MAC address from the database.\n In another embodiment, the user device 102 performs a device discovery process that works via the network 104 using specific protocols like SNMP, ICMP, Bonjour, etc. For example, the user device 102 queries the NAaaS middleware server 101 to\ndiscover devices.  The NAaaS middleware server 101 uses SNMP or ICMP protocols to discover devices and reports back to the user device 102 with the found devices together with their internet protocol (IP) address, media access control (MAC) addresses,\netc.\n The collaborative computing devices 103a-103n are devices associated with a particular location and/or a particular function.  Collaborative computing devices 103a-103n can be assigned to a conference room or are assigned for meetings.  For\nexample, a projector and an interactive whiteboard can be assigned to a select conference room from a plurality of conference rooms inside a building.  The collaborative computing devices 103a-103n are each coupled to the network 104 via signal lines\n113a-113n respectively.  The collaborative computing device 103 is any computing device including a memory and a processor.  For example, the collaborative computing device 103 can be a projector, a monitor, a television, an interactive whiteboard, a\nwebcam, a microphone, a loudspeaker, a CD/DVD player, an electronic paper device, an electronic reader, a desktop computer, a tablet, a smartphone, etc.\n The collaborative computing device 103 is adapted for sending and receiving data to and from the NAaaS middleware server 101.  For example, a projector in a conference room can receive a presentation program document from the NAaaS middleware\nserver 101.  In another example, a video conferencing device including a webcam, a microphone and a monitor in a first location can capture a real-time audio-video synchronous communication data stream and send it to another video conferencing device in\na second location through the enterprise service bus 107 in the NAaaS middleware server 101.\n The NAaaS middleware server 101 is any computing device including a memory and a processor which is connected to the network 104 via signal line 116.  The NAaaS middleware server 101 comprises an enterprise service bus 107.  The enterprise\nservice bus is described in further detail below with reference to FIG. 2A.\n The enterprise service bus 107 includes software and routines for providing a standard interface to one or more networks of disparate devices and their corresponding resource servers that are deployed independently to communicate with each\nother.  In one embodiment, the enterprise service bus 107 executes one or more services including invocation support, routing (e.g., content based routing, static/deterministic routing, policy based routing, rules based routing) mediation, message queue\n(e.g., publish-subscribe), process choreography, service orchestration, complex event processing, security and management (e.g., monitoring, logging).  The enterprise service bus 107 also calls methods contained in the code on the NAaaS application\nservices 106 that implement the services.  For example, the enterprise service bus 107 instructs the NAaaS application services to authenticate users, log device usage entries, store media, analyze media, index keywords related to users' skills and\nsearch a database for user profiles that include skills that match a user query.\n The user devices 102a-102n or the collaborative computing devices 103a-103n use a particular messaging format over a particular communication protocol to communicate with and send service requests to each other through the enterprise service bus\n107.  A message format defines the structure and form of the message.  For example, message formats include Simple Object Access Protocol (SOAP), eXtensible Markup Language (XML), etc. A communication protocol defines a set of rules governing the syntax,\nsemantics, and synchronization of communications.  For example, communication protocols include File Transfer Protocol (FTP), Hypertext Transfer Protocol (HTTP), Message Queue (MQ), Internet Inter-Orb Protocol (HOP), etc. While the enterprise service bus\n107 and the NAaaS application services 106 are illustrated as being on separate servers, in one embodiment they are on the same server.\n The NAaaS application server 123 is any computing device including a memory and a processor which is connected to the network 104 via signal line 124.  The NAaaS application server 123 includes NAaaS application services 106, which is a\ncollection of implementation services that are abstracted as an application and composed by the enterprise service bus 107 in order to deliver higher level services.  The NAaaS application services 106 are described in further detail below with reference\nto FIG. 2B.\n The web server 120 is any computing device including a memory and a processor that is connected to the network 104 via signal line 122.  The web server 120 comprises a user interface engine 121.  While the web server 120 is illustrated in Figure\nlA as being a separate server, in some embodiments the user interface engine 121 could be stored on a user device 102 or function as a standalone application.\n The user interface engine 121 is software and routines for generating graphical data for displaying a user interface.  In one embodiment, the user interface engine 121 is a set of instructions executable by the processor to provide the\nfunctionality described below for generating graphical data for displaying a user interface.  In another embodiment, the user interface engine 121 is stored in the memory and is accessible and executable by the processor.\n The user interface engine 121 receives a request for generating graphical data for displaying information for controlling collaborative computing devices 103.  For example, the user interface engine 121 generates graphical data for displaying a\nwebpage that lists the available collaborative computing devices 103 in a particular location or functions associated with each collaborative computing device 103.  The user inputs information and the user interface engine 121 transmits the information\nto the enterprise service bus 107, which determines the action to be performed.  For example, if the user device 102 transmits an instruction for a projector to project an image contained in the request, the user interface engine 121 transmits the\nrequest to the enterprise service bus 107, which transmits the request to the projector and instructs the NAaaS application services 106 to perform other actions on the request.  FIGS. 3-14 includes example user interfaces.\n In another embodiment, the user interface engine 121 receives a request for generating graphical data for displaying a search user interface.  For example, the user interface engine 121 generates graphical data for displaying a webpage with a\ntext box for inputting a query of one or more terms.  The user inputs the query and the user interface engine 121 transmits the query to the enterprise service bus 107, which transmits the request to the NAaaS application services 106.  The search user\ninterface may include an input for entering a search query by a requestor.  The search query may include text information, visual information or audio information.\n The NAaaS application services 106 retrieve matching search results from one or more resource servers and send the search results to the enterprise service bus 107, which transmits the search results to the user device 102.  In one embodiment,\nthe user interface engine 121 transmits one or more of the profile and one or more types of media to the requestor via the enterprise service bus 107.  An example of the search user interfaces are described below in more detail with reference to FIGS.\n9-10.\n FIG. 1B illustrates another high-level block diagram of a system 110 for implementing a service oriented architecture that supports complex event processing and business activity monitoring are described below.  The illustrated embodiment of the\nsystem 110 comprises: user devices 102a-102n as a first layer, the enterprise service bus 107 as a second layer and the NAaaS application services 106, the collaborative computing devices 103a-103n and their corresponding resource servers 105a-105n as a\nthird layer.  Each one of the resource servers 105a-105n store a copy of the media type and index the media type associated with the corresponding collaborative computing devices 103a-103n whenever the collaborative computing devices 103a-103n execute a\ntransaction in response to a request.  For example, a projector resource server stores a projected presentation program document, a video conferencing resource server stores a video recording of the video conference and an interactive whiteboard resource\nserver stores an image of the interactive whiteboard.\n The enterprise service bus 107 layer processes requests coming in from the user devices 102a-102n layer and relays the requests to the NAaaS application services 106 for processing and the collaborative computing devices 103a-103n and their\ncorresponding resource servers 105a-105n.  In one embodiment, the enterprise service bus 107 layer comprises one or more ports that provide an interface for user applications on the user devices 102a-102n to connect with the enterprise service layer 107\nto send messages and receive responses.  In another embodiment, the enterprise service bus 107 layer comprises one or more ports to communicate with the NAaaS application services 106 layer and the layer with the collaborative computing devices 103 and\ntheir corresponding resource servers 105a-105n.  In one embodiment, a port on the enterprise service bus 107 may be of a particular port type that handles only messages and communications of a particular message format and communication protocol of a\nuser application.  In another embodiment, a port on the enterprise service bus 107 may be of a universal port type that includes a generic interface to the enterprise service bus 107 and can handle any messaging format and communication protocol\ncombination.\n Enterprise Service Bus\n Referring now to FIG. 2A, an example of the enterprise service bus 107 is shown in more detail.  FIG. 2A is a block diagram of a server 101 that includes: a processor 240, a memory 245, a communication unit 250 and the enterprise service bus\n107.\n The processor 240, the memory 245, the communication unit 250 and the enterprise service bus 107 are communicatively coupled to the bus 220.  The bus 220 may represent one or more buses including an industry standard architecture (ISA) bus, a\nperipheral component interconnect (PCI) bus, a universal serial bus (USB), or some other bus known in the art to provide similar functionality.\n The processor 240 comprises an arithmetic logic unit, a microprocessor, a general purpose controller or some other processor array to perform computations and provide electronic display signals to a display device.  The processor 240 is coupled\nto the bus 220 for communication with the other components of the server 101 via signal line 231.  The processor 240 processes data signals and may comprise various computing architectures including a complex instruction set computer (CISC) architecture,\na reduced instruction set computer (RISC) architecture, or an architecture implementing a combination of instruction sets.  Although only a single processor is shown in FIG. 2A, multiple processors may be included.  The processing capability may be\nlimited to supporting the display of images and the capture and transmission of images.  The processing capability might be enough to perform more complex tasks, including various types of feature extraction and sampling.  It will be obvious to one\nskilled in the art that other processors, operating systems, sensors, displays and physical configurations are possible.\n The memory 245 stores instructions and/or data that may be executed by processor 240.  The memory 245 is coupled to the bus 220 for communication with the other components of the NAaaS middleware server 101 via signal line 233.  The instructions\nand/or data may comprise code for performing any and/or all of the techniques described herein.  The memory 245 may be a dynamic random access memory (DRAM) device, a static random access memory (SRAM) device, flash memory or some other memory device\nknown in the art.  In one embodiment, the memory 245 also includes a non-volatile memory or similar permanent storage device and media such as a hard disk drive, a floppy disk drive, a CD-ROM device, a DVD-ROM device, a DVD-RAM device, a DVD-RW device, a\nflash memory device, or some other mass storage device known in the art for storing information on a more permanent basis.\n The communication unit 250 is hardware for receiving and transmitting data by linking the processor 240 to the network 104 and other processing systems.  The communication unit 250 receives data such as images, videos or documents from a\nplurality of user devices 102a-102n.  The communication unit 250 receives requests for media associated with a particular subject from the web server 120.  The communication unit 250 also receives requests for one or more types of media from the web\nserver 120.  The communication unit 250 transmits information to the plurality of collaborative computing devices 103a-103n.  For example, the communication unit 250 transmits graphical data for displaying images, documents or videos.  The communication\nunit 250 is coupled to the bus 220 for communication with the other components of the NAaaS middleware server 101 via signal line 235.\n In one embodiment, the communication unit 250 includes a port for direct physical connection to the user devices 102, the collaborative computing devices 103, the resource servers 105, the NAaaS application server 123, the web server 120 or to\nanother communication channel.  For example, the communication unit 250 includes an RJ14 or similar port for wired communication with the ESB 107.  In another embodiment, the communication unit 250 includes a wireless transceiver for exchanging data with\nthe user devices 102 or any other communication channel using one or more wireless communication methods, such as IEEE 802.11, IEEE 802.16, Bluetooth.RTM.  or another suitable wireless communication method.\n In yet another embodiment, the communication unit 250 includes a cellular communications transceiver for sending and receiving data over a cellular communications network such as via short messaging service (SMS), multimedia messaging service\n(MMS), hypertext transfer protocol (HTTP), direct data connection, WAP, e-mail or another suitable type of electronic communication.  In still another embodiment, the communication unit 250 includes a wired port and a wireless transceiver.  The\ncommunication unit 250 also provides other conventional connections to the network for distribution of files and/or media objects using standard network protocols such as TCP/IP, FTP, HTTP, HTTPS and SMTP as will be understood to those skilled in the\nart.\n The data storage 255 is a non-transitory memory that stores data for the functionality of the NAaaS middleware server 101.  The data storage 255 is coupled to the bus 220 for communication with other components of the server 101 via signal line\n237.\n In one embodiment, the data storage 255 stores a library of communication protocols and messaging formats for protocol conversion.  The communication protocols and messaging formats that the data storage 255 stores include, for example, Simple\nObject Access Protocol (SOAP), eXtensible Markup Language (XML), Java Message Service (JMS), File Transfer Protocol (FTP), Hypertext Transfer Protocol (HTTP), Message Queue (MQ), Internet Inter-Orb Protocol (HOP), Representational State Transfer (REST),\nJavaScript Object Notation (JSON), Distributed Component Object Model (DCOM), Simple Network Management Protocol (SNMP), etc. In some embodiments, the protocol adaptation engine 205 accesses the protocols and messaging formats to convert requests into a\nprotocol and messaging format that is compatible with the recipient.\n The enterprise service bus 107 includes: a workflow engine 201, a device management engine 203, a protocol adaptation engine 205, a message transformation engine 207 and a message enhancement engine 209.  These components of the enterprise\nservice bus 107 are communicatively coupled to each other via the bus 220.\n The workflow engine 201 is software and routines for performing basic enterprise service bus functionalities and for handling communications between the components of the NAaaS middleware server 101 and other components of the system 100.  In\none embodiment, the workflow engine 201 is a set of instructions executable by the processor 240 to provide the functionality described below for receiving a request, routing the request, performing several steps and interacting with the NAaaS\napplication services 106 and the collaborative computing devices 103 and their corresponding resource servers 105a-105n to satisfy the request.  In either embodiment, the workflow engine 201 is adapted for cooperation and communication with the processor\n240, the communication unit 250 and other components of the NAaaS middleware server 101 via the signal line 222.\n The workflow engine 201 receives a request, processes the request and communicates with the NAaaS application services 106 and the collaborative computing devices 103 and their corresponding resource servers 105a-105n to complete the request. \nFor example, the workflow engine 201 receives a request from a user device 102 for one of the collaborative computing devices 103 to project an image.  The workflow engine 201 authenticates the user associated with the user device 102 by interacting with\nthe user management service 210 that is part of the NAaaS application services 106, instructing the device usage analytics service 212 to log the command, a user identifier for the user associated with the user device 102, the date and time of the\nrequest and the IP address of the user device 102, copying the image that the user sent to the repository by interacting with the media repository services 206, performing optical character recognition of the image and indexing keywords in the image by\ninteracting with the media analysis service 208 and transmits the image to the collaborative computing device 103 for projection.\n The workflow engine 201 receives information via the communication unit 250 and transmits the information to the appropriate component of the enterprise service bus 107 or components of the system 100.  In one embodiment, the workflow engine 201\nreceives a request to transmit media to a collaborative computing device 103.  The workflow engine 201 can receive one or more types of media.  The workflow engine 201 routes or transmits the media to the collaborative computing device 103.  For example,\nthe workflow engine 201 receives an image from a user device 102 (e.g., a smart phone) for display by a collaborative computing device 103 (e.g., a projector or a monitor).\n In another embodiment, the workflow engine 201 receives a request from a web server 120 for a list of media in a particular area or subject.  The workflow engine 201 transmits the request to the NAaaS application services 106 to perform a search\nof the data storage 268 based on search terms included in the request.  Once the NAaaS application services 106 return matching results, the workflow engine 201 transmits the results to a requestor at the user device 102 that submitted the request to the\nweb server 120.  In one embodiment, responsive to determining the context of the request, the workflow engine 201 determines one or more resource servers 105 that the request is directed toward based on the request matching a global index.  The workflow\nengine 201 accesses the global index stored in the data storage 268.\n The requestor may provide the search query for one or more types of media associated with a subject.  For example, the requestor may request to find media associated with Linux.  The search query may include text.  For example, the requestor\nprovides the word \"Linux\" in a text input box of a search user interface generated by the user interface engine 121 on the web server 120.  The search query may include visual information.  For example, the requestor may provide an image or video of Tux,\nthe mascot of Linux.  The search query may include audio of a voice.  For example, the requestor may provide audio of a voice saying the word \"Linux.\" The workflow engine 201 transmits the search query to the NAaaS application services 106 for\nprocessing.\n In another embodiment, the workflow engine 201 receives requests and sends the requests to the protocol adaptation engine 205 and instructs the protocol adaptation engine 205 to send the requests to the search engine 221, which is one of the\nNAaaS application services 106.  In one embodiment, the workflow engine 201 receives a request from an Application Programming Interface (API) associated with a first device in a first communication protocol for translation.  In one embodiment, the\nrequests are buffered in the message queue of the enterprise service bus 107 and the workflow engine 201 polls the message queue periodically to process the requests in queue.  The workflow engine 201 determines the context of the request.  For example,\nan HTTP request with a first line such as \"category\": \"SEARCH\" provides a context to the workflow engine 201 that the request is for searching one or more resource servers, a second line such as \"service\": \"video\" provides a context to the workflow\nengine 201 that the request is for the one or more resource servers associated with storing video recordings, a third line such as \"type\": \"video device X\" provides further context to the workflow engine 201 that the one or more resource servers are\nassociated with a device of \"video device X\" type, a fourth line such as \"arguments\": {\"userid\": \"u_0613902\", \"URL\": \"10.154.25.9\", \"date\": \"11/21/2012\", \"filename\": \"iOS\"} further narrows the context for the workflow engine 201.  In situations where the\nprotocol needs to be translated to communicate with the resource servers 105, the workflow engine instructs the protocol adaptation engine 205 to translate the request appropriately.\n The device management engine 203 is code and routines for determining a location of the one or more types of devices, functionality associated with the one or more types of devices and a status of the one or more types of devices.  In one\nembodiment, the device management engine 203 is a set of instructions executable by the processor 240 to provide the functionality described below for determining location, functionality and status associated with the one or more types of devices.  In\nanother embodiment, the device management engine 203 is stored in the memory 245 and is accessible and executable by the processor 240.  In either embodiment, the device management engine 203 is adapted for cooperation and communication with the\nprocessor 240, the communication unit 250, the controller 202 and other components of the NAaaS middleware server 101 via signal line 223.\n In one embodiment, the device management engine 203 communicates with the NAaaS application services 106 to determine the location of the one or more types of devices.  For example, the device management engine 203 receives a query from the user\ndevice 102 to discover the location of the user device 102 and discover the collaborative computing devices 105.  The device management engine 203 interacts with the location service 299 that is part of the NAaaS application services 106 to match the\nlocation of the user device 102 with a known location.  In another embodiment, the device management engine 203 transmits the location of the collaborative computing device 103 to the user device 102.\n In one embodiment, the device management engine 203 performs a device discovery process that works via the network 104 using specific protocols like SNMP, ICMP, Bonjour, etc. For example, the device management engine 203 uses SNMP or ICMP\nprotocols to discover devices together with their internet protocol (IP) address, media access control (MAC) addresses, etc. In another embodiment, the device management engine 203 determines location information of a device by using global positioning\nsystem (GPS) circuitry included within the device itself.  In another embodiment, the device management engine 203 determines the device location indoors as the device employs radio frequency, ultra-sound signal or invisible light communication.  For\nexample, the device determines its location through wireless access points based on measuring the intensity of received signals.  The device management engine 203 accesses a database including pairs of media access control (MAC) addresses and locations\nover the Internet.  To determine a location, the device management engine 203 retrieves the location corresponding to the access point MAC address from the database.\n In one embodiment, once the device management engine 203 receives the user device 102 location from the location service 299, the device management engine 203 asks a device inventory service 265 that is part of the NAaaS application services 106\nto provide a list of devices given a certain location.  The device management engine 203 then requests that a device management service 212 that is also part of the NAaaS application services 106 filter the list of devices based on the availability of\nthe devices.\n In one embodiment, the request from the user device 102 specifies a location associated with the collaborative computing devices 105.  For example, a user wants to know about devices in a first conference room in Tokyo, Japan because the user is\nscheduled to give a presentation from a second conference room in California, USA to people who will be in the first conference room.  In another example, a user creates a calendar event at a location and wants to know the list of available devices at\nthe location of the event.  In one embodiment, the location service 299 matches the location associated with a request to a known location from the data storage 268 of NAaaS application services 106.  A location identifier is sent to the device\nmanagement engine 203 by the location service 299.  In another embodiment, the device inventory service 265 provides a list of all devices on the network 104 to the device management engine 203 responsive to the device management engine 203 using the\nlocation identifier of the user device 102 for querying the device inventory service 265.  In yet another embodiment, the device management engine 203 queries the device management service 212 to limit the list of available devices to a particular area\nand the device management service 212 transmits the information back to the device management engine 203.  For example, a location of an available projector from a list of collaborative computing device 105 can be identified to be in a first conference\nroom inside a building in Tokyo, Japan.\n In one embodiment, the device management engine 203 determines a list of functions associated with each of the plurality of devices.  For example, the list of functions include power on, power off, projection, zoom, enhance, automatic focus,\nprint, two-way video recording and transmission, two-way audio recording and transmission, language translation, text to speech translation and speech to text translation, etc.\n In one embodiment, the device management engine 203 determines a current status of one or more devices.  For example, a status of a projector in a conference room can be determined to be in use, out of service, idle, off, etc. In another\nembodiment, the device management engine 203 determines that one or more devices reserved by a user are idle for a set amount of time.  In one embodiment, the device management engine 203 interacts with the device management service 212 that is part of\nthe NAaaS application services 106 to update the status of a device.\n In one embodiment, the device management engine 203 receives an access control list for a calendar event from the workflow engine 201 and determines the one or more user devices 102 associated with the people participating in the calendar event. The device management engine 203 determines the one or more user devices 102 by querying the device inventory service 265 using the access control list.  For example, the device management engine 203 uses the user identifiers in the access control list\nand queries the device inventory service 265 for a list of devices associated with the users participating in the calendar event.  The device inventory service 265 provides the device management engine 203 with a list of device identifiers for the one or\nmore user devices 102 associated with the users participating in the calendar event.\n In one embodiment, the device management engine 203 determines the location of one or more user devices 102 and consequently the location of the users carrying the user devices 102 prior to the start of the calendar event.  For example, the\ndevice management engine 203 determines the location of user devices five minutes prior to the start of the calendar event.  In another embodiment, the device management engine 203 determines the location of the one or more users within a threshold\ndistance from the venue of the calendar event.  For example, a first user might be in an adjacent building near the venue and a second user might be negotiating road traffic five miles from the venue where the calendar event is scheduled.  The device\nmanagement engine 203 interacts with the device inventory service 265 and the location service 299 in the NAaaS application services 106 to store the determined location information.\n In one embodiment, when all the users in the access control list are determined to be present at the venue, the device management engine 203 interacts with the notification service 269 to generate a notification that suggest to start the\ncalendar event.  In another embodiment, when at least one user in the access control list is absent from the venue, the device management engine 203 interacts with the notification service 269 to generate a notification to delay the start of the event.\n The protocol adaptation engine 205 is software and routines for adapting and translating protocols.  In one embodiment, the protocol adaptation engine 205 is a set of instructions executable by the processor 240 to provide the functionality\ndescribed below for adapting and translating protocols.  In either embodiment, the protocol adaptation engine 205 is adapted for cooperation and communication with the processor 240, the communication unit 250 and other components of the NAaaS middleware\nserver 101 via the signal line 224.\n In one embodiment, the protocol adaptation engine 205 receives a request from the workflow engine 201 in a first communication protocol and performs adaptation and translation to make the communication protocol compatible with the recipient of\nthe request.  For example, the protocol adaptation engine 205 receives an HTTP request from the user device 102 to \"turn off\" a projector.  The projector communicates using TCP.  The protocol adaptation engine 205 adapts an HTTP request to be forwarded\nto a TCP endpoint and transforms the \"turn off\" HTTP request to its counterpart TCP command.\n In one embodiment, the protocol adaptation engine 205 identifies one or more types of second communication protocols associated with the one or more resource servers.  For example, the protocol adaptation engine 205 identifies that a first\nresource server 105a associated with a projector device uses a Transmission Control Protocol (TCP) as a communication protocol, a second resource server 105b associated with an interactive whiteboard uses a Simple Network Management Protocol (SNMP) as a\ncommunication protocol and so on.  In another embodiment, the protocol adaptation engine 205 stores the one or more protocols in the data storage 255 for each of the one or more resource servers 105a-105n.\n In one embodiment, the protocol adaptation engine 205 translates the request from the first communication protocol into each one of the one or more types of second communication protocols associated with the one or more collaborative computing\ndevices 103a-103n and resource servers 105a-105n, respectively.  For example, the protocol adaptation engine 205 translates request from an HTTP protocol to a TCP protocol that the first resource server 105a associated with the projector device can\nunderstand, translates the HTTP protocol into an SNMP protocol that the second resource server 105b associated with the interactive whiteboard can understand and so on.  In yet another embodiment, the workflow engine 201 receives the request which is\npassed to the protocol adaptation engine 205 which sends the translated request to the message queue in the enterprise service bus 107 from where the translated requests are routed to the appropriate resource servers 105 and their APIs.\n The message transformation engine 207 is software and routines for transforming message formats.  In one embodiment, the message transformation engine 207 is a set of instructions executable by the processor 240 to provide the functionality\ndescribed below for transforming message formats.  In either embodiment, the message transformation engine 207 is adapted for cooperation and communication with the processor 240, the communication unit 250 and other components of the NAaaS middleware\nserver 101 via the signal line 225.\n In one embodiment, the message transformation engine 207 transforms message formats in requests from the user device 102 going toward a collaborative computing device 103.  For example, the message transformation engine 207 transforms the\npayload from an eXtensible Markup Language (XML) to JavaScript Object Notation (JSON).\n Continuing with the above example, the request includes a JSON message that is passed to the message transformation engine 207 and the message enhancement engine 209:\n TABLE-US-00001 \"naaasprotocol\": \"1.0\", \"device\": { \"category:\" \"PJS\", \"type\": \"SOLEIL\", \"url\": \"10.154.25.9\" } \"ricohuserid_opt\": \"3\" }\n Where \"naaasprotocol\" is the Web API version of the platform, \"PJS\" designates the projector devices, \"SOLEIL\" is a type of projector, \"url\" is the device IP address for the projector and \"ricohuserid_opt\" is the ID of the user operating the\ndevice.  In one embodiment, the message transformation engine 207 transforms the JSON message into XML.\n In another embodiment, the message transformation engine 207 transforms message formats for querying resource servers 105.  For example, the message transformation engine 207 identifies a request with a messaging standard in JavaScript Object\nNotation (JSON) and translates the JSON to an eXtensible Markup Language (XML) for a first resource server 105a.  When the resources are associated with resource servers that use different messaging formats, the message transformation engine 207\ntranslates messages into multiple formats.\n The message enhancement engine 209 is software and routines for enhancing messages.  In one embodiment, the message enhancement engine 209 is a set of instructions executable by the processor 240 to provide the functionality described below for\nenhancing messages.  In either embodiment, the message enhancement engine 209 is adapted for cooperation and communication with the processor 240, the communication unit 250 and other components of the NAaaS middleware server 101 via the signal line 226.\n The message enhancement engine 209 can also enhance the message by adding information not originally present in the request for interacting with a collaborative computing device 103.  For example, continuing with the example above, where the\nuser wants to project an image onto a projector that requires additional authentication information, the message enhancement engine 209 retrieves the additional authentication information, such as a password, from the data storage 255 using a user\nidentifier in the request and adds it to the message.  Once the message is ready, the message enhancement engine 209 transmits the message to the workflow engine 201 in the enterprise service bus 107, which transmits the message to the collaborative\ncomputing device 103 or the NAaaS application services 106 for retrieving media, user profiles, etc. from the resource servers 105.\n NAaaS Application Services 106\n FIG. 2B illustrates one embodiment of a NAaaS application server 123 that comprises NAaaS application services 106, a processor 262, a memory 264, a communication unit 266 and data storage 268.  Some of the components of the NAaaS application\nserver 123 have similar function and form as has been described above with reference to FIG. 2A so like reference numbers and terminology have been used to indicate similar functionality.  For example, the communication bus 220, the processor 240, the\nmemory 225 and the communication unit 250 are similar to that described above with reference to FIG. 2A so they will not be described here again.\n In one embodiment, the data storage 260 device usage entries, an index of the media, the media and user profiles.  The device usage entry describes transactions executed on the collaborative devices 103 and user identifiers associated with the\ntransaction.  In some embodiments, the device usage entry includes multiple user identifiers.  For example, the device usage entry includes a user identifier for the presenter, e.g. a first user that sends a request from a user device 102 to project an\nimage from a collaborative device 103 and a user identifier for the author of the media The device usage entry includes the type of request (e.g., project, power on, power off, etc.), the type of device involved in the exchange of request and service\n(e.g., smart phone, projector, etc.), an IP address for the device, a measure of device resource spent (e.g., time, power, etc.), a type of functionality of the device used (e.g., auto-focus, enhance, imaging, etc.), a type of media exchanged (e.g., a\npresentation program document, a text document, a spreadsheet document, a video recording, an audio recording, an image, etc.), etc.\n The data storage 260 stores an index of media.  In one embodiment, the index of media includes records for each media including metadata for each media.  For example, the metadata may include pointer data for accessing the original media (e.g. a\nfull presentation instead of merely an image of a slide of the presentation) from the cloud, an author of the media, etc. In one embodiment, the metadata also includes results from the media analysis service 217, such as a text version of the image.  In\none embodiment, the data storage 260 also stores a copy of the media included in the requests.  For example, the data storage 260 receives a copy of the media from the media repository services 214.\n The data storage 260 stores user profiles.  In one embodiment, the user profiles include records for each user.  The records for each user may include a graphical representation of the user (e.g. a photo of the user), name, a title, keywords\nrelated to the user, media associated with the user (e.g., media authored by the user or media presented by the user), presentations associated with the user, etc. In one embodiment, the keywords related to the user include a list of keywords.  The NAaaS\napplication services 106 is a collection of individual lower-level services with individual application programming interfaces (APIs) that are composed by the enterprise service bus 107 to deliver higher services.  For example, a \"project\" command sent\nby a user device 107 to the enterprise service bus 107 will invoke a module in the NAaaS application services 106 that will authenticate the user device, identify text in the media and save the document in the data storage 260.  The services do not\ncommunicate with each other.  Instead the services receive instructions from the enterprise service bus 107, complete the requested task, save data in the data storage 260 if applicable and return information to the enterprise service bus 107.\n The NAaaS application services 106 is a collection of individual lower-level services with individual application programming interfaces (APIs) that are composed by the enterprise service bus 107 to deliver higher services.  For example, a\n\"project\" command sent by a user device 102 to the enterprise service bus 107 will invoke a module in the NAaaS application services 106 that will authenticate the user device 102, identify text in the media, create a database index and save the document\nalong with the database index in the data storage 268.  The services do not communicate with each other.  Instead the services receive instructions from the enterprise service bus 107, complete the requested task, save data in the data storage 268 if\napplicable and return information to the enterprise service bus 107.\n In one embodiment the services include a user management service 211, a device inventory service 265, a location service 299, a device management service 212, a device usage analytics service 213, a media repository service 215, a media analysis\nservice 217, an indexing service 287, a calendar module 286, a billing service 267, a search engine 221, a server maintenance service 227 and a notification service 269.  Persons of ordinary skill in the art will recognize that the enterprise service bus\n107 can compose additional services to complete requests.\n The user management service 211 is software and routines for registering users in the network 104 and performing authentication of users.  In one embodiment, the user management service 211 is a set of instructions executable by the processor\n262 to provide the functionality described below for registering users.  In another embodiment, the user management service 211 is stored in the memory 264 and is accessible and executable by the processor 262.  In either embodiment, the user management\nservice 211 is adapted for cooperation and communication with the processor 262 and the communication unit 266 via signal line 271.\n The user management service 211 receives user information and generates a user profile.  For example, the user management service 211 receives a name, a job title, a job code, an e-mail address, a phone number, a username, a password, a retina\nscan of the user, a fingerprint swipe of the user, etc. The user management service 211 generates login credentials for the user based on the registration information and stores the login credentials in the data storage 268 for the user.  The user\nmanagement service 211 associates a unique identifier with the user.  This can be the user's full name, an email address for the user, a series of numbers, an employee identifier, etc. The unique identifier is used to track the user's activities in the\nsystem.\n In one embodiment, the login credentials are generated as a single sign-on with a property that provides access control to multiple and independent devices and services using the same login credentials.  For example, a user logs in with the user\ncredentials and gains access to all registered services without being prompted to login at each one of them.  In another example, a user logs into different types of devices such as a projector, an interactive whiteboard, etc. using the same user\ncredentials.  In one embodiment, the user management service 211 stores the login credentials of users as an organized set of records with a hierarchical structure in a Lightweight Directory Access Protocol (LDAP) server (not shown) associated with a\nbusiness.  The user management service 211 also manages preferences for the user that are received during registration or at other times.  For example, the user can upload an image to associate with the login credentials.\n In one embodiment where the user devices 102 and/or collaborative computing devices 103 use different login information than the login information used to authenticate the user with the system 100, the user management service 211 receives the\nlogin information from the user or an administrator and adds the information to the user's profile.  For example, a user's laptop includes a four-digit code that needs to be input before the user can access the contents of the laptop.  The user\nmanagement service 211 adds the information to the user's profile.\n The user management service 211 performs authentication.  For example, a user enters login credentials into a user interface on the user device 102.  The user device 102 transmits the login credentials to the enterprise service bus 107, which\nrequests that the user management service 211 authenticate the user based on the login credentials.  The user management service 211 identifies the user associated with the user device 102, compares the login credentials to the user profile and either\nsends a confirmation back to the enterprise service bus 107 that the login credentials were correct or a notification that there was a login error.  The confirmation includes the user identification associated with the user.\n In one embodiment, if the login request for authentication does not include a correct user name and password, the login request is denied.  In another embodiment, the user management service 211 is used in account recovery when the user has\nforgotten his, or her, username and/or password.  In some instances, the user management service 211 detects a potentially fraudulent authentication by analyzing the secondary information included in the login request.  For example, if the username\nassociated with the login request is a username of a user no longer employed at a company, or a suspected bot, the user management service 211 identifies the login request as potentially fraudulent.  The user management service 211 detects a potentially\nfraudulent authentication by comparing the secondary information to historical authentication information of the user.  For example, if the login request for authentication originates from a country, or device, that the user has not attempted to send\nlogin requests from before (e.g., unregistered device), the user management service 211 identifies the login request as potentially fraudulent.  In one embodiment, a potentially fraudulent authentication attempt includes a failed authentication attempt.\n The device inventory service 265 is code and routines for registering devices in the network 104.  In one embodiment, the device inventory service 265 is a set of instructions executable by the processor 262 to provide the functionality\ndescribed below for registering devices.  In another embodiment, the device inventory service 265 is stored in the memory 264 and is accessible and executable by the processor 262.  In either embodiment, the device inventory service 265 is adapted for\ncooperation and communication with the processor 262 and the communication unit 266 via signal line 293.\n The device inventory service 265 receives requests to add, remove and update devices in the network 104 from the workflow engine 201.  The device inventory service 265 receives a request to register one or more types of user devices 102a-102n\nand one or more types of collaborative computing devices 103a-103n.  In one embodiment, the device inventory service 265 registers the type of device and an IP address (or MAC address) for the device with the network 104 and creates a device identifier\nspecific for the device.  For example, a new projector with model number 1042 is registered and assigned a device identifier, such as, \"prj1042u1\".  In one embodiment, the device inventory service 265 receives a request to register one or more types of\nuser devices 102a-102n under a particular username stored in the data storage 268.  For example, a user can register personal devices such as a tablet PC, a smartphone, etc. and associate the devices with a form of identification, such as an employee\nidentifier, user identifier, etc. In one embodiment, the device inventory service 265 maps the location with the device by importing the location identifier provided by the location services 299.  In another embodiment, the device inventory service 265\nreceives a request to register the one or more types of collaborative computing devices 103a-103n for use at a particular location or within certain limits of space.  For example, a projector can be registered for use on the first floor of a building and\nan interactive whiteboard can be registered for use in a conference room.  The first floor of the building and the conference room each has a location identifier associated with them.  The device inventory service 265 stores the device registration\ninformation (e.g., IP addresses, device identifiers, etc.) in the data storage 268.\n In another embodiment, the device inventory service 265 identifies devices associated with a location using a location identifier generated by the location service 299 which is explained in further detail below.  The location identifier is\nreceived from the device management engine 203 in the enterprise service bus 107 for storing in the data storage 268.  For example, a projector can be registered to a conference room in a building in Tokyo, Japan and the conference room has a location\nidentifier.  A user can access the projector from San Francisco, Calif.  using the login credentials indicating the user is a registered user.  In yet another embodiment, the device inventory service 265 receives requests to update information associated\nwith the devices that are registered.  For example, a user can change the name of a projector, the location of an interactive whiteboard and the firmware version on the video conferencing device, etc.\n The location service 299 is code and routines for providing and storing location information of one or more types of devices.  In one embodiment, the location service 299 is a set of instructions executable by the processor 262 to provide the\nfunctionality described below for storing device location information.  In another embodiment, the location service 299 is stored in the memory 264 and is accessible and executable by the processor 262.  In either embodiment, the location service 299 is\nadapted for cooperation and communication with the processor 262 and the communication unit 266 via signal line 298.\n In one embodiment, the device management engine 203 from the enterprise service bus 107 interacts with the location service 299 to store location information including name, geographical coordinates, etc. In another embodiment, the location\nservice 299 receives a request to register a location before devices can be registered or assigned to the location.  For example, a user may register the location of a new branch in Singapore before another user from Menlo Park, Calif.  can send media to\nthe devices that are registered at the Singapore location.\n The device management service 212 is code and routines for reserving devices in the network 104 and granting access to the devices.  In one embodiment, the device management service 212 is a set of instructions executable by the processor 262 to\nprovide the functionality described below for reserving devices.  In another embodiment, the device management service 212 is stored in the memory 264 and is accessible and executable by the processor 262.  In either embodiment, the device management\nservice 212 is adapted for cooperation and communication with the processor 262 and the communication unit 266 via signal line 272.\n In one embodiment, the device management service 212 receives a request from the device management engine 203 to reserve one or more types of collaborative computing devices 103a-103n that are shared with a group of people, for example,\nemployees in a company.  For example, a user employee can reserve a projector, an interactive whiteboard, etc. temporarily under his username or user identifier for use in a calendar event (for e.g., business meeting).  In another example, the user\nemployee can supply a device identifier along with the user identifier for reserving devices.  The device management service 212 grants an access request for a user of the group to the one or more types of collaborative computing devices 103a-103n that\nare registered previously using the user's login credentials created by the user management service 211.  For example, the user accesses collaborative computing devices 103a-103n such as a projector, a webcam, an interactive whiteboard, etc. using one\nand the same username and password.\n In one embodiment, the device management service 212 grants access to the one or more types of collaborative computing devices 103a-103n reserved for a calendar event.  For example, the device management service 212 grants access to a reserved\nprojector in the \"conference room 4C\" for a calendar event \"Staff Meeting\" at 10 AM.  In another embodiment, the device management service 212 receives a confirmation from a user organizer of the calendar event via the device management engine 203 to\npower on (or power off) and connect (or disconnect) the collaborative computing devices 103a-103n at a start time (or stop time) as indicated in the calendar event.\n The device usage analytics service 213 is software and routines for logging device usage entries associated with the requests in the network 104.  In one embodiment, the device usage analytics service 213 is a set of instructions executable by\nthe processor 262 to provide the functionality described below for logging device usage entries.  In another embodiment, the device usage analytics service 213 is stored in the memory 264 and is accessible and executable by the processor 262.  In either\nembodiment, the device usage analytics service 213 is adapted for cooperation and communication with the processor 262 and the communication unit 266 via signal line 273.\n The device usage analytics service 213 receives a request and a user identifier associated with the request and logs the metadata associated with the request as a device usage entry.  If the user associated with request is different from the\nauthor of the media, the device usage entry includes a user identifier for both the presenter and the author of the media.  For example, a doctor gives a talk about a medical subject to residents at a hospital by using slides that are transmitted from\nthe doctor's user device 102 (e.g. a laptop) to a collaborative computing device 103 (e.g. a projector).  Each time the doctor wants to display a slide on the projector, the doctor sends a request to display an image of the slide from the laptop to the\nprojector.  The enterprise service bus 107 transmits the request to the user management service 211, which identifies the user associated with the user device 102.  The enterprise service bus 107 receives a confirmation of authentication from the user\nmanagement service 211 and an identity of the user and transmits the request and user identifier to the device usage analytics service 213, which logs a device usage entry.  In one embodiment, the device usage entry includes a user identifier for the\nuser associated with the user device 102, an author of the media (if different), a set of actions performed on the collaborative computing device 103, a unique device identifier of the collaborative computing device 103 and a unique identifier (or\nmetadata associated with the media) referring to the stored media in the data storage 268.\n The media repository service 215 is code and routines for storing media associated with a request in data storage 268.  In one embodiment, the media repository service 215 is a set of instructions executable by the processor 262 to provide the\nfunctionality described below for storing media.  In another embodiment, the media repository service 215 is stored in the memory 264 and is accessible and executable by the processor 262.  In either embodiment, the media repository service 215 is\nadapted for cooperation and communication with the processor 262 and the communication unit 266 via signal line 275.\n In some embodiments, a collaborative computing device 103 is lacking a backend server that stores media.  In other embodiments, the data storage 268 includes a copy of media in the request, for example, an image from a PowerPoint presentation. \nIn these instances, the media repository service 215 receives a request from the enterprise service bus 107 that includes media.  The media repository service 215 generates a unique identifier associated with the media and stores the media in the data\nstorage 268.  In one embodiment, the media repository service 215 receives one or more types of media to associate with a device identifier of a collaborative computing device 103 from the workflow engine 201 and stores the media and the associated\ndevice identifier in the data storage 268.  In another embodiment, the media repository service 215 receives device identifier along with the user identifier for storing one or more types of media.\n The media analysis service 217 is code and routines for analyzing media.  In one embodiment, the media analysis service 217 is a set of instructions executable by the processor 262 to provide the functionality described below for analyzing\nmedia.  In another embodiment, the media analysis service 217 is stored in the memory 264 and is accessible and executable by the processor 262.  In either embodiment, the media analysis service 217 is adapted for cooperation and communication with the\nprocessor 262 and the communication unit 266 via signal line 277.\n The media analysis service 217 receives media associated with a request from the enterprise service bus 107.  For example, the media analysis service 217 receives an image that was captured from a slide.  The media analysis service 217 applies\noptical character recognition to the image to identify text associated with the image.  The media analysis service 217 stores the text in the data storage 260.  In one embodiment, the media analysis service 217 converts content from the image including\nhandwritten, typewritten or printed text to machine-encoded text.\n In one embodiment, the media analysis service 217 receives video and/or audio data.  The media analysis service 217 may identify a user associated with the video and/or audio data received from a video conferencing device.  For example, a video\nmay include the doctor from the example above giving the presentation in front of an interactive whiteboard.  The media analysis service 217 may identify the doctor based on performing facial recognition on the video data or performing voice analysis on\nthe audio data.  In another embodiment, the media can be provided by audience members that participate in an event.  The media analysis service 217 determines an event.  For example, the media analysis service 217 determines a presentation.  The media\nanalysis service 217 determines audience members at an event.  In some embodiments, the media analysis service 217 determines audience member attendance based on location of a user device 102.  The media analysis service 217 determines which audience\nmembers participated in the event.  In one embodiment, the media analysis service 217 determines which audience members participated in an event based on performing facial recognition from video data or performing voice analysis on the audio data.\n In one embodiment, the media analysis service 217 receives video data of an event or a presentation from the enterprise service bus 107.  For example, the video includes a person in front of an interactive whiteboard for presenting information. \nThe media analysis service 217 may perform optical character recognition on one or more frames of the video.  For example, the media analysis service 217 performs optical character recognition on the information presented on the interactive whiteboard. \nIn another embodiment, the media analysis service 217 receives audio data.  The media analysis service 217 may identify text from the audio data by using speech-to-text technology.\n The indexing service 287 is software and routines for creating one or more database indices for a plurality of resource servers 105a-105n.  In one embodiment, the indexing service 287 is a set of instructions executable by the processor 262 to\nprovide the functionality described below for creating the database indices.  In another embodiment, the indexing service 287 is stored in the memory 264 and is accessible and executable by the processor 262.  In either embodiment, the indexing service\n287 is adapted for cooperation and communication with the processor 262 and the communication unit 266 via signal line 245.\n In one embodiment, the indexing service 287 determines one or more types of media stored in one or more resource servers 105.  The one or more resource servers 105 are associated with one or more types of collaborative computing devices 103. \nFor example, a resource server 105 associated with a collaborative computing device 103 (e.g., a projector) stores slides that are transmitted from the user device 102 (e.g., a laptop) to the collaborative computing device 103.  The indexing service 287\nidentifies metadata associated with the one or more types of media and creates a database index on the database storage 268.  The database index holds the metadata and a reference pointer to a location where the one or more types of media related to the\nmetadata are stored.  In one embodiment, the indexing service 287 stores the database index in the data storage 268.\n The indexing service 287 creates a database index based on a cardinality of metadata.  The cardinality refers to the uniqueness of the metadata.  The lower the cardinality, the more duplicity present in the metadata.  In one embodiment, the\nindexing service 287 identifies metadata with a high-cardinality (for e.g., user identifier, username, employer identifier, email address, title, specific keywords, etc.) for creating an index.  For example, the indexing service 287 identifies a user\nidentifier associated with the user that transmitted the slides to the projector and creates a database index that holds the user identifier and a pointer pointing to a location where the slides related to that user identifier are stored.  In another\nembodiment, the indexing service 287 identifies metadata with a normal-cardinality (e.g., device name, device type, device identifier, internet protocol (IP) address, media type, location, date of event, etc.).  For example, the indexing service 287\nidentifies a device identifier associated with the projector that projected the slides and creates a database index that holds the device identifier and a pointer pointing to a location where the slides related to that device identifier are stored.  In\nyet another embodiment, the indexing service 287 identifies metadata with a low-cardinality (e.g., current employee, former employee, junior employee, senior employee, etc.).  For example, the indexing service 287 identifies that the slides were\nprojected by an employee who is a junior employee and creates a database index that holds the expertise level and a pointer pointing to a location where the slides related to that expertise level are stored.\n In one embodiment, the workflow engine 201 receives identified text from the media analysis service 217 that analyzed the media for the workflow engine 201.  For example, the media analysis service 217 extracts text from video and/or audio data\nsent by the workflow engine 201.  The workflow engine 201 sends the text to the indexing service 287 for identifying keywords in the media, indexing data related to the keywords in the media and creating a global index that includes the keywords and\npointers to the media in the resource servers 105.  In one embodiment, the indexing service 287 also associates the keywords with user profiles.  In one embodiment, the indexing service 287 determines one or more parts of the media.  The indexing service\n287 determines a title, a table of contents, an overview, a key information section, etc. of one or more slides of a slide show.  The indexing service 287 may determine parts of the one or more slides based on location of text, formatting of text, a\nposition of a slide in the slide show, etc. For example, the title of a slide or slide show may appear at the top of a first slide of the slide show appearing in a bold font.  The title may include text having a size that is larger relative to other text\non the slide or other slides.  In another example, a slide having a position at the beginning of the slide show may include an overview of the slide show.  In another example, the indexing service 287 determines a key information section based on\nidentifying text having one or more bullet points.\n The indexing service 287 determines weights for the keywords.  In one embodiment, the indexing service 287 determines weights based on the parts.  For example, the indexing service 287 determines a higher weight for a keyword from the title of a\nslide show than other parts (e.g., text) of the slide show.  In another embodiment, the indexing service 287 determines weights based on a count associated with a keyword.  For example, the indexing service 287 identifies the number of times each keyword\nappears in the media.\n In one embodiment, the indexing service 287 determines related content.  For example, the indexing service 287 determines synonyms for the keywords.  In one embodiment, the indexing service 287 performs a search in a thesaurus.  In another\nexample, the indexing service 287 determines related content based on a knowledge graph.\n The indexing service 287 indexes the keywords and synonyms in the database indices for the plurality of resource servers 105a-105n.  In one embodiment, the indexing service 287 indexes the keywords and synonyms by excluding extraneous words. \nExtraneous words may include common words (e.g., \"a\" and \"the\").  In another embodiment, the indexing service 287 selects a top number of keywords for the index.  The indexing service 287 may select a top number of keywords based on the weights for each\nkeyword.  In one embodiment, the indexing service 287 stores an index of keywords and synonyms in the data storage 268.  For example, the indexing service 287 stores the database index in one or more tables of a database in data storage 268.  In one\nembodiment, the index of keywords includes data describing an association between a keyword and the media.  For example, the index includes records with a keyword and pointer data (e.g., uniform resource locator or document/file identifier) associated\nwith the media.  The pointer data may include data for locating the media on the resource servers 105 (or the database storage 268 if a collaborative computing device 103 is not associated with a resource server 105 that stores media).  In one\nembodiment, a keyword may be associated with a plurality of media.\n In one embodiment, the indexing service 287 creates and maintains a global database index.  The global database index is a master index that is comprised of database indices created separately for the one or more resource servers 105.  The\nindexing service 287 determines one or more types of updates occurring in the one or more resource servers 105, retrieves the database indices of the one or more resource servers and compiles the global index.  The global index holds the database indices\nof the one or more resource servers 105 and consequently pointer references pointing to a location of one or more source materials relating to the database indices.  The global index also holds the keywords for the media associated with the one or more\nresource servers 105 so that the search engine 221 can query the global index for the keywords and receive the pointer references for retrieving the media from the one or more resource servers 105.\n The one or more source materials are associated with the one or more types of updates occurring in the one or more resource servers 105.  The one or more types of updates include storing, deleting or moving at least one of a presentation program\ndocument, a text document, a spreadsheet document, a video recording, an audio recording, an image, etc. For example, a global database index comprises a first database index on a user identifier from a first resource server 105a associated with a\nprojector device, a second database index on a device identifier from a second resource server 105b associated with a video conferencing device, and so on.  In another embodiment, the indexing service 287 creates a global database index that includes\npointer references that point to the database indices in the one or more resource servers 105a-105n.  In one embodiment, the indexing service 287 stores the global database index in the data storage 268.\n The indexing service 287 updates a user profile for an author or presenter based on one or more keywords.  A user profile for the author or presenter may include a list of keywords relating to content in the media associated with the author or\npresenter.  The indexing service 287 updates the user profile by adding the one or more keywords to the list of keywords to the user profile associated with the unique user identifier.  In one embodiment, the user profile for the author includes metadata\ndescribing the media associated with the author or presenter.  The metadata may include the list of keywords, presentation information (e.g., a date, location and device information), media information (e.g., uniform resource locator or document/file\nidentifier of media authored), etc. If the author gives the same presentation information at multiple events, in one embodiment, the presentation information includes multiple dates and locations associated with the presentation.\n The calendar module 286 is code and routines for creating one or more events in a calendar for a user.  In one embodiment, the calendar module 286 is a set of instructions executable by the processor 262 to provide the functionality described\nbelow for creating events in a calendar.  In another embodiment, the calendar module 286 is stored in the memory 264 and is accessible and executable by the processor 262.  In either embodiment, the calendar module 286 is adapted for cooperation and\ncommunication with the processor 262 and the communication unit 266 via signal line 282.\n In one embodiment, the calendar module 286 receives a request from a user via the workflow engine 201 to create one or more events in one or more calendars associated with the user.  The one or more calendars (for e.g., a personal calendar, a\nbill payment calendar, etc.) and the event data (for e.g., a title, a start time, an end time, a start date, an end date, a number of attendees, etc.) associated with the calendars are stored in the data storage 268.  For example, the calendar module 286\nreceives a request from the user to create an event \"Staff Meeting\" in a time slot between \"10 AM\" and \"11 AM\" on a \"Tuesday\".  In one embodiment, the calendar module 286 receives a list of attendees from the user via the workflow engine 201 to be\nassociated with the event.  The user creates a list of attendees by inviting the attendees to the event using a name, a user identifier, an employee identifier, etc. For example, a team leader can invite members in an engineering department to attend the\nevent \"Staff Meeting\".  In one embodiment, the calendar module 286 generates an access control list for the calendar event based on the list of attendees and stores the access control list in the data storage 268.  In another embodiment, the calendar\nmodule 286 sends the access control list to the workflow engine 201 in the enterprise service bus 107.  The workflow engine 201 sends the access control list to the device management engine 203 for determining devices associated with people in the access\ncontrol list.  In one embodiment, the calendar module 286 receives a selection of a location from the user where the event is to take place and transmits the selection of location to the workflow engine 201.  The workflow engine 201 sends the selection\nof location to the device management engine 203 for determining a list of available collaborative computing devices at the selected location.  For example, the user can select a first available conference room \"conference room 301\" on a third floor of a\nbuilding.\n The billing service 267 is code and routines for generating a bill for a user.  In one embodiment, the billing service 267 is a set of instructions executable by the processor 262 to provide the functionality described below for generating and\nstoring billing information.  In another embodiment, the billing service 267 is stored in the memory 264 and is accessible and executable by the processor 262.  In either embodiment, the billing service 267 is adapted for cooperation and communication\nwith the processor 262 and the communication unit 266 via signal line 291.\n The device usage data is stored by the device usage analytics engine 213 in the data storage 268.  The workflow engine 201 retrieves the data associated with device usage from the device usage analytics engine 213 and provides the data to the\nbilling service 267.  In one embodiment, the billing service 267 determines a cost associated with the device usage of a single device.  For example, the billing service 267 determines that the cost of using a Magnetic Resonance Imaging (MRI) device to\nimage the spine of a patient is 2000 dollars.  The billing service 267 then generates a bill based on the cost associated with the device usage and sends the bill to the workflow engine 201 or stores the bill in the data storage 268.  In another\nembodiment, the billing service 267 determines a cost associated with a user device 102 accessing a plurality of collaborative computing devices 103.  For example, the billing service 267 generates a bill for the user device 102 accessing a printer, a\nprojector, conferencing services, etc. for the month of November.\n The search engine 221 is software and routines for identifying media related to a search query of one or more terms.  In one embodiment, the search engine 221 is a set of instructions executable by the processor 262 to provide the functionality\ndescribed below for identifying media related to a search query.  In another embodiment, the search engine 221 is stored in the memory 264 and is accessible and executable by the processor 262.  In either embodiment, the search engine 221 is adapted for\ncooperation and communication with the processor 262 and the communication unit 266 via the signal line 249.\n The search engine 221 receives one or more search queries from the workflow engine 201 for one or more resource servers 105a-105n.  In one embodiment, the search queries originate at the search server 120.  The search engine 221 identifies, from\nthe database indices one or more keywords that match one or more terms in the search queries, determines the one or more resource servers the request is bound for and sends the determination of the one or more resource servers to the workflow engine 201. For example, the search engine 221 accesses a global database index for identifying one or more keywords matching one or more terms in the search queries and determines the one or more resource servers are associated with a projector, an interactive\nwhiteboard, etc. In one embodiment, the search engine 221 identifies keywords by searching for exact matches to a term in the search query.  In another embodiment, the search engine 221 identifies keywords by searching for keywords similar to the term in\nthe search query.  For example, if the requestor provides the term \"open source operating system,\" the search engine 221 may identify \"Linux\" as a keyword from the database index.\n In one embodiment, the search engine 221 receives the search query from the workflow engine 201.  In one embodiment, the search engine 221 identifies one or more types of media associated with the keywords and user profiles for authors or\npresenters of the media.  In another embodiment, the database index includes data describing an association between a keyword and media.  For example, if a search query term, such as, \"Linux\" is received, the search engine 221 identifies a user\nidentifier, a device identifier, date of event, etc. as database indices and using the database indices retrieves from the one or more resource servers 105a-105n one or more of a presentation program document, a text document, a spreadsheet document, a\nvideo file, an audio file, an image, etc. for the search query term, \"Linux\" and sends the retrieved information to the enterprise service bus 107.\n In one embodiment, the search engine 221 identifies a number of times a keyword appears in the retrieved media.  For example, the search engine 221 determines the number of times the keyword appears in an image of one or more slides or on an\ninteractive whiteboard.  In another example, the search engine 221 determines the number of times the author says the keyword in audio (the audio is subjected to speech-to-text translation by the media analysis service 217).  The search engine 221 ranks\nthe media retrieved from the one or more resource servers 105a-105n based on a number of times the keyword appears in the media.\n The server maintenance service 227 is software and routines for managing one or more resource servers.  In one embodiment, the server maintenance service 227 is a set of instructions executable by the processor 262 to provide the functionality\ndescribed below for managing one or more resource servers.  In another embodiment, the server maintenance service 227 is stored in the memory 264 and is accessible and executable by the processor 262.  In either embodiment, the server maintenance service\n227 is adapted for cooperation and communication with the processor 262 and the communication unit 266 via signal line 251.\n In one embodiment, the server maintenance service 227 receives a request from the workflow engine 201 for procuring a server license for one or more resource servers 105a-105n.  For example, the license is an operating system license for an\noperating system installed on the resource server 105.  In another example, the server license is per physical resource server (or per virtual instance of the resource server), per socket (or per CPU) in the resource server 105 and per total number of\ncores in the resource server 105.  In another embodiment, the server maintenance service 227 receives a request for procuring a connection license to allow one or more types of devices to connect to the one or more resource servers 105 and use software\ndistributed by a manufacturer of the one or more types of devices.  The connection license is one from a group of per device licenses that connects to the resource server 105 (for e.g., 45 projector devices connecting to a resource server for a projector\ndevice includes 45 connection licenses), per concurrent connection to the resource server 105 (for e.g., 25 unique sessions open concurrently at any one time to the resource server 105 per 45 projector devices) and per user account (for e.g., using a\nusername and password for a unique connection regardless of type of device used to connect to the resource server 105).\n In one embodiment, the server maintenance service 227 determines compatibility of one or more types of devices with the one or more resource servers 105 by accessing a hardware compatibility list (HCL) associated with the one or more resource\nservers 105.  A hardware compatibility list (HCL) is a database of hardware models and their compatibility, for example, with a certain operating system on a resource server 105 and is stored in the data storage 268.\n In one embodiment, the server maintenance service 227 determines one or more updates to an Application Programming Interface (API) associated with one or more types of collaborative computing devices 103 that are associated with one or more\nresource servers 105.  The one or more updates include software updates and firmware updates.  The Application Programming Interface (API) associated with one or more types of collaborative computing devices 103 is exposed by the enterprise service bus\n107 to the user devices 102a-102n.  The server maintenance service 227, responsive to determining the one or more updates, sends a notification to the enterprise service bus 107 to update its internal engines so that the protocol adaptation engine 205,\nthe message transformation engine 207 and the message enhancement engine 209 are prepared to deal with any updates that occur in the APIs of the collaborative computing devices 103.  This avoids a situation where the user has to manually install updates\non the resource servers 105.\n The notification service 269 is code and routines for generating notifications in the network 104.  In one embodiment, the notification service 269 is a set of instructions executable by the processor 262 to provide the functionality described\nbelow for generating and issuing notifications.  In another embodiment, the notification service 269 is stored in the memory 264 and is accessible and executable by the processor 262.  In either embodiment, the notification service 269 is adapted for\ncooperation and communication with the processor 262 and the communication unit 266 via signal line 292.\n In one embodiment, the notification service 269 receives the bill from the workflow engine 201 or retrieves the bill from data storage 268 and generates a notification indicating that the bill is ready.  The notification is sent to the workflow\nengine 201, which transmits the notification to the user device 102 or an email account of a user using the user device 102.  In another embodiment, the notification service 269 generates a notification indicating when to replace one or more types of\ndevices based on the device usage.  For example, the notification service 269 determines that a projector is nearing its lifetime based on the number of hours the projector has been used and generates a notification to send to the service personnel.\n The notification service 269 receives an access control list and event information associated with a calendar event from the workflow engine 201.  In one embodiment, the notification service 269 generates a notification to query the user that\ncreated the calendar event whether to start the calendar event at start time.  For example, the notification service 269 generates a notification five minutes before the scheduled start time so a projector device is warmed up if the user agrees to start\nthe event.  In one embodiment, the workflow engine 201 receives a request from the calendar module 286 to alert users about the start of the event.  The workflow engine 201 instructs the notification service 269 to generate a notification requesting the\nuser to start the calendar event when all the users in the access control list are present at the calendar event.  In another embodiment, the workflow engine 201 instructs the notification service 269 to generate a notification requesting other\nparticipants to delay the start of the event when at least one user is absent at the venue.  For example, if a user for a staff meeting is held up in traffic five miles from the venue, a notification is generated to delay the staff meeting.\n In one embodiment, the notification service 269 determines the end of the calendar event when the workflow engine 201 receives a request from the calendar module 286 to ask users if they want to end the event.  The workflow engine 201 instructs\nthe notification service 269 to generate a notification asking the list of users participating whether they want to end the event.  Responsive to receiving a confirmation from at least one of the users in the list, the workflow engine 201 transmits the\nconfirmation to power off all the devices at the venue to the device management service 212.  In another embodiment, the notification service 269 receives a list of devices from the workflow engine 201 that are idle for a set amount of time and the\nworkflow engine 201 instructs the notification service 269 to generate a notification indicating the list of devices which are idle and providing an option to power off the devices in the list.  For example, an employer leaving a parking lot after\nattending a group meeting inside a building can be notified by the notification service 269 that a projector, an interactive whiteboard, a web camera, etc. in the conference room are idle.  The user can send a request to power off all the devices\nresponsive to receiving the notification from the notification service 269.  The workflow engine 201 transmits the request from the user to power off to the device management service 212.\n Example User Interfaces\n Turning now to FIG. 3, a graphic representation of an embodiment of a user interface 300 for displaying a webpage providing an overview of services for a user that is generated by the user interface engine 121.  The user interface 300 includes a\nnumber of services, for example, \"My devices\" section 302, \"My calendar\" section 304, \"Federated search\" section 306 and \"Expert search\" section 308.  The \"My devices\" section 302 displays a list 310 of the user's devices, device names (or device\nlocations) and device statuses in a table.  The listed devices include one or more user devices 102 of the user, for example, a tablet PC and one or more collaborative computing devices 103 that were reserved by the user, for example, a projector.  The\n\"My calendar\" section 304 displays a miniature calendar 312 of the user.  The miniature calendar 312 displays an event entry 314 and a thumbnail representation 316 of a collaborative computing device reserved for the event.  The \"Federated search\"\nsection 306 displays a search box 318 for the user to input search terms for media.  The search results display one or more types of media from one or more resource servers 105a-105n or from the database storage 268 (that was stored by the media\nrepository service 215) matching the search terms.  The \"Expert search\" section 308 displays a similar search box for the user to input search terms on a particular subject and find users with varying degrees of expertise on the particular subject.  The\nsearch results display a list of users matching one or more keywords in the search terms and associated media presented by the users including the one or more keywords.\n FIG. 4 is a graphic representation of an embodiment of a user interface 400 for selecting a media type for viewing on a collaborative computing device such as a projector device.  The user interface 400 displays a message 401 in an inbox on a\nsmartphone of a user.  The message 401 includes an attachment 403.  When the user selects the attachment 403, an overlay 407 pop ups proposing a list 405 of options on how to open the attachment 403 including preview, open with NAaaS remote control, open\nin or cancel.  Upon selecting \"Open with NAaaS remote control\" option 409 the user is directed to another user interface for selecting a location where at the attachment 403 should be displayed and is explained in further detail with reference to FIG. 5.\n FIG. 5 is a graphic representation of an embodiment of a user interface 500 for selecting a location to send a media for display.  FIG. 4 could be the next screen that a user sees after selecting the \"Open in .  . . \" option from the list 405 of\noptions in FIG. 4 or it could be related to a different step, for example, a situation where the user wants to see devices associated with a particular location.  The user interface 500 includes a search box 517 for entering a location to search.  The\nuser interface 500 includes a list 515 of locations which the user can select to send the media for display.  Each item in the list 515 includes a name 513 of the location and an address 519 of the location.  Upon selecting the button 511 for \"Conference\nroom 4C\" the user is directed to another user interface that displays a list of devices and is explained in further detail with reference to FIG. 6.\n Turning now to FIG. 6, a graphic representation of an embodiment of a user interface 600 for displaying a list of devices at a location or within a set distance of a user device such as a smartphone.  The user interface 600 displays a list 603\nof devices detected at a location 601 named \"Conf.  room 4C\".  The list 603 of devices includes one or more collaborative computing devices 103.  Each item of the list 603 includes a device name 605, a short description 607 of the device and a thumbnail\nrepresentation 609 of the device.  Upon selecting the button 611 for \"Projector 1\" the user is directed to another user interface for sending a command to the selected device \"Projector 1\" which is explained in further detail with reference to FIG. 7.\n FIG. 7 is a graphic representation of an embodiment of a user interface 700 for displaying a list of commands to send to a selected collaborative computing device (e.g., projector) from a user device 102 such as a smartphone.  FIG. 7 could be\nthe next screen that is displayed after selecting a location from the list 415 in FIG. 5.  FIG. 7 would not, however, be displayed if the user went from FIG. 4 to FIG. 5 because the user would have already selected the source material (attachment 303) to\ndisplay on the collaborative computing device 105 in FIG. 3.  The user interface 700 displays a thumbnail representation 709 of the selected device \"Projector 1\".  In one embodiment, the user interface 700 displays the list 701 of commands that is\nreached responsive to the user clicking button 611 for \"Projector 1\" in FIG. 6.  The user can turn on the device \"Projector 1\" by clicking on the tab 703 for \"Turn On\", turn off the device \"Projector 1\" by clicking on the tab 705 for \"Turn Off\" and\nproject an item on the device \"Projector 1\" by clicking on the tab 707 for \"Project\".  Upon clicking on the tab 707 for \"Project\" the user is directed to another user interface for selecting a source of operation which is explained in further detail with\nreference to FIG. 8.\n FIG. 8 is a graphic representation of an embodiment of a user interface 800 for displaying a list of options to select a source of operation for a selected collaborative computing device (e.g., projector) from a user device 102 such as a\nsmartphone.  The user interface 800 includes an overlay 801 over the display of the user device 102.  The overlay 801 displays a list 803 of source of operations for the selected device \"Projector 1\".  The user can project from the web onto the device\n\"Projector 1\" by clicking on the tab 805 for \"Web\".  The user can project from a memory associated with the user device 102 onto the device \"Projector 1\" by clicking on the tab 807 for \"Camera Roll\".  The user can project from a camera associated with\nthe user device 102 onto the device \"Projector 1\" by clicking on the tab 809 for \"Camera\".\n FIG. 9 is a graphic representation of an embodiment of a user interface 900 for displaying a list of media, such as documents, that match a query.  The user interface 900 includes a search box 902 for a user to input one or more query terms, for\nexample, \"ios\".  The user can select the type of media to retrieve from the one or more resource servers by clicking an adjoining option box 904.  The option box 904 provides a drop down list where the user can select, for example, an option 906 for\n\"document\" and click search.  The user interface 900 displays a table 908 providing a list of documents matching the query term \"ios\".  Each row 910 in the table 908 includes a thumbnail representation of a document 912, for example, a presentation\nprogram document that the user can click to open.  The other information provided in each row of the table 908 include a name of the presenter, a date (for e.g., presentation date), a location (for e.g., presentation location) and a device (for e.g.,\nprojector) associated with the document.\n FIG. 10 is another graphic representation of an embodiment of a user interface 1000 for displaying a list of media, such as documents, that match a query on a user device 102 such as a smartphone.  The user interface 1000 displays a search box\n1002 for a user to input one or more query terms, for example, \"ios\".  The user interface 1000 displays a list 1004 of documents matching the query term \"ios\" in the one or more resource servers.  Each entry in the list 1004 includes a thumbnail\nrepresentation of a document 1006, for example, a presentation program document that the user can select to open.  The other information provided in each entry of the list 1004 include a name of the presenter, a date (for e.g., presentation date), a\nlocation (for e.g., presentation location) and a device (for e.g., projector) associated with the document.\n FIG. 11 is a graphic representation of an embodiment of a user interface 1100 for creating an event in an electronic calendar for a user.  The user interface 1100 displays an interactive calendar 1102 in an hourly view mode.  Other embodiments\nare possible where the interactive calendar 1102 is displayed in other view modes, for example, daily, weekly, agenda, etc. When the user selects a time slot 1104 in the interactive calendar 1102 a window 1106 is displayed for entering event information. For example, the user enters the subject of the event by typing \"Staff Meeting\" into the field 1108 for subject.  The other information entered includes attendees for the event, a location of the event, a start date, an end date, a start time, end time,\netc. The window 1106 also displays a list 1110 of devices available in a location 1114 named \"Conference Room 4C\" chosen by the user for the event \"Staff Meeting\".  The user reserves the device for use in the event \"Staff Meeting\" by checking the radio\nbutton 1112 adjacent to the listed device and clicks the button 1116 indicating \"Next\".  Upon clicking the button 1116 the user is directed to another user interface for selecting media to upload to the reserved devices and create the event which is\nexplained in further detail with reference to FIG. 12.\n FIG. 12 is a graphic representation of an embodiment of a user interface 1200 for selecting media to upload for one or more devices reserved for an event.  The user interface 1200 displays a window 1202 as a pop up for selecting and uploading\nmedia and is displayed responsive to the user clicking the button 1116 in FIG. 11.  In the window 1202 the user can click \"Choose file\" button 1104 to browse a list of documents (from the internet or a user device 102) and then click \"Upload\" button\n1206.  The files that are uploaded to the device are displayed in a list 1208.  The user then clicks \"Create event\" button 1210 to create the event.  Upon clicking the button 1210 the user is directed to another user interface that displays an overview\nof the created event and lets the user makes changes if applicable which is explained in further detail with reference to FIG. 13.\n FIG. 13 is a graphic representation of an embodiment of a user interface 1300 for providing an overview of the created event in the calendar and to make changes if applicable.  The user interface 1300 displays a window 1302 that lists event\ninformation including subject of the event, list of attendees, location of the event, a number of devices reserved for the event and a list of documents uploaded for the devices.  In the window 1302 the user can add more attendees if needed to the event\n\"Staff Meeting\" by clicking on the \"Add more\" button 1304.  The user can change the location of the event \"Staff Meeting\" by clicking on the \"Choose a different location\" drop down menu 1306.  The user can select a device 1308 to control and click \"Turn\nOn Devices\" button 1310.\n FIG. 14 is a graphic representation of an embodiment of a user interface 1400 for specifying configuration settings of a collaborative computing device 105 (name, IP address, etc.) and for turning them on or off.  The user interface 1400\ndisplays a window 1402 as a pop up for specifying instruction for the reserved device and is displayed responsive to the user clicking the button 1310 in FIG. 13.  In the window 1402 the user could chooses to power on the device (and all other devices)\nnow instead of waiting the for the scheduled time by clicking the \"Turn On\" link 404 and then clicking the \"Update Device\" button 1406 for updating the reserved device with this instruction.  In another embodiment, the calendar module 286 determines when\ndevices should be turned on in anticipation of a specified meeting time.\n Methods\n Referring now to FIGS. 15-22, various embodiments of the methods will be described.  FIG. 15 is a flow diagram 1500 of an embodiment of a method for managing communication between one or more types of devices using an enterprise service bus 107\nand NAaaS application services 106.  The enterprise service bus 107 includes a workflow engine 201, a device management engine 203, a protocol adaptation engine 205, a message transformation engine 207 and a message enhancement engine 209.  The NAaaS\napplication services 106 include a user management service 211, a device management service 212, a device inventory service 265, a location service 299, a device usage analytics service 213, a media repository service 215, a media analysis service 217,\nan indexing service 267, a calendar module 286, a billing service 267, a search engine 221, a server maintenance service 227 and a notification service 269.\n The workflow engine 201 instructs the device management engine 203 to identify 1502 one or more types of devices in a network.  The device management engine 203 instructs the device inventory service 265 to register 1504 one or more types of\ndevices for use in the network 404 by assigning an IP address and a name (e.g., a device identifier) to each device.  For example, a user can register personal devices such as a tablet PC, a smartphone, etc. and the device inventory service 265\nassociates the devices with his username or another unique identifier.  In another example, a user can register collaborative computing devices 103 such as a projector, an interactive whiteboard, etc. for use at a particular location or within certain\nlimits of space such as a conference room or a floor of a building.  The device inventory service 265 adds 1506 the IP addresses to a list of IP addresses.  For example, the device inventory service 265 stores a list of approved IP addresses in data\nstorage 268.\n In some embodiments, the workflow engine 201 instructs the user management service 211 to authenticate 1508 each user based on authentication credentials.  For example, the authentication credentials can be the registered user's login\ncredentials.  The protocol adaptation engine 205, the message transformation engine 207 and the message enhancement engine 209 manage 1510 requests between the one or more types of devices in the network.  For example, the message transformation engine\n207 translates a first message in JSON format into a JMS format.  The protocol adaptation engine 205 translates a HTTP protocol from a smartphone into a TCP protocol for a projector.  The message enhancement engine 209 adds information (for e.g.,\nadditional authentication information, etc.) to the message for the target application to receive.  The workflow engine 201 instructs the device usage analytics service 213 to log 1512 device usage entries for the requests occurring between the one or\nmore types of devices in the network 104.\n FIG. 16A is a flow diagram 1600 of an embodiment of a method for controlling interoperation between different types of devices by translating requests.  The enterprise service bus 107 includes a workflow engine 201, a device management engine\n203, a protocol adaptation engine 205, a message transformation engine 207 and a message enhancement engine 209.  The NAaaS application services 106 include a user management service 211, a device inventory service 265, a device management service 212,\nmedia repository service 215, a media analysis service 217, a billing engine 267, a notification service 269, a location service 299 and a device usage analytics service 213.\n The device management engine 203 identifies 1602 a location of a first device in the network and generates 1604 a first list of one or more devices within a set distance of the location of the first device.  For example, the user device 102\ninforms the device management engine 203 of its location and the device management engine 203 interacts with the device inventory service 265 and the location service 299 that are part of the NAaaS application services 106 to generate a list of\ncollaborative computing devices 130 such as a projector, an interactive whiteboard and a web camera that are in a location within a threshold distance of the user device's 102 location.  In another embodiment, the user device 102 specifies conference\nrooms where the user wants to control the devices and the device management engine 203 instructs the device inventory service 265 to generate a list of devices in those conference rooms.  In another embodiment, the set distance is the reach of the\nnetwork 104.  FIG. 6 is an example where the device management engine 203 provides the user with a list of the devices available in conference room 4C 313.  The device management engine 203 provides the list to the user device 102.\n The device management engine 203 receives 1606 a selection from a user associated with the first device of a second device in the first list.  The device management engine 203 generates 1608 a second list that includes functions associated with\nthe second device in the first list.  FIG. 7 is an example where the user selected a projector from the first list and the second list includes the functions associated with the projector.\n The workflow engine 201 receives 1610 a request from the first device to perform one of the functions in the second list.  For example, the list of functions include power on, power off, projection, zoom, enhance, automatic focus, print, two-way\nvideo recording and transmission, two-way audio recording and transmission, language translation, text to speech translation and speech to text translation, etc. The protocol adaptation engine 205 translates 1612 a first communication protocol of the\nrequest from the first device into a second communication protocol used by the second device.  The message transformation engine 207 translates 1614 a first message format into a second message format 514.  The workflow engine 201 transmits 1616 the\ntranslated request to the second device.\n The workflow engine 201 instructs the device usage analytics service 213 to log 1618 the device usage entry associated with the translated request and a transaction performed by the second device.  For example, the device usage analytics service\n213 logs a type of request sent (e.g., project, power on, power off, search, etc.), an IP address for the user device 102 that made the request (e.g., smartphone, laptop, etc.), a user identifier for the user associated with the user device 102, an IP\naddress for the collaborative computing device 103 that receives the request (e.g. projector, laptop, conferencing device, etc.), a type of functionality used (e.g., auto-focus, enhance, imaging, etc.) and a type of media that is being exchanged between\nthe one or more devices (e.g., a presentation program document, a text document, a spreadsheet document, a video recording, an audio recording, an image, etc.).\n FIG. 16B is a flow diagram 1650 of another embodiment of a method for controlling interoperation between different types of devices by translating requests.  The enterprise service bus 107 includes a workflow engine 201 and a device management\nengine 203.  The NAaaS application services 106 include a user management service 211, a device inventory service 265, a location service 299 and device management service 212.\n The workflow engine 201 receives 1652 a selection of media from a first user.  For example, the user selects an email attachment.  The device management engine 203 generates 1654 a first list of functions associated with the media.  For example,\nthe user could project the email.  The workflow engine 201 transmits the first list to the user.  The workflow engine 201 receives 1656 a selected function from the first list.  The device management engine 203 instructs the location service 299 to\ndetermine the user's location.  The device management engine 203 then instructs the location inventory service 299 to generate 1658 a second list that includes locations where the selected function could be performed.  For example, the location service\n299 generates a list of locations within a threshold distance from the user or the user also provides additional input about a location where the user wants to select a device.  The workflow engine 201 transmits the second list to the user.  In some\nembodiments, instead of a list of locations where the selected function can be performed, the second list includes devices within a certain location that can perform the selected function.  For example, the device management engine 203 interacts with the\ndevice inventory service 265 and the location service 299 to generate a list of collaborative computing devices 130 within the location.\n The workflow engine 201 receives 1660 a selected location from the second list.  The device management engine 203 instructs the device inventory service 265 and device management service 212 to generate 1662 a third list of devices at the\nselected location that can perform the selected function.  For example, the third list includes devices that are available.  The workflow engine 201 transmits the third list to the user.  The workflow engine 201 receives 1664 a selected device from the\nthird list.  The workflow engine 201 transmits 1666 a request to perform the function to the selected device.  For example, the workflow engine 201 transmits a request to project the email attachment to a projector in a conference room.\n FIG. 17 is a flow diagram 1700 of an embodiment of a method for generating a bill based on a cost determined for each transaction performed by a device.  The workflow engine 201 instructs the device usage analytics service 213 to retrieve 1702\ndevice usage entries associated with transactions performed by each device in a network 104 from data storage 268.  For example, the device usage analytics service 213 retrieves device usage entries for all transactions requested by a user device 102 in\nthe past month.  The workflow engine 201 sends the retrieved data to the billing service 267.  The billing service 267 calculates 1704 a cost associated with each transaction and generates 1706 a bill based on the cost associated with each transaction. \nFor example, the billing service 267 determines the cost of using Magnetic Resonance Imaging (MRI) device to image the spine of a patient is 2000 dollars.  The notification service 269 sends 1708 a notification indicating the bill is ready.  The workflow\nengine 201 receives the notification and sends the notification to the device or an email account of a user associated with the device.\n FIG. 18 is a flow diagram 1800 of an embodiment of a method for compiling a global database index for one or more resource servers 105a-105n using an enterprise service bus 107 and NAaaS application services 106.  The enterprise service bus 107\ncomprises a workflow engine 201.  The NAaaS application services 106 include a media analysis service 217 and an indexing service 287.  The workflow engine 201 receives 1802 requests that include media.  For example, a user device 102 sends a request to\nthe workflow engine 201 to conduct a video conference with people using a video conferencing device.  The workflow engine 201 transmits 1804 the media to collaborative computing devices 103, the collaborative computing devices storing the media on\nresource servers 105.  For example, the workflow engine 201 transmits the video conference data to the video conferencing device and transmits the video conference data to the media analysis service 217 for analysis.\n The media analysis service 217 identifies 1806 text in the media, for example, by performing speech-to-text translation.  The workflow engine 201 instructs the indexing service 287 to generate 1808 an individual database index for each resource\nserver 105 that associates keywords with the media based on the identified text in the media.  The workflow engine 201 instructs the indexing service 287 to compile 1810 a global database index from the individual database indexes, the global database\nindex including the keywords and pointer references pointing to locations where the media are stored in the resource servers 105.  For example, the video conference is stored on a resource server associated with the video conferencing device.\n FIG. 19 is a flow diagram 1900 of an embodiment of a method for translating a request for one or more resource servers using an enterprise service bus 107 and NAaaS application services 106.  The enterprise service bus 107 comprises a workflow\nengine 201 and a protocol adaption engine 205.  The NAaaS services 106 include a search engine 221.\n The workflow engine 201 receives 1902 a query from a first device for one or more types of media related to the query.  The workflow engine 201 transmits the request to the search engine 221, which identifies 1904 keywords in a global database\nindex that match the query.  The workflow engine 201 receives the keywords from the search engine 221 and determines 1906 one or more resource servers 105 that store media associated with the keywords.  The workflow engine 201 generates 1908 a request\nfor media for the one or more resource servers 105.  If there is a difference between the communication protocol of the request and the communication protocol used by the resource server 105, the workflow engine 201 transmits the request to the protocol\nadaptation engine 205, which determines 1910 a first communication protocol of the request and one or more types of second communication protocols of the one or more resource servers.  The protocol adaptation engine 205 translates 1912 the request from\nthe first communication protocol into each of the one or more types of second communication protocols.  If there is a difference between the message format of the request and the message format of the resource server, the message transformation engine\n207 determines 1914 a first message format of the request and one or more types of second message formats of the one or more resource servers.  The message transformation engine 207 transforms 1916 the request from the first message format into each of\nthe one or more types of second message formats.  The workflow engine 201 retrieves 1918 one or more results matching the translated request from the one or more resource servers 105 and sends 1920 the one or more results to the first device.\n FIG. 20 is a flow diagram 2000 of an embodiment of a method for controlling one or more devices reserved for a calendar event using an enterprise service bus 107 and NAaaS application services 106.  The enterprise service bus 107 comprises a\nworkflow engine 201 and a device management engine 203.  The NAaaS application services 106 comprise a calendar module 286, a device inventory service 265, a device management service 212 and a notification service 269.\n The calendar module 286 receives 2002, via the workflow engine 201, a request from an organizer to create an event for a list of attendees and a selection of a location for the event in a calendaring application.  The workflow engine 201\ntransmits the selected location to the device management engine 203 in the enterprise service bus 107.  The device management engine 203 instructs the device inventory service 265 to determine 2004 a set of devices provided for use in the location.  For\nexample, the user specifies that the event takes place in a conference room in Menlo Park, Calif., but users will also join from a conference room in Tokyo, Japan.  Further details for selecting the location can be found above with reference to FIGS. 16A\nand 16B.  The workflow engine 201 receives a selection of at least some of the devices from the set of devices, a starting time and end time for the event and a name for the event.  The device management engine 203 determines 2008 a location of the\norganizer's device within a threshold time of a start of the event.  The workflow engine 201 transmits 2010 a query asking the organizer if the event should start at a start time.  For example, the workflow engine 201 transmits the query five minutes\nbefore the event is scheduled to ask the organizer if the event should start at the scheduled time.  The query is generated by the notification service 269 and received by the workflow engine 201.  The workflow engine 201 instructs the calendar module\n286 to receive 2012 a confirmation from the organizer that the event should start at the same time.  The device management engine 203 instructs the set of devices to power on for use at the start time.  For example, the device management engine 203\ninstructs the set of devices five minutes before the start time of the event.\n FIG. 21 is a flow diagram of another embodiment of a method for controlling one or more devices reserved for a calendar event using an enterprise service bus 107 and NAaaS application services 106.  The enterprise service bus 107 comprises a\nworkflow engine 201.  The NAaaS application services 106 comprise a calendar module 286, a device inventory service 265, a device management service 212 and a notification service 269.\n The workflow engine 201 instructs the calendar module 286 to receive 2102 a request to create an event for a list of attendees and a selection of a location for the event in a calendaring application.  For example, the location is Conference\nRoom 201B in Building A. The organizer also sets the start time and end time for the event.  The workflow engine 201 instructs the device inventory service 265 to determine 2104 a set of devices provided for use in the location.  The workflow engine 201\nreceives 2106 a request to reserve one or more devices in the set of devices provided for use in the event.  The workflow engine 201 receives 2108 an indication of whether users associated with the device are required to attend or optionally attending. \nThe workflow engine 201 instructs the media repository service 215 to receive 2110 one or more types of media to associate with each of the one or more devices for display.  The workflow engine 201 instructs 2112 the one or more devices to power on at a\nstart of the event.  Around the end of the event, for example, five minutes before the event is scheduled to end, the workflow engine 201 instructs the notification service 269 to generate a notification asking if the event should end.  Optionally, the\nworkflow engine 201 receives the notification and queries 2114 at least one of the list of attendees if the event should end and responsive to receiving a confirmation from one of the attendees the workflow engine 201 instructs 2116 the one or more\ndevices that were turned on to power off.\n FIG. 22 a flow diagram of an embodiment of a method for generating one or more notifications for a calendar event using an enterprise service bus 107 and NAaaS application services 106.  The enterprise service bus 107 comprises a workflow engine\n201 and a device management engine 203.  The NAaaS application services 106 comprise a notification service 269.  The device management engine 203 determines 2202 a location of one or more devices associated with a list of attendees prior to a start of\nan event at a venue.  The device management engine 203 checks 2204 whether at least one or more devices absent from the venue.  If at least one of the one or more devices is absent from the venue, the device management engine 203 checks 2208 whether at\nleast one of the one or more devices within a threshold distance of the venue.  The threshold can be a preset default, defined by an organizer of the event, etc.\n If at least one of the one or more devices is not absent from the venue, the device management engine 203 instructs the notification service 269 to suggest 2206 to start the event.  In some embodiments, the device management engine 203 checks\nthe identity of the user associated with the device that is absent.  If the presence of the user is not important to the event, the device management engine 203 instructs the notification service 269 to suggest 2206 to start the event.  For example, as\nreferenced in FIG. 21, the user that created the event specifies mandatory attendance for some of the attendees and optional attendance for other attendees.  If the user is one of the optional attendees, the event can start.\n If at least one of the one or more devices within a threshold distance of the venue, the device management engine 203 instructs the notification service 269 to suggest 2206 to start the event.  If at least one of the one or more devices is not\nwithin a threshold distance of the venue, the device management engine 203 instructs the notification service 269 to generate 2210 a notification to delay the start of the event.\n The foregoing description of the embodiments has been presented for the purposes of illustration and description.  It is not intended to be exhaustive or to limit the specification to the precise form disclosed.  Many modifications and\nvariations are possible in light of the above teaching.  It is intended that the scope of the embodiments be limited not by this detailed description, but rather by the claims of this application.  As will be understood by those familiar with the art,\nthe examples may be embodied in other specific forms without departing from the spirit or essential characteristics thereof.  Likewise, the particular naming and division of the modules, routines, features, attributes, methodologies and other aspects are\nnot mandatory or significant, and the mechanisms that implement the description or its features may have different names, divisions and/or formats.  Furthermore, as will be apparent to one of ordinary skill in the relevant art, the modules, routines,\nfeatures, attributes, methodologies and other aspects of the specification can be implemented as software, hardware, firmware or any combination of the three.  Also, wherever a component, an example of which is a module, of the specification is\nimplemented as software, the component can be implemented as a standalone program, as part of a larger program, as a plurality of separate programs, as a statically or dynamically linked library, as a kernel loadable module, as a device driver, and/or in\nevery and any other way known now or in the future to those of ordinary skill in the art of computer programming.  Additionally, the specification is in no way limited to implementation in any specific programming language, or for any specific operating\nsystem or environment.  Accordingly, the disclosure is intended to be illustrative, but not limiting, of the scope of the specification, which is set forth in the following claims.", "application_number": "14614191", "abstract": " A system and method for integrating one or more resource servers for one\n     or more types of devices are described using an enterprise service bus\n     and network appliance as a service (NAaaS) application services. The\n     enterprise service bus receives a request for one or more types of media,\n     determines a first communication protocol of the request and one or more\n     types of second communication protocols of the one or more resource\n     servers, translates the request from a first communication protocol into\n     one or more types of second communication protocols for one or more\n     resource servers and retrieves one or more results matching the\n     translated request from the one or more resource servers. A NAaaS\n     application services determine one or more resource servers that the\n     request is for based on the request matching a global index.\n", "citations": ["5550966", "6363434", "6480901", "6636238", "7516415", "7984099", "8135342", "8856355", "9294723", "20020032677", "20020054345", "20030103075", "20030217186", "20050071519", "20050240417", "20060045029", "20070268121", "20080195312", "20080303748", "20100121666", "20100169951", "20110040828", "20110112832", "20110196972", "20110238495", "20110282968", "20110298596", "20120079080", "20120270567", "20120293605", "20130117460", "20130332979", "20150193433"], "related": ["13689752"]}, {"id": "20160133162", "patent_code": "10347151", "patent_name": "Student specific learning graph", "year": "2019", "inventor_and_country_data": " Inventors: \nContractor; Danish (Gurgaon, IN), Gopinath; Ramesh Ambat (Millwood, NY), Mohania; Mukesh Kumar (New Delhi, IN), Negi; Sumit (Ghaziabad, IN), Rajput; Nitendra (Gurgaon, IN)  ", "description": "BACKGROUND\n People continually try to learn new skills.  Some people like to learn new skills for themselves, while others need or desire to learn new skills to advance their careers.  Whatever the motivation in the desire to learn new skills, people may\nhave trouble determining what is necessary to acquire the skill(s) they desire.  For example, a person may not understand which courses act as prerequisites for the desired skill, at what proficiency a person should be at before attempting to learn the\ndesired skill, what gaps a person should address to achieve the skill, what learning materials are associated with attaining the desired skill, and the like.  As another example, a person may know the skill they desire to learn, but may not know where to\nstart and what learning path to following, in learning that new skill.\nBRIEF SUMMARY\n In summary, one aspect of the invention provides a method of generating a learning graph, said method comprising: utilizing at least one processor to execute instructions to perform the steps of: receiving a proficiency input relating to a\nstudent; receiving a target knowledge node, wherein the target knowledge node represents at least one skill the student does not currently possess; determining at least one skill requirement of the at least one skill; identifying at least one path\nbetween the proficiency input and the target knowledge node based upon the at least one determined skill requirement; calculating a gap between the proficiency input and the target knowledge node at the at least one identified path; and recommending at\nleast one learning content module based upon the calculated gap.\n Another aspect of the invention provides an apparatus for generating a learning graph, said apparatus comprising: at least one processor; and a computer readable storage medium having computer readable program code embodied therewith and\nexecutable by the at least one processor, the computer readable program code comprising: computer readable program code configured to receive a proficiency input relating to a student; computer readable program code configured to receive a target\nknowledge node, wherein the target knowledge node represents at least one skill the student does not currently possess; computer readable program code configured to determine at least one skill requirement of the at least one skill; computer readable\nprogram code configured to identify at least one path between the proficiency input and the target knowledge node based upon the at least one determined skill requirement; computer readable program code configured to calculate a gap between the\nproficiency input and the target knowledge node at the at least one identified path; and computer readable program code configured to recommend at least one learning content module based upon the calculated gap.\n An additional aspect of the invention provides a computer program product for generating a learning graph comprising: a computer readable storage medium having computer readable program code embodied therewith, the computer readable program code\ncomprising: computer readable program code configured to receive a proficiency input relating to a student; computer readable program code configured to receive a target knowledge node, wherein the target knowledge node represents at least one skill the\nstudent does not currently possess; computer readable program code configured to determine at least one skill requirement of the at least one skill; computer readable program code configured to identify at least one path between the proficiency input and\nthe target knowledge node based upon the at least one determined skill requirement; computer readable program code configured to calculate a gap between the proficiency input and the target knowledge node at the at least one identified path; and computer\nreadable program code configured to recommend at least one learning content module based upon the calculated gap.\n A further additional aspect of the invention provides a method of generating a learning graph, said method comprising: utilizing at least one processor to execute instructions to perform the steps of receiving a proficiency input relating to a\nstudent; receiving a target knowledge node, wherein the target knowledge node represents at least one skill the student does not currently possess; determining at least one skill requirement of the at least one skill; identifying at least one path\nbetween the proficiency input and the target knowledge node based upon the at least one determined skill requirement; calculating a gap between the proficiency input and the target knowledge node at the at least one identified path; and recommending at\nleast one learning content module based upon the calculated gap via: computing a total effort required for attaining skills the student does not currently possess associated with the target knowledge node; determining a path, comprising the skills the\nstudent does not currently possess, requiring a total effort determined to be the least; and identifying at least one learning content module, within the determined path, the student does not currently possess.\n For a better understanding of exemplary embodiments of the invention, together with other and further features and advantages thereof, reference is made to the following description, taken in conjunction with the accompanying drawings, and the\nscope of the claimed embodiments of the invention will be pointed out in the appended claims. BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWINGS\n FIG. 1 depicts an example knowledge graph.\n FIG. 2 depicts an example knowledge graph with a student proficiency input.\n FIG. 3 schematically illustrates a system architecture for a student specific knowledge graph.\n FIG. 4 illustrates a computer system.\nDETAILED DESCRIPTION\n It will be readily understood that the components of the embodiments of the invention, as generally described and illustrated in the figures herein, may be arranged and designed in a wide variety of different configurations in addition to the\ndescribed exemplary embodiments.  Thus, the following more detailed description of the embodiments of the invention, as represented in the figures, is not intended to limit the scope of the embodiments of the invention, as claimed, but is merely\nrepresentative of exemplary embodiments of the invention.\n Reference throughout this specification to \"one embodiment\" or \"an embodiment\" (or the like) means that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment of the\ninvention.  Thus, appearances of the phrases \"in one embodiment\" or \"in an embodiment\" or the like in various places throughout this specification are not necessarily all referring to the same embodiment.\n Furthermore, the described features, structures, or characteristics may be combined in any suitable manner in at least one embodiment.  In the following description, numerous specific details are provided to give a thorough understanding of\nembodiments of the invention.  One skilled in the relevant art may well recognize, however, that embodiments of the invention can be practiced without at least one of the specific details thereof, or can be practiced with other methods, components,\nmaterials, et cetera in other instances, well-known structures, materials, or operations are not shown or described in detail to avoid obscuring aspects of the invention.\n The description now turns to the figures.  The illustrated embodiments of the invention will be best understood by reference to the figures.  The following description is intended only by way of example and simply illustrates certain selected\nexemplary embodiments of the invention as claimed herein.\n It should be noted that the flowchart and block diagrams in the figures illustrate the architecture, functionality, and operation of possible implementations of systems, apparatuses, methods and computer program products according to various\nembodiments of the invention.  In this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of code, which comprises at least one executable instruction for implementing the specified logical function(s).  It\nshould also be noted that, in some alternative implementations, the functions noted in the block may occur out of the order noted in the figures.  For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the\nblocks may sometimes be executed in the reverse order, depending upon the functionality involved.  It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of Hocks in the block diagrams and/or flowchart\nillustration, can be implemented by special purpose hardware-based systems that perform the specified functions or acts, or combinations of special purpose hardware and computer instructions.\n Specific reference will now be made here below to FIGS. 1-3.  It should be appreciated that the processes, arrangements and products broadly illustrated therein can be carried out on, or in accordance with, essentially any suitable computer\nsystem or set of computer systems, which may, by way of an illustrative and non-restrictive example, include a system or server such as that indicated at 12' in FIG. 4.  In accordance with an example embodiment, most if not all of the process steps,\ncomponents and outputs discussed with respect in FIGS. 1-3 can be performed or utilized by way of a processing unit or units and system memory such as those indicated, respectively, at 16' and 28' in FIG. 4, whether on a server computer, a client\ncomputer, a node computer in a distributed network, or any combination thereof.\n Broadly contemplated herein, in accordance with at least one embodiment of the invention, are systems and arrangements that provide a learning graph to enable a student to determine what additional skills, if any, are necessary to learning a new\ndesired skill.  In one embodiment, the system may map the current proficiencies of a user on a skill dependency graph (also known as a knowledge graph) and calculate a gap between the student's current proficiencies and the proficiencies required to be\nproficient in the desired skill.  Thus, an embodiment may use the knowledge relationship to determine the skills that an individual needs to acquire to learn a desired skill.  One embodiment may identify skills that a student needs to increase before\nlearning the desired skill.  In accordance with one embodiment, a learning graph including knowledge paths may be displayed.\n FIG. 1 illustrates an example knowledge graph.  In one embodiment, a knowledge graph 100, such as the one shown in FIG. 1, may be generated.  A target knowledge node 101, may include a skill that a student wants to learn.  Nodes 105 may\nrepresent skills that must be learned (i.e., prerequisites) before the skill represented by the target knowledge node 101 may be acquired.  Each node 105 may have a data structure that may contain a variety of information including, for example,\ncomprehension quantification score, difficulty level quantification score, content, optional prerequisites, mandatory prerequisites, minimum scoring criteria of prerequisites, and the like.  Prerequisites may be indicated by neighboring nodes 105, for\nexample Node B is a prerequisite of Node A. Prerequisites may also be considered any node 105 that is required within the knowledge path (\"path\"), for example, Node G is a prerequisite of Node A. Table 1 below represents an example of how a node may\ncontain information for course material relevant for that node.\n TABLE-US-00001 TABLE 1 Content Link Comprehension Burden Reviews Format &lt;link1&gt; 24 6.5 PPT &lt;link2&gt; 34 5.6 Text\n A knowledge path may be considered the whole of the nodes 105 and edges 103 required to get from a skill to the target knowledge node 101.  The edges may represent a prerequisite.  For example, the edge between B and A identifies B as a\nprerequisite to A. The weights 102 of the edges 103 may represent proficiency required in the child node (i.e., the node required before a present node) to start the learning of the parent node (i.e., the present node).  For example, Node H is the child\nnode of Node C and Node C is the parent node of Node H. The arcs 104 may represent optional or alternative learning paths.  For example, to start learning the skill associated with its ode B, a student can either have a proficiency of 35 in the skill\nassociated with Node E or a proficiency of 50 in the skill associated with Node F.\n FIG. 3 schematically illustrates a system architecture for a student specific knowledge graph, in accordance with at least one embodiment of the invention.  At 301, an embodiment may receive a proficiency input relating to a student (\"student\nproficiency input\").  The input may be received in a variety of ways.  For example, the input may be entered manually into a computer, for example, through a keyboard input by a student, or the data may be obtained.  For example, the data may be obtained\nthrough a network connection from a transcript of a student.  Other methods of receiving the input are contemplated.  In one embodiment, this student proficiency input may contain data including, for example, a skill a student currently possesses,\npreviously completed course, the score associated with the previously completed course, retention ability score, learning style, comprehension ability, and other such information.\n Additionally, in accordance with one embodiment, the student proficiency input may contain a previously completed course and that previously completed course may include a decay value.  This decay value may reduce the proficiency score\nassociated with the previously completed course.  The decay value may allow a computation of a decrease in proficiency due to a passage of time.  For example, a student may have preformed well in algebra.  However, the student has not used algebra for a\nfew years and their proficiency has been reduced.  The decay value would account for this loss of proficiency.  Within one embodiment, the student may be allowed to test or quiz the skill in order to prove their proficiency and reduce the decay value.\n In accordance with one embodiment, the student proficiency input represents the concepts/skills the student has already acquired.  FIG. 2 shows an example knowledge graph, as shown in FIG. 1, including a student proficiency input 201.  The\nstudent may have an amount of proficiency 201 in skills included within a knowledge path.  Per FIG. 2, the student may be have a proficiency of 50 in the skill associated with Node K, a proficiency of 10 in the skill associated with Node L, a proficiency\nof 20 in the skill associated with Node J, a proficiency of 50 in the skill associated with Node H, and a proficiency of 30 in the skill associated with Node I.\n Referring back to FIG. 3, one embodiment may at 302 receive a target knowledge node 101, which may represent a skill a student does not currently possess.  The skills that the student does not currently possess may be skills the student has not\nlearned yet or may include skills that the student is not completely proficient in. In other words, the skills the student does not currently possess may include skills the student has some (proficiency in. Like the student proficiency input, these data\nmay be received in a variety of ways.  For example, it may be received manually, for example a student selecting a desired skill, or it may be received from a system.\n At 303, an embodiment may determine at least one skill requirement of the target knowledge node.  These skill requirements may be represented by the nodes 105, edges 103, and weights 102.  In one embodiment, the skill requirement may comprise\ninformation such as comprehension difficulty score, proficiency requirement (represented by the edges 103 and weights 102) for the skill (represented by the nodes 105), prerequisite skill, proficiency requirement for the prerequisite skill, and other\nsuch information.  In one embodiment, determining the skill requirement may include identifying at least one concept or skill as a prerequisite for the target knowledge node 101.  For example, Node J is a prerequisite to Node D. Additionally, in\naccordance with one embodiment, determining the skill requirement may include identifying a necessary proficiency for the concept or skill identified as a prerequisite.\n At 304, an embodiment may identify a knowledge path between the student proficiency input and the target knowledge node based upon the skill requirements.  In other words, an embodiment may determine what skills the student needs to learn in\norder to learn the skill associated with the target knowledge node.  The knowledge path identifies the skills and proficiencies associated with a target knowledge node 101.\n At 305, an embodiment may calculate a gap between the student proficiency input and the target knowledge node based upon the identified knowledge path.  The calculating of this gap, in one embodiment, may be accomplished by associating a known\nvalue representing the student proficiency with a node representing a skill.  In addition, a required value may be associated with the node which represents a necessary proficiency needed to learn the target skill.  The two values may then be compared to\ndetermine the deficiency of the student.  Based upon this calculation, one embodiment may identify the requirements that a student must fulfill in order to reach the target skill.  These requirements may include the skills that a student needs to learn\nto complete a knowledge path.\n At 306, an embodiment may recommend a learning content module based upon the calculated gap.  For example, if after calculating the gap at 305 an embodiment has identified requirements that the student is not proficient in yet, an embodiment may\nrecommend learning content based upon identified requirements.  This recommendation may be stored in memory for later access.  Additionally or alternatively, this recommendation may be displayed on a display device (e.g., touch screen, monitor, display\nintegrated in an information handling device, etc.).\n In one embodiment, this recommendation may include recommending a particular class.  Alternatively or additionally, this recommendation may include recommending a particular skill in which the student needs to increase proficiency before\nlearning the desired skill.  The recommendation may include more than one skill.  Therefore, the learning content module may include a single skill and/or proficiency, or it may include a plurality of skills and/or proficiencies.  Each node may contain\ninformation regarding what learning content needs to be learned in order to attain proficiency.  For example, the edges 103 and weights 102 include a proficiency requirement.  Additionally or alternatively, each node may contain other information\nregarding requirements for a particular node.  For example, each node may have associated information, such as shown in Table 1 above, which may include information such as the comprehension burden for a particular node.  In accordance with one\nembodiment, the recommendation may include a graphical representation.  Additionally or alternatively, the recommendation may include text describing the skills a student needs to learn.\n For example, FIG. 2 shows the knowledge path between the student proficiency input and the target knowledge, Node A. Each node 105 and 101 in the knowledge path may have a potential associated with it.  This potential may be a function of the\nscore required in any prerequisites and the minimum number of prerequisites and alternatives.  An example calculation may be: Potential of a node=.SIGMA.(w_i))+.SIGMA.(.beta._j*w_j)/|M|+.SIGMA.(.beta._j*|O|) w_i=Minimum Score in the ith mandatory\npre-requisite w_j=Minimum Score in the jth non-mandatory pre-requisite .beta._j=Importance factor in non-mandatory pre-requisite M=Set of mandatory pre-requisites O=Set of non-mandatory pre-requisites |X|=Sum of Maximum Score possible in pre-requisites\nconsidered For a specific student, the calculation may be slightly modified, for example, like the following: Potential of a node=.PI.1{w'_i&gt;w_i}[.SIGMA.(w'_i))+.SIGMA.(.beta._j*w'_j)/|M|+.SIGMA.- (.beta._j*|O|)] 1{f(x)} indicator function that\nreturns 1 if f(x) is true.  w'_i=Score of the student in the ith mandatory pre-requisite of the new skill/concept w'_j=Score of the student in the jth non-mandatory pre-requisite of the new skill/concept.  .beta._j=Importance factor in non-mandatory\npre-requisite M=Set of mandatory pre-requisites O=Set of non-mandatory pre-requisites |X|=Sum of Maximum Score possible in pre-requisites considered\n Referring back to the example in FIG. 2, the student has been determined to be proficient in the skills associated with Nodes K, L, and H. In other words, the student's potential difference in these skills is greater than or equal to 0.  The\nstudent has some proficiency in the skills associated with Nodes I and J, but not the required proficiency amount.  In other words, the student's potential difference is less than 0.  Therefore, the student's knowledge path requires the student to become\nproficient (meeting the required proficiency in a skill) in the skills associated with E or F (F and F are alternates of each other so the student could become proficient in one or the other), G, B, I, J, and C or D. In order for the student to learn\ntarget skill A, the student would need to fulfill one of the knowledge paths.  The different knowledge paths are identified by the alternatives.  For the following abbreviated example, assume the skill associated with Node A only requires the\nprerequisites included the left side of FIG. 2 (i.e., Nodes A, B, E, F, and G).  One knowledge path would include a student learning skills E, G, and B. The alternate knowledge path would include a student learning skills F, G, and B.\n As an example, consider the following:\n Factorization requires the following pre-requisites:\n Multiplication (Score=80%) (Mandatory)\n Division (Score=80%) (Mandatory)\n Arithmetic properties (Score=80%) (Not Mandatory)\n The potential for this node \"Factorization\" is 0.8.\n Consider two students.  The first having the following\n Multiplication (Has score=80%)\n Division (Has scored=50%)\n Arithmetic properties (Has scored=80%)\n The potential for this student is 0.68.\n The second student having the following skills:\n Multiplication (Score=80%)\n Division (Score=80%)\n The potential for this student is 0.8.\n The first student would not be able to take Factorization and would need to gain proficiency in Division.  The second student, however, would be able to take Factorization.\n In one embodiment, the recommending at 306, may be accomplished by computing a total effort required for attaining skills the student does not currently possess.  An embodiment may then determine a path comprising these skills that the student\ndoes not currently possess requiring the least amount of total effort.  The skills that the student does not currently possess may be skills the student has not learned yet or may include skills that the student is not completely proficient in. In other\nwords, the skills the student does not currently possess may include skills the student has some proficiency in. An embodiment may then recommend the learning content module based upon the path including the skills the student does not currently possess\nthat require the least amount of effort by the student to become proficient in.\n In one embodiment, a preferred knowledge path may be identified.  The preferred knowledge path may represent the knowledge path in which the calculated gap is determined to be the least.  In other words, this preferred path may represent the\nknowledge path that a student may take in order to expend the least amount of effort.  For example, referring to FIG. 2, the preferred knowledge path may include the student acquiring a proficiency in the skills associated with Nodes F, G, B, I (up to\n35%), and C.\n One example of the way this preferred knowledge path may be identified is represented in the code that follows.  The code includes information regarding target node t and current student skill profile S, which consists of nodes in the knowledge\ngraph along with information, for example, about student aptitude, performance in pre-requisites of each skill node, decay of each pre-requisite, and the like.  The code then overlays the student profile and the target node t on the knowledge graph and\nexecutes a breadth-first search (BFS) traversal from the target node to the nodes in the student profile.  Let the first level of nodes reached during the traversal be M.\n TABLE-US-00002 For each m in M { { For each path p between m and T { p.path_effort=0; For each node n on path p { profScore=GetStudentProficiencyScore(n,S); If(profScore &lt; MinRequiredProficiencyScore(n) { p.path_effort=p.path_effort+\ngetGapPotential(n,S); }} }} recommendPath = path with minimum path effort score.\n In accordance with one embodiment, a knowledge graph may be displayed.  This knowledge graph 100 in FIG. 1, may include at least one knowledge path.  The knowledge path may contain the target knowledge node 101, any prerequisite nodes 105, and a\nproficiency 103 and 102 associated with the skill represented by the nodes 105 and 101.\n Referring now to FIG. 4, a schematic of an example of a cloud computing node is shown.  Cloud computing node 10' is only one example of a suitable cloud computing node and is not intended to suggest any limitation as to the scope of use or\nfunctionality of embodiments of the invention described herein.  Regardless, cloud computing node 10' is capable of being implemented and/or performing any of the functionality set forth hereinabove.  In accordance with embodiments of the invention,\ncomputing node 10' may not necessarily even be part of a cloud network but instead could be part of another type of distributed or other network, or could represent a stand-alone node.  For the purposes of discussion and illustration, however, node 10'\nis variously referred to herein as a \"cloud computing node\".\n In cloud computing node 10' there is a computer system/server 12', which is operational with numerous other general purpose or special purpose computing system environments or configurations.  Examples of well-known computing systems,\nenvironments, and/or configurations that may be suitable for use with computer system/server 12' include, but are not limited to, personal computer systems, server computer systems, thin clients, thick clients, hand-held or laptop devices, multiprocessor\nsystems, microprocessor-based systems, set top boxes, programmable consumer electronics, network PCs, minicomputer systems, mainframe computer systems, and distributed cloud computing environments that include any of the above systems or devices, and the\nlike.\n Computer system/server 12' may be described in the general context of computer system-executable instructions, such as program modules, being executed by a computer system.  Generally, program modules may include routines, programs, objects,\ncomponents, logic, data structures, and so on that perform particular tasks or implement particular abstract data types.  Computer system/server 12' may be practiced in distributed cloud computing environments where tasks are performed by remote\nprocessing devices that are linked through a communications network.  In a distributed cloud computing environment, program modules may be located in both local and remote computer system storage media including memory storage devices.\n As shown in FIG. 4, computer system/server 12' in cloud computing node 10 is shown in the form of a general-purpose computing device.  The components of computer system/server 12' may include, but are not limited to, at least one processor or\nprocessing unit 16', a system memory 28', and a bus 18' that couples various system components including system memory 28' to processor 16'.\n Bus 18' represents at least one of any of several types of bus structures, including a memory bus or memory controller, a peripheral bus, an accelerated graphics port, and a processor or local bus using any of a variety of bus architectures.  By\nway of example, and not limitation, such architectures include Industry Standard Architecture (ISA) bus, Micro Channel Architecture (MCA) bus, Enhanced ISA (EISA) bus, Video Electronics Standards Association (VESA) local bus, and Peripheral Component\nInterconnects (PCI) bus.\n Computer system/server 12' typically includes a variety of computer system readable media.  Such media may be any available media that are accessible by computer system/server 12', and include both volatile and non-volatile media, removable and\nnon-removable media.\n System memory 28' can include computer system readable media in the form of volatile memory, such as random access memory (RAM) 30' and/or cache memory 32'.  Computer system/server 12' may further include other removable/non-removable,\nvolatile/non-volatile computer system storage media.  By way of example only, storage system 34' can be provided for reading from and writing to a non-removable, non-volatile magnetic media (not shown and typically called a \"hard drive\").  Although not\nshown, a magnetic disk drive for reading from and writing to a removable, non-volatile magnetic disk (e.g., a \"floppy disk\"), and an optical disk drive for reading from or writing to a removable, non-volatile optical disk such as a CD-ROM, DVD-ROM or\nother optical media can be provided.  In such instances, each can be connected to bus 18' by at least one data media interface.  As will be further depicted and described below, memory 28' may include at least one program product having a set (e.g., at\nleast one) of program modules that are configured to carry out the functions of embodiments of the invention.\n Program/utility 40', having a set (at least one) of program modules 42', may be stored in memory 28' (by way of example, and not limitation), as well as an operating system, at least one application program, other program modules, and program\ndata.  Each of the operating systems, at least one application program, other program modules, and program data or some combination thereof, may include an implementation of a networking environment.  Program modules 42' generally carry out the functions\nand/or methodologies of embodiments of the invention as described herein.\n Computer system/server 12' may also communicate with at least one external device 14' such as a keyboard, a pointing device, a display 24', etc.; at least one device that enables a user to interact with computer system/server 12'; and/or any\ndevices (e.g., network card, modem, etc.) that enable computer system/server 12' to communicate with at least one other computing device.  Such communication can occur via I/O interfaces 22'.  Still yet, computer system/server 12' can communicate with at\nleast one network such as a local area network (LAN), a general wide area network (WAN), and/or a public network (e.g., the Internet) via network adapter 20'.  As depicted, network adapter 20' communicates with the other components of computer\nsystem/server 12' via bus 18'.  It should be understood that although not shown, other hardware and/or software components could be used in conjunction with computer system/server 12'.  Examples include, but are not limited to: microcode, device drivers,\nredundant processing units, external disk drive arrays, RAID systems, tape drives, and data archival storage systems, etc.\n It should be noted that aspects of the invention may be embodied as a system, method or computer program product.  Accordingly, aspects of the invention may take the form of an entirely hardware embodiment, an entirely software embodiment\n(including firmware, resident software, micro-code, etc.) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a \"circuit,\" \"module\" or \"system.\" Furthermore, aspects of the invention may take the form\nof a computer program product embodied in at least one computer readable medium having computer readable program code embodied thereon.\n Any combination of one or more computer readable media may be utilized.  The computer readable medium may be a computer readable signal medium or a computer readable storage medium.  A computer readable storage medium may be, for example, but\nnot limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any suitable combination of the foregoing.  More specific examples (a non-exhaustive list) of the computer readable storage\nmedium would include the following: an electrical connection having at least one wire, a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory),\nan optical fiber, a portable compact disc read-only memory (CD-ROM), an optical storage device, a magnetic storage device, or any suitable combination of the foregoing.  In the context of this document, a computer readable storage medium may be any\ntangible medium that can contain, or store, a program for use by, or in connection with, an instruction execution system, apparatus, or device.\n A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein, for example, in baseband or as part of a carrier wave.  Such a propagated signal may take any of a variety of forms,\nincluding, but not limited to, electro-magnetic, optical, or any suitable combination thereof.  A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate, propagate, or\ntransport a program for use by or in connection with an instruction execution system, apparatus, or device.\n Program code embodied on a computer readable medium may be transmitted using any appropriate medium, including but not limited to wireless, wire line, optical fiber cable, RF, etc., or any suitable combination of the foregoing.\n Computer program code for carrying out operations for aspects of the invention may be written in any combination of at least one programming language, including an object oriented programming language such as Java.RTM., Smalltalk, C++ or the\nlike and conventional procedural programming languages, such as the \"C\" programming language or similar programming languages.  The program code may execute entirely on the user's computer (device), partly on the user's computer, as a stand-alone\nsoftware package, partly on the user's computer and partly on a remote computer, or entirely on the remote computer or server.  In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a\nlocal area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).\n Aspects of the invention are described herein with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems) and computer program products.  It will be understood that each block of the flowchart illustrations\nand/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer program instructions.  These computer program instructions may be provided to a processor of a general purpose\ncomputer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, create means for\nimplementing the functions/acts specified in the flowchart and/or block diagram block or blocks.\n These computer program instructions may also be stored in a computer readable medium that can direct a computer, other programmable data processing apparatus, or other devices to function in a particular manner, such that the instructions stored\nin the computer readable medium produce an article of manufacture.  Such an article of manufacture can include instructions which implement the function/act specified in the flowchart and/or block diagram block or blocks.\n The computer program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other devices to cause a series of operational steps to be performed on the computer, other programmable apparatus or other\ndevices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions/acts specified in the flowchart and/or block diagram block or\nblocks.\n This disclosure has been presented for purposes of illustration and description but is not intended to be exhaustive or limiting.  Many modifications and variations will be apparent to those of ordinary skill in the art.  The embodiments were\nchosen and described in order to explain principles and practical application, and to enable others of ordinary skill in the art to understand the disclosure.\n Although illustrative embodiments of the invention have been described herein with reference to the accompanying drawings, it is to be understood that the embodiments of the invention are not limited to those precise embodiments, and that\nvarious other changes and modifications may be affected therein by one skilled in the art without departing from the scope or spirit of the disclosure.", "application_number": "14537344", "abstract": " Methods and arrangements for generating a learning graph. A contemplated\n     method includes: utilizing at least one processor to execute instructions\n     to perform the steps of: receiving a proficiency input relating to a\n     student; receiving a target knowledge node, wherein the target knowledge\n     node represents at least one skill the student does not currently\n     possess; determining at least one skill requirement of the at least one\n     skill; identifying at least one path between the proficiency input and\n     the target knowledge node based upon the at least one determined skill\n     requirement; calculating a gap between the proficiency input and the\n     target knowledge node at the at least one identified path; and\n     recommending at least one learning content module based upon the\n     calculated gap.\n", "citations": ["8571462", "20020142274", "20040202987", "20120329027", "20130266922", "20150106377"], "related": []}, {"id": "20160269498", "patent_code": "10375186", "patent_name": "Frequent sites based on browsing patterns", "year": "2019", "inventor_and_country_data": " Inventors: \nKannan; Vishwac Sena (Redmond, WA), Surti; Tanvi Saumil (Seattle, WA)  ", "description": "BACKGROUND\n Some web browsers may present a user with a list of websites that have been previously browsed to by the user to enable future browsing.  The websites presented to the user, however, may not be representative of the websites that are most\nrelevant to the user based on the user's browsing patterns.\nSUMMARY\n Various embodiments provide a frequent sites module which is designed to generate frequent sites for a user that include websites that are relevant to the user based on browsing patterns of the user.  In one or more embodiments, the frequent\nsites are generated from user-engagement data that indicates engagement by the user with websites identified in the user's browsing history.  A web platform, e.g., a web browser, can display the frequent sites for the user in a frequent sites user\ninterface container to enable the user to efficiently navigate to the websites that are relevant to the user by selection of websites from the frequent sites user interface container.\n Various embodiments describe other aspects of frequent sites based on browsing patterns, including multiple-device frequent sites, device-specific frequent sites, domain-specific frequent sites, URL-specific frequent sites, decaying of frequent\nsites, recent frequent sites, and contextual frequent sites.\n This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description.  This Summary is not intended to identify key features or essential features of the claimed subject\nmatter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter. BRIEF DESCRIPTION OF THE DRAWINGS\n The detailed description is described with reference to the accompanying figures.  The same numbers are used throughout the drawings to reference like features and components.\n FIG. 1 is an illustration of an environment in an example implementation in accordance with one or more embodiments.\n FIG. 2 is an illustration of a system in an example implementation showing FIG. 1 in greater detail.\n FIG. 3 illustrates an example user interface in accordance with one or more embodiments.\n FIG. 4 illustrates an additional example user interface in accordance with one or more embodiments.\n FIG. 5 illustrates an example of multiple-device frequent sites in accordance with one or more embodiments.\n FIG. 6 illustrates an example of device-specific frequent sites in accordance with one or more embodiments.\n FIG. 7 illustrates an example of domain-specific frequent sites in accordance with one or more embodiments.\n FIG. 8 is a flow diagram that describes steps in a method in accordance with one or more embodiments.\n FIG. 9 is an additional flow diagram that describes steps in a method in accordance with one or more embodiments.\n FIG. 10 is an additional flow diagram that describes steps in a method in accordance with one or more embodiments.\n FIG. 11 is an additional flow diagram that describes steps in a method in accordance with one or more embodiments.\n FIG. 12 illustrates an example system including various components of an example device that can be implemented as any type of computing device as described with reference to FIGS. 1 and 2 to implement various embodiments described herein.\nDETAILED DESCRIPTION\nOverview\n Frequent sites may be used by a web platform to display a representation of websites that are frequently navigated to by a user.  Conventional approaches utilized to display frequent sites may not be representative of the amount of time, or the\nuser's interaction, with the websites in the frequent sites.  Other problems of previous solutions to frequent sites include failing to take device-specific browsing patterns of a user into account, selecting a wrong subdomain of a frequented domain, and\nfailing to take recent changes in a user's browsing patterns into account, to name just a few.\n Embodiments are described herein, however, that provide a user with frequent sites that include websites that are relevant to the user based on browsing patterns of the user.  For example, the frequent sites can be generated from user-engagement\ndata that indicates engagement by the user with websites identified in the user's browsing history and/or engagement by the user with a web platform, such as a web browser.\n Various embodiments describe other aspects of frequent sites based on browsing patterns, including multiple-device frequent sites, device-specific frequent sites, domain-specific frequent sites, URL-specific frequent sites, decaying of frequent\nsites, recent frequent sites, and contextual frequent sites.\n A web platform, e.g., a web browser, can display the frequent sites for the user in a frequent sites user interface container to enable the user to efficiently navigate to the websites that are relevant to the user by selection of websites from\nthe frequent sites user interface container.  A web platform is a platform that works in connection with content of the web, e.g. public content.  A web platform can include and make use of many different types of technologies such as, by way of example\nand not limitation, URLs, HTTP, REST, HTML, CSS, JavaScript, DOM, as well as other technologies.  The web platform can also work with a variety of data formats such as XML, JSON, and the like.  Web platforms can include web browsers, local applications\nsuch as Windows.RTM.  Store applications that can be executed on a user's local computing device, and the like.\n Further discussion of these embodiments and others may be found in the following sections.\n Example Environment\n FIG. 1 is an illustration of an environment 100 in an example implementation that is operable to employ the techniques described in this document.  The illustrated environment 100 includes an example of a computing device 102 that may be\nconfigured in a variety of ways.  For example, computing device 102 may be configured as a traditional computer (e.g., a desktop personal computer, laptop computer, and so on), a mobile station, an entertainment appliance, a set-top box communicatively\ncoupled to a television, a wireless phone, a netbook, a game console, a handheld device, and so forth as further described in relation to FIG. 2.  Thus, computing device 102 may range from full resource devices with substantial memory and processor\nresources (e.g., personal computers, game consoles) to a low-resource device with limited memory and/or processing resources (e.g., traditional set-top boxes, hand-held game consoles).  Computing device 102 also includes software that causes computing\ndevice 102 to perform one or more operations as described below.\n Computing device 102 also includes a web platform 104.  As noted above, the web platform works in connection with content of the web, e.g. public content such as web pages and the like.  A web platform can include and make use of many different\ntypes of technologies such as, by way of example and not limitation, URLs, HTTP, REST, HTML, CSS, JavaScript, DOM, and the like.  The web platform can also work with a variety of data formats such as XML, JSON, and the like.  Web platforms can include\nweb browsers, local applications such as a Windows.RTM.  Store application, and the like.  In the examples described below, a web platform in the form of a web browser that navigates to various websites is utilized.  It is to be appreciated and\nunderstood, however, that the inventive principles can be employed by web platforms other than web browsers.\n Computing device 102 may also include a frequent sites module 106.  Frequent sites module 106 is configured to generate frequent sites for a user that includes websites that are relevant to the user based on browsing patterns of the user.  In\nsome cases, frequent sites module 106 generates the frequent sites based on user-engagement data that indicates engagement by the user with websites identified in the user's browsing history.  As described in more detail below, frequent sites module 106\nmay also be implemented to generate multiple-device frequent sites, device-specific frequent sites, domain-specific frequent sites, URL-specific frequent sites, recent frequent sites, and contextual frequent sites, and to decay frequent sites.  While\nfrequent sites module 106 is illustrated as residing at computing device 102, it is to be appreciated that frequent sites module may also be implemented, in whole or in part, at a platform 210, which is described below with regards to FIG. 2.  Frequent\nsites module 106 will be described in more detail below.\n Computing device 102 also includes a gesture module 108 that recognizes input pointer gestures that can be performed by one or more fingers, and causes operations or actions to be performed that correspond to the gestures.  The gestures may be\nrecognized by module 108 in a variety of different ways.  For example, gesture module 108 may be configured to recognize a touch input, such as a finger of a user's hand 108a as proximal to display device 110 of computing device 102 using touchscreen\nfunctionality, or functionality that senses proximity of a user's finger that may not necessarily be physically touching the display device 110, e.g., using near field technology.  Module 108 can be utilized to recognize single-finger gestures and bezel\ngestures, multiple-finger/same-hand gestures and bezel gestures, and/or multiple-finger/different-hand gestures and bezel gestures.\n Computing device 102 may also be configured to detect and differentiate between a touch input (e.g., provided by one or more fingers of the user's hand 108a) and a stylus input (e.g., provided by a stylus 112).  The differentiation may be\nperformed in a variety of ways, such as by detecting an amount of the display device 110 that is contacted by the finger of the user's hand 108a versus an amount of the display device 110 that is contacted by stylus 112.\n Thus, gesture module 108 may support a variety of different gesture techniques through recognition and leverage of a division between stylus and touch inputs, as well as different types of touch inputs and non-touch inputs.\n FIG. 2 illustrates an example system 200 that includes computing device 102 as described with reference to FIG. 1.  The example system 200 enables ubiquitous environments for a seamless user experience when running applications on a personal\ncomputer (PC), a television device, and/or a mobile device.  Services and applications run substantially similar in all three environments for a common user experience when transitioning from one device to the next while utilizing an application, playing\na video game, watching a video, and so on.\n In the example system 200, multiple devices are interconnected through a central computing device.  The central computing device may be local to the multiple devices or may be located remotely from the multiple devices.  In one embodiment, the\ncentral computing device may be a cloud of one or more server computers.  These computers can be connected to the multiple devices through a network, the Internet, or other data communication link.  In one embodiment, this interconnection architecture\nenables functionality to be delivered across multiple devices to provide a common and seamless experience to a user of the multiple devices.  Each of the multiple devices may have different physical requirements and capabilities, and the central\ncomputing device uses a platform to enable the delivery of an experience to the device that is both tailored to the device and yet common to all devices.  In one embodiment, a class of target devices is created and experiences are tailored to the generic\nclass of devices.  A class of devices may be defined by physical features, types of usage, or other common characteristics of the devices.\n In various implementations, computing device 102 may assume a variety of different configurations, such as for computer 202, mobile 204, and television 206 uses.  Each of these configurations includes devices that may have generally different\nconstructs and capabilities, and thus the computing device 102 may be configured according to one or more of the different device classes.  For instance, computing device 102 may be implemented as the computer 202 class of a device that includes a\npersonal computer, desktop computer, a multi-screen computer, laptop computer, netbook, and so on.  Each of these different configurations may employ a web platform, e.g., a web browser, as described above and below.\n Computing device 102 may also be implemented as the mobile 204 class of device that includes mobile devices, such as a mobile phone, portable music player, portable gaming device, a tablet computer, a multi-screen computer, and so on.  The\ncomputing device 102 may also be implemented as the television 206 class of device that includes devices having or connected to generally larger screens in casual viewing environments.  These devices include televisions, set-top boxes, gaming consoles,\nand so on.  The techniques described herein may be supported by these various configurations of the computing device 102 and are not limited to the specific examples the techniques described herein.\n Cloud 208 includes and/or is representative of a platform 210 that can include frequent sites module 106.  Platform 210 abstracts underlying functionality of hardware (e.g., servers) and software resources of cloud 208.  Frequent sites module\n106 may include applications and/or data that can be utilized while computer processing is executed on servers that are remote from the computing device 102.  Frequent sites module 106 can be provided as a service over the Internet and/or through a\nsubscriber network, such as a cellular or Wi-Fi network.\n Frequent sites module 106 is configured to generate frequent sites for a user that includes websites that are relevant to the user based on browsing patterns of the user.  In some cases, frequent sites module 106 generates the frequent sites\nbased on user-engagement data that indicates engagement by the user with websites identified in the user's browsing history.  As described in more detail below, frequent sites module 106 may also be implemented to generate multiple-device frequent sites,\ndevice-specific frequent sites, domain-specific frequent sites, URL-specific frequent sites, recent frequent sites, and contextual frequent sites, and to decay frequent sites.  While frequent sites module 106 is illustrated in FIG. 2 as residing at\nplatform 210, it is to be appreciated that frequent sites module may also be implemented, in whole or in part, at computing device 102 as described above.  Frequent sites module 106 will be described in more detail below.\n Platform 210 may abstract resources and functions to connect computing device 102 with other computing devices.  Platform 210 may also serve to abstract scaling of resources to provide a corresponding level of scale to encountered demand for\nfrequent sites module 106 that are implemented via platform 210.  Accordingly, in an interconnected device embodiment, implementation of functionality described herein may be distributed throughout system 200.  For example, the functionality may be\nimplemented in part on computing device 102 as well as via platform 210 that abstracts the functionality of cloud 208.\n Generally, any of the functions described herein can be implemented using software, firmware, hardware (e.g., fixed logic circuitry), manual processing, or a combination of these implementations.  The terms \"module,\" \"functionality,\" and \"logic\"\nas used herein generally represent software, firmware, hardware, or a combination thereof.  In the case of a software implementation, the module, functionality, or logic represents program code that performs specified tasks when executed on or by a\nprocessor (e.g., CPU or CPUs).  The program code can be stored in one or more computer readable memory devices.  The features of the gesture techniques described below are platform-independent, meaning that the techniques may be implemented on a variety\nof commercial computing platforms having a variety of processors.\n Having described example operating environments in which the inventive principles can be employed, consider now a discussion of various embodiments.\n Frequent Sites Based on User-Engagement Data\n In one or more embodiments, frequent sites module 106 is configured to generate frequent sites for a user that includes websites that are relevant to the user based on browsing patterns of the user.  As described herein, the term \"website\" is\nused to refer to websites or web pages that can be navigated to via a web platform using a web address of the website.  Examples of web addresses include uniform resource locators (URLs), uniform resource identifiers (URIs), internationalized resource\nidentifiers (IRIs), and internationalized domain names (IDNs).\n Web platform 104 is configured to monitor a user's browsing history as the user navigates to various websites.  Frequent sites module 106 receives the user's browsing history from web platform 104, and uses the browsing history to generate the\nfrequent sites.  In some cases frequent sites module 106 can be executed locally at a computing device.  For example, both web platform 104 and frequent sites module 106 can be implemented locally at computing device 102.  Alternately, at least part of\nfrequent sites module 106 can be implemented remote from computing device 102, such as at one or more server computers.\n In one or more embodiments, a user's browsing history includes user-engagement data for each website identified in the browsing history.  The user-engagement data may include user interactions with each website identified in the browsing\nhistory, such as clicking on links on websites, typing into data entry fields on websites, scrolling on websites, watching videos on websites, listening to audio on the websites, and the like.  In some cases, frequent sites module 106 measures the\nspecific user interaction with a website, such as the number of clicks on the website or the number of letters typed into text boxes on the website.  Different values may be assigned to different interaction items.  For example, typing into a data entry\nfield of a website may be assigned a higher user interaction value than scrolling the website.\n The user-engagement data may also include a time value indicating an amount of time that the user was engaged with the website.  For example, the time value may correspond to an amount of time that a user watched a video on a website, or an\namount of time that the user read an article on the website which may be identified by the user scrolling a page of the website.  In some cases, the time value may correspond to the amount of time the website is displayed as an \"active\" tab within a web\nbrowser.\n Frequent sites module 106 can be configured to generate and order the frequent sites based on the user-engagement data.  For example, frequent sites module 106 may arrange websites with higher user-engagement data values ahead of websites with\nlower user-engagement data values.  The user-engagement data values may be a weighted aggregate of the user-engagement with a website for multiple navigations by the user to the website.\n In one or more embodiments, frequent sites module 106 calculates a navigation value for each navigation to a website by a user, and a navigation count is calculated as the sum of the navigation values of each navigation to the website.  The\nfrequent sites are then determined and ordered based on the navigation count corresponding to each website.  For example, websites with higher navigation counts are ordered ahead of websites with lower navigation counts in the frequent sites.\n In some cases, the navigation count for a website is incremented by one for each navigation to a website by a user.  Thus, a user that has navigated to the website Facebook.com six times, and to the website Mashable.com three times, may see\nFacebook.com arranged ahead of Mashable.com in the user's frequent sites.\n In one or more embodiments, however, frequent sites module 106 calculates a navigation value for each instance of navigation to a website based on user-engagement with the website during the navigation.  The navigation value may be normalized\nbetween 1 and 0, such that if the user has a high level of engagement with a website during a navigation, the navigation value is closer to 1.  Conversely, if the user has a low level of engagement with the website during the navigation, the navigation\nvalue is closer to 0.  Frequent sites module 106 then generates the frequent sites using the navigation count for each website.\n As an example, consider the following.  Sarah habitually navigates to Facebook.com and bounces off within a couple of seconds several times every day, but visits Mashble.com and reads articles for several hours every day.  In previous\napproaches, Sarah's frequent sites may position Facebook.com ahead of Mashable.com because Sarah navigated to Facebook.com more frequently, and thus Facebook.com would have a higher navigation count.  However, this may not be representative of the amount\nof time, or the level of engagement, that Sarah has with each website.\n Now, frequent sites module 106 generates the frequent sites based on user-engagement with each website.  Continuing with the example above, frequent sites module 106 now calculates a high navigation count for Mashable.com based on Sarah's\nengagement with Mashable.com.  For example, Sarah may interact with Mashable.com by scrolling articles on Mashable.com and clicking links to different articles, which causes frequent sites module 106 to calculate a high navigation value each time Sarah\nnavigates to Mashable.com.  Conversely, frequent sites module 106 now calculates a low navigation count for Facebook.com based on Sarah's engagement with Facebook.com.  For example, while Sarah frequently visits Facebook.com, the low amount of time Sarah\nspends during each visit causes frequent sites module 106 to calculate a low navigation value each time Sarah navigates to Facebook.com.  Thus, by accounting for Sarah's engagement with Mashable.com and Facebook.com, frequent sites module 106 may now\narrange Mashable.com ahead of Facebook.com in Sarah's frequent sites.\n In one or more embodiments, frequent sites module 106 is configured to generate the frequent sites based on both user-engagement data with each website, as well as average user-engagement data.  The average user-engagement data can include the\naverage level of user interaction with a website, as well as an average time value corresponding to an amount of time of the average user interaction during a typical navigation.  The average engagement data may be aggregated over a period of time, and\nmay be updated each time that a user navigates to a website.  Frequent sites module 106 may normalize the user-engagement data for a specific navigation, with the average user-engagement data, to generate a navigation value for the specific navigation.\n In one or more embodiments, frequent sites module 106 is configured to determine whether a navigation to a website corresponds to a page bounce.  As described herein, a page bounce occurs when a user navigates to a website and then navigates\naway to another website in a short period of time (e.g., less than 5 seconds).  A page bounce may also be defined to include script or markup induced navigations by a website.  In some cases a navigation value is not assigned for a navigation to a\nwebsite if the navigation to the website is determined to correspond to a page bounce.  Alternately, frequent sites module 106 may assign a low navigation value if the navigation is determined to correspond to a page bounce.\n Web platform 104 is configured to render the frequent sites in a frequent sites user interface container by displaying a representation of websites (e.g., icon, tile, and so on) that are frequently navigated to by the user.  As an example,\nconsider FIG. 3.  There, a web browser user interface is shown generally at 300 in accordance with one or more embodiments.  In this particular example, web browser user interface 300 includes a frequent sites user interface container 302 that is\ndisplayed on a new tab page 304 of web browser user interface 300.  Frequent sites user interface container 302 displays multiple tiles 306-324 that each correspond to a website in the frequent sites.  In this particular example, frequent sites user\ninterface container 302 display ten different websites.  It is to be appreciated, however, that any number of websites can be displayed in frequent sites user interface container 302.\n Tiles 306-324 are arranged based on the navigation count of each website corresponding to each tile.  In FIG. 3, for example, the arrangement of tile 306 ahead of tile 308 in frequent sites user interface container 302 indicates that the website\nCNN.com has a higher navigation count than the website Amazon.com, and so on.  As described throughout, the navigation count may be calculated using user-engagement data.  By clicking on one of tiles 306-324, the browser can be navigated to the\ncorresponding website.  Thus, frequent sites user interface container 302 greatly facilitates and enhances a user's experience navigating to websites that are relevant to the user.\n FIG. 4 illustrates an alternate web browser user interface that is shown generally at 400 in accordance with one or more embodiments.  In this particular example, web browser user interface 400 includes a frequent sites user interface container\n402 that is displayed in a quick site access container 404 of web browser user interface 400.  Quick site access container 404 can be displayed when a user interacts with an address bar of the web browser.  Similar to the frequent sites user interface\ncontainer 302 of FIG. 3, frequent sites user interface container 402 displays multiple tiles 406-424 that correspond to websites that the user frequently visits.  Frequent sites user interface containers 302 and 402 are illustrated to show example\ndisplays of frequent sites.  It is to be appreciated, however, that the frequent sites may be displayed by web platform 104 in any suitable manner.\n Multiple-Device Frequent Sites\n In one or more embodiments, frequent sites module 106 enables a user's browsing history to roam across multiple devices.  For example, a user may own, or use, multiple different computing devices, such as a desktop computer the user uses at\nwork, a laptop computer, a tablet computer, and a mobile phone.  The user may use each of these devices to navigate to different websites via web platform 104 executed on each device.  Frequent sites module 106 can be configured to receive the user's\nbrowsing history from each device, and aggregate the user's browsing history for all of the user's devices to generate multiple-device frequent sites for the user.\n In some cases, frequent sites module 106 can display the multiple-device frequent sites on a new device of the user.  For example, if the user buys a new laptop, frequent sites module 106 can display the multiple frequent sites the first time\nthat the user users web platform 104 on the new laptop.\n As an example, consider FIG. 5 which illustrates an example 500 of multiple-device frequent sites 502.  As illustrated in FIG. 5, multiple-device frequent sites 502 are generated using browsing history data from a desktop computer 504 and a\ntablet computer 506.  FIG. 5 illustrates multiple-device frequent sites as being generated by frequent sites module 106 at platform 210.  Alternately, however, frequent sites module 106 can be implemented at each of device 504 and device 506.  Frequent\nsites module 106, at each of devices 504 and 506, can receive a roamed browsing history from platform 210, and generate the multiple-device frequent sites at each of devices 504 and 506, based on the roamed browsing history.\n In this example, consider that a user, Matt, owns both desktop computer 504 and tablet computer 506, and uses these devices to navigate to various websites via web platform 104 installed on each device.  Web platform 104, installed on Matt's\ndesktop computer, maintains a local browsing history 508 that indicates that Amazon.com, USBank.com, and Seahawks.com have navigation counts of 90, 60, and 30, respectively.  Similarly, web platform 104, installed on Matt's tablet computer, maintains a\nlocal browsing history 510 that indicates that MSNBC.com, Facebook.com, and Seahawks.com have navigation counts of 100, 50, and 40, respectively.  As described above, the navigation count may be generated from user-engagement data that identifies\nengagement by Matt with each website identified in Matt's browsing history.\n Web platform 104, implemented at Matt's desktop computer and Matt's tablet computer, are each configured to send these local browsing histories to frequent sites module 106, which may be located remote from each of these devices.  In this\nexample, frequent sites module 106 is implemented at platform 210.  Frequent sites module 106 receives the local browsing history from each of Matt's devices, and aggregates local browsing history 508 from Matt's desktop computer with local browsing\nhistory 510 from Matt's tablet computer to generate Matt's multiple-device frequent sites 502.  In this example, frequent sites module 106 lists MSNBC.com first on multiple-device frequent sites 502 because MSNBC.com has the highest navigation count of\nany of the websites identified in Matt's browsing history.\n Frequent sites module 106 lists Amazon.com second on multiple-device frequent sites 502 because Amazon.com has the second highest navigation count.  Note, therefore, that Matt's multiple-device frequent sites reflect navigation by Matt via both\nMatt's desktop computer (Amazon.com) and Matt's tablet computer (MSNBC.com).\n Next on multiple-device frequent sites 502 is Seahawks.com.  Frequent sites module 106 lists Seahawks.com third on multiple-device frequent sites 502 by combining the navigation count of Seahawks.com on Matt's desktop computer with the\nnavigation count to Seahawks.com on Matt's tablet computer.  Thus, the navigation count for Seahawks.com, via both devices, is 70.  The combined navigation count to Seahawks.com places Seahawks.com ahead of both USBank.com and Facebook.com, which have\nnavigation counts of 60 and 50, respectively.\n Device-Specific Frequent Sites\n While multiple-device frequent sites may be beneficial to the user for a variety of different reasons, some users may establish device-specific browsing patterns and want their frequent sites to be customized for each of their devices.\n In one or more embodiments, web platform 104 is configured to modify multiple-device frequent sites for a computing device based at least in part on a local browsing history at the computing device.  As the user navigates to various websites on\na computing device, web platform 104 can store a local browsing history at the computing device.  Web platform 104 can also send the local browsing history to frequent sites module 106 to aggregate with the user's browsing history from one or more\nadditional devices to generate the multiple-device frequent sites.  Subsequently, web platform 104 can modify the multiple-device frequent sites using the local browsing history to generate device-specific frequent sites at the computing device. \nGenerally, the device-specific frequent sites lists websites that have been navigated to on the local computing device ahead of websites that have not been navigated to on the local computing device.  The device-specific frequent sites may include at\nleast one website that is not included in the multiple-device frequent sites.\n In some cases, to modify the multiple-device frequent sites, a weight is applied to websites that have been navigated to on the computing device to cause the websites that have been navigated to on the computing device to be listed higher in the\ndevice-specific frequent sites than websites that have not been navigated to on the local computing device.\n In one or more embodiments, websites with a local navigation count at a computing device above a certain threshold are listed ahead of websites that have not been navigated to using the computing device in the device-specific frequent sites. \nFor example, if the local browsing history indicates that five different websites have a local navigation count above a certain threshold, then these five websites will be listed ahead of any websites that have not been navigated to via the computing\ndevice in the device-specific frequent sites.  The threshold, for example, may be one or more navigations to a website via the local computing device.  Alternately, the threshold may be a specific local navigation count that is based on user-engagement\ndata.  In some cases, the websites that have been navigated to locally may be arranged based on the local navigation count.  Any remaining spots in the device-specific frequent sites may then be filled in with websites from the multiple-device frequent\nsites that were navigated to by the user via different devices.\n Returning to example 500, note that Matt has shown a preference to use his tablet computer to browse the web for news, such as at MSNBC.com, and to stay in touch with social network sites such as at Facebook.com.  However, Matt rarely uses his\ntablet computer to visit shopping sites such as Amazon.com.  Therefore, Matt may prefer to not have Amazon.com be placed in his frequent sites when Matt is using his tablet computer.  Conversely, Matt has shown a preference to use his desktop computer to\nshop online for products, such as at Amazon.com, and to manage finances, such as at USBank.com.  Matt rarely uses his desktop computer to visit social network sites, however, such as Facebook.com.  Therefore, Matt may prefer to not have Facebook.com be\nplaced in his frequent sites when Matt is using his desktop computer.\n FIG. 6 illustrates an example 600 of device-specific frequent sites 602 corresponding to a desktop computer 604, and device-specific frequent sites 606 corresponding to a tablet computer 608.  Example 600 further includes multiple-device\nfrequent sites 610.  In this example, multiple-device frequent sites 610 correspond to multiple-device frequent sites 502, from FIG. 5.  As described in FIG. 5, therefore, multiple-device frequent sites 610 were generated based on the local browsing\nhistories of desktop computer 604 and tablet computer 608, which are illustrated in FIG. 5 as desktop computer 504 and tablet computer 506, respectively.\n In this particular example, web platform 104 has modified multiple-device frequent sites 610 at desktop computer 604 based on the local browsing history at the desktop computer to generate device-specific frequent sites 602.  Device-specific\nfrequent sites 602 lists websites that have been navigated to on Matt's desktop computer ahead of websites that have not been navigated to at his desktop computer.  In this example, Amazon.com has the highest navigation count of websites that have been\nnavigated to at Matt's desktop computer, and thus is placed first in device-specific frequent sites 602.\n Device-specific frequent sites 602 lists Seahawks.com next, followed by USBank.com, because while both Seahawks.com and USBank.com have been navigated to on Matt's desktop computer, Seahawks.com has a higher navigation count of 70.\n In some embodiments, web platform 104 lists websites in the device-specific frequent sites that have not been navigated to on the local computing device if there are available spaces.  In device-specific frequent sites 602, for example,\nMSNBC.com and Facebook.com are listed fourth and fifth, respectively, even though these websites have not been navigated to by Matt using his desktop computer because there are open spots in the device-specific frequent sites.  In addition, note that\nMSNBC.com has the highest navigation count, but is listed fourth in device-specific frequent sites 602 because it has not been navigated to at desktop computer 604.\n Similarly, web platform 104 has modified multiple-device frequent sites 610 at tablet computer 608 based on the local browsing history at the tablet computer to generate device-specific frequent sites 606.  Device-specific frequent sites 606\nlists websites that have been navigated to on Matt's tablet computer ahead of websites that have not been navigated to on his tablet computer.  In this example, MSNBC.com has the highest navigation count of websites that have been navigated to at Matt's\ntablet computer, and thus is placed first in device-specific frequent sites 606.\n Device-specific frequent sites 606 lists Seahawks.com next, followed by Facebook.com, because while both Seahawks.com and Facebook.com have been navigated to on Matt's desktop computer, Seahawks.com has a higher navigation count.  Amazon.com and\nUSBank.com are listed fourth and fifth, respectively, in device-specific frequent sites 606 even though these websites have not been navigated to by Matt using his tablet computer because there are open spots in device-specific frequent sites 606.\n Domain-Specific and URL-Specific Frequent Sites\n In one or more embodiments, the frequent sites are generated to contain a maximum of one website per domain.  Websites can be grouped into top-level websites associated with a domain, and sub-level websites, or pages, within the domain.  For\nexample, ESPN.com is a top-level website associated with the URL ESPN.com as well as the domain ESPN.  In contrast, ESPN.com/NFL is also associated with the domain ESPN, but is a sub-level website, or page, associated with the URL ESPN.com/NFL.\n Frequent sites module 106 generates the frequent sites based on a domain-specific navigation count that is defined as the sum of the URL-specific navigation count for each URL within the domain.  In other words, each domain receives a\ndomain-specific navigation count that is the sum of the navigation count of each page within the domain.  In this way, the domain receives a single navigation value that is used to qualify the domain for the frequent sites.  The URL-specific navigation\ncount can be determined based on user-engagement data, as described above.\n In one or more embodiments, a URL within a domain that is qualified for the frequent sites is selected to represent the domain.  In some cases, the URL with the highest URL-specific navigation count of all URLs within a domain is selected to\nrepresent the domain in the frequent sites.\n As an example, consider FIG. 7 which illustrates an example 700 of domain-specific frequent sites 702 that are generated based on browsing history data 704 from a desktop computer 706.  In this example, consider that a user, Chad, owns desktop\ncomputer 706 and uses his desktop computer to navigate to Facebook.com which provides Chad with updates from his friends.  Additionally, Chad spends a good amount of time on other pages within the Facebook domain, such as his personal page on Facebook\nthat is associated with the URL Facebook.com/Chad, his girlfriend Stephanie's page on Facebook which is associated with the URL Facebook.com/Stephanie, and the official Facebook page of the Portland Trailblazer which is associated with the URL\nFacebook.com/Blazers.\n Frequent sites module 106 maintains a URL-specific navigation count for each of the URLs within the Facebook domain.  In this example, the URL-specific navigation counts for the URLs Facebook.com, Facebook.com/Chad, Facebook.com/Stephanie, and\nFacebook.com/Blazers, are 60, 40, 35, and 5, respectively.  Frequent sites module 106 also maintains URL-specific navigation counts for Chipotle.com of 70, ESPN.com of 20, ESPN.com/Blazers of 30, ESPN.com/NBA of 25, and ESPN.com/NFL of 10.  It is to be\nunderstood that frequent sites module 106 may generate these URL-specific navigation counts based on user-engagement data, as described throughout.\n Previous solutions might list each URL separately in Chad's frequent sites.  For example, conventional frequent sites might include the URLs Facebook.com, Facebook.com/Chad, Facebook.com/Stephanie, Facebook.com/Seahawks, Chiptole.com, ESPN.com,\nESPN.com/Blazers, ESPN.com/NBA, and ESPN.com/NFL in Chad's frequent sites.  This conventional approach causes Chad's frequent sites to be cluttered, and prevents other frequent sites from being displayed.  Furthermore, Chad may usually navigate initially\nto Facebook.com, and then navigate further to additional pages within Facebook.com.  Therefore, Chad does not need his frequent sites to include Facebook.com/Stephanie, for example, because he always navigates to Facebook.com, and then navigates from\nwithin the domain Facebook.com to the page Facebook.com/Stephanie.\n Additionally, by listing each URL within a domain separately, previous solutions may not account for the actual level of engagement that the user has with a domain.  For example, while Chad may spend a lot of time within the Facebook domain,\nChad's URL-specific navigation count for the URL Facebook.com is not greater than the URL-specific navigation count for the URL Chipotle.com.  Thus, previous solutions may generate Chad's frequent sites in the following order: Chipotle.com, Facebook.com,\nFacebook.com/Chad, Facebook.com/Stephanie, ESPN.com/Blazers, ESPN.com/NBA, ESPN.com, ESPN.com/NFL, and Facebook.com/Blazers.\n Now, frequent sites module 106 generates the frequent sites based on the domain-specific navigation count.  In this example, the domain-specific navigation count for the Facebook domain is 140, which is the sum of the URL-specific navigation\ncounts for Facebook.com, Facebook.com/Chad, Facebook.com/Stephanie, and Facebook.com/Blazers.  Similarly, the domain-specific navigation count for the domain ESPN is 85, which is the sum of the URL-specific navigation counts for ESPN.com,\nESPN.com/Blazers, ESPN.com/NBA, and ESPN.com/NFL.  The domain-specific navigation count for the Chipotle domain is 70, because the only URL within the Chipotle domain that Chad has navigated to is Chipotle.com.\n Note, in this example, ESPN.com/Blazers is listed in domain-specific frequent sites 702 instead of ESPN.com.  This is because frequent sites module 106 selects the URL with the highest URL-specific navigation count of all URLs within a domain to\nrepresent the domain in the frequent sites.  Thus, in this example, the domain-specific navigation count of 85 places the domain ESPN second in domain-specific frequent sites 702, and the URL-specific navigation count of ESPN.com/Blazers places the URL\nESPN.com/Blazers in domain-specific frequent sites 702 to represent the ESPN domain.\n Decaying Frequent Sites\n In one or more embodiments, frequent sites module 106 is configured to decay websites from the frequent sites based on a recent browsing history of a user.  If the user has not navigated to a website listed in the frequent sites during a\nthreshold browsing period, the navigation count of the website may be decayed.  Decaying the navigation count of the website may cause the position of the website in the frequent sites to be modified or removed completely from the user's frequent sites. \nThe threshold browsing period may be defined in terms of \"browsing days\".  As described herein, a browsing day corresponds to a day in which the user navigates to one or more websites via web platform 104.  By determining the threshold browsing period\nusing browsing days, instead of just regular days, the user's frequent sites are not modified if the user does use web platform 104 for a period of time.  For example, if the user goes on vacation for 7 days and does not use web platform 104 during that\ntime, then the user's frequent sites may not be modified by frequent sites module 106.\n In one or more embodiments, frequent sites module 106 is configured to decay a navigation count of a website to modify a position of the website in the frequent sites if the user does not navigate to the website via web platform 104 during a\nfirst threshold browsing period.  The first threshold browsing period, for example, may correspond to 1 browsing day.  Thus, this change can be quickly noticed by the user.  For example, if a website is listed first in the frequent sites, and the user\ndoes not navigate to the website during a browsing day, frequent sites module 106 may modify the frequent sites so that the website is now listed second in the frequent sites.  This provides the user with immediate feedback, and helps the user understand\nhow the frequent sites are generated.\n Previous approaches were slow to decay websites from the frequent sites.  For example, in some cases if a user did not visit a website for over a week, the website might still be displayed in the user's frequent sites.  Consider, for example,\nthat a user, John, navigates to Yahoo.com to check e-mail twice per day.  Last week, however, John switched his primary email account to Hotmail.com.  Now, John still performs his regular habit of checking email twice per day, but navigates to\nHotmail.com to check email instead of Yahoo.com.  Because John switched email providers, Yahoo.com is no longer relevant to John.  Previous solutions, however, would take a long time to move Yahoo.com out of John's frequent sites.\n Now, frequent sites module 106 quickly decays Yahoo.com from John's frequent sites.  For example, in some embodiments, a position of Yahoo.com in John's frequent sites will be modified if John does not browse to Yahoo.com during a single\nbrowsing day.\n In one or more embodiments, frequent sites module 106 is configured to remove a website from the frequent sites if the user does not navigate to the website via web platform 104 during a second threshold browsing period.  The second threshold\nbrowsing period may be greater than the first threshold browsing period.  For example, the second threshold browsing period may be 7 browsing days, 14 browsing days, or 30 browsing days.  Thus, continuing with the example above, if John does not browse\nto Yahoo.com within 7 browsing days, Yahoo.com may be removed completely from John's frequent sites.\n In some embodiments, frequent sites module 106 is configured to implement device-specific decaying of frequent sites.  Thus, if John has stopped browsing to Facebook.com on his desktop computer, but still navigates to Facebook.com on his tablet\ncomputer, frequent sites module 106 can begin to decay Yahoo.com from the frequent sites on John's desktop computer, but not from the frequent sites on John's tablet computer.\n Recent Frequent Sites\n In one or more embodiments, frequent sites module 106 is configured to generate the frequent sites based on a recency value that measures trends in the user's browsing history in the recent past.  For example, the recent past may include the\nlast three days of browsing.  This is the converse approach to decaying websites from the frequent sites, and accounts for websites which have recently made their way into the user's browsing history.  If frequent sites module 106 detects that a user has\na level of engagement with a website that is above a threshold in a recent time period, the frequent sites module 106 may cause the website to be included in the user's frequents sites.  For instance, weightage may be added to the website's navigation\ncount to cause the website to be included in the frequent sites.\n As an example, consider a user, Kim, who recently discovered the website Pinterest.com, and is immediately engaged with the website.  Even though three days ago Kim had never visited Pinterest.com before, for the last three days Kim is\nrepeatedly navigating back to Pinterest.com to check for new content every few hours.  In this scenario, frequent sites module 106 can modify the position of Pinterest.com in Kim's frequent sites by applying weightage to Pinterest.com to move\nPinterest.com up Kim's frequent sites.\n In some cases, frequent sites module 106 may include an indicator that can be displayed with the website in the frequent sites user interface container in web platform 104 that indicates that the website is a trending, or recently popular site. \nAlternately, frequent sites module 106 can include a separate section within the frequent sites user interface container that includes recent or trending sites.  For example, web platform 104 can display two or three recent frequent sites, or trending\nfrequent sites, in the frequent sites user interface container in a separate section from the frequent sites.\n Contextual Frequent Sites\n In one or more embodiments, frequent sites module 106 generates contextual frequent sites that include at least one additional website, that is not generally included in the frequent sites, based on a current context of a user.  The current\ncontext of a user may be based on detected browsing patterns of the user that correspond to a time of day or a time of week.  For example, frequent sites module 106 may detect that the user navigates to banking websites on the weekend, and navigates to\nsocial media sites after work hours.  Frequent sites module 106 then generates the contextual frequent sites for the user based on the time of day or the time of week.  Continuing with the example above, the frequent sites may be modified to include one\nor more banking websites on Saturday and Sunday, and may be modified to include one or more social media websites after 5 pm Monday through Friday.\n Alternately or additionally, the current context of a user may be based on detected browsing patterns of the user that correspond to an established navigation pattern.  To determine an established navigation pattern, frequent sites module 106\nmay identify companion websites that are frequently navigated to within a threshold period of time.  For example, frequent sites module 106 may determine that Twitter.com, Facebook.com, and Instagram.com are companion websites if these websites are\nfrequently navigated to within a 20-minute time period.  Then, when a user navigates to a companion website, the frequent sites can be modified to include the other companion websites.\n In some cases, frequent sites module 106 may include an indicator that can be displayed with the website in the frequent sites user interface container in web platform 104 that indicates that the website is a contextual frequent site. \nAlternately, frequent sites module 106 can include a separate section within the frequent sites user interface container that includes contextual frequent sites.\n Example Methods\n FIGS. 8-11 illustrate example flow diagrams that describe steps in methods in accordance with one or more embodiments.  These methods can be implemented in connection with any suitable hardware, software, firmware, or combination thereof.  In at\nleast some embodiments, aspects of the methods are implemented by web platform 104 that can be implemented as, by way of example and not limitation, a web browser.  Other aspects of the methods can be performed by a service provider, such as frequent\nsites module 106.  It is to be appreciated, however, that either frequent sites module 106 or web platform 104 can be implemented to perform any of the steps in FIGS. 8-11.  In some cases, frequent sites module 106 is implemented local to a computing\ndevice on which web platform 104 is implemented.  Alternately, at least a portion of frequent sites module 106 may be implemented remote from the computing device on which web platform 104 is implemented, such as at one or more server computers.\n FIG. 8 illustrates a flow diagram depicting an example method 800 for generating frequent sites based on user-engagement data.  Step 802 receives user-engagement data corresponding to engagement by a user with multiple websites via a web\nplatform.  For example, frequent sites module 106 receives user-engagement data corresponding to engagement by a user with multiple websites using web platform 104.  The user-engagement data can include user interactions with the multiple websites, which\ncan include clicking on links on the websites, typing data into data entry fields on the websites, scrolling the websites, watching videos on the websites, listening to audio on the websites, and the like.  The user-engagement data may also include a\ntime value indicating an amount of time of each of the user interactions with the websites.\n Step 804 calculates a navigation count for each of the multiple websites based on the user-engagement data.  In some cases, frequent sites module 106 calculates, for each navigation to a particular one of the multiple websites, a navigation\nvalue for the navigation based on user-engagement with the website during the navigation.  In some cases, the navigation value may be normalized between 1 and 0, such that if the user has a high level of engagement with a website during a navigation, the\nnavigation value is closer to 1.  Conversely, if the user has a low level of engagement with the website during the navigation, the navigation value is closer to 0.  The navigation count can then be calculated as a sum of the navigation values for each\nof the navigations to a particular website.\n Step 806 generates frequent sites based on the navigation counts of the multiple websites.  The frequent sites include a list of one or more of the multiple websites that is ordered based on the navigation counts of the multiple websites.  In\nsome cases, websites with higher navigation counts are ordered ahead of websites with lower navigation counts in the frequent sites.\n Step 808 renders the frequent sites in a frequent sites user interface container of the web platform.  The frequent sites can be rendered in the frequent sites user interface container of web platform 104 in any suitable way.  For example, in\nsome cases, the frequent sites are rendered in a frequent sites user interface container in a new tab page of a web browser user interface, as illustrated in FIG. 3.  Alternately, the frequent sites can be rendered in a frequent sites user interface\ncontainer in a quick access sites container of a web browser user interface, as illustrated in FIG. 4.\n FIG. 9 illustrates a flow diagram depicting an example method 900 for generating device-specific frequent sites.  Step 902 receives, at a web platform implemented at a computing device, multiple-device frequent sites corresponding to a user. \nFor example, web platform 104 implemented at computing device 102, receives multiple-device frequent sites from frequent sites module 106.  Frequent sites module 106 may be implemented remote from computing device 102, such as at one or more server\ncomputers.  The multiple-device frequent sites may be generated for a user based on the user's browsing history using multiple computing devices.  As described throughout, the browsing history of the user may include user-engagement data that indicates\nengagement by the user with websites identified in the user's browsing history.\n Step 904 modifies the multiple-device frequent sites using a local browsing history of the computing device to generate device-specific frequent sites corresponding to the user.  In some cases, to modify the multiple-device frequent sites, a\nweightage is applied to websites that have been navigated to on the computing device to cause the websites that have been navigated to on the computing device to be listed higher in the device-specific frequent sites than websites that have not been\nnavigated to on the local computing device.  In one or more embodiments, websites that have a local navigation count above a certain threshold are listed ahead of websites that have not been navigated to using the computing device in the device-specific\nfrequent sites.  Any remaining spots in the device-specific frequent sites may then be filled in with websites from the multiple-device frequent sites that were navigated to using different devices.\n Step 906 causes the device-specific frequent sites to be rendered in a frequent sites user interface container of the web platform.  The device-specific frequent sites can be rendered in the frequent sites user interface container of web\nplatform 104 in any suitable way.  For example, in some cases, the device-specific frequent sites are rendered in a frequent sites user interface container in a new tab page of a web browser user interface, as illustrated in FIG. 3.  Alternately, the\ndevice-specific frequent sites can be rendered in a frequent sites user interface container in a quick access sites container of a web browser user interface, as illustrated in FIG. 4.\n FIG. 10 illustrates a flow diagram depicting an example method 1000 for generating domain-specific frequent sites.  Step 1002 receives browsing history data of a user that indicates navigation by the user to one or more URLs associated with one\nor more domains.\n Step 1004 determines a URL-specific navigation count for each URL identified in the browsing history data.  In some cases, the URL-specific navigation count can be determined using user-engagement data that indicates engagement by the user with\nURLs identified in the browsing history data.\n Step 1006 calculates a domain-specific navigation count for each domain identified in the browsing history data as a sum of the URL-specific navigation count for each URL within the domain, and step 1008 generates frequent sites based on the\ndomain-specific navigation counts.\n Step 1010 selects the URL with the highest URL-specific navigation count of each domain in the frequent sites for display in the frequent sites.  For example, the URL with the highest URL-specific navigation count of all URLs within a domain is\nselected to represent the domain in the frequent sites.\n FIG. 11 illustrates a flow diagram depicting an example method 1100 for decaying frequent sites.  Step 1102 determines that a website listed in frequent sites has not been navigated to using a web platform during a first threshold browsing\nperiod.  The first threshold browsing period, for example, may correspond to 1 browsing day.\n Step 1104 modifies a position of the website in the frequent sites responsive to determining that the website was not navigated to during the first threshold browsing period.  For example, if the website was listed first in the frequent sites,\nthe frequent sites may be modified so that the website is now listed second in the frequent sites.  To modify the position of the website in the frequent sites, frequent sites module 106 may decay a navigation count of the website.  Steps 1102 and 1104\nmay be repeated during multiple first threshold browsing periods.  For example, frequent sites module 106 may move the website down a position in the frequent sites for each browsing day during which the website is not navigated to by the user using web\nplatform 104.\n Step 1106 determines that the website in the frequent sites was not navigated to using the web platform during a second threshold browsing period.  The second threshold browsing period may be greater than the first threshold browsing period. \nFor example, the second threshold browsing period may correspond to 7 browsing days or 14 browsing days.  In some cases, both the first threshold browsing period and the second threshold browsing period may be user customizable.\n Step 1108 removes the website from the frequent sites responsive to determining that the website in the frequent sites was not navigated to using the web platform during the second threshold browsing period.\n Having considered various embodiments, consider now a discussion of example device that can be utilized to implement the embodiments described above.\n Example Device\n FIG. 12 illustrates various components of an example device 1200 that can be implemented as any type of portable and/or computer device as described with reference to FIGS. 1 and 2 to implement embodiments described herein.  Device 1200 includes\ncommunication devices 1202 that enable wired and/or wireless communication of device data 1204 (e.g., received data, data that is being received, data scheduled for broadcast, data packets of the data, etc.).  Device data 1204 or other device content can\ninclude configuration settings of the device, media content stored on the device, and/or information associated with a user of the device.  Media content stored on device 1200 can include any type of audio, video, and/or image data.  Device 1200 includes\none or more data inputs 1206 via which any type of data, media content, and/or inputs can be received, such as user-selectable inputs, messages, music, television media content, recorded video content, and any other type of audio, video, and/or image\ndata received from any content and/or data source.\n Device 1200 also includes communication interfaces 1208 that can be implemented as any one or more of a serial and/or parallel interface, a wireless interface, any type of network interface, a modem, and as any other type of communication\ninterface.  The communication interfaces 1208 provide a connection and/or communication links between device 1200 and a communication network by which other electronic, computing, and communication devices communicate data with device 1200.\n Device 1200 includes one or more processors 1210 (e.g., any of microprocessors, controllers, and the like) which process various computer-executable or readable instructions to control the operation of device 1200 and to implement the\nembodiments described above.  Alternatively or in addition, device 1200 can be implemented with any one or combination of hardware, firmware, or fixed logic circuitry that is implemented in connection with processing and control circuits which are\ngenerally identified at 1212.  Although not shown, device 1200 can include a system bus or data transfer system that couples the various components within the device.  A system bus can include any one or combination of different bus structures, such as a\nmemory bus or memory controller, a peripheral bus, a universal serial bus, and/or a processor or local bus that utilizes any of a variety of bus architectures.\n Device 1200 also includes computer-readable media 1214, such as one or more memory components, examples of which include random access memory (RAM), non-volatile memory (e.g., any one or more of a read-only memory (ROM), flash memory, EPROM,\nEEPROM, etc.), and a disk storage device.  A disk storage device may be implemented as any type of magnetic or optical storage device, such as a hard disk drive, a recordable and/or rewriteable compact disc (CD), any type of a digital versatile disc\n(DVD), and the like.  Device 1200 can also include a mass storage media device 1216.\n Computer-readable media 1214 provides data storage mechanisms to store device data 1204, as well as various device applications 1218 and any other types of information and/or data related to operational aspects of device 1200.  For example, an\noperating system 1220 can be maintained as a computer application with computer-readable media 1214 and executed on processors 1210.  Device applications 1218 can include a device manager (e.g., a control application, software application, signal\nprocessing and control module, code that is native to a particular device, a hardware abstraction layer for a particular device, etc.), as well as other applications that can include, web browsers, image processing applications, communication\napplications such as instant messaging applications, word processing applications and a variety of other different applications.  Device applications 1218 also include any system components or modules to implement embodiments of the techniques described\nherein.  In this example, device applications 1218 include an interface application 1222 and a gesture-capture driver 1224 that are shown as software modules and/or computer applications.  Gesture-capture driver 1224 is representative of software that is\nused to provide an interface with a device configured to capture a gesture, such as a touchscreen, track pad, camera, and so on.  Alternatively or in addition, interface application 1222 and gesture-capture driver 1224 can be implemented as hardware,\nsoftware, firmware, or any combination thereof.  In addition, computer readable media 1214 can include a web platform 1225a, a frequent sites module 1225b, and a gesture module 1225c that function as described above.\n Device 1200 also includes an audio and/or video input-output system 1226 that provides audio data to an audio system 1228 and/or provides video data to a display system 1230.  Audio system 1228 and/or display system 1230 can include any devices\nthat process, display, and/or otherwise render audio, video, and image data.  Video signals and audio signals can be communicated from device 1200 to an audio device and/or to a display device via an RF (radio frequency) link, S-video link, composite\nvideo link, component video link, DVI (digital video interface), analog audio connection, or other similar communication link.  In an embodiment, audio system 1228 and/or display system 1230 are implemented as external components to device 1200. \nAlternatively, audio system 1228 and/or display system 1230 are implemented as integrated components of example device 1200.\nCONCLUSION\n Although the example implementations have been described in language specific to structural features and/or methodological acts, it is to be understood that the implementations defined in the appended claims is not necessarily limited to the\nspecific features or acts described.  Rather, the specific features and acts are disclosed as example forms of implementing the claimed features.", "application_number": "15159562", "abstract": " Various embodiments provide a frequent sites module which is designed to\n     generate frequent sites for a user that include websites that are\n     relevant to the user based on browsing patterns of the user. In one or\n     more embodiments, the frequent sites are generated from user-engagement\n     data that indicates engagement by the user with websites identified in\n     the user's browsing history. A web platform, e.g., a web browser, can\n     display the frequent sites for the user in a frequent sites user\n     interface container to enable the user to efficiently navigate to the\n     websites that are relevant to the user by selection of websites from the\n     frequent sites user interface container. Various embodiments describe\n     other aspects of frequent sites based on browsing patterns, including\n     multiple-device frequent sites, device-specific frequent sites,\n     domain-specific frequent sites, URL-specific frequent sites, decaying of\n     frequent sites, recent frequent sites, and contextual frequent sites.\n", "citations": ["6564213", "6745367", "7360171", "7437312", "7548915", "7603360", "7809744", "7818332", "7979415", "8027990", "8050675", "8069182", "8082278", "8108778", "8117208", "8135616", "8204897", "8209344", "8326859", "8332748", "8341157", "8386955", "8601019", "8631004", "8639684", "8671106", "8799276", "9092527", "9244985", "9355140", "9374431", "9489458", "20030083938", "20030126560", "20030135582", "20040186774", "20050028104", "20050114881", "20060064411", "20060173825", "20060242586", "20060248078", "20060288015", "20070011616", "20070050251", "20070100650", "20070100799", "20070100915", "20070185847", "20070226058", "20070276790", "20070294240", "20080022216", "20080104037", "20080109491", "20080154859", "20080172374", "20080189281", "20080288492", "20090006974", "20090019002", "20090083232", "20090106224", "20090112848", "20090171929", "20090177744", "20090210806", "20090271390", "20090313100", "20100010959", "20100057698", "20100131835", "20100146012", "20100169364", "20100180001", "20100205202", "20100312782", "20110035397", "20110087686", "20110093488", "20110107242", "20110161311", "20110184981", "20110191364", "20110202520", "20110202876", "20110218862", "20110225155", "20110264648", "20110295952", "20110320423", "20120005201", "20120059838", "20120066645", "20120084291", "20120123857", "20120131032", "20120137201", "20120144317", "20120158461", "20120197837", "20120246165", "20120265647", "20120265779", "20120265787", "20120284253", "20120296743", "20120316955", "20120323828", "20130036109", "20130054631", "20130073509", "20130275456", "20130282749", "20140006399", "20140149932", "20140156262", "20140280015", "20140280092", "20140280093", "20140282136", "20140379893", "20150201040"], "related": ["13923267"]}, {"id": "20160321542", "patent_code": "10325202", "patent_name": "Incorporating top-down information in deep neural networks via the bias\n     term", "year": "2019", "inventor_and_country_data": " Inventors: \nTowal; Regan Blythe (La Jolla, CA)  ", "description": "BACKGROUND\nField\n Certain aspects of the present disclosure generally relate to neural system engineering and, more particularly, to systems and methods for adjusting a bias term of activation functions of neurons in the network to increase sensitivity to an\nelement based on whether the element has an increased probability of being present in an input to the network.\nBackground\n An artificial neural network, which may comprise an interconnected group of artificial neurons (e.g., neuron models), is a computational device or represents a method to be performed by a computational device.\n Convolutional neural networks are a type of feed-forward artificial neural network.  Convolutional neural networks may include collections of neurons that each have a receptive field and that collectively tile an input space.  Convolutional\nneural networks (CNNs) have numerous applications.  In particular, CNNs have broadly been used in the area of pattern recognition and classification.\n Deep learning architectures, such as deep belief networks and deep convolutional networks, are layered neural networks architectures in which the output of a first layer of neurons becomes an input to a second layer of neurons, the output of a\nsecond layer of neurons becomes and input to a third layer of neurons, and so on.  Deep neural networks may be trained to recognize a hierarchy of features and so they have increasingly been used in object recognition applications.  Like convolutional\nneural networks, computation in these deep learning architectures may be distributed over a population of processing nodes, which may be configured in one or more computational chains.  These multi-layered architectures may be trained one layer at a time\nand may be fine-tuned using back propagation.\n Other models are also available for object recognition.  For example, support vector machines (SVMs) are learning tools that can be applied for classification.  Support vector machines include a separating hyperplane (e.g., decision boundary)\nthat categorizes data.  The hyperplane is defined by supervised learning.  A desired hyperplane increases the margin of the training data.  In other words, the hyperplane should have the greatest minimum distance to the training examples.\n Although these solutions achieve excellent results on a number of classification benchmarks, their computational complexity can be prohibitively high.  Additionally, training of the models may be challenging.\nSUMMARY\n In one aspect of the present disclosure, a method of biasing a deep neural network is disclosed.  The method includes determining whether an element has an increased probability of being present in an input to the network.  The method also\nincludes adjusting a bias of activation functions of neurons in the network to increase sensitivity to the element.  In one configuration, the bias is adjusted without adjusting weights of the network.  The method further includes adjusting an output of\nthe network based at least in part on the biasing.\n Another aspect of the present disclosure is directed to an apparatus including means for determining whether an element has an increased probability of being present in an input to the network.  The apparatus also includes means for adjusting a\nbias of activation functions of neurons in the network to increase sensitivity to the element.  In one configuration, the bias is adjusted without adjusting weights of the network.  The apparatus further includes means for adjusting an output of the\nnetwork based at least in part on the biasing.\n In another aspect of the present disclosure, a computer program product for biasing a deep neural network is disclosed.  The computer program product has a non-transitory computer-readable medium with non-transitory program code recorded\nthereon.  The program code is executed by a processor and includes program code to determine whether an element has an increased probability of being present in an input to the network.  The program code also includes program code to adjust a bias of\nactivation functions of neurons in the network to increase sensitivity to the element.  In one configuration, the bias is adjusted without adjusting weights of the network.  The program code further includes program code to adjust an output of the\nnetwork based at least in part on the biasing.\n Another aspect of the present disclosure is directed to an apparatus for biasing a deep neural network, the apparatus having a memory unit and one or more processors coupled to the memory.  The processor(s) is configured to determine whether an\nelement has an increased probability of being present in an input to the network.  The processor(s) is also configured to adjust a bias of activation functions of neurons in the network to increase sensitivity to the element.  In one configuration, the\nbias is adjusted without adjusting weights of the network.  The processor(s) is further configured to adjust an output of the network based at least in part on the biasing.\n Additional features and advantages of the disclosure will be described below.  It should be appreciated by those skilled in the art that this disclosure may be readily utilized as a basis for modifying or designing other structures for carrying\nout the same purposes of the present disclosure.  It should also be realized by those skilled in the art that such equivalent constructions do not depart from the teachings of the disclosure as set forth in the appended claims.  The novel features, which\nare believed to be characteristic of the disclosure, both as to its organization and method of operation, together with further objects and advantages, will be better understood from the following description when considered in connection with the\naccompanying figures.  It is to be expressly understood, however, that each of the figures is provided for the purpose of illustration and description only and is not intended as a definition of the limits of the present disclosure. BRIEF\nDESCRIPTION OF THE DRAWINGS\n The features, nature, and advantages of the present disclosure will become more apparent from the detailed description set forth below when taken in conjunction with the drawings in which like reference characters identify correspondingly\nthroughout.\n FIG. 1 illustrates an example implementation of designing a neural network using a system-on-a-chip (SOC), including a general-purpose processor in accordance with certain aspects of the present disclosure.\n FIG. 2 illustrates an example implementation of a system in accordance with aspects of the present disclosure.\n FIG. 3A is a diagram illustrating a neural network in accordance with aspects of the present disclosure.\n FIG. 3B is a block diagram illustrating an exemplary deep convolutional network (DCN) in accordance with aspects of the present disclosure.\n FIG. 4 is a block diagram illustrating an exemplary software architecture that may modularize artificial intelligence (AI) functions in accordance with aspects of the present disclosure.\n FIG. 5 is a block diagram illustrating the run-time operation of an AI application on a smartphone in accordance with aspects of the present disclosure.\n FIG. 6 is a diagram illustrating an image, filters, and neurons of a neural classifier network.\n FIGS. 7 and 8 illustrate examples of graphs for evidence inputs and activation outputs of a neural classifier network according to aspects of the present disclosure.\n FIG. 9 is a diagram illustrating filters and neurons of a neural classifier network according to aspects of the present disclosure.\n FIG. 10 is a diagram illustrating an image, filters, and neurons of a neural classifier network according to aspects of the present disclosure.\n FIGS. 11 and 12 are flow diagrams for methods of adjusting a bias in a neural classifier network according to aspects of the present disclosure.\nDETAILED DESCRIPTION\n The detailed description set forth below, in connection with the appended drawings, is intended as a description of various configurations and is not intended to represent the only configurations in which the concepts described herein may be\npracticed.  The detailed description includes specific details for the purpose of providing a thorough understanding of the various concepts.  However, it will be apparent to those skilled in the art that these concepts may be practiced without these\nspecific details.  In some instances, well-known structures and components are shown in block diagram form in order to avoid obscuring such concepts.\n Based on the teachings, one skilled in the art should appreciate that the scope of the disclosure is intended to cover any aspect of the disclosure, whether implemented independently of or combined with any other aspect of the disclosure.  For\nexample, an apparatus may be implemented or a method may be practiced using any number of the aspects set forth.  In addition, the scope of the disclosure is intended to cover such an apparatus or method practiced using other structure, functionality, or\nstructure and functionality in addition to or other than the various aspects of the disclosure set forth.  It should be understood that any aspect of the disclosure disclosed may be embodied by one or more elements of a claim.\n The word \"exemplary\" is used herein to mean \"serving as an example, instance, or illustration.\" Any aspect described herein as \"exemplary\" is not necessarily to be construed as preferred or advantageous over other aspects.\n Although particular aspects are described herein, many variations and permutations of these aspects fall within the scope of the disclosure.  Although some benefits and advantages of the preferred aspects are mentioned, the scope of the\ndisclosure is not intended to be limited to particular benefits, uses or objectives.  Rather, aspects of the disclosure are intended to be broadly applicable to different technologies, system configurations, networks and protocols, some of which are\nillustrated by way of example in the figures and in the following description of the preferred aspects.  The detailed description and drawings are merely illustrative of the disclosure rather than limiting, the scope of the disclosure being defined by\nthe appended claims and equivalents thereof.\n In conventional systems, filters may be specified to modify or enhance an image.  Additionally, a filter may be used to determine if a specific element is present in a portion of an image.  For example, a filter may determine if a horizontal\nline is present in a 3.times.3 pixel portion of an image.  Thus, by applying various types of filters, a system may determine whether specific objects are present in an image.  Accordingly, the filtering may be used to facilitate classifying the image.\n Convolution may be specified for linear filtering of an image.  The convolution output is the weighted sum of input pixels.  A matrix of weights may be referred to as the convolution kernel or filter.  The convolution may be obtained by a matrix\nmultiply of a linearized image and a linearized filter.\n In conventional systems, an image may be classified based on the pixels of the image.  Still, in some cases, there may be a priori knowledge that an object will be present in an image or has an increased probability of being present in the\nimage.  Aspects of the present disclosure are directed to biasing a network toward classifying an object based on a priori knowledge that the object will be present in an image or has an increased probability of being present in the image.\n FIG. 1 illustrates an example implementation of the aforementioned network biasing using a system-on-a-chip (SOC) 100, which may include a general-purpose processor (CPU) or multi-core general-purpose processors (CPUs) 102 in accordance with\ncertain aspects of the present disclosure.  Variables (e.g., neural signals and synaptic weights), system parameters associated with a computational device (e.g., neural network with weights), delays, frequency bin information, and task information may\nbe stored in a memory block associated with a neural processing unit (NPU) 108 or in a dedicated memory block 118.  Instructions executed at the general-purpose processor 102 may be loaded from a program memory associated with the CPU 102 or may be\nloaded from a dedicated memory block 118.\n The SOC 100 may also include additional processing blocks tailored to specific functions, such as a graphics processing unit (GPU) 104, a digital signal processor (DSP) 106, a connectivity block 110, which may include fourth generation long term\nevolution (4G LTE) connectivity, unlicensed Wi-Fi connectivity, USB connectivity, Bluetooth connectivity, and the like, and a multimedia processor 112 that may, for example, detect and recognize gestures.  The SOC 100 may also include a sensor processor\n114, image signal processors (ISPs), and/or navigation 120, which may include a global positioning system.  The SOC may be based on an ARM instruction set.\n The SOC 100 may also include additional processing blocks tailored to specific functions, such as a GPU 104, a DSP 106, a connectivity block 110, which may include fourth generation long term evolution (4G LTE) connectivity, unlicensed Wi-Fi\nconnectivity, USB connectivity, Bluetooth connectivity, and the like, and a multimedia processor 112 that may, for example, detect and recognize gestures.  In one implementation, the NPU is implemented in the CPU, DSP, and/or GPU.  The SOC 100 may also\ninclude a sensor processor 114, image signal processors (ISPs), and/or navigation 120, which may include a global positioning system.\n The SOC 100 may be based on an ARM instruction set.  In an aspect of the present disclosure, the instructions loaded into the general-purpose processor 102 may comprise code for determining whether an element has an increased probability of\nbeing present in an input to the network.  The instructions loaded into the general-purpose processor 102 may also comprise code for adjusting a bias of activation functions of neurons in the network to increase sensitivity to the element.  In one\nconfiguration, the bias is adjusted without adjusting weights of the network.  The instructions loaded into the general-purpose processor 102 may further comprise code for adjusting an output of the network based on the biasing.\n FIG. 2 illustrates an example implementation of a system 200 in accordance with certain aspects of the present disclosure.  As illustrated in FIG. 2, the system 200 may have multiple local processing units 202 that may perform various operations\nof methods described herein.  Each local processing unit 202 may comprise a local state memory 204 and a local parameter memory 206 that may store parameters of a neural network.  In addition, the local processing unit 202 may have a local (neuron) model\nprogram (LMP) memory 208 for storing a local model program, a local learning program (LLP) memory 210 for storing a local learning program, and a local connection memory 212.  Furthermore, as illustrated in FIG. 2, each local processing unit 202 may\ninterface with a configuration processor unit 214 for providing configurations for local memories of the local processing unit, and with a routing connection processing unit 216 that provides routing between the local processing units 202.\n Deep learning architectures may perform an object recognition task by learning to represent inputs at successively higher levels of abstraction in each layer, thereby building up a useful feature representation of the input data.  In this way,\ndeep learning addresses a major bottleneck of traditional machine learning.  Prior to the advent of deep learning, a machine learning approach to an object recognition problem may have relied heavily on human engineered features, perhaps in combination\nwith a shallow classifier.  A shallow classifier may be a two-class linear classifier, for example, in which a weighted sum of the feature vector components may be compared with a threshold to predict to which class the input belongs.  Human engineered\nfeatures may be templates or kernels tailored to a specific problem domain by engineers with domain expertise.  Deep learning architectures, in contrast, may learn to represent features that are similar to what a human engineer might design, but through\ntraining.  Furthermore, a deep network may learn to represent and recognize new types of features that a human might not have considered.\n A deep learning architecture may learn a hierarchy of features.  If presented with visual data, for example, the first layer may learn to recognize relatively simple features, such as edges, in the input stream.  In another example, if presented\nwith auditory data, the first layer may learn to recognize spectral power in specific frequencies.  The second layer, taking the output of the first layer as input, may learn to recognize combinations of features, such as simple shapes for visual data or\ncombinations of sounds for auditory data.  For instance, higher layers may learn to represent complex shapes in visual data or words in auditory data.  Still higher layers may learn to recognize common visual objects or spoken phrases.\n Deep learning architectures may perform especially well when applied to problems that have a natural hierarchical structure.  For example, the classification of motorized vehicles may benefit from first learning to recognize wheels, windshields,\nand other features.  These features may be combined at higher layers in different ways to recognize cars, trucks, and airplanes.\n Neural networks may be designed with a variety of connectivity patterns.  In feed-forward networks, information is passed from lower to higher layers, with each neuron in a given layer communicating to neurons in higher layers.  A hierarchical\nrepresentation may be built up in successive layers of a feed-forward network, as described above.  Neural networks may also have recurrent or feedback (also called top-down) connections.  In a recurrent connection, the output from a neuron in a given\nlayer may be communicated to another neuron in the same layer.  A recurrent architecture may be helpful in recognizing patterns that span more than one of the input data chunks that are delivered to the neural network in a sequence.  A connection from a\nneuron in a given layer to a neuron in a lower layer is called a feedback (or top-down) connection.  A network with many feedback connections may be helpful when the recognition of a high-level concept may aid in discriminating the particular low-level\nfeatures of an input.\n Referring to FIG. 3A, the connections between layers of a neural network may be fully connected 302 or locally connected 304.  In a fully connected network 302, a neuron in a first layer may communicate its output to every neuron in a second\nlayer, so that each neuron in the second layer will receive input from every neuron in the first layer.  Alternatively, in a locally connected network 304, a neuron in a first layer may be connected to a limited number of neurons in the second layer.  A\nconvolutional network 306 may be locally connected, and is further configured such that the connection strengths associated with the inputs for each neuron in the second layer are shared (e.g., 308).  More generally, a locally connected layer of a\nnetwork may be configured so that each neuron in a layer will have the same or a similar connectivity pattern, but with connections strengths that may have different values (e.g., 310, 312, 314, and 316).  The locally connected connectivity pattern may\ngive rise to spatially distinct receptive fields in a higher layer, because the higher layer neurons in a given region may receive inputs that are tuned through training to the properties of a restricted portion of the total input to the network.\n Locally connected neural networks may be well suited to problems in which the spatial location of inputs is meaningful.  For instance, a network 300 designed to recognize visual features from a car-mounted camera may develop high layer neurons\nwith different properties depending on their association with the lower versus the upper portion of the image.  Neurons associated with the lower portion of the image may learn to recognize lane markings, for example, while neurons associated with the\nupper portion of the image may learn to recognize traffic lights, traffic signs, and the like.\n A DCN may be trained with supervised learning.  During training, a DCN may be presented with an image 326, such as a cropped image of a speed limit sign, and a \"forward pass\" may then be computed to produce an output 322.  The output 322 may be\na vector of values corresponding to features such as \"sign,\" \"60,\" and \"100.\" The network designer may want the DCN to output a high score for some of the neurons in the output feature vector, for example the ones corresponding to \"sign\" and \"60\" as\nshown in the output 322 for a network 300 that has been trained.  Before training, the output produced by the DCN is likely to be incorrect, and so an error may be calculated between the actual output and the target output.  The weights of the DCN may\nthen be adjusted so that the output scores of the DCN are more closely aligned with the target.\n To adjust the weights, a learning algorithm may compute a gradient vector for the weights.  The gradient may indicate an amount that an error would increase or decrease if the weight were adjusted slightly.  At the top layer, the gradient may\ncorrespond directly to the value of a weight connecting an activated neuron in the penultimate layer and a neuron in the output layer.  In lower layers, the gradient may depend on the value of the weights and on the computed error gradients of the higher\nlayers.  The weights may then be adjusted so as to reduce the error.  This manner of adjusting the weights may be referred to as \"back propagation\" as it involves a \"backward pass\" through the neural network.\n In practice, the error gradient of weights may be calculated over a small number of examples, so that the calculated gradient approximates the true error gradient.  This approximation method may be referred to as stochastic gradient descent. \nStochastic gradient descent may be repeated until the achievable error rate of the entire system has stopped decreasing or until the error rate has reached a target level.\n After learning, the DCN may be presented with new images 326 and a forward pass through the network may yield an output 322 that may be considered an inference or a prediction of the DCN.\n Deep belief networks (DBNs) are probabilistic models comprising multiple layers of hidden nodes.  DBNs may be used to extract a hierarchical representation of training data sets.  A DBN may be obtained by stacking up layers of Restricted\nBoltzmann Machines (RBMs).  An RBM is a type of artificial neural network that can learn a probability distribution over a set of inputs.  Because RBMs can learn a probability distribution in the absence of information about the class to which each input\nshould be categorized, RBMs are often used in unsupervised learning.  Using a hybrid unsupervised and supervised paradigm, the bottom RBMs of a DBN may be trained in an unsupervised manner and may serve as feature extractors, and the top RBM may be\ntrained in a supervised manner (on a joint distribution of inputs from the previous layer and target classes) and may serve as a classifier.\n Deep convolutional networks (DCNs) are networks of convolutional networks, configured with additional pooling and normalization layers.  DCNs have achieved state-of-the-art performance on many tasks.  DCNs can be trained using supervised\nlearning in which both the input and output targets are known for many exemplars and are used to modify the weights of the network by use of gradient descent methods.\n DCNs may be feed-forward networks.  In addition, as described above, the connections from a neuron in a first layer of a DCN to a group of neurons in the next higher layer are shared across the neurons in the first layer.  The feed-forward and\nshared connections of DCNs may be exploited for fast processing.  The computational burden of a DCN may be much less, for example, than that of a similarly sized neural network that comprises recurrent or feedback connections.\n The processing of each layer of a convolutional network may be considered a spatially invariant template or basis projection.  If the input is first decomposed into multiple channels, such as the red, green, and blue channels of a color image,\nthen the convolutional network trained on that input may be considered three-dimensional, with two spatial dimensions along the axes of the image and a third dimension capturing color information.  The outputs of the convolutional connections may be\nconsidered to form a feature map in the subsequent layer 318 and 320, with each element of the feature map (e.g., 320) receiving input from a range of neurons in the previous layer (e.g., 318) and from each of the multiple channels.  The values in the\nfeature map may be further processed with a non-linearity, such as a rectification, max(0,x).  Values from adjacent neurons may be further pooled, which corresponds to down sampling, and may provide additional local invariance and dimensionality\nreduction.  Normalization, which corresponds to whitening, may also be applied through lateral inhibition between neurons in the feature map.\n The performance of deep learning architectures may increase as more labeled data points become available or as computational power increases.  Modern deep neural networks are routinely trained with computing resources that are thousands of times\ngreater than what was available to a typical researcher just fifteen years ago.  New architectures and training paradigms may further boost the performance of deep learning.  Rectified linear units may reduce a training issue known as vanishing\ngradients.  New training techniques may reduce over-fitting and thus enable larger models to achieve better generalization.  Encapsulation techniques may abstract data in a given receptive field and further boost overall performance.\n FIG. 3B is a block diagram illustrating an exemplary deep convolutional network 350.  The deep convolutional network 350 may include multiple different types of layers based on connectivity and weight sharing.  As shown in FIG. 3B, the exemplary\ndeep convolutional network 350 includes multiple convolution blocks (e.g., C1 and C2).  Each of the convolution blocks may be configured with a convolution layer, a normalization layer (LNorm), and a pooling layer.  The convolution layers may include one\nor more convolutional filters, which may be applied to the input data to generate a feature map.  Although only two convolution blocks are shown, the present disclosure is not so limiting, and instead, any number of convolutional blocks may be included\nin the deep convolutional network 350 according to design preference.  The normalization layer may be used to normalize the output of the convolution filters.  For example, the normalization layer may provide whitening or lateral inhibition.  The pooling\nlayer may provide down sampling aggregation over space for local invariance and dimensionality reduction.\n The parallel filter banks, for example, of a deep convolutional network may be loaded on a CPU 102 or GPU 104 of an SOC 100, optionally based on an ARM instruction set, to achieve high performance and low power consumption.  In alternative\nembodiments, the parallel filter banks may be loaded on the DSP 106 or an ISP 116 of an SOC 100.  In addition, the DCN may access other processing blocks that may be present on the SOC, such as processing blocks dedicated to sensors 114 and navigation\n120.\n The deep convolutional network 350 may also include one or more fully connected layers (e.g., FC1 and FC2).  The deep convolutional network 350 may further include a logistic regression (LR) layer.  Between each layer of the deep convolutional\nnetwork 350 are weights (not shown) that are to be updated.  The output of each layer may serve as an input of a succeeding layer in the deep convolutional network 350 to learn hierarchical feature representations from input data (e.g., images, audio,\nvideo, sensor data and/or other input data) supplied at the first convolution block C1.\n FIG. 4 is a block diagram illustrating an exemplary software architecture 400 that may modularize artificial intelligence (AI) functions.  Using the architecture, applications 402 may be designed that may cause various processing blocks of an\nSOC 420 (for example a CPU 422, a DSP 424, a GPU 426 and/or an NPU 428) to perform supporting computations during run-time operation of the application 402.\n The AI application 402 may be configured to call functions defined in a user space 404 that may, for example, provide for the detection and recognition of a scene indicative of the location in which the device currently operates.  The AI\napplication 402 may, for example, configure a microphone and a camera differently depending on whether the recognized scene is an office, a lecture hall, a restaurant, or an outdoor setting such as a lake.  The AI application 402 may make a request to\ncompiled program code associated with a library defined in a SceneDetect application programming interface (API) 406 to provide an estimate of the current scene.  This request may ultimately rely on the output of a deep neural network configured to\nprovide scene estimates based on video and positioning data, for example.\n A run-time engine 408, which may be compiled code of a Runtime Framework, may be further accessible to the AI application 402.  The AI application 402 may cause the run-time engine, for example, to request a scene estimate at a particular time\ninterval or triggered by an event detected by the user interface of the application.  When caused to estimate the scene, the run-time engine may in turn send a signal to an operating system 410, such as a Linux Kernel 412, running on the SOC 420.  The\noperating system 410, in turn, may cause a computation to be performed on the CPU 422, the DSP 424, the GPU 426, the NPU 428, or some combination thereof.  The CPU 422 may be accessed directly by the operating system, and other processing blocks may be\naccessed through a driver, such as a driver 414-418 for a DSP 424, for a GPU 426, or for an NPU 428.  In the exemplary example, the deep neural network may be configured to run on a combination of processing blocks, such as a CPU 422 and a GPU 426, or\nmay be run on an NPU 428, if present.\n FIG. 5 is a block diagram illustrating the run-time operation 500 of an AI application on a smartphone 502.  The AI application may include a pre-process module 504 that may be configured (using for example, the JAVA programming language) to\nconvert the format of an image 506 and then crop and/or resize the image 508.  The pre-processed image may then be communicated to a classify application 510 that contains a SceneDetect Backend Engine 512 that may be configured (using for example, the C\nprogramming language) to detect and classify scenes based on visual input.  The SceneDetect Backend Engine 512 may be configured to further preprocess 514 the image by scaling 516 and cropping 518.  For example, the image may be scaled and cropped so\nthat the resulting image is 224 pixels by 224 pixels.  These dimensions may map to the input dimensions of a neural network.  The neural network may be configured by a deep neural network block 520 to cause various processing blocks of the SOC 100 to\nfurther process the image pixels with a deep neural network.  The results of the deep neural network may then be thresholded 522 and passed through an exponential smoothing block 524 in the classify application 510.  The smoothed results may then cause a\nchange of the settings and/or the display of the smartphone 502.\n In one configuration, a machine learning model, such as a neural network, is configured for determining whether an element has an increased probability of being present in an input to the network; adjusting a bias of activation functions of\nneurons in the network to increase sensitivity to the element; and adjusting an output of the network based at least in part on the biasing.  The model includes a determining means and/or an adjusting means.  In one aspect, the determining means and/or\nan adjusting means may be the general-purpose processor 102, program memory associated with the general-purpose processor 102, memory block 118, local processing units 202, and or the routing connection processing units 216 configured to perform the\nfunctions recited.  In another configuration, the aforementioned means may be any module or any apparatus configured to perform the functions recited by the aforementioned means.\n According to certain aspects of the present disclosure, each local processing unit 202 may be configured to determine parameters of the model based upon desired one or more functional features of the model, and develop the one or more functional\nfeatures towards the desired functional features as the determined parameters are further adapted, tuned and updated.\n Incorporating Top-Down Information in Deep Neural Networks Via the Bias Term\n As previously discussed, there may be a priori knowledge that an object will be present in an image or has an increased probability of being present in the image.  For example, a time/location of an image may provide information regarding\nobjects that may be present in the image.  That is, in one example, if an image is taken at a football game, there is an increased probability that a football, grass, and/or helmets are present in the image.  As another example, a probability of an\nobject being present in the image may increase based on the presence of other objects in the image.  For example, an image of a snowboarder has an increased probability of including snow.\n Although aspects of the present disclosure are described for determining objects in images, aspects of the present disclosure are not limited to determining objects in images.  Of course, aspects of the present disclosure are also contemplated\nfor determining whether any element is present or has an increased probability of being present in an input to a network.  For example, aspects of the present disclosure may be used to determine whether a specific sound is present in an audio input.\n In one configuration, a network is biased toward classifying an object based on a priori knowledge that the object will be present in an image or has an increased probability of being present in the image.  The bias may be specified to prevent\nfalse positives.  That is, rather than increasing the output of a classifier neuron based on a probability that an object is present, aspects of the present disclosure scale a bias to amplify responses for objects detected in an image.\n FIG. 6 illustrates an example of an image 600 and filters 602-608 that may be applied to the image 600.  As shown in FIG. 6, the image 600 is an image of a kickball game.  In this example, the image includes green grass 610, a red ball 612,\nplayers on a blue team 614, and a player on a purple team 616.  The filters include a horizontal filter 602 that filters horizontal lines, a vertical filter 604 that filters vertical lines, a green filter 606 that filters green objects, and a red/purple\nfilter 608 that filters red/purple objects.  The filters of FIG. 6 are exemplary filters.  Aspects of the present disclosure are not limited to the filters of FIG. 6 as aspects of the present disclosure are contemplated for a variety of filters to be\napplied to an input.\n In the present example, after applying the filters 602-608 to the image 600, the output of the network may be:\n 1.  0.24--ball\n 2.  0.60--blue team\n 3.  0.15--purple team\n 4.  0.01--tree\n The output refers to the determined probability of an object being in the input based on evidence derived from the input.  In this example, there is a twenty-four percent probability that a ball is in the image, a sixty percent probability that\na player on the blue team is in the image, a fifteen percent probability that a player on the purple team is in the image, and a one percent probability that a tree is in the image.\n As shown in FIG. 6, each filter 602-608 has an input to a classifier neuron associated with a specific object (e.g., class).  In this example, for illustrative purposes, a thick line indicates a strong output from the filter and a thin line\nindicates a weak output from the filter.  As the amount of evidence for the presence of an object increases, the strength of the output from the filter increases.  For example, an output from the red/purple filter 608 to the red ball neuron 618 is strong\nbased on the filter determining that there is evidence that a red object is present in the image.\n However, as shown in FIG. 6, the output from the vertical filter 604 to the purple team neuron 620 is weak because the vertical filter 604 does not find any evidence of the purple team 616.  As previously discussed, the vertical filter 604\ndetermines whether vertical lines are present in the image.  That is, the vertical filter 604 does not filter for features associated with the purple team 616, such as humans wearing purple shirts.  Thus, because players on the purple team 616 are not\nassociated with vertical lines, there is a weak connection between the vertical filter 604 to the purple team neuron 620.\n According to aspects of the present disclosure, connections between network elements, such as filters and neurons may be referred to as synapses.  Furthermore, the classifier neurons may be referred to as output neurons and/or object neurons. \nThe classifier neurons, output neurons, and object neurons refer to neurons that output a value from an activation function based on an input from a filter.\n As previously discussed, the image 600 includes a red ball 612 and an individual wearing a purple shirt (e.g., player on the purple team 616).  Still, in the image 600, the red ball 612 is relatively small in comparison to other objects. \nMoreover, in the image 600, the individual wearing a purple shirt is not as numerous as other objects, such as the group of individuals wearing blue shirts.  Accordingly, the red ball 612 and the individual wearing a purple shirt might be missed or\nassumed not to be present based on the network output.\n Still, in the present configuration, the classification is specified to determine whether a player on the purple team 616 is in the image.  In conventional systems, based on prior knowledge that the image 600 is an image of the blue team 614\nplaying kickball with the purple team 616, the response for the classifier neuron of the purple team (e.g., purple team neuron 620) may be increased based on the probability that the image 600 includes a player on the purple team 616.  Still, there is a\nlikelihood that a player on the purple team 616 is not present in an image.  Therefore, increasing the response (e.g., activation value output) for the purple team neuron 620 based on the probability that the image 600 includes a player on the purple\nteam 616 may lead to a false positive.\n Thus, in addition to preventing false positives, it is desirable to mitigate incorrect or weak classifications of objects that are relatively small in comparison to other objects and/or not as numerous as other objects.  According to aspects of\nthe present disclosure, based on a priori knowledge that an object will be present in an image or an object has an increased probability of being present in the image, a bias of an activation function may be adjusted so that the output of the filters is\nadjusted based on the biasing.  In one configuration, the bias of a synapse to a classifier neuron may be adjusted based on the probability that object is present in an image.  As an example, the bias of synapses 622 to the purple team neuron 620 may be\nadjusted based on the probability that a player on the purple team 616 is present in the image.\n In some cases, it may be undesirable to adjust the weights of the filter to alter the output of the network based on a priori knowledge that an object will be present in an image or an object has an increased probability of being present in the\nimage.  Specifically, the weights of the filters have been determined from numerous training passes.  Therefore, adjusting the weights after training may alter the results of the training and lead to false values.\n Additionally, directly changing the activation values may result in the network classifying objects that are not present (e.g., hallucinations).  Therefore, in one configuration, the bias term is scaled to amplify responses that are likely to\nindicate the presence of an object.  That is, in the present configuration, scaling the bias changes the operating range of the activation function to be more sensitive to the input.  EQUATION 1 shows an equation for the activation function. \nactivation=f(.SIGMA..sub.iw.sub.ix.sub.i+.gamma.b.sub.i) (1)\n In EQUATION 1, w.sub.i is the weight, x.sub.i is the activation value output from a lower layer, such as a filter, and .gamma.b.sub.i is the bias term.  Specifically, .gamma.  is the amount of adjustment for the bias and b.sub.i is the bias. \nAccording to EQUATION 1, the bias term may be scaled for all synapses that lead to a particular classifier neuron.  That is, the gain of an input to a classifier neuron may be increased or decreased based on the bias.\n FIG. 7 illustrates a coordinate graph 700 with the input (.SIGMA..sub.i w.sub.ix.sub.i+.gamma.b.sub.i) to a classifier neuron on the x-axis and a value of an activation function (EQUATION 1) that is output from the classifier neuron on the\ny-axis.  The value of the activation function may be referred to as the activation value and the input to a classifier neuron may be referred to as the evidence input.  The evidence input on the x-axis is a value for an amount of evidence for the\npresence of an object.  In this example, the input values range from -10 to 10, such that a value of -10 indicates that there is little to no evidence that the object is present in the image and 10 indicates a large amount of evidence for the presence of\nthe object.  Furthermore, the activation value is the probability that an object is present in the image based on the amount of evidence (e.g., x-axis input) for the presence of the object in the image.  Thus, as shown in FIG. 7, the activation value\nincreases as the evidence input to the classifier neuron increases.  That is, a strong evidence input to the classifier neuron results in a strong activation value output.\n Additionally, FIG. 7 illustrates numerous lines plotted on the graph 700.  The lines indicate results of adjusting the bias of an input.  For example, a first line 702 indicates a baseline for an input and activation (e.g., no bias adjustment). \nIn this example, as shown in the first line 702, an evidence input of 0 results in an activation value of approximately 0.5.  Additionally, a second line 704 provides an example of adjusting the bias by 1.5.  As shown on the second line 704, an evidence\ninput of 0 results in an activation value of approximately 0.9.\n Accordingly, as shown in FIG. 7, although the first line 702 and the second line 704 receive a same value for the evidence input, the activation value that is output from the classifier neuron is adjusted based on the scaled bias.\n It should be noted that the bias may be positively adjusted or negatively adjusted.  For example, FIG. 7 illustrates both positive and negative adjustments.  The second line 704 plots the coordinates for adjusting the bias by 1.5.  The third\nline 706 plots the coordinates for adjusting the bias by -1.5.\n As previously discussed, the bias may be positively adjusted based on a priori knowledge of an item being present in an input.  For example, because birds are associated with trees the bias may be positively adjusted for a tree when an image of\na bird is presented.  Furthermore, the bias may be negatively adjusted based on a priori knowledge of an item not being present in an input.  For example, because baseballs are not associated with football games, the bias may be negatively adjusted for a\nbaseball when an image of a football game is presented.\n It should be noted that the bias is applied to each input of a classifier neuron.  That is, the bias is applied to each piece of evidence, such as the output of each filter.  For example, based on the example of FIG. 6, a bias may be applied to\neach of the synapses 622 that is input to the purple team neuron 620.  As previously discussed, the value determined for the presence of an object may vary based on the type of filter.\n For example, based on the example of FIG. 6, the horizontal filter 602 is specified to determine whether horizontal lines are present in the image.  Accordingly, because a red ball has little to no vertical lines, the value of the evidence input\nto the red ball neuron 618 from the horizontal filter 602 is low.  That is, the horizontal filter 602 found little to no evidence for horizontal lines that are associated with a red kick ball.  Thus, because the bias is applied to an input from each\nfilter, the probability that the object is present increases based on the amount of evidence found for the object from each filter.\n As previously discussed, based on the example of FIG. 6, the output of the network with an unadjusted bias may be:\n 1.  0.24--ball\n 2.  0.60--blue team\n 3.  0.15--purple team\n 4.  0.01--tree\n In the present configuration, based on the example of FIG. 6, the bias is scaled to the ball based on a priori knowledge that the ball will be present in an image or has an increased probability of being present in the image.  Based on a\npositive bias applied for the ball, the output of the network may be:\n 1.  0.50--ball\n 2.  0.35--blue team\n 3.  0.05--purple team\n 4.  0.00--tree\n As shown in the output provided above for a positive bias adjusted for the ball, in comparison to the unadjusted bias output, the probability of the ball is changed from twenty-four percent to fifty percent.\n In the present configuration, based on the example of FIG. 6, the bias is scaled to the tree based on a priori knowledge that the tree has an increased probability of being present in the image.  Based on this configuration, the output of the\nnetwork may be:\n 1.  0.10--ball\n 2.  0.35--blue team\n 3.  0.05--purple team\n 4.  0.02--tree\n As shown in the output provided above for a positive bias adjusted for the tree, in comparison to the unadjusted bias output, the probability of the tree is changed from one percent to two percent.  That is, because a tree is not present in the\nimage 600 of FIG. 6, scaling a bias to a tree does not cause a significant increase in the probability of the tree being present.\n FIG. 8 illustrates a graph 800 having an x-axis representing an evidence value that is input to a classifier neuron from a filter and a y-axis representing a value of an activation function that is output from a classifier neuron.  In FIG. 8,\nthe different curved lines indicate results of adjusting the bias of an input.  For example, a first line 802 indicates an unadjusted baseline for an input and activation.  In this example, as shown in the first line 802, when a bias is not adjusted for\nthe evidence input, an input of -1 results in an activation of approximately 0.24.  Additionally, a second line 804 provides an example of adjusting the bias by 0.5 for the evidence input.  As shown on the second line 804, an input of -1 results in an\nactivation of approximately 0.5.  Thus, as previously discussed, in the unadjusted network output, the value for an object, such as a ball, is 0.24.  Furthermore, as described above, when a bias is adjusted for the object, the value is 0.5.\n Additionally, as shown in FIG. 8, for a second object with a low evidence value, such as -5, the unadjusted activation value from the first line 802 is 0.01.  Furthermore, a second line 804 provides an example of adjusting the bias by 0.5 for\nthe evidence input of the second object.  As shown on the second line 804, a value of -5 for evidence input results in an activation value of approximately 0.02.  Thus, as described above, in the unadjusted network output, the activation value for the\nsecond object is 0.01.  Furthermore, as described above, when a bias is adjusted for the second object, the activation value is 0.02.  As previously discussed, because there is little to no evidence that the second object is present, adjusting the bias\nfor the evidence input will not cause a significant change in the activation value.\n In one configuration, the bias is adjusted as a function of the weights that lead to the object.  For example, if a bias of ball is to be adjusted, an adjustment term that is proportional to the weight of the synapses is back propagated from the\nclassifier neuron of the ball.\n FIG. 9 illustrates an example 900 of a network with classifier neurons at a top layer (layer J) connected to object specific filters at a middle layer (Layer I).  The classifiers are connected to the general filters at the lower layer (Layer H). In one example, the bias may be adjusted for the evidence of the ball.  Thus, in this example, the adjustment value may be present at the top layer so that the adjustment value (.gamma..sub.ij) is back propagated from the ball neuron 902 to the network\nin proportion to the weight of the synapses in the network.  In this example, the adjustment value may be applied at the top layer when it is known that an object is present in the image or there is an increased probability that the object is present in\nthe image.\n For example, as shown in FIG. 9, the weight of the synapse 904 from the ball filter 906 to the ball neuron 902 is high.  However, the weight of the other synapses 908 from the other object specific filters to the ball neuron 902 is weak. \nTherefore, the adjustment value back propagated to the ball filter 906 is stronger in comparison to the adjustment value that is back propagated from the ball neuron 902 to the other object specific filters of Layer I. That is, the adjustment value is\nback propagated in proportion to the weight of the synapse from each object specific filter at layer I to the classifier neuron at Layer J.\n Additionally, the adjustment value is back propagated from the object specific filters at layer I to the general filters at layer H based on the weight of the synapses from the object specific filters to each general filter at layer H.\n The equation for determining the adjustment value based on the weight of each synapse is as follows: .gamma..sub.ij=.gamma..sub.0w.sub.ij.A-inverted.j.di-elect cons.ball class (2) .gamma..sub.hi=.gamma..sub.ijw.sub.hi (3)\n In EQUATIONS 2 and 3, based on the example of FIG. 8, w.sub.ij is the weight of the synapse from Layer J to Layer I, w.sub.hi is the weight of the synapse form Layer H to Layer I, .gamma..sub.0 is the amount of bias adjustment present at the\noutput neuron, .gamma..sub.ij is the adjustment value applied to the synapse from Layer J to Layer I, and .gamma..sub.hi is the adjustment value applied to the synapse from Layer H to Layer I.\n In another configuration, instead of adjusting the bias for a specific object (e.g., class), the bias may be adjusted to specific features, such as red objects, and/or objects with round edges.  In this example, there may not be a priori\nknowledge of an object in an image.  Still, in this example, the network may be searching for a specific object, such as a purple shirt.  Therefore, the bias may be adjusted at any layer in the network.  For example, based on FIG. 9, the bias may be\nadjusted for the purple image filter 910 at Layer I and the adjustment value may be back propagated to the filters at Layer H in proportion to the weight of each synapse from Layer H to Layer I. The equation for back propagating the adjustment value to\neach synapse connected to a filter at Layer I is as follows: .gamma..sub.ij=0 (4) .gamma..sub.hi=.gamma..sub.0w.sub.hi (5)\n In EQUATIONS 4 and 5, based on the example of FIG. 9, w.sub.hi is the weight of the synapse from Layer H to Layer I, .gamma..sub.0 is the amount of bias adjustment present at the output neuron, .gamma..sub.ij is the adjustment value applied to\nthe synapse from Layer J to Layer I, and .gamma..sub.hi is the adjustment value applied to the synapse from Layer H to Layer I. In this configuration .gamma..sub.ij=0 because the adjustment is back propagated from Layer I to Layer H instead of being\napplied and back propagated from Layer J.\n In another configuration, the bias may be adjusted based on a measured response of the network to an exemplary image.  For example, an image may be presented to the network and the response of the network is measured in response to the image. \nFurthermore, the bias may be adjusted based on the response.  The adjusting may be performed at an internal level of the network.\n FIG. 10 illustrates an example of generating a bias based on a measured response to an image 1002 presented to the network 1000.  As shown in FIG. 10, the network 1000 includes a top layer (Layer J) of classifier neurons, a middle layer (Layer\nI) of object specific filters, and a bottom layer (Layer H) of general filters.  Furthermore, as shown in FIG. 10, an image 1002 is presented to the network 1000.  In this example, the image 1002 is a purple ball with a background of leaves.  As shown in\nFIG. 10, the purple ball of the image 1002 is not present as an object in the object neurons.  Therefore, to determine the adjustment value for the objects in the image 1002, the image 1002 is presented to the network 1000 to measure the response of the\nnetwork 1000.\n In the present example, when the image 1002 is presented to the network 1000, the activations of the network are measured at the neurons, synapses, and layers.  For example, as shown in FIG. 10, the activations are distributed at various\nfilters, synapses, and neurons.  Specifically, in this example, a purple filter 1004, a green filter 1006, a red ball filter 1008, a purple square filter 1010, and a tree filter 1012 are the filters that are activated in response to the image 1002. \nFurthermore, the activations are distributed at the classifier neurons, such that a tree neuron 1014, a purple team neuron 1016, and a ball neuron 1018 are activated.  It should be noted that in FIG. 10, the synapses with thick lines represent the\nsynapses that are activated in response to the image 1002.  In the example of FIG. 10, the size of a circle relative to the filter/neuron is indicative of the level of activation, such that a bigger circle represents an activation that is greater than a\nsmaller circle.\n After determining the activations for the specific object, the bias may be adjusted as a function of the activations.  For example, a new image can be presented to the network and the pattern of activations throughout the network is observed. \nThe bias is then distributed to each synapse proportional to the activation of the neuron to which the synapse is connected.  In this example, the bias is adjusted from the bottom-up, such that some of the bias is distributed among the synapses at each\nlayer.  In this configuration, the bias may be adjusted from the bottom-up based on the following equations:\n .gamma..gamma..times..times..times..gamma..gamma..times..times..times.  ##EQU00001##\n In EQUATIONS 6 and 7, based on the example of FIG. 10, .gamma..sub.0 is the amount of bias adjustment present at the output neuron, .gamma..sub.ij is the adjustment value applied to the synapse from Layer J to Layer I, and .gamma..sub.hi is the\nadjustment value applied to the synapse from Layer H to Layer I, x.sub.i is the value output from a specific synapse of Layer I, and x.sub.h is the value output from a specific synapse of Layer H.\n In another configuration, the adjustment value is back propagated from the output based on the following equations: .gamma..sub.ij=(.gamma..sub.0w.sub.ij)x.sub.j (8) .gamma..sub.hi=.gamma..sub.ijw.sub.hi (9)\n In EQUATIONS 8 and 9, based on the example of FIG. 10, .gamma..sub.0 is the amount of bias adjustment present at the output neuron, .gamma..sub.ij is the adjustment value applied to the synapse from Layer J to Layer I, and .gamma..sub.hi is the\nadjustment value applied to the synapse from Layer H to Layer I, x.sub.j is the pattern of activations at Layer J, w.sub.hi is the weight of the synapse from Layer H to Layer I, and w.sub.ij is the weight of the synapse from Layer I to Layer J.\n Based on aspects of the present disclosure, multiple configurations are presented for adjusting the bias.  In one configuration, the bias may be adjusted as a constant.  The bias may be adjusted as a constant when the top-down signal is\ndetermined from a knowledge-graph type source.  For example, the bias may be adjusted as a constant when it is known that an image of a bird has an increased probability of including an image of a tree.  EQUATION 1 may be used for adjusting the bias as a\nconstant.\n In another configuration, the bias is adjusted as a function of the synaptic weight.  The bias may be adjusted as a function of the synaptic weight so that important weights for a given object are biased.  Additionally, or alternatively, the\nbias may be adjusted as a function of the synaptic weight so that the adjustment value is back propagated through the network.  The equation for adjusting the bias as a function of the synaptic weight is:\nactivation=f(.SIGMA..sub.iw.sub.ix.sub.i+.gamma.(w.sub.i)b.sub.i) (10)\n In EQUATION 10, w.sub.i is the weight, ( ) is the bias adjustment (e.g., change in bias), x.sub.i is the value output from a lower layer, and b.sub.i is the bias.\n In another configuration, the bias is adjusted as a function of the activations in response to a target class presentation.  This configuration may be used when the top-down signal is derived from an example presented to the network.  For\nexample, as shown in FIG. 10, an image 1002 is presented to the network 1000 and the bias is determined based on the distribution of activations in the network.  The equation for adjusting the bias as a function of the activations in response to a target\nclass presentation may be based on the following equation: activation=f(.SIGMA..sub.iw.sub.ix.sub.i+.gamma.(x.sub.i)b.sub.i) (11)\n In EQUATION 11, w.sub.i is the weight, ( ) is the bias adjustment (e.g., change in bias), x.sub.i is the value output from a lower layer, and b.sub.i is the bias.\n Furthermore, the bias adjustment may be applied additively or multiplicatively.  The application of the bias may depend on the activation function.\n The adjustment of the bias may be additively applied based on the following equation: activation=f(.SIGMA..sub.iw.sub.ix.sub.i+(.gamma.+b.sub.i)) (12)\n In EQUATION 12, w.sub.i is the weight, .gamma.  is the bias adjustment (e.g., change in bias), x.sub.i is the value output from a lower layer, and b.sub.i is the bias.\n In one configuration, the adjustment of the bias is multiplicatively applied based on EQUATION 1.  Multiplicatively applying the bias may be desirable because the bias is scaled from the original value.\n FIG. 11 illustrates a method 1100 of adjusting a bias for an activation function in a machine learning network, such as a neural classifier network.  At block 1102, a network determines whether an element has an increased probability of being\npresent in an input to the network.  At block 1104, the network adjusts a bias term of activation functions of neurons in the network to increase sensitivity to the element.  In one configuration, the bias is adjusted without adjusting weights of the\nnetwork.  Furthermore, at block 1106, the network adjusts an output of the network based on the biasing.\n FIG. 12 illustrates a method 1200 of adjusting a bias for an activation function in a machine learning network, such as a neural classifier network.  At block 1202, a network determines attributes associated with an input, such as an image.  As\nan example, the attributes may include a time of an image, location of an image, and/or specific objects that are present in the image.  Based on the determined attributes, at block 1204, the network determines whether an element has an increased\nprobability of being present in the input.\n If the element has an increased probability of being present in an input to the network, at block 1206, the network adjusts a bias term of activation functions of neurons in the network to increase sensitivity to the element.  Furthermore, at\nblock 1210, the network adjusts the network output based on the adjusted bias term.\n If the element does not have an increased probability of being present in an input to the network, at block 1208, the network adjusts a bias term of activation functions of neurons in the network to decrease sensitivity to the element. \nFurthermore, at block 1210, the network adjusts the network output based on the adjusted bias term.\n The various operations of methods described above may be performed by any suitable means capable of performing the corresponding functions.  The means may include various hardware and/or software component(s) and/or module(s), including, but not\nlimited to, a circuit, an application specific integrated circuit (ASIC), or processor.  Generally, where there are operations illustrated in the figures, those operations may have corresponding counterpart means-plus-function components with similar\nnumbering.\n As used herein, the term \"determining\" encompasses a wide variety of actions.  For example, \"determining\" may include calculating, computing, processing, deriving, investigating, looking up (e.g., looking up in a table, a database or another\ndata structure), ascertaining and the like.  Additionally, \"determining\" may include receiving (e.g., receiving information), accessing (e.g., accessing data in a memory) and the like.  Furthermore, \"determining\" may include resolving, selecting,\nchoosing, establishing and the like.\n As used herein, a phrase referring to \"at least one of\" a list of items refers to any combination of those items, including single members.  As an example, \"at least one of: a, b, or c\" is intended to cover: a, b, c, a-b, a-c, b-c, and a-b-c.\n The various illustrative logical blocks, modules and circuits described in connection with the present disclosure may be implemented or performed with a general purpose processor, a digital signal processor (DSP), an application specific\nintegrated circuit (ASIC), a field programmable gate array signal (FPGA) or other programmable logic device (PLD), discrete gate or transistor logic, discrete hardware components or any combination thereof designed to perform the functions described\nherein.  A general-purpose processor may be a microprocessor, but in the alternative, the processor may be any commercially available processor, controller, microcontroller or state machine.  A processor may also be implemented as a combination of\ncomputing devices, e.g., a combination of a DSP and a microprocessor, a plurality of microprocessors, one or more microprocessors in conjunction with a DSP core, or any other such configuration.\n The steps of a method or algorithm described in connection with the present disclosure may be embodied directly in hardware, in a software module executed by a processor, or in a combination of the two.  A software module may reside in any form\nof storage medium that is known in the art.  Some examples of storage media that may be used include random access memory (RAM), read only memory (ROM), flash memory, erasable programmable read-only memory (EPROM), electrically erasable programmable\nread-only memory (EEPROM), registers, a hard disk, a removable disk, a CD-ROM and so forth.  A software module may comprise a single instruction, or many instructions, and may be distributed over several different code segments, among different programs,\nand across multiple storage media.  A storage medium may be coupled to a processor such that the processor can read information from, and write information to, the storage medium.  In the alternative, the storage medium may be integral to the processor.\n The methods disclosed herein comprise one or more steps or actions for achieving the described method.  The method steps and/or actions may be interchanged with one another without departing from the scope of the claims.  In other words, unless\na specific order of steps or actions is specified, the order and/or use of specific steps and/or actions may be modified without departing from the scope of the claims.\n The functions described may be implemented in hardware, software, firmware, or any combination thereof.  If implemented in hardware, an example hardware configuration may comprise a processing system in a device.  The processing system may be\nimplemented with a bus architecture.  The bus may include any number of interconnecting buses and bridges depending on the specific application of the processing system and the overall design constraints.  The bus may link together various circuits\nincluding a processor, machine-readable media, and a bus interface.  The bus interface may be used to connect a network adapter, among other things, to the processing system via the bus.  The network adapter may be used to implement signal processing\nfunctions.  For certain aspects, a user interface (e.g., keypad, display, mouse, joystick, etc.) may also be connected to the bus.  The bus may also link various other circuits such as timing sources, peripherals, voltage regulators, power management\ncircuits, and the like, which are well known in the art, and therefore, will not be described any further.\n The processor may be responsible for managing the bus and general processing, including the execution of software stored on the machine-readable media.  The processor may be implemented with one or more general-purpose and/or special-purpose\nprocessors.  Examples include microprocessors, microcontrollers, DSP processors, and other circuitry that can execute software.  Software shall be construed broadly to mean instructions, data, or any combination thereof, whether referred to as software,\nfirmware, middleware, microcode, hardware description language, or otherwise.  Machine-readable media may include, by way of example, random access memory (RAM), flash memory, read only memory (ROM), programmable read-only memory (PROM), erasable\nprogrammable read-only memory (EPROM), electrically erasable programmable Read-only memory (EEPROM), registers, magnetic disks, optical disks, hard drives, or any other suitable storage medium, or any combination thereof.  The machine-readable media may\nbe embodied in a computer-program product.  The computer-program product may comprise packaging materials.\n In a hardware implementation, the machine-readable media may be part of the processing system separate from the processor.  However, as those skilled in the art will readily appreciate, the machine-readable media, or any portion thereof, may be\nexternal to the processing system.  By way of example, the machine-readable media may include a transmission line, a carrier wave modulated by data, and/or a computer product separate from the device, all which may be accessed by the processor through\nthe bus interface.  Alternatively, or in addition, the machine-readable media, or any portion thereof, may be integrated into the processor, such as the case may be with cache and/or general register files.  Although the various components discussed may\nbe described as having a specific location, such as a local component, they may also be configured in various ways, such as certain components being configured as part of a distributed computing system.\n The processing system may be configured as a general-purpose processing system with one or more microprocessors providing the processor functionality and external memory providing at least a portion of the machine-readable media, all linked\ntogether with other supporting circuitry through an external bus architecture.  Alternatively, the processing system may comprise one or more neuromorphic processors for implementing the neuron models and models of neural systems described herein.  As\nanother alternative, the processing system may be implemented with an application specific integrated circuit (ASIC) with the processor, the bus interface, the user interface, supporting circuitry, and at least a portion of the machine-readable media\nintegrated into a single chip, or with one or more field programmable gate arrays (FPGAs), programmable logic devices (PLDs), controllers, state machines, gated logic, discrete hardware components, or any other suitable circuitry, or any combination of\ncircuits that can perform the various functionality described throughout this disclosure.  Those skilled in the art will recognize how best to implement the described functionality for the processing system depending on the particular application and the\noverall design constraints imposed on the overall system.\n The machine-readable media may comprise a number of software modules.  The software modules include instructions that, when executed by the processor, cause the processing system to perform various functions.  The software modules may include a\ntransmission module and a receiving module.  Each software module may reside in a single storage device or be distributed across multiple storage devices.  By way of example, a software module may be loaded into RAM from a hard drive when a triggering\nevent occurs.  During execution of the software module, the processor may load some of the instructions into cache to increase access speed.  One or more cache lines may then be loaded into a general register file for execution by the processor.  When\nreferring to the functionality of a software module below, it will be understood that such functionality is implemented by the processor when executing instructions from that software module.  Furthermore, it should be appreciated that aspects of the\npresent disclosure result in improvements to the functioning of the processor, computer, machine, or other system implementing such aspects.\n If implemented in software, the functions may be stored or transmitted over as one or more instructions or code on a computer-readable medium.  Computer-readable media include both computer storage media and communication media including any\nmedium that facilitates transfer of a computer program from one place to another.  A storage medium may be any available medium that can be accessed by a computer.  By way of example, and not limitation, such computer-readable media can comprise RAM,\nROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium that can be used to carry or store desired program code in the form of instructions or data structures and that can be\naccessed by a computer.  Additionally, any connection is properly termed a computer-readable medium.  For example, if the software is transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair,\ndigital subscriber line (DSL), or wireless technologies such as infrared (IR), radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the\ndefinition of medium.  Disk and disc, as used herein, include compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk, and Blu-ray.RTM.  disc where disks usually reproduce data magnetically, while discs reproduce data\noptically with lasers.  Thus, in some aspects computer-readable media may comprise non-transitory computer-readable media (e.g., tangible media).  In addition, for other aspects computer-readable media may comprise transitory computer-readable media\n(e.g., a signal).  Combinations of the above should also be included within the scope of computer-readable media.\n Thus, certain aspects may comprise a computer program product for performing the operations presented herein.  For example, such a computer program product may comprise a computer-readable medium having instructions stored (and/or encoded)\nthereon, the instructions being executable by one or more processors to perform the operations described herein.  For certain aspects, the computer program product may include packaging material.\n Further, it should be appreciated that modules and/or other appropriate means for performing the methods and techniques described herein can be downloaded and/or otherwise obtained by a user terminal and/or base station as applicable.  For\nexample, such a device can be coupled to a server to facilitate the transfer of means for performing the methods described herein.  Alternatively, various methods described herein can be provided via storage means (e.g., RAM, ROM, a physical storage\nmedium such as a compact disc (CD) or floppy disk, etc.), such that a user terminal and/or base station can obtain the various methods upon coupling or providing the storage means to the device.  Moreover, any other suitable technique for providing the\nmethods and techniques described herein to a device can be utilized.\n It is to be understood that the claims are not limited to the precise configuration and components illustrated above.  Various modifications, changes and variations may be made in the arrangement, operation and details of the methods and\napparatus described above without departing from the scope of the claims.", "application_number": "14848288", "abstract": " A method of biasing a deep neural network includes determining whether an\n     element has an increased probability of being present in an input to the\n     network. The method also includes adjusting a bias of activation\n     functions of neurons in the network to increase sensitivity to the\n     element. In one configuration, the bias is adjusted without adjusting\n     weights of the network. The method further includes adjusting an output\n     of the network based on the biasing.\n", "citations": ["8239336", "8386401", "8854248", "20130036078", "20150019468"], "related": ["62154097"]}, {"id": "20160322049", "patent_code": "10354647", "patent_name": "Correcting voice recognition using selective re-speak", "year": "2019", "inventor_and_country_data": " Inventors: \nBakshi; Dhruv (Zurich, CH), Sabur; Zaheed (Adliswil, CH), Judd; Tilke Mary (Zurich, CH), Fey; Nicholas G. (Mountain View, CA)  ", "description": "BACKGROUND\n The Internet provides access to a wide variety of resources, such as image files, audio files, video files, and web pages.  A search system can identify resources in response to queries submitted by users and provide information about the\nresources in a manner that is useful to the users.  The users can navigate through, e.g., click on, search results to acquire information of interest.\nSUMMARY\n This specification relates to speech recognition, e.g., voice-to-text, in search systems.\n Implementations of the present disclosure are generally directed to correcting speech recognition using selective re-speak.  More particularly, implementations of the present disclosure are directed to correcting a portion of a textual search\nquery based on re-speaking (by a user) the portion of the textual search query.  In some examples, the portion of the textual search query is corrected to provide a corrected textual search query.\n In general, innovative aspects of the subject matter described in this specification can be embodied in methods that include actions of providing first text for display on a computing device of a user, the first text being provided from a first\nspeech recognition engine based on first speech received from the computing device, and being displayed as a search query, receiving a speech correction indication from the computing device, the speech correction indication indicating a portion of the\nfirst text that is to be corrected, receiving second speech from the computing device, receiving second text from a second speech recognition engine based on the second speech, the second speech recognition engine being different from the first speech\nrecognition engine, replacing the portion of the first text with the second text to provide a combined text, and providing the combined text for display on the computing device as a revised search query.  Other implementations of this aspect include\ncorresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices.\n These and other implementations can each optionally include one or more of the following features: the portion includes an entirety of the first text; the portion comprises less than an entirety of the first text; the second speech recognition\nengine includes the first speech recognition engine and at least one additional function; the at least one additional function includes selecting a potential text as the second text based on one or more entities associated with the first text; actions\nfurther include: receiving first search results based on the first text, and providing the first search results for display on the computing device; actions further include: receiving second search results based on the second text, and providing the\nsecond search results for display on the computing device in place of the first search results; and the speech correction indication includes user selection of at least one word of a plurality of words of the first text.\n Particular implementations of the subject matter described in this specification can be implemented so as to realize one or more of the following advantages.  In some examples, correction of a portion of the initial query is faster and more\nresource efficient.  For example, from the user perspective, it is faster/easier to re-speak the portion instead of the entirety of the initial query.  From a resource perspective, less bandwidth is required and less computer processing power and/or\nmemory are required to perform speech recognition on the portion, as opposed to the entirety of the initial query.  In some examples, user interaction with the computing device is simplified, e.g., the user spelling out a single word instead of an entire\nquery.  In some examples, a more complex speech recognition can be used to more accurately convert the second speech to text without consuming increased resources, e.g., because the second speech is shorter than the entirety of the initial query.\n The details of one or more implementations of the subject matter described in this specification are set forth in the accompanying drawings and the description below.  Other features, aspects, and advantages of the subject matter will become\napparent from the description, the drawings, and the claims. BRIEF DESCRIPTION OF THE DRAWINGS\n FIG. 1 depicts an example environment in which a search system provides search results.\n FIGS. 2A-2D depict an example use case in accordance with implementations of the present disclosure.\n FIG. 3 depicts an example speech recognition system in accordance with implementations of the present disclosure.\n FIG. 4 depicts an example process that can be executed in accordance with implementations of the present disclosure.\n Like reference numbers and designations in the various drawings indicate like elements.\nDETAILED DESCRIPTION\n Implementations of the present disclosure are generally directed to correcting speech recognition using selective re-speak.  More particularly, implementations of the present disclosure are directed to correcting a portion of a search query\nbased on a user re-speaking the portion.  In some implementations, first text is provided for display to a user, the first text being provided from a first speech recognition engine based on first speech of the user received from the computing device. \nIn some examples, the first text is a search query that is submitted to a search system.  In some implementations, the user can indicate a portion of the first text that is to be corrected, and can provide second speech, which is processed using a second\nspeech recognition engine to provide second text.  In some implementations, the portion of the first text is replaced with the second text to provide a combined text.  In some examples, the combined text is a revised search query that is submitted to the\nsearch system.\n FIG. 1 depicts an example environment 100 in which a search system provides search results based on user queries.  In some examples, the example environment 100 enables users to interact with one or more computer-implemented services.  Example\ncomputer-implemented services can include a search service, an electronic mail service, a chat service, a document sharing service, a calendar sharing service, a photo sharing service, a video sharing service, blogging service, a micro-blogging service,\na social networking service, a location (location-aware) service, a check-in service and a ratings and review service.  In the example of FIG. 1, a search system 120 is depicted, which provides a search service, as described in further detail herein.\n With continued reference to FIG. 1, the example environment 100 includes a network 102, e.g., a local area network (LAN), wide area network (WAN), the Internet, or a combination thereof, connects web sites 104, user devices 106, and the search\nsystem 120.  In some examples, the network 102 can be accessed over a wired and/or a wireless communications link.  For example, mobile computing devices, such as smartphones can utilize a cellular network to access the network 102.  The environment 100\nmay include millions of web sites 104 and user devices 106.\n In some examples, a web site 104 is provided as one or more resources 105 associated with a domain name and hosted by one or more servers.  An example web site is a collection of web pages formatted in an appropriate machine-readable language,\ne.g., hypertext markup language (HTML), that can contain text, images, multimedia content, and programming elements, e.g., scripts.  Each web site 104 is maintained by a publisher, e.g., an entity that manages and/or owns the web site.\n In some examples, a resource 105 is data provided over the network 102 and that is associated with a resource address, e.g., a uniform resource locator (URL).  In some examples, resources 105 that can be provided by a web site 104 include web\npages, word processing documents, and portable document format (PDF) documents, images, video, and feed sources, among other appropriate digital content.  The resources 105 can include content, e.g., words, phrases, images and sounds and may include\nembedded information, e.g., meta information and hyperlinks, and/or embedded instructions, e.g., scripts.\n In some examples, a user device 106 is an electronic device that is capable of requesting and receiving resources 105 over the network 102.  Example user devices 106 include personal computers, laptop computers, and mobile computing devices,\ne.g., smartphones and/or tablet computing devices, that can send and receive data over the network 102.  As used throughout this document, the term mobile computing device (\"mobile device\") refers to a user device that is configured to communicate over a\nmobile communications network.  A smartphone, e.g., a phone that is enabled to communicate over the Internet, is an example of a mobile device.  A user device 106 can execute a user application, e.g., a web browser, to facilitate the sending and\nreceiving of data over the network 102.\n In some examples, to facilitate searching of resources 105, the search system 120 identifies the resources 105 by crawling and indexing the resources 105 provided on web sites 104.  Data about the resources 105 can be indexed based on the\nresource to which the data corresponds.  The indexed and, optionally, cached copies of the resources 105 are stored in a search index 122.\n The user devices 106 submit search queries 109 to the search system 120.  In some examples, a user device 106 can include one or more input modalities.  Example modalities can include a keyboard, a touchscreen and/or a microphone.  For example,\na user can use a keyboard and/or touchscreen to type in a search query.  As another example, a user can speak a search query, the user speech being captured through a microphone, and being processed through speech recognition to provide the search query.\n In response to receiving a search query 109, the search system 120 accesses the search index 122 to identify resources 105 that are relevant to, e.g., have at least a minimum specified relevance score for, the search query 109.  The search\nsystem 120 identifies the resources 105, generates a search results display 111 that includes search results 112 identifying resources 105, and returns the search results display 111 to the user devices 106.  In an example context, a search results\ndisplay can include one or more web pages, e.g., one or more search results pages.  In some examples, a web page can be provided based on a web document that can be written in any appropriate machine-readable language.  It is contemplated, however, that\nimplementations of the present disclosure can include other appropriate display types.  For example, the search results can be provided in a display generated by an application that is executed on a computing device, and/or a display generated by an\noperating system, e.g., mobile operating system.  In some examples, search results can be provided based on any appropriate form, e.g., Javascript-html, plaintext.\n A search result 112 is data generated by the search system 120 that identifies a resource 105 that is responsive to a particular search query, and includes a link to the resource 105.  An example search result 112 can include a web page title, a\nsnippet of text or a portion of an image extracted from the web page, and the URL of the web page.  In some examples, data provided in the search results 112 can be retrieved from a resource data store.  For example, the search system 120 can provide the\nsearch results display 111, which displays the search results 112.  In some examples, the search results display 111 can be populated with information, e.g., a web page title, a snippet of text or a portion of an image extracted from the web page, that\nis provided from the resource data store.\n In some examples, data for the search queries 109 submitted during user sessions are stored in a data store, such as the historical data store 124.  For example, the search system 120 can store received search queries in the historical data\nstore 124.\n In some examples, selection data specifying actions taken in response to search results 112 provided in response to each search query 109 are also stored in the historical data store 124, for example, by the search system 120.  These actions can\ninclude whether a search result 112 was selected, e.g., clicked or hovered over with a pointer.  The selection data can also include, for each selection of a search result 112, data identifying the search query 109 for which the search result 112 was\nprovided.\n In some implementations, a user can submit a search query 109 based on speech.  For example, the user can speak into a microphone of a user device 106, and the user's speech can be captured as speech data (also referred to as first speech) in a\ndata file.  In some examples, the speech data is provided as a search query 109 submitted to the search system 120 through the network 102.  In some implementations, the search system 120 can provide the speech data to a speech recognition system 130. \nIn some examples, the speech recognition system 130 can process the speech data to provide text.  For example, the speech recognition system 130 can process the speech data using a voice-to-text engine (also referred to as the first speech recognition\nengine) to provide the text.  In some examples, the speech recognition system 130 provides the text to the search system 120, which processes the text as a search query to provide search results 112.  In some examples, the search query can be provided\nfor display to the user, e.g., with the search results.  In this manner, the user can see how the speech was recognized, and the search query that the search results are based on.\n Although the search system 120 and the speech recognition system 130 are depicted as separate systems in the example of FIG. 1, it is appreciated that the search system 120 and the speech recognition system 130 can be included in the same\nsystem, e.g., the search system 120 can include the speech recognition system 130 therein.\n In accordance with implementations of the present disclosure, the user can correct a portion, e.g., one or more words, of the search query.  In some examples, the user can determine that a portion of the search query is incorrect, e.g., the\nspeech recognition for the portion did not accurately recognize the user's speech, and can indicate that the portion is to be corrected.  For example, the user can select the portion of the search query displayed to the user using the user device 106. \nIn some implementations, the user again speaks into the microphone of the user device 106, and the user's speech can be captured as portion speech data (also referred to as second speech) in a data file.  In some examples, the portion speech data is\nprovided as a re-speak of the portion of the search query and is submitted to the search system 120 through the network 102.  In some implementations, the search system 120 can provide the portion speech data to the speech recognition system 130, and can\ninclude an indication that the portion speech data corresponds to a re-speak of speech input.  In some examples, the speech recognition system 130 can process the portion speech data to provide revised text.  For example, the speech recognition system\n130 can process the portion speech data using a voice-to-text engine (also referred to as a second speech recognition engine).\n In some implementations, the first speech recognition engine used to process the speech data is different from the second speech recognition engine used to process the portion speech data.  In some examples, the first speech recognition engine\ncan be used for providing relatively quick processing of the speech data with a less stringent threshold for accuracy, relative to the second speech recognition engine.  For example, the first speech recognition engine can use less resources, e.g.,\nprocessors, memory, and can provide results more quickly than the second speech recognition engine.  In some examples, the second speech recognition engine can be used for providing more accurate processing of the portion speech data, e.g., a more\nstringent threshold for accuracy, relative to the first speech recognition engine.  For example, the second speech recognition engine can use more resources, e.g., processors, memory, and can provide results that are more accurate than the first speech\nrecognition engine.  In some examples, although the second speech recognition engine is more complex than the first speech recognition engine, and might consume more resources, the speed, at which results are provided can be similar.  For example, and as\ndescribed in further detail herein, the first speech recognition engine can be processing more speech than the second speech recognition engine, e.g., the second speech recognition engine is only processing a portion of the originally submitted speech.\n In some implementations, the speech recognition system 130 provides the revised text to the search system 120, which provides a revised search query based on the search query and the revised text.  For example, the search system 120 replaces the\nportion of the search query with the revised text to provide a revised search query.  The search system 120 processes the revised search query to provide revised search results 112.  In some examples, the revised search query can be provided for display\nto the user, e.g., with the search results.  In this manner, the user can see how the speech was revised, and the search query that the search results are based on.\n FIGS. 2A-2D depict an example use case in accordance with implementations of the present disclosure.  With particular reference to FIG. 2A, a user 200 uses a computing device 202 to conduct searching.  More particularly, the user 200 uses the\ncomputing device 202 to submit search queries to a search system, e.g., the search system 120 of FIG. 1, which provides search results for display to the user on the computing device 202.  In the depicted example, the computing device 202 is provided as\na mobile computing device, e.g., a smartphone, a tablet.  It is appreciated, however, that implementations of the present disclosure can be realized using any appropriate type of computing device, e.g., desktop computer, laptop computer, wearable\ncomputing device, such as a smartwatch.\n In the depicted example, the computing device 202 displays a search interface 204 that the user 200 can use to submit search queries and receive search results.  The example search interface 204 includes a search box 206, a search button 208, a\nsearch results area 210, and a microphone button 212.  In some examples, search queries submitted to the search system are displayed in the search box 206, and the resulting search results a displayed in the search results area 210.  In some examples,\nthe user 200 can select the search button 208 to initiate submission of the search query to the search system.  In some examples, the search query is automatically submitted to the search system without requiring the user to select the search button 208.\n In accordance with implementations of the present disclosure, the user 200 provides first speech 220 as input to the search interface 204.  In the depicted example, the first speech 220 includes the query [show me pictures of Maradona].  In some\nexamples, the computing device 202 records the first speech 220 using a microphone, and generates one or more data files, e.g., .wav files, .mp3 files, that store the first speech 220 as audio data.  In some implementations, the computing device 202\nprovides the first speech 220 to the search system.  In some examples, the first speech 220 is automatically provided to the search system, e.g., the user is not required to select the search button 208.  In some examples, the first speech 220 is\ntransmitted to the search system, after the user has spoken the entirety of the first speech 220.  For example, the search system 120 receives the entirety of the first speech 220 in one request.  In some examples, portions of the first speech 220 are\ntransmitted to the search system as they are spoken.  For example, the search systems receives portions, e.g., words, of the first speech 220 as each portion is spoken.\n In some implementations, and as described in further detail herein, the first speech 220 is processed by a speech recognition system, e.g., the speech recognition system 130 of FIG. 1, to provide first text, e.g., a textual search query.  For\nexample, the search system can provide the audio data to the speech recognition system for processing.  In some examples, the speech recognition system processes the first speech 220 using a first speech recognition engine to provide the first text.\n In some implementations, the first text is provided for display on the computing device 202.  In some examples, the search system receives the first text from the speech recognition system and transmits the first text to the computing device\n202.  In some examples, the first text is displayed in the search box 206 as a search query 222.  In some examples, the search system processes the first text as a search query to provide search results, at least some of which are sent to the computing\ndevice 202 and are displayed as search results 224 in the search results area 210.  In some examples, the search query 222 is displayed to the user before the search results 224 are received and displayed by the computing device 202.  For example, the\nfirst text can be received and displayed as the search query 222 in the search box 206, and the search results 224 can be subsequently received and displayed, e.g., there can be a time lag between display of the search query 222, and display of the\nsearch results 224.\n In the depicted example, the search query 222 is provided as [show me pictures of Madonna].  Accordingly, the word [Maradona] was incorrectly recognized as [Madonna] by the speech recognition system.  Consequently, the search results 224 include\nimages of Madonna, the singer-songwriter, actress, and producer.  That is, the search query 222, which the search results 224 are based on, is incorrect, as it should be provided as [show me pictures of Maradona].\n In accordance with implementations of the present disclosure, the user 200 can correct a portion of the search query 222 to provide a revised search query, which can be submitted to the search system.  In some implementations, the user 200 can\nprovide a speech correction indication, which indicates the portion of the search query 222, e.g., the first text, that is to be corrected.  For example, the user can select one or more words of the search query 222, which are to be corrected.  In some\nexamples, the user 200 can tap on the portion, e.g., the computing device 202 including a touchscreen device.\n FIG. 2B depicts a portion 230 of the search query 222 that is to be corrected.  In the depicted example, the user 200 selected the word [Madonna] to be corrected.\n FIG. 2C depicts the user 200 re-speaking the portion 230.  In the depicted example, the user 200 provides second speech 232 as input to the search interface 204.  In the depicted example, the second speech 232 includes [Maradona].  In some\nexamples, the user 200 can spell out the portion 230 that is to be corrected.  In the examples of FIGS. 2A-2C, the user 200 can spell out the second text 232, e.g., \"M-A-R-A-D-O-N-A.\" In some examples, the computing device 202 records the second speech\n232 using the microphone, and generates one or more data files, e.g., .wav files, .mp3 files, that store the second speech 232 as audio data.  In some implementations, the computing device 202 provides the second speech 232 to the search system.  In some\nexamples, the second speech 232 is automatically provided to the search system, e.g., the user is not required to select the search button 208.\n In some implementations, and as described in further detail herein, the second speech 232 and an indication that the second speech 232 corresponds to a portion that is to be corrected (also referred to as a correction indication) are provided to\nthe speech recognition system.  In some examples, the second speech 232 is processed by the speech recognition system to provide second text.  In some examples, and in response to also receiving the correction indication, the speech recognition system\nprocesses the second speech 232 using a second speech recognition engine to provide the second text.  In some examples, and as described in further detail herein, the second speech recognition engine is different from the first speech recognition engine.\n In accordance with implementations of the present disclosure, a combined text is provided based on the first text and the second text.  In some implementations, the portion of the first text, e.g., [Madonna] in the depicted example, is deleted\nfrom the first text and is replaced by the second text, e.g., [Maradona], to provide the combined text.  In some implementations, the search system receives the second text from the speech recognition system and provides the combined text based on the\nfirst text and the second text.\n Referring now to FIG. 2D, and in some implementations, the combined text is provided for display on the computing device 202.  In some examples, the search system transmits the combined text to the computing device 202.  In some examples, the\ncombined text is displayed in the search box 206 as a revised search query 222'.  In some examples, the search system processes the combined text as a search query to provide search results, at least some of which are sent to the computing device 202 and\nare displayed as search results 240 in the search results area 210.  In some examples, the revised search query 222' is displayed to the user before the search results 240 are received and displayed by the computing device 202.  For example, the combined\ntext can be received and displayed as the revised search query 222' in the search box 206, and the search results 240 can be subsequently received and displayed, e.g., there can be a time lag between display of the revised search query 222', and display\nof the search results 240.\n In some implementations, and as depicted in the example of FIGS. 2A-2D, the user selects the portion of text that is to be corrected, and speaks the correction, e.g., selects [Madonna] and speaks [Maradona], e.g., as the second speech.  In some\nexamples, the microphone of the computing device is automatically activated in response to user selection of the portion of text that is to be corrected.  In some examples, the user selects the portion of text that is to be corrected, and the user\nactivates the microphone, e.g., selects the microphone button 212.\n In some implementations, the instead of re-speaking the portion that is to be corrected, the user speaks a phrase that provides context to the portion that is to be corrected.  For example, and continuing with the example of FIGS. 2A-2D, instead\nof speaking [Maradona] or [M-A-R-A-D-O-N-A], as described above, the user can speak [I meant the soccer player], e.g., as the second speech, which provides context to the selected portion that is to be corrected.  In some examples, the context can be\nused to discern between potential corrections to the portion, as described in further detail herein.\n In some implementations, a portion that is to be corrected is not explicitly selected by the user.  In the example of FIGS. 2A-2D, the user selected the word [Madonna], e.g., by tapping [Madonna] on the touchscreen of the computing device.  In\nsome examples, and instead of selecting a portion, the user can provide second speech to provide context for the correction, which second speech is processed to determine the portion that is to be corrected, and to provide the second text used to correct\nthe portion.  For example, and continuing with the example of FIGS. 2A-2D, the second speech can include [no, I meant the soccer player], [I meant Maradona], [change Madonna to Maradona], or [delete Madonna].  Accordingly, the second speech provides\ncontext to select the portion that is to be corrected, and can be used to discern between potential corrections to the portion, as described in further detail herein.  In some examples, the user activates the microphone, e.g., selects the microphone\nbutton 212, before providing the second speech.\n In some implementations, the microphone is automatically activated in the event that a portion is to be corrected.  For example, and as discussed above, the microphone can be automatically activated in response to user selection of a portion\nthat is to be corrected.  In some examples, the microphone can be automatically activated after the search query is displayed to the user.  In this manner, the user can provide second speech without requiring the user to first activate the microphone.\n FIG. 3 depicts an example speech recognition system 300 in accordance with implementations of the present disclosure.  The example speech recognition system 300 includes a first speech recognition engine 302, and a second speech recognition\nengine 304.  In some examples, the speech recognition system 300 receives input data 306, processes the input data 306 using the first speech recognition engine 302 or the second speech recognition engine 304, and provides output data 308.  In some\nexamples, the input data 306 is provided to the speech recognition system 300 from a search system, and the speech recognition system 300 provides the output data to the search system.\n In some implementations, the input data 306 includes audio data of first speech (speech data) provided by a user to the search system.  With reference to the example of FIGS. 2A-2D, the input data 306 can include an audio file of the first\nspeech [show me pictures of Maradona].  In some examples, the first speech recognition engine 302 processes the input data 306 to provide the output data 308.  In some implementations, the output data 308 is first text based on the audio data.  With\nreference to the example of FIGS. 2A-2D, the output data 308 can include a text file including first text, e.g., [show me pictures of Madonna].\n In some examples, the first text can be selected by the first speech recognition engine based on a set of potential texts.  In some examples, the first text is selected from the set of potential texts based on respective confidence scores\nassociated with the potential texts.  For example, and using the example of FIGS. 2A-2D, the first speech can be processed to provide [show me pictures of Madonna] and [show me pictures of Maradona].  In this example, [show me pictures of Madonna] is\nassociated with a first confidence score, e.g., 95%, and [show me pictures of Maradona] is associated with a second confidence score, e.g., 92%.  It can be determined that the first confidence score exceeds the second confidence score.  Consequently,\n[show me pictures of Madonna] is selected as the first text.  In other words, the potential text in the set of potential texts having the highest confidence score can be selected as the first text.\n In some implementations, the input data 306 includes audio data of second speech (portion speech data) provided by the user to the search system, and a correction indication.  With reference to the example of FIGS. 2A-2D, the input data 306 can\ninclude an audio file of the second speech [Maradona], and a correction indication.  In some examples, the second speech recognition engine 304 processes the input data 306 to provide the output data 308.\n For example, in response to the input data 306 including the correction indication, the second speech recognition engine 304 is used to process the audio data.  In some implementations, the output data 308 is second text based on the audio data. With reference to the example of FIGS. 2A-2D, the output data 308 can include a text file including second text, e.g., [Maradona].\n In some implementations, the first speech recognition engine 302 is different from the second speech recognition engine 304.  In some examples, the first speech recognition engine 302 can be used for providing relatively quick processing of the\nspeech data with a less stringent threshold for accuracy, relative to the second speech recognition engine 304.  For example, the first speech recognition engine 302 can implement a less complex, less accurate speech recognition algorithm relative to the\nsecond speech recognition engine 304.  In this manner, the first speech recognition engine 302 can provide relatively quick results and can use less resources, e.g., processors, memory, than the second speech recognition engine 304.  In some examples,\nthe second speech recognition engine 304 can be used for providing more accurate processing of the portion speech data, e.g., a more stringent threshold for accuracy, relative to the first speech recognition engine 302.  For example, the second speech\nrecognition engine 304 can implement a more complex, more accurate speech recognition algorithm relative to the first speech recognition engine 302.  In this manner, the second speech recognition engine 304 would provide relatively slower results and can\nuse more resources, e.g., processors, memory, than the first speech recognition engine 304, if processing the same audio data.\n In some implementations, although the second speech recognition engine 302 is more complex than the first speech recognition engine 304, the speed at which results are provided and the amount of resources used to provide the results can be\nsimilar.  For example, and as described in further detail herein, the first speech recognition engine 302 can be processing more audio data than the second speech recognition engine 304, e.g., the second speech recognition engine 304 is only processing a\nportion of the originally submitted speech.  With reference to the example of FIGS. 2A-2D, the first speech recognition engine 302 processes the text [show me pictures of Maradona], while the second speech recognition engine 304 processes only the text\n[Maradona].\n As described above, the first speech recognition engine is different from the second speech recognition engine.  In some examples, the second speech recognition is different from in that the second speech recognition includes the first speech\nrecognition engine, as well as additional functionality and/or different parameters for processing the second speech.  That is, and in some examples, the second speech recognition engine is the first speech recognition with additional functionality\nand/or different parameters for processing the second speech.\n In some implementations, the second speech is processed to provide a set of potential texts, from which the second text can be determined.  In some examples, a text that is included in the first text is excluded from selection from the set of\npotential texts that is provided based on the second speech.  For example, the portion of text that is to be corrected can be excluded from selection from the set of potential texts.  Continuing with the example of FIGS. 2A-2D, the second speech can\ninclude [Maradona], which can be processed to provide a set of potential texts that includes [Madonna] and [Maradona], for example.  Because [Madonna] is already included in the first text, e.g., and was selected for correction, [Madonna] is excluded\nfrom selection for the second text.  Consequently, the potential text [Maradona] is selected as the potential text.\n In some implementations, and in response to the second speech, the potential texts used to determine the first text can be processed to have respective entities associated with each.  In some examples, the second speech can be processed and one\nor more entities can be associated therewith.  In some implementations, entities associated with the second speech can be compared to entities associated with each of the potential texts in the set of potential texts.  In some examples, the potential\ntext having at least one entity that matches an entity associated with the second speech is selected.\n By way example, and using the example of FIGS. 2A-2D, the first speech can be processed to provide [show me pictures of Madonna] and [show me pictures of Maradona] as potential texts in a set of potential texts.  In some examples, [show me\npictures of Madonna] can be associated with the entities [singer], [actress], [producer], and [musician], among others, and [show me pictures of Maradona] can be associated with the entities [athlete], [soccer player], and [footballer], among others.  In\nsome examples, the second speech is provided as [I meant the soccer player], and can be associated with the entities [sport], [soccer], and [soccer player].  It can be determined that the potential text and the second speech have the entity [soccer\nplayer] in common.  Consequently, the potential text [show me pictures of Maradona] can be selected as the corrected text, e.g., combined text.\n In some implementations, a plurality of entities and information associated therewith can be stored as structured data in a knowledge graph.  In some examples, a knowledge graph includes a plurality of nodes and edges between nodes.  In some\nexamples, a node represents an entity and an edge represents a relationship between entities.  In some examples, the knowledge graph can be provided based on an example schema that structures data based on domains, types and properties.  In some\nexamples, a domain includes one or more types that share a namespace.  In some examples, a namespace is provided as a directory of uniquely named objects, where each object in the namespace has a unique name, e.g., identifier.  In some examples, a type\ndenotes an \"is a\" relationship about a topic, and is used to hold a collection of properties.  In some examples, a topic represents an entity, such as a person, place or thing.  In some examples, each topic can have one or more types associated\ntherewith.  In some examples, a property is associated with a topic and defines a \"has a\" relationship between the topic and a value of the property.  In some examples, the value of the property can include another topic.\n FIG. 4 depicts an example process 400 that can be executed in accordance with implementations of the present disclosure.  The example process 400 can be implemented, for example, by the example environment 100 of FIG. 1, e.g., the search system\n120 and/or the speech recognition system 130.  In some examples, the example process 400 can be provided by one or more computer-executable programs executed using one or more computing devices.\n First speech data is received (402).  For example, a search system, e.g., the search system 120 of FIG. 1, receives first speech data from a user device.  First text based on the first speech data is received (404).  For example, the search\nsystem receives the first text from a speech recognition system, e.g., the speech recognition system 130 of FIG. 1.  In some examples, the search system provides the first speech data to the speech recognition system.\n The first text is provided for display (406).  For example, the search system transmits the first text for display to the user device, e.g., displaying the first text as a search query in a search box of a search interface.  It is determined\nwhether a correction to the first text has been indicated (408).  For example, the search system can receive a correction indication from the user device.  In some examples, the correction indication can be provided in response to a user of the user\ndevice selecting a portion of the first text.  In some examples, the correction indication can be provided in response to the user providing subsequent speech input.  If a correction indication is not received, search results are provided for display\n(410).  For example, the search system can determine search results that are responsive to the first text, as a search query, and can provide the search results for display.\n If a correction indication is received, second speech data is received (412).  For example, the search system receives second speech data from the user device.  Second text based on the second speech data is received (414).  For example, the\nsearch system receives the second text from a speech recognition system.  In some examples, the search system provides the second speech data to the speech recognition system.  Combined text is provided for display (416).  For example, the search system\ntransmits the combined text for display to the user device, e.g., displaying the combined text as a revised search query in the search box of the search interface.  In some examples, the search system provides the combined text based on the first text\nand the second text.  For example, the portion of the first text that is to be corrected can be replaced by the second text.  In some examples, the portion of the first text is the entirety of the first text.  In some examples, the portion of the first\ntext is less than the entirety of the first text.  Search results are provided for display (410).  For example, the search system can determine search results that are responsive to the combined text, as a revised search query, and can provide the search\nresults for display.\n The example process 400 of FIG. 4 includes providing search results for display after the first text has been provided for display, e.g., as a search query, or after the combined text has been provided for display, e.g., as a revised search\nquery.  It is contemplated, however, that search results can be displayed relatively concurrent with display of either the first text or the combined text.  For example, in some implementations, the first text and the search results based on the first\ntext can be displayed before it is determined whether correction is to be made to the first text.\n Implementations of the subject matter and the operations described in this specification can be realized in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification\nand their structural equivalents, or in combinations of one or more of them.  Implementations of the subject matter described in this specification can be realized using one or more computer programs, i.e., one or more modules of computer program\ninstructions, encoded on computer storage medium for execution by, or to control the operation of, data processing apparatus.  Alternatively or in addition, the program instructions can be encoded on an artificially-generated propagated signal, e.g., a\nmachine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.  A computer storage medium can be, or be included in, a\ncomputer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them.  Moreover, while a computer storage medium is not a propagated signal, a computer storage\nmedium can be a source or destination of computer program instructions encoded in an artificially-generated propagated signal.  The computer storage medium can also be, or be included in, one or more separate physical components or media (e.g., multiple\nCDs, disks, or other storage devices).\n The operations described in this specification can be implemented as operations performed by a data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.\n The term \"data processing apparatus\" encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or multiple ones, or combinations, of the\nforegoing The apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).  The apparatus can also include, in addition to hardware, code that creates an\nexecution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of\none or more of them.  The apparatus and execution environment can realize various different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.\n A computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be\ndeployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment.  A computer program may, but need not, correspond to a file in a file system.  A program\ncan be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one\nor more modules, sub-programs, or portions of code).  A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication\nnetwork.\n The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output.  The processes and\nlogic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).\n Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.  Generally, a processor will receive\ninstructions and data from a read-only memory or a random access memory or both.  Elements of a computer can include a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and data. \nGenerally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.  However, a computer need\nnot have such devices.  Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable\nstorage device (e.g., a universal serial bus (USB) flash drive), to name just a few.  Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example\nsemiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.  The processor and the memory can be supplemented by, or\nincorporated in, special purpose logic circuitry.\n To provide for interaction with a user, implementations of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for\ndisplaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.  Other kinds of devices can be used to provide for interaction with a user as well; for example,\nfeedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.  In addition, a computer\ncan interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.\n Implementations of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or\nthat includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or\nmore such back-end, middleware, or front-end components.  The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network.\n Examples of communication networks include a local area network (\"LAN\") and a wide area network (\"WAN\"), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).\n The computing system can include clients and servers.  A client and server are generally remote from each other and typically interact through a communication network.  The relationship of client and server arises by virtue of computer programs\nrunning on the respective computers and having a client-server relationship to each other.  In some implementations, a server transmits data (e.g., an HTML page) to a client device (e.g., for purposes of displaying data to and receiving user input from a\nuser interacting with the client device).  Data generated at the client device (e.g., a result of the user interaction) can be received from the client device at the server.\n While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any implementation of the present disclosure or of what may be claimed, but rather as descriptions of features\nspecific to example implementations.  Certain features that are described in this specification in the context of separate implementations can also be implemented in combination in a single implementation.  Conversely, various features that are described\nin the context of a single implementation can also be implemented in multiple implementations separately or in any suitable sub-combination.  Moreover, although features may be described above as acting in certain combinations and even initially claimed\nas such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a sub-combination or variation of a sub-combination.\n Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations\nbe performed, to achieve desirable results.  In certain circumstances, multitasking and parallel processing may be advantageous.  Moreover, the separation of various system components in the implementations described above should not be understood as\nrequiring such separation in all implementations, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.\n Thus, particular implementations of the subject matter have been described.  Other implementations are within the scope of the following claims.  In some cases, the actions recited in the claims can be performed in a different order and still\nachieve desirable results.  In addition, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results.  In certain implementations, multitasking and parallel\nprocessing may be advantageous.", "application_number": "15140891", "abstract": " Implementations of the present disclosure include actions of providing\n     first text for display on a computing device of a user, the first text\n     being provided from a first speech recognition engine based on first\n     speech received from the computing device, and being displayed as a\n     search query, receiving a speech correction indication from the computing\n     device, the speech correction indication indicating a portion of the\n     first text that is to be corrected, receiving second speech from the\n     computing device, receiving second text from a second speech recognition\n     engine based on the second speech, the second speech recognition engine\n     being different from the first speech recognition engine, replacing the\n     portion of the first text with the second text to provide a combined\n     text, and providing the combined text for display on the computing device\n     as a revised search query.\n", "citations": ["4328562", "4866778", "5027406", "5500920", "5510981", "5712957", "5737724", "5794189", "5799273", "5799279", "5829000", "5855000", "5857099", "5864805", "5909667", "5960394", "5970451", "6064959", "6067521", "6070140", "6088671", "6138099", "6182028", "6192343", "6195635", "6195637", "6219640", "6286064", "6314397", "6327566", "6338035", "6374214", "6374220", "6374221", "6397180", "6405170", "6490561", "6513005", "6581033", "6587824", "6606598", "6735565", "6912498", "6922669", "7085716", "7149970", "7200555", "7216077", "7310600", "7366668", "7383185", "7440895", "7444286", "7542902", "7565282", "7634408", "7689420", "7756710", "7809574", "7840407", "7890326", "7930168", "7949524", "7949533", "7974844", "7983912", "8036464", "8155959", "8209175", "8355914", "8438142", "8478590", "8494852", "8504372", "8620659", "8775175", "8831946", "8972240", "9087517", "9123339", "9190055", "9263048", "9418152", "9466287", "9542932", "9711145", "9875738", "20010041978", "20020052740", "20020052742", "20020091520", "20020111990", "20020138265", "20030104839", "20030182113", "20030187642", "20030200093", "20030216912", "20030229497", "20040006481", "20040030556", "20040153321", "20040249637", "20050005240", "20050033574", "20050043949", "20050055209", "20050086059", "20050091054", "20050102140", "20050203751", "20050256710", "20060036438", "20060041427", "20060085186", "20060095268", "20060149551", "20060215821", "20060287868", "20060293889", "20060293890", "20070001012", "20070005372", "20070033037", "20070073540", "20070094022", "20070100635", "20070106492", "20070136060", "20070150275", "20070233482", "20080052073", "20080059167", "20080059186", "20080077859", "20080126091", "20080162137", "20080235017", "20080243507", "20080300874", "20080319744", "20090012792", "20090067719", "20090271189", "20090306980", "20090306995", "20090313016", "20090326938", "20090327279", "20100004930", "20100076765", "20100179801", "20100179812", "20100287486", "20110054900", "20110066970", "20110112837", "20110112921", "20110125499", "20110137653", "20110145224", "20110161079", "20110161347", "20110166851", "20110202386", "20120016671", "20120022868", "20120059652", "20120059653", "20120203776", "20120215539", "20120232904", "20130030805", "20130262117", "20130304467", "20140058732", "20140108453", "20150058018", "20150294668", "20160063994", "20160063998", "20160092447", "20170270926"], "related": ["62153839"]}, {"id": "20160350414", "patent_code": "10318567", "patent_name": "Providing knowledge panels with search results", "year": "2019", "inventor_and_country_data": " Inventors: \nHenry; Jeromy William (Aptos, CA)  ", "description": "BACKGROUND\n This specification relates to presenting data with search results.\n The Internet provides access to a wide variety of resources, such as image files, audio files, video files, and web pages.  A search system can identify resources in response to queries submitted by users and provide information about the\nresources in a manner that is useful to the users.  The users then navigate through (e.g., click on) the search results to acquire information of interest to the users.\n Users of search systems are often searching for information regarding a specific entity.  For example, users may want to learn about a singer that they just heard on the radio.  Conventionally, the user would initiate a search for the singer and\nselect from a list of search results determined to be relevant to the singer.\nSUMMARY\n In general, one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of obtaining search results that are responsive to a received query; identifying a factual entity\nreferenced by the query; identifying content for display in a knowledge panel for the factual entity, the content including at least one content item obtained from a first resource and at least one second content item obtained from a second resource\ndifferent than the first resource; and providing data that causes the identified search results and the knowledge panel to be presented on a search results page, the knowledge panel presenting the identified content in a knowledge panel area along side\nat least a portion of the search results.\n Other embodiments of this aspect include corresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices.\n These and other embodiments can each optionally include one or more of the following features.  The content can include at least two of an image for the factual entity, a title for the factual entity, or a fact regarding the factual entity.  The\nknowledge panel area can consume a larger area than each of the search results.\n Identifying content for display in the knowledge panel for the factual entity can include selecting the content for the knowledge panel based on a ranking of content for the factual entity, the ranking being based on user search events related\nto the factual entity.\n The knowledge panel can include an interactive user interface object.  Aspects can further include receiving information specifying an interaction with the interactive user interface object and providing data that causes additional content to be\npresented within the knowledge panel.  The additional content can be selected based on the interaction.\n Aspects can further include determining that the received query is associated with multiple distinct meanings and identifying content related to each of the distinct meanings.  The knowledge panel can include content for two or more of the\ndistinct meanings.\n Aspects can further include identifying a type of entity for the factual entity; identifying a knowledge panel template based on the type of entity; and generating the knowledge panel by populating the knowledge panel template with the\nidentified content items.\n Identifying content for display in the knowledge panel for the factual entity can include identifying types of content items specified by the knowledge panel template.\n The factual entity can include a person.  The knowledge panel can include a placeholder for each of an image of the person, a description of the person, and at least one fact about the person.\n The factual entity can include a place.  The knowledge panel can include a placeholder for each of an image depicting a map associated with the place, a description of the place, and at least one fact about the place.\n Particular embodiments of the subject matter described in this specification can be implemented so as to realize one or more of the following advantages.  A knowledge panel is presented with search results (or other pages) to provide data\nregarding particular entities that have been identified as relevant to a search query.  Presenting the knowledge panel with the search results reduces the number of web pages users have to visit in order to obtain factual information for which the users\nare searching, thereby reducing the time required for the users to find information that satisfies their informational needs.  Knowledge panel templates developed for particular types of entities enable content relevant to the entities to be displayed to\nusers.\n Knowledge panels can improve users' search experiences, in particular for queries directed to learning, browsing, or discovery.  For example, the knowledge panel supplies users with basic factual information or a summary of information about a\nparticular entity referenced in a search query.  Knowledge panels can assist users in navigating to related content in a seamless and natural way.  Knowledge panels can supply new content that may not otherwise be encountered by a user without selecting\nseveral search results.  Knowledge panels can also help users obtain information faster than they would if the users were required to click through multiple search results to obtain the information.\n The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below.  Other features, aspects, and advantages of the subject matter will become\napparent from the description, the drawings, and the claims. BRIEF DESCRIPTION OF THE DRAWINGS\n FIG. 1 is a block diagram of an example environment in which a search system provides search services.\n FIG. 2 is a block diagram illustrating a process for supplying search results and a knowledge panel.\n FIG. 3 is a screen shot of an example search interface in which a knowledge panel is presented with search results.\n FIG. 4 is a screen shot of another example search interface in which a knowledge panel is presented with search results.\n FIG. 5A is a screen shot of another example search interface in which a knowledge panel is presented with search results.\n FIG. 5B is a screen shot of another example search interface in which a knowledge panel is presented with search results.\n FIG. 6 is a screen shot of another example search interface in which a knowledge panel is presented with search results.\n FIG. 7 is a screen shot of another example search interface in which a knowledge panel is presented with search results.\n FIG. 8 is a screen shot of another example search interface in which a knowledge panel is presented with search results.\n FIG. 9 is a screen shot of another example search interface in which a knowledge panel is presented with search results.\n FIG. 10 is a screen shot of another example search interface in which a knowledge panel is presented with search results.\n FIG. 11 is a screen shot of another example search interface in which a knowledge panel is presented with search results.\n FIG. 12 is a flow chart of an example process for providing a knowledge panel with search results for a search query.\n Like reference numbers and designations in the various drawings indicate like elements.\nDETAILED DESCRIPTION\n A system can provide one or more knowledge panels in response to a received search query.  A knowledge panel is a user interface element that provides information or other content related to a particular entity referenced by a search query, such\nas a person, place, country, landmark, animal, historical event, organization, business, sports team, sporting event, movie, song, album, game, work of art, or any other entity.\n In some implementations, a knowledge panel provides a summary of information for the entity.  For example, a knowledge panel for a singer may include the name of the singer, an image of the singer, a description of the singer, one or more facts\nabout the singer, and content that identifies songs and albums recorded by the singer.  Other types of information and content can also be presented in the knowledge panel.\n The content of a knowledge panel may include content published or otherwise provided by multiple resources, such as multiple web pages.  For example, a knowledge panel for a landmark may include an image of the landmark that has been published\non a first web page that is hosted by a first publisher.  The knowledge panel may also include a set of facts about the landmark that have been published on a second web page published by a second publisher different from the first publisher.\n A knowledge panel may be presented inline or adjacent to other search results for a received query or in place of the other search results.  For example, standard search results that provide a link to resources deemed to be responsive to the\nsearch query may be presented on one side of a search results page and a knowledge panel may be presented on the other side of the search results page.\n FIG. 1 is a block diagram of an example environment 100 in which a search system 120 provides search services.  The example environment 100 includes a network 102, e.g., a local area network (LAN), wide area network (WAN), the Internet, or a\ncombination of them, connects web sites 104, user devices 106, and the search system 120.  The network 102 can be accessed over a wired and/or a wireless communications link.  For example, mobile computing devices, such as smartphones can utilize a\ncellular network to access the network.  The environment 100 may include millions of web sites 104 and user devices 106.\n A web site 104 is one or more resources 105 associated with a domain name and hosted by one or more servers.  An example web site is a collection of web pages formatted in hypertext markup language (HTML) that can contain text, images,\nmultimedia content, and programming elements, e.g., scripts.  Each web site 104 is maintained by a publisher, e.g., an entity that manages and/or owns the web site.\n A resource 105 is data provided over the network 102 and that is associated with a resource address, e.g., a uniform resource locator.  Resources 105 that can be provided by a web site 104 include HTML pages, word processing documents, and\nportable document format (PDF) documents, images, video, and feed sources, to name just a few.  The resources 105 can include content, e.g., words, phrases, images and sounds and may include embedded information, e.g., meta information and hyperlinks,\nand/or embedded instructions, e.g., scripts.\n A user device 106 is an electronic device that is under control of a user and is capable of requesting and receiving resources 105 over the network 102.  Example user devices 106 include personal computers, mobile computing devices, e.g.,\nsmartphones and/or tablet computing devices, that can send and receive data over the network 102.  As used throughout this document the term mobile computing device (\"mobile device\") refers to a user device that is configured to communicate over a mobile\ncommunications network.  A smartphone, (i.e., a phone that is enabled to communicate over the Internet) is an example of a mobile device.  A user device 106 typically includes a user application, e.g., a web browser, to facilitate the sending and\nreceiving of data over the network 102.\n To facilitate searching of resources 105, the search system 120 identifies the resources 105 by crawling and indexing the resources 105 provided on web sites 104.  Data about the resources 105 can be indexed based on the resource to which the\ndata corresponds.  The indexed and, optionally, cached copies of the resources 105 are stored in a search index 122.\n The user devices 106 submit search queries 109 to the search system 120.  In response, the search system 120 accesses the search index 122 to identify resources 105 that are relevant to, e.g., have at least a minimum specified relevance score\nfor, the search query 109.  The search system 120 identifies the resources 105, generates search results 111 that identify the resources 105, and returns the search results 111 to the user devices 106.  A search result 111 is data generated by the search\nsystem 120 that identifies a resource 105 that is responsive to a particular search query, and includes a link to the resource 105.  An example search result 111 can include a web page title, a snippet of text or a portion of an image extracted from the\nweb page, and the URL of the web page.\n Data for the search queries 109 submitted during user sessions are stored in a data store, such as the historical data store 124.  For example, the search system 110 can store received search queries in the historical data store 124.\n Selection data specifying actions taken in response to search results 111 provided in response to each search query 109 are also stored in the historical data store 124, for example, by the search system 120.  These actions can include whether a\nsearch result 111 was selected (e.g., clicked or hovered over with a pointer).  The selection data can also include, for each selection of a search result 111, data identifying the search query 109 for which the search result 111 was provided.\n The example environment 100 also includes a knowledge panel apparatus 130 communicably coupled to the search system 120 (e.g., directly coupled or coupled over a network such as network 102).  The search system 120 can interact with the\nknowledge panel apparatus 130 to determine whether to provide a knowledge panel 113 with search results 111 for a search query 109.  If it is determined that a knowledge panel 113 should be provided, the knowledge panel apparatus 130 can generate the\nknowledge panel 113 and provide the generated knowledge panel 113 to the search system 120, which in turn provides search results 111 and a knowledge panel 113 to the user device 106 that submitted the search query 109.\n A knowledge panel 113 is typically provided for queries that have been determined to reference a particular entity, such as a person, place, country, landmark, animal, historical event, organization, business, sports team, sporting event, movie,\nsong, album, game, work of art, or any other entity.  In general, a knowledge panel 113 for a particular entity includes content related to the particular entity.  In some implementations, a knowledge panel 113 includes a set of factual information\ndeemed relevant to the particular entity.  For example, a knowledge panel 113 for an actor may include biographical information for the actor, as well as content associated with movies or television shows that the actor has appeared.  In some\nimplementations, the knowledge panel 113 includes a summary of information related to the particular entity.  For example, a knowledge panel 113 for a nation may include a map of the nation, the flag of the nation, the official language of the nation,\nand/or other facts and content related to the nation.\n In some implementations, a knowledge panel 113 is much larger and consumes more area of a search interface than standard search results 111.  For example, the knowledge panel 113 may span the length of three or more standard search results 111\nto accommodate the content items in the knowledge panel 113 and to draw attention to the knowledge panel 113.\n In some implementations, a knowledge panel 113 is displayed in a knowledge panel area.  The knowledge panel area may be presented with or alongside to a search results area that presents search results 111.  In some implementations, the\nknowledge panel area consumes a larger area than the area consumed by two or more of the search results presented in the search results area.\n The knowledge panel apparatus 130 includes or is communicably coupled to one or more data storage units that include a content items store 132 and a knowledge panel templates store 134.  The content items store 132 stores content items that can\nbe inserted into a knowledge panel.  In general, the content items are discrete units of content and can be in the form of text, images, videos, graphics, audio, tables, or other types of content.\n In some implementations, the content items store 132 includes an index of entities and data identifying content items relevant to the entities.  For example, the index may include data identifying a title for an entity, one or more images\nrelevant to the entity, a description of the entity, one or more facts about the entity, one or more videos relevant to the entity, upcoming events related to the entity, content from a social networking page associated with the entity, and other\ninformation or types or categories of content that have been deemed relevant to the entity.\n The content items for each entity can come from disparate sources and/or disparate publishers.  For example, content items for a particular entity can be obtained from several different web pages or other resources that are each hosted by\ndifferent publishers.  To illustrate, the content items for a particular actor may include images of the actor received from several different sources and these images can be included in the same knowledge panel that is provided in response to a query\nthat references the particular actor.  As another example, an image of a person may be obtained from an official site for the person, and a set of facts and/or description for the person may be obtained from an online encyclopedia.\n In some implementations, the knowledge panel apparatus 130 identifies content items for each of a set of entities by crawling the Internet for content related to the entity.  The knowledge panel apparatus 130 can also determine which of the\nobtained content items to provide with a knowledge panel 113 for the entity.  For example, some of the content items may be more relevant to the entity or more appropriate for the knowledge panel 113 than other content items.  As an example, an image of\na person's face may be more appropriate for a knowledge panel 113 than an image of the person taken from a long distance.\n The type or categories of content provided in a knowledge panel 113 may be determined based on the particular entity or the type of entity referenced by the knowledge panel 113.  For example, a knowledge panel 113 for a person may include an\nimage of the person, facts about the person, and references to any public works produced by the person.  By way of another example, a knowledge panel 113 for a landmark may include images of the landmark, facts about the landmark, and travel information,\nsuch as a map, for a person to travel to the landmark.  The categories of content that are included in a knowledge panel 113 can also vary based on the particular entity.  For example, a person that has acted in movies and that has also recorded one or\nmore albums may include references to both types of works.  A knowledge panel template may specify the content for a knowledge panel 113, as described in more detail below.\n Although different types of content can be provided for different types of entities and/or different entities within an entity type, the knowledge panels 113 can be configured to provide a recognizable and consistent user interface for users. \nIn some implementations, each knowledge panel 113 can have one or more standard types of content items.  For example, the standard types of content items can include a title, an image, a description, and one or more facts about the entity for which the\nknowledge panel 113 is provided.\n In some implementations, the title for a knowledge panel 113 is the name of the entity or an alias of the entity.  For example, the title for a knowledge panel 113 provided for a person can be the name of the person.  Similarly, the title for a\nknowledge panel 113 provided for a country can be the name of the country.  As the name of the entity may be different from the search query 109 for which the knowledge panel 113 is provided, the title may also be different than the search query 109. \nFor example, a search query 109 may reference an alias or a shortened version of a celebrity's name, while the title for the knowledge panel 113 may include the celebrity's full legal name.  In this way, the title provides confirmation of what the\nknowledge panel 113 references.\n The description for the knowledge panel 113 can provide an adequate explanation of what the entity is, such as a summary of the entity, without going into so much detail to distract from the search interface (or other page) in which the\nknowledge panel 113 is presented.  Candidate descriptions can be obtained from a variety of places, such as prefixes of text from trusted encyclopedia articles or top ranking web pages.\n The image for a knowledge panel 113 can be an image representative of the entity for which the knowledge panel 113 is generated.  This image may vary based on the type of entity and the entity itself.  For example, an image for a country may\ninclude a map of the country, while an image for a person may include a representative image of the person.\n The image for a knowledge panel 113 may be selected by taking a top ranking image from search results for the entity of the knowledge panel 113.  For example, a web search directed to images can be performed for an entity and the top ranking\nimage may be selected for the search results for inclusion in knowledge panels 113 for the entity.\n The set of facts that are presented in a knowledge panel 113 may be determined based on the type of entity and/or the entity itself.  For example, certain facts may be preferred for actors, and a different set of facts may be preferred for\nsingers.  Also, certain facts may be relevant to certain entities within an entity type, while not being relevant to other entities within the entity type.  For example, a knowledge panel 113 for an actor that has won many awards may include a listing of\nthe awards in the set of facts.  However, a listing of awards may be omitted for an actor that has not yet won any awards.\n The set of facts that are presented for a particular entity may be based on previously received search queries 109 that referenced the particular entity.  For example, if a significant number, e.g., at least a threshold number, of previously\nreceived search queries 109 have referenced a person's height, then the person's height may be included in the set of facts that are presented in a knowledge panel for that person.  In general, the facts for an entity can be ranked based on the number of\nthe previously received search queries that have been deemed to be requesting information about the entity and that have been deemed to be requesting information about that fact.\n Each of the standard types of content items, e.g., title, image, description, and set of facts, can consistently be located in a same portion of the knowledge panel 113 to provide a consistent user interface across knowledge panels 113 for\ndifferent types of entities.  For example, an image may be located near the top left corner of the knowledge panel 113 and the description and facts may be located to the right of the image.  If a content item for one of the types of content items is not\navailable for an entity, then that type of content item may be replaced with another type of content item.  For example, if an entity does not have an available image, one or more facts may be presented in the knowledge panel in place of an image.  Note\nthat the standard types of content items referenced throughout this document are provided above for purpose of example, and that other types of content items can be selected as standard content items.\n The knowledge panel templates store 134 stores knowledge panel templates that can be populated with content items to generate a knowledge panel 113.  In general, a knowledge panel template specifies types of content items to include in the\nknowledge panel 113 and includes placeholders for content items of the specified type.  For example, a knowledge panel template may include placeholders for a title, one or more images, a description, a set of facts, and/or other types of content items.\n The knowledge panel templates store 134 can include one or more templates for each of a set of entity types.  For example, the knowledge panel templates store 134 may include one or more \"person\" templates, \"place\" templates, \"landmark\"\ntemplates, \"movie\" templates, \"business\" templates, \"game\" templates, \"sports team\" templates, \"sports event\" templates, and/or \"disambiguation\" templates.  A knowledge panel template for a particular type of entity may have placeholders for different\ncontent item types than a knowledge panel template for another type of entity.  For example, a knowledge panel template for a country may include a placeholder for a set of images of cities in the country, while a knowledge panel template for a landmark\nmay include a placeholder for a set of images of other landmarks.  Other types of content particular to a type of entity are described below.\n The knowledge panel templates store 134 may also include knowledge panel templates for entity subtypes.  For example, under the \"person\" entity type, there may be an \"actor\" knowledge panel template, a \"singer\" knowledge panel template, and/or a\n\"historical figure\" knowledge panel template.  A knowledge panel template for actors may include a placeholder for a set of images of movie or television show posters for movies or television shows in which the actor performed, while a knowledge panel\ntemplate for singers may include a placeholder for a table element having information regarding songs released by the singer.\n FIG. 2 is a block diagram illustrating a process 200 for supplying search results 206 and a knowledge panel 202.  In response to receiving a search query, for example a search query that references a particular entity, the search system 120 can\nidentify a set of search results 206 and interact with the knowledge panel apparatus 130 to generate the knowledge panel 202.  The search system 120 can also associate the knowledge panel 202 with the set of search results 206.  In this example, the set\nof search results 206 includes Result_1 through Result_N. The content identified and generated by the search system 120 in conjunction with the knowledge panel apparatus 130 is shown in block 201.\n The example knowledge panel 202 includes a control 204.  The control 204 can include a minimization control, an expansion control, and/or a hide control.  A minimization control causes the knowledge panel 202 to be minimized when activated. \nSimilarly, an expansion control causes the knowledge panel 202 to expand when activated.  The hide control causes the knowledge panel 202 to be hidden when activated.  A user may select the control 204 to arrange the knowledge panel 202, for example to\nfacilitate a user optimized search session.\n The search system 120 can provide the set of search results 206 or a subset thereof to a user device for presentation in a search interface 211.  The search system 120 can also provide the knowledge panel 202 to the user device for presentation\nin the search interface 211.  In this example, the knowledge panel 202 is displayed on the right side of the search results 206.  In some implementations, the knowledge panel 202 may be displayed to the left of the search results 206, above the search\nresults 206, below the search results 206, or between two of the search results 206.\n FIG. 3 is a screen shot of an example search interface 300 in which a knowledge panel 320 is presented with search results 311a-311d.  The search interface 300 can be presented at a user device 106 and includes a query entry field 305 in which a\nuser can enter a search query 309.  The search interface 300 also includes a query initiation element 307 with which the search query 309 can be submitted to the search system 120.  For example, in response to user interaction with the search initiation\nelement 307, the user device 106 can submit the search query 309 to the search system 120.\n The search interface 300 also includes a knowledge panel 320 and search results 311a-311d.  In this example search interface 300, the knowledge panel 320 and the search results 311a-311d have been provided in response to the search query\n\"European Country.\" Although one knowledge panel 320 and four search results 311a-311d are illustrated in the search interface 300, other numbers of knowledge panels and other numbers of search results can be presented in a search interface.\n The knowledge panel 320 is for a country (\"European Country\"), and can be generated using a \"place\" template for example.  The \"place\" template can specify types of content items with which the knowledge panel 320 is to be populated and include\nplaceholders for the types of content items.  For example, the \"place\" template may include a placeholder for each of a title, an image depicting a map of the place, a description, a set of facts, one or more sets of images, and/or related search\nqueries.\n The knowledge panel 320 includes a title 324 and links 326 to resources having additional information about the European Country.  For example, the links 326 include a link to an official web site, weather information, and news about the\nEuropean Country.\n The knowledge panel 320 also includes an image depicting the flag for the European Country 322, an image depicting a map of the European Country 328, a description of the European Country 329, and a set of facts about the European Country 330. \nIn this example, the set of facts 330 includes population, GDP, capital and language.  The set of facts 330 for the knowledge panel 320 may be selected based on the popularity of the facts with respect to historical search data for the entity, in this\nexample the European Country.  For example, facts about the European Country searched for more often, e.g., the top X number of searched for facts, may be selected for inclusion in the set of facts over facts that have not been search for as often.\n The knowledge panel 320 also includes two sets of images 332 and 334.  The set of images 332 includes images of cities in the country and the set of images 334 includes images of landmarks located in the country.  Each of these types of images\ncan be considered a different type of content item for purposes of populating a knowledge panel.  Each image may include a link that, when selected, causes a new search to be initiated for the subject of the image.  For example, if an image of a city is\nselected, the search interface 320 may cause the user device 106 displaying the search interface 320 to submit a search query to the search system 120 for the city depicted in the image.\n The knowledge panel 320 also includes a set of related search queries 336.  The related search queries 336 may be queries related to the European Country.  For example, the related search queries 336 may include queries that reference the\nEuropean Country, cities within the European Country, or famous residents of the European Country.  Each related search query may include a link that, when selected, initiates a search for the selected search query.\n The content items included in the knowledge panel 320 can be obtained from multiple disparate sources.  For example, the image of the flag 322 and the description 329 may be obtained from an online encyclopedia, while the images 332 of the\ncities may be obtained from a travel site independent of (e.g., provided by a different publisher than) the online encyclopedia.\n As described above, a knowledge panel may be displayed in a knowledge panel area.  In this example search interface 300, the area inside perimeter 340 may be considered the knowledge panel area.  As shown in FIG. 3, this knowledge panel area and\nthe knowledge panel 320 consume a larger area than each individual search results 311a-311d.  In some implementation, the knowledge panel area or the knowledge panel may be substantially the same size as a standard search result or smaller than a\nstandard search result.\n FIG. 4 is a screen shot of another example search interface 400 in which a knowledge panel 420 is presented with search results 411a-411d.  Similar to the search interface 300, the search interface 400 includes a query entry field 405 in which a\nuser can enter a search query 409 and a search initiation element 407 with which a search query 409 can be submitted to the search system 120.  The search interface 400 also includes search results 411a-411d and a knowledge panel 420.  In this example\nsearch interface 400, the knowledge panel 420 and the search results 411a-411d have been received from the search system 120 and/or the knowledge panel apparatus 130 in response to the submission of the search query \"Ancient Landmark.\"\n The knowledge panel 420 is for a landmark (\"Ancient Landmark\"), and can be generated using a \"landmark\" template.  The \"landmark\" template can include placeholders for types of content related to landmarks, such as a title, an image of the\nlandmark, a description, a set of facts, an image of a map for the landmark, one or more sets of other images that have been identified as relevant to the landmark, and related search queries.\n The knowledge panel 420 includes a title 422, an image of the Ancient Landmark 424, a description of the Ancient Landmark 426, and a set of facts about the Ancient Landmark 428.  This example set of facts 428 includes the height of the Ancient\nLandmark and the year the Ancient Landmark was built.  The knowledge panel 420 also includes an image of a map for the Ancient Landmark 430, a set of images 432 that includes images of other landmarks, and a set of images 434 that includes images for\nrelated search queries.  Each of the images 432 and 434 can include a link that, when selected, initiates a search for the subject of the image.\n In some implementations, a knowledge panel for a place or for a landmark may include a reservation element that enables the user to secure reservations to visit the place or to visit the landmark.  For example, the knowledge panel may include an\ninteractive user interface element that receives user information, such as travel dates, origination location, and credit card information, and enables the user to secure the reservation within the knowledge panel.  In some implementations, a knowledge\npanel for a place or landmark may include an interactive map that enables a user to obtain directions to the place or landmark.\n FIG. 5A is a screen shot of another example search interface 500 in which a knowledge panel 520 is presented with search results 511a-511d.  Similar to the search interface 300, the search interface 500 includes a query entry field 505 in which\na user can enter a search query 509 and a search initiation element 507 with which a search query 509 can be submitted to the search system 120.  The search interface 500 also includes search results 511a-511d and a knowledge panel 520.  In this example\nsearch interface 500, the knowledge panel 520 and the search results 511a-511d have been received from the search system 120 and/or the knowledge panel apparatus 130 in response to the submission of the search query \"Famous Actor.\"\n The knowledge panel 520 is for a person (\"Famous Actor\"), and can be generated using a \"person\" template.  The \"person\" template can include placeholders for types of content related to a person, such as a title, an image of the person, a\ndescription, a set of facts, one or more sets of images, a table having information regarding the person, and related search queries.\n In some implementations, there may be templates for particular types of people and the content of the templates may differ based on the type of person.  For example, a template for a singer may include a table that includes a placeholder for a\nlist of songs released by the singer, while a template for an actor may include a placeholder for images depicting posters of movies in which the actor appeared.\n The example knowledge panel 520 includes a title 522, an image of Famous Actor 524, a description of Famous Actor 526, and a set of facts about Famous Actor 528.  This example set of facts 528 includes the date of birth of Famous Actor and the\nnet worth of Famous Actor.  Other facts could also be presented in a \"person\" template and can vary based on the person.  For example, if the person is deceased, the date of death may be included in the set of facts.\n In this example, Famous Actor is credited with movie appearances and music.  To present information regarding the movie appearances and the music of Famous Actor, the knowledge panel 520 includes a set of images 530 of movie posters for movies\nin which Famous Actor appeared and a set of images 532 for albums released by Famous Actor.  In some implementations, a table may be used to provide information about movies, albums, or songs related to Famous Actor.  An example table is depicted in FIG.\n9 and described below.\n The knowledge panel 520 also includes a social networking element 534.  A social networking element is a user interface element that provides information about a social network page that is related to the entity for which the knowledge panel is\nbeing provided.  For example, the social networking element 534 depicts a recent post that has been published on a social networking page for Famous Actor.  Social networking elements may be included in knowledge panels for people that have a public\nsocial networking page and/or for people that have been referenced by public user posts in social networking environments.\n FIG. 5B is a screen shot of another example search interface 550 in which a knowledge panel 560 is presented with search results 511a-511d.  The example search interface 550 has content and features similar to those of the search interface 500\nof FIG. 5A.  However, the knowledge panel 560 of FIG. 5B includes a row of images 565 related to Famous Actor near the top of the knowledge panel 560.  In some implementations, the knowledge panel apparatus 130 can perform a web search, for example at\nquery time, to identify content to include in a knowledge panel.  This search can be limited to certain types of content, such as for images, video, books, other types of content.  In some implementations, the knowledge panel apparatus 130 performs\ncertain types of searches based on the type of entity referenced in the knowledge panel.  For example, the knowledge panel apparatus 130 may initiate a search for images or videos to include in a knowledge panel for a famous person.  By way of another\nexample, the knowledge panel apparatus 130 may initiate a search for maps or images of landmarks to include in a knowledge panel for a place or a landmark.\n In some implementations, the knowledge panel apparatus 130 provides search criteria to the search system 120.  For example, to identify the images 565 of Famous Actor included in the knowledge panel 560, the knowledge panel apparatus 130 can\nprovide search criteria that identifies Famous Actor and that also identifies the type of search, i.e., an image search.  The search criteria can identify the entity for which the knowledge panel is provided or the search query received from the user\ndevice 106.  The search system 120 can identify content responsive to the search criteria and provide the content to the knowledge panel apparatus 130.  In turn, the knowledge panel apparatus 130 may select at least a portion of the content for the\nknowledge panel.  For example, the search system 120 may have provided many images related to Famous Actor.  The knowledge panel apparatus 130 can select a portion of the received images, for example a portion of the higher ranked images, for inclusion\nin the knowledge panel 560\n FIG. 6 is a screen shot of another example search interface 600 in which a knowledge panel 620 is presented with search results 611a-611d.  Similar to the search interface 300, the search interface 600 includes a query entry field 605 in which a\nuser can enter a search query 609 and a search initiation element 607 with a search query 609 can be submitted to the search system 120.  The search interface 600 also includes search results 611a-611d and a knowledge panel 620.  In this example search\ninterface 600, the knowledge panel 620 and the search results 611a-611d have been received from the search system 120 and/or the knowledge panel apparatus 130 in response to the submission of the search query \"Blockbuster Movie.\"\n The knowledge panel 620 is for a movie (\"Blockbuster Movie\"), and can be generated using a \"movie\" template.  The \"movie\" template can include placeholders for types of content related to movies, such as a title, an image for the movie, e.g.,\nimage of movie poster for the movie, a description of the movie, a set of facts about the movie, one or more sets of images that have been deemed relevant to the movie, and related search queries for the movie.\n The knowledge panel 620 includes a title 622, an image of a movie poster for Blockbuster Movie 624, a description of Blockbuster Movie 626, and a set of facts about Blockbuster Movie 628.  This example set of facts includes the genre, release\ndate, and rating of Blockbuster Movie.  Other facts regarding a movie that may be presented in a knowledge panel include the cast, director, producer, production companies, revenue generated by the movie, location where the movie was filmed and any other\nrelevant facts.\n The knowledge panel 620 also includes a set of images 630 that includes images of the cast for Blockbuster Movie, and a set of images 632 that includes images for related or similar movies.  Each of the images 630 and 632 can include a link\nthat, when selected, initiates a search by the search system 120 for information about the subject of the image.  The knowledge panel 620 also includes links to related search queries 634.\n A similar knowledge panel may be provided for other types of entities such as music, television shows, etc. In some implementations, a knowledge panel for movies or other purchasable items includes a user interface element that enables users to\npurchase the item.\n FIG. 7 is a screen shot of another example search interface 700 in which a knowledge panel 720 is presented with search results 711a-711d.  Similar to the search interface 300, the search interface 700 includes a query entry field 705 in which a\nuser can enter a search query 709 and a search initiation element 707 with which a search query 709 can be submitted to the search system 120.  The search interface 700 also includes search results 711a-711d and a knowledge panel 720.  In this example\nsearch interface 700, the knowledge panel 720 and the search results 711a-711d have been received from the search system 120 and/or the knowledge panel apparatus 130 in response to the submission of the search query \"Big Business.\"\n The knowledge panel 720 is for a business (\"Big Business\"), and can be generated using a \"business\" knowledge panel template.  The \"business\" template can include placeholders for content related to businesses, such as a title, an image\ndepicting a logo for the business, a description of the business, a set of facts about the business, a stock quote element having stock information for the business, if appropriate, one or more sets of images that have been deemed relevant to the\nbusiness, a map to a location of the business, links to additional information about the business, and related search queries related to the business.  The \"business\" template can be used for other organizations as well, such as non-profit organizations,\nschools, churches, or clubs to name a few.\n The knowledge panel 720 includes a title 722, an image depicting a logo for Big Business 724, a stock element 726 that presents stock information for Big Business, and a description of Big Business 728.  The knowledge panel 720 also includes a\nset of links 730 that each link to a resource having additional information about Big Business.\n The knowledge panel 720 also includes a set of images 732 that includes images of the leadership of Big Business.  For example, the images 732 may include images of the CEO, COO, or other prominent leaders of Big Business.  Each of the images\n732 can include a link that, when selected, initiates a search for the subject of the image.  For example, user interaction with (e.g., a user click of) an image of the CEO of Big Business may cause the name of the CEO to be submitted as a search query\nto the search system 120.  Other sets of images may also be included for businesses, such as images of products offered by the business.  The knowledge panel 720 also includes links to related search queries 734.\n FIG. 8 is a screen shot of another example search interface 800 in which a knowledge panel 820 is presented with search results 811a-811d.  Similar to the search interface 300, the search interface 800 includes a query entry field 805 in which a\nuser can enter a search query 809 and a search initiation element 807 with which a search query 809 can be submitted to the search system 120.  The search interface 800 also includes search results 811a-811d and a knowledge panel 820.  In this example\nsearch interface 800, the knowledge panel 820 and the search results 811a-811d have been received from the search system 120 and/or the knowledge panel apparatus 130 in response to the submission of the search query \"Fun Game.\"\n The knowledge panel 820 is for a game (\"Fun Game\"), and can be generated using a \"game\" template.  The \"game\" template can include placeholders for types of content related to games, such as a title, an image of a logo for the game, a\ndescription of the game, a set of facts about the game, one or more sets of images that have been deemed relevant to the game, and related search queries related to the game.\n The knowledge panel 820 includes a title 822, an image of a logo for Fun Game 824, a description of Fun Game 826, and a list of vendors at which Fun Game is offered 828.  The knowledge panel 820 also includes a set of images 830 that includes\nimages of screen shots from Fun Game, and a set of images 832 that includes images for related or similar games, such as other games that are often purchased by users that purchased Fun Game.  Each of the images 830 and 832 can include a link that, when\nselected, initiates a search for the subject of the image.  The knowledge panel 820 also includes links to related search queries 834.\n FIG. 9 is a screen shot of another example search interface 900 in which a knowledge panel 920 is presented with search results 911a-911d.  Similar to the search interface 300, the search interface 900 includes a query entry field 905 in which a\nuser can enter a search query 909 and a search initiation element 907 with which a search query 909 can be submitted to the search system 120.  The search interface 900 also includes search results 911a-911d and a knowledge panel 920.  In this example\nsearch interface 920, the knowledge panel 920 and the search results 911a-911d have been received from the search system 120 in response to the submission of the search query \"Big League Team.\"\n The knowledge panel 920 is for a sports team (\"Big League Team\"), and can be generated using a \"sports team\" knowledge panel template.  The \"sports team\" template can include placeholders for content related to sports teams, such as a title, an\nimage of a logo for the team, a description of the team, a set of facts about the team, one or more sets of images that have been deemed relevant to the sports team, information regarding members of the team, a schedule for the team, and related search\nqueries.\n The knowledge panel 920 includes a title 922, an image of a logo for Big League Team 924, a description of Big League Team 926, and a set of facts about Big League Team 928.  The set of facts include the stadium, manager, division, and years in\nwhich Big League Team won championships.\n The knowledge panel 920 also includes a table element 930 that includes information regarding players of Big League Team.  In particular, the table element 930 includes the names of the players, the jersey numbers of the players, and the\nposition for the players.  The example table element 930 includes a scroll bar 932.  The scroll bar 932 enables a user to view additional information in the table element 930.  For example, if the user scrolls the scroll bar 932 down, the user device 106\npresenting the search interface 900 will display lower rows of the table element 930 that are not currently in view.\n Some queries may be related to multiple entities, such that identifying the entity for which the query is directed may be difficult.  For example, a search query of \"phoenix\" may be directed to the city in Arizona or the mythical bird.  For such\nqueries, the knowledge panel apparatus 130 may provide a disambiguation knowledge panel having information about multiple entities.  For example, the knowledge panel apparatus 130 may generate a disambiguation knowledge panel for the search query\n\"phoenix\" that includes disambiguation information about the city in Arizona and information about the mythical bird.  Disambiguation information can be considered content that distinguishes between different meanings associated with an ambiguous term. \nAccordingly, disambiguation information can include content characterizing at least one distinct meaning for a term that has multiple meanings.\n FIG. 10 is a screen shot of another example search interface 1000 in which a knowledge panel 1020 is presented with search results 1011a-1011d.  Similar to the search interface 300, the search interface 1000 includes a query entry field 1005 in\nwhich a user can enter a search query 1009 and a search initiation element 1007 with which a search query 1009 can be submitted to the search system 120.  The search interface 1000 also includes search results 1011a-1011d and an example disambiguation\nknowledge panel 1120.  In this example search interface 1000, the knowledge panel 1020 and the search results 1011a-1011d have been received from the search system 120 in response to the submission of the search query \"California Universities.\"\n As there are many universities in California, it may difficult to determine an appropriate university for which to provide a knowledge panel.  Thus, the knowledge panel apparatus 130 may provide the disambiguation knowledge panel 1020, which\nincludes content about multiple universities in California.  In particular, the knowledge panel 1020 includes content for several public universities located in California 1022 and content for several private universities located in California 1024.  The\ncontent for each university can include a link that, when selected, initiates a search for that university, or that links to the official web page for the university.\n FIG. 11 is a screen shot of another example search interface 1100 in which a knowledge panel 120 is presented with search results 1111a-1111d.  Similar to the search interface 300, the search interface 1100 includes a query entry field 1105 in\nwhich a user can enter a search query 1109 and a search initiation element 1107 with which a search query 1109 can be submitted to the search system 120.  The search interface 1100 also includes search results 1111a-1111d and a knowledge panel 1120.  In\nthis example search interface 1120, the knowledge panel 1120 and the search results 1111a-1111d have been received from the search system 120 and/or the knowledge panel apparatus 130 in response to the submission of the search query \"profitable\ncorporation stock quote.\"\n The example knowledge panel 1120 depicts a financial stock report for a corporation (\"Profitable Corporation\").  The knowledge panel 1120 includes a title 1122, an image depicting a logo for Profitable Corporation 1124, a description of\nProfitable Corporation 1126, and a fact about Profitable Corporation 1128.  The knowledge panel 1120 also includes stock pricing information 1130 for Profitable Corporation and an interactive chart user interface element 1132.\n The interactive chart user interface element 1132 enables users to select a time period for which to view stock prices in chart form and includes a chart 1134, a scroll bar 1136, and scroll bar element 1138.  To move about the chart 1132 and to\nview stock pricing for a time period that is not currently in view, a user can move the scroll bar element 1138 left and right along the scroll bar 1136.  In response, the user device 1106 on which the search interface 1100 is displayed can update the\nchart 1132 to present the appropriate stock pricing information.\n In some implementations, stock pricing information that can be presented in the interactive chart user interface element 1132 is provided to the user device 106 in advance of user interaction with the interactive chart user interface element\n1132.  In this way, the user device 106 can update the chart 134 in response to user interaction with the scroll bar 1136 without initiating additional content requests to the search system 120 or to another resource.\n In some implementations, the interactive chart user interface element 1132 causes the user device 106 to request additional stock pricing information in response to user interaction with the scroll bar 1136.  Either way, the chart can be updated\nwithin the knowledge panel 1120 without navigating away from the page depicting the search results 1111a-1111d and the knowledge panel 1120.\n Other types of knowledge panels can also incorporate similar interactive user interface elements.  For example, a knowledge panel having weather related content for a particular location may include an interactive user interface element that\nenables the user to select a time period for weather information or a location for which weather information is desired.\n FIG. 12 is a flow chart of an example process 1200 for providing a knowledge panel with search results for a search query.  The process 1200 can be implemented, for example, by the search system 120 in conjunction with the knowledge panel\napparatus 130.\n Query data specifying a search query 109 is received (1202).  For example, the search system 120 may receive query data specifying a user query from a user device 106.  In response to receiving the query data, the search system 120 may identify\nsearch results 111 responsive to the search query 109 specified by the query data (1204).\n A determination is made whether to provide a knowledge panel 113 with the search results (1206).  In some implementations, the knowledge panel apparatus 130 determines whether to provide a knowledge panel 113 based on the received search query\n109.  For example, the search system 120 may provide the search query 109 to the knowledge panel apparatus 130 with a request to provide a knowledge panel 113.\n In some implementations, the knowledge panel apparatus 130 determines whether to provide a knowledge panel 113 for the search query based on whether the received search query 109 includes or references a factual entity.  The knowledge panel\napparatus 130 can determine whether the search query 109 references a known factual entity and, if so, determine to provide a knowledge panel 113 for the factual entity.  In some implementations, a factual entity is a single conceptual entity, such as a\nperson, place, country, landmark, animal, historical event, organization, business, sports team, sporting event, movie, song, album, game, work of art, or any other entity.  In some implementations, a factual entity is a concept, subject, or topic.\n In some implementations, the knowledge panel apparatus 130 determines whether a search query references a factual entity by comparing one or more terms of the search query to a list of known factual entities.  For example, if the search query is\n\"songs by Ima Singer,\" the knowledge panel apparatus 130 may determine whether \"Ima Singer\" is a factual entity by comparing \"Ima\" and/or \"Singer\" to a list of known factual entities that are stored in a database.  If there is a match to a factual entity\nof \"Ima Singer\" or an alias of \"Ima Singer,\" the knowledge panel apparatus 130 may determine to provide a knowledge panel 113 for \"Ima Singer\" with search results 111 for the search query \"songs by Ima Singer.\"\n In some implementations, the knowledge panel apparatus 130 determines whether to provide a knowledge panel 113 based on whether there are multiple distinct meanings associated with the received search query 109.  For example, the knowledge panel\napparatus 130 may determine whether the received search query 109 is associated with multiple distinct meanings by comparing each term of the received search query 109 to a list of terms that have multiple distinct meanings.  For example, if the received\nsearch query 109 includes the term \"phoenix,\" the knowledge panel apparatus 130 may determine that the received search query 109 has multiple distinct meanings as the term \"phoenix\" can correspond to the city in Arizona (one distinct meaning) or the\nmythical bird (another distinct meaning).\n In some implementations, the knowledge panel apparatus 130 considers other terms in the received search query 109 when determining whether the search query 109 includes multiple distinct meanings.  For example, if the received search query 109\nincludes \"phoenix\" and \"Arizona,\" the knowledge panel apparatus 130 may determine that the search query 109 is directed to the city in Arizona rather than the mythical bird.\n If the knowledge panel apparatus 130 determines that the received search query 109 is associated with multiple distinct meanings, the knowledge panel apparatus 130 may determine to provide a disambiguation knowledge panel with content directed\nto two or more of the distinct meanings.\n If a determination is made to not provide a knowledge panel 113, the search system 1100 provides the identified search results for the received search query (1208).  For example, the knowledge panel apparatus 130 may provide data to the search\nsystem 120 that specifies that a knowledge panel 113 will not be provided for the received search query 109.  In turn, the search system 120 can provide the identified search results 111 to the user device 106 from which the search query 109 was received\nwithout providing a knowledge panel (or content items for populating a knowledge panel) for presentation on the user device 106.\n If a determination is made to provide a knowledge panel 113, the knowledge panel apparatus 130 identifies a knowledge panel template for the entity referenced by the received search query (1210).  As described above, the knowledge panel\ntemplates store 134 can include one or more templates for each of a set of entity types.  For example, the knowledge panel templates store 134 may include a \"person\" template and/or a \"place\" template.\n The knowledge panel apparatus 130 can determine the type of entity referenced by the received search query 109 and access the knowledge panel templates store 134 to retrieve the appropriate knowledge panel template for the entity.  For example,\nthe index of the content items store 132 may include data identifying the type of entity for each entity indexed therein.  In another example, the index of the content items store 132 may include data identifying the appropriate knowledge panel template\nfor each entity.  The knowledge panel apparatus 130 can access the index to identify the appropriate template for the entity referenced by the received search query.\n Content items are identified for the knowledge panel (1212).  In some implementations, the knowledge panel apparatus 130 identifies content items for the entity referenced by the search query based on the identified knowledge panel template. \nFor example, the knowledge panel template may specify types of content items for the template and include placeholders for the specified types of content items.  For example, a knowledge panel template for a person may include placeholders for a title,\ne.g., name of the person, an image of the person, a description of the person, a set of facts about the person, an additional content for the person.\n In some implementations, the knowledge panel apparatus 130 accesses the content items store 132 to identify the appropriate content for the knowledge panel based on the content specified by the identified knowledge panel template.  For example,\nthe content items store 132 can include an index of content items for each of a set of entities.  The index may include data identifying a title for the entity, an image for the entity, and each other type of content for use in a knowledge panel.  As\ndescribed above, the content items for a particular entity may be obtained from multiple disparate content sources.\n In some implementations, the knowledge panel apparatus 130 accesses several databases dynamically to identify content items for the knowledge panel.  For example, if one of the specified content items is a birth date for an actor or singer, the\nknowledge panel apparatus 130 may access a database of celebrity birthdates to obtain the birth date of the actor or singer.  This operation may be performed before or after receipt of the search query 109.\n The knowledge panel and search results are provided (1214).  In some implementations, the knowledge panel apparatus 130 generates the knowledge panel 113 and provides the knowledge panel 113 to the search system 120.  For example, the knowledge\npanel apparatus 130 may generate the knowledge panel 113 by populating the identified knowledge panel template with the identified content items and provide the generated knowledge panel 113 to the search system 120.  In turn, the search system 120 can\nprovide the identified search results and the knowledge panel 113 to the user device 106 for presentation.\n Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and\ntheir structural equivalents, or in combinations of one or more of them.  Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions,\nencoded on computer storage medium for execution by, or to control the operation of, data processing apparatus.  Alternatively or in addition, the program instructions can be encoded on an artificially-generated propagated signal, e.g., a\nmachine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.  A computer storage medium can be, or be included in,\na computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them.  Moreover, while a computer storage medium is not a propagated signal, a computer\nstorage medium can be a source or destination of computer program instructions encoded in an artificially-generated propagated signal.  The computer storage medium can also be, or be included in, one or more separate physical components or media (e.g.,\nmultiple CDs, disks, or other storage devices).\n The operations described in this specification can be implemented as operations performed by a data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.\n The term \"data processing apparatus\" encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or multiple ones, or combinations, of the\nforegoing The apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).  The apparatus can also include, in addition to hardware, code that creates an\nexecution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of\none or more of them.  The apparatus and execution environment can realize various different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.\n A computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be\ndeployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment.  A computer program may, but need not, correspond to a file in a file system.  A program\ncan be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one\nor more modules, sub-programs, or portions of code).  A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication\nnetwork.\n The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output.  The processes and\nlogic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).\n Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.  Generally, a processor will receive\ninstructions and data from a read-only memory or a random access memory or both.  The essential elements of a computer are a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and\ndata.  Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.  However, a computer\nneed not have such devices.  Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable\nstorage device (e.g., a universal serial bus (USB) flash drive), to name just a few.  Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example\nsemiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.  The processor and the memory can be supplemented by, or\nincorporated in, special purpose logic circuitry.\n To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for\ndisplaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.  Other kinds of devices can be used to provide for interaction with a user as well; for example,\nfeedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.  In addition, a computer\ncan interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.\n Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that\nincludes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more\nsuch back-end, middleware, or front-end components.  The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network.  Examples of communication networks include a local area network\n(\"LAN\") and a wide area network (\"WAN\"), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).\n The computing system can include clients and servers.  A client and server are generally remote from each other and typically interact through a communication network.  The relationship of client and server arises by virtue of computer programs\nrunning on the respective computers and having a client-server relationship to each other.  In some embodiments, a server transmits data (e.g., an HTML page) to a client device (e.g., for purposes of displaying data to and receiving user input from a\nuser interacting with the client device).  Data generated at the client device (e.g., a result of the user interaction) can be received from the client device at the server.\n While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any inventions or of what may be claimed, but rather as descriptions of features specific to particular\nembodiments of particular inventions.  Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment.  Conversely, various features that are described in the\ncontext of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination.  Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or\nmore features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.\n Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations\nbe performed, to achieve desirable results.  In certain circumstances, multitasking and parallel processing may be advantageous.  Moreover, the separation of various system components in the embodiments described above should not be understood as\nrequiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.\n Thus, particular embodiments of the subject matter have been described.  Other embodiments are within the scope of the following claims.  In some cases, the actions recited in the claims can be performed in a different order and still achieve\ndesirable results.  In addition, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results.  In certain implementations, multitasking and parallel\nprocessing may be advantageous.", "application_number": "15233125", "abstract": " Methods, systems, and apparatus, including computer programs encoded on a\n     computer storage medium, for providing knowledge panels with search\n     results. In one aspect, a method includes obtaining search results that\n     are responsive to a received query. A factual entity referenced by the\n     query is identified. Content is identified for display in a knowledge\n     panel for the factual entity. The content includes at least one content\n     item obtained from a first resource and at least one second content item\n     obtained from a second resource different than the first resource. Data\n     is provided that causes the identified search results and the knowledge\n     panel to be presented on a search results page. The knowledge panel\n     presents the identified content in a knowledge panel area that is along\n     side at least a portion of the search results.\n", "citations": ["5944769", "5964831", "8005842", "8468153", "9047278", "9268820", "9454611", "20010038395", "20060179069", "20070233656", "20080133483", "20080147590", "20080147709", "20080313147", "20090070322", "20090132581", "20090192968", "20090271368", "20090307183", "20090327268", "20100198837", "20100306166", "20120059838", "20130311458"], "related": ["15001533", "13566489", "61515305"]}, {"id": "20160352743", "patent_code": "10326768", "patent_name": "Access control for enterprise knowledge", "year": "2019", "inventor_and_country_data": " Inventors: \nVerWeyst; Brent (San Francisco, CA), Cochran; Martin James (San Francisco, CA), Sivathanu; Muthian (Chennai, IN)  ", "description": "BACKGROUND\n This specification relates to access control for enterprise information, personal assistance based on enterprise information and personal information, and searches associated with the enterprise information.\n Enterprises can have access to a large number and wide variety of resources (e.g., documents).  Each of the resources may have access control lists that are associated with the resource to dictate who can view and/or alter the resource.  Members\nof an enterprise may have difficulty in finding and searching for necessary information because the resources are not connected or associated with one another in any meaningful way.  Furthermore, and for similar reasons, electronic personal assistants\ntypically cannot leverage enterprise information in meaningful ways to satisfy users' informational needs.\nSUMMARY\n This specification describes technologies relating to access control for enterprise information, personal assistance based on enterprise information and personal information, and searches associated with the enterprise information.\n In general, one innovative aspect of the subject matter described in this specification can be embodied in a method that includes the actions of receiving resources of an enterprise, each resource having a respective access control list\nspecifying access privileges to the resource for one or more members of the enterprise, and the resources including data describing entities related to the enterprise and relationships among the entities.  Entity facts may be identified from the entities\nfrom the resources of the enterprise, each entity fact describing at least one feature of the entity, wherein the features include corresponding relationships between the entities.  For each entity fact, a determination of an entity fact access control\nlist may be made from the access control list of each resource, an entity fact access control list, and the data describing the entities, entity facts and the respective entity fact access control lists may be stored in a searchable index, wherein each\nentity fact is associated with its corresponding entity fact access control list.  Also, the method may include providing, to each of the members of the enterprise, access privileges to the data describing the entities and the entity facts in the\nsearchable index according to the respective entity fact access control lists.\n Particular implementations of the subject matter described in this specification can be implemented so as to realize one or more of the following advantages.  In some enterprise structures, it can be beneficial to extract information from the\nresources and compile that information while still respecting the access control list of the resource with the extracted information, as this will cause the extracted information to be easier to obtain by members of the enterprise while also protecting\nthe access control lists associated with the resources.  Also, it may be beneficial to have a computer-implemented person assistant to perform actions, where the computer-implemented personal assistant uses information from each member and extracted\ninformation from the resources while respecting the access control lists of the resources.\n The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below.  Other features, aspects, and advantages of the subject matter will become\napparent from the description, the drawings, and the claims. BRIEF DESCRIPTION OF THE DRAWINGS\n FIG. 1 is a block diagram of an example environment in which enterprise data is integrated.\n FIG. 2 is a block diagram of a data system.\n FIG. 3 is a flow diagram of an example process for providing access privileges to entity facts.\n FIG. 4 is a block diagram of a portion of an example enterprise knowledge graph.\n FIG. 5 is a flow chart of an example process for providing search results to an enterprise member.\n FIG. 6 is a flow chart of an example process for ranking and providing the search results to an enterprise member.\n FIG. 7 is a block diagram that incorporates computer-implemented assistants into the example environment structure of FIG. 1.\n FIG. 8 is a flow chart of an example process for providing enterprise actions by a computer-implemented assistant.\nDETAILED DESCRIPTION\n Overview\n FIG. 1 is a block diagram of an example environment 100 in which enterprise data is integrated.  The example environment 100 includes a network 102, e.g., a local area network (LAN), wide area network (WAN), the Internet, or a combination of\nthem, connects enterprise information system 110, application programming interface 120, and data system 130.  The network 102 can be accessed over a wired and/or a wireless communications link.  For example, mobile computing devices, such as smartphones\ncan utilize a cellular network to access the network.\n Enterprise information system 110 may include one or more information sources 112.  As seen in FIG. 1, information sources 112 are identified as 112a, 112b, .  . . , 112N, which include one or more sub-system, application, program, or database,\namong others.  For example, information sources 112 may be an internal document database for an enterprise, an enterprise intranet, an enterprise email application, or any third party information source that the enterprise information system 110 is\nconnected to (e.g., internet, software application).  Each of the information sources 112 include resources 115, which in FIG. 1, may be identified as 115a1, 115a2 .  . . 115an .  . . 115n1 .  . . 115NN.  Resources 115 may be different for each\ninformation source 112.  For example, resources 115 may be a document stored within the enterprise information system 110, intranet URL, email message, internet URL, or any similar type of informational content (e.g., information stored on third party\napplication that the enterprise information system 110 has access to).\n As used in this document, an \"enterprise\" is public or private legal entity to which members belong and for which the members undertake projects, tasks, and other actions on behalf of the enterprise.  An example enterprise may be a privately\nheld company, a publically traded company, and the like.  \"Enterprise information\" is at least information that is authored, owned, curated or otherwise controlled, either partially or exclusively, by the enterprise.\n Each resource 115 has an associated access control list (ACL) that indicates which members of the enterprise are allowed to access and/or modify that particular resource.  The access control list for each resource may indicate access privileges,\nsuch as an owner (or creator) of the resource, read access, write access, and delete access, among others.  The access control list for each resource may define access and/or modification privileges to individual members and/or one or more groups of\nmembers (e.g., sales department of an enterprise).  Each resource 115 may also allow for versioning of the particular resource (e.g., version 1 of resource 115a1 and version 2 of resource 115a1), and the access control list for each version of the\nparticular resource may be different from one another.  Also, a particular resource may be created or defined as a \"parent\" or \"child\" of another resource.  The access control list for any child resource will inherit the access control list for its\ndefined parent resource; however, in some implementations, this is only a default access control list and the access control list for the child resource and/or parent resource may be modified without affecting the other.  In variations of this\nimplementation, a child resource may have additional access control list requirements beyond those inherited from the parent resource.\n Resources 115 may be provided over network 102 to a data system 130 via an application programming interface (API) 120.  API 120 may be designed by the enterprise, the operator of data system 130, or a third party (e.g., developed by a third\nparty for a specific application or a more generally applicable API).  API 120 may accept different access control list types that are used or presented in information sources 112 that include more or fewer different definitions or fields (e.g., owner,\nread or write access, parent, child, versions, etc.).  Additionally, API 120 allows for multiple identifications for each member of the enterprise.  For example, if a member has an identification of \"Jane Doe\" for information source 112a and has\nidentification \"JaneDoe@enterprise.com\" for information source 112b, and the enterprise information system 110 has provided a description that both of these identifications relate to the same member, the API 120 can link both of these identifications\nwith that particular member.  Additionally, API 120 enables members to be classified in one or more group, and if the group is updated, added, or removed on at the enterprise information system 110, then the API 120 may sync with the enterprise\ninformation system 110 to reflect the changes at the API 120.\n The resources 115 are received through the API 120 by the data system 130.  A database schema implemented by data system 130 provides for the data system 130 to understand the information provided by enterprise information system 110 via the API\n120 in order to integrate and structure the information.  Further, the API 120 may tag the information associated with resources 115 according to the predefined schema in order for the data system 130 to integrate the information.  In the current\nimplementation, data system 130 includes an integration system 132, a data storage 134, and an indexer 136.  Integration system 132 may be a database, server, module, application, or combination thereof, among others.  Information from the integration\nsystem 132 is stored in data storage 134, and indexer 136 indexes the information stored in data storage 134 for the information to be searched by the members of the enterprise, which will be described below.\n Entity Fact Access Control\n FIG. 2 is a block diagram of data system 130.  In some implementations, integration system 132 receives a plurality of resources 115 that each have an access control list specifying access privileges to the resource 115 for one or more members\nof the enterprise.  Each resource includes data and/or information describing entities 210, which may be identified as entities 210a, 210b .  . . 210N in FIG. 2.  The entities 210 are related to the enterprise and relationships between the entities. \nEntities 210 of the enterprise may be, for example, people (or members), positions, projects, departments, sales information, performance evaluations, or any other information that is in the resources 115 of the enterprise.  More generally, in some\nimplementations, entities are topics of discourse, concepts or things that each have a distinct and independent existence and that can be referred to by a text fragment, e.g., a term or phrase, and are distinguishable from one another, e.g., based on\ncontext.  An entity may thus be a physical or conceptual realization having a factual definition and attributes.\n Additionally, integration system 132 derives entity facts 220, which may be identified as entity facts 220a, 220b, .  . . 220N in FIG. 2 from the entities 210 of the resources 115.  Each entity fact 220 describes at least one feature of an\nentity 210, where the features describe corresponding relationships between the entities 210.  For example, a resource 115 (e.g., a document) of the enterprise may identify the names and positions of those in management of the enterprise where \"Jane Doe\"\nmay be in management of the enterprise in the position of \"Vice-President of Sales.\" In the current example, \"Jane Doe\" and \"Vice-President of Sales\" are entities, and an entity fact 220 may be defined in the integration system 132 that creates a\nrelationship between the entities, where the entity relation may be expressed as a factual relation \"Jane Doe\"--\"Vice-President of Sales.\" A variety of techniques may be used to identify entity facts.  These include linguistic parsers that can be issued\nto determine, e.g., nouns and related attributes; database parsers that can be used to determine keyed relations between date fields; and so on.\n Further, integration system 132 determines an entity fact access control list 230, which may be identified as 230a, 230b, .  . . 230N in FIG. 2.  An entity fact access control list 230 is determined for each entity fact 220, which is determined\nfrom the access control list of each resource 115.  In some implementations, the access control list of a resource is inherited by each entity fact derived from the resource.  For example, resource 115a1 may be a document with salary information of the\nmembers of the enterprise.  Resource 115a1 may include \"Jane Doe\" as entity 210a and \"$50,000\" as entity 210b in a separate column (indicating Jane Doe's salary).  As salary information is personal and sensitive information, the access control list for\nresource 115a1 may only include the Director of Human Resources, for example, \"Mary.\" Resource 115b1 may be an intranet page that breaks down the salary amount of members by percentages, and may include \"Jane Doe\" as entity 211a and \"Top 5%\" as entity\n211b (indicating Jane Doe's income percentage).  The access control list for this information may provide access to all members of the enterprise (or a larger group of the members than resource 115a1).  Entity facts may then be identified, and in the\npresent example, may be \"Jane Doe\" and \"$50,000\" salary as entity fact 220a, and \"Jane Doe\" and \"Top 5%\" as entity fact 220b, which are derived from entities 210a, 210b, 211a, and 211b of resources 115a1 and 115b1.\n Entity fact access control lists 230 may then be determined based on the resource from which the respective entity fact was derived.  For example, entity fact access control list 230a for entity fact 220a will only include Mary because resource\n115a1, from which entity fact 220a was derived, included Mary in the access control list.  Entity fact access control list 230b for entity fact 220b will include all members of the enterprise because resource 115b1 included all members of the enterprise\nin the access control list.  Based on the current example, Mary would have access to entity fact 220a and entity fact 220b because she is included in the access control list for both resource 115a1 and 115b1; however, other members, for example, \"Alvin,\"\na sales representative, would not have access to 220a because he is not part of the access control list for the underlying resource 115a1, but he would have access to entity fact 220b because he has access privileges to resource 115b1 based on the access\ncontrol list associated with the document from which the fact 220b was derived.\n The information sent to and obtained by the integration system 132 is stored in data storage 134.  Data storage 134 stores, for example, data describing the entities 210, entity facts 220 and the respective entity fact access control lists 230,\nwhere each entity fact 220 is associated with its corresponding entity fact access control list 230.  Indexer 136 then indexes the information in data storage 134.  The members of the enterprise may then use a search engine 138 to, for example, search\ndata storage 134 to access entity facts 220 of the resources 115 of the enterprise.  Each member of the enterprise is provided access privileges to the data describing the entities 210 and the entity facts 220 according to the respective entity fact\naccess control list 230.\n Any appropriate search process can be used to realize the search engine 138.  The search engine 138 can use, for example, one or more of an information retrieval (IR) score algorithm, an authority score algorithm, or a combination of relevance\nand authority algorithms.\n FIG. 3 is a flow chart of an example process 300 for providing access privileges to entity facts 220 based on respective entity fact access control lists 230 obtained from underlying resources 115 of the enterprise.  The process 300 can, for\nexample, be implemented by the data system 130.  In some implementations, the operations of the example process 300 can be implemented as instructions stored on a non-transitory computer readable medium, where the instructions cause a data processing\napparatus to perform operations of the example process 300.\n Resources 115 of an enterprise, each resource having a respective access control list specifying access privileges to the resource for one or more members of the enterprise, and the resources including data describing entities 210 related to the\nenterprise and relationships among the entities 210 are received (302).  Each information source 112 of the enterprise information system 110 includes resources 115, and the resources 115 may be provided over network 102 to a data system 130 via the API\n120.  Each resource 115 includes an access control list that indicates which members of the enterprise are allowed to access and/or modify that particular resource, and the access control list information for each resource 115 is received along with each\nrespective resource.  Each resource 115 includes entities 210 that are information related to the enterprise.  For example, a resource 115a1 of the enterprise that identifies the names and positions of those in management of the enterprise where \"Jane\nDoe\" may be in management of the enterprise in the position of \"Vice-President of Sales,\" may include \"Jane Doe\" as entity 210a and \"Vice-President of Sales\" as entity 210b.\n Entity facts 220 of the entities 210 from the resources 115 of the enterprise are identified where each entity fact 220 describes at least one feature of the entity 210, and the features include corresponding relationships between the entities\n210 (304).  For example, from the example above, an entity fact 220 may be a relationship between \"Jane Doe\" and \"Vice-President of Sales,\" where the entity fact 220 is \"Jane Doe\" is the \"Vice-President of Sales.\" As such, a feature of \"Jane Doe\" is\n\"Vice-President of Sales,\" and a feature of \"Vice-President of Sales\" is \"Jane Doe.\"\n For each entity fact 220, an entity fact access control list 230 is determined from the access control list of the resource from which the entities 210 of the entity fact 220 were derived (306).  In the current implementation, the access control\nlist of resource 115a1, which included entities 210a and 210b, will be used as the entity fact access control list 230 of entity fact 220.  For example, if the access control list for resource 115a1 is the \"Sales Department,\" then the members of the\nenterprise that are included in the \"Sales Department\" group, as defined at the enterprise information system 110, will have access privileges to the entity fact 220 of \"Jane Doe\" is the \"Vice-President of Sales.\"\n Data describing the entities 210, entity facts 220, and the respective entity fact access control lists 230 are stored in data storage 134 (308).  Indexer 136 then indexes the information in data storage 134.  The members of the enterprise may\nthen use indexer 136 to, for example, search data storage 134 to access entity facts 220 of the resources 115 of the enterprise.\n Each member of the enterprise is provided access privileges to the data describing the entities 210 and the entity facts 220 according to the respective entity fact access control list 230 in an indexer 136.  For example, in the example above,\nif \"Alvin\" were a member of the \"Sales Department,\" then Alvin would have access privileges to the entity fact 220 of \"Jane Doe\" is the \"Vice-President of Sales\" because Alvin is included in the entity fact access control list 230.\n Members of the enterprise can query the search engine 138 for information and/or one or more resources 115.  A query provided by a member of the enterprise will search indexer 136 for data, including resources 115, entities 210, and entity facts\n220 that are stored in data storage 134, which may be relevant to the member's query.  The entity fact access control lists 230, in some implementations, are applied to the query provided by the member, and the member may only receive information,\nincluding resources 115, entities 210, and entity facts 220, that the member has access privileges to according the access control lists associated with the information.  Additionally, in some implementations, member information may also be provided from\nthe enterprise information system 110 to the data system 130.  Member information may include, for example, the role or position of one or more of the members in the enterprise, an organizational structure of the enterprise, the contacts of one or more\nmembers in the enterprise (e.g., email or telephone), the member's relationship with the provided query, as described below.\n Information may be determined to be relevant to the member's query based on how relevant a resource, entity, and/or entity fact is determined to be to a member's query.  Additional analysis and criteria may be applied to the member's query to\ndetermine what is to be provided in response to the member's query.  For example, the member's title, position, or group within the enterprise can factor into what is relevant to the member as well as the content, including the source information 112 and\nresources 115, the member interacts with and uses.  Additionally, activities of the member and the time and date the query was provided can factor into the relevancy.  For example, interests related to the query member and information in the query\nmember's email and calendar resources can factor into the relevancy.  The search engine 138 may determine and analyze the relevancy to a member's query by the use of a knowledge graph; however, other methods of determining relevancy and relationships\nbetween resources 115, entities 210, and entity facts 220 may be used.\n In the process of determining that an entity 210 or entity fact 220 is relevant to the query provided by a member, the search engine 138 also determines the query member's relationship with one or more entity 210 and entity fact 220.  In some\nimplementations, the search engine 138 may first determine whether the provided query corresponds to, or otherwise is determined to be relevant to, at least one of an entity 210 and entity fact 220.  The search engine 138 then forms a relation between\nthe provided query and the entities 210 and entity facts 220 determined to be relevant to the provided query.  The search engine 138 may also determine the query member's relationship with the entities 210 and entity facts 220 determined to be relevant\nto the provided query.\n FIG. 4 is a block diagram of a portion of an example enterprise knowledge graph representation 400 of enterprise relationship information derived by the integration system 132.  The knowledge graph has nodes and edges.  Each node in the\nknowledge graph represents resources 115 and entities 210, and pairs of nodes in the knowledge graph are connected by one or more edges.  Each edge representing a relationship dimension that defines an entity fact 220 between two resources 115, two\nentities 210, or one of each of a resource 115 and an entity 210 represented by the pair of nodes, or several edges represent a series of relationships that connect two nodes by one or more intermediate nodes.  As previously stated, the information to\nform an entity fact 220 between entities 210 is determined from the content provided in resources 115.  As shown in FIG. 4, the edges are unidirectional, but in other variations the edges may be bidirectional.\n For example, the enterprise knowledge graph 400 includes nodes 410 and 414 that are connected by edge 412.  In the present example, node 410, a member of the enterprise, \"Jane Doe,\" is defined to be connected, by edge 412, with node 414, \"Sales\nDepartment.\" Additionally, node 410 is determined to be connected, by edge 416, with node 418, \"Project A.\" Node 422, \"Karen,\" another \"Sales Department\" employee, as shown by the edge 424 between node 422 and node 414, is also connected by edge 420 to\nnode 418.  In the present example, node 410 is indirectly connected to node 422 via two separate relationships (node 414 and node 418).  Node 410 is also connected to another project, \"Project B,\" at node 428 by edge 426, and node 410 is connected to\n\"Engineering Department,\" at node 432, by edge 430.  For example, \"Jane Doe,\" at node 410, may have corresponded with the \"Engineering Department\" in the past to determine if a project was feasible or to check on the status of an event or project.  As\nsuch, an information source 112 (e.g., enterprise email) would include a resource 115 (e.g., email message) that would include the entities of \"Jane Doe\" and \"Engineering Department,\" where an entity fact 220 could describe a relationship between the\nentities.  Further, another enterprise member, \"Karen,\" who works in the Engineering Department, at node 436 is connected to node 432 by edge 434.\n Based on the exemplary enterprise knowledge graph 400, if enterprise member \"Jane Doe,\" the entity at node 410, were to provide a search query \"Karen,\" the Data System 130 would analyze the enterprise knowledge graph 400 to determine what is\nrelevant to the query.  For example, based on the connection between \"Jane Doe\" at node 410 and \"Karen\" at node 422 via node 418, \"Project A,\" and node 420, \"Sales Department,\" contact information or meeting options of \"Karen\" at node 422, and resources\n115 (e.g., shared documents or email messages) between identifications associated with \"Jane Doe\" and \"Karen\" may be determined to be relevant to the query provided.  However, contact information or other information pertaining to \"Karen\" at node 436 may\nbe determined to be less relevant or not relevant at all based on the connection shown between \"Jane Doe\" at node 410 and \"Karen\" at node 436.\n Additionally, based on the exemplary search query \"Karen,\" the data system 130 may also determine information and resources 115 related to \"Project A,\" shown at node 418, are relevant to the query based on the connection of both node 410 and\nnode 422 to \"Project A.\" However, information and resources 115 related to \"Project B,\" shown at node 428, may not be determined to be relevant based on the lack of a connection between node 428 and node 422.\n Further, the enterprise knowledge graph 400 includes \"Susan\" at node 440 that is connected to \"Jane Doe\" at node 410 by edge 438.  \"Susan\" may be, for example, a sales lead that \"Jane Doe\" has previously met with and/or exchanged correspondence\nwith.  \"Susan,\" in the current example, at node 440 is connected to \"Company A\" at node 444 by edge 442, as she may be, for example, an account manager at \"Company A.\" Additionally, node 444 is connected to \"Clients\" at node 448 of the enterprise by edge\n446, and node 448 is connected to the \"Sales Department\" at node 414 by edge 450.  For example, if \"Mark,\" seen at node 458, were another account manager for \"Company A,\" and in Jane Doe's previous email exchange with Susan, she told Jane Doe that her\nsales proposal needed to be discussed with \"Mark\" for approval, edge 451 may be created between nodes 440 and 458 based on the entities provided in resources 115 (e.g., enterprise email).\n Based on the exemplary enterprise knowledge graph 400, if enterprise member \"Jane Doe,\" the entity at node 410, were to provide a search query \"Company A,\" the Data System 130 would analyze the enterprise knowledge graph 400 to determine what is\nrelevant to the query.  For example, based on the connection between nodes 410 and 440, documents, emails, and biographical and contact information related to \"Susan\" may be determined to be relevant.  Additionally, based on the exemplary sales lead\nemail where Susan advised Jane Doe to speak to Mark, biographical and contact information for Mark may also be determined to be relevant.\n Further, for example, Jane Doe has an upcoming trip scheduled to \"Atlanta,\" seen at node 454 and connected by edge 452 to node 410.  If, for example, the information related to Mark at node 458 indicates that Mark's office is in \"Atlanta,\" then\na connection can be made between Atlanta node 454 and Mark node 458 by edge 456.  As such, in the results provided to enterprise member Jane Doe's query of \"Company A,\" the data system 130 may determine that it is relevant to provide enterprise member\nJane Doe with an option to compose an email to Mark, call Mark, or schedule a calendar invitation to meet with Mark.  As such, the entity facts 220 created based on entities 210 and resources 115, in some implementations, can be opportunities for current\nor future action.  The query results as described above are merely exemplary, and other types and methods of providing query results may be included.  In some implementations, the enterprise knowledge graph 400 may include information related to an\norganizational structure of the enterprise, which can be obtained from one or more resource 115, where the positions and members holding those positions are relationally structured in the enterprise knowledge graph 400.\n FIG. 5 is a flow chart of an example process 500 for providing search results to an enterprise member based on a query provided by the enterprise member.  The process 500 can, for example, be implemented by the data system 130.  In some\nimplementations, the operations of the example process 500 can be implemented as instructions stored on a non-transitory computer readable medium, where the instructions cause a data processing apparatus to perform operations of the example process 500.\n A search query provided by a member of the enterprise is received by the data system 130 (502).  In some implementations, the enterprise information system 110 may provide an interface to the member of the enterprise to receive queries that are\ninput by the member.  The interface may be provided via, for example, an application or program that is in communication with the data system 130, or in some implementations, the interface may communicate to the data system 130 directly.\n Once the query is received, the search engine 138 may search the data storage 134, which stores the data received from the enterprise information system 110 and that has been indexed (504).  As previously described, the data system 130 receives\nresources 115, including member information, and the resources 115 include data describing entities 210 and entity facts 220, where the entity facts 220 are identified from the entities 210 of the resources 115.  Each entity fact 220 describes at least\none feature of an entity 210, where the features describe corresponding relationships between the entities 210.  Additionally, the indexer 136 includes data defining access privileges to the data entity fact access control list 230 for each entity fact\n220, which is determined from the access control list of each resource 115, as previously described.\n The entity facts 220 that are accessible to the member are determined based on the entity fact access control lists 230 (506), as previously described.  For example, only facts and resources for having an access control list that specifies the\nmember has access to the fact or resource are provided.  Other facts and resources to which the member does not have read access are not identified for the member.\n Further, search result data including data describing entities 210 and entity facts 220 relevant to the query are determined based on member information of the member and entity facts 220 that are accessible to the member (508).  For example,\nentities 210 and entity facts 220, as previously described, which are determined to be relevant to the member's query are identified.  Additionally, the member's position or relationships within the enterprise can factor into what is relevant to the\nmember query as well as the source information 112 and resources 115 the member interacts with and uses.  Activities of the member and the time and date the query was provided can determine or factor into the relevancy.  Further, an enterprise knowledge\ngraph representation, like enterprise knowledge graph 400, may be used to determine the relevancy of resources 115, entities 210, and entity facts 220 of enterprise relationship information.\n Search results to the query provided by the enterprise member are provided based on the search result data (510).  The search result data may include entities 210, entity facts 220, the underlying resources 115 from which the entities 210 and/or\nentity facts 220 are derived, or a combination thereof.  The underlying resources 115 from which the entities 210 and/or entity facts 220 are derived may be embedded in the entity 210 and/or entity fact 220 that is presented.  For example, the underlying\nresource 115 may be provided as a link (e.g., hyperlink), drop down selection box, hover selection, among others.  In some implementations, if an entity fact, for example 220a, is derived from more than one resource, for example both resource 115a1 and\nresource 115b1, then the embedded underlying resource for the entity fact 220a may include one or both (or more) of the resources from which the entity fact 220a was derived.  In implementations where only one of the underlying resources 115 is provided,\nthe selection of the underlying resource may be based on the most relevant resource of each of the underlying resources to the member.  For example, if the query member is part of the sales department, and underlying resource 115a1 is stored in the sales\ndepartment location of the database while underlying resource 115b1 is stored in the engineering department location of the database, resource 115a1 may be determined to be most relevant to the member.  Additionally, in some implementations, query\nsuggestions may be provided to the member based on the query that was provided or is in the process of being provided (e.g., the member is currently inputting) and/or the search results.  The data system 130 may be configured to interpret and understand\nnatural language input.\n The presentation of the search results may be provided as a list of the search result data, and in some implementations one or more knowledge panel may be provided in the search results.  A knowledge panel may be presented inline or adjacent to\nother search results for a received query or in place of the other search results.  The knowledge panel may provide information related to an entity 210 and/or entity fact 220 referenced by a search query.  In some implementations, a knowledge panel may\nprovide a summary of information for the entity 210 and/or entity fact 220.  For example, a knowledge panel for \"Jane Doe,\" as provided above at node 410, may include an enterprise profile picture of Jane Doe, the department that she works in, her\ncontact information, a biography, among other content (including a link to Jane Doe's enterprise webpage).  A knowledge panel for a particular entity 210 and/or entity fact 220 may also, or alternatively, include information about other entities 210\nand/or entity facts 220 that have been identified as related to the particular entity 210 and/or entity fact 220.  For example, if the enterprise member Jane Doe referenced above is the sales account manager for \"Company A,\" the knowledge panel for the\nenterprise member Jane Doe may include information about Company A, like contact and location information.  For example, the information about Company A may also include information about a sales lead contact at Company A, for example \"Susan\" (at node\n440).  Information about Susan may also include a Company A profile picture of Susan, the department that she works in, her contact information, a biography, among other content (including a link to Susan's Company A enterprise webpage).\n Entity Fact Search Processing\n FIG. 6 is a flow chart of an example process 600 for ranking and providing the search results to an enterprise member based on a query provided by the enterprise member.  The process 600 can, for example, be implemented by the data system 130. \nIn some implementations, the operations of the example process 600 can be implemented as instructions stored on a non-transitory computer readable medium, where the instructions cause a data processing apparatus to perform operations of the example\nprocess 500.\n In some implementations, the search result data may be ranked based on the relevance of the entities 210 and entity facts 220 in relation to the query and member information of the query member (602).  As described above, the relevant entities\n210 and entity facts 220 may be determined, and then results may be ranked or organized based on the level of relevance based on the query member and query provided.  The search result data may then be provided in the order of the rankings to the member\nof the enterprise (604).\n In some implementations, a quality score may be determined for each entity 210 and entity fact 220 based on the query provided by the member of the enterprise.  The quality score may indicate the level of relevance for each entity 210 and entity\nfact 220 to the query.  The quality score can be member specific, a partial or total aggregate, or a combination thereof.  The enterprise knowledge graph, as described in FIG. 4, may be used and factored in to the quality score to associate the relevance\nof the entities 210 and entity facts 220 in relation to the query and member of the enterprise providing the query.  Additionally, the quality score for the entities 210 and entity facts 220 can be based on explicit indications and/or implicit\nindications by the member of the enterprise or a collection of members of the enterprise.  For example, a member of the enterprise may explicitly indicate they prefer information (e.g., entities 210 and entity facts 220) that are from particular\nresources 115 or information sources 112 (e.g., the internet, email, or a sales department database).  Other explicit indications may include a preference of resources 115 the member is an author of, resources 115 created by a member in the member of the\nenterprise's department or group, or interests expressed by the user (e.g., the stock market price of the enterprise), among others.\n Implicit indications may be any type of indication that is provided without the member's explicit input.  As already described above, in order to determine the relevancy of entities 210 and entity facts 220, different information related to the\nmember's interactions with the enterprise system 110 and data system 130 can provide implicit indications.  For example, implicit indications may include the member's position, group, or department within the enterprise, the information sources 112 and\nresources 115 the member has recently or regularly interacted with, and activities of the member.\n Additionally, implicit indications can be provided by the member or a collection of members of the enterprise based on how the member or collection of members interacts with the search results that are provided to the query.  For example, in\nsome implementations, parameters may be applied to the quality score.  Exemplary parameters may be one or more of a mean reciprocal rank, a mean precision, an average click position, a click through rate, and an abandonment rate, among others.  The mean\nreciprocal rank can be a statistical measure for evaluating any process that produces a list of possible query results to a sample of queries, ordered by probability of correctness.  The reciprocal rank of a query response is the multiplicative inverse\nof the rank of the first correct answer, and the mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries.  Average click position may be the average rank in the order of provided query search results that the member\nselected.  Click through rate may be the rate that a particular query search result is selected, and abandonment rate may be a number of abandoned queries divided by the number of queries related to one or more of the same or similar queries or the same\nor similar queries provided by a member a collection of members.  An abandoned query is a query that has no subsequent query search result selection and does not have any subsequent refinements to the query.  These are only exemplary parameters and other\nparameters or modifications to the parameters above may be provided.\n Further, in some implementations, click result information can be included in the quality score.  For example, if the member or average of collection of members, select a search result with a long click (e.g., the click lasting longer than a\nthreshold period of time), then the search result in relation to the search query may be determined to be more relevant than if a search result is selected with a short click, where it could be determined that a certain percentage of time the search\nresult selected with the short click was inadvertently selected.  Additionally, search results that are not clicked to be selected may be determined to be less relevant to the query provided.\n The relevancy, including the quality score, of each search result to each query provided may auto-tuned, that is modified and/or adjusted, by the data system 130 based on the member's or collection of member's interaction with the search\nresults.  For example, if the member provides a query for \"Company A\" and selects the second search result that is related to an entity fact 220 about \"Susan\" at Company A, then the quality score for that entity fact 220 may increase while quality scores\nfor other entities 210 and entity facts 220 provided in the search results, that were determined to be relevant, may decrease or otherwise adjust.  Additionally, enterprise acronyms and synonyms associated with entities 210 and entity facts 220 may be\ndetermined by the data system 130 based on the auto-tuning process.\n In some implementations, the auto-tuning process may be performed automatically by the data system 130 without human input or intervention after the auto-tuning process is established.  Many enterprise data systems 110 contain sensitive and\nsecret information that must remain confidential within the enterprise; therefore, intervention or input at the data system 130 may not be provided in order to preserve the confidentiality and secrecy of information within the enterprise.  In some\nimplements, the relevancy determination, including the quality score, may be based on a static and/or dynamic algorithm, formula, heuristic, or a combination thereof.\n Enterprise Assistants\n The entity facts and related information can also be used for other information support operations for members of the enterprise.  One example expanding the role of computer-implemented assistants to use enterprise information, subject to the\naccess control lists.\n FIG. 7 is an example environment 700 that incorporates computer-implemented assistants into the example environment structure of example environment 100.  Additionally, in some implementations, one or more members of the enterprise can use the\ncomputer-implemented assistants 710, as seen in example environment 700.  As seen in FIG. 7, the one or more computer-implemented assistants 710a, 710b, .  . . , 710N, are communicatively connected to the enterprise information system 110, data system\n130, member information system 720 (seen in FIG. 7 as 720a, 720b, .  . . , 720N), and with each other computer-implemented assistant 710, or combinations thereof.  However, such communication is not required, and in some embodiments, communication with\nmore or fewer devices and systems may be included.  Member information system 720 can include information that is not described or included in the resources 115, but is accessible to the particular member associated with a particular computer-implemented\nassistant.  For example, a member's personal email, personal calendar, or personal travel information may be included in the member information system 720.  The member information system 720 may be implemented on one or more devices (e.g., computer,\nmobile device, smart phone, tablet), and the storage of the member personal information may be on one or more of the devices, on different devices, or in a cloud storage environment, among others.  The computer-implemented assistant 710 is configured to\nbe implemented in any compatible software application, and the computer-implemented assistant 710 is not required to be used on only one application and/or one user device.  Additionally, as seen in FIG. 7, in some implementations, the\ncomputer-implemented assistant 710 may communicate with data system 130 without the use of API 120.\n As previously described, data system 130 accesses and integrates enterprise resources 115 that include data describing entities 210 and entity facts 220.  Each entity fact 220 includes an entity fact access control list 230 that is provided from\nthe underlying resource 115 from which the entity fact 220 was obtained, and each member has access privileges to resources 115, entities 210, and entity facts 220 according the respective entity fact access control lists 230.  Additionally, in some\nimplementations, member personal information may also be provided from the member information system 720 to the enterprise information system 110 and/or the data system 130.\n The computer-implemented assistant 710 for each member of the enterprise can determine enterprise information specific to the member, including entities 210 and entity facts 220, and integrate the member personal information with the enterprise\ninformation specific to the member.  For example, the computer-implemented assistant 710 can determine a calendar for the particular member based on the member personal information and the enterprise information specific to the member.  Based on the\ncomputer-implemented assistant 710 combining this information, the computer-implemented assistant 710 can determine enterprise actions to perform for the member, where the enterprise actions are actions specific to the member's enterprise-related\nresponsibilities.  For example, the enterprise-related responsibilities include work meetings, arrangements, and travel, among others, but may not include scheduling a personal vacation.\n In order to provide enterprise actions for the member, the computer-implemented assistant 710 may analyze and determine data related to one or more locations a user device has visited or is scheduled to visit, one or more enterprise member user\ndevices that the user device has interacted with, interests of the member, industry information relevant to the member, and calendar information of the member, among others.  This exemplary data may be provided by one or more user devices of the member,\nthe enterprise information system 110, the data system 130, the member information system 720, or a combination thereof.\n Further, in some implementation, public resources 730 (seen in FIG. 7 as 730a, 730b, .  . . , 730N) that are published by entities external to the enterprise may be included in the information the computer-implemented assistant 710 uses to\nperform enterprise actions for the member.  The computer-implemented assistant 710, as previously described, may communicate with the data system 130, and the schema provided at the data system 130 may also be used by the computer-implemented assistant\n710 in order to sort and understand the information provided by the public resources.  In some implementations, the API 120 is used to tag and index content and information received by the computer-implemented assistant 710.  Public resources may be, for\nexample, information obtained on the internet, a travel database system, a professional networking application, among others.  In the current implementation, the computer-implemented assistant 710 may determine a subset of public information from the\npublic resources that is relevant to the particular member.  For example, when the public resource is a travel database system and the member has a scheduled meeting in \"Atlanta,\" the computer-implemented system 710 may determine that the subset of\npublic information that is relevant to the particular member is the flights or other transportation modes from the current city the member is located to Atlanta, Ga.  The computer-implemented assistant 710 may determine the subset of public information\nthat is relevant to the particular member based on the member personal information, enterprise information specific to the member, and the entity facts 220, among other information.  After determining the subset of public information relevant to the\nparticular member, the computer-implemented assistant 710 may determine enterprise actions to perform for the member.\n Enterprise actions to perform for the member can be context specific and may include presenting insights or information to the member, presenting opportunities (e.g., a business lead) to the member, or a completion of one or more task for the\nmember.  For example, an enterprise action may include declining an invitation for a meeting.  Declining could be based on a multitude of reasons, including the member having a vacation scheduled (even if this is only listed on the member's personal\ncalendar) or a scheduling conflict.  Additionally, enterprise actions may include rescheduling a conflicting meeting, booking a flight, booking a hotel room, presenting local transportation options, finding restaurants suitable to the member, creating\nreservations at restaurants suitable to the member, presenting a daily agenda (where some items can be automatically generated), among others.\n Additionally, an enterprise action may include providing relevant information (e.g., a document or slide presentation) to the member at the time of a scheduled meeting or event.  The relevancy of information to a scheduled meeting or event may\nbe based on the information available to the computer-implemented assistant 710, including member personal information, enterprise information specific to the member, the entity facts, and the subset of public information.  The computer-implemented\nassistant 710 may present to one or more of the user devices of the member the relevant information, including an enterprise resource, prior to or at the time of the event or meeting scheduled.  For example, when the member has a scheduled meeting with\n\"Tom,\" another member of the enterprise, and Tom has scheduled the meeting to discuss a particular document, the computer-implemented assistant 710 can determine the particular document is relevant to the meeting--for example, based on the title of the\nmeeting, from an email exchange between Tom and the member, among others), and provide the document to the member at the time of the meeting or at a time prior to the scheduled meeting.  Further, in some implementations, the computer-implemented\nassistant 710 may provide informational material, including public resources, to the one or more user devices of the member if that material is determined to be relevant to a scheduled meeting or event.  For example, the member may have a meeting\nscheduled to discuss a complicated technical topic, and the computer-implemented assistant 710 may provide technical background information or other type of information to the member prior to the meeting.\n In some implementations, the enterprise actions include presenting opportunities (e.g., a business lead) to the member.  For example, if the member is in the sales department of the enterprise and has a scheduled trip to \"Atlanta,\" where a sales\nlead is also visiting (e.g., known based on a previous email exchange between the member and the sales lead), the computer-implemented assistant 710 may present the one or more user devices of the member with a notification that the sales lead will also\nbe in Atlanta at that time, and the computer-implemented assistant 710 can also ask or prompt the member if they would like to schedule a meeting or otherwise contact the sales lead while in Atlanta.  After each enterprise action is determined to be\nperformed, the computer-implemented assistant 710 causes the one or more user devices of the member to present a notification describing the enterprise action to the member.\n The computer-implemented assistant 710, in some embodiments, can coordinate between the computer-implemented assistant 710 of one or more other member of the enterprise.  The computer-implemented assistant 710 of the member can determine that an\nenterprise action involves at least one other member of the enterprise.  For example, if the computer-implemented assistant 710 of the member scheduled a business trip after planning a meeting with another member of the enterprise, the\ncomputer-implemented assistant 710 of the member can determine that the business trip that has been scheduled affects the currently scheduled meeting.  The computer-implemented assistant 710 may communicate with the computer-implemented assistant 710 of\nthe other member and include information about the enterprise action that pertains to the at least one other member.  For example, when the newly scheduled business trip conflicts with the scheduled business meeting, the computer-implemented assistant\n710 of the member may provide information to computer-implemented assistant 710 of the other member that notifies the computer-implemented assistant 710 of the other member that the member is not able to make the meeting and other meeting times could be\nproposed.  However, information from the enterprise action about where the member is traveling, who the member is meeting with, when the member is leaving for the trip, among other information related to the enterprise action, is not provided to the\ncomputer-implemented assistant 710 of the other member because that information would not pertain to the at least one other member.  Further, in some implementations, the computer-implemented assistant 710 of each of the at least one or more other member\nmay perform an enterprise action based on receiving the pertaining information of the enterprise action of the member.\n Further, in some implementations, the computer-implemented assistant 710 is configured to be implemented on a third party application on the member device.  For example, the third party application may be a travel system application that\nincludes the member's boarding pass and other travel information.  As such, the third party application can provide relevant third party application member personal information to the computer-implemented assistant 710.  Additional third party\napplications may also be used, for example, applications for banking, correspondence (e.g., email), professional networking, among others.\n FIG. 8 is a flow chart of an example process 800 for providing enterprise actions by a computer-implemented assistant 710 for a member of the enterprise.  The process 800 can, for example, be implemented by the computer-implemented assistant\n710.  In some implementations, the operations of the example process 800 can be implemented as instructions stored on a non-transitory computer readable medium, where the instructions cause a data processing apparatus to perform operations of the example\nprocess 800.\n Enterprise resources 115 that are accessible to a member of the enterprise, based on the entity fact access control lists 230, where the resources 115 include data describing entities 210 and relationships among the entities (including entity\nfacts 220), and enterprise information specific to the member are accessed (802).  Additionally, as previously described, entity facts 220 of the entities 210 from the resources 115, each entity fact 220 describing at least one feature of the entity 210\nare identified (804).  Further, as previously described, member personal information specific to the member, the member personal information including information not described by the enterprise resources that are accessible to the member is accessed\n(806).  The computer-implemented assistant 710 then determines enterprise actions, as described above, to perform for the member based on the member personal information, enterprise information specific to the member and the entity facts (808).  For each\nenterprise action determined to be performed, the computer-implemented assistant 710 provides a notification to the member describing the enterprise action performed (810).\n Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and\ntheir structural equivalents, or in combinations of one or more of them.  Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions,\nencoded on computer storage medium for execution by, or to control the operation of, data processing apparatus.  Alternatively or in addition, the program instructions can be encoded on an artificially-generated propagated signal, e.g., a\nmachine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.  A computer storage medium can be, or be included in,\na computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them.  Moreover, while a computer storage medium is not a propagated signal, a computer\nstorage medium can be a source or destination of computer program instructions encoded in an artificially-generated propagated signal.  The computer storage medium can also be, or be included in, one or more separate physical components or media (e.g.,\nmultiple CDs, disks, or other storage devices).\n The operations described in this specification can be implemented as operations performed by a data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.\n The term \"data processing apparatus\" encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or multiple ones, or combinations, of the\nforegoing The apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).  The apparatus can also include, in addition to hardware, code that creates an\nexecution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of\none or more of them.  The apparatus and execution environment can realize various different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.\n A computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be\ndeployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment.  A computer program may, but need not, correspond to a file in a file system.  A program\ncan be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one\nor more modules, sub-programs, or portions of code).  A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication\nnetwork.\n The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output.  The processes and\nlogic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).\n Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.  Generally, a processor will receive\ninstructions and data from a read-only memory or a random access memory or both.  The essential elements of a computer are a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and\ndata.  Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.  Devices suitable for\nstoring computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal\nhard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.  The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.\n To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for\ndisplaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.\n Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that\nincludes a front-end component, e.g., a user computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more\nsuch back-end, middleware, or front-end components.  The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network.  Examples of communication networks include a local area network\n(\"LAN\") and a wide area network (\"WAN\"), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).\n While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any inventions or of what may be claimed, but rather as descriptions of features specific to particular\nembodiments of particular inventions.  Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment.  Conversely, various features that are described in the\ncontext of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination.  Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or\nmore features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.\n Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations\nbe performed, to achieve desirable results.  In certain circumstances, multitasking and parallel processing may be advantageous.  Moreover, the separation of various system components in the embodiments described above should not be understood as\nrequiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.\n Thus, particular embodiments of the subject matter have been described.  Other embodiments are within the scope of the following claims.  In some cases, the actions recited in the claims can be performed in a different order and still achieve\ndesirable results.  In addition, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results.  In certain implementations, multitasking and parallel\nprocessing may be advantageous.", "application_number": "14723760", "abstract": " Methods, systems, and apparatus, including computer programs encoded on a\n     computer storage medium for access control for enterprise information. In\n     one aspect, a method includes receiving resources of an enterprise, each\n     resource having a respective access control list specifying access\n     privileges to the resource for one or more members, and the resources\n     including entities related to the enterprise and relationships;\n     identifying entity facts of the entities from the resources; determining,\n     for each entity fact, an entity fact access control list; storing data\n     describing the entities, entity facts and the respective entity fact\n     access control lists, wherein each entity fact is associated with its\n     corresponding entity fact access control list; and providing, to each of\n     the members of the enterprise, access privileges to the data describing\n     the entities and the entity facts according to the respective entity fact\n     access control lists.\n", "citations": ["6262730", "7233933", "8135655", "8725770", "8751505", "8880466", "9998472", "20040128616", "20060059144", "20060129445", "20060129538", "20070005654", "20070027866", "20080140579", "20080168048", "20090327271", "20110087661", "20110173035", "20110173679", "20110282652", "20120143863", "20120158633", "20130238631", "20130238659", "20130278492", "20140018112", "20140046934", "20140157370", "20140282219", "20140298481", "20140379699", "20150006581", "20150026260", "20150074819", "20150294007", "20160071140", "20160350134"], "related": []}, {"id": "20160371288", "patent_code": "10296658", "patent_name": "Use of context-dependent statistics to suggest next steps while exploring\n     a dataset", "year": "2019", "inventor_and_country_data": " Inventors: \nLe Biannic; Yann (Suresnes, FR), Hamon; Olivier (Levallois Perret, FR), Dumant; Bruno (Verneuil sur Seine, FR)  ", "description": "TECHNICAL FIELD\n This description relates to using context-dependent statistics to suggest next steps (e.g., to a user) during the exploration of a dataset.\nBACKGROUND\n A typical organization, such as a business enterprise collects large amounts of data.  In such a situation, sophisticated data solutions are in demand in order to quickly and accurately access data desired by users from massive amounts of data\nmanaged by the business enterprise.\nSUMMARY\n According to one general aspect, a computer-implemented method for generating a plurality of data suggestions is described.  The method includes receiving, from a user, a textual input in a user interface of a software application implementing a\nplurality of business processes and determining a query context associated with the textual input, the user, and data that the user is viewing in the software application.  The method also includes computing a plurality of statistical metrics for the\nquery context.  The statistical metrics may be computed using information obtained from datasets associated with the query context.  The method also includes determining, using the statistical metrics, a plurality of candidate data combinations, the data\ncombinations including a plurality of dimensions, measures, and filters compatible with the query context.  Determining a plurality of candidate data combinations may include modifying the query context by performing actions on a dataset associated with\nthe query context, the actions selected from the group consisting of substituting one dimension for another dimension, adding a dimension, adding a measure, and adding a filter.  The method also includes ranking the plurality of candidate data\ncombinations according to at least one of the plurality of statistical metrics and generating at least one data suggestion using the plurality of candidate data combinations and providing the at least one data suggestion in the user interface.\n One or more implementations may include the following features.  For example, the statistical metrics may include correlation calculations between one or more measures.  In some implementations, the statistical metrics may be used to recommend\none or more graphics corresponding to the at least one data suggestion.  In some implementations, the statistical metrics may be used to select an additional correlated measure for at least one of the datasets associated with the query context, and\nprovide the additional correlated measure as a basis for modifying the at least one data suggestion.  In some implementations, each statistical metric is modeled in a knowledge graph including a plurality of edges between a vertex, the plurality of edges\nrepresenting a timestamp associated with a first discovery time for the metric, and a count for each additional discovery time associated with the metric.\n Determining a query context associated with the user and data that the user is viewing in the software application may include accessing user profiles and a plurality of datasets associated with the data the user is viewing, aggregating at least\ntwo of the plurality of datasets, and extracting information from the user profiles and the aggregated datasets to select a plurality of dimensions and measures that are configured to be correlated.\n In some implementations, providing the at least one data suggestion in the user interface may include providing a graphical representation of the data suggestion, the graphical representation depicting a pattern associated with the data\nsuggestion and a plurality of actions to further modify the graphical representation based on the pattern.\n According to another general aspect, a query management system is described.  The system includes instructions stored on a non-transitory computer-readable storage medium.  The system further includes a dataset statistics engine configured to\ncompute a first correlation between a plurality of measures, and compute a second correlation between a plurality of dimensions and determine dependencies between the plurality of dimensions.  The system also includes a query engine configured to\ngenerate a plurality of search queries based on the first correlation and the second correlation, and a knowledge graph configured to store one or more correlations generated by the dataset statistics engine and to store time-based hierarchical data\nassociated with a plurality of datasets.\n One or more implementations may include the following features.  For example, the dataset statistics engine may be configured to compute an exception between two or more correlated measures, compute a Pareto distribution for one or more additive\nmeasures, and/or compute an exception of a distribution for one or more dimensions, measures, or filters.  The dataset statistics engine may be further configured to compute an entropy of at least one measure in the plurality of measures by aggregating\nthe at least one measure over one dimension and in response to detecting the entropy above a predefined threshold level, the dataset statistics engine can generate an edge in the knowledge graph from the at least one measure to the one dimension.  Using\nthe predefined threshold level and the entropy, the dataset statistics engine can generate one or more data suggestions.\n In some implementations, the dataset statistics engine can compute the correlation between the plurality of measures using an online Knuth algorithm by performing data shifting to avoid cancellation and loss of precision.  The dataset statistics\nengine may additionally be configured to compute the correlation between the plurality of measures using a Pearson correlation coefficient.\n According to another general aspect, a computer program product may be tangibly embodied on a non-transitory computer-readable medium/recordable storage medium and may include executable code that, when executed, is configured to cause at least\none data processing apparatus to perform the following operations.  Specifically, the executable code may cause steps including receiving, from a user, a textual input in a user interface of a software application implementing a plurality of business\nprocesses, determining a query context associated with the textual input, the user, and data that the user is viewing in the software application, computing a plurality of statistical metrics for the query context, the statistical metrics being computed\nusing information obtained from datasets associated with the query context, determining, using the statistical metrics, a plurality of candidate data combinations, the data combinations including a plurality of dimensions, measures, and filters\ncompatible with the query context, ranking the plurality of candidate data combinations according to at least one of the plurality of statistical metrics, and generating at least one data suggestion using the plurality of candidate data combinations and\nproviding the at least one data suggestion in the user interface.\n The details of one or more implementations are set forth in the accompanying drawings and the description below.  Other features will be apparent from the description and drawings, and from the claims. BRIEF DESCRIPTION OF THE DRAWINGS\n FIG. 1 is a block diagram of a system for generating and providing query suggestions and completions in a user interface associated with business software executing at a client device.\n FIG. 2 is a block diagram illustrating an example query suggestion and completion system that can be implemented in the system of FIG. 1.\n FIG. 3 is a flowchart illustrating an example method for query suggestion and completion.\n FIGS. 4A and 4B are screenshots illustrating an example of query completion suggestions.\n FIG. 5 is a flowchart illustrating an example method for suggesting data visualizations based on a user query.\n FIG. 6 is a screenshot illustrating suggested data visualizations based on a user query.\n FIG. 7 is a block diagram depicting an overview of example components used with statistical analysis techniques described herein.\n FIG. 8 is an example block diagram illustrating statistics modeling in a knowledge graph.\n FIGS. 9A-9F illustrate example processes of analyzing metrics and patterns that are detected and stored in a knowledge graph.\n FIG. 10 is a screenshot illustrating an example user interface for providing data suggestions to a user.\n FIG. 11 is a flowchart illustrating example operations of the system of FIG. 1.\nDETAILED DESCRIPTION\n Performing online analytical processing on datasets with tens or hundreds of data dimensions can be a complex task.  Data mining algorithms can be used to extract structure and patterns from large datasets with many columns of data, but many of\nthese algorithms may not be capable of operating on the various levels of aggregation and filtering involved in online analytical processing of un-aggregated data.  Insights from data mining can be illustrated using a variety of aggregation functions,\nbut many of these algorithms may not be tuned to operate on the various levels of aggregation and filtering involved in online analytical processing.  In some implementations, a data mining algorithm may compute incorrect aggregations on data content\n(e.g., numbers), if the algorithm has no prior knowledge about calculation semantics associated with the numbers.  For example, it may be correct to cumulate quantities of goods stored in warehouses over multiple locations, but it may be incorrect or\nimprecise for data mining algorithms to cumulate such quantities over time.  In another example, a measure used in analytical processing may be defined by an arbitrarily complex formula involving multiple aggregated values and such a measure (from an\nonline analytical processing dataset) may be accurately assessed if computed using a dedicated calculation engine for a particular aggregation and filtering context, but may not be accurate using other formulas, aggregation contexts, or filtering\ncontexts.  Thus users may benefit from automated insights about online analytical processing datasets that do not match insights typically generated by data mining performed on raw data.\n The systems and methods described in this disclosure can perform online analytical processing by adapting a number of data mining techniques to operate on data with a view focused on a current context (e.g., a data aggregation context that is\nrelevant at a given point in time) that an end user may be interested in, including particular dimensions, measures, and filters being accessed.  The current context can be used to determine relevant data (e.g., search query suggestions) for presentation\nto the user, rather than performing data mining on raw unfiltered data.  The adapted data mining techniques can be adapted to operate on aggregated data (e.g., summative data).  The input of such techniques may include aggregated datasets defined by a\nparticular context.  The input may be generated by the systems described in this disclosure.\n The systems and methods described in this disclosure can execute such data mining techniques or algorithms by modifying one or more measures using a user-defined context.  In this fashion, the applied algorithms may depend on the context being\nselected.  A context may be defined by a combination of dimensions, measures and filters that can be used to obtain context-dependent data (e.g., search query suggestions).  The systems and methods described here can be configured to identify one or more\nmodifications in a determined context in order to provide relevant search query suggestions and search results.  Modifying one or more contexts can include switching to a different dimension, suggesting an additional dimension, suggesting an additional\nmeasure, and/or suggesting an additional filter.\n In some implementations, the systems and methods described herein can adapt online analytical processing visualizations (e.g., graphics, combined data structures, metric charts, etc.) to provide a graphical representation that the user can\nreview to determine patterns identified by data mining algorithms, or to highlight exceptions from found patterns.  Such visualizations may be provided to explain data suggestions and/or contexts.\n In some implementations, the online analytical processing techniques described herein can allow a user (e.g., human analyst) to navigate through a number of successive perspectives on aggregated data.  The navigation/exploration can include\nreducing a number of dimensions (e.g., consolidating, rolling up), adding additional dimensions (e.g., drill down), and applying additional filters (e.g., slice and dice).  The techniques can allow an interactive and iterative process of information\ndiscovery driven by the user providing a particular context.\n In some implementations, the techniques described herein can be used to identify structures and patterns in data and display such structures and patterns to a user.  The user can perform additional operations on data identified.  The operations\nmay include preparing data or cleansing data to ensure the data is suitable to be processed by one or more of the analytical processing techniques or algorithms described herein.  The operations may also include selecting relevant algorithms to produce\ndata models depicting data patterns.  The operations may additionally include evaluating models to assess accuracy and applicability to similar and/or other data.\n Identified patterns and exceptions to such patterns can be scored based on a number of system-identified statistics.  The statistics may pertain to the information each pattern may provide to a user.  Suggested query completions and/or\nrecommended data visualizations can be based on such statistics alone, or in combination with usage information (e.g., associated with a current user of a data system) and/or system rules (such as those described herein).\n In some implementations, the systems and methods described in this disclosure can perform compatibility determinations and provide relevant data without burdening the user with the analysis of comparing data, data fields, data attributes, data\ncategories, etc. In short, the systems and methods described in this disclosure can provide relevant query strings to obtain relevant search results (i.e., contextually relevant data) based on dataset statistics 127, usage information (e.g., usage\nstatistics) 130 and/or system rules associated with a particular context (e.g., including characteristic of a current user, entered query terms, etc.).\n Referring to FIG. 1, a block diagram of a system 100 is shown.  The system 100 may be configured to provide a user interface 102 associated with business software executing at a client device 104.  The client device 104 can display the user\ninterface 102, which may be provided by an enterprise software application 106 and/or query management system 108, each executing as one or more server devices and connectable through network 110.\n The example user interface 102 shown here includes a business application accessing or running enterprise software application 106 and/or requesting data via query management system 108.  The user can access user interface 102 to obtain business\ndata regarding business objects in the enterprise software applications 106.  Textual and graphical content displayed in user interface 102 may include business data associated with a number of datasets, measures, and dimensions, each of which can be\nassociated with a knowledge graph 112.  In other implementations, data being accessed can be organized in other ways.\n Datasets can represent a group of data combined based on a particular query context or based on a business object.  The query context may refer to the context of data currently displayed to a user in interface 102.  The list of selectable\ndataset names may be associated with datasets that are compatible with the particular query context, including user-based context.  When datasets are presented, an initial dataset is automatically selected by the system 100 and this dataset is typically\nused to provide the initial query context with respect to measures and dimensions that pertain to the initial dataset.  In some implementations, the initial dataset is used to set the query context until a user selects more content from user interface\n102.  Datasets may be stored and accessed from datasets repository 114.\n Measures can represent data objects that include metrics such as sales revenue, salary, inventory stock, or number of employees, etc. In some implementations, the measures include a plurality of data objects that quantitatively define at least\none attribute within a number of datasets.  Measures provided for selection in the user interface 102 are generally within the query context (e.g., including the user context) configured based on which datasets are selected and the particular user\nentering the query.  In some implementations, a user can select several measures to be included in data presented in interface 102.  However, selecting one or more measures can affect other data presented in interface 102.  Measures may be stored and\naccessed from measures repository 116.\n Dimensions can represent data objects that include categorical data in a dataset.  Example dimensions may include categories such as products for a region or sales for a region.  In some implementations, the dimensions may define a plurality of\ndata categories for attributes in a number of datasets.  In general, dimensions can include two selection modes.  The first selection mode can cause the dimension to be used as an axis of data visualization graph (e.g., by country).  The second selection\nmode can cause the dimension to be used as a filter (e.g., for Q3/2014 as Quarter/Year).  For example, selecting a dimension can cause an axis or value to be modified within data depicted in interface 102.  In this fashion, selecting dimensions can\nfunction to filter data.  Dimensions may be stored and accessed from dimensions repository 118.\n The example system 100 also includes a knowledge graph 112.  The knowledge graph 112 may represent a hierarchically arranged platform in which to manage business data.  This platform can be configured to organize and distribute business data for\na particular organization.  The knowledge graph 112 can function as a repository to be used to structure, simplify, and connect business data to users accessing such data.  The data in the knowledge graph 112 may be aggregated from a variety of internal\nand external sources.  In some implementations, the knowledge graph 112 can include metadata 120 that defines a path to obtain a document that may be responsive, e.g., based on a current context, to a particular search query.  In this example, rather\nthan store the data in the graph, the information for accessing the data is stored in the graph.\n The knowledge graph 112 can access or provide access to a number of repositories including, but not limited to datasets repository 114, measures repository 116, dimensions repository 118, metadata 120 and usage statistics 130, which can be\nstored internal to the graph 112, external to the graph 112, or both.  In general, the knowledge graph 112 may be implemented using any suitable software constructs.  In a non-limiting example, the knowledge graph 112 may be constructed using object\noriented constructs in which each node is a business object with associated functions and/or variables.  Edges of knowledge graph 112 may represent business objects that have associated functions and variables.  In some implementations, data contained in\nthe knowledge graph 112 can be constructed of edges and nodes and can be stored in any suitable number of data repositories across one or more servers located in one or more geographic locations coupled by any suitable network architecture.  As used\nherein, a business object refers generally to a construct of data and a methodology regarding how to interact with the data.  The knowledge graph 112 can include business object data, metadata, and associating data for such business objects.\n In some implementations, the knowledge graph 112 can include query trees configured to connect or be connected to other query trees by edges or nodes.  The connections may be based, at least in part, on adherence to system or grammar rules.  In\nsome examples, the nodes may represent business objects with a number of functions and variables.  The edges may represent similarities between one or more functions or variables associated with at least two business objects connected by at least one\nedge.\n The metadata 120 can include data associated with one or more datasets, measures, and/or dimensions.  The metadata 120 may describe semantic enhancements or enrichments to the datasets, measures, and/or dimensions.  For example, a dataset can\ncontain metadata that defines time and geography hierarchies, measures, formulas, and calculations, just to name a few examples.\n The knowledge graph can include, or have access to, a modifiable grammar 122 and association rules 124 in which to build and update the graph 112, including building and updating nodes 126 in the graph.  The grammar 122 may represent a set of\nassociation rules 124 (or other rules) that can be used to verify business intelligence semantics stored in the knowledge graph 112.  In some implementations, the grammar 122 is located external to query management system 108.  For example, the grammar\n122 may be included as part of enterprise software application 106 within suggest query 138.\n In one example, rules in the association rules 124 (which may also be referred to as system rules, rules, and so forth) can be used to determine how each element of a search query can be connected to other elements (whether included in the query\nor not) using a specific set of relation types.  Relation types can include parent nodes, child nodes, similar nodes based on keyword, business object overlap, business unit overlap, usage history (e.g., included in the usage statistics 130), the dataset\nstatistics 127, etc. The rules can be used to determine a path through the knowledge graph 112 to connect each element of the search query, e.g., to ensure that the system 100 finds and joins conditions that can be performed to execute the query. \nAnother example rule may include specific terms.  For example, the keyword \"current\" may be associated with a rule in the grammar that states that \"current\" should be followed by an attribute with a time semantic (e.g., \"current year\" translates to\n\"2015,\" or the actual current year).  In some implementations, the grammar can be used to detect intent and modify particular visualization and/or query suggestion in the user interface 102.  For example, a rule can be defined in the association rules\n124 for the keyword combination \"comparison of\" This keyword combination can be associated with a rule that ensures at least two measures are used so that the query management system 108 can generate a bar graph/chart.  Another rule may include\nspecifying a drill path or a drill order used to follow a defined hierarchy order (e.g., by year, then by month, etc.).\n In some implementations, the dataset statistics 127 may be generated by a data statistics engine 132 within query management system 108.  The data statistics engine 132 may be configured to compute a first correlation between a plurality of\nmeasures and compute a second correlation between a plurality of dimensions and determine dependencies between the plurality of dimensions.  In one example, the correlation can be computed between the plurality of measures using an online Knuth algorithm\nby performing data shifting to avoid cancellation and loss of precision.  In another example, the correlation can be computed between the plurality of measures using a Pearson correlation coefficient.\n In some implementations, the data statistics engine 132 can be configured to compute an exception between two or more correlated measures.  For example, a context (filter) for which the correlation is not respected anymore may constitute an\nexception.  In some implementations, the data statistics engine 132 can be configured to compute a Pareto distribution for one or more additive measures.  For example, when an additive measure (e.g., Revenue) is mainly obtained with few values of one\ndimension (ex.: 1% Customer makes 81% of Revenue), then a Pareto distribution may show in the data.  In some implementations, the data statistics engine 132 can compute an exception of a distribution for one or more dimensions, measures, or filters.  For\nexample, when the repartition of an additive measure over a dimension changes significantly when a filter is added on another dimension, an exception may be shown in the data.\n In some implementations, the data statistics engine 132 may be configured to compute an entropy of at least one measure by aggregating the measure over one dimension.  In response to detecting the entropy above a predefined threshold level, the\nengine 132 can generate an edge in the knowledge graph from the measure to the one dimension.  The predefined threshold level and the entropy can be used to generate one or more data suggestions.\n In the example system 100, the query management system 108 also includes a query engine 134 and a security engine 136.  In some implementations, the query engine 134 and/or the security engine 136 may be provided external to query management\nsystem 108.\n The query engine 134 may be configured to generate a number of search queries based on a first correlation and a second correlation determined in the data.  For example, correlations determined between measures, between dimensions, or between\nfilters.\n A search query, as used herein, can include building a query before actually performing a search for information.  For example, query objects may not exist and may be generated in near real time by query engine 134.  The generated query objects\ncan then be used to retrieve information from the knowledge graph 112, for example.  The retrieved information can be used to build answers to a particular received search query.  The answers can be executed against business intelligence data models\n(e.g., view, dataset, Lumira documents, etc.) and can be displayed as a visualization of data in a user interface.\n The query engine 134 can be configured to build and perform queries based on data provided in user interface 102, for example.  The data provided in user interface 102 may be system-generated, user-entered, or a combination of both.  In some\nimplementations, the query engine 134 can be configured to determine a query context associated with data entered into interface 102 (or with respect to data presented in interface 102).  Determining a query context can include using a combination of\nuser entered data and data sources to ascertain context from the user entered data.  For example, one way to determine the query context can include accessing a knowledge graph to compare the selected and displayed data from user interface 102 with data\n(e.g., nodes and edges) in the knowledge graph 112.  The comparison can include determining associations between information stored in the knowledge graph and determining which of those associations are compatible with the data displayed in user\ninterface 102.\n In operation, the query engine 134 may be configured to generate a number of keyword search queries using one or more keyword tokens that may be generated by receiving user input, such as a free text query or question in a search field.  The\nquery engine 134 can execute the keyword searches against a metadata repository and obtain search results responsive to the one or more keyword searches.  Using the search results, the query engine 134 can generate several query trees (e.g., one for each\ntoken).  Using the query trees, the query engine 134 can generate a list of search query suggestions.  The query suggestions can be generated using the query trees to retrieve data corresponding to one or more rule compliant data paths defined by the\nquery trees.  The query suggestions can be provided to the user for selection.  Selecting a query suggestion can trigger execution of a system-wide search for business data.\n The security engine 136 can be configured to determine whether a user accessing user interface 102 (and thereby accessing content in query management system 108 and enterprise software application 106) is authorized to access particular data. \nFor example, the security engine 136 can determine whether insufficient security credentials have been provided for a user of the software application.  If the engine 136 determines that particular data cannot be accessed, the user interface 102 can be\nmodified to exclude that data.  That is, the security engine 136 can remove data from the interface 102 and/or, terminate view access to datasets, measures, dimensions, or any associated business objects.\n In some implementations, the security engine 136 may be configured to implement security rules to allow or deny presentation of query suggestions to a user of the user interface, the security rules being based on user usage data, knowledge graph\nrules, and grammar rules.  In some implementations, the security engine 136 may be configured to deny access to one or more query suggestions by removing the one or more query suggestions from a list of generated query suggestions before providing the\nlist to a user.  The denied access may be because the security engine 136 determined that insufficient user credentials are associated with the user accessing a user interface in the software application.\n In some implementations, the security engine 136 can access information provided by entities wishing to access query management system 108.  For example, such information can include security model information, metadata describing sources of\nsuch information, and access control list data to be indexed in index 128, for example.  In addition, the actual access control lists can also be indexed.  For example, the query engine 134 can perform a search query according to user credential rules\nthat allow secure access to a portion of repositories within an organization.  The user credential rules may block particular repositories from being searched by the query engine 134 (via application/user interface 102) based on the user's access\nprivileges.\n In some implementations, the security engine 136 can be configured to determine an identity of a user accessing user interface 102 to determine historical usage statistics 130 associated with the enterprise software application 106, or query\nmanagement system 108.  The usage metrics (e.g., statistics) 130 may pertain to historical data access, previously suggested queries and/or previously user-edited queries, or a present query combined with the identification of the user.  Determining user\nidentification can include retrieving data about the user from login credentials or other repository storing user data.  The retrieved data can be used to obtain the user's job title, management statistics, security groups, hierarchy within a group, etc.\n Referring again to FIG. 1, the query management system 108 also includes (or has access to) enterprise software application 106.  The enterprise software application 106 represents computer software used to satisfy the needs of a business\norganization.  Enterprise software application 106 generally includes a collection of computer programs (i.e., software applications and repositories) with common business applications, tools for modeling how an organization functions, and development\ntools for building applications unique to the organization.  The enterprise software application 106 can be used in combination with query management system 108 to improve enterprise productivity and efficiency by providing business logic support\nfunctionality and contextual query resolution.\n Services provided by the enterprise software application 106 may include business-oriented tools such as query context management and search query management.  Other services are possible including, but not limited to, online shopping and online\npayment processing, interactive product management, automated billing systems, security, enterprise content management, IT service management, customer relationship management, enterprise resource planning, business intelligence, project management,\ncollaboration, human resource management, manufacturing, enterprise application integration, and enterprise forms automation.\n The enterprise software application 106 shown here includes a suggest query service 138 and a knowledge graph API service 140.  Both services 138 and 140 can be configured to manipulate user interfaces (such as user interface 102) using stored\ndata from query management system 108.  In particular, the suggest query service 138 can transform a user query or question (suggested, or otherwise) into query descriptions over existing datasets and artifacts in enterprise software application 106\nand/or query management system 108.  An associated query service (not shown) can function to retrieve data corresponding to the suggested query and used to provide visualization of the data to a user.  A visualization recommendation service (not shown)\ncan be used to determine how a suggested query could be properly visualized in a user interface.  The suggested queries may be based on information stored in the knowledge graph 112.  The knowledge graph 112 may be built from information that can be\ncrawled from various data sources, or derived from usage.\n The knowledge graph API service 140 can be provided to a number of entities that wish to utilize query management system 108 and enterprise software application 106.  Entities can push information to the knowledge graph 112 by sending\ninformation on a message bus.  For example, to insert data into the knowledge graph 112, entities can send security model information, metadata describing the sources of information, and access control list data to be indexed, as well as the access\ncontrols lists.  In some implementations, the entities (e.g., source systems) may decide which information should be sent, and to what extent dimensions can be indexed.  Information sent over the bus can be collected by dedicated collectors (not shown)\nthat can store such information in the knowledge graph 112.  The collectors may be deployable independently of one another to make scaling and graph updating convenient.\n The query management system 108 in system 100 can be communicatively coupled to client device 104.  Client device 104 can access query management system 108 and any associated software applications.  Client device 104 can be connected (wired or\nwirelessly) to system 108, which can provide business data, user interfaces, and facets for display.  In some implementations, the client device 104 can execute one or more applications on the query management system 108 and provide business content\nand/or services to client device 104.\n In some implementations, one or more additional content servers and one or more computer-readable storage devices can communicate with the client device 104 and query management system 108 using network 110 to provide business content to the\ndevices hosting client device 104 and query management system 108.  In some implementations, the network 110 can be a public communications network (e.g., the Internet, cellular data network, dialup modems over a telephone network) or a private\ncommunications network (e.g., private LAN, leased lines).  In some implementations, the client device 104 and query management system 108 can communicate with the network 110 using one or more high-speed wired and/or wireless communications protocols\n(e.g., 802.11 variations, WiFi, Bluetooth, Transmission Control Protocol/Internet Protocol (TCP/IP), Ethernet, IEEE 802.3, etc.).\n Although only two computing devices are depicted in FIG. 1, the example system 100 may include a plurality of computing devices that can exchange data over a network 110 (or additional networks not shown).  The computing devices hosting query\nmanagement system 108 and enterprise software application 106 may represent clients or servers and can communicate via network 110, or other network.  Example client devices may include a mobile device, an electronic tablet, a laptop, or other such\nelectronic device that may be used to access business content from query management system 108.  Each client device can include one or more processors and one or more memory devices.  The client devices can execute a client operating system and one or\nmore client applications that can access, control, and/or display business data on a display device included in each respective device.  The query management system 108 may represent a server device.  In general, the query management system 108 may\ninclude any number of repositories storing content and/or business software modules that can search, generate, modify (e.g., edit), or execute business software and associated business objects, data, and knowledge graphs.\n Additional devices are possible and such devices may be configured to be substituted for one another.  In some implementations, a client device 104 and the query management system 108 can be laptop or desktop computers, smartphones, personal\ndigital assistants, portable media players, tablet computers, gaming devices, or other appropriate computing devices that can communicate, using the network 110, with other computing devices or computer systems.\n In operation of system 100, a user can pose a query/question in interface 102 and receive one or more answers in the form of raw data, visual/graphical data, and/or other data format.  The answers can be provided by query management system 108. \nIn a non-limiting example, the query management system 108 can build a set of consistent queries using rich business intelligence semantic information, syntactic keyword rules, combination rules, security rules, dataset statistics and/or usage statistics\nand can do so with near real time performance.\n In short, the query management system 108 can receive textual data from a user accessing interface 102 and can use the query to access the grammar 122.  The query management system 108 can use the grammar 122 and the received data to generate a\nset of consistent queries.  The text in the data can be analyzed and tokenized (i.e., portioned into tokens), and associated to keywords if a matching textual query portion can be found in the grammar.  In some implementations, the matching can be\nperformed using a Levenshtein distance algorithm to resist account for typing errors.  Other approximating algorithms can of course be substituted.  The query management system 108 can attempt to match each token (including keywords) against the metadata\nusing a full text search engine, such as query engine 134 or an external search engine.  The matches can be combined using predefined association rules (e.g., association rules 124) as a default.  In some implementations, the rules may include particular\nrules associated with the received/detected keywords.  Combining the matches can include generating a tree/graph in which nodes 126 of the graph 112 represent matched metadata or keywords item, (or a node to mark a non-matched item).  A new node can be\nadded as a child node if an existing node satisfies the applied rules.\n Upon applying the rules, the query management system 108 can select a number of generated queries that appear to be within the same context and relevancy as the user entered textual data (i.e., by analyzing the score).  The suggested queries can\nbe translated to a human readable format and translated to a query that can be accessed by query service 138, for example.  The translation can be performed using the grammar keywords to express the interpreted semantic.  The human readable format of the\nsuggested queries can be provided to the user of interface 102, while the machine-readable query can be provided to suggest query service 138.  The user can select which query suits his needs.\n In general, a path from the root of the graph 112 to a leaf may represent a query that has been judged consistent by the system 100.  The above process can be used to generate several consistent queries that can be scored using a cumulative\nsearch score for each individual item, where the cumulative search score can be based rules (such as described herein), the dataset statistics 127 and/or the usage statistics.\n The system 100 can provide the advantage of an easy to use full text search to generate consistent queries/query objects without user action.  The queries may be expressive of data in repositories and include keyword and grammar support.  In\naddition, the system 100 can provide the advantage of ranking such queries.  The system 100 may also be fault tolerant with respect to typing and semantic errors.  The system 100 may provide incremental learning for a user because the user can reuse\nkeywords presented in previous query suggestions that the user received.\n FIG. 2 is a block diagram illustrating an example query suggestion and completion system 200 that can be implemented in the system 100 of FIG. 1.  Accordingly, for purposes of illustration, the system 200 is illustrated as including elements of\nthe system 100, and FIG. 1 will be referenced, as appropriate, in the discussion of FIG. 2.\n In an implementation, the system 200 can assist a user in composing (building) a query string to be executed on one or more dataset included in a database 205, where information about the datasets can be maintained in a knowledge graph, such as\nthe knowledge graph 112 discussed above and shown, for purposes of illustration, in FIG. 2.  In other implementations, the system 200 can be used to assist a user in building a query that is to be executed on datasets that are stored in a database system\nhaving a configuration other than those described herein.  Briefly, the system 200 can be used to determine a current user-context (such as user-contexts as described herein), identify query parameters (e.g., measures, dimensions and/or filters) related\nto that user-context, provide suggestions for building a query that are consistent (e.g., as determined using the scoring techniques described herein) with the user-context and provide suggested related data visualizations based on the user-context and\nan executed and/or edited query.\n As shown in FIG. 2, the system 200 includes the client device 104, the suggest query service 138 and graph storage 212, which can be implemented as, at least a portion of, the knowledge graph 112 of the system 100.  As illustrated in FIG. 2,\ngraph storage 212 can be included in a database 205, which can take a number of forms, such as an in-memory database, a distributed database, or other database configuration.  The client 104 in the system 200 includes a suggest query service client API\n201, which can be configured to communicate with the suggest query service 138 to build a query string based on a current user-context and/or provide related visualization suggestions based on the current user-context.  Likewise, the suggest query\nservice 138 of FIG. 2 includes a graph API that can be configured to communicate with graph storage 212 to provide query suggestions and related visualization suggestions (e.g., in conjunction with the suggest query service client API 201).\n In FIG. 2, the suggest query service 138 is illustrated as including a request analysis service 215, a request metadata enhancement service 216, a query building service 217 and a query enrichment service 218.  The request analysis service 215\nanalyzes and tokenizes the textual user input, and associates some tokens to keywords if a matching textual query portion can be found in the grammar 122.  The request metadata enhancement service 216 attempts to associate each token with a list of\nmatching metadata 120 or data contained in index 128, using a suitable search engine.  In some implementations, the matching can be performed using a Levenshtein distance algorithm to account for typing errors.  Other approximating algorithms can, of\ncourse, be substituted.  The query building service 217 combines the measures, dimensions and/or filter values determined by 216 into a plurality of query suggestions, and selects the most suitable suggestions, according to rules, usage statistics or\ndata statistics.  The query enrichment service 218 further enhances the suggestions created by service 217 by adding compatible measures, dimensions or filters that could be of interest given the user context, for further exploration of the dataset.  In\nan implementation, the request analysis service 215, the request metadata enhancement service 216, the query building service 217 and the query enrichment service 218 can be used to implement the methods of FIGS. 3 and 5, which are discussed below.  For\ninstance, these elements of the query suggest service 138 can be configured to provide query suggestions and/or suggest related data visualizations, where those determinations can be based on the usage statistics 130 in combination with a current\nuser-context.  In such an approach, the usage statistics 130 can be used to compute numerical scores to rank the relevance of each of a plurality of query parameters that may be relevant to a user based on a current user-context.\n In some implementations, suggestions (query completion and related visualization suggestions) can be also be based on other scores (e.g., other ranked numerical scores), such as rule-based scores and/or dataset statistic based scores, where such\nother scores can also be based on the current user-context.  In such implementations, different (ranked) scores for a given set of possible suggestions can be merged (blended, combined, etc.) to produce a single set of ranked numerical scores for the\ngiven set of possible suggestions.  The suggest query service 138 can then make suggestions to a user (e.g., on the user interface 102 of the client 104 via the suggest query service client API 201) for building a query and/or for a data visualization\nthat is (are) consistent with (relevant to) the current user context, where the suggestions are based on such ranked scores (usage based scores, or otherwise).\n As shown in FIG. 2, the system 200 also includes a usage metrics service 222, a crawler service (crawlers) 224, a statistical analysis service 226 and an indexer service (e.g., indexers) 228.  In the system 200, usage metrics service 222 can be\nconfigured to communicate with graph storage 212 to collect and maintain usage statistics (such as those described herein) for data that is included in a knowledge graph.  The crawlers 224 can be configured to extract (determine, etc.) metadata from the\ndatasets 114 and store them in the graph storage 212.  Such metadata can include dimensions, measures and default aggregation functions corresponding with data stored in the datasets 114.  The metadata generated by the crawlers 224 can then be stored in\nthe graph storage 212.  The statistical analysis service 226 can be configured to determine a set of statistics based on data included in datasets of the graph storage 212.  Such statistics can include correlations (symmetric and asymmetric) between\ndimensions and/or measures, determinations of whether measures are \"continuous\" or \"discrete\", cardinality, minimums, medians, maximums, identification of Pareto distributions, etc. The indexers 228 can be configured to index different sets of values of\ndimensions for datasets of the graph storage 212, so that those dimension values can be quickly retrieved from the graph (e.g., for responding to a query and/or for providing a query suggestion).\n FIG. 3 is a flowchart illustrating an example method 300 for query suggestion and completion.  The method 300 can be implemented in the systems 100 and 200 of FIGS. 1 and 2.  Accordingly, for purposes of illustration, the method 300 will be\ndescribed with further reference to FIGS. 1 and 2, as appropriate.  It will be appreciated, however, that the method 300 can be implemented in systems having other configurations than those shown in FIGS. 1 and 2.\n As illustrated in FIG. 3, the method 300 can include, at block 310, a request analysis.  The request analysis at block 310 can be triggered by a given user accessing (logging into) the enterprise software application 106 (e.g., via the user\ninterface 102) and entering textual input of at least a portion of a question (query) about datasets included in or represented by the graph storage 212 (the knowledge graph 112).  During the request analysis at block 310, the query suggest service can\ndetermine a current user-context, such as determining a user identifier (user ID), the user's job function, business groups and/or hierarchies the user is associated with and/or the textual input received from the user.  In addition, the request analysis\nat block 310 analyzes the text entered by the user, splits the text into tokens, and recognizes grammar keywords.\n At block 320, an artifact search is performed in response to the request analysis.  The artifact search at block 320 can include using the index 128 to determine for each token from the textual input of the users a number of measures, dimensions\nand/or filter values whose names or descriptions match perfectly or partially the token.  At block 330, the method 300 includes a query building process.  The query building process can include combining the measures, dimensions and/or filter values\ndetermined by block 320 into a plurality of query suggestions.  At block 340, the method 300 includes a query completion process.  The query completion process at block 340 can include adding other measures, dimensions and/or filter values to the query\nsuggestions provided at block 330 to make sure that they can be turned into executable queries.  These additions can be determined by following some of the association rules 124.  They can also be determined using information provided by the statistical\nanalysis service 226, and/or using usage statistics 130.  They can also be determined thanks to the user context and/or previously executed or edited queries in the system 200 (such as previous queries associated with the current user or other users the\ncurrent user is connected with based on the determined user context).  At block 350, the method 300 includes a suggestion ranking process.  The suggestion ranking process at block 350 can include associating with each completed query a numeric score and\nordering the completed queries according to their scores.\n FIGS. 4A and 4B are screenshots of a user interface 400 illustrating an example process of using query suggestions for query completion.  The user interface 400 can implement query completion suggestions that are made using the method 300, such\nas when implemented by the systems 100 and/or 200.  Accordingly, for purposes of illustration, the user interface in FIGS. 4A and 4B will be described with further reference to FIGS. 1-3, as appropriate.  It will be appreciated, however, that the user\ninterface 400 can be implemented in systems having other configurations than those shown in FIGS. 1 and 2 and/or using query completion and suggestion methods other than the method 300.\n Referring to FIG. 4A, the user interface 400 is illustrated showing that a current user has entered the query terms (parameters, etc.) \"Revenue by\" as query string 410 (e.g., a dimension parameter with an expected measure to follow).  Using the\napproaches described herein, the suggest query service 138 can generate query completion suggestions 420 based on a current user-context, including the entered query string 410, usage metrics, dataset statistics and/or rules, such as those described\nherein.  For instance, the suggest query service 138 may determine measures, other dimensions and/or filters that are considered relevant to the query string 410 based on the current user-context and scores (e.g., usage metric scores, dataset statistics\nscores and/or rule-based scores) for query parameters that are identified as being related to the query string 410 by, for example, an artifact search at block 310 of the method 300.  The ranked suggestions, such as those produced at block 350 of the\nmethod 500 can then be displayed to a user (e.g., in the user interface 102 of the client 104) as query completion suggestions 420.\n Referring to FIG. 4B, the user interface 400 is illustrated after selection (e.g., by the current user) of a suggestion from the query completion suggestions 420.  Specifically, in FIG. 4B, the query string 410 of FIG. 4B has been modified to\nquery string 430 to include the selected suggested measure of \"Contract Type\" for a query string 430 of \"Revenue by `Contract Type.`\" In the query string 430, the selected measure may be displayed in quotes to illustrate that \"Contract Type\" was selected\nfrom the query completion suggestions 420 made by the suggest query service 138.  While not shown in FIG. 4B, in some implementations, the query completion suggestions 420, after selection of the measure \"Contract Type\", can be modified based on the\naddition of the selected measure to the current user-context.\n FIG. 5 is a flowchart illustrating an example method 500 for suggesting data visualizations based on a user query string (e.g., user entered and/or completed using query suggestions).  As with the method 300 of FIG. 3, the method 300 can be\nimplemented in the systems 100 and 200 of FIGS. 1 and 2.  Accordingly, for purposes of illustration, the method 500 will be described with further reference to FIGS. 1 and 2, as appropriate.  It will be appreciated, however, that the method 500 can be\nimplemented in systems having other configurations than those shown in FIGS. 1 and 2.\n The method 500, at block 510, includes a user entering a query string and the suggest query service 138 generating a list of query suggestions, such as using the approaches described herein.  In response to the entry of a query string and\ngeneration of query suggestions at block 510, the method 500 can include, at block 520, finding related query suggestions that are stored in graph storage 212 using the approaches described herein.  Also in response to the entry of a query string and\ngeneration of query suggestions at block 510, the method 500 can include, at block 530 and in parallel with block 520, finding related query suggestions using statistics, such as a usage metrics and/or dataset statistics.  At block 540, the related query\nsuggestions found at block 520 and block 530 can be combined into a single related query suggestions list.  At blocks 550, 560 and 570, the query suggestions of the related list of block 540 can be scored based on the current-user context and,\nrespectively, usage metrics (block 550), dataset statistics (block 560) and/or rules (block 570).  At block 580, the scores for the list of related query suggestions of block 540 from blocks 550, 560 and 570 can be merged into respective aggregate scores\nfor each of the related query suggestions.  At block 590, the list of related query suggestions can be ordered based on the respective aggregate scores.  Visualizations of the related suggestions can then be presented to a user based on the ordered list\nof block 590.\n FIG. 6 is a screenshot of a user interface 600 illustrating suggested data visualizations based on a user query and current user-context.  The suggested data visualizations provided in the user interface 600 can be provided, for example, based\non an ordered, scored list of related query suggestions produced using the method 500.  As illustrated in highlight 610 in FIG. 6, a user may enter a query string of Revenue at Risk and a direct answer to that query string (a sum of 16.9 B) can be shown\nin the user interface 600.  Further, in the highlight 620 in FIG. 6, various visualizations of the related suggestions (e.g., relevant to the current user-context, including the entered query string) may be presented.  In this example, a target value\n(sum) for Revenue at Risk of 2.58 B, a graph of Revenue at Risk by Deal Status and a graph of Revenue at Risk and Contracts by Region are shown.  These related visualizations may be presented based on an ordered list of related suggestions, such as a\nlist produced at block 590 of the method 500.  In an implementation, a user can select one of the presented related visualizations to get more information on that view of data from one or more datasets corresponding with graph storage 212.\n FIG. 7 is a block diagram 700 depicting an overview of example components used with statistical analysis techniques described herein.  The components include an admin web application 702, a statistics extractor 704, a knowledge graph 706, a\ndatabase 708, and a suggest web application 710.  The admin web application 702 may allow an administrator to schedule statistical analysis.  The statistical analysis can be automatically executed at the scheduled times by the statistics extractor 704. \nThe statistics extractor 704 can use metadata from the knowledge graph 706 and data from the database (Hana View, Lumira artifact, etc.) 708 to compute statistics.  In general, information and metrics and patterns found by the statistical analysis are\nstored in the knowledge graph 706.  At a later time, the suggest web application 710 can access and use the stored statistics and to validate them at runtime.\n FIG. 8 is an example block diagram 800 illustrating statistics modeling in a knowledge graph.  The modeling components represent vertices and include measures 802, dimensional attributes 804, time level based hierarchies 806, and datasets 808. \nThe modeling components also include divergence data 810, and measures correlations 812.  The measures 802, dimensional attributes 804, time level based hierarchies 806, and the datasets 808 represent metadata already present in the knowledge graph when\na particular statistical analysis begins (e.g., datasets, dimensions, measures, hierarchies).  The edges shown in the diagram 800 and the other vertices (divergence data 810 and measures correlations 812)) represent different metrics and patterns that\nhave been found by the statistical analysis (e.g., relationships between measures and dimensions that influence particular measures, the measures or dimension correlations and their exceptions, the distribution exceptions (divergence data 810)).\n FIGS. 9A-9F illustrate example processes of detecting and analyzing metrics and patterns that are detected and stored in a knowledge graph.  The processes can obtain insights from a suggest service by computing statistical metrics (e.g., dataset\nstatistics 127, usage statistics 130) on detected data and/or patterns in the system 100, for example.  In some implementations, statistical analysis can be performed by a statistical extractor (e.g., statistical extractor 704 in FIG. 7) that is part of\nquery management system 108.  The statistical analysis is generally executed after a knowledge graph is generated by the crawlers 224, but may be performed before indexation.  Each metric determined in the statistical analysis can be persisted in the\nknowledge graph so that other serveries can leverage the metrics at a later point in time.  In addition, such metrics can be stored and verified as valid within particular contexts, in the event that the context is changed in a particular database or\nvisualization.\n The systems described herein can perform a batch analysis of a dataset.  The analysis may result in finding information about the dataset to support natural query variants.  For example, the system can find structural dependencies between\ndimensions (e.g., hierarchical drill paths) and/or find correlation between measures.  In some implementations, the analysis may result in finding combinations of dimensions and measures that may contain interesting information for the user based at\nleast in part on the user's current query context.  In one example, Pareto distributions can be determined.  In particular, high entropy can be determined in an example in which 20% of customers generate 80% of the revenue.  This can be further drilled\ndown to determining that the 20% metric depends on time, geographical region or other user-applied filter.  In another example, patterns and exceptions to patterns can be found.\n Referring to FIG. 9A, a time period can be defined using a current/last year, quarter, and/or month 902 as a useful time interval 902 and/or common filters can be applied according to user usage data 904.  The time period 902 and filters and/or\nusage data 904 can be applied when performing source analysis 906.  The source analysis 906 can include analyzing dimensions 908, analyzing measures 910, analyzing dimensions and measures 912, and/or analyzing two dimensions and one measure 914.\n Referring to FIG. 9B, analyzing dimensions 908 can include computing inter-dimension correlations to find dependencies between particular dimensions.  If an inter-dimension correlation is found, an edge is added (as DependencyEdge) in the\nknowledge graph between the two dimensions.  The dimensions may be oriented from less granular to more granular dimensions (e.g., from country to city).  Such an edge may have two properties (1) strength and (2) fanout.  The strength may be used to\nevaluate how strong a dependency is (e.g., mean perfect dependency, actual hierarchy, etc.).  The fan-out may be used to evaluate cardinalities after drilling.  Information gathered when analyzing dimensions can be used to suggest drill-paths and to\ncompute other metrics (e.g., 2 dimensions+1 measure analysis and divergences).\n As shown in FIG. 9B, the analysis can include detecting cardinality per dimensions 920, determining a metric (e.g., time) as nominal versus ordinal 922, and comparing pairs of dimensions 924.  Comparing pairs of dimensions 924 can include\naggregating on two dimensions 926, computing similar cardinalities 928, computing different cardinalities 930, and determining fuzzy dependencies 932.  Determining similar cardinalities 928 can include attempting to select a best caption and waiting for\na measure to determine whether similarities apply.  Determining different cardinalities 930 can include determining that there is no cardinality reduction, determining that an interval for one dimension is different than another with 95% confidence,\nand/or quantifying duplicate leaves in the knowledge graph.  Determining fuzzy dependencies 932 can include employing Goodman/Kruskal Lambda or Kruskal Tau algorithms.\n Referring to FIG. 9C, analyzing measures 910 can include computing correlations between particular measures, using a Pearson correlation coefficient, for example.  In another example, the measure correlation can be computed in a single pass\nusing a Knuth algorithm and data shifting to avoid cancellation and loss of precision.  For each pair of correlated measures, the systems described herein can create a vertex of correlated measures (e.g., MeasuresCorrelation) linked by two edges (e.g.,\nCorrelationEdge) between the correlated measures.  The vertex may have a property that contains the Pearson coefficient with double precision, for example.  The measure correlation may be used to suggest a related visualization with additional relevant\nmeasures or to avoid suggesting two measures that are perfectly correlated.\n In some implementations, a modified Thompson Tau technique can be used to compute correlation outliers.  For example, the modified Thompson Tau technique can include using linear regression and Tau (computed from T critical values).  Any\ndetermined outliers can be stored in the knowledge graph in the form of a CorrelationOutlierEdge.  This edge may have the property which contains values for which outliers are found.  The technique can be used to suggest a related visualizations with the\nmeasure sliced by the dimensions and with a filter to keep only the values of the dimension in the CorrelationOutlierEdge from the graph.\n As shown in FIG. 9C, analyzing measures 910 includes any or all of determining unassigned values 934, negative values 936, selecting a grouping set 938, and/or determining a Pearson correlation between measures 940.  Selecting a grouping set may\ninclude selecting an initial dataset and/or selecting a fine grain grouping set 942 (e.g., 2 or 3 N-M dimensions).  Determining the Pearson correlation of measures 940 can include determining a weak (e.g., independent measures), strong (bubble plot), or\nvery strong (peek from usage) correlation 944.\n Referring to FIG. 9D, analyzing one dimension and one measure 912 can include computing entropy of a measure aggregated over one dimensions (e.g., coefficient of variation).  When a high entropy is detected, the system can generate an edge\n(e.g., InfluencerEdge) in the knowledge graph.  The edge may include a property which contains the entropy with double precision.  The entropy can be used to suggest dimensions for which the measures already present have a high entropy in order to\nproduce a graph that contains such measures.  The entropy can also be used when computing other metrics.\n In some implementations, the systems described herein can employ Pareto principle detection.  Pareto principle detection can include computing a minimum percentage of dimension values that are used to reach 80 percent of a total of the measure. \nThe measure is generally additive (e.g., sum or count).  If less than 20% of the dimension values are necessary, the system detects an 80-20 Pareto principle and generates an edge (e.g., InfluencerEdge) in the knowledge graph from the measure to the\ndimension.  This edge may have a property that contains the percentage of the dimension values to reach 80% of the total of the measure.  The Pareto principle can be used to suggest a related visualization with a top explaining that 80% of the measure is\nobtained on less than 20% of the dimension values.\n As shown in FIG. 9D, analyzing one dimension and one measure 912 can include focusing on dimensions where few members catch 80% of the total 946, aggregating the measure on the dimension 948, determining whether pruning is possible 950,\nanalyzing distribution 952, and/or analyzing high/low values 954.  Determining whether pruning is possible 950 may include analyzing how few or many members exist and determining how many members catch 80% of the data (e.g., many=3 clusters; few=TOP N or\nTOP 80%).  Analyzing distribution 952 can include calculating a standard deviation or calculation skewness for ordered dimensions.  Analyzing high values 954 can include determining unusually high values by analyzing influence on total, one or many UHV\n(e.g., datapoint/TOP N).  Analyzing low values 954 can include analyzing unusually low values with very few members (e.g., keep all members), few members (e.g., discard ULV), or many members (e.g., discard TOP N).\n Referring to FIG. 9E, analyzing two dimensions and one measure 914 can include detecting divergences.  For example, considering one measure and one dimension, the systems described herein can look for a filter on another dimension value which\nsignificantly changes the distribution of the measure on the first dimension.  Such information can be stored in the knowledge graph in a vertex (Divergence) and three edges (DivergenceDimensionEdge), (DivergenceMeasureEdge), and (DivergenceFilterEdge). \nThe (DivergenceDimensionEdge) applies to the sliced dimension.  The (DivergenceMeasureEdge) applies to the measure.  The (DivergenceFilterEdge) applies to the filtered dimension.  The (DivergenceFilterEdge) may have a property which contains the value of\nthe dimension that can be added as a filter to change the distribution and a coefficient indicating how much the distribution is modified by such a filter.\n As shown in FIG. 9E, analyzing two dimensions and one measure 914 includes focusing on N-M relationship between the first and second dimensions 956, calculating distribution for the first dimension and the measure 958, and/or comparing the first\ndimension with the measure filtered by each second dimension 960.  Comparing the first dimension with the measure filtered by each second dimension 960 can include pruning the second dimension values with low impact 962 and/or computing the Pearson\ncorrelation 964.  Computing the Pearson correlation 964 may include determining weak correlations (e.g., generate a heatmap) and determining exceptions from the strong correlation (e.g., filter on the second dimension) 966.\n FIG. 9F depicts visualizations that can be provided to the user upon performing one or more processes depicted in FIGS. 9A-9E.  For example, analyzing dimensions 908 can result in generating plot-able dimension visualizations and drill path\nvisualizations.  Analyzing measures 910 can result in generating visualizations with suggested correlated measures.  Analyzing one dimension and one measure can result in generating plot-able dimensions and/or suggesting other dimensions with unusual\ndata points or high dispersion, for example.  Analyzing two dimensions and one measure 914 can result in generate visualizations with suggested filters on one or more of the dimensions.\n FIG. 10 is a screenshot illustrating an example user interface for providing data suggestions to a user.  In this example, the user may have entered the phrase \"revenue by region\" in the text box 1002 and selected enter to submit the phrase. \nThe visualization corresponding to such a submission may be depicted as shown by 1004, 1006, and 1008.  The visualizations may be displayed to the user for selection.  The user in this example may have selected the visualization 1004 to be shown\nadditional information (e.g., in panel 1010, for example).  In the panel 1010, the user can see a fact about the visualization.  In this example, the user may be presented with statistical insight such as \"1% of the customers represent more than 80% of\nthe revenue.\" This statistical insight information may have been found and provided using statistical analysis using statistical extractor 704, for example.  The analysis may have previously stored the information in the knowledge graph at batch time,\nand at runtime.  The suggest web application 710, for example, may have obtained this information using the statistical extractor 704 and provided the information to the user.\n Other suggestions in panel 1010 may be provided in the event that the user selects different visualizations.  In general, a pattern can be assessed for user entered text and the statistical extractor 704 can find information and store such\ninformation in the knowledge graph.  At runtime, the suggest web application 710 can check validity of the pattern according to time and user context, for example before providing the information to the user.  Selecting statistical insight content in\npanel 1010 can trigger display of additional visualizations that illustrate particular patterns used to obtain statistical insights.\n Referring to FIG. 11, an exemplary flowchart illustrates example process 1100 implemented in the system of FIG. 1.  Process 1100 includes a computer-implemented method for generating a plurality of data suggestions.  The process 1100 includes,\nreceiving, from a user, a textual input in a user interface of a software application implementing a plurality of business processes (1102).\n The process 1100 also includes determining a query context associated with the textual input, the user, and data that the user is viewing in the software application (1104).  Determining a query context associated with the user and data that the\nuser is viewing in the software application may include accessing user profiles and a plurality of datasets associated with the data the user is viewing, aggregating at least two of the plurality of datasets, and/or extracting information from the user\nprofiles and the aggregated datasets to select a plurality of dimensions and measures that are configured to be correlated.\n The process 1100 includes computing a plurality of statistical metrics for the query context, the statistical metrics being computed using information obtained from datasets associated with the query context (1106).  The process 1100 also\nincludes determining, using the statistical metrics, a plurality of candidate data combinations, the data combinations including a plurality of dimensions, measures, and filters compatible with the query context (1108).  Determining a plurality of\ncandidate data combinations may include modifying the query context by performing actions on a dataset associated with the query context, the actions selected from the group consisting of substituting one dimension for another dimension, adding a\ndimension, adding a measure, and adding a filter.\n The process 1100 also includes ranking the plurality of candidate data combinations according to at least one of the plurality of statistical metrics (1110).  The process 1100 also includes generating at least one data suggestion using the\nplurality of candidate data combinations and providing the at least one data suggestion in the user interface (1112).  Providing the at least one data suggestions in the user interface can include providing a graphical representation of the data\nsuggestion.  The graphical representation may depict a pattern associated with the data suggestion and a plurality of actions to further modify the graphical representation based on the pattern.\n The statistical metrics may be used to recommend one or more graphics corresponding to the at least one data suggestion.  In some implementations, the statistical metrics may be used to select an additional correlated measure for at least one of\nthe datasets associated with the query context, and provide the additional correlated measure as a basis for modifying the at least one data suggestion.\n In some implementations, each statistical metric is modeled in a knowledge graph including a plurality of edges between a vertex, the plurality of edges representing a timestamp associated with a first discovery time for the metric, and a count\nfor each additional discovery time associated with the metric.  In some implementations, the statistical metrics include correlation calculations between one or more measures.\n According to one general aspect, a computer-implemented method for generating a plurality of data suggestions is described.  The method includes receiving, from a user, a textual input in a user interface of a software application implementing a\nplurality of business processes and determining a query context associated with the textual input, the user, and data that the user is viewing in the software application.  The method also includes computing a plurality of statistical metrics for the\nquery context.  The statistical metrics may be computed using information obtained from datasets associated with the query context.  The method also includes determining, using the statistical metrics, a plurality of candidate data combinations, the data\ncombinations including a plurality of dimensions, measures, and filters compatible with the query context.  Determining a plurality of candidate data combinations may include modifying the query context by performing actions on a dataset associated with\nthe query context, the actions selected from the group consisting of substituting one dimension for another dimension, adding a dimension, adding a measure, and adding a filter.  The method also includes ranking the plurality of candidate data\ncombinations according to at least one of the plurality of statistical metrics and generating at least one data suggestion using the plurality of candidate data combinations and providing the at least one data suggestion in the user interface.\n One or more implementations may include the following features.  For example, the statistical metrics may include correlation calculations between one or more measures.  In some implementations, the statistical metrics may be used to recommend\none or more graphics corresponding to the at least one data suggestion.  In some implementations, the statistical metrics may be used to select an additional correlated measure for at least one of the datasets associated with the query context, and\nprovide the additional correlated measure as a basis for modifying the at least one data suggestion.  In some implementations, each statistical metric is modeled in a knowledge graph including a plurality of edges between a vertex, the plurality of edges\nrepresenting a timestamp associated with a first discovery time for the metric, and a count for each additional discovery time associated with the metric.\n Determining a query context associated with the user and data that the user is viewing in the software application may include accessing user profiles and a plurality of datasets associated with the data the user is viewing, aggregating at least\ntwo of the plurality of datasets, and extracting information from the user profiles and the aggregated datasets to select a plurality of dimensions and measures that are configured to be correlated.\n In some implementations, providing the at least one data suggestion in the user interface may include providing a graphical representation of the data suggestion, the graphical representation depicting a pattern associated with the data\nsuggestion and a plurality of actions to further modify the graphical representation based on the pattern.\n According to another general aspect, a query management system is described.  The system includes instructions stored on a non-transitory computer-readable storage medium.  The system further includes a dataset statistics engine configured to\ncompute a first correlation between a plurality of measures, and compute a second correlation between a plurality of dimensions and determine dependencies between the plurality of dimensions.  The system also includes a query engine configured to\ngenerate a plurality of search queries based on the first correlation and the second correlation, and a knowledge graph configured to store one or more correlations generated by the dataset statistics engine and to store time-based hierarchical data\nassociated with a plurality of datasets.\n One or more implementations may include the following features.  For example, the dataset statistics engine may be configured to compute an exception between two or more correlated measures, compute a Pareto distribution for one or more additive\nmeasures, and/or compute an exception of a distribution for one or more dimensions, measures, or filters.  The dataset statistics engine may be further configured to compute an entropy of at least one measure in the plurality of measures by aggregating\nthe at least one measure over one dimension and in response to detecting the entropy above a predefined threshold level, the dataset statistics engine can generate an edge in the knowledge graph from the at least one measure to the one dimension.  Using\nthe predefined threshold level and the entropy, the dataset statistics engine can generate one or more data suggestions.\n In some implementations, the dataset statistics engine can compute the correlation between the plurality of measures using an online Knuth algorithm by performing data shifting to avoid cancellation and loss of precision.  The dataset statistics\nengine may additionally be configured to compute the correlation between the plurality of measures using a Pearson correlation coefficient.\n Implementations of the various techniques described herein may be implemented in digital electronic circuitry, or in computer hardware, firmware, software, or in combinations of them.  Implementations may be implemented as a computer program\nproduct, i.e., a computer program tangibly embodied in an information carrier, e.g., in a machine-readable storage device, for execution by, or to control the operation of, data processing apparatus, e.g., a programmable processor, a computer, or\nmultiple computers.  A computer program, such as the computer program(s) described above, can be written in any form of programming language, including compiled or interpreted languages, and can be deployed in any form, including as a stand-alone program\nor as a module, component, subroutine, or other unit suitable for use in a computing environment.  A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and\ninterconnected by a communication network.\n Method steps may be performed by one or more programmable processors executing a computer program to perform functions by operating on input data and generating output.  Method steps also may be performed by, and an apparatus may be implemented\nas, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).\n Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.  Generally, a processor will receive\ninstructions and data from a read-only memory or a random access memory or both.  Elements of a computer may include at least one processor for executing instructions and one or more memory devices for storing instructions and data.  Generally, a\ncomputer also may include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.  Information carriers suitable for\nembodying computer program instructions and data include all forms of non-volatile memory, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable\ndisks; magneto-optical disks; and CD-ROM and DVD-ROM disks.  The processor and the memory may be supplemented by, or incorporated in special purpose logic circuitry.\n To provide for interaction with a user, implementations may be implemented on a computer having a display device, e.g., a cathode ray tube (CRT) or liquid crystal display (LCD) monitor, for displaying information to the user and a keyboard and a\npointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.  Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory\nfeedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.\n Implementations may be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client\ncomputer having a graphical user interface or a Web browser through which a user can interact with an implementation, or any combination of such back-end, middleware, or front-end components.  Components may be interconnected by any form or medium of\ndigital data communication, e.g., a communication network.  Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.\n While certain features of the described implementations have been illustrated as described herein, many modifications, substitutions, changes and equivalents will now occur to those skilled in the art.  It is, therefore, to be understood that\nthe appended claims are intended to cover all such modifications and changes as fall within the scope of the embodiments.", "application_number": "14876285", "abstract": " A computer-implemented method for generating a plurality of data\n     suggestions is described. The method includes receiving a textual input\n     in a user interface of a software application implementing a plurality of\n     business processes, determining a query context associated with the\n     textual input, a user, and data that the user is viewing in the software\n     application, and computing a plurality of statistical metrics for the\n     query context, the statistical metrics being computed using information\n     obtained from datasets associated with the query context. The method also\n     includes determining a plurality of candidate data combinations, the data\n     combinations including a plurality of dimensions, measures, and filters\n     compatible with the query context, ranking the plurality of candidate\n     data combinations according to one of the plurality of statistical\n     metrics, and generating at least one data suggestion using the plurality\n     of candidate data combinations and providing the at least one data\n     suggestion in the user interface.\n", "citations": ["5485564", "6587102", "6859937", "8898140", "20080189655", "20090043749", "20100211588", "20110035403", "20120191745", "20160063093", "20160371395"], "related": ["62180277", "62180280"]}, {"id": "20160371385", "patent_code": "10339190", "patent_name": "Question answering using entity references in unstructured data", "year": "2019", "inventor_and_country_data": " Inventors: \nKeysar; Dvir (Herzliya, IL), Shmiel; Tomer (Ramat Gan, IL)  ", "description": "BACKGROUND\n This specification generally relates to answering questions using a search system.  Answers to questions have been determined based on previously user-answered questions and manually generated databases.\nSUMMARY\n In some implementations, a system provides answers to natural language search queries by relying on entity references identified based in the unstructured data associated with search results.  In some implementations, a system receives a natural\nlanguage query such as a \"who\" question.  For example, \"Who is the President?\" or \"Who was the first person to climb Mt.  Everest?\" In some implementations, the system retrieves a number of search results, for example, a list of references to webpages on\nthe Internet.  In some implementations, the system retrieves additional, preprocessed information associated each respective webpage of at least some of the search results.  In some implementations, the additional information includes, for example, names\nof people that appear in the webpages.  In an example, in order to answer a \"who\" question, the system compiles names appearing in the first ten search results, as identified in the additional information.  The system identifies the most commonly\nappearing name as the answer, and returns that answer to the user.  It will be understood that in some implementations, the system answers questions other than \"who\" questions using the above-described technique, such as \"what\" or \"where\" questions.\n In some implementations, a computer-implemented method includes receiving a query, wherein the query is associated at least in part with a type of entity.  The method includes generating one or more search results based at least in part on the\nquery.  The method includes retrieving previously generated data associated with at least one search result of the one or more of search results, the data comprising one or more entity references in the at least one search result corresponding to the\ntype of entity.  The method includes ranking the one or more entity references.  The method includes selecting an entity result from the one or more entity references based at least in part on the ranking.  The method includes providing an answer to the\nquery based at least in part on the entity result.  Other implementations of this aspect include corresponding systems and computer programs, configured to perform the actions of the methods, encoded on computer storage devices.\n These and other implementations can each include one or more of the following features.  In some implementations, the query is a natural language query.  In some implementations, the type of the entity is a person type.  In some implementations,\nranking the one or more entity references comprises ranking based on at least one ranking signal.  In some implementations, the one or more ranking signals comprise a frequency of occurrence of each respective entity reference.  In some implementations,\nthe one or more ranking signals comprise a topicality score of each respective entity reference.  In some implementations, the previously generated data corresponds to unstructured data.\n One or more of the implementations of the subject matter described herein may provide one or more of the following advantages.  In some implementations, questions may be provided for queries in an automated and continuously updated fashion.  In\nsome implementations, question answering may take advantage of search result ranking techniques.  In some implementations, question answers may be identified automatically based on unstructured content of a network such as the Internet. BRIEF\nDESCRIPTION OF THE FIGURES\n FIG. 1 is a high level block diagram of a system for question answering in accordance with some implementations of the present disclosure;\n FIG. 2 shows an illustrative example of answering a question in accordance with some implementations of the present disclosure;\n FIG. 3 shows a flow diagram including illustrative steps for answering questions in accordance with some implementations of the present disclosure;\n FIG. 4 shows an illustrative data graph containing nodes and edges in accordance with some implementations of the present disclosure;\n FIG. 5 shows an illustrative knowledge graph portion in accordance with some implementations of the present disclosure;\n FIG. 6 shows another illustrative knowledge graph portion in accordance with some implementations of the present disclosure; and\n FIG. 7 shows an illustrative computer system in accordance with some implementations of the present disclosure.\n FIG. 8 is a block diagram of a computer in accordance with some implementations of the present disclosure.\nDETAILED DESCRIPTION OF THE FIGURES\n In some implementations, answers to questions are provided by a question answering system.  In some implementations, the system receives a query, retrieves search results, retrieves content associated with the search results, and determines an\nanswer to the question based on the retrieved content.\n In some implementations, the associated content includes entity references.  As used herein, an entity is a thing or concept that is singular, unique, well-defined and distinguishable.  For example, an entity may be a person, place, item, idea,\nabstract concept, concrete element, other suitable thing, or any combination thereof.  Generally, entities include things or concepts represented linguistically by nouns.  For example, the color \"Blue,\" the city \"San Francisco,\" and the imaginary animal\n\"Unicorn\" may each be entities.  An entity generally refers to the concept of the entity.  In some implementations, an entity reference is a reference, for example a text string, which refers to the entity.  For example, the entity reference \"New York\nCity\" is a reference to the physical city.\n In some implementations, an entity is associated with a type of entity.  As used herein, a type is a categorization or defining characteristic associated with one or more entities.  For example, types may include persons, locations, movies,\nmusicians, animals, and so on.  For example, \"who\" questions may have answers of the person type.  It will be understood that while the system described below is generally shown in reference to natural language \"who\" questions, any suitable type of\nquestion may be answered.\n In some implementations, the system described below receives natural language search queries.  As used herein, natural language refers to words, syntax, and other language such as it could be used in conversation or prose.  For example, natural\nlanguage may include complete sentences, questions, idiom, punctuation, any other suitable language elements or structures, or any combination thereof.  For example, the query [Who was the first person to fly an airplane?] is a natural language query. \nIt will be understood that queries, including natural language queries, may be in any suitable language such as English, French, Chinese, and so on.  It will be understood that in some implementations, the system need not receive a natural language query\nand may receive a query in any suitable form.\n FIG. 1 is a high level block diagram of a system for question answering in accordance with some implementations of the present disclosure.  System 100 includes query 102, processing block 104, answer 106, index 108 and entity references 110. \nSystem 100 may be any suitable hardware, software, or both for implementing the features described in the present disclosure and will generally be referred to, herein, as \"the system.\" In some implementations, query 102 represents a query received by a\nsystem.  In some implementations, query 102 is a natural language search query.  In some implementations, query 102 is received from user input, from another application, is generated by the system based on previously received input or data, is\ndetermined in any other suitable way, or any combination thereof.  In an example, the system receives a natural language query in search box on a search webpage from a user.  In another example, the system receives voice input from a user.\n In some implementations, processing block 104 receives a query from query 102.  In some implementations, processing block 104 generates one or more search results based on the query.  In some implementations, search results are determined by\nprocessing block 104 based on index 108.  In some implementations, index 108 is an index of webpages and other content.  For example, index 108 includes indexed webpages on the internet.  Processing block 104 retrieves information from entity references\n110, including one or more entity references associated with search results.  For example, the system may retrieve entity references associated with the top ten search results.  Processing block 104 ranks the one or more retrieved entity references,\nselects an entity result based on the ranking, and provides an answer based on the entity result to answer 106.  In some implementations, the ranking and/or selecting is based on a quality score, a freshness score, a relevance score, on any other\nsuitable information, or any combination thereof.\n In an example, the information retrieved from entity references 110 associated with a particular webpage is a list of persons appearing in that webpage.  For example, a particular webpage may include a number of names of persons, and entity\nreferences 110 may include a list of the names included within the webpage.  Entity references 110 may also include other information.  In some implementations, entity references 110 includes entity references of different types, for example, people,\nplaces, and dates.  In some implementations, entity references for multiple entity types are maintained as a single annotated list of entity references, as separated lists, in any other suitable format of information, or any combination thereof.  It will\nbe understood that in some implementations, entity references 110 and index 108 may be stored in a single index, in multiple indices, in any other suitable structure, or any combination thereof.\n In some implementations, the system identifies a type of query, for example a \"who\" query, and retrieves entity references from entity references 110 associated with search results for the query and also associated with the type of query.  In\nsome implementations, the system retrieves entity references associated with a particular number of search results, for example, the top ten ordered search results.  In some implementations, the system aggregates the identified entity references\nassociated with each of the webpages into a single list of references, ranks the identified entities references, and selects the highest ranked entity reference as the answer to the query.  In some implementations, the system provides the identified most\ncommonly occurring entity reference as answer 106.  In some implementations, one or more ranking metrics are used to rank the entity references, including frequency of occurrence and a topicality score.  Frequency of occurrence relates to the number of\ntimes an entity reference occurs within a particular document, collection of documents, or other content.  Topicality scores include a relationship between the entity reference and the content in which it appears.  Answer 106 is an answer to query 102. \nIt will be understood, and explained in further detail below, that the aforementioned technique is merely exemplary and that the system may use any other suitable techniques in processing the query, identifying entity references, aggregating entity\nreferences from multiple search results, providing the answer, and other suitable steps performed by the system.\n FIG. 2 shows an illustrative example of answering a question in accordance with some implementations of the present disclosure.  In the illustrated example, a webpage search engine receives a \"who\" question, retrieves a number of search results,\nand provides an answer based on the search results and information associated with the search results.  It will be understood that while the illustrated example refers to person entities in response to a \"who\" question, any suitable type of entity may be\nidentified in response to any suitable type of question.  For example, location entities may be identified in response to a \"where\" question.\n Search example 200 includes search query box 202 and search button 204.  In the illustrated example, query box 202 includes the natural language search query [Who is the King of Spain?]. In some implementations, the content of query box 202 may\nbe received using a keyboard, a virtual keypad, voice input, any other suitable input, or any combination thereof.  It will be understood that the system may receive any suitable query in query box 202.  In some implementations, search query box 202\nreceives query 102 of FIG. 1.  For example, the query may not be a natural language query, may omit the question mark, may include additional search terms, may include any other suitable search query elements, or any combination thereof.  In some\nimplementations, the query includes text, images, video, audio, voice input, data from other applications, social media data, any other suitable content, or any combination thereof.  In some implementations, search button 204 receives input indicating a\nuser's desire to perform a search on the query in query box 202.  In some implementations, input to search button 204 includes a mouse click, a touchscreen selection, a keystroke, voice input, any other suitable input, or any combination thereof.  In\nsome implementations, search button 204 is omitted.\n In some implementations, the system retrieves one or more search results based on the search query in query box 202.  In some implementations, the system displays search results in search results box 206.  As illustrated, search results include\nsearch result 208, search result 218, search result 222, and search result 226.  Search result 208 includes title 210 [Monarchy of Spain], URL 212, and snippet 214.  Title 210 includes a manually or automatically generated title associated with the\nsearch result.  URL 212 includes a universal resource locator, sometimes referred to as a web address, that is associated with the search result.  Snippet 214 includes a short section or selection of content from the search result.  It will be understood\nthat in some implementations, a search result such as search result 208 includes any suitable content including title 210, URL 212, snippet 214, other suitable text, images, video, audio, an abstract of the search result, a summary of the search result,\nautomatically generated content, manually generated content, social network data, user reviews, a search result relevancy score, any other suitable content associated with the search result, or any combination thereof.  It will be understood that search\nresults 218, 222, and 226 may include any suitable content or combination of content as described for search result 208.  It will also be understood that search result box 206 may include any suitable number of search results.  It will also be understood\nthat the particular arrangement and display of search results in search result box 206 is merely exemplary and that any suitable display of search results may be used.  In an example, multiple pages of search results are provided.\n In some implementations, the system orders the search results in search results box 206.  In some implementations, the system orders the search results based on one or more quality scores.  In some implementations, quality scores include a\nrelevance to search query, a quality score associated with the search result, a freshness score associated with the time when the data the content was last generated or updated, scores associated with previous selection of a particular search result from\na collection of search results, any other suitable quality score, or any combination thereof.  In an example, a quality score associated with a search result may include the number of links to and from a corresponding webpage.\n In some implementations, the system selects a particular number of the top orders search results for use in answering a question.  In the illustrated example, the system uses the top three results in search results box 206.  It will be\nunderstood that any suitable number may be selected using any suitable technique.\n In some implementations, the system identifies person entity references in the content of search results.  In some implementations, this processing is performed offline, such that when a search is performed, a list of entity references\nassociated with a webpage is also retrieved.  In some implementations, the identification of entity references is performed at the time of the search.\n In some implementations, the system generates a collection of entity references appearing in a webpage by comparing the structured or unstructured text to a list of known entity references, for example a list of names.  In some implementations,\nentity references not previously known are identified based on a frequency of occurrence or other clustering techniques.  In some implementations, entity references are person entity references, for example, names of people appearing in the text of a\nwebpage.  In an example, the system maintains a list of all of the names of people that appear in a particular webpage, and that list is retrieved when the webpage appears in the top results of search results box 206.\n In the illustrated example the system identifies person entity references associated with the search results.  Entity reference 216 [Juan Carlos] appears in snippet 214.  It will be understood that while in the illustrated example, entity\nreferences are shown in the snippets, entity references may appear in any suitable content of the webpage.  In some implementations, entity references appear in the unstructured text of a webpage.\n In the illustrated example, a list of person entity references associated with search result 208 is shown in list 230.  In an example, entity reference 216 [Juan Carlos] appears once in the snippet, as shown, and once elsewhere in the search\nresult, and thus list 230 includes a frequency value \"2\" associated with [Juan Carlos].  List 230 also includes the entry [Philip II] with a frequency of 1, and [Mike Jones] with a frequency of 1.  In an example, [Philip II] corresponds to a 16.sup.th\ncentury king of Spain, and [Mike Jones] corresponds to the author of a book on Spanish Kings.  List 232 includes person entity references associated with search result 218, including 5 occurrences of entity reference 220 [Juan Carlos], 3 occurrences of\n[Sophia of Greece] and 1 occurrence of [Bob Smith].  List 234 includes person entity references associated with search result 222, including 2 occurrences of entity reference 224 [Philip II], 2 occurrences of [Isabella], and 1 occurrence of [Charles V].\n In some implementations, the system calculates the sum the entity references occurring in list 230, list 232, and list 234, as shown in summation 236.  In the illustrated example, there are 7 instances of [Juan Carlos I] identified in the top\nthree search results shown in search result box 206.  In some implementations, the system identifies [Juan Carlos I] as entity result 240 and provides answer 242 based on entity result 240.  In some implementations, the system displays answer 242\nincluding the text [The King of Spain is Juan Carlos I].  In some implementations, the system displays only text [Juan Carlos I].  In some implementations, the system generates a natural language or other format response based in part on the received\nquery.  In some implementations, the system may display additional information associated with the answer, for example, a picture of Juan Carlos I or a link to an encyclopedia entry.  It will be understood that in some implementations, the system may\ndisplay content in search results box 206 before displaying answer 240.\n FIG. 3 shows flow diagram 300 including illustrative steps for answering questions in accordance with some implementations of the present disclosure.\n In step 302, the system receives a query.  In an example, the query is received in query box 202 of FIG. 2 or query 102 of FIG. 2.  In some implementations, the query is a \"who\" query, that is to say, it includes a question where the answer is a\nperson.  In some implementations, a person is defined as an individual, a group of people, a company, a legal entity, a team, any other suitable identification, or any combination thereof.  In an example, a \"who\" queries include [Who was the first to\nclimb Mount Everest?] and [Who won the 1997 World Series].  In some implementations, \"who\" queries may be implicit, in that they do not necessarily include a question mark and/or an interrogative.  In an example, the query [first to climb Mount Everest]\nis interpreted as a \"who\" query.  It will also be understood that in some implementations, the system may use the technique described herein to answer \"what,\" \"where,\" and other questions.\n In step 304, the system generates one or more search results based at least in part on the query.  In an example, search results include content displayed in search results box 206 of FIG. 2.  In some implementations, search results are\nidentified by processing block 104 of FIG. 1 based in part on index 108 of FIG. 1.  In some implementations, search results include a collection of content identified by a search engine as being related to the query.  In an example, search results\ninclude webpages, images, videos, audio, documents, any other suitable results, or any combination thereof.\n In some implementations, search results are ordered based on a number of metrics.  For example, metrics may include the metrics described above in relation to search box 206.  In an example, the number of links to and/or from a webpage including\nin a search result impacts its ordering.  In another example, the strength of a relationship between a search query and a search result impacts its ordering.  In another example, the freshness or date the information was posted or updated impacts\nordering.  It will be understood that any suitable ordering metric or combination of metrics may be used.\n In step 306, the system retrieves previously generated data comprising one or more entity references associated with at least one search result.  In an example, list 230 of FIG. 2 is data comprising entity references associated with search\nresult 208 of FIG. 2.  In some implementations, the system retrieves previously generated data from entity references 110 of FIG. 1.  In some implementations, previously generated data includes a list such as list 230 of FIG. 2.  In some implementations,\nthe system processes webpages and other content to identify entity references.  In some implementations, the system performs this processing offline, such that it is retrieved at the time of search.  In some implementations, the system processes the\ninformation in real-time at the time of search.\n In some implementations, the previously generated data includes one or more lists of entity references that occur in content associated with a search result.  In some implementations, list entries include an entity reference, a unique identifier\nassociated with the entity reference, a frequency of occurrence of the entity reference, the location on the page where the entity reference occurs, metadata associated with the content such as freshness and ordering, any other suitable data, or any\ncombination thereof.  In some implementations, previously generated data may include the type of entity reference, for example, a person, a location, a date, any other suitable type, or any combination thereof.  In some implementations, previously\ngenerated data includes information identifying entities as a particular type, such as a person entity reference, a place entity reference, or a time entity reference.  In some implementations, multiple sets of data may be generated for a website or\nother content, where each set is associated with one or more types.  In an example, a website may be associated with a list of the person entity references occurring therein and a list of the location entity references occurring therein.\n In some implementations, entity references are identified in unstructured content by comparing words and phrases in the content with known entity references.  For example, the system may compare words in the content with a database of names.  In\nsome implementations, additional contextual information is used in disambiguation of entity references identification.  For example, other names or entity references occurring in the content may be used to disambiguate a reference.  In an example, the\nname [George Washington] occurring in the same text as [Martha Washington] may be identified as relating to a unique entity reference in a list of the U.S.  President, whereas [George Washington] occurring in the same content as [University] and\n[Washington D.C.] is identified as relating to [George Washington University].\n In some implementations, the system uses clustering techniques to identify entity references in unstructured content.  Clustering is a statistical technique that groups similar objects into groups.  Clustering can identify natural groupings in\ndata elements.  The groups of objects, such as groups of text strings, may be used to identify frequently occurring words and/or phrases in structured and/or unstructured content.  For example, a person's first and last name that appear together\nrepeatedly in unstructured text may be identified as an entity reference.\n In step 308, the system ranks the entity references.  In an example, ranking the entity references includes the compiling of references shown in summation 236 of FIG. 2.  In some implementations, ranking the entity references includes ranking\nbased on one or more ranking signals.  In some implementations, the system uses a weighted combination of ranking signals to determine a ranking.  In an example, weighting includes the ranking of the search results or other search quality metrics.\n In some implementations, the system uses the frequency of occurrence of a particular entity reference as a ranking signal.  In some implementations, for example, frequency of occurrence corresponds to the total number of times an entity\nreference appears in a document.  In another example, the system determines a frequency by normalizing a number of appearances by the length of the document or any other suitable metric.\n In some implementations, the system uses a topicality score as a ranking signal.  In some implementations, topicality scores include freshness, the age of the document, the number of links to and/or from the document, the number of selections of\nthat document in previous search results, a strength of the relationship between the document and the query, any other suitable score, or any combination thereof.  In some implementations, a topicality score depends on a relationship between the entity\nreference and the content within which the entity reference appears.  For example, the entity reference [George Washington] may have a higher topicality score on a history webpage than on a current news webpage.  In another example, the entity reference\n[Barak Obama] may have a higher topicality score on a politics website than on a law school website.\n In some implementations, the system identifies entity references occurring in a particular number of search results, for example, the top ten ordered search results.  In some implementations, the number of search results is determined based on\nsystem design, user preferences, the type of query, system availability, system speed, network speed, previous question answering, the quality of an identified answer, any other suitable criteria, or any combination thereof.  In some implementation the\nsystem may rely on the quality of an identified entity result by adding search results to the processing until the system establishes a degree of confidence in the answer.  For example, processing only the top two search results may yield a tie or\nnear-tie for several search results, and the system may successively add previously generated data from lower ordered search results until a particular answer is significantly more common in the results than the others.  In some implementations, the\nsystem retrieves data from the top ordered search results, where the results are ordered as described above.  In some implementations, the particular search results for which previously processed data is retrieved is based on the search result ordering,\nquality values associated with the particular result, previously processed information, user input, any other suitable data, or any combination thereof.\n In step 310, the system selects an entity result from the entity references.  In some implementations, selecting a result includes selecting the most commonly occurring entity reference from the references ranked in step 308.  In an example, all\nof the entity references occurring in the top ten search results are compiled and ranked based on frequency of occurrence.  In some implementations, the system selects the most frequently occurring result as the entity result.  In the example shown in\nFIG. 2, this selection is illustrated as the box around [Juan Carlos I] in summation 236 of FIG. 2.\n In step 312, the system provides an answer to the query based on the entity result.  In some implementations, the answer is the entity result selected in step 310.  Displaying answer 240 of FIG. 2 is an example of displaying an example.  Answer\n106 of FIG. 1 is a further example of providing the answer.  In some implementations, displaying an answer includes displaying text, images, videos, audio, hyperlinks, any other suitable content, or any combination thereof.  In some implementations, the\nsystem generates a natural language answer based on the entity result of step 310.  In some implementations, a natural language answer is based in part on the query received in step 302.  In some implementations, the system displays the answer along\nwith, or in place of, the search results.\n FIG. 4 shows an illustrative data graph containing nodes and edges in accordance with some implementations of the present disclosure.  In some implementations, data in entity references 110 of FIG. 1 is stored as a data graph.  In some\nimplementations, illustrative data graph 400 is a portion of a knowledge graph.  The knowledge graph is a particular implementation of a data graph and will be described in further detail in relation to FIGS. 5 and 6 below.  It will be understood that\nthe data graph implementation of FIG. 4, and the knowledge graph, are merely examples of a data structure that may be used by the system to store entity references and other data, and that any suitable data format may be used.  For example, data in\nentity references 110 of FIG. 1 may be stored as a list of entities and associated entity types.  Data stored by the data structure may include any suitable data such as references to data, text, images, characters, computer files, databases, any other\nsuitable data, or any combination thereof.  It will be understood that in some implementations, the node and edge description is merely illustrative and that the construction of the data structure may include any suitable technique for describing\ninformation and relationships.  In an example, nodes may be assigned a unique identification number, and an edge may be described using the identification numbers that a particular edge connects.  It will be understood that the representation of data as\na graph is merely exemplary and that data may be stored, for example, as a computer file including pieces of data and links and/or references to other pieces of data.\n In some implementations, data may be organized in a database using any one or more data structuring techniques.  For example, data may be organized in a graph containing nodes connected by edges.  In some implementations, the data may include\nstatements about relationships between things and concepts, and those statements may be represented as nodes and edges of a graph.  The nodes each contain a piece or pieces of data and the edges represent relationships between the data contained in the\nnodes that the edges connect.  In some implementations, the graph includes one or more pairs of nodes connected by an edge.  The edge, and thus the graph, may be directed, i.e. unidirectional, undirected, i.e. bidirectional, or both, i.e. one or more\nedges may be undirected and one or more edges may be directional in the same graph.  Nodes may include any suitable data or data representation.  Edges may describe any suitable relationships between the data.  In some implementations, an edge is labeled\nor annotated, such that it includes both the connection between the nodes, and descriptive information about that connection.  A particular node may be connected by distinct edges to one or more other nodes, or to itself, such that an extended graph is\nformed.  For purposes of clarity, a graph based on the structure described immediately above is referred to herein as a knowledge graph.  In some implementations, the knowledge graph may be a useful for representing information and in providing\ninformation in search.\n FIG. 4 shows illustrative knowledge graph 400 containing nodes and edges.  Illustrative knowledge graph 400 includes nodes 402, 404, 406, and 408.  Knowledge graph 400 includes edge 410 connecting node 402 and node 404.  Knowledge graph 400\nincludes edge 412 connecting node 402 and node 406.  Knowledge graph 400 includes edge 414 connecting node 404 and node 408.  Knowledge graph 400 includes edge 416 and edge 418 connecting node 402 and node 408.  Knowledge graph 400 includes edge 420\nconnecting node 408 to itself.  Each aforementioned group of an edge and one or two distinct nodes may be referred to as a triple or 3-tuple.  As illustrated, node 402 is directly connected by edges to three other nodes, while nodes 404 and 408 are\ndirectly connected by edges to two other nodes.  Node 406 is connected by an edge to only one other node, and in some implementations, node 406 is referred to as a terminal node.  As illustrated, nodes 402 and 408 are connected by two edges, indicating\nthat the relationship between the nodes is defined by more than one property.  As illustrated, node 408 is connected by edge 420 to itself, indicating that a node may relate to itself.  While illustrative knowledge graph 400 contains edges that are not\nlabeled as directional, it will be understood that each edge may be unidirectional or bidirectional.  It will be understood that this example of a graph is merely an example and that any suitable size or arrangement of nodes and edges may be employed.\n Generally, nodes in a knowledge graph can be grouped into several categories.  Nodes may represent entities, organizational data such as entity types and properties, literal values, and models of relationships between other nodes.  A node of a\nknowledge graph may represent an entity, as defined above.\n In some implementations, entity types, properties, and other suitable content is created, defined, redefined, altered, or otherwise generated by any suitable technique.  For example, content may be generated by manual user input, by automatic\nresponses to user interactions, by importation of data from external sources, by any other suitable technique, or any combination thereof.  For example, if a commonly searched for term is not represented in the knowledge graph, one or more nodes\nrepresenting that node may be added.  In another example, a user may manually add information and organizational structures.\n A node representing organizational data may be included in a knowledge graph.  These may be referred to herein as entity type nodes.  As used herein, an entity type node may refer to a node in a knowledge graph, while an entity type may refer to\nthe concept represented by an entity type node.  An entity type may be a defining characteristic of an entity.  For example, entity type node Y may be connected to an entity node X by an [Is A] edge or link, discussed further below, such that the graph\nrepresents the information \"The Entity X Is Type Y.\" For example, the entity node [George Washington] may be connected to the entity type node [President].  An entity node may be connected to multiple entity type nodes, for example, [George Washington]\nmay also be connected to entity type node [Person] and to entity type node [Military Commander].  In another example, the entity type node [City] may be connected to entity nodes [New York City] and [San Francisco].  In another example, the concept [Tall\nPeople], although incompletely defined, i.e., it does not necessarily include a definition of the property [tall], may exist as an entity type node.  In some implementations, the presence of the entity type node [Tall People], and other entity type\nnodes, may be based on user interaction.\n In some implementations, an entity type node may include or be connected to data about: a list of properties associated with that entity type node, the domain to which that entity type node belongs, descriptions, values, any other suitable\ninformation, or any combination thereof.  A domain refers to a collection of related entity types.  For example, the domain [Film] may include, for example, the entity types [Actor], [Director], [Filming Location], [Movie], any other suitable entity\ntype, or any combination thereof.  In some implementations, entities are associated with types in more than one domain.  For example, the entity node [Benjamin Franklin] may be connected with the entity type node [Politician] in the domain [Government]\nas well as the entity type node [Inventor] in the domain [Business].\n In some implementations, properties associated with entity nodes or entity type nodes may also be represented as nodes.  For example, nodes representing the property [Population] or [Location] may be connected to the entity type node [City]. \nThe combination and/or arrangement of an entity type and its properties is referred to as a schema.  In some implementations, schemas are stored in tables or other suitable data structures associated with an entity type node.  In some implementations,\nthe knowledge graph may be self-defining or bootstrapping, such that it includes particular nodes and edges that define the concept of nodes, edges, and the graph itself.  For example, the knowledge graph may contain an entity node [Knowledge Graph] that\nis connected to property nodes that describe a knowledge graph's properties such as [Has Nodes] and [Has Edges].\n Specific values, in some implementations referred to as literals, may be associated with a particular entity in a terminal node by an edge defining the relationship.  Literals may refer to values and/or strings of information.  For example,\nliterals may include dates, names, and/or numbers.  In an example, the entity node [San Francisco] may be connected to a terminal node containing the literal [815000] by an edge annotated with the property [Has Population].  In some implementations,\nterminal nodes may contain a reference or link to long text strings and other information stored in one or more documents external to the knowledge graph.  In some implementations, literals are stored as nodes in the knowledge graph.  In some\nimplementations, literals are stored in the knowledge graph but are not assigned a unique identification reference as described below, and are not capable of being associated with multiple entities.  In some implementations, literal type nodes may define\na type of literal, for example [Date/Time], [Number], or [GPS Coordinates].\n In some implementations, the grouping of an edge and two nodes is referred to as a triple.  The triple represents the relationship between the nodes, or in some implementations, between the node and itself.  In some implementations, higher order\nrelationships are modeled, such as quaternary and n-ary relationships, where n is an integer greater than 2.  In some implementations, information modeling the relationship is stored in a node, which may be referred to as a mediator node.  In an example,\nthe information \"Person X Donates Artifact Y To Museum Z\" is stored in a mediator node connected entity nodes to X, Y, and Z, where each edge identifies the role of each respective connected entity node.\n In some implementations, the knowledge graph may include information for differentiation and disambiguation of terms and/or entities.  As used herein, differentiation refers to the many-to-one situation where multiple names are associated with a\nsingle entity.  As used herein, disambiguation refers to the one-to-many situation where the same name is associated with multiple entities.  In some implementations, nodes may be assigned a unique identification reference.  In some implementations, the\nunique identification reference may be an alphanumeric string, a name, a number, a binary code, any other suitable identifier, or any combination thereof.  The unique identification reference may allow the system to assign unique references to nodes with\nthe same or similar textual identifiers.  In some implementations, the unique identifiers and other techniques are used in differentiation, disambiguation, or both.\n In some implementations of differentiation, a node may be associated with multiple terms or differentiation aliases in which the terms are associated with the same entity.  For example, the terms [George Washington], [Geo.  Washington],\n[President Washington], and [President George Washington] may all be associated with a single entity, i.e., node, in the knowledge graph.  This may provide differentiation and simplification in the knowledge graph.\n In some implementations of disambiguation, multiple nodes with the same or similar names are defined by their unique identification references, by associated nodes in the knowledge graph, by any other suitable information, or any combination\nthereof.  For example, there may be an entity node related to the city [Philadelphia], an entity node related to the movie [Philadelphia], and an entity node related to the cream cheese brand [Philadelphia].  Each of these nodes may have a unique\nidentification reference, stored for example as a number, for disambiguation within the knowledge graph.  In some implementations, disambiguation in the knowledge graph is provided by the connections and relationships between multiple nodes.  For\nexample, the city [New York] may be disambiguated from the state [New York] because the city is connected to an entity type [City] and the state is connected to an entity type [State].  It will be understood that more complex relationships may also\ndefine and disambiguate nodes.  For example, a node may be defined by associated types, by other entities connected to it by particular properties, by its name, by any other suitable information, or any combination thereof.  These connections may be\nuseful in disambiguating, for example, the node [Georgia] that is connected to the node [United States] may be understood represent the U.S.  State, while the node [Georgia] connected to the nodes [Asia] and [Eastern Europe] may be understood to\nrepresent the country in eastern Europe.\n In some implementations, a node may include or connect to data defining one or more attributes.  The attributes may define a particular characteristic of the node.  The particular attributes of a node may depend on what the node represents.  In\nsome implementations, an entity node may include or connect to: a unique identification reference, a list of entity types associated with the node, a list of differentiation aliases for the node, data associated with the entity, a textual description of\nthe entity, links to a textual description of the entity, other suitable information, or any combination thereof.  As described above, nodes may contain a reference or link to long text strings and other information stored in one or more documents\nexternal to the knowledge graph.  In some implementations, the storage technique may depend on the particular information.  For example, a unique identification reference may be stored within the node, a short information string may be stored in a\nterminal node as a literal, and a long description of an entity may be stored in an external document linked to by a reference in the knowledge graph.\n An edge in a knowledge graph may represent a semantic connection defining a relationship between two nodes.  The edge may represent a prepositional statement such as [Is A], [Has A], [Is Of A Type], [Has Property], [Has Value], any other\nsuitable statement, or any combination thereof.  For example, the entity node of a particular person may be connected by a [Date Of Birth] edge to a terminal node containing a literal of his or her specific date of birth.  In some implementations, the\nproperties defined by edge connections of an entity may relate to nodes connected to the type of that entity.  For example, the entity type node [Movie] may be connected to entity nodes [Actor] and [Director], and a particular movie may be connected by\nan edge property [Has Actor] to an entity node representing a particular actor.\n In some implementations, nodes and edges define the relationship between an entity type node and its properties, thus defining a schema.  For example, an edge may connect an entity type node to a node associated with a property, which may be\nreferred to as a property node.  Entities of the type may be connected to nodes defining particular values of those properties.  For example, the entity type node [Person] may be connected to property node [Date of Birth] and a node [Height].  Further,\nthe node [Date of Birth] may be connected to the literal type node [Date/Time], indicating that literals associated with [Date of Birth] include date/time information.  The entity node [George Washington], which is connected to entity type node [Person]\nby an [Is A] edge, may also be connected to a literal [Feb 22, 1732] by the edge [Has Date Of Birth].  In some implementations, the entity node [George Washington] is connected to a [Date Of Birth] property node.  It will be understood that in some\nimplementations, both schema and data are modeled and stored in a knowledge graph using the same technique.  In this way, both schema and data can be accessed by the same search techniques.  In some implementations, schemas are stored in a separate\ntable, graph, list, other data structure, or any combination thereof.  It will also be understood that properties may be modeled by nodes, edges, literals, any other suitable data, or any combination thereof.\n For example, the entity node [George Washington] may be connected by an [Is A] edge to the entity type node representing [Person], thus indicating an entity type of the entity, and may also be connected to a literal [Feb 22, 1732] by the edge\n[Has Date Of Birth], thus defining a property of the entity.  In this way, the knowledge graph defines both entity types and properties associated with a particular entity by connecting to other nodes.  In some implementations, [Feb 22, 1732] may be a\nnode, such that it is connected to other events occurring on that date.  In some implementations, the date may be further connected to a year node, a month node, and a day of node.  It will be understood that this information may be stored in any\nsuitable combination of literals, nodes, terminal nodes, interconnected entities, any other suitable arrangement, or any combination thereof.\n FIG. 5 shows illustrative knowledge graph portion 500 in accordance with some implementations of the present disclosure.  Knowledge graph portion 500 includes information related to the entity [George Washington], represented by [George\nWashington] node 502.  [George Washington] node 502 is connected to [U.S.  President] entity type node 504 by [Is A] edge 514 with the semantic content [Is A], such that the 3-tuple defined by nodes 502 and 504 and the edge 514 contains the information\n\"George Washington is a U.S.  President.\" Similarly, \"Thomas Jefferson Is A U.S.  President\" is represented by the tuple of [Thomas Jefferson] node 510, [Is A] edge 520, and [U.S.  President] node 504.  Knowledge graph portion 500 includes entity type\nnodes [Person] 524, and [U.S.  President] node 504.  The person type is defined in part by the connections from [Person] node 524.  For example, the type [Person] is defined as having the property [Date Of Birth] by node 530 and edge 532, and is defined\nas having the property [Gender] by node 534 and edge 536.  These relationships define in part a schema associated with the entity type [Person].\n [George Washington] node 502 is shown in knowledge graph portion 500 to be of the entity types [Person] and [U.S.  President], and thus is connected to nodes containing values associated with those types.  For example, [George Washington] node\n502 is connected by [Has Gender] edge 518 to [Male] node 506, thus indicating that \"George Washington has gender Male.\" Further, [Male] node 506 may be connected to the [Gender] node 534 indicating that \"Male Is A Type Of Gender.\" Similarly, [George\nWashington] node 502 is connected by [Has Date of Birth] edge 516 to [Feb 22, 1732] node 508, thus indicating that \"George Washington Has Date Of Birth Feb 22, 1732.\" [George Washington] node 502 may also be connected to [1789] node 528 by [Has Assumed\nOffice Date] edge 538.\n Knowledge graph portion 500 also includes [Thomas Jefferson] node 510, connected by [Is A] edge 520 to entity type [U.S.  President] node 504 and by [Is A] edge 522 to [Person] entity type node 524.  Thus, knowledge graph portion 500 indicates\nthat \"Thomas Jefferson\" has the entity types \"U.S.  President\" and \"Person.\" In some implementations, [Thomas Jefferson] node 510 is connected to nodes not shown in FIG. 5 referencing his date of birth, gender, and assumed office date.\n It will be understood that knowledge graph portion 500 is merely an example and that it may include nodes and edges not shown.  For example, [U.S.  President] node 504 may be connected to all of the U.S.  Presidents.  [U.S.  President] node 504\nmay also be connected to properties related to the entity type such as a duration of term, for example [4 Years], a term limit, for example [2 Terms], a location of office, for example [Washington D.C.], any other suitable data, or any combination\nthereof.  For example, [U.S.  President] node 504 is connected to [Assumed Office Date] node 542 by [Has Property] edge 540, defining in part a schema for the type [U.S.  President].  Similarly, [Thomas Jefferson] node 510 may be connected to any\nsuitable number of nodes containing further information related to his illustrated entity type nodes [U.S.  President], and [Person], and to other entity type nodes not shown such as [Inventor], [Vice President], and [Author].  In a further example,\n[Person] node 524 may be connected to all entities in the knowledge graph with the type [Person].  In a further example, [1789] node 528 may be connected to all events in the knowledge graph with the property of year [1789].  [1789] node 528 is unique to\nthe year 1789, and disambiguated from, for example, a book entitled [1789], not shown in FIG. 5, by its unique identification reference.  In some implementations, [1789] node 528 is connected to the entity type node [Year].\n FIG. 6 shows illustrative knowledge graph portion 600 in accordance with some implementations of the present disclosure.  Knowledge graph portion 600 includes [California] node 602, which may also be associated with differentiation aliases such\nas, for example, [CA], [Calif.], [Golden State], any other suitable differentiation aliases, or any combination thereof.  In some implementations, these differentiations are stored in [California] node 602.  California is connected by [Is A] edge 604 to\nthe [U.S.  State] entity type node 606.  [New York] node 610 and [Texas] node 614 are also connected to [U.S.  State] node 606 by [Is A] edges 608 and 612, respectively.  [California] node 602 is connected by [Has Capital City] edge 620 to [Sacramento]\nnode 622, indicating the information that \"California Has Capital City Sacramento.\" Sacramento node 622 is further connected by [Is A] edge 624 to the [City] entity type node 626.  Similarly, [Texas] node 614 is connected by [Has City] edge 630 to\n[Houston] node 628, which is further connected to the [City] entity type node 626 by [Is A] edge 340.  [California] node 602 is connected by [Has Population] edge 616 to node 618 containing the literal value [37,691,912].  In an example, the particular\nvalue [37,691,912] may be periodically automatically updated by the knowledge graph based on an external website or other source of data.  Knowledge graph portion 600 may include other nodes not shown.  For example, [U.S.  State] entity type node 606 may\nbe connected to nodes defining properties of that type such as [Population] and [Capital City].  These type--property relationships may be used to define other relationships in knowledge graph portion 600 such as [Has Population] edge 616 connecting\nentity node [California] 602 with terminal node 618 containing the literal defining the population of California.\n It will be understood that while knowledge graph portion 500 of FIG. 5 and knowledge graph portion 600 of FIG. 6 below show portions of a knowledge graph, all pieces of information may be contained within a single graph and that these selections\nillustrated herein are merely an example.  In some implementations, separate knowledge graphs are maintained for different respective domains, for different respective entity types, or according to any other suitable delimiting characteristic.  In some\nimplementations, separate knowledge graphs are maintained according to size constraints.  In some implementations, a single knowledge graph is maintained for all entities and entity types.\n A knowledge graph may be implemented using any suitable software constructs.  In an example, a knowledge graph is implemented using object oriented constructs in which each node is an object with associated functions and variables.  Edges, in\nthis context, may be objects having associated functions and variables.  In some implementations, data contained in a knowledge graph, pointed to by nodes of a knowledge graph, or both, is stored in any suitable one or more data repositories across one\nor more servers located in one or more geographic locations coupled by any suitable network architecture.\n The following description and accompanying FIGS. 7 and 8 describe illustrative computer systems that may be used in some implementations of the present disclosure.  It will be understood that elements of FIGS. 7 and 8 are merely exemplary and\nthat any suitable elements may be added, removed, duplicated, replaced, or otherwise modified.\n It will be understood that the system may be implemented on any suitable computer or combination of computers.  In some implementations, the system is implemented in a distributed computer system including two or more computers.  In an example,\nthe system may use a cluster of computers located in one or more locations to perform processing and storage associated with the system.  It will be understood that distributed computing may include any suitable parallel computing, distributed computing,\nnetwork hardware, network software, centralized control, decentralized control, any other suitable implementations, or any combination thereof.\n FIG. 7 shows an illustrative computer system in accordance with some implementations of the present disclosure.  System 700 may include one or more computing device 702.  In some implementations, computing device 702, and any other device of\nsystem 700, includes one or more computers and/or one or more processors.  In some implementations, a processor includes one or more hardware processors, for example, integrated circuits, one or more software modules, computer-readable media such as\nmemory, firmware, or any combination thereof.  In some implementations, computing device 702 includes one or more computer-readable medium storing software, include instructions for execution by the one or more processors for performing the techniques\ndiscussed above with respect to FIG. 3, or any other techniques disclosed herein.  In some implementations, computing device 702 includes a smartphone, tablet computer, desktop computer, laptop computer, server, personal digital assistant (PDA), portable\naudio player, portable video player, mobile gaming device, other suitable user device capable of providing content, or any combination thereof.\n Computing device 702 may be coupled to network 704 directly through connection 706, through wireless repeater 710, by any other suitable way of coupling to network 704, or by any combination thereof.  Network 704 may include the Internet, a\ndispersed network of computers and servers, a local network, a public intranet, a private intranet, other coupled computing systems, or any combination thereof.\n Computing device 702 may be coupled to network 704 by wired connection 706.  Connection 706 may include Ethernet hardware, coaxial cable hardware, DSL hardware, T-1 hardware, fiber optic hardware, analog phone line hardware, any other suitable\nwired hardware capable of communicating, or any combination thereof.  Connection 706 may include transmission techniques including TCP/IP transmission techniques, IEEE 802 transmission techniques, Ethernet transmission techniques, DSL transmission\ntechniques, fiber optic transmission techniques, ITU-T transmission techniques, any other suitable transmission techniques, or any combination thereof.\n Computing device 702 may be wirelessly coupled to network 704 by wireless connection 708.  In some implementations, wireless repeater 710 receives transmitted information from computing device 702 by wireless connection 708 and communicates it\nwith network 704 by connection 712.  Wireless repeater 710 receives information from network 704 by connection 712 and communicates it with computing device 702 by wireless connection 708.  In some implementations, wireless connection 708 may include\ncellular phone transmission techniques, code division multiple access (CDMA) transmission techniques, global system for mobile communications (GSM) transmission techniques, general packet radio service (GPRS) transmission techniques, satellite\ntransmission techniques, infrared transmission techniques, Bluetooth transmission techniques, Wi-Fi transmission techniques, WiMax transmission techniques, any other suitable transmission techniques, or any combination thereof.\n Connection 712 may include Ethernet hardware, coaxial cable hardware, DSL hardware, T-1 hardware, fiber optic hardware, analog phone line hardware, wireless hardware, any other suitable hardware capable of communicating, or any combination\nthereof.  Connection 712 may include wired transmission techniques including TCP/IP transmission techniques, IEEE 802 transmission techniques, Ethernet transmission techniques, DSL transmission techniques, fiber optic transmission techniques, ITU-T\ntransmission techniques, any other suitable transmission techniques, or any combination thereof.  Connection 712 may include may include wireless transmission techniques including cellular phone transmission techniques, code division multiple access\n(CDMA) transmission techniques, global system for mobile communications (GSM) transmission techniques, general packet radio service (GPRS) transmission techniques, satellite transmission techniques, infrared transmission techniques, Bluetooth\ntransmission techniques, Wi-Fi transmission techniques, WiMax transmission techniques, any other suitable transmission techniques, or any combination thereof.\n Wireless repeater 710 may include any number of cellular phone transceivers, network routers, network switches, communication satellites, other devices for communicating information from computing device 702 to network 704, or any combination\nthereof.  It will be understood that the arrangement of connection 706, wireless connection 708 and connection 712 is merely illustrative and that system 700 may include any suitable number of any suitable devices coupling computing device 702 to network\n704.  It will also be understood that any computing device 702, may be communicatively coupled with any user device, remote server, local server, any other suitable processing equipment, or any combination thereof, and may be coupled using any suitable\ntechnique as described above.\n In some implementations, any suitable number of remote servers 714, 716, 718, 720, may be coupled to network 704.  Remote servers may be general purpose, specific, or any combination thereof.  One or more search engine servers 722 may be coupled\nto the network 704.  In some implementations, search engine server 722 may include the knowledge graph, may include processing equipment configured to access the knowledge graph, may include processing equipment configured to receive search queries\nrelated to the knowledge graph, may include any other suitable information or equipment, or any combination thereof.  One or more database servers 724 may be coupled to network 704.  In some implementations, database server 724 may store the knowledge\ngraph.  In some implementations, where there is more than one knowledge graph, the more than one may be included in database server 724, may be distributed across any suitable number of database servers and general purpose servers by any suitable\ntechnique, or any combination thereof.  It will also be understood that the system may use any suitable number of general purpose, specific purpose, storage, processing, search, any other suitable server, or any combination.\n FIG. 8 is a block diagram of a computing device of system 700 of FIG. 7 in accordance with some implementations of the present disclosure.  FIG. 8 includes computing device 800.  In some implementations, computing device 800 corresponds to\ncomputing device 702 of FIG. 7, a remote computer illustrated in system 700 of FIG. 7, any other suitable computer corresponding to system 700 of FIG. 7, any other suitable device, or any combination thereof.  In some implementations, computing device\n800 is an illustrative local and/or remote computer that is part of a distributed computing system.  Computing device 800 may include input/output equipment 802 and processing equipment 804.  Input/output equipment 802 may include display 806,\ntouchscreen 808, button 810, accelerometer 812, global positions system (GPS) receiver 836, camera 838, keyboard 840, mouse 842, and audio equipment 834 including speaker 814 and microphone 816.  In some implementations, the equipment of computing device\n800 may be representative of equipment included in a smartphone user device.  It will be understood that the specific equipment included in the illustrative computer system may depend on the type of user device.  For example, the input/output equipment\n802 of a desktop computer may include a keyboard 840 and mouse 842 and may omit accelerometer 812 and GPS receiver 836.  It will be understood that computing device 800 may omit any suitable illustrated elements, and may include equipment not shown such\nas media drives, data storage, communication devices, display devices, processing equipment, any other suitable equipment, or any combination thereof.\n In some implementations, display 806 may include a liquid crystal display, light emitting diode display, organic light emitting diode display, amorphous organic light emitting diode display, plasma display, cathode ray tube display, projector\ndisplay, any other suitable type of display capable of displaying content, or any combination thereof.  Display 806 may be controlled by display controller 818 or by processor 824 in processing equipment 804, by processing equipment internal to display\n806, by other controlling equipment, or by any combination thereof.  In some implementations, display 806 may display data from a knowledge graph.\n Touchscreen 808 may include a sensor capable of sensing pressure input, capacitance input, resistance input, piezoelectric input, optical input, acoustic input, any other suitable input, or any combination thereof.  Touchscreen 808 may be\ncapable of receiving touch-based gestures.  Received gestures may include information relating to one or more locations on the surface of touchscreen 808, pressure of the gesture, speed of the gesture, duration of the gesture, direction of paths traced\non its surface by the gesture, motion of the device in relation to the gesture, other suitable information regarding a gesture, or any combination thereof.  In some implementations, touchscreen 808 may be optically transparent and located above or below\ndisplay 806.  Touchscreen 808 may be coupled to and controlled by display controller 818, sensor controller 820, processor 824, any other suitable controller, or any combination thereof.  In some implementations, touchscreen 808 may include a virtual\nkeyboard capable of receiving, for example, a search query used to identify data in a knowledge graph.\n In some implementations, a gesture received by touchscreen 808 may cause a corresponding display element to be displayed substantially concurrently (i.e., immediately following or with a short delay) by display 806.  For example, when the\ngesture is a movement of a finger or stylus along the surface of touchscreen 808, the search system may cause a visible line of any suitible thickness, color, or pattern indicating the path of the gesture to be displayed on display 806.  In some\nimplementations, for example, a desktop computer using a mouse, the functions of the touchscreen may be fully or partially replaced using a mouse pointer displayed on the display screen.\n Button 810 may be one or more electromechanical push-button mechanism, slide mechanism, switch mechanism, rocker mechanism, toggle mechanism, other suitable mechanism, or any combination thereof.  Button 810 may be included in touchscreen 808 as\na predefined region of the touchscreen (e.g., soft keys).  Button 810 may be included in touchscreen 808 as a region of the touchscreen defined by the search system and indicated by display 806.  Activation of button 810 may send a signal to sensor\ncontroller 820, processor 824, display controller 820, any other suitable processing equipment, or any combination thereof.  Activation of button 810 may include receiving from the user a pushing gesture, sliding gesture, touching gesture, pressing\ngesture, time-based gesture (e.g., based on the duration of a push), any other suitable gesture, or any combination thereof.\n Accelerometer 812 may be capable of receiving information about the motion characteristics, acceleration characteristics, orientation characteristics, inclination characteristics and other suitable characteristics, or any combination thereof, of\ncomputing device 800.  Accelerometer 812 may be a mechanical device, microelectromechanical (MEMS) device, nanoelectromechanical (NEMS) device, solid state device, any other suitable sensing device, or any combination thereof.  In some implementations,\naccelerometer 812 may be a 3-axis piezoelectric microelectromechanical integrated circuit which is configured to sense acceleration, orientation, or other suitable characteristics by sensing a change in the capacitance of an internal structure. \nAccelerometer 812 may be coupled to touchscreen 808 such that information received by accelerometer 812 with respect to a gesture is used at least in part by processing equipment 804 to interpret the gesture.\n Global positioning system (GPS) receiver 836 may be capable of receiving signals from global positioning satellites.  In some implementations, GPS receiver 836 may receive information from one or more satellites orbiting the earth, the\ninformation including time, orbit, and other information related to the satellite.  This information may be used to calculate the location of computing device 800 on the surface of the earth.  GPS receiver 836 may include a barometer (not shown) to\nimprove the accuracy of the location.  GPS receiver 836 may receive information from other wired and wireless communication sources regarding the location of computing device 800.  For example, the identity and location of nearby cellular phone towers\nmay be used in place of, or in addition to, GPS data to determine the location of computing device 800.\n Camera 838 may include one or more sensors to detect light.  In some implementations, camera 838 may receive video images, still images, or both.  Camera 838 may include a charged coupled device (CCD) sensor, a complementary metal oxide\nsemiconductor (CMOS) sensor, a photocell sensor, an IR sensor, any other suitable sensor, or any combination thereof.  In some implementations, camera 838 may include a device capable of generating light to illuminate a subject, for example, an LED\nlight.  Camera 838 may communicate information captured by the one or more sensor to sensor controller 820, to processor 824, to any other suitable equipment, or any combination thereof.  Camera 838 may include lenses, filters, and other suitable optical\nequipment.  It will be understood that computing device 800 may include any suitable number of camera 838.\n Audio equipment 834 may include sensors and processing equipment for receiving and transmitting information using acoustic or pressure waves.  Speaker 814 may include equipment to produce acoustic waves in response to a signal.  In some\nimplementations, speaker 814 may include an electroacoustic transducer wherein an electromagnet is coupled to a diaphragm to produce acoustic waves in response to an electrical signal.  Microphone 816 may include electroacoustic equipment to convert\nacoustic signals into electrical signals.  In some implementations, a condenser-type microphone may use a diaphragm as a portion of a capacitor such that acoustic waves induce a capacitance change in the device, which may be used as an input signal by\ncomputing device 800.\n Speaker 814 and microphone 816 may be contained within computing device 800, may be remote devices coupled to computing device 800 by any suitable wired or wireless connection, or any combination thereof.\n Speaker 814 and microphone 816 of audio equipment 834 may be coupled to audio controller 822 in processing equipment 804.  This controller may send and receive signals from audio equipment 834 and perform pre-processing and filtering steps\nbefore transmitting signals related to the input signals to processor 824.  Speaker 814 and microphone 816 may be coupled directly to processor 824.  Connections from audio equipment 834 to processing equipment 804 may be wired, wireless, other suitable\narrangements for communicating information, or any combination thereof.\n Processing equipment 804 of computing device 800 may include display controller 818, sensor controller 820, audio controller 822, processor 824, memory 826, communication controller 828, and power supply 832.\n Processor 824 may include circuitry to interpret signals input to computing device 800 from, for example, touchscreen 808 and microphone 816.  Processor 824 may include circuitry to control the output to display 806 and speaker 814.  Processor\n824 may include circuitry to carry out instructions of a computer program.  In some implementations, processor 824 may be an integrated electronic circuit based, capable of carrying out the instructions of a computer program and include a plurality of\ninputs and outputs.\n Processor 824 may be coupled to memory 826.  Memory 826 may include random access memory (RAM), flash memory, programmable read only memory (PROM), erasable programmable read only memory (EPROM), magnetic hard disk drives, magnetic tape\ncassettes, magnetic floppy disks optical CD-ROM discs, CD-R discs, CD-RW discs, DVD discs, DVD+R discs, DVD-R discs, any other suitable storage medium, or any combination thereof.\n The functions of display controller 818, sensor controller 820, and audio controller 822, as have been described above, may be fully or partially implemented as discrete components in computing device 800, fully or partially integrated into\nprocessor 824, combined in part or in full into combined control units, or any combination thereof.\n Communication controller 828 may be coupled to processor 824 of computing device 800.  In some implementations, communication controller 828 may communicate radio frequency signals using antenna 830.  In some implementations, communication\ncontroller 828 may communicate signals using a wired connection (not shown).  Wired and wireless communications communicated by communication controller 828 may use Ethernet, amplitude modulation, frequency modulation, bitstream, code division multiple\naccess (CDMA), global system for mobile communications (GSM), general packet radio service (GPRS), satellite, infrared, Bluetooth, Wi-Fi, WiMax, any other suitable communication configuration, or any combination thereof.  The functions of communication\ncontroller 828 may be fully or partially implemented as a discrete component in computing device 800, may be fully or partially included in processor 824, or any combination thereof.  In some implementations, communication controller 828 may communicate\nwith a network such as network 704 of FIG. 7 and may receive information from a knowledge graph stored, for example, in database 724 of FIG. 7.\n Power supply 832 may be coupled to processor 824 and to other components of computing device 800.  Power supply 832 may include a lithium-polymer battery, lithium-ion battery, NiMH battery, alkaline battery, lead-acid battery, fuel cell, solar\npanel, thermoelectric generator, any other suitable power source, or any combination thereof.  Power supply 832 may include a hard wired connection to an electrical power source, and may include electrical equipment to convert the voltage, frequency, and\nphase of the electrical power source input to suitable power for computing device 800.  In some implementations of power supply 832, a wall outlet may provide 720 volts, 60 Hz alternating current (AC).  A circuit of transformers, resistors, inductors,\ncapacitors, transistors, and other suitable electronic components included in power supply 832 may convert the 720V AC from a wall outlet power to 5 volts at 0 Hz (i.e., direct current).  In some implementations of power supply 832, a lithium-ion battery\nincluding a lithium metal oxide-based cathode and graphite-based anode may supply 3.7V to the components of computing device 800.  Power supply 832 may be fully or partially integrated into computing device 800, or may function as a stand-alone device. \nPower supply 832 may power computing device 800 directly, may power computing device 800 by charging a battery, may provide power by any other suitable way, or any combination thereof.\n The foregoing is merely illustrative of the principles of this disclosure and various modifications may be made by those skilled in the art without departing from the scope of this disclosure.  The above described implementations are presented\nfor purposes of illustration and not of limitation.  The present disclosure also may take many forms other than those explicitly described herein.  Accordingly, it is emphasized that this disclosure is not limited to the explicitly disclosed methods,\nsystems, and apparatuses, but is intended to include variations to and modifications thereof, which are within the spirit of the following claims.", "application_number": "15251989", "abstract": " Methods, systems, and computer-readable media are provided for collective\n     reconciliation. In some implementations, a query is received, wherein the\n     query is associated at least in part with a type of entity. One or more\n     search results are generated based at least in part on the query.\n     Previously generated data is retrieved associated with at least one\n     search result of the one or more of search results, the data comprising\n     one or more entity references in the at least one search result\n     corresponding to the type of entity. The one or more entity references\n     are ranked, and an entity result is selected from the one or more entity\n     references based at least in part on the ranking. An answer to the query\n     is provided based at least in part on the entity result.\n", "citations": ["5666502", "5946647", "6185558", "6513036", "6832218", "6847959", "6944612", "7028026", "7502770", "7562076", "7565139", "7603374", "7761414", "7765206", "7797336", "7818324", "7895196", "7933900", "7953720", "8005720", "8051104", "8069175", "8086604", "8204856", "8286885", "8316029", "8429103", "20020049752", "20020083039", "20020091688", "20030195877", "20040093321", "20050165744", "20050222975", "20060026147", "20070073748", "20070124291", "20070260594", "20080010273", "20080126143", "20090177644", "20090224867", "20100070448", "20100235351", "20100257171", "20110004608", "20110022549", "20110137883", "20110184981", "20110202493", "20120016678", "20120021783", "20120101858", "20120101901", "20120150572", "20120215773", "20120246153", "20120317103", "20120330906", "20130054542", "20130054569", "20130110833", "20130110863"], "related": ["13842591"]}, {"id": "20170069000", "patent_code": "10366422", "patent_name": "Quality visit measure for controlling computer response to query\n     associated with physical location", "year": "2019", "inventor_and_country_data": " Inventors: \nDuleba; Krzysztof (Seattle, WA)  ", "description": "BACKGROUND\n Aspects of physical locations, such as business establishments, may be determined from Internet documents that are related to the physical locations and/or explicit input of individuals that are related to the physical locations.  For example, a\nwebpage related to a business may be analyzed to identify a category of the business, a location of the business, etc. As another example, reviews submitted by users related to a business may be analyzed to determine an overall rating of the business. \nAs yet another example, an owner of the business or another user may manually input information to a service that maintains business information to have the business information reflected by the service.  For instance, an owner may enter the address,\noperating hours, webpage, and other information related to the business.  However, reliance solely on such aspects may have one or more drawbacks.  For example, such aspects may not be available for many physical locations, may contain significant bias,\nmay be outdated, may not be based on reliable data, and/or may not include certain unique aspects.\nSUMMARY\n This specification is directed generally to technical features for controlling the response of a computer to a query associated with a physical location using a quality visit measure that is based at least in part on the number and/or frequency\nof repeat visits by one or more individuals to that physical location.  In this manner, data related to visits by one or more individuals to a physical location may be used as an indication of the popularity of that location, with the incidences of\nrepeat visits used to effectively incorporate the \"quality\" of visits into the popularity indication.\n In some implementations, for example, computer interaction data associated with actual visits to a physical location may be used to determine a quality visit measure for the physical location, which in turn may be used to control a computer\nresponse to a query associated with the physical location.  Further, in some implementations, the quality visit measure may be based at least in part on the number and/or frequency of repeat visits by an individual, and weighted to emphasize greater\nnumbers of repeat visits by individuals.\n Controlling a computer response to a query based upon a quality visit measure determined from data related to repeat visits by individuals to a physical location gives rise to various technical advantages.  For example, in some implementations\nthe determination of such data and/or a quality visit measure may improve the accuracy of information that is identified as relevant to the query and/or provide information with an appropriate prominence to increase the likelihood that the information\nwill be consumed in response to the query.  Doing so may also decrease the likelihood that further searches will be needed to identify information, thereby preventing consumption of network and computer resources in response to such subsequent, and\notherwise unnecessary, queries.\n Therefore, in some implementations, a computer implemented method includes determining, by one or more processors, a group of computing interactions by one or more individuals pertaining to a physical location, wherein the computing interactions\nof the group are each indicative of an actual visit to the physical location by the one or more individuals; determining, by the one or more processors, from the determined group of computing interactions a number of actual visits to the physical\nlocation by each of the one or more individuals; determining, by the one or more processors, a quality visit measure for the physical location based at least in part on the determined number of actual visits to the physical location by each of the one or\nmore individuals, including weighting the quality visit measure to emphasize greater numbers of repeat visits by an individual among the one or more individuals; and controlling, by the one or more processors, a computer response to a query associated\nwith the physical location using the determined quality visit measure.\n In some implementations, the method may also include determining from the determined number of actual visits to the physical location a plurality of visitor counts, each of the plurality of visitor counts associated with a predetermined number\nof actual visits and equal to a number of individuals among the one or more individuals determined to have the predetermined number of actual visits to the physical location, where weighting the quality visit measure includes weighting the quality visit\nmeasure based upon the plurality of visitor counts.  In some implementations, weighting the quality visit measure based upon the plurality of visitor counts includes applying a different weight to each of the plurality of visitor counts, and in some\nimplementations, determining the quality visit measure includes determining the quality visit measure based upon a function of C.sub.i, w.sub.i, C.sub.0 and N, where C.sub.i is the visitor count equal to the number of individuals among the one or more\nindividuals having visited the physical location i times, w.sub.i is the weight applied to visitor count C.sub.i, C.sub.0 is an offset that deemphasizes the quality visit measure for the physical location in response to a dearth of determined computing\ninteractions for the physical location, and N is the number of visitor counts for the physical location.  Further, in some implementations, weights w.sub.1 .  . . w.sub.N increase from w.sub.1 to w.sub.N to emphasize when an individual among the one or\nmore individuals visits the physical location many times.\n In some implementations, the method also includes restricting the group of computing interactions to computing interactions of individuals matching an individual characteristic criterion, while in some implementations, the method includes\nrestricting the group of computing interactions to computing interactions associated with actual visits matching a temporal criterion.\n In some implementations, the method also includes detecting a recommendation of the physical location made by a first individual, and determining from the determined group of computing interactions an actual visit made to the physical location\nby a second individual after the recommendation made by the first individual, where determining the quality visit measure for the physical location is further based at least in part on the determined actual visit made to the physical location by the\nsecond individual after the recommendation made by the first individual.\n In addition, in some implementations, the method also includes determining from the determined group of computing interactions a first actual visit made to the physical location by a first individual, and determining from the determined group of\ncomputing interactions a second actual visit made to the physical location by a second individual that is a friend of the first individual, where determining the quality visit measure for the physical location is further based at least in part on the\nsecond actual visit being made to the physical location by the second individual after the first actual visit made by the first individual.  Further, in some implementations, the method also includes determining from the determined group of computing\ninteractions that the second individual visited the physical location with the first individual, where determining the quality visit measure for the physical location is further based at least in part on the second actual visit being made to the physical\nlocation by the second individual with the first individual after the first actual visit made by the first individual.\n In some implementations, each computing interaction is determined based upon a location of a computing device of an individual, a navigation request made by an individual, a check-in by an individual, or geotag data of an electronic file of an\nindividual.  In some implementations, the physical location is associated with a point of interest, the query is a user-generated query for points of interest within a geographical area or for points of interest of a category, and controlling the\ncomputer response to the query associated with the physical location using the determined quality visit measure includes ranking an information entry associated with the physical location relative to other information entries associated with other\nphysical locations matching the query based at least in part on the determined quality visit measure, and generating a search result response to the query including the information entry associated with the physical location.  In some such\nimplementations, generating the search result response includes including with the information entry additional information associated with the determined quality visit measure.\n Also, in some implementations, the physical location is associated with a point of interest, the query is a user-generated structured query for points of interest matching a visit quality criterion, and controlling the computer response to the\nquery associated with the physical location using the determined quality visit measure includes including an information entry associated with the physical location in a search result response based at least in part based upon the determined quality\nvisit measure meeting the visit quality criterion.  Further, in some implementations, the query is a query for information about the physical location, and controlling the computer response to the query associated with the physical location using the\ndetermined quality visit measure includes including information associated with the determined quality visit measure with an information entry associated with the physical location in a search result response.\n Other implementations may include a non-transitory computer readable storage medium storing instructions executable by a processor to perform a method such as one or more of the methods described above.  Yet another implementation may include a\nsystem including memory and one or more processors operable to execute instructions, stored in the memory, to perform a method such as one or more of the methods described above.\n It should be appreciated that all combinations of the foregoing concepts and additional concepts described in greater detail herein are contemplated as being part of the subject matter disclosed herein.  For example, all combinations of claimed\nsubject matter appearing at the end of this disclosure are contemplated as being part of the subject matter disclosed herein. BRIEF DESCRIPTION OF THE DRAWINGS\n FIG. 1 illustrates an example architecture of a computer system.\n FIG. 2 is a block diagram of an example distributed physical location processing environment.\n FIG. 3 is a flowchart illustrating an example method of determining a quality visit measure using the environment of FIG. 2.\n FIG. 4 is a flowchart illustrating an example method of determining a quality visit measure from the actual visits determined in the flowchart of FIG. 3.\n FIG. 5 is a flowchart illustrating another example method of determining a quality visit measure using the environment of FIG. 2.\n FIG. 6 is a flowchart illustrating an example method of determining a \"recommended\" visit measure using the environment of FIG. 2.\n FIG. 7 is a flowchart of an example method of determining a \"brought a friend\" visit measure using the environment of FIG. 2.\n FIG. 8 is a flowchart of an example method of processing a query and controlling a computer response thereto using the environment of FIG. 2.\n FIG. 9 is a display of an example output generated by the flowchart of FIG. 8.\nDETAILED DESCRIPTION\n In the implementations discussed hereinafter, data related to visits by one or more individuals to a physical location such as a place of business is used as an indication of popularity, with particular emphasis placed on subsequent or repeat\nvisits by an individual to the physical location.\n It has been found, for example, that finding a \"good\" restaurant can be a challenge, often due to the fact that web mentions and reviews can be unreliable indicators of restaurant quality.  Web mentions may not focus sufficiently on quality, as\nsome mentions may be negative due to an isolated bad experience, as well as the fact that as less popular restaurants may not have sufficient mentions from which to derive sufficient information.  Reviews may require substantial efforts on the part of\nusers, and may be written by unreliable authors having an axe to grind and/or having insufficient understanding of the subject matter.\n On the other hand, the actual patronage of individuals at restaurants and other place of businesses in many instances can provide an indication of the quality of such establishments in a way that does not have the same dependencies on any\npotential biases of those individuals and does not require any conscious effort on the part of those individuals.  If an individual has an unpleasant experience at a restaurant, that individual is less likely to ever return, so regardless of whether or\nnot the individual makes the effort to write a negative review or web mention, the fact that the individual never returns is likely a good indicator that the individual does not particularly like the restaurant.  Consequently, the fact that an individual\nis found to return to a restaurant multiple times and/or on a regular basis may be an indication that the individual likes the restaurant, as an individual that has a good experience at a restaurant is in general more likely to return for another visit. \nIn addition, in some implementations, an assumption may be made that the more times an individual visits the same restaurant, the more likely it is that the individual considers the restaurant to be a quality place of business.\n As such, in some implementations, a ranking signal referred to herein as a \"quality visit measure\" may be used as an indication of the relative quality of a physical location based upon visits made to that location.  A physical location, in this\nregard, may be considered to represent a point of interest, often a business establishment, and an actual visit represents an individual's actual physical presence at a physical location.  An actual visit, in some implementations, may be required to be\nof a sufficient duration to quality as a visit to the physical location, and the duration required may be dependent upon the type of physical location.  As but one example, for a full service restaurant, a visit that only lasts 5 minutes or less may be\ndisregarded as an actual visit since 5 minutes would be insufficient time to order and consume a meal at the restaurant, while for a dry cleaner or even a takeout restaurant, a 5 minute visit would still be sufficient to indicate an actual visit to the\nestablishment.\n In some implementations, visits may be determined based upon analysis of a group of computing interactions stored in one or more databases, and pertaining to one or more individuals who have been determined to have visited the physical location. Computing interactions, for example, may be based upon GPS location data, \"check-ins\" by individuals using social media or other online services, geotagged pictures or videos, navigation requests, etc., as will become more apparent below.\n In some implementations, the group of computing interactions may be analyzed to determine a number of actual visits to a physical locations by each individual, and from the determined numbers of actual visits, a quality visit measure may be\ndetermined.  Further, the quality visit measure may be weighted to emphasize greater numbers of repeat visits by different individuals.  Thus, for example, if it is found that certain individuals frequent a physical location on a regular basis, or that a\nlarge number of individuals return to a physical location multiple times, a greater quality visit measure may be determined.  It will also be appreciated that in addition to or in lieu of accumulating counts of actual visits, some implementations may\ndetermine frequencies of actual visits to physical locations related to numbers of visits within a time frame and/or amount of time between visits, e.g., to determine physical locations that are visited by an individual once per week, six times a month,\netc.\n The quality visit measure may be used, in some implementations, to rank a physical location higher in a list of search results.  Thus, for example, if a user is searching for a restaurant in a particular geographical area, a restaurant having a\nrelatively high quality visit measure may be promoted over other restaurants in the same area due to the frequency of revisits by other individuals.  In some implementations, a quality visit measure may be used as part of a structured query, e.g., so\nthat a user could search only for restaurants with many \"regulars\" or repeat customers determined to have visited N or more times.  Further, in some implementations, a quality visit measure may be used to provide information about a physical location,\neven to a proprietor of a physical location (e.g., \"36% customers have visited 5+ times, 24% customers have visited 2-4 times,\" etc.).\n In some implementations, a quality visit measure may be based upon accumulated visit counts for a plurality of individuals, referred to herein as a repeat visit measure.  In one implementation, for example, where there are multiple individuals\nand multiple physical locations, each individual may be processed by listing all of the physical locations visited by that individual, making a note of the number of times each physical location was visited by that user.  Then, every physical location\nmay be processed to count the number of individuals that visited the physical location 1 time, 2 times, 3 times, etc. Then, these counts may be weighted and combined to generate the quality visit measure for each physical location.  For example, in some\nimplementations, weights may be determined empirically to emphasize greater numbers of visits by individuals, such that individuals visiting only once are strongly deemphasized whereas individuals visiting numerous times are strongly emphasized.\n In some implementations, a quality visit measure may also incorporate additional considerations.  For example, specific scenarios that may be particularly indicative of quality may be incorporated into a calculation, and may be used as a type of\nquality visit measure that may be used individually or in combination with other types of quality visit measures, often using relationships between multiple individuals to identify particular types of actual visits.\n In some implementations, for example, the fact that an individual visited a physical location or point of interest (e.g., a restaurant) once and then later returned with a friend may be considered to be indicative of the fact that the individual\nliked the restaurant.  As such, another type of quality visit measure, referred to herein as a \"brought a friend\" visit measure, may be used as an indication of the prevalence of individuals to bring others to a physical location.  With such a measure,\nfor example, actual visits by individuals that are identified as connected through social media may be analyzed to detect when multiple individuals have visited a physical location at the same time, and then other visits by any of those individuals prior\nto the joint visits may be identified and effectively linked to the joint visits to generate a measure that is indicative of the likelihood of individuals to bring along friends to particular physical locations.\n Also, in some implementations, the fact that an individual received a recommendation for a physical location or point of interest (e.g., a restaurant) from a friend and later visited the restaurant may be considered to be indicative of the fact\nthat the friend liked the restaurant.  As such, another type of quality visit measure, referred to herein as a \"recommended\" visit measure, may be used as an indication of the prevalence of individuals to find a physical location of sufficient quality to\nmake recommendations to others in their social network.  With such a measure, for example, mentions or recommendations of a physical location by an individual may be identified, e.g., from social media, and subsequent visits to the physical location by\nothers connected to that individual via the social media (e.g., within a particular time frame) may effectively be tied back to the individual making the recommendation as a further indication of that individual's opinion of the physical location.\n In addition, the aforementioned types of quality visit measures may be combined in some implementations.  For example, in some implementations, the individual measures may be summed together after weighting some or all of the individual measures\nusing empirically-determined weights.\n Further details regarding selected implementations are discussed hereinafter.  It will be appreciated however that other implementations are contemplated so the implementations disclosed herein are not exclusive.\n Now turning to the Drawings, wherein like numbers denote like parts throughout the several views, FIG. 1 is a block diagram of electronic components in an example computer system 10.  System 10 typically includes at least one processor 12 that\ncommunicates with a number of peripheral devices via bus subsystem 14.  These peripheral devices may include a storage subsystem 16, including, for example, a memory subsystem 18 and a file storage subsystem 20, user interface input devices 22, user\ninterface output devices 24, and a network interface subsystem 26.  The input and output devices allow user interaction with system 10.  Network interface subsystem 26 provides an interface to outside networks and is coupled to corresponding interface\ndevices in other computer systems.\n In some implementations, user interface input devices 22 may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touchscreen incorporated into the display, audio input devices such as voice\nrecognition systems, microphones, and/or other types of input devices.  In general, use of the term \"input device\" is intended to include all possible types of devices and ways to input information into computer system 10 or onto a communication network.\n User interface output devices 24 may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices.  The display subsystem may include a cathode ray tube (CRT), a flat-panel device such as a liquid\ncrystal display (LCD), a projection device, or some other mechanism for creating a visible image.  The display subsystem may also provide non-visual display such as via audio output devices.  In general, use of the term \"output device\" is intended to\ninclude all possible types of devices and ways to output information from computer system 10 to the user or to another machine or computer system.\n Storage subsystem 16 stores programming and data constructs that provide the functionality of some or all of the modules described herein.  For example, the storage subsystem 16 may include the logic to perform selected aspects of the methods\ndisclosed hereinafter.\n These software modules are generally executed by processor 12 alone or in combination with other processors.  Memory subsystem 18 used in storage subsystem 16 may include a number of memories including a main random access memory (RAM) 28 for\nstorage of instructions and data during program execution and a read only memory (ROM) 30 in which fixed instructions are stored.  A file storage subsystem 20 may provide persistent storage for program and data files, and may include a hard disk drive, a\nfloppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges.  The modules implementing the functionality of certain implementations may be stored by file storage subsystem 20 in the storage\nsubsystem 16, or in other machines accessible by the processor(s) 12.\n Bus subsystem 14 provides a mechanism for allowing the various components and subsystems of system 10 to communicate with each other as intended.  Although bus subsystem 14 is shown schematically as a single bus, alternative implementations of\nthe bus subsystem may use multiple busses.\n System 10 may be of varying types including a mobile device, a portable electronic device, an embedded device, a desktop computer, a laptop computer, a tablet computer, a wearable device, a workstation, a server, a computing cluster, a blade\nserver, a server farm, or any other data processing system or computing device.  In addition, functionality implemented by system 10 may be distributed among multiple systems interconnected with one another over one or more networks, e.g., in a\nclient-server, peer-to-peer, or other networking arrangement.  Due to the ever-changing nature of computers and networks, the description of system 10 depicted in FIG. 1 is intended only as a specific example for purposes of illustrating some\nimplementations.  Many other configurations of system 10 are possible having more or fewer components than the computer system depicted in FIG. 1.\n Implementations discussed hereinafter may include one or more methods implementing various combinations of the functionality disclosed herein.  Other implementations may include a non-transitory computer readable storage medium storing\ninstructions executable by a processor to perform a method such as one or more of the methods described herein.  Still other implementations may include an apparatus including memory and one or more processors operable to execute instructions, stored in\nthe memory, to perform a method such as one or more of the methods described herein.\n Various program code described hereinafter may be identified based upon the application within which it is implemented in a specific implementation.  However, it should be appreciated that any particular program nomenclature that follows is used\nmerely for convenience.  Furthermore, given the endless number of manners in which computer programs may be organized into routines, procedures, methods, modules, objects, and the like, as well as the various manners in which program functionality may be\nallocated among various software layers that are resident within a typical computer (e.g., operating systems, libraries, API's, applications, applets, etc.), it should be appreciated that some implementations may not be limited to the specific\norganization and allocation of program functionality described herein.\n Furthermore, it will be appreciated that the various operations described herein that may be performed by any program code, or performed in any routines, workflows, or the like, may be combined, split, reordered, omitted, performed sequentially\nor in parallel and/or supplemented with other techniques, and therefore, some implementations are not limited to the particular sequences of operations described herein.\n FIG. 2 illustrates an example environment 50 in which one or more quality visit measures may be determined and used in determining aspects of physical locations (also referred to herein simply as \"locations\") based on computing interactions\n(also referred to herein as \"interactions\") that pertain to and/or are associated with the physical locations.  The example environment includes a client device 52, an aspect determination system 54, and an information system 56 interconnected with one\nanother by one or more networks 58.  Aspect determination system 54 may be implemented in one or more computers that communicate, for example, through a network (not depicted).  Aspect determination system 54 is an example of a system in which the\nsystems, components, and techniques described herein may be implemented and/or with which systems, components, and techniques described herein may interface.\n Client device 52, aspect determination system 54, and information system 56 each include one or more memories for storage of data and software applications, one or more processors for accessing data and executing applications, and other\ncomponents that facilitate communication over a network.  The operations performed by aspect determination system 54, and/or information system 56 may be distributed across multiple computer systems.\n Client device 52 may be a computer coupled to the aspect determination system 54, the information system 56, and/or other components (e.g., interactions database 60 and/or a component managing interactions database 60) through one or more\nnetworks 58 such as a local area network (LAN) or wide area network (WAN) such as the Internet.  The client device 52 may be, for example, a desktop computing device, a laptop computing device, a tablet computing device, a mobile phone computing device,\na computing device of a vehicle of the individual (e.g., an in-vehicle communications system, an in-vehicle entertainment system, an in-vehicle navigation system), or a wearable apparatus of the individual that includes a computing device (e.g., a watch\nof the individual having a computing device, glasses of the individual having a computing device).  Additional and/or alternative client devices may be provided.\n As described herein, in determining aspects of physical locations, aspect determination system 54 may utilize interactions from interactions database 60 that are indicative of computing interactions by individuals that pertain to the physical\nlocations.  One or more of the computing interactions may be indicative of activities of individuals via computing devices such as client device 52.  For the sake of brevity, only a single client device 52 is illustrated in FIG. 2 and described in some\nexamples herein.  However, activities of multiple individuals via multiple client devices may be utilized in determining aspects of physical locations.  Moreover, although an individual will likely operate a plurality of computing devices, and aspects of\nphysical locations may be determined based on user actions via multiple of the computing devices, for the sake of brevity, certain examples described in this disclosure will focus on the individual operating client device 52.\n Client device 52 may operate one or more applications and/or components such as those that facilitate user selections and/or input that may be indicative of a computing interaction pertaining to a physical location, those that provide location\ndata that may be indicative of a user interaction related to a physical location, and/or those that facilitate provision of search results, suggestions, and/or other information related to physical locations based on output of information system 56. \nThese applications and/or components may include, but are not limited to, a browser 66, a position coordinate component, such as a global positioning system (\"GPS\") component 68 (other position coordinate technologies such as cellular or Wi-Fi-based\ntriangulation may be used), a mapping application 70 (e.g., to obtain driving directions to or from the location), and so forth.  In some instances, one or more of these applications and/or components may be operated on multiple client devices operated\nby the individual.  Other components of client device 52 not depicted in FIG. 2 that may provide indications of interactions of the individual with a physical location may include, but are not limited to, a calendar application (e.g., based on an entry\nidentifying the location), a phone application (e.g., based on a call to or from a number associated with the location), an email application (e.g., based on an e-mailed receipt from the location, e-mailed reservations for the location), a social\nnetworking application (e.g., based on a post related to the location, a check-in to the location, a review of the location, a mention of the location), a virtual wallet application (e.g., based on a purchase associated with the location), a search\napplication (e.g., based on searches associated with the physical location), a camera application (e.g., based on a geotag included in photos captured via the camera), and so forth.  Some of the aforementioned example components may be standalone\ncomponents or may optionally be accessed via the browser 66 or another component.  It will be appreciated that at least a subset of the interactions will be indicative of an actual visit to a physical location by an individual, e.g., the physical\npresence of that individual at the physical location.\n Interactions database 60 may store records of computing interactions by a plurality of individuals that pertain to physical locations.  Generally, a computing interaction of interactions database 60 that pertains to a physical location includes\nan identifier of that physical location such as an address (e.g., latitude/longitude, street address), an alias, and/or an entity identifier.  The computing interaction may optionally include additional information related to the interaction such as, for\nexample, a date of the interaction, time(s) associated with the interaction (e.g., a single time, a time range, and/or time indicative of duration of the interaction), a confidence measure (e.g., based on confidence in the source of the interaction), and\nso forth.  Computing interactions that may be logged in interactions database 60 include, for example, directional queries seeking driving directions to the locations, searches related to the locations, location data from mobile devices, check-ins to the\nlocations, reviews of the locations, calendar entries identifying the locations, media items that include geotags identifying the locations, extracted reservation or receipt information (e.g., extracted from emails) related to the locations, browsing\nhistory of the individual related to the locations (e.g., indicating one or more documents accessed by the individual such as webpages), and/or starring or otherwise flagging the locations on a map and/or other interface.\n Various components may provide indication of computing interactions for storage in interactions database 60 and a separate component may optionally maintain interactions database 60.  Examples of components that may provide interactions for\nstorage in interactions database 60 include, for example, client device 52 and other computing devices of other individuals, information system 56 and other systems, an email system executing on one or more computing devices, navigation systems and/or\nGPS-enabled devices, and/or one or more other components that may identify interactions with a location.  Although only a single interactions database 60 is illustrated, in various implementations interactions database 60 may include multiple databases. \nFor example, a first database may include directional queries related to locations and a second database may include location data from mobile computing devices of individuals that are related to locations.  In some implementations, interactions database\n60 may include entries of a plurality of individuals and access to entries of an individual in database may be allowed for only the individual and/or one or more other individuals or components authorized by the individual such as aspect determination\nsystem 54.\n In situations in which the systems described herein collect personal information about individuals, or may make use of personal information, the individuals may be provided with an opportunity to control whether programs or features collect user\ninformation (e.g., information about an individual's social network, social actions or activities, profession, an individual's preferences, or an individual's current geographic location), or to control whether and/or how to receive content from the\ncontent server that may be more relevant to the individual.  Also, certain data may be treated in one or more ways before it is stored or used, so that personal identifiable information is removed.  For example, an individual's identity may be treated so\nthat no personal identifiable information can be determined for the individual, or an individual's geographic location may be generalized where geographic location information is obtained (such as to a city, ZIP code, or state level), so that a\nparticular geographic location of an individual cannot be determined.  Thus, the individual may have control over how information is collected about the individual and/or used.  In addition, in order to obtain the benefit of the techniques described\nherein, a user may have to install an application and/or invoke a setting on a computing device.\n Point of interest (\"POI\") database 62 may include a collection of entities, and for each of one or more of the entities, a mapping to one or more properties associated with the entity and/or one or more other entities related to the entity.  For\nexample, the POI database 62 may be a knowledge graph, such as a local knowledge graph that includes entities associated with businesses and/or other locations and includes properties for each of the entities such as phone numbers, addresses, open hours,\nmost popular hours, etc.\n In some implementations, POI database 62 may be utilized to identify one or more locations that are associated with computing interactions of interactions database 60.  For example, physical locations identified in POI database 62 may be\nassociated with an address, longitude and latitude, and/or other coordinates that may be utilized to map interactions of interactions database 60 with physical locations.  Also, for example, each of one or more physical locations identified in POI\ndatabase 62 may be associated with one or more aliases for the physical location and/or aliases for properties of the location and those aliases may be utilized to map interactions of interactions database 60 with physical locations.\n In various implementations, grouping engine 72 may utilize information from POI database 62 to determine a group of interactions from interactions database 60.  For example, grouping engine 72 may identify a point of interest in POI database 62\nthat is associated with longitude and latitude coordinates and further determine a group of interactions that are associated with those same longitude and latitude coordinates and/or associated with coordinates within a threshold distance of those\nlongitude and latitude coordinates.  For instance, grouping engine 72 may identify Restaurant 1 and a latitude and longitude for Restaurant 1 from POI database 62.  Grouping engine 72 may further determine a group of interactions that are associated with\nlocations that are within 50 yards of the identified latitude and longitude of Restaurant 1.  Also, for example, Restaurant 1 may be associated with a street address in POI database 62 and grouping engine 72 may identify one or more interactions from\ninteractions database 60 that include navigational directions to the street address of Restaurant 1.  Also, for example, grouping engine 72 may identify one or more aliases for Restaurant 1 in POI database 62 and determine a group of interactions from\ninteractions database 60 that are associated with the one or more of the aliases (and optionally associated with additional properties such as a location near Restaurant 1 or an alias of a category associated with Restaurant 1).\n In this specification, the term \"database\" will be used broadly to refer to any collection of data.  The data of the database does not need to be structured in any particular way, or structured at all, and it can be stored on storage devices in\none or more locations.  Thus, for example, the databases 60, 62, and/or 64 may each include multiple collections of data, each of which may be organized and accessed differently.  Also, for example, all or portions of the databases 60, 62, and/or 64 may\nbe combined into one database and/or may contain pointers and/or other links between entries in the database(s).  Also, in this specification, the term \"entry\" will be used broadly to refer to any mapping of a plurality of associated information items. \nA single entry need not be present in a single storage device and may include pointers or other indications of information items that may be present on other storage devices.  For example, an entry that identifies a computing interaction in interactions\ndatabase 60 may include multiple nodes mapped to one another, with each node including an identifier of an entity or other information item that may be present in another data structure and/or another storage medium.\n Aspect determination system 54 may determine one or more aspects to assign to a location based on computing interactions of individuals that pertain to the location.  In various implementations, aspect determination system 54 may include a\ngrouping engine 72, an interaction measure engine 74, an aspect engine 76, and/or a textual summary generation engine 78.  In some implementations, one or more of engines 72, 74, 76, and/or 78 may be omitted.  In some implementations, all or aspects of\none or more of engines 72, 74, 76, and/or 78 may be combined.  In some implementations, one or more of engines 72, 74, 76, and/or 78 may be implemented in a component that is separate from aspect determination system 54.\n Generally, grouping engine 72 determines groups of computing interactions that are each associated with a physical location.  For example, grouping engine 72 may determine a group of computing interactions from interactions database 60 that\nincludes computing interactions of individuals that pertain to a particular physical location such as a business, a landmark, a tourist attraction, or a park.  As described herein, the grouping engine 72 may optionally take one or more additional\nparameters into account in determining a group of user interactions--that are in addition to the group being associated with a physical location.  For example, the grouping engine 72 may determine a group of interactions that are associated with one or\nmore physical locations and that are also associated with: one or more dates such as particular dates, particular date ranges, and/or particular day(s) of the week; one or more times such as a single time, a time range, and/or times indicative of\ndurations of the interactions; relative data differences (e.g., when successive visits by an individual are within a particular number of days); a threshold confidence measure (e.g., based on confidence in the source of the interaction); one or more user\nattributes such as an attribute indicating a particular user group (e.g., local individuals, travelers, etc.); one or more particular sources (e.g., only location data, only driving directions); relationships between individuals (e.g., to detect joint\nvisits by multiple individuals, including multiple individuals that are connected via social media); and so forth.\n For example, grouping engine 72 may determine a group of interactions of individuals at a location at one or more time periods (e.g., interactions of individuals at Restaurant 1 between 11 am and 1 pm).  Also, for example, grouping engine 72 may\ndetermine a group of interactions that includes interactions of individuals at multiple locations of a particular business (e.g., interactions of individuals with various locations of a restaurant chain).  Which parameters are utilized by grouping engine\n72 in determining one or more groups of interactions may be based at least in part on the desired aspect to be determined about a location.  Examples are provided herein of example aspects that may be determined for a location and parameters that may\noptionally be taken into account in determining one or more groups of interactions for utilization in determining the example aspects.\n In some implementations of determining a group, grouping engine 72 may determine that one or more interactions are outliers and not include the interactions in the group.  Outlying interactions may be identified and removed from groups based on\none or more techniques, such as truncated means and/or Winsorized means, and/or interactions that are below a threshold duration and/or above a threshold duration may be discarded.  For example, grouping engine 72 may not include interactions that are\nassociated with a duration of visit that is greater than a threshold as they may be indicative of interactions of employees instead of customers.  Also, for example, grouping engine 72 may identify opening hours and/or closing hours of a location and\ndetermine that interactions from individuals that are present before opening time and/or after closing time are likely employees.  Also, for example, in some implementations one or more interactions may be associated with a confidence level and only\ninteractions that satisfy a threshold confidence level may be included by grouping engine 72 in a group of interactions.  For example, an interaction that indicates multiple potential locations may be associated with a low confidence level if an exact\nlocation may not be accurately determined (e.g., the interaction may be based on inaccurate location data that indicates an area that encompasses multiple points of interest).\n In some implementations, grouping engine 72 may determine a group of interactions indicative of joint visits by multiple individuals, e.g., when multiple individuals visit a location as a single party (e.g., a group of friends having dinner\ntogether at a restaurant).  As such, grouping engine 72 may access a social media account for an individual to and attempt to identify overlapping visits by his or her friends within the same time frame.  In some implementations, joint visits may be\nindicated by similar arrival and departure times, e.g., to exclude instances where friends just happen to be at the same location but are part of a different party.  Visits that are representative of joint visits may be specifically marked or linked by\ngrouping engine in some implementations and may include indications of the individuals participating in those joint visits.  In other implementations, determinations of joint visits, including accesses to social media accounts to identify an individual's\nfriends, may be performed in other components, e.g., within interaction measure engine 74.\n Interaction measure engine 74 may determine one or more computing interaction measures based on the one or more groups of computing interactions determined by grouping engine 72.  For example, grouping engine 72 may determine a group of\ninteractions that includes computing interactions by that pertain to a location and interaction measure engine 74 may determine one or more measures therefrom.  Among other measures, interaction measure engine 74 may determine one or more measures\nindicative of \"quality\" visits by one or more individuals, herein referred to as quality visit measures.  For example, grouping engine 72 may determine a group of interactions of one or more individuals with Restaurant 1 and interaction measure engine 74\nmay determine a quality visit measure such as a repeat visit measure that is generally indicative of how many individuals visit Restaurant 1 on a repeat and/or regular basis.  For such a measure, the fact that a particular individual tends to patronize\nthe restaurant on a regular basis, or at least is willing to return after a prior visit, may be considered to be an indication that the individual has an overall positive opinion of the restaurant, and thus an indication of the popularity or quality of\nthe restaurant.  Furthermore, the collective incidence of repeat and/or regular visits by multiple individuals may be considered to be an even greater indication of the popularity or quality of the restaurant.\n Another type of quality visit measure that may be determined by interaction measure engine 74 is a \"brought a friend\" visit measure, which, as noted above, is based upon the fact that an individual who has visited a location once and then later\nreturned with a friend may be considered to be indicative of the individual liking the location.  Yet another type of quality visit measure that may be determined by interaction measure engine 74 is a \"recommended\" visit measure, which, as noted above,\nis based on the fact that an individual who has received a recommendation for a location from a friend and later visited the location may be considered to be indicative of the friend liking the location.\n Other types of quality visit measures may also be determined by interaction measure engine 74, as will be appreciated by those of ordinary skill having the benefit of the instant disclosure.\n In addition, in some implementations, interaction measure engine 74 may determine additional measures that may be combined with and/or utilized separately of a quality visit measure.  For example, in some implementations, interaction measure\nengine 74 may determine measures that are indicative of durations of interactions of the individuals indicated by a group of interactions.  In some implementations, the measures may include an average or median duration of visits (e.g., as indicated by a\nlength of time an individual's GPS reading remains within a particular radius, or the time between an ingress and egress GPS reading).  In some implementations, the measures may additionally or alternatively include a continuous or discrete distribution\nof durations of visits, e.g., a mean with a standard deviation, a vector including all lengths of visits or a count of lengths of visits for one or more durations (e.g., a count of durations from 0-5 minutes, a count of durations from 5-10 minutes,\netc.), and so forth.  Also, for example, interaction measure engine 74 may determine, based on a group of interactions for a location, one or more measures indicative of a quantity of visits of individuals to the location.  For instance, the measures may\ninclude one or more of a raw count of the interactions of the group, an average and/or median count per day or other time period, and/or a continuous or discrete distribution of quantity of interactions (e.g., a mean with a standard deviation, a vector\nincluding raw counts or averages for each of a plurality of time periods (e.g., a count of interactions each day for the past two weeks; a count of interactions during a first time period, a second time period, etc.)).  Which measures are determined by\ninteraction measure engine 74 may be based at least in part on the desired aspect to be determined about a location.  Examples are provided herein of example aspects that may be determined for a location and one or more measures that may optionally be\ntaken into account in determining the example aspects.\n Aspect engine 76 may determine an aspect of a location based on the one or more interaction measures determined by interaction measure engine 74, and may assign the aspect to the location.  For example, the aspect engine 76 may associate the\naspect with the location in POI database 62.  As one example of determining an aspect, interaction measure engine 74 may determine a quality visit measure that indicates whether a restaurant has many regular or repeat customers, and aspect engine 76 may\ncompare that quality visit measure to a second measure such as a threshold to determine whether the restaurant location has enough such customers so as to be characterized as being \"frequented by regulars.\" As another example, interaction measure engine\n74 may determine a quality visit measure that indicates whether a restaurant has many visits tied to prior visits by friends and/or recommendations by friends, and aspect engine 76 may compare that quality visit measure to a second measure such as a\nthreshold to determine whether the restaurant location has enough such customers so as to be characterized as being \"frequently recommended by friends.\"\n Aspect engine 76 may also determine additional aspects of a location based upon interaction measures, e.g., for a restaurant, aspects such as \"fast food\" or \"sit down,\" \"trending up\" or \"trending down,\" \"breakfast/lunch/dinner spot,\" \"weekday\nspot\" or \"weekend spot,\" \"most popular Italian restaurant in the neighborhood,\" etc.\n Textual summary generation engine 78 may be configured to automatically generate one or more textual summaries about a physical location based on signals provided by various components, such as interaction measure engine 74 and/or aspect engine\n76.  For example, in some implementations, textual summary generation engine 78 may be configured to automatically generate a textual description of a physical location based on comparison of two or more interaction measures.  This comparison may be the\nsame comparison that yields one or more aspects that are assigned to the physical location, and in fact, in some implementations, textual summary generation engine 78 may additionally or alternatively automatically generate a textual description of a\nphysical location based on one or more aspects assigned to the physical location.  Textual summaries may fall into various categories, including textual summaries that describe a physical location in comparison to a \"peer group\" of similar locations (the\npeer group may include the location itself, too), and textual summaries that describe absolute characteristics of a physical location.\n Textual summaries that compare a physical location to a peer group may include textual summaries that compare the physical location to other physical locations that satisfy one or more criteria.  For example, the textual summary \"neighborhood\nbar most frequented by locals in the East Village\" compares the physical location (a bar in the East Village) to a peer group of bars in the East Village and relies in part on a quality visit measure to assess how many individuals who are local to the\narea are repeat customers of that location.\n Likewise, the textual summary \"most searched for Indian restaurant in Silicon Valley\" compares the physical location (an Indian restaurant in Silicon Valley) to a peer group of Indian restaurants in Silicon Valley.  \"The most popular bakery in\nDeer Park\" compares the physical location (a bakery in Deer Park) to a peer group of bakeries in Deer Park.  \"The cheapest French restaurant with a full bar in Queens\" compares the physical location (a French restaurant in Queens) to a peer group of\nFrench restaurants in Queens that have full bars.\n The peer group to which a physical location is compared may be selected based on one or more criteria.  These criteria may include but are not limited to being located within a particular geographic area, being of a particular category (e.g.,\nIndian restaurant, Italian restaurant, Mexican restaurant, bar, bakery, coffee shop, etc.), selling goods/services within a particular price range, having one or more characteristics (repeat customers, visits by friends, quiet ambiance, outdoor seating,\nfull bar, waterfront view, live music, etc.), and so forth.\n The one or more criteria for including physical locations in a peer group may be selected, e.g., by grouping engine 72, based on various signals.  These signals may come from various sources, such as client device 52 (directly or indirectly), or\nfrom information system 56.  One such signal is a particular location of potential interest for an individual.  If the individual indicates (explicitly or implicitly) potential interest in booking a room at a particular fancy hotel, then criteria for\nselecting a peer group of hotels may include hotels having a similar price range as the particular hotel, or other hotels that best match the individual's particular preferences (e.g., hotels having particular number of stars, hotels with gyms, etc.).\n Another signal for selecting the one or more criteria for including physical locations in a peer group may additionally or alternatively be contextual data associated with an individual (as opposed to the individual's computing device).  Suppose\nan online calendar entry associated with an individual indicates that the individual is scheduled to be in a particular geographical area soon after lunch.  Suppose further that the individual uses her phone to search for \"quick lunch spots.\" Grouping\nengine 72 may select, e.g., from interactions database 60, interactions that pertain to locations at or near that geographic location that are known to provide relatively fast lunch options.  Other user-related contextual data that may be used to\ndetermine criteria for selecting a peer group of physical locations may include user preferences, user budget, user social network status updates, and so forth.\n Another signal for selecting the one or more criteria for including physical locations in a peer group may additionally or alternatively be non-user related information such as a population or size of a geographic area.  For example, suppose the\nindividual is located in Hollywood and searches for \"Indian food.\" A textual summary describing a particular search result as \"the best Indian restaurant in California\" would not be as useful as, for example, \"the best Indian restaurant in Los Angeles.\"\nThus, a size and/or population of a geographic area associated with an individual's location may be used, e.g., by grouping engine 72, to determine criteria for selecting interactions from interactions database 60.  In some implementations, if a\nparticular physical location is unique within a geographic area (or more generally, the sole member of a particular peer group), a special textual summary may be generated, e.g., by textual summary generation engine 78, such as \"the only bourbon bar in\nSt.  Matthews.\"\n Another categories of textual summaries includes textual summaries that describe a physical location in its own, absolute terms, rather than in comparison to a peer group of other physical locations.  Computing interactions that pertain to a\nphysical location may be analyzed, for instance, in terms of different time periods (e.g., lunch hours versus dinner hours), as described above.  Examples of absolute summaries include but are not limited to \"a significant number of visitors come revisit\nthis location,\" \"this place is getting busier,\" \"this spot is more popular for brunch than for dinner,\" and so forth.  In various implementations, such textual summaries may be generated, for instance, by textual summary generation engine 78 based on one\nor more aspects assigned to a physical location by aspect engine 76.\n Generally, information system 56 utilizes interaction measures and/or aspects that have been determined by aspect determination system 54, and/or textual summaries generated by textual summary generation engine 78, in providing information to a\nuser, e.g., by generating a computer response to a query generated by that individual.  The information system 56 may utilize the aspects and/or textual summaries, for example, in determining the information to provide (e.g., the aspects may be included\nin the information) and/or determining when or how to provide the information (e.g., the interaction measures and/or aspects may be utilized to select the information and/or to rank the information relative to other information).  For example,\ninformation system 56 may identify individual interest in one or more locations and provide information related to the one or more locations based on determined aspects of the locations.  Further, interaction measures such as quality visit measures may\nbe used to increase the relative ranking of locations that are associated with repeat visits and/or visits associated with prior recommendations or visits.\n In some implementations, textual summary generation engine 78 may be integral with information system 56 instead of with aspect determination system 54.  In other implementations, textual summary generation engine 78 may be a standalone\ncomponent independent from aspect determination system 54 and information system 56.\n Information system 56 may be, for example, a search engine, a notification and/or suggestion system, and/or one or more other systems that may provide information related to locations to a computing device of an individual based on implicit\nand/or explicit indications from the computing device (e.g., based on a query from the computing device, a selection of an individual via the computing device, based on access of an application via the computing device such as access of a suggestion\nsystem).  Information system 56 may optionally be in communication with information database 64.  Information database 64 may include information that may be utilized by information system 56 to provide information to the individual.  For example,\ninformation database 64 may include an index of documents, webpages, and/or other information items that may be utilized to identify information to provide to the individual.  As one example, information system 56 may be a search engine and the\ninformation database 64 may be a search index utilized to identify documents responsive to search queries.  As described herein, such documents may optionally be ranked for search queries based on determined aspects that are associated with the\ndocuments.  In some implementations, information system 56 and/or another component may optionally update information database 64 based on interaction measures determined by interaction measure engine 74, aspects determined by aspect engine 76 and/or\ntextual summaries generated by textual summary generation engine 78.  For example, when information database 64 is an index of webpages and other documents, the index may be updated to include information related to interaction measures and/or aspects\ndetermined herein (e.g., a webpage associated with a location may be indexed based on determined aspects of that location).\n In some implementations, information system 56 may select and/or rank search results responsive to a submitted search query based on determined interaction measures and/or aspects.  For example, an individual may provide a search query of\n\"neighborhood bars\" and information system 56 may identify information in information database 64 related to one or more nearby locations that are bars.  Based on inclusion of \"neighborhood\" in the search query, the information system 56 may further\nidentify that one or more of the identified locations are associated with determined aspects that are indicative of being frequented by regulars.  The information system 56 may select such identified locations for providing in response to the search\nquery and/or promote the ranking of such locations in search results provided in response to the search query, e.g., based upon a quality visit measure.\n In some implementations, one or more search results may be selected and/or ranked based on determined interaction measures and/or aspects, even when terms of a query do not indicate interest in the determined interaction measures and/or aspects. For example, the individual may provide a search query of \"Italian restaurants\" and information system 56 may identify one or more webpages responsive to the query via information database 64.  Information system 56 may identify one or more nearby\nlocations in POI database 62 that are associated with the webpages.  Based at least in part on a quality visit measure such as a repeat visit measure, any nearby locations that are found to have high incidences of repeat customers may be promoted and\ndisplayed higher in the search results.  In some implementations, ranking signals based upon quality visit measures may be combined with other ranking signals, e.g., based upon user reviews, popularity on the web, and other potential indicators of\nquality or popularity of such locations.\n In some implementations, information system 56 may provide information related to a location independent of any query submission of an individual.  For example, information system 56 may be a notification or suggestion system that may provide\ninformation related to popular locations without the individual submitting a query related to a location.  For instance, the information may be \"pushed\" to the individual and/or provided in response to the individual accessing the suggestion system or\nselecting an interface element in the suggestion system.  Also, for example, information system 56 may provide one or more suggestions responsive to a selection of the individual that is not the individual explicitly typing, speaking, or otherwise\ninputting a query.  For instance, the individual may select a user interface element to request \"popular restaurants\" nearby and information system 56 may select information related to one or more restaurants based on determined aspects that indicate the\nrestaurants are \"popular\".  Also, for example, information system 56 may be a mapping application that may provide a map with various points of interest for display on a computing device of an individual.  Information system 56 may determine to include\none or more of the points of interest based on association of the points of interest to one or more determined aspects and/or may determine to display one or more of the points of interest more prominently and/or with additional descriptions based on\nassociation of the points of interest to one or more determined aspects and/or based on associated textual summaries generated by textual summary generation engine 78.  As such, a computer response to a query in some implementations may refer to a query\nmade internally within an information system or made by another system, and not specifically generated by an individual.\n Also, in some implementations, interaction measures such as quality visit measures may be used in structural searches, e.g., by providing checkboxes in advanced searches so that users may elect to search only for frequently revisited locations. \nIn some implementations, interaction measures such as quality visit measures may also be displayed to users, as well as to owners or other parties affiliated with the locations, e.g., to enable an owner to see what percentage of his or her customers was\nrepeat customers and/or how frequently a typical customer returned to the location.\n Now turning to FIG. 3, this figure is a flowchart illustrating an example method of determining a quality visit measure.  For convenience, the operations of the flowchart are described with reference to a system that performs the operations. \nThis system may include various components of various computer systems.  For instance, some operations may be performed by one or more components of the aspect determination system 54, such as grouping engine 72 and interaction measure engine 74. \nMoreover, while operations of the method of FIG. 3 are shown in a particular order, this is not meant to be limiting.  One or more operations may be reordered, omitted or added.\n The flowchart may be used, for example, to determine a quality visit measure for a single physical location.  The flowchart begins in block 100 by determining a group of interactions that pertain to a particular physical location, e.g., using\ngrouping engine 72 to determine those interactions in interactions database 60 that are pertinent to the physical location in question.  In some implementations, a group of interactions may be determined that includes interactions of individuals based on\none or more characteristics of the individuals, e.g., to restrict the interactions based on an individual characteristic criterion.  For example, grouping engine 72 may determine a group of interactions that are associated with certain user attributes\nand/or certain user groups.  In some implementations, grouping engine 72 may determine a group of interactions that occurred during a time period, e.g., to restrict the interactions based on a temporal criterion.  For example, grouping engine 72 may\ndetermine a group of interactions that occurred during a particular week, month, day of the week, and/or during a particular time of day over a period of time (e.g., interactions during lunchtime over the last month).  The interactions, as noted above,\nmay include various types of interactions that are associated with actual visits to the physical location by one or more individuals, and as such, in block 102 the number of actual visits to the physical location by each of one or more individuals may be\ndetermined based upon the group of interactions pertaining the physical location, e.g., using the interaction measure engine 74.  In some implementations, for example, the group of interactions may be used to determine that individual A visited the\nlocation 10 times, individual B visited the location 1 time, and individual C did not visit the location.  Next, in block 104 interaction measure engine determines a quality visit measure for the location based upon the determined numbers of actual\nvisits by each of the one or more individuals, and the flowchart is complete.\n It will be appreciated that a quality visit measure may be determined by interaction measure engine 74 in block 110 in a number of manners in different implementations.  As shown in the flowchart of FIG. 4, for example, visitor counts may be\ndetermined (block 110) from the actual visits determined in block 102.  A visitor count, in this regard, may be considered to include the number of individuals that have visited a physical location a predetermined number of times.  Thus, for example, one\nvisitor count may be associated with one visit, and may include a count of the number of visitors who have visited the location one time.  Another visitor count may be associated with five visits, and may include a count of the number of visitors who\nhave visited the location five times.  Next, in block 112, the visitor counts may be weighted, e.g., to emphasize visitor counts associated with higher numbers of visits over visitor counts associated with lower numbers of visits, and in block 114, the\nweighted visitor counts may be used to calculate the quality visit measure.  The flowchart of FIG. 4 is then complete.\n In some implementations, the quality visit measure may be based upon a formula that is based upon the accumulated visitor counts, e.g. as illustrated by equation (1) below:\n .times..times..times.  ##EQU00001## where C.sub.i is a visitor count equal to the number of individuals having visited the physical location i times, w.sub.i is the weight applied to visitor count C.sub.i, and C.sub.0 is an offset that is used\nto deemphasize physical locations lacking sufficient visits by individuals to make a meaningful assessment.  In some implementations, the weights w.sub.i may be determined empirically, e.g., by tuning experimentally to match the responses of local\nrecommendation engines.  Further, generally the weights may be selected to emphasize greater numbers of visits by individuals, such that individuals visiting only once are strongly deemphasized (e.g., close to 0) whereas individuals visiting numerous\ntimes are strongly emphasized (e.g., close to 1).  Other implementations may rely on different formulas and/or calculations, however, and as such, the formula discussed herein is not the exclusive manner in which a quality visit measure may be\ncalculated.\n Next, turning to the flowchart of FIG. 5, it may be desirable in some implementations to process multiple locations collectively in order to determine quality visit measures for a set of physical locations, e.g., within program code executing in\ninteraction measure engine 74.  Nested FOR loops may be initiated in blocks 120 and 122, for example, to process each individual among a group of individuals and each physical location visited by each individual in the group.  For each such\nindividual/visited location, block 124 may list the location and record the number of times the individual visited that location.  Block 124 is repeated for every visited location for the current individual, and once all visited locations have been\nprocessed, block 122 returns control to block 120 to process the next individual and the locations visited thereby.  Once all individuals have been processed, block 120 then passes control to block 126 to initiate a FOR loop that processes each location. For each location, block 128 generates visitor counts by counting the number of individuals that have visited the location one time, two times, three times, etc. Then, in block 130, the quality visit measure for that location is calculated, e.g., using\nequation (1) above.\n The quality visit measure calculated above in FIGS. 3-5 incorporates a repeat visit measure, indicative of the number and/or frequency of repeat visits by individuals.  As noted above, however, other types of measures (e.g., \"brought a friend\"\nvisit measures and/or \"recommended\" visit measures) may be utilized as quality visit measures, and in some implementations, multiple quality visit measures may be combined.  For example, in some implementations, an overall quality visit measure may be\ndetermined by weighting each of a repeat visit measure, a \"brought a friend\" visit measure and a \"recommended\" visit measure and summing or averaging the weighted values.\n FIG. 6, for example, is a flowchart illustrating an example method of determining a \"recommended\" visit measure.  In this flowchart, it is assumed that interaction measure engine 74 additionally has access to social media information about each\nindividual, in particular any postings by an individual to various social media sites, as well as any other individuals identified as \"friends\" of that individual.  The flowchart begins in block 140 by detecting a recommendation made by a first\nindividual for a physical location, e.g., via social media, via email, via text message.  Detection may include, for example, performing text analysis to detect a physical location, identifying the physical location through a check-in or other location\ninformation associated with a social media posting, etc. In addition, text analysis may be used to determine if the content of the posting is positive in nature or otherwise indicative of a recommendation, e.g., a posting that states \"Best fried chicken\nI've ever had!\" and that is tagged with the location of a Restaurant A, or an email or text to another individual stating \"You really need to try Restaurant A!\"\n Next, in block 142 the first individual's list of friends is accessed, and in block 144 each of the individuals determined to be friends of the first individual are processed to determine if any actual visits were made to the physical location\nby any friend subsequent to the recommendation.  Then, in block 146 a quality visit measure is determined, e.g., a \"recommended\" visit measure or a quality visit measure based at least in part on such a measure.  In some implementations, for example, the\nnumber of actual visits by friends subsequent to an individual making a recommendation may be accumulated and weighted.  The determination of the quality visit measure is then complete.\n FIG. 7 is a flowchart illustrating an example method of determining a \"brought a friend\" visit measure.  In this flowchart, it is also assumed that interaction measure engine 74 additionally has access to social media information about each\nindividual, in particular any other individuals identified as \"friends\" of that individual.  The flowchart begins in block 150 by detecting an actual visit made by a first individual to a physical location.  Next, in block 152 the first individual's list\nof friends is accessed, and in block 154 each of the individuals determined to be friends of the first individual are processed to determine if any joint visits were made to the physical location by any friend subsequent to the recommendation, i.e., to\ndetermine whether the first individual and the friend visited the location together.  Then, in block 156 a quality visit measure is determined, e.g., a \"brought a friend\" visit measure or a quality visit measure based at least in part on such a measure. \nIn some implementations, for example, the number of joint visits by friends with the first individual subsequent to an individual making the first visit may be accumulated and weighted.  The determination of the quality visit measure is then complete.\n Now turning to FIG. 8, this figure is a flowchart illustrating an example method of processing a query and controlling a computer response thereto, which illustrates one manner in which a quality visit measure may be used to improve computer\nprocessing of queries.  The flowchart begins in block 160 by receiving a free or structured query from a user.  A free query, for example, may include a list of keywords or a natural language input and may or may not include any specific request from a\nuser regarding repeat visits.  A structured query, on the other hand, may include user controls such as text boxes, check boxes, etc., through which a user may generate a query with generally greater specificity, including requests regarding repeat\nvisits.  As noted above, for example, a structured query may be used in some implementations to specifically request to receive only results that match a particular visit quality criterion, e.g., to show only results that have a number and/or frequency\nof repeat visits above a particular threshold.  In addition, in some instances, a query may be directed specifically to information about a particular physical location, while in other instances, a query may be directed to multiple physical locations\nmatching some other criterion.\n Next, block 162 determines one or more matching information entries for the received query using the quality visit measure associated with various physical locations, and block 164 then ranks the matching information entries using the quality\nvisit measure.  Thus, for example, in some implementations, if a user specifies a particular visit quality criterion, block 162 may include or exclude information entries for particular physical locations based upon quality visit measures for those\nlocations.  In addition, in some implementations, block 164 may rank information entries based upon quality visit measures for associated physical locations, e.g., to promote physical locations having higher quality visit measures.  It will be\nappreciated that in other implementations, a quality visit measure may be used only to determine matching information entries, or only to rank matching information entries.\n Next, block 166 generates and communicates a search result response to the query, e.g., including one or more information entries matching the query.  In some implementations, additional information associated with a quality visit measure may\nalso be incorporated into some or all of the information entries, e.g., as generated by textual summary generation engine 78.  For example, in some implementations, if a physical location is found to have a relatively high quality visit measure, an\nindication such as \"lots of repeat customers\" may be included with an information entry.  In other implementations, however, no additional information may be included.  It will be appreciated that once communicated, the search result response may be\nreceived and/or consumed by a user device, e.g., client device 52.  In some implementations, for example, some or all of the search result response may be rendered on a client device and displayed to a user.  Processing of the query is then complete.\n It will be appreciated that a wide variety of queries may be processed and/or a wide variety of responses may be generated using a quality visit measure in various implementations.  FIG. 9, for example, illustrates a computer display 170 that\nmay be generated, rendered and/or displayed in response to a query, e.g., on a client device, and that illustrates various features described herein.  Display 170 includes a search box 172 illustrating an example query for \"neighborhood bar,\" along with\ndisplays for a number of information entries 174 representing the search results generated in response to the query.  In this example, it is assumed that the location of the client device has been provided along with the query such that bars close to the\ncurrent location of the client device will be returned (here, New York City).  An option may also be provided to specify another location, as shown in FIG. 9.  Also, in this example it is assumed that the search for a \"neighborhood bar\" implies that the\nuser is interested in an establishment that serves mostly regular customers, and as such, a quality visit measure may be used at least to emphasize search results related to bars for which greater numbers of repeat visits are detected.\n The displayed search results include three information entries 174 for a Bar A, Bar B and Bar C. Each bar has a rating based upon 1-4 stars, and it may be seen that Bar A and Bar B, while being lower rated (e.g., based upon user-submitted\nreviews), are ranked ahead of Bar C, because each of Bar A and Bar B has been found to have higher quality visit measures than Bar C. Also illustrated is additional information for each of Bar A and Bar B based upon a quality visit measure associated\nwith each bar.  For Bar A, for example, the information entry states \"the most popular bar for locals,\" indicating that Bar A is determined to have the greatest number of customers with the greatest number of repeat visits each, and moreover, that the\ncustomers that are assessed in this determination are limited to those having a home address nearby.  For Bar B, the information entry states \"people bring friends back with them\" to indicate that a relatively high \"brought a friend\" visit measure has\nbeen determined for Bar B. For Bar C, the information entry states \"best reviews,\" indicating that Bar C has more and/or better reviews, but given the lack of information regarding repeat visits or other quality visits, Bar C is deemphasized in the\nresponse.\n In addition, as discussed above, in some instances a query may specify a quality visit criterion that limits results to those locations for which a quality visit measure meets a particular criterion.  A link 176, for example, illustrates a\nselectable user control that may be activated as part of a structured query to limit search results to physical locations matching a quality visit criterion.  Selection of link 176, for example, may omit Bar C from updated search results generated in\nresponse to the selection.\n While several implementations have been described and illustrated herein, a variety of other means and/or structures for performing the function and/or obtaining the results and/or one or more of the advantages described herein may be utilized,\nand each of such variations and/or modifications is deemed to be within the scope of the implementations described herein.  More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary and that the\nactual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications for which the teachings is/are used.  Those skilled in the art will recognize, or be able to ascertain using no more than routine\nexperimentation, many equivalents to the specific implementations described herein.  It is, therefore, to be understood that the foregoing implementations are presented by way of example only and that, within the scope of the appended claims and\nequivalents thereto, implementations may be practiced otherwise than as specifically described and claimed.  Implementations of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described\nherein.  In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of\nthe present disclosure.", "application_number": "14849368", "abstract": " The response of a computer to a query associated with a physical location\n     may be controlled using a quality visit measure that is based at least in\n     part on the number and/or frequency of repeat visits by one or more\n     individuals to that physical location.\n", "citations": ["8768379", "8838586", "20060123014", "20080059455", "20140088856", "20140259189", "20140379696", "20150100383"], "related": []}, {"id": "20170161389", "patent_code": "10296527", "patent_name": "Determining an object referenced within informal online communications", "year": "2019", "inventor_and_country_data": " Inventors: \nChu; Stephen M. (Beabercreek, OH), Duan; Ning (BeiJing, CN), Gong; Min (Shanghai, CN), Qiu; Yun Jie (Shanghai, CN), Yan; Junchi (Shanghai, CN)  ", "description": "BACKGROUND OF THE INVENTION\n The present invention relates generally to the electrical, electronic and computer arts, and, more particularly, to techniques for determining an object referenced within informal online communications.\n A company may want to monitor online content in order to see the frequency with which it is discussed and/or the manner in which it is perceived.  However, unlike a formal press release or news report, which purport to present an accurate\nstatement of facts, informal online communications (e.g., chat, message boards, and/or social media) typically consist primarily of short segments of text which typically focus less on presenting facts accurately and more on presenting a personal opinion\nor commentary, often with the goal of being fresh, humorous, and/or dynamic.  Analysis (e.g., data mining) of these informal communications accordingly presents unique challenges.  Informal online communications often use words which may initially appear\nto refer to one concept but actually refer to another concept.  The concepts to which words refer may be obscured.  For example, rather than referring to a person by his or her actual name, informal online communications may instead refer to him or her\nusing a nickname or other slang.\n Conventional techniques for determining the concepts to which a given word refers often utilize a knowledge graph which analyzes the context within which a given word is used.  For example, the word \"apple\" likely refers to a company when it\nappears with product names such as \"iPhone\" or \"iPad\" or with other company names such as \"Microsoft\" or \"Google,\" but likely refers to a fruit when it appears in conjunction with other fruit names such as \"pear\" or \"orange\" or with food-related terms\nsuch as \"sauce\" or \"juice.\" (The product names \"iPhone\" and \"iPad\" are registered trademarks, as are the company names \"Apple,\" \"Microsoft,\" and \"Google.\") But informal online communications may not include sufficient contextual cues to permit the use of\nconventional knowledge graphs to determine the meaning of a given word.\nBRIEF SUMMARY\n Principles of the invention, in accordance with embodiments thereof, provide techniques for determining an object referenced within a set of one or more informal online communications.  Thus, for example, one or more illustrative embodiments of\nthe present invention advantageously provide the capability to determine that informal online communications which appear to involve different objects are actually referring to the same object.\n In one aspect, a method for determining an object referenced within a set of one or more informal online communications comprises: generating a knowledge graph for at least a given company based at least on a set of one or more formal online\ncommunications comprising at least one of one or more press releases about the given company and one or more news items about the given company, the knowledge graph comprising a plurality of node elements, and the knowledge graph further comprising, for\neach node element of the knowledge graph, a corresponding halo comprising one or more words which are temporally proximate to that node element within at least one of the one or more formal online communications; determining whether at least one of the\nnode elements of at least the knowledge graph for the given company is present in a given informal online communication within the set of one or more informal online communications; for each node element of the knowledge graph for the given company which\nis determined to be present in the given informal online communication, detecting a halo comprising one or more words which are temporally proximate to that node element within the given informal online communication; and identifying which of the\nplurality of node elements has a corresponding halo, within the knowledge graph for the given company, most similar to the detected halo, wherein the identified node element is determined to be the referenced object.\n In accordance with another embodiment of the invention, an apparatus includes a memory and at least one processor coupled to the memory.  The processor is operative: to generate a knowledge graph for at least a given company based at least on a\nset of one or more formal online communications comprising at least one of one or more press releases about the company and one or more news items about the company, the knowledge graph comprising a plurality of node elements, and the knowledge graph\nfurther comprising, for each node element of the knowledge graph, a corresponding halo comprising one or more words which are temporally proximate to that node element within at least one of the one or more formal online communications; to determine\nwhether at least one of the node elements of at least the knowledge graph for the given company is present in a given informal online communication within the set of one or more informal online communications; for each node element of the knowledge graph\nfor the given company which is determined to be present in the given informal online communication, to detect a halo comprising one or more words which are temporally proximate to that node element within the given informal online communication; and to\nidentify which of the plurality of node elements has a corresponding halo, within the knowledge graph for the given company, most similar to the detected halo, wherein the identified node element is determined to be the referenced object.\n As used herein, \"facilitating\" an action includes performing the action, making the action easier, helping to carry the action out, or causing the action to be performed.  Thus, by way of example and not limitation, instructions executing on one\nprocessor might facilitate an action carried out by instructions executing on a remote processor, by sending appropriate data or commands to cause or aid the action to be performed.  For the avoidance of doubt, where an actor facilitates an action by\nother than performing the action, the action is nevertheless performed by some entity or combination of entities.\n One or more embodiments of the invention or elements thereof can be implemented in the form of a computer program product including a computer readable storage medium with computer usable program code for performing the method steps indicated. \nFurthermore, one or more embodiments of the invention or elements thereof can be implemented in the form of a system (or apparatus) including a memory, and at least one processor coupled to the memory and operative to perform exemplary method steps.  Yet\nfurther, in another aspect, one or more embodiments of the invention or elements thereof can be implemented in the form of means for carrying out one or more of the method steps described herein; the means can include (i) hardware module(s), (ii)\nsoftware module(s) stored in a computer readable storage medium (or multiple such media) and implemented on a hardware processor, or (iii) a combination of (i) and (ii); any of (i)-(iii) implement the specific techniques set forth herein.\n These and other features and advantages of the present invention will become apparent from the following detailed description of illustrative embodiments thereof, which is to be read in connection with the accompanying drawings. BRIEF\nDESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWINGS\n The following drawings are presented by way of example only and without limitation, wherein like reference numerals (when used) indicate corresponding elements throughout the several views, and wherein:\n FIG. 1 depicts at least a portion of an exemplary system and/or method for determining the actual objects of references within informal online communications according to an embodiment of the invention;\n FIG. 2 depicts at least a portion of an exemplary system and/or method for establishing a halo-based knowledge graph according to an embodiment of the invention; and\n FIG. 3 depicts a computer system that may be useful in implementing one or more aspects and/or elements of the invention.\n It is to be appreciated that elements in the figures are illustrated for simplicity and clarity.  Common but well-understood elements that may be useful or necessary in a commercially feasible embodiment may not be shown in order to facilitate a\nless hindered view of the illustrated embodiments.\nDETAILED DESCRIPTION\n As will be further discussed herein, embodiments of the present invention leverage the property that informal online communications are strongly timely and thus utilize a knowledge graph in which each node has a halo (temporal event profile)\nwhich can be used as an index of reference.  Embodiments of the present invention will be described herein in the context of illustrative methods and apparatus in which a knowledge graph is generated for a company.  It is to be appreciated, however, that\nthe invention is not limited to the specific apparatus and/or methods illustratively shown and described herein.  For example, in other embodiments, a knowledge graph may be generated for a person, an inanimate object, or even an abstract concept. \nMoreover, it will become apparent to those skilled in the art given the teachings herein that numerous modifications can be made to the embodiments shown that are within the scope of the claimed invention.  Thus, no limitations with respect to the\nembodiments shown and described herein are intended or should be inferred.\n FIG. 1 depicts at least a portion of an exemplary system and/or method for determining the actual objects of references within informal online communications according to an embodiment of the invention.  Before module 130 analyzes informal\nonline communications obtained from text source 110, knowledge graph collection 125 should be configured to store one or more company-specific halo-based knowledge graphs.  Knowledge graph collection 125 can be configured to store a plurality of\nhalo-based knowledge graphs each corresponding to a different company.  Module 120 establishes a halo-based knowledge graph within knowledge graph collection 125 for a specific company by extracting critical elements and relationships for that specific\ncompany using company information database 111, which may be an ERP (Enterprise Resource Planning) database, and news source 112, which may include formal press releases and news reports.\n FIG. 2 depicts at least a portion of an exemplary system and/or method for establishing a halo-based knowledge graph according to an embodiment of the invention.  In an embodiment of the present invention, module 120 shown in FIG. 1 may include\nmodules 201, 202 and 203 shown in FIG. 2.  As discussed above, the apparatus shown in FIG. 2 is described herein in connection with establishing a halo-based knowledge graph for a company, but it may also be used to establish a halo-based knowledge graph\nfor a person, an inanimate object, or even an abstract concept.\n From company information database 111, node element selection module 201 selects the major products, fields, and/or industries for a given company as the node elements.  Based on analysis of news source 112, relationship generation module 202\ncalculates relationships (e.g., connections) between pairs of nodes in a manner similar to construction of a conventional knowledge graph.  For each node element within a company's knowledge graph, halo generation module 203 first performs a search of\nnews source 112 for items in which that node element is used in temporal proximity with the company's name, then extracts the keywords and/or tags from these items, and finally generates a temporal halo for the node element based on these extracted\nkeywords and/or tags.\n Returning now to FIG. 1, once module 120 constructs at least one halo-based knowledge graph within knowledge graph collection 125, text source analysis module 130 may be used to analyze informal online communications from text source 110.  Text\nsource analysis module 130 includes keyword detection module 131, halo detection module 132, and reference matching module 133.  Module 131 performs keyword detection by using conventional context and topic segmentation techniques, which segment online\ncommunications into different topics based on the quote or reply information in online communications, and common keywords detection in the communications' context.\n Halo detection module 132 performs the following steps for each segmentation of an informal online communication from text source 110: first, search the text for any node elements of at least one company's knowledge graph within knowledge graph\ncollection 125.  This may involve searching each of a plurality of company-specific knowledge graphs within knowledge graph collection 125.  Next, if a node element is found within a company's knowledge graph, construct the \"detected-halo\" for that node. This construction is performed by the following steps: first, determine a company-specific knowledge graph within knowledge graph collection 125 which includes the found node element; next, do word segmentation for the text, then detect all the words in\nthe text which also appear as the halo words of any node element in the previously-determined company-specific knowledge graph within knowledge graph collection 125; finally, construct the \"detected-halo\" as the set of the detected words in the previous\nstep.\n Next, reference matching module 133 performs the following steps for each node element in the previously-determined company-specific knowledge graph within knowledge graph collection 125: first, calculate similarity between the \"detected-halo\"\nfrom module 132 and each halo in the previously-determined company-specific knowledge graph within knowledge graph collection 125.  The halo similarity may be calculated using, for example, an unweighted overlap percentage and/or a weighted overlap\npercentage in which a time distance between news time of the temporal halo and communication time of text source 110 may be used as a weight.  Finally, select the node element in the previously-determined company-specific knowledge graph within knowledge\ngraph collection 125 with the best halo similarity to be the actual reference, which then forms output 135.\n Given the discussion thus far, it will be appreciated that, in general terms, an exemplary method for determining an object referenced within a set of one or more informal online communications according to an aspect of the invention comprises:\ngenerating a knowledge graph for at least a given company based at least on a set of one or more formal online communications comprising at least one of one or more press releases about the given company and one or more news items about the given\ncompany, the knowledge graph comprising a plurality of node elements, and the knowledge graph further comprising, for each node element of the knowledge graph, a corresponding halo comprising one or more words which are temporally proximate to that node\nelement within at least one of the one or more formal online communications; determining whether at least one of the node elements of at least the knowledge graph for the given company is present in a given informal online communication within the set of\none or more informal online communications; for each node element of the knowledge graph for the given company which is determined to be present in the given informal online communication, detecting a halo comprising one or more words which are\ntemporally proximate to that node element within the given informal online communication; and identifying which of the plurality of node elements has a corresponding halo within the knowledge graph for the given company most similar to the detected halo,\nwherein the identified node element is determined to be the referenced object.\n Exemplary System and Article of Manufacture Details\n As will be appreciated by one skilled in the art, aspects of the present invention may be embodied as a system, method or computer program product.  Accordingly, aspects of the present invention may take the form of an entirely hardware\nembodiment, an entirely software embodiment (including firmware, resident software, micro-code, etc.) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a \"circuit,\" \"module\" or \"system.\" Furthermore,\naspects of the present invention may take the form of a computer program product embodied in one or more computer readable medium(s) having computer readable program code embodied thereon.\n One or more embodiments of the invention, or elements thereof, can be implemented in the form of an apparatus including a memory and at least one processor that is coupled to the memory and operative to perform exemplary method steps.\n One or more embodiments can make use of software running on a general purpose computer or workstation.  With reference to FIG. 3, such an implementation might employ, for example, a processor 302, a memory 304, and an input/output interface\nformed, for example, by a display 306 and a keyboard 308.  The term \"processor\" as used herein is intended to include any processing device, such as, for example, one that includes a CPU (central processing unit) and/or other forms of processing\ncircuitry.  Further, the term \"processor\" may refer to more than one individual processor.  The term \"memory\" is intended to include memory associated with a processor or CPU, such as, for example, RAM (random access memory), ROM (read only memory), a\nfixed memory device (for example, hard drive), a removable memory device (for example, diskette), a flash memory and the like.  In addition, the phrase \"input/output interface\" as used herein, is intended to include, for example, one or more mechanisms\nfor inputting data to the processing unit (for example, mouse), and one or more mechanisms for providing results associated with the processing unit (for example, printer).  The processor 302, memory 304, and input/output interface such as display 306\nand keyboard 308 can be interconnected, for example, via bus 310 as part of a data processing unit 312.  Suitable interconnections, for example via bus 310, can also be provided to a network interface 314, such as a network card, which can be provided to\ninterface with a computer network, and to a media interface 316, such as a diskette or CD-ROM drive, which can be provided to interface with media 318.\n Accordingly, computer software including instructions or code for performing the methodologies of the invention, as described herein, may be stored in one or more of the associated memory devices (for example, ROM, fixed or removable memory)\nand, when ready to be utilized, loaded in part or in whole (for example, into RAM) and implemented by a CPU.  Such software could include, but is not limited to, firmware, resident software, microcode, and the like.\n A data processing system suitable for storing and/or executing program code will include at least one processor 302 coupled directly or indirectly to memory elements 304 through a system bus 310.  The memory elements can include local memory\nemployed during actual implementation of the program code, bulk storage, and cache memories which provide temporary storage of at least some program code in order to reduce the number of times code must be retrieved from bulk storage during\nimplementation.\n Input/output or I/O devices (including but not limited to keyboards 308, displays 306, pointing devices, and the like) can be coupled to the system either directly (such as via bus 310) or through intervening I/O controllers (omitted for\nclarity).\n Network adapters such as network interface 314 may also be coupled to the system to enable the data processing system to become coupled to other data processing systems or remote printers or storage devices through intervening private or public\nnetworks.  Modems, cable modem and Ethernet cards are just a few of the currently available types of network adapters.\n As used herein, including the claims, a \"server\" includes a physical data processing system (for example, system 312 as shown in FIG. 3) running a server program.  It will be understood that such a physical server may or may not include a\ndisplay and keyboard.\n As noted, aspects of the present invention may take the form of a computer program product embodied in one or more computer readable medium(s) having computer readable program code embodied thereon.  Any combination of one or more computer\nreadable medium(s) may be utilized.  The computer readable medium may be a computer readable signal medium or a computer readable storage medium.  A computer readable storage medium may be, for example, but not limited to, an electronic, magnetic,\noptical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any suitable combination of the foregoing.  Media block 418 is a non-limiting example.  More specific examples (a non-exhaustive list) of the computer readable storage\nmedium would include the following: an electrical connection having one or more wires, a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory),\nan optical fiber, a portable compact disc read-only memory (CD-ROM), an optical storage device, a magnetic storage device, or any suitable combination of the foregoing.  In the context of this document, a computer readable storage medium may be any\ntangible medium that can contain, or store a program for use by or in connection with an instruction execution system, apparatus, or device.\n A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein, for example, in baseband or as part of a carrier wave.  Such a propagated signal may take any of a variety of forms,\nincluding, but not limited to, electro-magnetic, optical, or any suitable combination thereof.  A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate, propagate, or\ntransport a program for use by or in connection with an instruction execution system, apparatus, or device.\n Program code embodied on a computer readable medium may be transmitted using any appropriate medium, including but not limited to wireless, wireline, optical fiber cable, RF, etc., or any suitable combination of the foregoing.\n Computer program code for carrying out operations for aspects of the present invention may be written in any combination of one or more programming languages, including an object oriented programming language such as Java, Smalltalk, C++ or the\nlike and conventional procedural programming languages, such as the \"C\" programming language or similar programming languages.  The program code may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software\npackage, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server.  In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area\nnetwork (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).\n Aspects of the present invention are described herein with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems) and computer program products according to embodiments of the invention.  It will be understood\nthat each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer program instructions.  These computer program instructions may be\nprovided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data\nprocessing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.\n These computer program instructions may also be stored in a computer readable medium that can direct a computer, other programmable data processing apparatus, or other devices to function in a particular manner, such that the instructions stored\nin the computer readable medium produce an article of manufacture including instructions which implement the function/act specified in the flowchart and/or block diagram block or blocks.\n The computer program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other devices to cause a series of operational steps to be performed on the computer, other programmable apparatus or other\ndevices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions/acts specified in the flowchart and/or block diagram block or\nblocks.\n The flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods and computer program products according to various embodiments of the present invention. \nIn this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of code, which comprises one or more executable instructions for implementing the specified logical function(s).  It should also be noted that, in\nsome alternative implementations, the functions noted in the block may occur out of the order noted in the figures.  For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed\nin the reverse order, depending upon the functionality involved.  It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be\nimplemented by special purpose hardware-based systems that perform the specified functions or acts, or combinations of special purpose hardware and computer instructions.\n It should be noted that any of the methods described herein can include an additional step of providing a system comprising distinct software modules embodied on a computer readable storage medium; the modules can include, for example, any or\nall of the elements depicted in the block diagrams and/or described herein; by way of example and not limitation, modules 120 and/or 130 shown in FIG. 1.  The method steps can then be carried out using the distinct software modules and/or sub-modules of\nthe system, as described above, executing on one or more hardware processors 302.  Further, a computer program product can include a computer-readable storage medium with code adapted to be implemented to carry out one or more method steps described\nherein, including the provision of the system with the distinct software modules.\n In any case, it should be understood that the components illustrated herein may be implemented in various forms of hardware, software, or combinations thereof; for example, application specific integrated circuit(s) (ASICS), functional\ncircuitry, one or more appropriately programmed general purpose digital computers with associated memory, and the like.  Given the teachings of the invention provided herein, one of ordinary skill in the related art will be able to contemplate other\nimplementations of the components of the invention.\n The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention.  As used herein, the singular forms \"a,\" \"an\" and \"the\" are intended to include the plural forms as\nwell, unless the context clearly indicates otherwise.  It will be further understood that the terms \"comprises\" and/or \"comprising,\" when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and/or\ncomponents, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof.\n The corresponding structures, materials, acts, and equivalents of all means or step plus function elements in the claims below are intended to include any structure, material, or act for performing the function in combination with other claimed\nelements as specifically claimed.  The description of the present invention has been presented for purposes of illustration and description, but is not intended to be exhaustive or limited to the invention in the form disclosed.  Many modifications and\nvariations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the invention.  The embodiment was chosen and described in order to best explain the principles of the invention and the practical\napplication, and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated.", "application_number": "14962232", "abstract": " A method for determining an object referenced within a set of one or more\n     informal online communications comprises: generating a knowledge graph\n     for a company based at least on formal online communications, the\n     knowledge graph comprising a plurality of node elements, and the\n     knowledge graph further comprising, for each node element of the\n     knowledge graph, a corresponding halo comprising one or more words which\n     are temporally proximate to that node element within the formal online\n     communications; for each node element of the knowledge graph which is\n     determined to be present in a given informal online communication,\n     detecting a halo comprising one or more words which are temporally\n     proximate to that node element within the given informal online\n     communication; and identifying which of the plurality of node elements\n     has a corresponding halo within the knowledge graph most similar to the\n     detected halo, wherein the identified node element is the referenced\n     object.\n", "citations": ["8407253", "20090240498", "20120054206", "20120078918", "20140372451", "20150006512", "20160364996"], "related": []}, {"id": "20170186425", "patent_code": "10311862", "patent_name": "Systems and methods for conversations with devices about media using\n     interruptions and changes of subjects", "year": "2019", "inventor_and_country_data": " Inventors: \nDawes; Charles (Ryton, GB), Klappert; Walter R. (Los Angeles, CA)  ", "description": "BACKGROUND\n Media guidance systems may include a voice interactive feature by which a user can provide voice commands and queries to the media guidance system.  The media guidance system may respond, through either text or a machine-generated voice, with a\nrelated answer or topic.  For instance, a user may request recommendations for popular action movies, and the media guidance system may access a database (such as the Apple Store or Google Play store), download a list of the top ten action movies, and\nprovide the list to the user.  The media guidance system may further improve the relevance of its responses by referencing a Knowledge Graph, which includes a semantic network that maintains relationships between certain topics and/or ideas.\n In this manner, traditional media guidance systems have employed a \"turn-based\" approach, wherein a user and the media guidance system take turns providing queries and answers.  However, such a turn-based approach can sometimes appear to be\nrigid or robotic.\nSUMMARY\n Accordingly, systems and methods for conversations with devices about media using interruption and changes of subjects are described herein.  A media guidance application may interact with a user through a voice interface in an interactive\nconversation, wherein the user speaks a voice command and the media guidance application response with a spoken response.  The media guidance application may, among other things, provide media guidance and recommendations to the user through the voice\ninterface.  For example, a user may state, \"Rachael, what should I watch tonight?\" The media guidance application may reply with \"I think you should try the new Bond movie.\" The media guidance application may also detect when the user hesitates and\ninterrupts or changes the subject.  For example, continuing with the previous example, the user may say \"hmm .  . . \", indicating some hesitation with the media guidance application's recommendation of a Bond movie.  The media guidance application,\ndetecting this hesitation, may interrupt and change the subject, stating \"How about Mission Impossible 3?\" By interrupting the conversation and changing the subject from time to time, media guidance systems can appear to be more intelligent and human.\n According to one aspect, systems and methods are described herein for providing media guidance.  According to one aspect, control circuitry may receive a first voice input.  The control circuitry may access a database of topics, the database of\ntopics including a semantic network indicating relationships between a plurality of topics and identify a first topic from the database of topics that is associated with the first voice input.  For example, the database of topics may be a Knowledge Graph\nthat maintains relationships between topics/ideas.  Knowledge graphs and their features are described in greater detail in U.S.  patent application Ser.  No. 14/501,504, filed Sep. 30, 2014, U.S.  patent application Ser.  No. 14/500,309, filed Sep. 29,\n2014, and U.S.  patent application Ser.  No. 14/448,308, filed Jul.  31, 2014, which are hereby incorporated by reference herein in their entireties.\n A user interface may generate a first response to the first voice input, and subsequent to generating the first response, the control circuitry may receive a second voice input.  The control circuitry may compare the second voice input to a list\nof interruption inputs to determine a match between the second voice input and an interruption input from the list of interruption inputs.  Interruption inputs may include, for example, a period of silence or a keyword or a phrase, such as \"Ahh,\",\n\"Umm,\", \"Hmm,\" or the like.  In response to determining the match between the second voice input and the interruption input, the user interface may generate a second response to the first voice input, wherein the second response is associated with a\nsecond topic from the database of topics that is associated with the first topic.\n In some embodiments, the control circuitry may also extract, from the database, relationships between the first topic and a reminder of the plurality of topics in order to select the second topic.  The control circuitry may compare each of the\nrelationships between the first topic and the remainder of the plurality of topics to a relationship threshold and store to memory a list indicating a subset of the relationships between the first topic and the remainder of the plurality of topics that\ndo not exceed the relationship threshold and a list of topics of the plurality of topics that correspond to the subset of the relationships.  The second topic may then be selected from the list of topics.  In this manner, the second topic may be chosen\nso that it is related to the topic of the user's original query.\n In some embodiments, the semantic network includes numerical relationships between the plurality of topics, the numerical relationships indicating a statistical likelihood that the second topic is related to the first topic.  For example, the\nstatistical likelihood may be determined by monitoring a plurality of voice inputs from a plurality of users.  Of the plurality of voice inputs, a first subset could be identified that relate to a first topic, and of the first subset, a smaller second\nsubset can be identified that relates to the second topic.  From the first and the second subset, a probability that the second topic follows the first topic may be determined and may form the basis of the statistical likelihood and/or relationship\nbetween the first topic and the second topic.  Other methods of determining and maintaining relationships in a database, semantic network, and/or Knowledge Graph are contemplated, as will be understood by those of ordinary skill in the art.\n The second response may further take into account a user's media preferences.  For example, the control circuitry may access a user profile indicating media preferences of a user.  A genre preference may be retrieved from the user profile, and\nthe control circuitry may identify a subset of the plurality of topics that are associated with the retrieved genre.  The second topic may then be selected from the subset of the plurality of topics.  Other methods of selecting the second topic may be\ncontemplated.  For instance, the user profile, as discussed herein, may include other information about the user's media preferences, including, but not limited to, ratings information, favorite actor information, preferred media format (e.g., standard\ndefinition, high definition, 3D, etc.), and preferred media category, among others.  In some embodiments, the database of topics may indicate, for each of the plurality of topics, a genre associated with each respective topic.  The control circuitry may\nextract, from the database of topics, a genre associated with the first topic and identify a subset of the plurality of topics that are associated with the extracted genre.  The second topic may then be selected from the subset of the plurality of\ntopics.\n In some embodiments, it may not be desirable to interrupt or change the subject of a conversation with high frequency.  For instance, some users may prefer little or no interruptions when interacting with the media guidance system.  In some\nembodiments, the control circuitry may receive a third voice input and compare the third voice input to the list of interruption inputs to determine a match between the third voice input and a second interruption input from the list of interruption\ninputs.  The control circuitry may determine whether an interruption threshold period of time has elapsed between a current time and the second response.  In some embodiments, the interruption threshold period of time may be specified by the user and may\nbe a minimum period of time that the media guidance system must wait before interrupting or changing the subject.  In some embodiments, the interruption threshold period of time may represent an average time between interruptions, such that the media\nguidance system is allowed flexibility to interrupt the user at any time, as long as, on average, the time between interruptions does not fall below the average time.  The control circuitry may, in response to determining that the interruption threshold\nperiod of time has elapsed between the current time and the second response, generate a third response to the first voice input, wherein the third response is associated with a third topic from the database of topics that is associated with the first\ntopic.\n In some embodiments, a voice personality profile may store certain user preferences regarding, among others, how often a media guidance application may interrupt or change the subject of a conversation and how far the media guidance application\nmay deviate from a current topic.  The control circuitry may receive an indication of a user associated with the first voice input.  The control circuitry may then access a plurality of voice personality profiles, each voice personality profile\ncorresponding to a respective user and including indications of a plurality of interruption inputs and select one of the plurality of voice personality profiles based on the indication of the user associated with the first voice input.  In order to\nidentify an interruption input in the second voice input, the control circuitry may compare the second voice input to the list of interruption inputs from the selected voice personality profile.  For example, one user may often use \"Ahh .  . . \" when\npausing, while another user may often use \"Hmm .  . . \" when pausing, and the media guidance application may maintain different voice personality profiles for each of these users.  In some embodiments, the voice personality profiles may output responses\nin different voices.  For example, one voice personality profile may sound like a man when responding to a user, while another voice personality profile may sound like a woman.\n In some embodiments, the control circuitry may limit the frequency or number of times the control circuitry interrupts the conversation based on an interruption threshold period of time.  The control circuitry may extract the interruption\nthreshold period of time from a voice personality profile.  The control circuitry may calculate a time elapsed since the first voice input by comparing a current time to a time associated with the first voice input.  The control circuitry may only\ngenerate the second response in this embodiment if the time elapsed has exceeded the interruption threshold period of time.\n As an illustrative example, a user may invoke the media guidance application and say \"Rachael, suggest a video, please.\" The user's reference to \"Rachael\" may cause the guidance system to load a specific artificial personality called \"Rachael.\"\nThis personality may use a specific female voice to interact with the user and may also indicate (1) an interruption threshold period of time that controls how frequently to interrupt the conversation; and (2) a relationship threshold that indicates how\nfar \"Rachael\" is allowed to move away from the current subject when interrupting or changing the subject.\n Rachael may reply, \"You were one hour and eighteen minutes into NOVA's `Becoming Human--Part 1.` Would you like to watch the rest of this program?\" The user may respond with, \"Ah .  . . \" The media guidance system may detect the word \"Ah\" as an\ninterruption input and determine that it is a suitable time to interrupt.  Rachael, interrupting the conversation, may then state \"Alternatively, the Smithsonian Channel has a similar program called `Smithsonian Spotlight: Human Origins.` How about this\nprogram?\" The media guidance application may have accessed a Knowledge Graph to determine that the Smithsonian program is related to the NOVA program.  The user may respond to the recommendation of the Smithsonian program with two seconds of silence. \nThe media guidance application may identify the silence as another interruption point.  However, this time, Rachael may respond with: \"How about something completely different?\" indicating whether the media guidance system may suggest a topic which\nexceeds the relationship threshold.  In this manner, the media guidance application may, in some embodiments, query the user for permission to exceed the relationship threshold after one or more unsuccessful recommendations.\n It should be noted that the systems and/or methods described above may be applied to, or used in accordance with, other systems, methods and/or apparatuses. BRIEF DESCRIPTION OF THE DRAWINGS\n The above and other objects and advantages of the disclosure will be apparent upon consideration of the following detailed description, taken in conjunction with the accompanying drawings, in which like reference characters refer to like parts\nthroughout, and in which:\n FIG. 1 shows an illustrative example of a display screen for use in accessing media content in accordance with some embodiments of the disclosure;\n FIG. 2 shows another illustrative example of a display screen used to access media content in accordance with some embodiments of the disclosure;\n FIG. 3 is a block diagram of an illustrative user equipment device in accordance with some embodiments of the disclosure;\n FIG. 4 is a block diagram of an illustrative media system in accordance with some embodiments of the disclosure;\n FIG. 5 is a flowchart of illustrative steps for providing media guidance in accordance with some embodiments of the disclosure;\n FIG. 6 is a flowchart of illustrative steps for receiving a first voice input in accordance with some embodiments of the disclosure;\n FIG. 7 is a flowchart of illustrative steps for accessing a database of topics in accordance with some embodiments of the disclosure;\n FIG. 8 is a flowchart of illustrative steps for identifying a first topic that is associated with the first voice input in accordance with some embodiments of the disclosure;\n FIG. 9 is a flowchart of illustrative steps for generating a first response to the first voice input in accordance with some embodiments of the disclosure;\n FIG. 10 is a flowchart of illustrative steps for receiving a second voice input subsequent to generating the first response in accordance with some embodiments of the disclosure;\n FIG. 11 is a flowchart of illustrative steps for comparing the second voice input to identify an interruption input in accordance with some embodiments of the disclosure; and\n FIG. 12 is a flowchart of another set of illustrative steps for generating a second response to the first voice input in accordance with some embodiments of the disclosure.\nDETAILED DESCRIPTION OF THE DRAWINGS\n Systems and methods for conversations with devices about media using interruption and changes of subjects are described herein.  A media guidance application may interact with a user through a voice interface in an interactive conversation,\nwherein the user speaks a voice command and the media guidance application response with a spoken response.  The media guidance application may, among other things, provide media guidance and recommendations to the user through the voice interface.  The\nmedia guidance application may also detect when the user hesitates and interrupts or changes the subject at that point.  The media guidance may change the subject to a topic that is related to the user's original query to provide more relevant answers to\nthe user.\n The amount of content available to users in any given content delivery system can be substantial.  Consequently, many users desire a form of media guidance through an interface that allows users to efficiently navigate content selections and\neasily identify content that they may desire.  An application that provides such guidance is referred to herein as an interactive media guidance application or, sometimes, a media guidance application or a guidance application.\n Interactive media guidance applications may take various forms depending on the content for which they provide guidance.  One typical type of media guidance application is an interactive television program guide.  Interactive television program\nguides (sometimes referred to as electronic program guides) are well-known guidance applications that, among other things, allow users to navigate among and locate many types of content or media assets.  Interactive media guidance applications may\ngenerate graphical user interface screens that enable a user to navigate among, locate and select content.  As referred to herein, the terms \"media asset\" and \"content\" should be understood to mean an electronically consumable user asset, such as\ntelevision programming, as well as pay-per-view programs, on-demand programs (as in video-on-demand (VOD) systems), Internet content (e.g., streaming content, downloadable content, Webcasts, etc.), video clips, audio, content information, pictures,\nrotating images, documents, playlists, websites, articles, books, electronic books, blogs, advertisements, chat sessions, social media, applications, games, and/or any other media or multimedia and/or combination of the same.  Guidance applications also\nallow users to navigate among and locate content.  As referred to herein, the term \"multimedia\" should be understood to mean content that utilizes at least two different content forms described above, for example, text, audio, images, video, or\ninteractivity content forms.  Content may be recorded, played, displayed or accessed by user equipment devices, but can also be part of a live performance.\n The media guidance application and/or any instructions for performing any of the embodiments discussed herein may be encoded on computer readable media.  Computer readable media includes any media capable of storing data.  The computer readable\nmedia may be transitory, including, but not limited to, propagating electrical or electromagnetic signals, or may be non-transitory including, but not limited to, volatile and non-volatile computer memory or storage devices such as a hard disk, floppy\ndisk, USB drive, DVD, CD, media cards, register memory, processor caches, Random Access Memory (\"RAM\"), etc.\n With the advent of the Internet, mobile computing, and high-speed wireless networks, users are accessing media on user equipment devices on which they traditionally did not.  As referred to herein, the phrase \"user equipment device,\" \"user\nequipment,\" \"user device,\" \"electronic device,\" \"electronic equipment,\" \"media equipment device,\" or \"media device\" should be understood to mean any device for accessing the content described above, such as a television, a Smart TV, a set-top box, an\nintegrated receiver decoder (IRD) for handling satellite television, a digital storage device, a digital media receiver (DMR), a digital media adapter (DMA), a streaming media device, a DVD player, a DVD recorder, a connected DVD, a local media server, a\nBLU-RAY player, a BLU-RAY recorder, a personal computer (PC), a laptop computer, a tablet computer, a WebTV box, a personal computer television (PC/TV), a PC media server, a PC media center, a hand-held computer, a stationary telephone, a personal\ndigital assistant (PDA), a mobile telephone, a portable video player, a portable music player, a portable gaming machine, a smart phone, or any other television equipment, computing equipment, or wireless device, and/or combination of the same.  In some\nembodiments, the user equipment device may have a front facing screen and a rear facing screen, multiple front screens, or multiple angled screens.  In some embodiments, the user equipment device may have a front facing camera and/or a rear facing\ncamera.  On these user equipment devices, users may be able to navigate among and locate the same content available through a television.  Consequently, media guidance may be available on these devices, as well.  The guidance provided may be for content\navailable only through a television, for content available only through one or more of other types of user equipment devices, or for content available both through a television and one or more of the other types of user equipment devices.  The media\nguidance applications may be provided as on-line applications (i.e., provided on a web-site), or as stand-alone applications or clients on user equipment devices.  Various devices and platforms that may implement media guidance applications are described\nin more detail below.\n One of the functions of the media guidance application is to provide media guidance data to users.  As referred to herein, the phrase \"media guidance data\" or \"guidance data\" should be understood to mean any data related to content or data used\nin operating the guidance application.  For example, the guidance data may include program information, guidance application settings, user preferences, user profile information, media listings, media-related information (e.g., broadcast times, broadcast\nchannels, titles, descriptions, ratings information (e.g., parental control ratings, critic's ratings, etc.), genre or category information, actor information, logo data for broadcasters' or providers' logos, etc.), media format (e.g., standard\ndefinition, high definition, 3D, etc.), advertisement information (e.g., text, images, media clips, etc.), on-demand information, blogs, websites, and any other type of guidance data that is helpful for a user to navigate among and locate desired content\nselections.\n In some embodiments, the media guidance data may include a semantic network that maintains relationships between topics, ideas, and/or concepts.  As will be understood by those of ordinary skill in the art, a semantic network or semantic graph\nmay include a directed or undirected graph consisting of vertices, which represent topics, ideas, or concepts, and edges which represent the relationships between the concepts.  In some embodiments, the semantic network may include a Knowledge Graph. \nThe semantic network may be populated using any suitable approach.  For instance, in some embodiments, the relationships may include numeric values indicating the similarity between different topics, ideas, or concepts.\n The media guidance data may also include one or more voice personality profiles including at least a relationship threshold and an interruption threshold period of time.  The relationship threshold may indicate a \"distance\" that a media guidance\napplication may deviate from a current subject when interrupting or changing the subject.  For example, in embodiments where a semantic network includes relationships including numeric values between the vertices, the relationship threshold may be a\nmaximum value between a first topic and a second topic, so that the media guidance application does not deviate beyond the relationship threshold when interrupting or changing the subject.  The interruption threshold period of time may indicate a\nfrequency with which the media guidance application may interrupt or change the subject of a conversation.  In some embodiments, the interruption threshold period of time may represent a minimum amount of time that must pass between subsequent\ninterruptions of the conversation.  In other embodiments, the interruption threshold period of time may represent an average amount of time between subsequent interruptions.  Thus, in these embodiments, the time between certain interruptions may be less\nthan the interruption threshold period of time, so long as, on average, the time between interruptions is kept at or above the interruption threshold period of time.\n FIGS. 1-2 show illustrative display screens that may be used to provide media guidance data.  The display screens shown in FIGS. 1-2 may be implemented on any suitable user equipment device or platform.  While the displays of FIGS. 1-2 are\nillustrated as full screen displays, they may also be fully or partially overlaid over content being displayed: A user may indicate a desire to access content information by selecting a selectable option provided in a display screen (e.g., a menu option,\na listings option, an icon, a hyperlink, etc.) or pressing a dedicated button (e.g., a GUIDE button) on a remote control or other user input interface or device.  In response to the user's indication, the media guidance application may provide a display\nscreen with media guidance data organized in one of several ways, such as by time and channel in a grid, by time, by channel, by source, by content type, by category (e.g., movies, sports, news, children, or other categories of programming), or other\npredefined, user-defined, or other organization criteria.\n FIG. 1 shows illustrative grid of a program listings display 100 arranged by time and channel that also enables access to different types of content in a single display.  Display 100 may include grid 102 with: (1) a column of channel/content\ntype identifiers 104, where each channel/content type identifier (which is a cell in the column) identifies a different channel or content type available; and (2) a row of time identifiers 106, where each time identifier (which is a cell in the row)\nidentifies a time block of programming.  Grid 102 also includes cells of program listings, such as program listing 108, where each listing provides the title of the program provided on the listing's associated channel and time.  With a user input device,\na user can select program listings by moving highlight region 110.  Information relating to the program listing selected by highlight region 110 may be provided in program information region 112.  Region 112 may include, for example, the program title,\nthe program description, the time the program is provided (if applicable), the channel the program is on (if applicable), the program's rating, and other desired information.\n In addition to providing access to linear programming (e.g., content that is scheduled to be transmitted to a plurality of user equipment devices at a predetermined time and is provided according to a schedule), the media guidance application\nalso provides access to non-linear programming (e.g., content accessible to a user equipment device at any time and is not provided according to a schedule).  Non-linear programming may include content from different content sources including on-demand\ncontent (e.g., VOD), Internet content (e.g., streaming media, downloadable media, etc.), locally stored content (e.g., content stored on any user equipment device described above or other storage device), or other time-independent content.  On-demand\ncontent may include movies or any other content provided by a particular content provider (e.g., HBO On Demand providing \"The Sopranos\" and \"Curb Your Enthusiasm\").  HBO ON DEMAND is a service mark owned by Time Warner Company L.P.  et al. and THE\nSOPRANOS and CURB YOUR ENTHUSIASM are trademarks owned by the Home Box Office, Inc.  Internet content may include web events, such as a chat session or Webcast, or content available on-demand as streaming content or downloadable content through an\nInternet web site or other Internet access (e.g. FTP).\n Grid 102 may provide media guidance data for non-linear programming including on-demand listing 114, recorded content listing 116, and Internet content listing 118.  A display combining media guidance data for content from different types of\ncontent sources is sometimes referred to as a \"mixed-media\" display.  Various permutations of the types of media guidance data that may be displayed that are different than display 100 may be based on user selection or guidance application definition\n(e.g., a display of only recorded and broadcast listings, only on-demand and broadcast listings, etc.).  As illustrated, listings 114, 116, and 118 are shown as spanning the entire time block displayed in grid 102 to indicate that selection of these\nlistings may provide access to a display dedicated to on-demand listings, recorded listings, or Internet listings, respectively.  In some embodiments, listings for these content types may be included directly in grid 102.  Additional media guidance data\nmay be displayed in response to the user selecting one of the navigational icons 120.  (Pressing an arrow key on a user input device may affect the display in a similar manner as selecting navigational icons 120.)\n Display 100 may also include video region 122, advertisement 124, and options region 126.  Video region 122 may allow the user to view and/or preview programs that are currently available, will be available, or were available to the user.  The\ncontent of video region 122 may correspond to, or be independent from, one of the listings displayed in grid 102.  Grid displays including a video region are sometimes referred to as picture-in-guide (PIG) displays.  PIG displays and their\nfunctionalities are described in greater detail in Satterfield et al. U.S.  Pat.  No. 6,564,378, issued May 13, 2003 and Yuen et al. U.S.  Pat.  No. 6,239,794, issued May 29, 2001, which are hereby incorporated by reference herein in their entireties. \nPIG displays may be included in other media guidance application display screens of the embodiments described herein.\n Advertisement 124 may provide an advertisement for content that, depending on a viewer's access rights (e.g., for subscription programming), is currently available for viewing, will be available for viewing in the future, or may never become\navailable for viewing, and may correspond to or be unrelated to one or more of the content listings in grid 102.  Advertisement 124 may also be for products or services related or unrelated to the content displayed in grid 102.  Advertisement 124 may be\nselectable and provide further information about content, provide information about a product or a service, enable purchasing of content, a product, or a service, provide content relating to the advertisement, etc. Advertisement 124 may be targeted based\non a user's profile/preferences, monitored user activity, the type of display provided, or on other suitable targeted advertisement bases.\n While advertisement 124 is shown as rectangular or banner shaped, advertisements may be provided in any suitable size, shape, and location in a guidance application display.  For example, advertisement 124 may be provided as a rectangular shape\nthat is horizontally adjacent to grid 102.  This is sometimes referred to as a panel advertisement.  In addition, advertisements may be overlaid over content or a guidance application display or embedded within a display.  Advertisements may also include\ntext, images, rotating images, video clips, or other types of content described above.  Advertisements may be stored in a user equipment device having a guidance application, in a database connected to the user equipment, in a remote location (including\nstreaming media servers), or on other storage means, or a combination of these locations.  Providing advertisements in a media guidance application is discussed in greater detail in, for example, Knudson et al., U.S.  Patent Application Publication No.\n2003/0110499, filed Jan.  17, 2003; Ward, III et al. U.S.  Pat.  No. 6,756,997, issued Jun.  29, 2004; and Schein et al. U.S.  Pat.  No. 6,388,714, issued May 14, 2002, which are hereby incorporated by reference herein in their entireties.  It will be\nappreciated that advertisements may be included in other media guidance application display screens of the embodiments described herein.\n Options region 126 may allow the user to access different types of content, media guidance application displays, and/or media guidance application features.  Options region 126 may be part of display 100 (and other display screens described\nherein), or may be invoked by a user by selecting an on-screen option or pressing a dedicated or assignable button on a user input device.  The selectable options within options region 126 may concern features related to program listings in grid 102 or\nmay include options available from a main menu display.  Features related to program listings may include searching for other air times or ways of receiving a program, recording a program, enabling series recording of a program, setting program and/or\nchannel as a favorite, purchasing a program, or other features.  Options available from a main menu display may include search options, VOD options, parental control options, Internet options, cloud-based options, device synchronization options, second\nscreen device options, options to access various types of media guidance data displays, options to subscribe to a premium service, options to edit a user's profile, options to access a browse overlay, or other options.  The options may also include the\nability to select one of a plurality of voice personality profiles, each voice personality profiles including at least an indication of a relationship threshold and an interruption threshold period of time.  In some embodiments, the voice personality\nprofile also includes a specific voice for providing responses to the user as well as pointers to a user's preference profile.  In this manner, a user may choose the \"personality\" of the media guidance application that he or she will be interacting with,\nwith each \"personality\" comprising parameters that determine how often the personality may interrupt the user and how far the personality may stray from a current topic when changing the subject of a conversation.\n The media guidance application may be personalized based on a user's preferences.  A personalized media guidance application allows a user to customize displays and features to create a personalized \"experience\" with the media guidance\napplication.  This personalized experience may be created by allowing a user to input these customizations and/or by the media guidance application monitoring user activity to determine various user preferences.  Users may access their personalized\nguidance application by logging in or otherwise identifying themselves to the guidance application.  Customization of the media guidance application may be made in accordance with a user profile.  The customizations may include varying presentation\nschemes (e.g., color scheme of displays, font size of text, etc.), aspects of content listings displayed (e.g., only HDTV or only 3D programming, user-specified broadcast channels based on favorite channel selections, re-ordering the display of channels,\nrecommended content, etc.), desired recording features (e.g., recording or series recordings for particular users, recording quality, etc.), parental control settings, customized presentation of Internet content (e.g., presentation of social media\ncontent, e-mail, electronically delivered articles, etc.) and other desired customizations.\n The media guidance application may allow a user to provide user profile information or may automatically compile user profile information.  The media guidance application may, for example, monitor the content the user accesses and/or other\ninteractions the user may have with the guidance application.  Additionally, the media guidance application may obtain all or part of other user profiles that are related to a particular user (e.g., from other web sites on the Internet the user accesses,\nsuch as www.allrovi.com, from other media guidance applications the user accesses, from other interactive applications the user accesses, from another user equipment device of the user, etc.), and/or obtain information about the user from other sources\nthat the media guidance application may access.  As a result, a user can be provided with a unified guidance application experience across the user's different user equipment devices.  This type of user experience is described in greater detail below in\nconnection with FIG. 4.  Additional personalized media guidance application features are described in greater detail in Ellis et al., U.S.  Patent Application Publication No. 2005/0251827, filed Jul.  11, 2005, Boyer et al., U.S.  Pat.  No. 7,165,098,\nissued Jan.  16, 2007, and Ellis et al., U.S.  Patent Application Publication No. 2002/0174430, filed Feb.  21, 2002, which are hereby incorporated by reference herein in their entireties.\n Another display arrangement for providing media guidance is shown in FIG. 2.  Video mosaic display 200 includes selectable options 202 for content information organized based on content type, genre, and/or other organization criteria.  In\ndisplay 200, television listings option 204 is selected, thus providing listings 206, 208, 210, and 212 as broadcast program listings.  In display 200 the listings may provide graphical images including cover art, still images from the content, video\nclip previews, live video from the content, or other types of content that indicate to a user the content being described by the media guidance data in the listing.  Each of the graphical listings may also be accompanied by text to provide further\ninformation about the content associated with the listing.  For example, listing 208 may include more than one portion, including media portion 214 and text portion 216.  Media portion 214 and/or text portion 216 may be selectable to view content in\nfull-screen or to view information related to the content displayed in media portion 214 (e.g., to view listings for the channel that the video is displayed on).\n The listings in display 200 are of different sizes (i.e., listing 206 is larger than listings 208, 210, and 212), but if desired, all the listings may be the same size.  Listings may be of different sizes or graphically accentuated to indicate\ndegrees of interest to the user or to emphasize certain content, as desired by the content provider or based on user preferences.\n Various systems and methods for graphically accentuating content listings are discussed in, for example, Yates, U.S.  Patent Application Publication No. 2010/0153885, filed Dec.  29, 2005, which is hereby incorporated by reference herein in its\nentirety.\n Users may access content and the media guidance application (and its display screens described above and below) from one or more of their user equipment devices.  FIG. 3 shows a generalized embodiment of illustrative user equipment device 300. \nMore specific implementations of user equipment devices are discussed below in connection with FIG. 4.  User equipment device 300 may receive content and data via input/output (hereinafter \"I/O\") path 302.  I/O path 302 may provide content (e.g.,\nbroadcast programming, on-demand programming, Internet content, content available over a local area network (LAN) or wide area network (WAN), and/or other content) and data to control circuitry 304, which includes processing circuitry 306 and storage\n308.  Control circuitry 304 may be used to send and receive commands, requests, and other suitable data using I/O path 302.  I/O path 302 may connect control circuitry 304 (and specifically processing circuitry 306) to one or more communications paths\n(described below).  I/O functions may be provided by one or more of these communications paths, but are shown as a single path in FIG. 3 to avoid overcomplicating the drawing.\n Control circuitry 304 may be based on any suitable processing circuitry such as processing circuitry 306.  As referred to herein, processing circuitry should be understood to mean circuitry based on one or more microprocessors, microcontrollers,\ndigital signal processors, programmable logic devices, field-programmable gate arrays (FPGAs), application-specific integrated circuits (ASICs), etc., and may include a multi-core processor (e.g., dual-core, quad-core, hexa-core, or any suitable number\nof cores) or supercomputer.  In some embodiments, processing circuitry may be distributed across multiple separate processors or processing units, for example, multiple of the same type of processing units (e.g., two Intel Core i7 processors) or multiple\ndifferent processors (e.g., an Intel Core i5 processor and an Intel Core i7 processor).  In some embodiments, control circuitry 304 executes instructions for a media guidance application stored in memory (i.e., storage 308).  Specifically, control\ncircuitry 304 may be instructed by the media guidance application to perform the functions discussed above and below.  For example, the media guidance application may provide instructions to control circuitry 304 to generate the media guidance displays. \nIn some implementations, any action performed by control circuitry 304 may be based on instructions received from the media guidance application.\n In client-server based embodiments, control circuitry 304 may include communications circuitry suitable for communicating with a guidance application server or other networks or servers.  The instructions for carrying out the above mentioned\nfunctionality may be stored on the guidance application server.  Communications circuitry may include a cable modem, an integrated services digital network (ISDN) modem, a digital subscriber line (DSL) modem, a telephone modem, Ethernet card, or a\nwireless modem for communications with other equipment, or any other suitable communications circuitry.  Such communications may involve the Internet or any other suitable communications networks or paths (which is described in more detail in connection\nwith FIG. 4).  In addition, communications circuitry may include circuitry that enables peer-to-peer communication of user equipment devices, or communication of user equipment devices in locations remote from each other (described in more detail below).\n Memory may be an electronic storage device provided as storage 308 that is part of control circuitry 304.  As referred to herein, the phrase \"electronic storage device\" or \"storage device\" should be understood to mean any device for storing\nelectronic data, computer software, or firmware, such as random-access memory, read-only memory, hard drives, optical drives, digital video disc (DVD) recorders, compact disc (CD) recorders, BLU-RAY disc (BD) recorders, BLU-RAY 3D disc recorders, digital\nvideo recorders (DVR, sometimes called a personal video recorder, or PVR), solid state devices, quantum storage devices, gaming consoles, gaming media, or any other suitable fixed or removable storage devices, and/or any combination of the same.  Storage\n308 may be used to store various types of content described herein as well as media guidance data described above.  Nonvolatile memory may also be used (e.g., to launch a boot-up routine and other instructions).  Cloud-based storage, described in\nrelation to FIG. 4, may be used to supplement storage 308 or instead of storage 308.\n Control circuitry 304 may include video generating circuitry and tuning circuitry, such as one or more analog tuners, one or more MPEG-2 decoders or other digital decoding circuitry, high-definition tuners, or any other suitable tuning or video\ncircuits or combinations of such circuits.  Encoding circuitry (e.g., for converting over-the-air, analog, or digital signals to MPEG signals for storage) may also be provided.  Control circuitry 304 may also include scaler circuitry for upconverting and\ndownconverting content into the preferred output format of the user equipment 300.  Circuitry 304 may also include digital-to-analog converter circuitry and analog-to-digital converter circuitry for converting between digital and analog signals.  The\ntuning and encoding circuitry may be used by the user equipment device to receive and to display, to play, or to record content.  The tuning and encoding circuitry may also be used to receive guidance data.  The circuitry described herein, including for\nexample, the tuning, video generating, encoding, decoding, encrypting, decrypting, scaler, and analog/digital circuitry, may be implemented using software running on one or more general purpose or specialized processors.  Multiple tuners may be provided\nto handle simultaneous tuning functions (e.g., watch and record functions, picture-in-picture (PIP) functions, multiple-tuner recording, etc.).  If storage 308 is provided as a separate device from user equipment 300, the tuning and encoding circuitry\n(including multiple tuners) may be associated with storage 308.\n The control circuitry 304 may also include audio comparison circuitry suitable for recognizing voice inputs by a user, converting the voice input into text, and comparing the voice input to a database of interruption inputs.  For example, the\ncontrol circuitry 304 may include dedicated hardware to perform audio processing algorithms and/or audio recognition and comparison software.  The control circuitry 304 may access and run audio processing software stored on storage, such as storage 308.\n A user may send instructions to control circuitry 304 using user input interface 310.  User input interface 310 may be any suitable user interface, such as a remote control, mouse, trackball, keypad, keyboard, touch screen, touchpad, stylus\ninput, joystick, voice recognition interface, or other user input interfaces.  Display 312 may be provided as a stand-alone device or integrated with other elements of user equipment device 300.  For example, display 312 may be a touchscreen or\ntouch-sensitive display.  In such circumstances, user input interface 310 may be integrated with or combined with display 312.  Display 312 may be one or more of a monitor, a television, a liquid crystal display (LCD) for a mobile device, amorphous\nsilicon display, low temperature poly silicon display, electronic ink display, electrophoretic display, active matrix display, electro-wetting display, electrofluidic display, cathode ray tube display, light-emitting diode display, electroluminescent\ndisplay, plasma display panel, high-performance addressing display, thin-film transistor display, organic light-emitting diode display, surface-conduction electron-emitter display (SED), laser television, carbon nanotubes, quantum dot display,\ninterferometric modulator display, or any other suitable equipment for displaying visual images.  In some embodiments, display 312 may be HDTV-capable.  In some embodiments, display 312 may be a 3D display, and the interactive media guidance application\nand any suitable content may be displayed in 3D.  A video card or graphics card may generate the output to the display 312.  The video card may offer various functions such as accelerated rendering of 3D scenes and 2D graphics, MPEG-2/MPEG-4 decoding, TV\noutput, or the ability to connect multiple monitors.  The video card may be any processing circuitry described above in relation to control circuitry 304.  The video card may be integrated with the control circuitry 304.  Speakers 314 may be provided as\nintegrated with other elements of user equipment device 300 or may be stand-alone units.  The audio component of videos and other content displayed on display 312 may be played through speakers 314.  In some embodiments, the audio may be distributed to a\nreceiver (not shown), which processes and outputs the audio via speakers 314.\n The guidance application may be implemented using any suitable architecture.  For example, it may be a stand-alone application wholly-implemented on user equipment device 300.  In such an approach, instructions of the application are stored\nlocally (e.g., in storage 308), and data for use by the application is downloaded on a periodic basis (e.g., from an out-of-band feed, from an Internet resource, or using another suitable approach).  Control circuitry 304 may retrieve instructions of the\napplication from storage 308 and process the instructions to generate any of the displays discussed herein.  Based on the processed instructions, control circuitry 304 may determine what action to perform when input is received from input interface 310. \nFor example, movement of a cursor on a display up/down may be indicated by the processed instructions when input interface 310 indicates that an up/down button was selected.\n Detection module 316, or the detection module circuitry, may be incorporated into, coupled to, or accessible by the media guidance application (such as control circuitry 304).  Detection module 316 may be used, among other things, to detect user\nvoice inputs.  For example the detection module 316 may capture audio signals and identify when a user is speaking to the media guidance application.  In some embodiments, the detection module 316 may establish a baseline signal, such as a period of\nsilence or background noise, by monitoring an audio signal before the user input.  The detection module 316 may detect the user voice input by detecting that the audio signal deviates past a threshold value between a start and an end time.  In some\nembodiments, the media guidance application may distinguish between whether the user is speaking specifically to the media guidance application or to someone else.  For example, the media guidance application may employ the use of keywords, such as a\nname of the media guidance application, that acts as a trigger for the detection module 316 to begin listening for a user voice input.  In such embodiments, the media guidance application may run, in real time, audio recognition software that can detect\nspoken words in a monitored audio signal.  In response to detecting the keyword, the media guidance application may start detecting a user voice input.  The media guidance application may determine the end of the user voice input by detecting a period of\nsilence or by detecting a different keyword.\n In some embodiments, the media guidance application is a client-server based application.  Data for use by a thick or thin client implemented on user equipment device 300 is retrieved on-demand by issuing requests to a server remote to the user\nequipment device 300.  In one example of a client-server based guidance application, control circuitry 304 runs a web browser that interprets web pages provided by a remote server.  For example, the remote server may store the instructions for the\napplication in a storage device.  The remote server may process the stored instructions using circuitry (e.g., control circuitry 304) and generate the displays discussed above and below.  The client device may receive the displays generated by the remote\nserver and may display the content of the displays locally on equipment device 300.  This way, the processing of the instructions is performed remotely by the server while the resulting displays are provided locally on equipment device 300.  Equipment\ndevice 300 may receive inputs from the user via input interface 310 and transmit those inputs to the remote server for processing and generating the corresponding displays.  For example, equipment device 300 may transmit a communication to the remote\nserver indicating that an up/down button was selected via input interface 310.  The remote server may process instructions in accordance with that input and generate a display of the application corresponding to the input (e.g., a display that moves a\ncursor up/down).  The generated display is then transmitted to equipment device 300 for presentation to the user.\n In some embodiments, the media guidance application is downloaded and interpreted or otherwise run by an interpreter or virtual machine (run by control circuitry 304).  In some embodiments, the guidance application may be encoded in the ETV\nBinary Interchange Format (EBIF), received by control circuitry 304 as part of a suitable feed, and interpreted by a user agent running on control circuitry 304.  For example, the guidance application may be an EBIF application.  In some embodiments, the\nguidance application may be defined by a series of JAVA-based files that are received and run by a local virtual machine or other suitable middleware executed by control circuitry 304.  In some of such embodiments (e.g., those employing MPEG-2 or other\ndigital media encoding schemes), the guidance application may be, for example, encoded and transmitted in an MPEG-2 object carousel with the MPEG audio and video packets of a program.\n User equipment device 300 of FIG. 3 can be implemented in system 400 of FIG. 4 as user television equipment 402, user computer equipment 404, wireless user communications device 406, or any other type of user equipment suitable for accessing\ncontent, such as a non-portable gaming machine.  For simplicity, these devices may be referred to herein collectively as user equipment or user equipment devices, and may be substantially similar to user equipment devices described above.  User equipment\ndevices, on which a media guidance application may be implemented, may function as a standalone device or may be part of a network of devices.  Various network configurations of devices may be implemented and are discussed in more detail below.\n A user equipment device utilizing at least some of the system features described above in connection with FIG. 3 may not be classified solely as user television equipment 402, user computer equipment 404, or a wireless user communications device\n406.  For example, user television equipment 402 may, like some user computer equipment 404, be Internet-enabled allowing for access to Internet content, while user computer equipment 404 may, like some television equipment 402, include a tuner allowing\nfor access to television programming.  The media guidance application may have the same layout on various different types of user equipment or may be tailored to the display capabilities of the user equipment.  For example, on user computer equipment\n404, the guidance application may be provided as a web site accessed by a web browser.  In another example, the guidance application may be scaled down for wireless user communications devices 406.\n In system 400, there is typically more than one of each type of user equipment device but only one of each is shown in FIG. 4 to avoid overcomplicating the drawing.  In addition, each user may utilize more than one type of user equipment device\nand also more than one of each type of user equipment device.\n In some embodiments, a user equipment device (e.g., user television equipment 402, user computer equipment 404, wireless user communications device 406) may be referred to as a \"second screen device.\" For example, a second screen device may\nsupplement content presented on a first user equipment device.  The content presented on the second screen device may be any suitable content that supplements the content presented on the first device.  In some embodiments, the second screen device\nprovides an interface for adjusting settings and display preferences of the first device.  In some embodiments, the second screen device is configured for interacting with other second screen devices or for interacting with a social network.  The second\nscreen device can be located in the same room as the first device, a different room from the first device but in the same house or building, or in a different building from the first device.\n The user may also set various settings to maintain consistent media guidance application settings across in-home devices and remote devices.  Settings include those described herein, as well as channel and program favorites, programming\npreferences that the guidance application utilizes to make programming recommendations, display preferences, and other desirable guidance settings.  For example, if a user sets a channel as a favorite on, for example, the web site www.allrovi.com on\ntheir personal computer at their office, the same channel would appear as a favorite on the user's in-home devices (e.g., user television equipment and user computer equipment) as well as the user's mobile devices, if desired.  Therefore, changes made on\none user equipment device can change the guidance experience on another user equipment device, regardless of whether they are the same or a different type of user equipment device.  In addition, the changes made may be based on settings input by a user,\nas well as user activity monitored by the guidance application.\n The user equipment devices may be coupled to communications network 414.  Namely, user television equipment 402, user computer equipment 404, and wireless user communications device 406 are coupled to communications network 414 via\ncommunications paths 408, 410, and 412, respectively.  Communications network 414 may be one or more networks including the Internet, a mobile phone network, mobile voice or data network (e.g., a 4G or LTE network), cable network, public switched\ntelephone network, or other types of communications network or combinations of communications networks.  Paths 408, 410, and 412 may separately or together include one or more communications paths, such as, a satellite path, a fiber-optic path, a cable\npath, a path that supports Internet communications (e.g., IPTV), free-space connections (e.g., for broadcast or other wireless signals), or any other suitable wired or wireless communications path or combination of such paths.  Path 412 is drawn with\ndotted lines to indicate that in the exemplary embodiment shown in FIG. 4 it is a wireless path and paths 408 and 410 are drawn as solid lines to indicate they are wired paths (although these paths may be wireless paths, if desired).  Communications with\nthe user equipment devices may be provided by one or more of these communications paths, but are shown as a single path in FIG. 4 to avoid overcomplicating the drawing.\n Although communications paths are not drawn between user equipment devices, these devices may communicate directly with each other via communication paths, such as those described above in connection with paths 408, 410, and 412, as well as\nother short-range point-to-point communication paths, such as USB cables, IEEE 1394 cables, wireless paths (e.g., Bluetooth, infrared, IEEE 802-11x, etc.), or other short-range communication via wired or wireless paths.  BLUETOOTH is a certification mark\nowned by Bluetooth SIG, INC.  The user equipment devices may also communicate with each other directly through an indirect path via communications network 414.\n System 400 includes content source 416 and media guidance data source 418 coupled to communications network 414 via communication paths 420 and 422, respectively.  Paths 420 and 422 may include any of the communication paths described above in\nconnection with paths 408, 410, and 412.  Communications with the content source 416 and media guidance data source 418 may be exchanged over one or more communications paths, but are shown as a single path in FIG. 4 to avoid overcomplicating the\ndrawing.  In addition, there may be more than one of each of content source 416 and media guidance data source 418, but only one of each is shown in FIG. 4 to avoid overcomplicating the drawing.  (The different types of each of these sources are\ndiscussed below.) If desired, content source 416 and media guidance data source 418 may be integrated as one source device.  Although communications between sources 416 and 418 with user equipment devices 402, 404, and 406 are shown as through\ncommunications network 414, in some embodiments, sources 416 and 418 may communicate directly with user equipment devices 402, 404, and 406 via communication paths (not shown) such as those described above in connection with paths 408, 410, and 412.\n Content source 416 may include one or more types of content distribution equipment including a television distribution facility, cable system headend, satellite distribution facility, programming sources (e.g., television broadcasters, such as\nNBC, ABC, HBO, etc.), intermediate distribution facilities and/or servers, Internet providers, on-demand media servers, and other content providers.  NBC is a trademark owned by the National Broadcasting Company, Inc., ABC is a trademark owned by the\nAmerican Broadcasting Company, Inc., and HBO is a trademark owned by the Home Box Office, Inc.  Content source 416 may be the originator of content (e.g., a television broadcaster, a Webcast provider, etc.) or may not be the originator of content (e.g.,\nan on-demand content provider, an Internet provider of content of broadcast programs for downloading, etc.).  Content source 416 may include cable sources, satellite providers, on-demand providers, Internet providers, over-the-top content providers, or\nother providers of content.  Content source 416 may also include a remote media server used to store different types of content (including video content selected by a user), in a location remote from any of the user equipment devices.  Systems and\nmethods for remote storage of content, and providing remotely stored content to user equipment are discussed in greater detail in connection with Ellis et al., U.S.  Pat.  No. 7,761,892, issued Jul.  20, 2010, which is hereby incorporated by reference\nherein in its entirety.\n Media guidance data source 418 may provide media guidance data, such as the media guidance data described above.  Media guidance data may be provided to the user equipment devices using any suitable approach.  In some embodiments, the guidance\napplication may be a stand-alone interactive television program guide that receives program guide data via a data feed (e.g., a continuous feed or trickle feed).  Program schedule data and other guidance data may be provided to the user equipment on a\ntelevision channel sideband, using an in-band digital signal, using an out-of-band digital signal, or by any other suitable data transmission technique.  Program schedule data and other media guidance data may be provided to user equipment on multiple\nanalog or digital television channels.\n In some embodiments, media guidance data source 418 may store a database of topics including a semantic network.  Upon request, media guidance data source 418 may access the database and transmit a list of topics or a subset of topics related to\na first topic.  Media guidance data source 418 may also search the database of topics for topics related to the first topic.\n Media guidance data source 418 may also include a list of potential interruption inputs.  For example, an interruption input may include a period of silence for a particular amount of time, a keyword, or a phrase, such as \"Ah,\", \"Um,\", \"Hmm,\",\n\"No .  . . \", or the like.  The keywords and phrases may be populated by a system administrator or an end user and may include both text as well as audio samples.  For instance, the list of potential interruption inputs may include a text list of the\npossible keywords that indicate an appropriate time for a media guidance application to interrupt or change the subject of a conversation.  The list of interruption inputs may also include a plurality of audio samples of a generic male voice and/or a\ngeneric female voice that speak these keywords or phrases.  In some embodiments, the media guidance data source 418 may store audio samples of the specific end user speaking the interruption keywords and phrases.  The audio samples may be established,\nfor example, in a setup or initialization procedure for the media guidance application.\n In some embodiments, guidance data from media guidance data source 418 may be provided to users' equipment using a client-server approach.  For example, a user equipment device may pull media guidance data from a server, or a server may push\nmedia guidance data to a user equipment device.  In some embodiments, a guidance application client residing on the user's equipment may initiate sessions with source 418 to obtain guidance data when needed, e.g., when the guidance data is out of date or\nwhen the user equipment device receives a request from the user to receive data.  Media guidance may be provided to the user equipment with any suitable frequency (e.g., continuously, daily, a user-specified period of time, a system-specified period of\ntime, in response to a request from user equipment, etc.).  Media guidance data source 418 may provide user equipment devices 402, 404, and 406 the media guidance application itself or software updates for the media guidance application.\n In some embodiments, the media guidance data may include viewer data.  For example, the viewer data may include current and/or historical user activity information (e.g., what content the user typically watches, what times of day the user\nwatches content, whether the user interacts with a social network, at what times the user interacts with a social network to post information, what types of content the user typically watches (e.g., pay TV or free TV), mood, brain activity information,\netc.).  The media guidance data may also include subscription data.  For example, the subscription data may identify to which sources or services a given user subscribes and/or to which sources or services the given user has previously subscribed but\nlater terminated access (e.g., whether the user subscribes to premium channels, whether the user has added a premium level of services, whether the user has increased Internet speed).  In some embodiments, the viewer data and/or the subscription data may\nidentify patterns of a given user for a period of more than one year.  The media guidance data may include a model (e.g., a survivor model) used for generating a score that indicates a likelihood a given user will terminate access to a service/source. \nFor example, the media guidance application may process the viewer data with the subscription data using the model to generate a value or score that indicates a likelihood of whether the given user will terminate access to a particular service or source. In particular, a higher score may indicate a higher level of confidence that the user will terminate access to a particular service or source.  Based on the score, the media guidance application may generate promotions and advertisements that entice the\nuser to keep the particular service or source indicated by the score as one to which the user will likely terminate access.\n Media guidance applications may be, for example, stand-alone applications implemented on user equipment devices.  For example, the media guidance application may be implemented as software or a set of executable instructions which may be stored\nin storage 308, and executed by control circuitry 304 of a user equipment device 300.  In some embodiments, media guidance applications may be client-server applications where only a client application resides on the user equipment device, and server\napplication resides on a remote server.  For example, media guidance applications may be implemented partially as a client application on control circuitry 304 of user equipment device 300 and partially on a remote server as a server application (e.g.,\nmedia guidance data source 418) running on control circuitry of the remote server.  When executed by control circuitry of the remote server (such as media guidance data source 418), the media guidance application may instruct the control circuitry to\ngenerate the guidance application displays and transmit the generated displays to the user equipment devices.  The server application may instruct the control circuitry of the media guidance data source 418 to transmit data for storage on the user\nequipment.  The client application may instruct control circuitry of the receiving user equipment to generate the guidance application displays.\n Content and/or media guidance data delivered to user equipment devices 402, 404, and 406 may be over-the-top (OTT) content.  OTT content delivery allows Internet-enabled user devices, including any user equipment device described above, to\nreceive content that is transferred over the Internet, including any content described above, in addition to content received over cable or satellite connections.  OTT content is delivered via an Internet connection provided by an Internet service\nprovider (ISP), but a third party distributes the content.  The ISP may not be responsible for the viewing abilities, copyrights, or redistribution of the content, and may only transfer IP packets provided by the OTT content provider.  Examples of OTT\ncontent providers include YOUTUBE, NETFLIX, and HULU, which provide audio and video via IP packets.  Youtube is a trademark owned by Google Inc., Netflix is a trademark owned by Netflix Inc., and Hulu is a trademark owned by Hulu, LLC.  OTT content\nproviders may additionally or alternatively provide media guidance data described above.  In addition to content and/or media guidance data, providers of OTT content can distribute media guidance applications (e.g., web-based applications or cloud-based\napplications), or the content can be displayed by media guidance applications stored on the user equipment device.\n Media guidance system 400 is intended to illustrate a number of approaches, or network configurations, by which user equipment devices and sources of content and guidance data may communicate with each other for the purpose of accessing content\nand providing media guidance.  The embodiments described herein may be applied in any one or a subset of these approaches, or in a system employing other approaches for delivering content and providing media guidance.  The following four approaches\nprovide specific illustrations of the generalized example of FIG. 4.\n In one approach, user equipment devices may communicate with each other within a home network.  User equipment devices can communicate with each other directly via short-range point-to-point communication schemes described above, via indirect\npaths through a hub or other similar device provided on a home network, or via communications network 414.  Each of the multiple individuals in a single home may operate different user equipment devices on the home network.  As a result, it may be\ndesirable for various media guidance information or settings to be communicated between the different user equipment devices.  For example, it may be desirable for users to maintain consistent media guidance application settings on different user\nequipment devices within a home network, as described in greater detail in Ellis et al., U.S.  patent application Ser.  No. 11/179,410, filed Jul.  11, 2005.  Different types of user equipment devices in a home network may also communicate with each\nother to transmit content.  For example, a user may transmit content from user computer equipment to a portable video player or portable music player.\n In a second approach, users may have multiple types of user equipment by which they access content and obtain media guidance.  For example, some users may have home networks that are accessed by in-home and mobile devices.  Users may control\nin-home devices via a media guidance application implemented on a remote device.  For example, users may access an online media guidance application on a website via a personal computer at their office, or a mobile device such as a PDA or web-enabled\nmobile telephone.  The user may set various settings (e.g., recordings, reminders, or other settings) on the online guidance application to control the user's in-home equipment.  The online guide may control the user's equipment directly, or by\ncommunicating with a media guidance application on the user's in-home equipment.  Various systems and methods for user equipment devices communicating, where the user equipment devices are in locations remote from each other, is discussed in, for\nexample, Ellis et al., U.S.  Pat.  No. 8,046,801, issued Oct.  25, 2011, which is hereby incorporated by reference herein in its entirety.\n In a third approach, users of user equipment devices inside and outside a home can use their media guidance application to communicate directly with content source 416 to access content.  Specifically, within a home, users of user television\nequipment 402 and user computer equipment 404 may access the media guidance application to navigate among and locate desirable content.  Users may also access the media guidance application outside of the home using wireless user communications devices\n406 to navigate among and locate desirable content.\n In a fourth approach, user equipment devices may operate in a cloud computing environment to access cloud services.  In a cloud computing environment, various types of computing services for content sharing, storage or distribution (e.g., video\nsharing sites or social networking sites) are provided by a collection of network-accessible computing and storage resources, referred to as \"the cloud.\" For example, the cloud can include a collection of server computing devices, which may be located\ncentrally or at distributed locations, that provide cloud-based services to various types of users and devices connected via a network such as the Internet via communications network 414.  These cloud resources may include one or more content sources 416\nand one or more media guidance data sources 418.  In addition or in the alternative, the remote computing sites may include other user equipment devices, such as user television equipment 402, user computer equipment 404, and wireless user communications\ndevice 406.  For example, the other user equipment devices may provide access to a stored copy of a video or a streamed video.  In such embodiments, user equipment devices may operate in a peer-to-peer manner without communicating with a central server.\n The cloud provides access to services, such as content storage, content sharing, or social networking services, among other examples, as well as access to any content described above, for user equipment devices.  Services can be provided in the\ncloud through cloud computing service providers, or through other providers of online services.  For example, the cloud-based services can include a content storage service, a content sharing site, a social networking site, or other services via which\nuser-sourced content is distributed for viewing by others on connected devices.  These cloud-based services may allow a user equipment device to store content to the cloud and to receive content from the cloud rather than storing content locally and\naccessing locally-stored content.\n A user may use various content capture devices, such as camcorders, digital cameras with video mode, audio recorders, mobile phones, and handheld computing devices, to record content.  The user can upload content to a content storage service on\nthe cloud either directly, for example, from user computer equipment 404 or wireless user communications device 406 having content capture feature.  Alternatively, the user can first transfer the content to a user equipment device, such as user computer\nequipment 404.  The user equipment device storing the content uploads the content to the cloud using a data transmission service on communications network 414.  In some embodiments, the user equipment device itself is a cloud resource, and other user\nequipment devices can access the content directly from the user equipment device on which the user stored the content.\n Cloud resources may be accessed by a user equipment device using, for example, a web browser, a media guidance application, a desktop application, a mobile application, and/or any combination of access applications of the same.  The user\nequipment device may be a cloud client that relies on cloud computing for application delivery, or the user equipment device may have some functionality without access to cloud resources.  For example, some applications running on the user equipment\ndevice may be cloud applications, i.e., applications delivered as a service over the Internet, while other applications may be stored and run on the user equipment device.  In some embodiments, a user device may receive content from multiple cloud\nresources simultaneously.  For example, a user device can stream audio from one cloud resource while downloading content from a second cloud resource.  Or a user device can download content from multiple cloud resources for more efficient downloading. \nIn some embodiments, user equipment devices can use cloud resources for processing operations such as the processing operations performed by processing circuitry described in relation to FIG. 3.\n As referred herein, the term \"in response to\" refers to initiated as a result of.  For example, a first action being performed in response to another action may include interstitial steps between the first action and the second action.  As\nreferred herein, the term \"directly in response to\" refers to caused by.  For example, a first action being performed directly in response to another action may not include interstitial steps between the first action and the second action.\n The media guidance application may provide media guidance and recommendation features to the user through a voice interface.  For example, the media guidance application may hold a conversation with the user, where the user provides a voice\nquery, the media guidance application provides a spoken answer, and so on.  In this manner, the media guidance application and the user may hold a back-and-forth conversation related to a topic of the user's choice.\n In some embodiments, the media guidance application may \"interrupt\" the user and/or change the subject of the conversation.  For example, the user may pause or hesitate, and the media guidance application may detect this pause or hesitation and\ninterject with another spoken response.  In this manner, the media guidance application is breaking the \"turn-based\" approach of a traditional voice interface and allowing the media guidance application to provide several potential answers when the user\nindicates hesitation.\n The media guidance application may maintain \"personalities\" that may be selected by the user.  The different personalities may utilize different models, as discussed further below, that differ at least in the frequency with which they interrupt\nthe user and how far the personality may stray from a current topic.  For example, \"Rachael\" may be a personality that employs a female voice, interrupts with a normal frequency, and rarely strays from a current topic.  For instance, if a user asked\nRachael for recommendations for science fiction movies, Rachael may respond with \"How about Star Trek?\", and after five seconds of silence, may respond with \"How about Star Wars?\" In contrast, \"Rambo\" may employ a tough male voice that interrupts with\nhigh frequency and interrupts with wide changes in subject.  For example, the user may ask Rambo for action movie recommendations.  In response, Rambo may bombard the user with rapid-fire suggestions: \"HOW ABOUT MISSION IMPOSSIBLE 3?? TOO SLOW!! HOW\nABOUT INDEPENDENCE DAY??\" The user may choose their preferred personality to interact with, or even create their own custom personality.\n FIG. 5 is a flowchart 500 of illustrative steps for control circuitry (such as control circuitry 304) to provide media guidance in accordance with some embodiments of the disclosure.  Flowchart 500 describes control circuitry 304 receiving a\nfirst voice input, providing a first response, detecting an interruption point, and providing a second response that is related to a topic of the first voice input.  In some embodiments, this process may be encoded on to non-transitory storage medium\n(such as storage device 308) as a set of instructions to be decoded and executed by processing circuitry (such as processing circuitry 306).  Processing circuitry may in turn provide instructions to other sub-circuits contained within control circuitry\n304, such as the tuning, video generating, encoding, decoding, encrypting, decrypting, scaling, analog/digital conversion circuitry, image recognition and analysis circuitry, and the like.\n At 502, the process to provide media guidance may begin.  In some embodiments, the process may begin either directly or indirectly in response to a request from the user or a user action, such as an input into user input interface 310.  For\nexample, the process may begin directly in response to control circuitry 304 receiving signals from user input interface 310 or control circuitry 304 may prompt the user to confirm their input using a display (such as display 312) prior to running the\nprocess.\n At 504, the control circuitry 304 may receive a first voice input.  In some embodiments, the control circuitry 304 may receive the first voice input using audio recognition hardware, such as detection module 316.  In some embodiments, the\ncontrol circuitry 304 may also run audio recognition software in order to recognize the first voice input.  At 506, the control circuitry 304 may access a database of topics, the database of topics including a semantic network indicating relationships\nbetween a plurality of topics.  As discussed above, the semantic network may be populated in any suitable method and may indicate relationships in any suitable manner.  In some embodiments, the semantic network may be a knowledge graph that represents\ntopics as nodes/vertices in a directed or undirected graph and relationships as numerical values indicating the similarity or dissimilarity between topics.\n At 508, the control circuitry 304 may identify a first topic from the database of topics that is associated with the first voice input.  The control circuitry 304 may identify the first topic in any suitable manner.  In some embodiments, as\ndiscussed below in relation to FIG. 8, the control circuitry 304 may use audio recognition software to extract keywords from the first voice input.  The control circuitry 304 may compare these extracted keywords to keywords of topics associated with the\nfirst topic.  For example, the database of topics may maintain, for each topic, a list of keywords associated with the topic.  In some embodiments, the keywords themselves may be indicated as separate vertices/nodes in the database of topics.  The\ncontrol circuitry 304 may identify a subset of topics and select, of the subset of topics, one topic that represents the closest match.  For instance, the matching topic may represent the topic of the subset of topics that matches the most number of\nkeywords extracted from the first voice input.\n At 510, a user interface may generate a first response to the first voice input.  The user interface may include speakers to output a voice response and/or a display for relaying text-based answers.  The first response may be determined in any\nsuitable manner.  For example, as discussed below in relation to FIG. 9, the first response may be determined by identifying a topic related to the topic of the first voice input.  As an illustrative example, the user may query, \"Please recommend a\npopular sci-fi movie.\" The system may identify \"science fiction\" as the topic of the user's voice input, and may identify \"Star Trek\" as a topic related to \"science fiction.\" After identifying the topic, the system may incorporate the related topic into\na suitable response to the user's query.  For example, the media guidance application may respond with \"How about the Star Trek movie from 2009?\" At 512, subsequent to generating the first response, the control circuitry 304 may receive a second voice\ninput.  512 may be substantially similar to 504.  For instance, the control circuitry 304 may receive the second voice input using audio recognition hardware, such as detection module 316.  In some embodiments, the control circuitry 304 may also run\naudio recognition software in order to recognize the second voice input.\n At 514, the control circuitry 304 may monitor the second voice input to determine whether it includes an interruption input, indicating that it may be an appropriate time for the media guidance application to interject with a change of subject. \nTo this end, the control circuitry 304 may compare the second voice input to a list of interruption inputs to determine a match between the second voice input and an interruption input from the list of interruption inputs.  The control circuitry 304 may\nretrieve the list of interruption inputs from any suitable storage, such as local storage 308 or remote storage, such as media guidance data source 418.  The control circuitry 304 may extract one or more keywords from the second voice input.  For\nexample, the control circuitry 304 may utilize audio recognition software to convert the second voice input into text and parse the text into keywords or phrases.  The control circuitry 304 may then compare the extracted keywords to the interruption\ninputs from the list of interruption inputs.  In some embodiments, the comparison includes a text-to-text search of the extracted keywords and the text of the interruption inputs.  Any other suitable comparison is contemplated, including audio-to-text,\ntext-to-audio, and audio-to-audio comparisons.  For example, the list of interruption inputs may include both a text representation of the keyword as well as one or more associated audio samples of a human speaking the keyword.  The control circuitry 304\nmay perform a direct audio comparison between the second voice input and the audio samples in the list of interruption inputs in order to identify a match.\n At 516, the control circuitry 304 may, in response to determining a match between the second voice input and the interruption input, generate a second response to the first voice input.  In some embodiments, the second response to the first\nvoice input may be determined in much the same way as the first response from 510.  The second response may be associated with a second topic from the database of topics that is associated with the first topic.  The control circuitry 304 may access the\ndatabase of topics and identify a subset of topics that are within a relationship threshold of the first topic of the first voice input.  The control circuitry 304 may select one topic from the subset of topics and form a response based on the topic, as\ndiscussed above in relation to 510 and below in relation with FIG. 12.\n FIG. 6 is a flowchart 600 of illustrative steps for receiving a first voice input.  Flowchart 600 describes that steps that control circuitry 304 would take to receive and store the first voice input to memory.  At 602, the process to receive a\nfirst voice input may begin.  In some embodiments, the process may begin either directly or indirectly in response to a request from the user or a user action, such as an input into user input interface 310.  For example, the process may begin directly\nin response to control circuitry 304 receiving signals from user input interface 310 or control circuitry 304 may prompt the user to confirm their input using a display (such as display 312) prior to running the process.  In some embodiments, the control\ncircuitry 304 may ping either local storage 308 or remote storage, such as media guidance data source 418, to determine whether the storage is available and ready to store the first voice input.  The control circuitry 304 may also initialize any\nvariables necessary to store the first voice input.  For example, the control circuitry 304 may initialize an array in memory to store audio signals for successive time periods.  In some embodiments, the control circuitry 304 may allocate a\none-dimensional variable to continuously store a stream of audio.\n At 604, the control circuitry may activate the voice detection module, such as detection module 316.  For example, the voice detection module may include a microphone or any other audio detection hardware suitable for detecting and recording\naudio signals.  At 606, the control circuitry 304 may monitor an audio signal using the voice detection module.  For example, the control circuitry 304 may receive the audio signal using the voice detection module and store and/or continuously stream the\naudio signal to memory.  At 608, the control circuitry 304 may establish a baseline signal.  For instance, the control circuitry 304 may monitor a certain period of time of silence and/or background noise.  As will be understood by those of skill in the\nart, suitable audio analytics, including performing low/high/band-pass filtering and/or transform analysis may be performed in order to identify and isolate the baseline signal.\n At 610, the control circuitry 304 may detect, at a start time, a deviation above threshold value of monitored audio signal from baseline signal.  For instance, the threshold value may include a threshold decibel level above silence, and the\ncontrol circuitry 304 may detect that the monitored audio signal has exceeded that threshold decibel level, thus indicating the start of a voice input.  In some embodiments, the control circuitry 304 may first remove the baseline signal using audio\nanalysis techniques, as will be understood by those of skill in the art.  As an illustrative example, the control circuitry 304 may develop a signature frequency profile of the baseline signal, representing the background noise.  The control circuitry\n304 may remove this signature frequency profile from a currently detected audio signal in order to remove the background noise, and may then detect whether the resulting audio signal exceeds a threshold value.\n At 612, the control circuitry 304 may detect, at an end time, that the monitored audio signal has returned to within a threshold value of the baseline signal.  For example, similar to 610 discussed above, the threshold value may be a threshold\ndecibel level, and the control circuitry 304 may detect when the monitored audio signal drops below the threshold decibel level.  As discussed above in 610, the control circuitry 304 may first remove a signature frequency profile of background noise from\nthe monitored audio signal before determining whether the signal has dropped below the threshold value.  In some embodiments, the control circuitry 304 detects whether there has been silence for a predetermined amount of time.  For example, a period of\nsilence of is may indicate that the user has finished his or her query to the media guidance application.  At 614, the control circuitry 304 may identify a portion of the monitored audio signal between the start time and the end time as the first voice\ninput and store the first voice input to memory at 616.\n FIG. 7 is a flowchart 700 of illustrative steps for accessing a database of topics.  Flowchart 700 describes the steps that control circuitry 304 may take to access the database of topics.  Control circuitry 304 may access the database of topics\nusing any suitable method, and flowchart 700 is provided as an illustrative example only.  For instance, as discussed above and throughout, the database of topics may include a semantic network such as a Knowledge Graph and may be stored either in local\nstorage, such as storage 308, or in remote storage, such as media guidance data source 418.  If the database of topics is stored in local storage 308, the control circuitry 304 may access the database through internal memory channels of the user\nequipment device 300.  If the database of topics is stored in remote storage, such as media guidance data source 418, the control circuitry 304 may access the database through, for example, communications network 414, including, but not limited to, local\narea networks (LAN), the Internet, or a combination of the two.\n At 702, the control circuitry may begin a process to access the database of topics, including any appropriate initialization and allocation routines.  At 704, the control circuitry 304 may transmit a ping to the database of topics over a\nnetwork.  As discussed above, the network may include any type of network, including a local network, the Internet, or local memory channels.  The ping may include a short data message that queries whether the database is ready to accept commands and/or\ndata.  At 706, the control circuitry 304 may receive a response to the ping from the database of topics, indicating that the database of topics is ready to receive a query.  As with the ping, the response may be a short data message that indicates that\nthe database is ready to accept commands and/or data.\n FIG. 8 is a flowchart 800 of illustrative steps for identifying a first topic that is associated with the first voice input.  FIG. 8 describes an iterative process in which each of the topics in the database of topics is compared to the first\nvoice input in order to determine the closest matching topic.  At 802, the process to identify a first topic that is associated with the first voice input may begin.  In some embodiments, the process may begin either directly or indirectly in response to\na request from the user or a user action, such as an input into user input interface 310.  For example, the process may begin directly in response to control circuitry 304 receiving signals from user input interface 310 or control circuitry 304 may\nprompt the user to confirm their input using a display (such as display 312) prior to running the process.\n At 804, the control circuitry may extract one or more keywords from the first voice input.  In some embodiments, extracting keywords from the first voice input may include converting the audio signal of the first voice input into a text format\nusing suitable audio recognition software.  The converted text may be parsed or grouped into suitable keywords and/or phrases.  In some embodiments, the keywords may be extracted as audio samples of the first voice input.  The control circuitry 304 may\nuse suitable audio recognition software to separate the individual words and/or phrases spoken by the user in the first voice input.  For instance, the control circuitry 304 may detect brief pauses or periods of silence in the first voice input to\ndelineate words or phrases.\n At 806, the control circuitry 304 may optionally transmit the extracted keywords to the database of topics, for example via network 414.  In such embodiments, the control circuitry 304 may query the database of topics to identify the first\ntopic, and the database of topics may perform the subsequent steps 808 to 816 and return the result to the control circuitry 304 via network 414.\n At 808, the control circuitry 304 may select a topic from the database of topics, and at 810, determine whether the selected topic matches an extracted keyword(s).  In some embodiments, the database of topics may list several keywords associated\nwith the selected topic.  These associated keywords may be compared to the extracted keywords using any suitable method, including a text-to-text comparison, text-to-audio comparison, audio-to-text comparison, or audio-to-audio comparison.  If none of\nthe extracted keywords match the keywords associated with the selected topic, then the control circuitry 304 may return to 808.  If one or more keywords match, then the control circuitry may continue to 812.\n At 812, the control circuitry 304 may determine whether the currently selected topic is the closest matching topic in the database of topics to the first voice input.  To this end, the control circuitry 304 may determine either (1) no topic is\ncurrently stored to memory as a matching topic; or (2) the currently selected topic contains more matching keywords than the currently stored matching topic.  If the control circuitry 304 determines that neither (1) nor (2) is true, then the control\ncircuitry 304 may return to 808.  If the control circuitry 304 determines that either (1), (2), or both are true, then the control circuitry 304 may continue to 814 and store the selected topic to memory as the current closest matching topic.  At 816,\nthe control circuitry 816 may determine whether there are any remaining topics in the database of topics to compare to the extracted keywords.  If there are topics remaining, then the control circuitry 304 may return to 808 and select a different topic\nfrom the database of topics.  If all topics have been compared against the extracted keywords, then the control circuitry 304 may, at 818, return the topic stored to memory as the result of the process.  In this manner, the topic from the database of\ntopics that most closely matches the keywords extracted from the first voice input is returned as the topic of the first voice input.  As will be understood by those of skill in the art, FIG. 8 is provided for illustrative purposes only, and other\nmethods for identifying a first topic of the first voice input may be utilized, as will be understood by those of ordinary skill in the art.\n FIG. 9 is a flowchart 900 of illustrative steps for generating a first response to the first voice input.  FIG. 9 describes the steps that control circuitry 304 may take to output a first response to the user's first voice input.  At 902, the\nprocess to generating a first response to the first voice input may begin.  In some embodiments, the process may begin either directly or indirectly in response to a request from the user or a user action, such as an input into user input interface 310. \nFor example, the process may begin directly in response to control circuitry 304 receiving signals from user input interface 310 or control circuitry 304 may prompt the user to confirm their input using a display (such as display 312) prior to running\nthe process.\n At 904, the control circuitry 904 may access the database of topics.  904 may be substantially similar to 506 and/or process 700, as discussed above in relation to FIGS. 5 and 7, respectively.  At 906, the control circuitry 304 may extract, from\nthe database, relationships between the first topic and a remainder of the plurality of topics.  As discussed above, the relationships may be kept in any suitable manner.  For example, the relationships may be represented by numerical values between\nnodes of a Knowledge Graph.  At 908, the control circuitry 904 may access a voice personality profile to retrieve a relationship threshold.  As discussed above, the relationship threshold may be a metric that may be used to determine whether a first\ntopic is related to a second topic.  The voice personality profile may be stored either on local storage, such as storage 308 or remote storage, such as media guidance data source 418.\n At 910, the control circuitry 304 may compare each of the relationships between the first topic and the remainder of topics to the relationship threshold.  For example, in some embodiments, the comparison may include determining whether a\nnumerical value of a particular relationship is greater than a numerical relationship threshold.  At 912, the control circuitry may identify, based on the comparison, a subset of the relationships that do not exceed the relationship threshold and a list\nof topics that corresponds to the subset of relationships.  These lists may represent the list of topics that are related to the first topics of the first voice input.\n At 914, the control circuitry may select one of the topics from the list of topics.  The control circuitry 304 may select the topic in any suitable manner.  For example, the control circuitry may select the topic that is the closest match to the\nfirst topic, similar to 812 discussed above in relation to FIG. 8.  In some embodiments, the control circuitry may incorporate a user's media preferences in the selection of a topic from the list of topics.  For example, the control circuitry 304 may\naccess a user profile, for instance stored on local storage 308, and extract one or more media preferences from the user profile.  As an illustrative example, the control circuitry 304 may extract a genre preference of the user from the user profile. \nThe control circuitry 304 may then further filter the subset of topics to those that satisfy the media preference of the user.  Extending the illustrative example above, the control circuitry 304 may select only those topics that both relate to the first\ntopic of the first voice input and that relates to a genre that is preferred by the user.  In some embodiments, where a plurality of topics remains after all selection criteria have been applied, the control circuitry 304 may select one of the remaining\ntopics at random.  As will be understood by one of ordinary skill in the art, these examples are provided for illustrative purposes only, and the control circuitry 304 may select one of the topics using any suitable means.\n At 916, the control circuitry 304 may output on a user interface a first response based on the selected topic.  In some embodiments, 916 includes generating a full sentence based on the selected topic.  In some embodiments, the control circuitry\n304 may access a list of responses that have been previously authored by a human, wherein each of the list of responses leaves a blank to insert a relevant keyword.  In such instances, the control circuitry 304 may access a list of keywords associated\nwith the selected topic, select one of the responses from the list of responses, and input a keyword into the blank in order to compose the full sentence.  As an illustrative example, the control circuitry 304 may select \"How about .sub.------------?\" as\na potential response, and identify \"Star Trek\" as the selected topic.  The control circuitry 304 may insert the key phrase \"Star Trek\" into the blank to compose the full sentence \"How about Star Trek?\" As will be understood by those of ordinary skill in\nthe art, these examples are provided for illustrative purposes only, and other methods of outputting the first response may be contemplated.\n FIG. 10 is a flowchart 1000 of illustrative steps for receiving a second voice input subsequent to generating the first response.  Flowchart 1000 describes the steps that a control circuitry may take to receive and store a second voice input to\nmemory.  The steps of FIG. 10 may be substantially similar to the steps discussed above in relation to FIG. 6.  At 1002, the process to receive a second voice input may begin.  In some embodiments, the process may begin either directly or indirectly in\nresponse to a request from the user or a user action, such as an input into user input interface 310.  For example, the process may begin directly in response to control circuitry 304 receiving signals from user input interface 310 or control circuitry\n304 may prompt the user to confirm their input using a display (such as display 312) prior to running the process.  In some embodiments, the control circuitry 304 may ping either local storage 308 or remote storage, such as media guidance data source\n418, to determine whether the storage is available and ready to store the second voice input.  The control circuitry 304 may also initialize any variables necessary to store the second voice input.  For example, the control circuitry 304 may initialize\nan array in memory to store audio signals for successive time periods.  In some embodiments, the control circuitry 304 may allocate a one-dimensional variable to continuously store a stream of audio.\n At 1004, the control circuitry 304 may activate the voice detection module, such as detection module 316.  For example, the voice detection module may include a microphone or any other audio detection hardware suitable for detecting and\nrecording audio signals.  At 1006, the control circuitry 304 may monitor an audio signal using the voice detection module.  For example, the control circuitry 304 may receive the audio signal using the voice detection module and store and/or continuously\nstream the audio signal to memory.  In some embodiments, the control circuitry 304 may continuously record audio from the second voice input to the second voice input.  At 1008, the control circuitry 304 may establish a baseline signal.  For instance,\nthe control circuitry 304 may monitor a certain period of time of silence and/or background noise.  As will be understood by those of skill in the art, suitable audio analytics, including performing low/high/band-pass filtering and/or transform analysis\nmay be performed in order to identify and isolate the baseline signal.\n At 1010, the control circuitry 304 may detect, at a start time, a deviation above threshold value of monitored audio signal from baseline signal.  For instance, the threshold value may include a threshold decibel level above silence, and the\ncontrol circuitry 304 may detect that the monitored audio signal has exceeded that threshold decibel level, thus indicating the start of a voice input.  In some embodiments, the control circuitry 304 may first remove the baseline signal using audio\nanalysis techniques, as will be understood by those of skill in the art.  As an illustrative example, the control circuitry 304 may develop a signature frequency profile of the baseline signal, representing the background noise.  The control circuitry\n304 may remove this signature frequency profile from a currently detected audio signal in order to remove the background noise, and may then detect whether the resulting audio signal exceeds a threshold value.\n At 1012, the control circuitry 304 may detect, at an end time, that the monitored audio signal has returned to within a threshold value of the baseline signal.  For example, similar to 1010 discussed above, the threshold value may be a threshold\ndecibel level, and the control circuitry 304 may detect when the monitored audio signal drops below the threshold decibel level.  As discussed above in 1010, the control circuitry 304 may first remove a signature frequency profile of background noise\nfrom the monitored audio signal before determining whether the signal has dropped below the threshold value.  In some embodiments, the control circuitry 304 detects whether there has been silence for a predetermined amount of time.  For example, a period\nof silence of is may indicate that the user has finished his or her query to the media guidance application.  At 1014, the control circuitry 304 may identify a portion of the monitored audio signal between the start time and the end time as the second\nvoice input and store the second voice input to memory at 1016.\n FIG. 11 is a flowchart 1100 of illustrative steps for comparing the second voice input to identify an interruption input.  Flowchart 1100 describes the process by which control circuitry 304 may compare the second voice input to a list of\ninterruption inputs in order to identify whether the second voice input includes an appropriate interruption input.  At 1102, the process to compare the second voice input to identify an interruption input may begin.  In some embodiments, the process may\nbegin either directly or indirectly in response to a request from the user or a user action, such as an input into user input interface 310.  For example, the process may begin directly in response to control circuitry 304 receiving signals from user\ninput interface 310 or control circuitry 304 may prompt the user to confirm their input using a display (such as display 312) prior to running the process.\n At 1104, the control circuitry 304 may access a database and retrieve a list of interruption inputs.  In some embodiments the database storing the list of interruption inputs may be the same database as the database of topics.  In some\nembodiments the database storing the list of interruption inputs may be a different database as the database of topics.  The database storing the list of interruption inputs may be stored in any suitable storage, including local storage 308 and remote\nstorage, such as media guidance data source 418.\n At 1106, the control circuitry 304 may extract keyword(s) from the second voice input.  1106 may be substantially similar to 804 discussed above in relation to FIG. 8.  In some embodiments, extracting keywords from the second voice input may\ninclude converting the audio signal of the second voice input into a text format using suitable audio recognition software.  The converted text may be parsed or grouped into suitable keywords and/or phrases.  In some embodiments, the keywords may be\nextracted as audio samples of the second voice input.  The control circuitry 304 may use suitable audio recognition software to separate the individual words and/or phrases spoken by the user in the second voice input.  For instance, the control\ncircuitry 304 may detect brief pauses or periods of silence in the second voice input to delineate words or phrases.\n At 1108, the control circuitry 304 may select an interruption input from the list of interruption input.  At 1110, the control circuitry 304 may compare the selected interruption input to the extracted keyword(s) to determine whether there is a\nmatch.  1110 may be substantially similar to 810 discussed above in relation to FIG. 8.  For instance, the selected interruption input may be compared to the extracted keywords in any suitable manner, including a text-to-text comparison, text-to-audio\ncomparison, audio-to-text comparison, or audio-to-audio comparison.  If none of the extracted keywords match the selected interruption input, then the control circuitry 304 may return to 1108.  If one or more keywords match, then the control circuitry\nmay continue to 1112 and return an indication of a match between the second voice input and the interruption input.\n FIG. 12 is a flowchart 1200 of another set of illustrative steps for generating a second response to the first voice input.  FIG. 12 describes the process by which a control circuitry 304 may change the subject, including how to integrate user\npreferences into the response and when to wait for an appropriate time to respond.  At 1202, the process to generate a second response to the first voice input may begin.  In some embodiments, the process may begin either directly or indirectly in\nresponse to a request from the user or a user action, such as an input into user input interface 310.  For example, the process may begin directly in response to control circuitry 304 receiving signals from user input interface 310 or control circuitry\n304 may prompt the user to confirm their input using a display (such as display 312) prior to running the process.\n 1204 to 1212 of FIG. 12 may be substantially similar to 904 to 912 of FIG. 9, discussed above.  At 1208, the control circuitry 304 may further retrieve an interruption threshold period of time from the voice personality profile.  At 1214, the\ncontrol circuitry 304 may determine whether to integrate user preferences to the generated response.  For example, the control circuitry 304 may identify whether an option to integrate user preferences into the media guidance application's voice\nresponses has been selected by the user.  If the user preferences are not to be integrated into the response, then the control circuitry may continue to 1220 and select a topic from the list of topics.  1220 may be substantially similar to 914 discussed\nabove in relation to FIG. 9.  If the user preferences are to be integrated into the response, then the control circuitry 304 may retrieve a genre preference from the user profile at 1216.  At 1218, the control circuitry 304 may select a topic from the\nlist of topics that relates to the genre preference.  For instance, the control circuitry 304 may filter the subset of topics based on the genre preference and select one of the resulting topics.  At 1222, the control circuitry 304 may compare a current\ntime to a time associated with the first voice input in order to calculate a time elapsed.  For example, the control circuitry 304 may access the current time from a clock or other timekeeping hardware and may retrieve the time associated with the first\nvoice input, such as a timestamp, from storage, such as local storage 308.  The control circuitry may take a difference of the current time and the time associated with the first voice input to calculate the time elapsed.\n At 1224, the control circuitry 304 may determine whether the time elapsed exceeds the interruption threshold period of time.  If the time elapsed does not exceed the interruption threshold period of time, then the control circuitry 304 may\nreturn to 1222 until the time elapsed does exceed the threshold period of time.  This may occur in a situation where the user does not wish to be interrupted with frequency greater than the interruption threshold period of time.  If the time elapsed\nexceeds the interruption threshold period of time, then the control circuitry 304 may continue to 1226 and output, using the user interface, a second response based on the selected topic.  1226 may be substantially similar to 916 described above in\nrelation to FIG. 9.  In some embodiments, 1226 includes generating a full sentence based on the selected topic.  In some embodiments, the control circuitry 304 may access a list of responses that have been previously authored by a human, wherein each of\nthe list of responses leaves a blank to insert a relevant keyword.  In such instances, the control circuitry 304 may access a list of keywords associated with the selected topic, select one of the responses from the list of responses, and input a keyword\ninto the blank in order to compose the full sentence.\n The above-described embodiments of the present disclosure are presented for purposes of illustration and not of limitation, and the present disclosure is limited only by the claims that follow.  Furthermore, it should be noted that the features\nand limitations described in any one embodiment may be applied to any other embodiment herein, and flowcharts or examples relating to one embodiment may be combined with any other embodiment in a suitable manner, done in different orders, or done in\nparallel.  In addition, the systems and methods described herein may be performed in real time.  It should also be noted that the systems and/or methods described above may be applied to, or used in accordance with, other systems and/or methods.", "application_number": "14757910", "abstract": " Systems and methods are described herein for providing media guidance.\n     Control circuitry may receive a first voice input and access a database\n     of topics to identify a first topic associated with the first voice\n     input. A user interface may generate a first response to the first voice\n     input, and subsequent to generating the first response, the control\n     circuitry may receive a second voice input. The control circuitry may\n     determine a match between the second voice input and an interruption\n     input such as a period of silence or a keyword or a phrase, such as\n     \"Ahh,\", \"Umm,\", or \"Hmm.\" The user interface may generate a second\n     response that is associated with a second topic related to the first\n     topic. By interrupting the conversation and changing the subject from\n     time to time, media guidance systems can appear to be more intelligent\n     and human.\n", "citations": ["5930751", "6397186", "6496799", "6721706", "7324947", "7720683", "8060525", "8510247", "8577671", "8645122", "8843372", "8935163", "9378740", "9465833", "20030125945", "20040006483", "20040100582", "20070005361", "20080046229", "20080059188", "20100262449", "20110060587", "20130154797", "20130268260", "20140046891", "20140282645", "20140337370", "20150039316", "20150142704", "20160065884", "20160269524", "20170061989", "20170116986", "20170147576"], "related": []}, {"id": "20170280191", "patent_code": "10362355", "patent_name": "Systems and methods for recording media assets", "year": "2019", "inventor_and_country_data": " Inventors: \nStathacopoulos; Paul (San Carlos, CA)  ", "description": "BACKGROUND\n The amount of media content available to users today is enormous.  When users are not able to consume media assets, at the time those media assets are broadcast, users schedule those media assets for recording.  However, users sometimes desire\nto only consume specific portions of media assets; thus, those users may want the ability to record only the desired portions.  Furthermore, optimizing storage of recorded media can be important from both capacity and cost perspectives.\nSUMMARY\n Therefore, systems and methods are provided herein to record portions of media assets.  Recording portions of media assets enables the system to both present parts of media assets that users desire to consume and also to ensure that those media\nassets occupy an optimal amount of storage space.  Specifically, a media guidance application may be configured to receive user input of a media asset to record, together with a criterion for recording portions of that media asset.  The media guidance\napplication may, upon receiving input from the user, schedule the media asset for recording, or, if the media asset is already in progress, start recording the media asset.  When the media guidance application starts recording the media asset, the media\nguidance application may create segments corresponding to portions of the media asset.  The media guidance application may execute a content recognition algorithm against each segment in order to determine a set of keywords associated with those\nsegments.  The media guidance application may also determine a set of keywords associated with the criterion.  The media guidance application may compare the keywords associated with each segment with the keywords associated with the criterion until a\nfirst segment is found that matches the criterion.\n When the media guidance application matches a segment to the criterion, the media guidance application may determine whether the next segment matches the criterion via the same keyword comparison described above.  If the next segment does not\nmatch the criterion, the media guidance application may store a delete indicator with that segment (e.g., in the metadata associated with that segment or in the segment map).  The media guidance application may compare the next segment with the criterion\nand, upon determining that this segment matches the criterion, determine whether the first segment matches the third segment.  If the media guidance application determines that the first segment matches the third segment, then the media guidance\napplication may remove the delete indicator from the second segment in order to retain continuity within the media asset of the segments related to one portion.\n For example, the media guidance application may receive input from a user to record a specific broadcast of the Winter Olympics with a criterion of \"ice hockey.\" A single broadcast of the Winter Olympics may include multiple sports and multiple\nhockey games.  Thus, when the media guidance application starts recording the specific broadcast, the media guidance application may add a delete indicator to all the segments until a first instance of an ice hockey game starts.  As the media guidance\napplication continues to record the ice hockey game, segments that are not related to ice hockey (e.g., news breaks, coverage of a highlight of another Olympic sport, etc.) may appear while the ice hockey game is on.  The media guidance application may\nretain those segments within the ice hockey game in order to retain continuity of the game.  However, if a segment (e.g., an Olympic news break) is between two different ice hockey games, the media guidance application may delete that segment or mark it\nfor deletion because the segment is not needed to maintain continuity of the ice hockey game.\n In some aspects, the media guidance application may perform the following actions for recording portions of a media asset based on a user's criteria.  Specifically, the media guidance application may receive input from the user including a user\nselection of the media asset and a criterion for retaining portions of the media asset.  For example, the media guidance application may provide an option to the user to record the media asset.  When the user selects that option, the media guidance\napplication may generate for display an option to input a criterion for recording portions of the media asset.  It should be noted that the media guidance application may provide an option for the user to input more than one criterion.  For example, the\nmedia guidance application may generate for display a text area that enables the user to enter one or more terms.  The media guidance application may then use those terms as a criterion or criteria.\n The media guidance application may determine whether the terms entered by the user should be used as one criterion or whether each term should be a criterion.  For example, if the media guidance application receives \"ice hockey, Canada\" as\ninput, the media guidance application may determine that both terms constitute one criterion and the user desires to retain only portions of the media assets that include ice hockey games where Team Canada is a participant.  However, if the media\nguidance application receives \"ice hockey, downhill skiing\" then the media guidance application may determine that the user desires to retain portions of the media asset that include ice hockey games and downhill skiing events.\n In addition or instead of generating for display a text area for user input, the media guidance application may generate one or more keywords associated with the media asset for a user to select.  The media guidance application may receive a\nuser selection of one or more keywords and enable the user to choose whether the keywords are to be used as one criterion or whether the keywords should constitute different criteria.\n When the media guidance application starts recording the media asset, the media guidance application may generate keywords for different segments of the media asset by executing a content recognition algorithm.  Specifically, the media guidance\napplication may generate a first set of keywords for a first segment of the media asset, a second set of keywords for a second segment of the media asset, and a third set of keywords for a third segment of the media asset by executing a content\nrecognition algorithm on the first segment, the second segment, and the third segment.  In this instance, the first segment within the media asset precedes the second segment and the second segment precedes the third segment.  To continue with the\nexample above, the media guidance application may record a first segment of the Winter Olympic games and execute a content recognition algorithm against that segment.\n Specifically, the media guidance application may use video content recognition to determine specific objects in each frame of the segment and then add to a list of keywords terms associated with each object.  For example, if the media guidance\napplication determines through video content recognition that a skier is within a frame of the segment, the term \"skiing\" may be added to the set of keywords.  Additionally or alternatively, if the media guidance application determines that a specific\nskier is in the frame, the skier's name may be added to the set of keywords.  The same process may be repeated using audio content recognition so that any audio keywords are added to the set of keywords.\n The media guidance application may generate a set of keywords for the criterion or criteria entered by the user.  Specifically, the media guidance application may generate a fourth set of keywords based on the criterion.  This is the set of\nkeywords that may later be compared with sets of keywords generated for each segment.  The media guidance application may generate the fourth set of words by, for example, retrieving, from a database, objects related to the terms input by the user.\n For example, if the media guidance application receives user input that includes the terms \"ice hockey\" and \"USA,\" the media guidance application may search the database for objects related to those two terms.  For example, the term \"ice hockey\"\nmay be related to objects such as a \"puck,\" \"hockey stick\", \"goalie,\" \"minor penalty,\" etc. Each of those objects may have keywords associated with the object.  Specifically, the object \"hockey stick\" may have associated keywords such as \"right-handed,\"\n\"left-handed,\" etc. The object \"goalie\" may have associated keywords such as \"goalie mask,\" \"pads,\" \"net minder,\" etc. The media guidance application may add all or some of those keywords to the fourth set of keywords.\n The media guidance application may compare keywords associated with different segments with keywords associated with the criterion input by the user.  Specifically, the media guidance application may compare the first set of keywords, the second\nset of keywords, and the third set of keywords with the fourth set of keywords.  For example, the media guidance application may compare each set of keywords and determine that for a first set ten out of twenty-one keywords match.  The media guidance\napplication may perform the same or similar comparison for the second and third sets of keywords.\n The media guidance application may determine, based on the comparing, that the first segment and the third segment match the criterion, and that the second segment does not match the criterion.  For example, a threshold value may be provided to\nthe media guidance application to determine whether a match has occurred.  The threshold value may be a percentage of the keywords matching or an absolute number.  In the example above, if ten out of 21 words have matched and the threshold value is 50%,\nthen the media guidance application may determine that no match has occurred based on ten out of 21 being less than 50%.\n If the media guidance application determines that a specific segment does not match the criterion/criteria, the media guidance application may mark that segment for deletion.  Specifically, the media guidance application may, based on\ndetermining that the second segment does not match the criterion, store a delete indicator for the second segment that indicates that the second segment is to be deleted.  For example, the media guidance application may store the delete indicator with\nmetadata associated with the specific segment.  Alternatively or additionally, the media guidance application may maintain a segment map that indicates, for each segment, whether the segment is to be retained or marked for deletion.  The media guidance\napplication may mark for deletion the second segment by adding a delete indicator to the location in the segment map corresponding to the second segment.\n When the media guidance application determines that the first set of keywords and the third set of keywords match the criterion/criteria and the second set of keywords, located between the first and third segments, does not, the media guidance\nmay compare the first and third segments to determine whether those segments are associated with the same portion of the media asset.  Specifically, the media guidance application may compare the first set of keywords to the third set of keywords.  The\ncomparison may be performed in the same or similar manner as the comparison between the first, second, and third sets of keywords and the set of keywords associated with the criterion, as described above.\n The media guidance application may determine whether the two segments match the same portion of the media asset.  Specifically, the media guidance application may determine, based on the comparing of the first set of keywords and the third set\nof keywords, whether the third segment matches the first segment.  The media guidance application may perform this step in the same or similar manner as determining that the first and the third segments match the criterion and the second does not, as\ndescribed above.\n If the media guidance application determines that the first and third segments correspond to the same portion of the media asset, the media guidance application may remove the delete indicator associated with the second segment.  For example,\nthe media guidance application may update the segment map to change the delete indicator associated with the second segment to an indicator indicating that the second segment is to be retained.  Additionally or alternatively, the media guidance\napplication may remove the delete indicator from the metadata associated with the second segment.\n For example, the media guidance application may receive user input to record a broadcast of the Winter Olympics that lasts 12 hours.  Within that broadcast, sports such as ice hockey, skiing, snowboarding, etc., may be shown.  The media guidance\napplication may receive from the user input to record the broadcast and specify that \"ice hockey\" is to be recorded.  The media guidance application may start recording the media asset and mark all segments for deletion until a first instance of \"ice\nhockey\" is detected.  Specifically, the media guidance application may determine that an ice hockey game between Team USA and Team Canada has started or is about to start.  If, for example, there is a break in showing the game that includes a segment\nwith some kind of Olympic update, the media guidance application may retain that segment to keep continuity of the game.  However, if the USA-Canada ice hockey game ends, a new game begins (e.g., Finland vs.  Sweden), and there is a segment included\nbetween those games, the media guidance application may mark the segment for deletion based on continuity not being needed between games, but rather just within the game.\n In some embodiments, the media guidance application, based on determining that the third segment does not match the first segment, may add a new event indicator to the third segment.  To continue with the example above, if the media guidance\napplication determines that the first segment is associated with one ice hockey game and the third segment with a different ice hockey game, then the media guidance application may add a new event indicator to the metadata of the third segment. \nAdditionally or alternatively, the media guidance application may add a new event indicator to the segment map, described above.\n In some embodiments, it may be desirable to start a new event (e.g., a different ice hockey game) without having to view the beginning of the segment that includes another sport.  Thus, the media guidance application may update the new event\nindicator with a location within the third segment where a first match between the third set of keywords and the fourth set of keywords occurred and delete a portion of the third segment prior to the location.  It should be noted that the media guidance\napplication may mark the portion of the third segment for deletion to be deleted at a later time.  For example, a second ice hockey game may start in the middle of a segment, where the first half of the segment includes skiing.  If the criterion does not\nmatch skiing, the media guidance application may delete or mark for deletion the first half of the segment.\n In some embodiments, the media guidance application may process the media asset and delete segments that have been marked for deletion and create cues in the media asset in those places that have associated new events.  Specifically, the media\nguidance application may determine a first plurality of segments of the media asset that have an associated delete indicator and a second plurality of segments of the media asset that have an associated event indicator.  The media guidance application\nmay, based on the determining, update the media asset, wherein the updating comprises deleting the first plurality of segments and inserting a cue to indicate every new event within the media asset.  For example, when the media asset has been recorded\nand all the segments marked either for deletion or retention (including new event markers), the media guidance application may concatenate the segments to be kept and delete the segments marked for deletion.  The media guidance application may also\ninsert cues into the media asset (e.g., visual indicators and/or audio indicators) to signal new events (e.g., a second ice hockey game).\n In some embodiments, the media guidance application may provide to the user options for use as part of the criterion.  Specifically, when the media guidance application receives the input of the media asset to record, the media guidance\napplication may receive a selection of a media listing associated with the media asset.  The media guidance application may retrieve, from a database that stores associations between entities, a plurality of entities that are related to the media asset\nand generate for display one or more indications corresponding to one or more entities of the plurality of entities that are most closely related to the media asset.  The media guidance application may receive the user selection of one or more entities\nto use in the criterion.\n For example, the media guidance application may have access to a database that includes a set of entities that have been determined to be associated with the media asset.  Those entities may be people, places, actors, characters, sports, videos,\nsongs, etc. The media guidance application may generate for display indications for those entities to be selected by the user.  Specifically, if the media asset is an Olympic broadcast, the media guidance application may generate for display athletes'\nnames participating in the Olympic events, places where the events are taking place, names of sports that will be broadcast, etc.\n In some embodiments, the first segment, the second segment, and the third segment are contiguous.  For example, the media guidance application may split a portion (e.g., the first 15 minutes of an Olympic broadcast) of the media asset into three\nsegments (five minutes each segment).  Those segments are contiguous.  In some embodiments, each segment may be a video frame or a specific number of video frames and associated audio.\n In some embodiments, when generating the fourth set of keywords based on the criterion, the media guidance application may perform the following actions.  The media guidance application may convert the criterion into textual data and parse the\ntextual data into a plurality of terms.  The media guidance application may retrieve, from a database that includes associations between terms and keywords, keywords associated with each term of the plurality of terms and add, to the fourth set of\nkeywords, the keywords associated with each term of the plurality of terms.\n For example, if the criterion is provided in the form of a user selecting items on the screen, the media guidance application may convert those selections into textual data and split the data into terms.  Specifically, if the criterion includes\na user selection of \"ice hockey\" and \"Canada,\" the media guidance application may retrieve from the database associated terms (e.g., stick, puck, Team Canada, etc.) The media guidance application may then add those keywords to the fourth set of keywords.\n In some embodiments, the media guidance application, when determining, based on the comparing, that the first segment and the third segment match the criterion and that the second segment does not match the criterion, may perform the following. \nThe media guidance application may determine that a number of keywords in the first set of keywords that matches keywords in the fourth set of keywords meets a threshold value and determine that a number of keywords in the second set of keywords that\nmatches keywords in the fourth set of keywords does not meet the threshold value.  The media guidance application may make these determinations, for example, based on a percentage of keywords matched or a specific number of keywords that matched.\nBRIEF DESCRIPTION OF THE DRAWINGS\n FIG. 1 shows an illustrative embodiment of a data structure that may be used to store data associated with specific segments and an updated state of that data structure, in accordance with some embodiments of the disclosure;\n FIG. 2 shows an illustrative embodiment of a display screen that may be used to provide media guidance application listings and other media guidance information, in accordance with some embodiments of the disclosure;\n FIG. 3 shows another illustrative embodiment of a display screen that may be used to provide media guidance application listings, in accordance with some embodiments of the disclosure;\n FIG. 4 is a block diagram of an illustrative device in accordance with some embodiments of the disclosure;\n FIG. 5 is a block diagram of an illustrative media system, in accordance with some embodiments of the disclosure;\n FIG. 6 depicts an illustrative process involved in recording portions of a media asset based on a user's criterion, in accordance with some embodiments of this disclosure;\n FIG. 7 depicts an illustrative process involved in generating a set of keywords, in accordance with some embodiments of this disclosure;\n FIG. 8 depicts an illustrative process involved in determining whether a segment matches a criterion entered by a user, in accordance with some embodiments of this disclosure; and\n FIG. 9 depicts an illustrative process involved in removing a delete indicator from metadata associated with a segment, in accordance with some embodiments of this disclosure.\nDESCRIPTION\n Systems and methods are provided herein to record portions of media assets.  Specifically, a media guidance application may be configured to receive user input of a media asset to record, together with a criterion for recording portions of that\nmedia asset.  The criterion in this case corresponds to content within the media asset that the user desires to consume.  However, it would be useful to retain portions of the media asset that do not match the criterion, but are between two portions that\ndo match the criterion.  This would happen when the two portions are part of the same presentation.\n For example, a user may like tennis and want to record a broadcast of the \"U.S.  Open,\" which may last for twelve hours.  However, the user may only want to watch matches with specific athletes (e.g., U.S.  Players).  Thus, the media guidance\napplication may receive as input a selection of the U.S.  Open broadcast and a criterion \"U.S.  Players.\"\n When the media guidance starts recording the broadcast, the media guidance application may split the media asset into segments (e.g., five minutes each).  The media guidance application may execute a content recognition algorithm against each\nsegment to determine whether the segment matches \"U.S.  Players\" (i.e., one of the players participating in the tennis match is a U.S.  Player).  As the match progresses, each segment is stored as a continuous presentation.  However, the tennis match may\nget interrupted by, for example, a news break.  The segment prior to the newsbreak would be labeled as the first segment and the newsbreak as the second segment.\n The news break would not match a \"U.S.  Player.\" Therefore, the news break would be marked for deletion.  However, the news break should be retained in order to keep continuity of the presentation (i.e., the tennis match in progress).  Thus,\nwhen the next segment (the third segment) is determined to match the criterion, the system may determine whether this next segment is part of the same tennis match (i.e., matches the same U.S.  tennis player).  If so, this current segment is not marked\nfor deletion and the prior segment is unmarked for deletion in order to maintain continuity of the tennis match.\n FIG. 1 illustrates how metadata associated with each of a first segment, a second segment, and a third segment may change.  The first, second, and third segments may corresponds to the first, second and third segments described above (i.e., the\nfirst segment is of a tennis match that features a U.S.  player, the second segment is the news break, and the third segment represents the continuation of the match Metadata excerpt 100 illustrates metadata structures 102, 104 and 106 associated with a\nfirst segment, the second segment, and the third segment, respectively.  The metadata structure associated with each segment may include a segment identifier 110 that uniquely identifies the segment within the media asset.  Structure 104 that is\nassociated with the second segment may include the tag for the delete indicator.  This would be the delete indicator added to the metadata as a result of the media guidance application determining that the news break does not match the criterion of \"U.S. Player.\"\n Metadata excerpt 150 illustrates metadata structures 152, 154, and 156 corresponding to the first segment, the second segment and the third segment, respectively.  Excerpt 150 represents metadata structure after process 600 has been executed and\nthe media guidance application has determined that the second segment is to be retained because it is between two segments of the same presentation (e.g., the same tennis match).  Each structure includes identifier 158 that uniquely identifies each\nsegment within the media asset.  It should be noted that the delete indicator has been removed as a result of executing process 900 of FIG. 9).  It should also be noted that segment metadata may include such information as the start time of the segment\nwithin the media asset, the end time of the segment within the media asset, keywords associated with the media asset, or a pointer to a file containing the keywords.\n The amount of content available to users in any given content delivery system can be substantial.  Consequently, many users desire a form of media guidance through an interface that allows users to efficiently navigate content selections and\neasily identify content that they may desire.  An application that provides such guidance is referred to herein as an interactive media guidance application or, sometimes, a media guidance application or a guidance application.\n Interactive media guidance applications may take various forms depending on the content for which they provide guidance.  One typical type of media guidance application is an interactive television program guide.  Interactive television program\nguides (sometimes referred to as electronic program guides) are well-known guidance applications that, among other things, allow users to navigate among and locate many types of content or media assets.  Interactive media guidance applications may\ngenerate graphical user interface screens that enable a user to navigate among, locate and select content.  As referred to herein, the terms \"media asset\" and \"content\" should be understood to mean an electronically consumable user asset, such as\ntelevision programming, as well as pay-per-view programs, on-demand programs (as in video-on-demand (VOD) systems), Internet content (e.g., streaming content, downloadable content, Webcasts, etc.), video clips, audio, content information, pictures,\nrotating images, documents, playlists, websites, articles, books, electronic books, blogs, chat sessions, social media, applications, games, and/or any other media or multimedia and/or combination of the same.  Guidance applications also allow users to\nnavigate among and locate content.  As referred to herein, the term \"multimedia\" should be understood to mean content that utilizes at least two different content forms described above, for example, text, audio, images, video, or interactivity content\nforms.  Content may be recorded, played, displayed or accessed by user equipment devices, but can also be part of a live performance.\n The media guidance application and/or any instructions for performing any of the embodiments discussed herein may be encoded on computer readable media.  Computer readable media includes any media capable of storing data.  The computer readable\nmedia may be transitory, including, but not limited to, propagating electrical or electromagnetic signals, or may be non-transitory including, but not limited to, volatile and non-volatile computer memory or storage devices such as a hard disk, floppy\ndisk, USB drive, DVD, CD, media cards, register memory, processor caches, Random Access Memory (\"RAM\"), etc.\n With the advent of the Internet, mobile computing, and high-speed wireless networks, users are accessing media on user equipment devices on which they traditionally did not.  As referred to herein, the phrase \"user equipment device,\" \"user\nequipment,\" \"user device,\" \"electronic device,\" \"electronic equipment,\" \"media equipment device,\" or \"media device\" should be understood to mean any device for accessing the content described above, such as a television, a Smart TV, a set-top box, an\nintegrated receiver decoder (IRD) for handling satellite television, a digital storage device, a digital media receiver (DMR), a digital media adapter (DMA), a streaming media device, a DVD player, a DVD recorder, a connected DVD, a local media server, a\nBLU-RAY player, a BLU-RAY recorder, a personal computer (PC), a laptop computer, a tablet computer, a WebTV box, a personal computer television (PC/TV), a PC media server, a PC media center, a hand-held computer, a stationary telephone, a personal\ndigital assistant (PDA), a mobile telephone, a portable video player, a portable music player, a portable gaming machine, a smart phone, or any other television equipment, computing equipment, or wireless device, and/or combination of the same.  In some\nembodiments, the user equipment device may have a front facing screen and a rear facing screen, multiple front screens, or multiple angled screens.  In some embodiments, the user equipment device may have a front facing camera and/or a rear facing\ncamera.  On these user equipment devices, users may be able to navigate among and locate the same content available through a television.  Consequently, media guidance may be available on these devices, as well.  The guidance provided may be for content\navailable only through a television, for content available only through one or more of other types of user equipment devices, or for content available both through a television and one or more of the other types of user equipment devices.  The media\nguidance applications may be provided as on-line applications (i.e., provided on a web-site), or as stand-alone applications or clients on user equipment devices.  Various devices and platforms that may implement media guidance applications are described\nin more detail below.\n One of the functions of the media guidance application is to provide media guidance data to users.  As referred to herein, the phrase \"media guidance data\" or \"guidance data\" should be understood to mean any data related to content or data used\nin operating the guidance application.  For example, the guidance data may include program information, guidance application settings, user preferences, user profile information, media listings, media-related information (e.g., broadcast times, broadcast\nchannels, titles, descriptions, ratings information (e.g., parental control ratings, critic's ratings, etc.), genre or category information, actor information, logo data for broadcasters' or providers' logos, etc.), media format (e.g., standard\ndefinition, high definition, 3D, etc.), on-demand information, blogs, websites, and any other type of guidance data that is helpful for a user to navigate among and locate desired content selections.\n FIGS. 2-3 show illustrative display screens that may be used to provide media guidance data.  The display screens shown in FIGS. 2-3 may be implemented on any suitable user equipment device or platform.  While the displays of FIGS. 2-3 are\nillustrated as full screen displays, they may also be fully or partially overlaid over content being displayed.  A user may indicate a desire to access content information by selecting a selectable option provided in a display screen (e.g., a menu\noption, a listings option, an icon, a hyperlink, etc.) or pressing a dedicated button (e.g., a GUIDE button) on a remote control or other user input interface or device.  In response to the user's indication, the media guidance application may provide a\ndisplay screen with media guidance data organized in one of several ways, such as by time and channel in a grid, by time, by channel, by source, by content type, by category (e.g., movies, sports, news, children, or other categories of programming), or\nother predefined, user-defined, or other organization criteria.\n FIG. 2 shows illustrative grid of a program listings display 200 arranged by time and channel that also enables access to different types of content in a single display.  Display 200 may include grid 202 with: (1) a column of channel/content\ntype identifiers 204, where each channel/content type identifier (which is a cell in the column) identifies a different channel or content type available; and (2) a row of time identifiers 206, where each time identifier (which is a cell in the row)\nidentifies a time block of programming.  Grid 202 also includes cells of program listings, such as program listing 208, where each listing provides the title of the program provided on the listing's associated channel and time.  With a user input device,\na user can select program listings by moving highlight region 210.  Information relating to the program listing selected by highlight region 210 may be provided in program information region 212.  Region 212 may include, for example, the program title,\nthe program description, the time the program is provided (if applicable), the channel the program is on (if applicable), the program's rating, and other desired information.\n In addition to providing access to linear programming (e.g., content that is scheduled to be transmitted to a plurality of user equipment devices at a predetermined time and is provided according to a schedule), the media guidance application\nalso provides access to non-linear programming (e.g., content accessible to a user equipment device at any time and is not provided according to a schedule).  Non-linear programming may include content from different content sources including on-demand\ncontent (e.g., VOD), Internet content (e.g., streaming media, downloadable media, etc.), locally stored content (e.g., content stored on any user equipment device described above or other storage device), or other time-independent content.  On-demand\ncontent may include movies or any other content provided by a particular content provider (e.g., HBO On Demand providing \"The Sopranos\" and \"Curb Your Enthusiasm\").  HBO ON DEMAND is a service mark owned by Time Warner Company L.P.  et al. and THE\nSOPRANOS and CURB YOUR ENTHUSIASM are trademarks owned by the Home Box Office, Inc.  Internet content may include web events, such as a chat session or Webcast, or content available on-demand as streaming content or downloadable content through an\nInternet web site or other Internet access (e.g. FTP).\n Grid 202 may provide media guidance data for non-linear programming including on-demand listing 214, recorded content listing 216, and Internet content listing 218.  A display combining media guidance data for content from different types of\ncontent sources is sometimes referred to as a \"mixed-media\" display.  Various permutations of the types of media guidance data that may be displayed that are different than display 200 may be based on user selection or guidance application definition\n(e.g., a display of only recorded and broadcast listings, only on-demand and broadcast listings, etc.).  As illustrated, listings 214, 216, and 218 are shown as spanning the entire time block displayed in grid 202 to indicate that selection of these\nlistings may provide access to a display dedicated to on-demand listings, recorded listings, or Internet listings, respectively.  In some embodiments, listings for these content types may be included directly in grid 202.  Additional media guidance data\nmay be displayed in response to the user selecting one of the navigational icons 220.  (Pressing an arrow key on a user input device may affect the display in a similar manner as selecting navigational icons 220.)\n Display 200 may also include video region 222, and options region 226.  Video region 222 may allow the user to view and/or preview programs that are currently available, will be available, or were available to the user.  The content of video\nregion 222 may correspond to, or be independent from, one of the listings displayed in grid 202.  Grid displays including a video region are sometimes referred to as picture-in-guide (PIG) displays.  PIG displays and their functionalities are described\nin greater detail in Satterfield et al. U.S.  Pat.  No. 6,564,378, issued May 13, 2003 and Yuen et al. U.S.  Pat.  No. 6,239,794, issued May 29, 2001, which are hereby incorporated by reference herein in their entireties.  PIG displays may be included in\nother media guidance application display screens of the embodiments described herein.\n Options region 226 may allow the user to access different types of content, media guidance application displays, and/or media guidance application features.  Options region 226 may be part of display 200 (and other display screens described\nherein), or may be invoked by a user by selecting an on-screen option or pressing a dedicated or assignable button on a user input device.  The selectable options within options region 226 may concern features related to program listings in grid 202 or\nmay include options available from a main menu display.  Features related to program listings may include searching for other air times or ways of receiving a program, recording a program, enabling series recording of a program, setting program and/or\nchannel as a favorite, purchasing a program, or other features.  Options available from a main menu display may include search options, VOD options, parental control options, Internet options, cloud-based options, device synchronization options, second\nscreen device options, options to access various types of media guidance data displays, options to subscribe to a premium service, options to edit a user's profile, options to access a browse overlay, or other options.\n The media guidance application may be personalized based on a user's preferences.  A personalized media guidance application allows a user to customize displays and features to create a personalized \"experience\" with the media guidance\napplication.  This personalized experience may be created by allowing a user to input these customizations and/or by the media guidance application monitoring user activity to determine various user preferences.  Users may access their personalized\nguidance application by logging in or otherwise identifying themselves to the guidance application.  Customization of the media guidance application may be made in accordance with a user profile.  The customizations may include varying presentation\nschemes (e.g., color scheme of displays, font size of text, etc.), aspects of content listings displayed (e.g., only HDTV or only 3D programming, user-specified broadcast channels based on favorite channel selections, re-ordering the display of channels,\nrecommended content, etc.), desired recording features (e.g., recording or series recordings for particular users, recording quality, etc.), parental control settings, customized presentation of Internet content (e.g., presentation of social media\ncontent, e-mail, electronically delivered articles, etc.) and other desired customizations.\n The media guidance application may allow a user to provide user profile information or may automatically compile user profile information.  The media guidance application may, for example, monitor the content the user accesses and/or other\ninteractions the user may have with the guidance application.  Additionally, the media guidance application may obtain all or part of other user profiles that are related to a particular user (e.g., from other web sites on the Internet the user accesses,\nsuch as www.allrovi.com, from other media guidance applications the user accesses, from other interactive applications the user accesses, from another user equipment device of the user, etc.), and/or obtain information about the user from other sources\nthat the media guidance application may access.  As a result, a user can be provided with a unified guidance application experience across the user's different user equipment devices.  This type of user experience is described in greater detail below in\nconnection with FIG. 5.  Additional personalized media guidance application features are described in greater detail in Ellis et al., U.S.  Patent Application Publication No. 2005/0251827, filed Jul.  11, 2005, Boyer et al., U.S.  Pat.  No. 7,165,098,\nissued Jan.  16, 2007, and Ellis et al., U.S.  Patent Application Publication No. 2002/0174430, filed Feb.  21, 2002, which are hereby incorporated by reference herein in their entireties.\n Another display arrangement for providing media guidance is shown in FIG. 3.  Video mosaic display 300 includes selectable options 302 for content information organized based on content type, genre, and/or other organization criteria.  In\ndisplay 300, television listings option 304 is selected, thus providing listings 306, 308, 310, and 312 as broadcast program listings.  In display 300 the listings may provide graphical images including cover art, still images from the content, video\nclip previews, live video from the content, or other types of content that indicate to a user the content being described by the media guidance data in the listing.  Each of the graphical listings may also be accompanied by text to provide further\ninformation about the content associated with the listing.  For example, listing 308 may include more than one portion, including media portion 314 and text portion 316.  Media portion 314 and/or text portion 316 may be selectable to view content in\nfull-screen or to view information related to the content displayed in media portion 314 (e.g., to view listings for the channel that the video is displayed on).\n The listings in display 300 are of different sizes (i.e., listing 306 is larger than listings 308, 310, and 312), but if desired, all the listings may be the same size.  Listings may be of different sizes or graphically accentuated to indicate\ndegrees of interest to the user or to emphasize certain content, as desired by the content provider or based on user preferences.  Various systems and methods for graphically accentuating content listings are discussed in, for example, Yates, U.S. \nPatent Application Publication No. 2010/0153885, filed Nov.  12, 2009, which is hereby incorporated by reference herein in its entirety.\n Users may access content and the media guidance application (and its display screens described above and below) from one or more of their user equipment devices.  FIG. 4 shows a generalized embodiment of illustrative user equipment device 400. \nMore specific implementations of user equipment devices are discussed below in connection with FIG. 5.  User equipment device 400 may receive content and data via input/output (hereinafter \"I/O\") path 402.  I/O path 402 may provide content (e.g.,\nbroadcast programming, on-demand programming, Internet content, content available over a local area network (LAN) or wide area network (WAN), and/or other content) and data to control circuitry 404, which includes processing circuitry 406 and storage\n408.  Control circuitry 404 may be used to send and receive commands, requests, and other suitable data using I/O path 402.  I/O path 402 may connect control circuitry 404 (and specifically processing circuitry 406) to one or more communications paths\n(described below).  I/O functions may be provided by one or more of these communications paths, but are shown as a single path in FIG. 4 to avoid overcomplicating the drawing.\n Control circuitry 404 may be based on any suitable processing circuitry such as processing circuitry 406.  As referred to herein, processing circuitry should be understood to mean circuitry based on one or more microprocessors, microcontrollers,\ndigital signal processors, programmable logic devices, field-programmable gate arrays (FPGAs), application-specific integrated circuits (ASICs), etc., and may include a multi-core processor (e.g., dual-core, quad-core, hexa-core, or any suitable number\nof cores) or supercomputer.  In some embodiments, processing circuitry may be distributed across multiple separate processors or processing units, for example, multiple of the same type of processing units (e.g., two Intel Core i7 processors) or multiple\ndifferent processors (e.g., an Intel Core i5 processor and an Intel Core i7 processor).  In some embodiments, control circuitry 404 executes instructions for a media guidance application stored in memory (i.e., storage 408).  Specifically, control\ncircuitry 404 may be instructed by the media guidance application to perform the functions discussed above and below.  For example, the media guidance application may provide instructions to control circuitry 404 to generate the media guidance displays. \nIn some implementations, any action performed by control circuitry 404 may be based on instructions received from the media guidance application.\n In client-server based embodiments, control circuitry 404 may include communications circuitry suitable for communicating with a guidance application server or other networks or servers.  The instructions for carrying out the above mentioned\nfunctionality may be stored on the guidance application server.  Communications circuitry may include a cable modem, an integrated services digital network (ISDN) modem, a digital subscriber line (DSL) modem, a telephone modem, Ethernet card, or a\nwireless modem for communications with other equipment, or any other suitable communications circuitry.  Such communications may involve the Internet or any other suitable communications networks or paths (which is described in more detail in connection\nwith FIG. 5).  In addition, communications circuitry may include circuitry that enables peer-to-peer communication of user equipment devices, or communication of user equipment devices in locations remote from each other (described in more detail below).\n Memory may be an electronic storage device provided as storage 408 that is part of control circuitry 404.  As referred to herein, the phrase \"electronic storage device\" or \"storage device\" should be understood to mean any device for storing\nelectronic data, computer software, or firmware, such as random-access memory, read-only memory, hard drives, optical drives, digital video disc (DVD) recorders, compact disc (CD) recorders, BLU-RAY disc (BD) recorders, BLU-RAY 3D disc recorders, digital\nvideo recorders (DVR, sometimes called a personal video recorder, or PVR), solid state devices, quantum storage devices, gaming consoles, gaming media, or any other suitable fixed or removable storage devices, and/or any combination of the same.  Storage\n408 may be used to store various types of content described herein as well as media guidance data described above.  Nonvolatile memory may also be used (e.g., to launch a boot-up routine and other instructions).  Cloud-based storage, described in\nrelation to FIG. 5, may be used to supplement storage 408 or instead of storage 408.\n Control circuitry 404 may include video generating circuitry and tuning circuitry, such as one or more analog tuners, one or more MPEG-2 decoders or other digital decoding circuitry, high-definition tuners, or any other suitable tuning or video\ncircuits or combinations of such circuits.  Encoding circuitry (e.g., for converting over-the-air, analog, or digital signals to MPEG signals for storage) may also be provided.  Control circuitry 404 may also include scaler circuitry for upconverting and\ndownconverting content into the preferred output format of the user equipment 400.  Circuitry 404 may also include digital-to-analog converter circuitry and analog-to-digital converter circuitry for converting between digital and analog signals.  The\ntuning and encoding circuitry may be used by the user equipment device to receive and to display, to play, or to record content.  The tuning and encoding circuitry may also be used to receive guidance data.  The circuitry described herein, including for\nexample, the tuning, video generating, encoding, decoding, encrypting, decrypting, scaler, and analog/digital circuitry, may be implemented using software running on one or more general purpose or specialized processors.  Multiple tuners may be provided\nto handle simultaneous tuning functions (e.g., watch and record functions, picture-in-picture (PIP) functions, multiple-tuner recording, etc.).  If storage 408 is provided as a separate device from user equipment 400, the tuning and encoding circuitry\n(including multiple tuners) may be associated with storage 408.\n A user may send instructions to control circuitry 404 using user input interface 410.  User input interface 410 may be any suitable user interface, such as a remote control, mouse, trackball, keypad, keyboard, touch screen, touchpad, stylus\ninput, joystick, voice recognition interface, or other user input interfaces.  Display 412 may be provided as a stand-alone device or integrated with other elements of user equipment device 400.  For example, display 412 may be a touchscreen or\ntouch-sensitive display.  In such circumstances, user input interface 410 may be integrated with or combined with display 412.  Display 412 may be one or more of a monitor, a television, a liquid crystal display (LCD) for a mobile device, amorphous\nsilicon display, low temperature poly silicon display, electronic ink display, electrophoretic display, active matrix display, electro-wetting display, electrofluidic display, cathode ray tube display, light-emitting diode display, electroluminescent\ndisplay, plasma display panel, high-performance addressing display, thin-film transistor display, organic light-emitting diode display, surface-conduction electron-emitter display (SED), laser television, carbon nanotubes, quantum dot display,\ninterferometric modulator display, or any other suitable equipment for displaying visual images.  In some embodiments, display 412 may be HDTV-capable.  In some embodiments, display 412 may be a 3D display, and the interactive media guidance application\nand any suitable content may be displayed in 3D.  A video card or graphics card may generate the output to the display 412.  The video card may offer various functions such as accelerated rendering of 3D scenes and 2D graphics, MPEG-2/MPEG-4 decoding, TV\noutput, or the ability to connect multiple monitors.  The video card may be any processing circuitry described above in relation to control circuitry 404.  The video card may be integrated with the control circuitry 404.  Speakers 414 may be provided as\nintegrated with other elements of user equipment device 400 or may be stand-alone units.  The audio component of videos and other content displayed on display 412 may be played through speakers 414.  In some embodiments, the audio may be distributed to a\nreceiver (not shown), which processes and outputs the audio via speakers 414.\n The guidance application may be implemented using any suitable architecture.  For example, it may be a stand-alone application wholly-implemented on user equipment device 400.  In such an approach, instructions of the application are stored\nlocally (e.g., in storage 408), and data for use by the application is downloaded on a periodic basis (e.g., from an out-of-band feed, from an Internet resource, or using another suitable approach).  Control circuitry 404 may retrieve instructions of the\napplication from storage 408 and process the instructions to generate any of the displays discussed herein.  Based on the processed instructions, control circuitry 404 may determine what action to perform when input is received from input interface 410. \nFor example, movement of a cursor on a display up/down may be indicated by the processed instructions when input interface 410 indicates that an up/down button was selected.\n In some embodiments, the media guidance application is a client-server based application.  Data for use by a thick or thin client implemented on user equipment device 400 is retrieved on-demand by issuing requests to a server remote to the user\nequipment device 400.  In one example of a client-server based guidance application, control circuitry 404 runs a web browser that interprets web pages provided by a remote server.  For example, the remote server may store the instructions for the\napplication in a storage device.  The remote server may process the stored instructions using circuitry (e.g., control circuitry 404) and generate the displays discussed above and below.  The client device may receive the displays generated by the remote\nserver and may display the content of the displays locally on equipment device 400.  This way, the processing of the instructions is performed remotely by the server while the resulting displays are provided locally on equipment device 400.  Equipment\ndevice 400 may receive inputs from the user via input interface 410 and transmit those inputs to the remote server for processing and generating the corresponding displays.  For example, equipment device 400 may transmit a communication to the remote\nserver indicating that an up/down button was selected via input interface 410.  The remote server may process instructions in accordance with that input and generate a display of the application corresponding to the input (e.g., a display that moves a\ncursor up/down).  The generated display is then transmitted to equipment device 400 for presentation to the user.\n In some embodiments, the media guidance application is downloaded and interpreted or otherwise run by an interpreter or virtual machine (run by control circuitry 404).  In some embodiments, the guidance application may be encoded in the ETV\nBinary Interchange Format (EBIF), received by control circuitry 404 as part of a suitable feed, and interpreted by a user agent running on control circuitry 404.  For example, the guidance application may be an EBIF application.  In some embodiments, the\nguidance application may be defined by a series of JAVA-based files that are received and run by a local virtual machine or other suitable middleware executed by control circuitry 404.  In some of such embodiments (e.g., those employing MPEG-2 or other\ndigital media encoding schemes), the guidance application may be, for example, encoded and transmitted in an MPEG-2 object carousel with the MPEG audio and video packets of a program.\n User equipment device 400 of FIG. 4 can be implemented in system 500 of FIG. 5 as user television equipment 502, user computer equipment 504, wireless user communications device 506, or any other type of user equipment suitable for accessing\ncontent, such as a non-portable gaming machine.  For simplicity, these devices may be referred to herein collectively as user equipment or user equipment devices, and may be substantially similar to user equipment devices described above.  User equipment\ndevices, on which a media guidance application may be implemented, may function as a standalone device or may be part of a network of devices.  Various network configurations of devices may be implemented and are discussed in more detail below.\n A user equipment device utilizing at least some of the system features described above in connection with FIG. 4 may not be classified solely as user television equipment 502, user computer equipment 504, or a wireless user communications device\n506.  For example, user television equipment 502 may, like some user computer equipment 504, be Internet-enabled allowing for access to Internet content, while user computer equipment 504 may, like some television equipment 502, include a tuner allowing\nfor access to television programming.  The media guidance application may have the same layout on various different types of user equipment or may be tailored to the display capabilities of the user equipment.  For example, on user computer equipment\n504, the guidance application may be provided as a web site accessed by a web browser.  In another example, the guidance application may be scaled down for wireless user communications devices 506.\n In system 500, there is typically more than one of each type of user equipment device but only one of each is shown in FIG. 5 to avoid overcomplicating the drawing.  In addition, each user may utilize more than one type of user equipment device\nand also more than one of each type of user equipment device.\n In some embodiments, a user equipment device (e.g., user television equipment 502, user computer equipment 504, wireless user communications device 506) may be referred to as a \"second screen device.\" For example, a second screen device may\nsupplement content presented on a first user equipment device.  The content presented on the second screen device may be any suitable content that supplements the content presented on the first device.  In some embodiments, the second screen device\nprovides an interface for adjusting settings and display preferences of the first device.  In some embodiments, the second screen device is configured for interacting with other second screen devices or for interacting with a social network.  The second\nscreen device can be located in the same room as the first device, a different room from the first device but in the same house or building, or in a different building from the first device.\n The user may also set various settings to maintain consistent media guidance application settings across in-home devices and remote devices.  Settings include those described herein, as well as channel and program favorites, programming\npreferences that the guidance application utilizes to make programming recommendations, display preferences, and other desirable guidance settings.  For example, if a user sets a channel as a favorite on, for example, the web site www.allrovi.com on\ntheir personal computer at their office, the same channel would appear as a favorite on the user's in-home devices (e.g., user television equipment and user computer equipment) as well as the user's mobile devices, if desired.  Therefore, changes made on\none user equipment device can change the guidance experience on another user equipment device, regardless of whether they are the same or a different type of user equipment device.  In addition, the changes made may be based on settings input by a user,\nas well as user activity monitored by the guidance application.\n The user equipment devices may be coupled to communications network 514.  Namely, user television equipment 502, user computer equipment 504, and wireless user communications device 506 are coupled to communications network 514 via\ncommunications paths 508, 510, and 512, respectively.  Communications network 514 may be one or more networks including the Internet, a mobile phone network, mobile voice or data network (e.g., a 4G or LTE network), cable network, public switched\ntelephone network, or other types of communications network or combinations of communications networks.  Paths 508, 510, and 512 may separately or together include one or more communications paths, such as, a satellite path, a fiber-optic path, a cable\npath, a path that supports Internet communications (e.g., IPTV), free-space connections (e.g., for broadcast or other wireless signals), or any other suitable wired or wireless communications path or combination of such paths.  Path 512 is drawn with\ndotted lines to indicate that in the exemplary embodiment shown in FIG. 5 it is a wireless path and paths 508 and 510 are drawn as solid lines to indicate they are wired paths (although these paths may be wireless paths, if desired).  Communications with\nthe user equipment devices may be provided by one or more of these communications paths, but are shown as a single path in FIG. 5 to avoid overcomplicating the drawing.\n Although communications paths are not drawn between user equipment devices, these devices may communicate directly with each other via communication paths, such as those described above in connection with paths 508, 510, and 512, as well as\nother short-range point-to-point communication paths, such as USB cables, IEEE 1394 cables, wireless paths (e.g., Bluetooth, infrared, IEEE 802-11x, etc.), or other short-range communication via wired or wireless paths.  BLUETOOTH is a certification mark\nowned by Bluetooth SIG, INC.  The user equipment devices may also communicate with each other directly through an indirect path via communications network 514.\n System 500 includes content source 516 and media guidance data source 518 coupled to communications network 514 via communication paths 520 and 522, respectively.  Paths 520 and 522 may include any of the communication paths described above in\nconnection with paths 508, 510, and 512.  Communications with the content source 516 and media guidance data source 518 may be exchanged over one or more communications paths, but are shown as a single path in FIG. 5 to avoid overcomplicating the\ndrawing.  In addition, there may be more than one of each of content source 516 and media guidance data source 518, but only one of each is shown in FIG. 5 to avoid overcomplicating the drawing.  (The different types of each of these sources are\ndiscussed below.) If desired, content source 516 and media guidance data source 518 may be integrated as one source device.  Although communications between sources 516 and 518 with user equipment devices 502, 504, and 506 are shown as through\ncommunications network 514, in some embodiments, sources 516 and 518 may communicate directly with user equipment devices 502, 504, and 506 via communication paths (not shown) such as those described above in connection with paths 508, 510, and 512.\n Content source 516 may include one or more types of content distribution equipment including a television distribution facility, cable system headend, satellite distribution facility, programming sources (e.g., television broadcasters, such as\nNBC, ABC, HBO, etc.), intermediate distribution facilities and/or servers, Internet providers, on-demand media servers, and other content providers.  NBC is a trademark owned by the National Broadcasting Company, Inc., ABC is a trademark owned by the\nAmerican Broadcasting Company, Inc., and HBO is a trademark owned by the Home Box Office, Inc.  Content source 516 may be the originator of content (e.g., a television broadcaster, a Webcast provider, etc.) or may not be the originator of content (e.g.,\nan on-demand content provider, an Internet provider of content of broadcast programs for downloading, etc.).  Content source 516 may include cable sources, satellite providers, on-demand providers, Internet providers, over-the-top content providers, or\nother providers of content.  Content source 516 may also include a remote media server used to store different types of content (including video content selected by a user), in a location remote from any of the user equipment devices.  Systems and\nmethods for remote storage of content, and providing remotely stored content to user equipment are discussed in greater detail in connection with Ellis et al., U.S.  Pat.  No. 7,761,892, issued Jul.  20, 2010, which is hereby incorporated by reference\nherein in its entirety.\n Media guidance data source 518 may provide media guidance data, such as the media guidance data described above.  Media guidance data may be provided to the user equipment devices using any suitable approach.  In some embodiments, the guidance\napplication may be a stand-alone interactive television program guide that receives program guide data via a data feed (e.g., a continuous feed or trickle feed).  Program schedule data and other guidance data may be provided to the user equipment on a\ntelevision channel sideband, using an in-band digital signal, using an out-of-band digital signal, or by any other suitable data transmission technique.  Program schedule data and other media guidance data may be provided to user equipment on multiple\nanalog or digital television channels.\n In some embodiments, guidance data from media guidance data source 518 may be provided to users' equipment using a client-server approach.  For example, a user equipment device may pull media guidance data from a server, or a server may push\nmedia guidance data to a user equipment device.  In some embodiments, a guidance application client residing on the users' equipment may initiate sessions with source 518 to obtain guidance data when needed, e.g., when the guidance data is out of date or\nwhen the user equipment device receives a request from the user to receive data.  Media guidance may be provided to the user equipment with any suitable frequency (e.g., continuously, daily, a user-specified period of time, a system-specified period of\ntime, in response to a request from user equipment, etc.).  Media guidance data source 518 may provide user equipment devices 502, 504, and 506 the media guidance application itself or software updates for the media guidance application.\n In some embodiments, the media guidance data may include viewer data.  For example, the viewer data may include current and/or historical user activity information (e.g., what content the user typically watches, what times of day the user\nwatches content, whether the user interacts with a social network, at what times the user interacts with a social network to post information, what types of content the user typically watches (e.g., pay TV or free TV), mood, brain activity information,\netc.).  The media guidance data may also include subscription data.  For example, the subscription data may identify to which sources or services a given user subscribes and/or to which sources or services the given user has previously subscribed but\nlater terminated access (e.g., whether the user subscribes to premium channels, whether the user has added a premium level of services, whether the user has increased Internet speed).  In some embodiments, the viewer data and/or the subscription data may\nidentify patterns of a given user for a period of more than one year.  The media guidance data may include a model (e.g., a survivor model) used for generating a score that indicates a likelihood a given user will terminate access to a service/source. \nFor example, the media guidance application may process the viewer data with the subscription data using the model to generate a value or score that indicates a likelihood of whether the given user will terminate access to a particular service or source. In particular, a higher score may indicate a higher level of confidence that the user will terminate access to a particular service or source.  Based on the score, the media guidance application may generate promotions that entice the user to keep the\nparticular service or source indicated by the score as one to which the user will likely terminate access.\n Media guidance applications may be, for example, stand-alone applications implemented on user equipment devices.  For example, the media guidance application may be implemented as software or a set of executable instructions which may be stored\nin storage 408, and executed by control circuitry 404 of a user equipment device 400.  In some embodiments, media guidance applications may be client-server applications where only a client application resides on the user equipment device, and server\napplication resides on a remote server.  For example, media guidance applications may be implemented partially as a client application on control circuitry 404 of user equipment device 400 and partially on a remote server as a server application (e.g.,\nmedia guidance data source 518) running on control circuitry of the remote server.  When executed by control circuitry of the remote server (such as media guidance data source 518), the media guidance application may instruct the control circuitry to\ngenerate the guidance application displays and transmit the generated displays to the user equipment devices.  The server application may instruct the control circuitry of the media guidance data source 518 to transmit data for storage on the user\nequipment.  The client application may instruct control circuitry of the receiving user equipment to generate the guidance application displays.\n Content and/or media guidance data delivered to user equipment devices 502, 504, and 506 may be over-the-top (OTT) content.  OTT content delivery allows Internet-enabled user devices, including any user equipment device described above, to\nreceive content that is transferred over the Internet, including any content described above, in addition to content received over cable or satellite connections.  OTT content is delivered via an Internet connection provided by an Internet service\nprovider (ISP), but a third party distributes the content.  The ISP may not be responsible for the viewing abilities, copyrights, or redistribution of the content, and may only transfer IP packets provided by the OTT content provider.  Examples of OTT\ncontent providers include YOUTUBE, NETFLIX, and HULU, which provide audio and video via IP packets.  Youtube is a trademark owned by Google Inc., Netflix is a trademark owned by Netflix Inc., and Hulu is a trademark owned by Hulu, LLC.  OTT content\nproviders may additionally or alternatively provide media guidance data described above.  In addition to content and/or media guidance data, providers of OTT content can distribute media guidance applications (e.g., web-based applications or cloud-based\napplications), or the content can be displayed by media guidance applications stored on the user equipment device.\n Media guidance system 500 is intended to illustrate a number of approaches, or network configurations, by which user equipment devices and sources of content and guidance data may communicate with each other for the purpose of accessing content\nand providing media guidance.  The embodiments described herein may be applied in any one or a subset of these approaches, or in a system employing other approaches for delivering content and providing media guidance.  The following four approaches\nprovide specific illustrations of the generalized example of FIG. 5.\n In one approach, user equipment devices may communicate with each other within a home network.  User equipment devices can communicate with each other directly via short-range point-to-point communication schemes described above, via indirect\npaths through a hub or other similar device provided on a home network, or via communications network 514.  Each of the multiple individuals in a single home may operate different user equipment devices on the home network.  As a result, it may be\ndesirable for various media guidance information or settings to be communicated between the different user equipment devices.  For example, it may be desirable for users to maintain consistent media guidance application settings on different user\nequipment devices within a home network, as described in greater detail in Ellis et al., U.S.  Patent Publication No. 2005/0251827, filed Jul.  11, 2005.  Different types of user equipment devices in a home network may also communicate with each other to\ntransmit content.  For example, a user may transmit content from user computer equipment to a portable video player or portable music player.\n In a second approach, users may have multiple types of user equipment by which they access content and obtain media guidance.  For example, some users may have home networks that are accessed by in-home and mobile devices.  Users may control\nin-home devices via a media guidance application implemented on a remote device.  For example, users may access an online media guidance application on a website via a personal computer at their office, or a mobile device such as a PDA or web-enabled\nmobile telephone.  The user may set various settings (e.g., recordings, reminders, or other settings) on the online guidance application to control the user's in-home equipment.  The online guide may control the user's equipment directly, or by\ncommunicating with a media guidance application on the user's in-home equipment.  Various systems and methods for user equipment devices communicating, where the user equipment devices are in locations remote from each other, is discussed in, for\nexample, Ellis et al., U.S.  Pat.  No. 8,046,801, issued Oct.  25, 2011, which is hereby incorporated by reference herein in its entirety.\n In a third approach, users of user equipment devices inside and outside a home can use their media guidance application to communicate directly with content source 516 to access content.  Specifically, within a home, users of user television\nequipment 502 and user computer equipment 504 may access the media guidance application to navigate among and locate desirable content.  Users may also access the media guidance application outside of the home using wireless user communications devices\n506 to navigate among and locate desirable content.\n In a fourth approach, user equipment devices may operate in a cloud computing environment to access cloud services.  In a cloud computing environment, various types of computing services for content sharing, storage or distribution (e.g., video\nsharing sites or social networking sites) are provided by a collection of network-accessible computing and storage resources, referred to as \"the cloud.\" For example, the cloud can include a collection of server computing devices, which may be located\ncentrally or at distributed locations, that provide cloud-based services to various types of users and devices connected via a network such as the Internet via communications network 514.  These cloud resources may include one or more content sources 416\nand one or more media guidance data sources 518.  In addition or in the alternative, the remote computing sites may include other user equipment devices, such as user television equipment 502, user computer equipment 504, and wireless user communications\ndevice 506.  For example, the other user equipment devices may provide access to a stored copy of a video or a streamed video.  In such embodiments, user equipment devices may operate in a peer-to-peer manner without communicating with a central server.\n The cloud provides access to services, such as content storage, content sharing, or social networking services, among other examples, as well as access to any content described above, for user equipment devices.  Services can be provided in the\ncloud through cloud computing service providers, or through other providers of online services.  For example, the cloud-based services can include a content storage service, a content sharing site, a social networking site, or other services via which\nuser-sourced content is distributed for viewing by others on connected devices.  These cloud-based services may allow a user equipment device to store content to the cloud and to receive content from the cloud rather than storing content locally and\naccessing locally-stored content.\n A user may use various content capture devices, such as camcorders, digital cameras with video mode, audio recorders, mobile phones, and handheld computing devices, to record content.  The user can upload content to a content storage service on\nthe cloud either directly, for example, from user computer equipment 504 or wireless user communications device 506 having content capture feature.  Alternatively, the user can first transfer the content to a user equipment device, such as user computer\nequipment 504.  The user equipment device storing the content uploads the content to the cloud using a data transmission service on communications network 514.  In some embodiments, the user equipment device itself is a cloud resource, and other user\nequipment devices can access the content directly from the user equipment device on which the user stored the content.\n Cloud resources may be accessed by a user equipment device using, for example, a web browser, a media guidance application, a desktop application, a mobile application, and/or any combination of access applications of the same.  The user\nequipment device may be a cloud client that relies on cloud computing for application delivery, or the user equipment device may have some functionality without access to cloud resources.  For example, some applications running on the user equipment\ndevice may be cloud applications, i.e., applications delivered as a service over the Internet, while other applications may be stored and run on the user equipment device.  In some embodiments, a user device may receive content from multiple cloud\nresources simultaneously.  For example, a user device can stream audio from one cloud resource while downloading content from a second cloud resource.  Or a user device can download content from multiple cloud resources for more efficient downloading. \nIn some embodiments, user equipment devices can use cloud resources for processing operations such as the processing operations performed by processing circuitry described in relation to FIG. 4.\n As referred herein, the term \"in response to\" refers to initiated as a result of.  For example, a first action being performed in response to a second action may include interstitial steps between the first action and the second action.  As\nreferred herein, the term \"directly in response to\" refers to caused by.  For example, a first action being performed directly in response to a second action may not include interstitial steps between the first action and the second action.\n A media guidance application may be configured to record portions of media assets based on a user's criteria.  For example, the media guidance application may perform (e.g., via control circuitry 404) process 600 of FIG. 6 to record portions of\nmedia assets based on a user's criteria.\n At step 602, the media guidance application receives (e.g., via control circuitry 404 from user input interface 410) input from the user including a user selection of a media asset and a criterion for retaining portions of the media asset.  For\nexample, the media guidance application may receive user input while display 200 (FIG. 2) or display 300 (FIG. 3) are displayed to the user.  The media guidance application may receive a user selection of media listing 208 (FIG. 2) or media listing 308\n(FIG. 3).  The user selection may be received via a remote control or another user input device (e.g., keyboard, mouse, touch screen interface, etc.).\n The media guidance application may receive the criterion as text input from the user.  For example, the media guidance application may generate for display an input area where the user is able to enter text.  User input may be received via a\nremote control, keyboard, touch screen interface, a voice interface (e.g., a microphone coupled with voice recognition algorithm), or any other suitable device.\n In some embodiments, the media guidance application, when receiving input from the user that includes a user selection of the media asset and a criterion for retaining portions of the media asset may perform the following actions.  The media\nguidance application may receive a selection of a media listing associated with the media asset.  For example, the media guidance application may receive the selection of a media listing while display 200 (FIG. 2) or display 300 (FIG. 3) is displayed to\nthe user.  The user selection may be received through user input interface 410.\n In some embodiments, the media guidance application may be able to take user input and the criterion in the form of a natural language query.  For example, a user may provide a voice query \"Record all Olympic ice hockey games.\" In response, the\nmedia guidance application may convert the query into input to record all Olympic broadcasts and use \"ice hockey\" or \"ice hockey games\" as the criterion.\n The media guidance application may retrieve, from a database that stores associations between entities, a plurality of entities that are related to the media asset.  The database may further include information describing relationships between\nthe entities.  Each entity may be stored as a node and the database may include a semantic network or knowledge graph of these nodes, where the semantic network maintains relationships between certain nodes.  These relationships may be stored as\npercentage values of how related the entities are.  Knowledge graphs and their features are described in greater detail in U.S.  patent application Ser.  No. 14/501,504, filed Sep. 30, 2014, U.S.  patent application Ser.  No. 14/500,309, filed Sep. 29,\n2014, and U.S.  patent application Ser.  No. 14/448,308, filed Jul.  31, 2014, which are hereby incorporated by reference herein in their entireties.  For example, the media guidance application may have access to a database that stores entities related\nto media assets.  Specifically, a media asset such as an Olympic broadcast may have various entities related to it.  If the Olympic broadcast is scheduled to include a number of sports (e.g., ice hockey, skiing, snowboarding, etc.), the database may\ninclude those sports as entities.  Additionally or alternatively, specific athletes participating in the sports included in the Olympic broadcast may be included as entities in the database associated with the broadcast.\n The database may be located at media content source 516 or at media guidance data source 518.  Alternatively, the database may have portions at both media content source 516 and media guidance data source 518.  The media guidance application may\naccess the database through communications network 514 via I/O path 402 (FIG. 4).  The media guidance application may store the received entities in storage 408.\n The media guidance application may generate for display one or more indications corresponding to one or more entities of the plurality of entities that are most closely related to the media asset.  For example, the media guidance may retrieve\nthe received indications from storage 408.  The retrieved entities may be stored in a data structure (e.g., an array).  The media guidance application may determine how many entities have been received from the database.  The media guidance application\nmay retrieve (e.g., from storage 408) a threshold value for the number of indications associated with entities that are to be displayed.  If the number of entities is greater than the threshold value, the media guidance application may select those\nentities that are most closely related with the media asset.  For example, the media asset may have relationship strength value for each entity that it is related to.  The media guidance application may compare those values in order to determine which\nentities should have associated indications generated for display.  For example, if the threshold value is ten, then the media guidance application may generate for display indications of ten entities that have the highest relationship strength values\n(i.e., the ten most closely related entities).\n Those entities may be people, places, actors, characters, sports, videos, songs, etc. For example, athletes participating in the sports being broadcast may be entities.  In another example, the different sports may be located in different places\n(e.g., skiing in the mountains and ice skating at a rink).  The media guidance application may provide an option to a user to record segments that include sporting events occurring in the mountains and/or at a skating rink.  Videos may include videos of\nathletes describing their bios, etc. The media guidance application may receive a user selection of one or more of those entities to use in the criterion.\n At step 604, the media guidance application generates (e.g., via control circuitry 404) a first set of keywords for a first segment of the media asset, a second set of keywords for a second segment of the media asset, and a third set of keywords\nfor a third segment of the media asset by executing a content recognition algorithm on the first segment, the second segment, and the third segment, wherein the first segment within the media asset precedes the second segment and the second segment\nprecedes the third segment.  For example, the media guidance application may receive (e.g., from media content source 516 and/or media guidance data source 518) a portion of the media asset.  The media guidance application may split the portion of the\nmedia asset into three segments (first, second, and third segments).  The media guidance application may perform the splitting in various ways.  For example, the media guidance application may split the segments according to time (e.g., five-minute\nintervals).  The media guidance application may also split the segments into segments that fit the criterion and segments that do not.  For example, the media guidance application may execute the content recognition algorithm against each video frame. \nAs video frames keep matching the criterion, they are maintained as the same segment.  As soon as the media guidance application reaches a video frame that does not fit the criterion, a new segment is started and all the frames that do not fit the\ncriterion are stored as one segment until a frame is reached that matches the criterion again.  It should be noted that once a segment is finalized, (i.e., frames that do not match the criterion end and frames that match the criterion begin), audio\ncontent recognition may be executed against the segment.\n The media guidance application may execute a content recognition algorithm against each segment.  The content recognition algorithm may include a visual content recognition process that is able to scan the video of the segment and recognize\nobjects and places in the segment.  For example, if a segment includes a portion of a soccer game, the media guidance application may execute a visual content recognition process that may recognize a soccer ball, the name of the stadium and the city\nwhere the game is being played, names of teams that are playing, the name of the competition, etc. Those objects may have associated keywords which the media guidance application may add to the set of keywords for the specific segment.  Additionally or\nalternatively the media guidance application may execute an audio content recognition process that may recognize words associated with the segment.  The words associated with the segment may be added to the set of keywords as well.\n It should be noted that instead of executing a content recognition algorithm, the media guidance application may transmit video and audio information associated with the segment to another device where the content recognition algorithm may be\nexecuted and the resulting keywords may be sent back.  In some embodiments, the content recognition algorithm may be executed at media guidance content source 516 and/or media guidance data source 518.  In those embodiments, the media guidance\napplication may transmit a request to execute the content recognition algorithm on a specific segment.  The media guidance application may include as part of the request the start time of the segment, the end time of the segment and a media asset\nidentifier associated with the media asset that the segment is in. The media guidance application may receive back a set of keywords for the segment.  The media guidance application may store the set of keywords in a keyword file associated with the\nsegment.\n If more keywords are required, the current set of keywords may be expanded to include additional keywords that are related to those already in the keyword file.  Process 700 of FIG. 7 illustrates one possible method of generating more keywords. \nAt step 702, the media guidance application retrieves (e.g., via control circuitry 404) one or more keywords stored in a keyword file.  The keyword file may be located in storage 408.  Additionally or alternatively, the keyword file may be located at\nmedia content source 516 and/or media guidance data source 518.  In those instances, the media guidance application may receive the keyword file via communications network 514.\n At step 704, the media guidance application determines (e.g., via control circuitry 404) whether a number of keywords in the keyword file meet or exceed a threshold value.  For example, a threshold value may be set by a service provider and may\nbe updated from time to time.  The threshold value may be stored in storage 408.  The threshold value may be preconfigured by the manufacturer or content distributor or specified by the user.  If the number of keywords in the keyword file meet or exceed\nthe threshold value, the media guidance application, at step 716, transmits (e.g., via control circuitry through I/O port 402) an indication that the keyword file is ready for further processing (e.g., a comparison with another keyword file).\n However, if the number of keywords in the keyword file does not meet or exceed the threshold value, process 700 moves to step 706.  At step 706, the media guidance application selects (e.g., via control circuitry 404) a previously unselected\nkeyword from the keyword file.  For example, the media guidance application may copy the keyword from hard disk into memory.\n At step 708, the media guidance application retrieves (e.g., via control circuitry 404), from a database, for the selected keyword a previously unselected object that is most related to the selected keyword.  The database may be located at media\ncontent source 516 and/or media guidance data source 518, as described above.  For example, the media guidance application may generate a query to the database to search for objects that are related to the selected keyword.  Specifically, if the selected\nkeyword is \"Lindsey Vonn,\" the media guidance application may retrieve objects associated with \"Lindsey Vonn\" (e.g., sport, her Olympic events, etc.).\n At step 710, the media guidance application adds (e.g., via control circuitry 404), to the keyword file, keywords that are associated with the most related object, and marks the previously unselected object and the previous unselected keyword as\nselected.  For example, if Lindsey Vonn's most related object is the event that she competes in the most (e.g., downhill slalom), the media guidance application may add that keyword to the keyword file.  The media guidance application may then mark\n\"downhill slalom\" as a previously selected object.  Also the keyword \"Lindsey Vonn\" is marked selected at this time.\n At step 712, the media guidance application determines whether all the keywords in the keyword file have been previously selected.  If all the keywords have not previously been selected, process 700 moves to step 706, where steps 706-710 are\nrepeated for each keyword in the keyword file.  If all keywords have previously been selected, process 700 moves to step 714.\n At step 714, the media guidance application marks (e.g., via control circuitry 304) all keywords in the keyword file as previously unselected.  For example, the media guidance application may store the keywords in a data structure (e.g., an\narray) in memory.  The media guidance application may iterate through each keyword and perform steps 708-710 on each keyword.  The media guidance application may mark a keyword as selected by moving a pointer from one keyword to the next in a specific\norder.  However, in step 714, the pointer is reset back to the beginning of the data structure so that the keywords may be selected again.\n After step 714, process 700 moves to step 704, where the media guidance application determines again whether the number of keywords in the keyword file meets or exceeds the threshold value.  As described above, process 700 continues until the\nthreshold value is met or exceeded.\n It should be noted that, in some embodiments, programming instructions for process 700 may be encoded onto a non-transitory storage medium (e.g., storage device 408) as a set of instructions to be decoded and executed by processing circuitry\n(e.g., processing circuitry 406).  Processing circuitry may in turn provide instructions to other sub-circuits contained within control circuitry 404, such as the tuning, video generating, encoding, decoding, encrypting, decrypting, scaling,\nanalog/digital conversion circuitry, and the like.\n It is contemplated that the descriptions of FIG. 7 may be used with any other embodiment of this disclosure.  In addition, the descriptions described in relation to the process of FIG. 7 may be done in alternative orders or in parallel to\nfurther the purposes of this disclosure.  For example, control circuitry 404 may submit multiple queries to the database in parallel, or it may submit multiple queries to a plurality of similar databases in order to reduce lag and speed the execution of\nthe process.  Furthermore, it should be noted that the process of FIG. 7 may be implemented on a combination of appropriately configured software and hardware, and that any of the devices or equipment discussed in relation to FIGS. 4-5 could be used to\nimplement one or more portions of the process.\n At step 606, the media guidance application generates (e.g., via control circuitry 404) a fourth set of keywords based on the criterion.  In some embodiments, the media guidance application may generate the fourth set of keywords based on the\ncriterion as follows.  The media guidance application may convert the criterion into textual data and parse the textual data into a plurality of terms.  For example, if the criterion is received as one or more user selections of specific terms generated\nfor display, the media guidance application converts those selections into text data.  Specifically, each selection may be an object that includes text data representing the object.  The media guidance application may parse the text for each object into\nterms.  For example, if textual data includes \"ice hockey\" and \"Canada,\" the media guidance application may parse the data into two terms.\n The media guidance application may retrieve, from a database that includes associations between terms and keywords, keywords associated with each term of the plurality of terms.  For example, the media guidance application may query a database\nlocated at media content source 516 and/or media guidance data source 518.  The database may also be stored in storage 408 on the same device where the media guidance application resides.  In some embodiments, portions of the database may be split among\nsome or all of these locations.  The media guidance application may transmit a request to the database for keywords associated with the terms \"ice hockey\" and \"Canada.\" The media guidance application may receive back keywords such as \"puck,\" \"hockey\nstick,\" \"net minder,\" etc. The media guidance application may add to the fourth set of keywords the keywords associated with each term of the plurality of terms (e.g., the keywords above).  The fourth set of keywords may be stored in a keyword file in\nstorage 408.\n Additionally or alternatively, the media guidance application may execute process 700 of FIG. 7 to add to the keywords located in the keyword file, as described above in relation to generating keywords for segments.\n At this point, the media guidance application compares the keywords associated with each segment with keywords associated with the criterion in order to determine which of the first, second, and third segments match the criterion.  At step 608,\nthe media guidance application compares (e.g., via control circuitry 404) the first set of keywords, the second set of keywords, and the third set of keywords with the fourth set of keywords.  For example, the media guidance application may iterate\nthrough each set of keywords to determine which keywords match.  The media guidance application may store the results of the comparison in storage 408 or remotely at media content source 516 and/or media guidance data source 518.  Specifically, the media\nguidance application may store the number of keywords that matched between the two sets, the total number of keywords in the set associated with the segment, and the total number of keywords in the set associated with the criterion.  It should be noted\nthat any combination of that information may be stored.  Additionally or alternatively, the media guidance application may store a percentage of keywords that match.  This may be the percentage of keywords that matched based on the matching keywords and\nthe keywords associated with the segment and/or a percentage of keywords that matched based on the keywords that matched and the keywords associated with criterion.\n At step 610, the media guidance application determines (e.g., via control circuitry 404) based on the comparing, that the first segment and the third segment match the criterion, and that the second segment does not match the criterion.  The\nmedia guidance application may make the determination based on any of a number of words matching, a percentage of words matching based on the keywords that matched and the keywords associated with the segment and/or the keywords that matched and the\nkeywords associated with the criterion.\n In some embodiments, the media guidance application may determine that a number of keywords in the first set of keywords that matches keywords in the fourth set of keywords meets a threshold value and determine that a number of keywords in the\nsecond set of keywords that matches keywords in the fourth set of keywords does not meet the threshold value.  In some embodiments, the threshold value may be a specific number of keywords that must match in order for the segment to match the criterion. \nThat number may be as small as one.  In other embodiments, the threshold value may be based a percentage of words matching based on the keywords that matched and the keywords associated with the segment and/or the keywords that matched and the keywords\nassociated with the criterion.\n For example, if the received criterion is \"ice hockey\" that has associated keywords such as \"goalie,\" \"puck \"and\" hockey stick,\" the media guidance application may try to match those keywords to the keywords associated with the segments.  Thus,\nif the content recognition algorithm detects a puck in the first segment, the keyword \"puck\" is added to the set of keywords associated with the first segment.  Thus, at least one keyword will match between the first segment and the criterion.\n For example, if a keyword file associated with a segment has twenty keywords, the keyword file associated with the criterion has one hundred keywords, and ten keywords match, the media guidance application may be configured for different ways to\ndetermine whether a segment matches the criterion.  For example, 50% of the words in the keyword file associated with the segment matched; thus a threshold value of 50% or lower produces a match.  However, only 10% of the keyword file associated with the\ncriterion match.  Thus, a threshold value of 10% or lower calculated in this manner meets the criterion.  Thus, two different types of threshold values are possible.\n At step 612, the media guidance application, based on determining that the second segment does not match the criterion, stores (e.g., via control circuitry 404 in storage 408) a delete indicator for the second segment that indicates that the\nsecond segment is to be deleted.  In continuing with the example above, if the set of keywords associated with the second segment does not match any keywords associated with the criterion or matches too few keywords (i.e., a threshold value is not\nreached), the media guidance application may determine that the second segment does not match the criterion.  The media guidance application may proceed to mark the second segment for deletion.  For example, the media guidance application may store the\ndelete indicator with metadata associated with the segment or in a segment map.  As referred to herein, the term \"segment map\" refers to a data structure that includes a variable for each segment of a media asset in order to indicate whether each segment\nis to be retained or deleted.\n Steps 608 to 612 may be illustrated by process 800 of FIG. 8.  Specifically, process 800 illustrates how the media guidance application determines whether to mark a segment for deletion or to retain a segment.  At step 802, the media guidance\napplication retrieves (e.g., via control circuitry 404 from storage 408) a set of keywords associated with a criterion and a set of keywords associated with a segment.  For example, the media guidance application may retrieve the sets of keywords from\nrespective keyword files and copy the sets of keywords into memory as data structures (e.g., arrays).  To continue with the example above, in this step the media guidance application may retrieve keywords \"puck,\" \"goalie,\" \"hockey stick,\" etc., that are\nassociated with the criterion.  Keywords associated with each segment are also retrieved.\n At step 804, the media guidance application sets (e.g., via control circuitry 404) a first pointer to the beginning of the set of keywords associated with the criterion and a second pointer to the set of keywords associated with the segment. \nFor example, the media guidance application may set the first pointer to the beginning of the data structure corresponding to a segment and the second pointer to the data structure corresponding to the criterion.  In this step the media guidance\napplication sets a pointer to iterate through the keywords associated with the criterion.\n At step 806, the media guidance application determines (e.g., via control circuitry 404) whether the first pointer is at the end of the set of keywords that are associated with the criterion.  If the first pointer is at the end of the set of\nkeywords and this step is reached, then process 800 has already iterated through all the keywords and a threshold value has not been reached in order to find a match between the segment and the criterion.  Thus, the media guidance application adds (e.g.,\nvia control circuitry 404) a delete indicator to the metadata of the segment and updates a segment map indicating that the segment is to be deleted.  If the first pointer is not at the end of the set of keywords, process 800 moves to step 808.  This step\nis part of the iteration process that insures that all keywords are properly selected for comparison and no words are missed.\n At step 808, the media guidance application advances (e.g., via control circuitry 404) the first pointer at a next keyword of the set of keywords associated with the criterion and rests the second pointer to the beginning of the set of keywords\nassociated with the segment.  For example, the media guidance application may advance the first pointer at the next variable within the data structure associated with the criterion and set the second pointer to the beginning of the data structure\nassociated with the segment.  This step is part of the iteration process described above.\n At step 810, the media guidance application determines (e.g., via control circuitry 404) whether the second pointer is at the end of the set of keywords that are associated with the segment.  If the media guidance application determines that the\nsecond pointer is at the end of the set of keywords that are associated with the segment, then a specific word in the set of keywords associated with the criterion did not match any keywords associated with the segment.  This step is part of the\niteration process.  In this step, the second pointer is reset so that the next keyword associated with the criterion can be compared against all the keywords associated with the segment.  Therefore, process 800 moves to step 806, described above in order\nto process the next keyword in the set associated with the criterion.  If the media guidance application determines that the second pointer is not at the end of the set of keywords that are associated with the segment, process 800 moves to step 812.\n At step 812, the media guidance application advances (e.g., via control circuitry 404) the second pointer to a next keyword of the set of keywords associated with the segment.  At step 814, the media guidance application compares (e.g., via\ncontrol circuitry 404) the keyword associated with the first pointer with the keywords associated with the second pointer.  This may be a simple character-by-character comparison.  In some embodiments, if a certain portion of the word matches a match may\nbe found.  For example, a match may be found between \"athlete\" and \"athletes\" despite the words not being fully the same.\n At step 816, the media guidance application determines (e.g., via control circuitry 404) whether the keyword associated with the first pointer matches the keyword associated with the second pointer.  If a match is not found, process 800 moves to\nstep 812, where steps 810 to 816 are repeated.  If a match is found process 800 moves to step 818.  Steps 812 to 816 illustrate how a specific keyword associated with the criterion is compared with every keyword associated with a segment.\n At step 818, the media guidance application increments (e.g., via control circuitry 404) a keyword match counter.  The media guidance application may maintain a keyword match counter for process 800 in order keep track of the number of keywords\nthat have matched between the two sets of keywords being compared.  The keyword match counter may be stored in storage 408 or remotely at media content source 516 and/or media guidance data source 518.\n At step 820, the media guidance application determines (e.g., via control circuitry 404) whether the keyword match counter is equal to or is greater than a threshold value.  It should be noted that the threshold value and the keyword match\ncounter may be a specific number or percentage as described above.  If the keyword match counter is not equal to or greater than the threshold value, process 800 moves to step 806, where steps 806 to 818 are repeated.  If the keyword match counter is\nequal to or greater than the threshold value, process 800 moves to step 822.\n At step 822 the media guidance application updates (e.g., via control circuitry 404) the segment map, indicating that the segment is to be retained.  Steps 818 to 822 use a keyword match counter to determine whether enough keywords match between\nthe segment and the criterion for the segment to be retained.\n It should be noted that, in some embodiments, programming instructions for process 800 may be encoded onto a non-transitory storage medium (e.g., storage device 408) as a set of instructions to be decoded and executed by processing circuitry\n(e.g., processing circuitry 406).  Processing circuitry may in turn provide instructions to other sub-circuits contained within control circuitry 404, such as the tuning, video generating, encoding, decoding, encrypting, decrypting, scaling,\nanalog/digital conversion circuitry, and the like.\n It is contemplated that the descriptions of FIG. 8 may be used with any other embodiment of this disclosure.  In addition, the descriptions described in relation to the process of FIG. 8 may be done in alternative orders or in parallel to\nfurther the purposes of this disclosure.  Furthermore, it should be noted that the process of FIG. 8 may be implemented on a combination of appropriately configured software and hardware, and that any of the devices or equipment discussed in relation to\nFIGS. 4-5 could be used to implement one or more portions of the process.\n When the media guidance application finishes executing (e.g., via process 800) step 612, the media guidance application moves to step 614.  At step 614, the media guidance application compares (e.g., via control circuitry 404) the first set of\nkeywords to the third set of keywords.  The media guidance application may perform this step in the same manner as step 608, where keyword sets are compared.  At this step, the media guidance application determines whether the first and the third segment\ncorrespond to the same presentation.  For example, the media guidance application may determine whether the first segment is associated with one ice hockey game and the third segment with another ice hockey game.\n At step 616, the media guidance application, based on the comparing of the first set of keywords and the third set of keywords, determines (e.g., via control circuitry 404) whether the third segment matches the first segment.  The media guidance\napplication may make this determines in the same manner as described in relation to step 610.  Specifically, a percentage of matching keywords may be used as well as absolute matching values that are compared to a threshold value of the same type.\n At step 618, the media guidance application, based on determining that the third segment matches the first segment, removes (e.g., via control circuitry 404) the delete indicator for the second media asset.  For example, process 900 of FIG. 9\nillustrates one possible way of removing the delete indicator.\n At step 902 the media guidance application determines (e.g., via control circuitry 404) that a third segment matches the criterion.  The media guidance application may make the determination as described in relation to step 610.\n At step 904, the media guidance application, based on a segment map, determines (e.g., via control circuitry 404) that the second segment has not matched the criterion and the first segment did match the criterion.  The media guidance\napplication may access the segment map stored in storage 408, media content source 516, or media guidance data source 518.  As described above, the segment map may be a data structure (e.g., an array) that includes a variable for every segment.  The data\nstructure may include an identifier corresponding to each segment and an indicator that indicates whether the segment is to be retained or not.  The indicator may be a Boolean variable where the value of \"True\" indicates that the segment is to be\nretained and the value of \"False\" indicates that the segment is to be deleted.\n At step 906, the media guidance application parses (e.g., via control circuitry 404) metadata associated with the second segment into a plurality of tags and stores the plurality of tags in a data structure.  For example, the metadata associated\nwith each segment may be stored in an XML file.  One XML file may be used for all segments.  Alternatively, one XML file may be used for each segment.  The media guidance application may read the tags of the XML file and store the tags in a data\nstructure (e.g., an array).\n At step 908, the media guidance application sets (e.g., via control circuitry 404) a pointer to a beginning of the data structure.  The pointer may simply be set to an address in memory representing the beginning of the data structure.  At step\n912, the media guidance application determines (e.g., via control circuitry 404) whether the pointer is at the last tag of the plurality of tags within the data structure.  It should be noted that if process 900 reaches this point without finding a tag\nassociated with the delete indicator, then the delete indicator is not present in the XML file associated with this particular segment.  Thus, process 900 moves to step 910 where the media guidance application transmits (e.g., via control circuitry 404)\na notification indicating that the delete indicator has not been found.  However, if the pointer is not at the last tag of the plurality of tags within the data structure process 900 moves to step 914.\n At step 914, the media guidance application advances (e.g., via control circuitry 404) a pointer to the next tag of the plurality of tags.  For example, the media guidance application may set the pointer to the address in memory associated with\nthe next tag.  At step 916, the media guidance application determines (e.g., via control circuitry 404) whether the tag associated with the pointer is the delete indicator.  The media guidance application may make the determination by comparing the text\nof the tag to a known delete indicator.  For example, &lt;delete indicator&gt; may be the tag used in the XML file.  At step 918, the media guidance application updates (e.g., via control circuitry 404) the metadata associated with the second media asset\nto remove the tag associated with the delete indicator.  For example, the media guidance application may remove &lt;delete indicator&gt; text from the metadata associated with a segment and save the metadata to storage (e.g., storage 408).\n It should be noted that, in some embodiments, programming instructions for process 900 may be encoded onto a non-transitory storage medium (e.g., storage device 408) as a set of instructions to be decoded and executed by processing circuitry\n(e.g., processing circuitry 406).  Processing circuitry may, in turn, provide instructions to other sub-circuits contained within control circuitry 404, such as the tuning, video generating, encoding, decoding, encrypting, decrypting, scaling,\nanalog/digital conversion circuitry, and the like.\n It is contemplated that the descriptions of FIG. 9 may be used with any other embodiment of this disclosure.  In addition, the descriptions described in relation to the process of FIG. 9 may be done in alternative orders or in parallel to\nfurther the purposes of this disclosure.  Furthermore, it should be noted that the process of FIG. 9 may be implemented on a combination of appropriately configured software and hardware, and that any of the devices or equipment discussed in relation to\nFIGS. 4-5 could be used to implement one or more portions of the process.\n In some embodiments, the media guidance application may based on determining that the third segment does not match the first segment, add a new event indicator to the third segment.  For example, the media guidance application may add the new\nevent indicator to the metadata corresponding to the third segment.  Alternatively or additionally, the media guidance application may add the start time of the new event within the segment to the metadata of the segment.  This information may be stored\nin storage 408 or remotely at media content source 516 and/or media guidance data source 518.\n In some embodiments, the media guidance application may update the new event indicator with a location within the third segment where a first match between the third set of keywords and the fourth set of keywords occurred and delete a portion of\nthe third segment prior to the location.  This feature may be desirable in the instances where the new event starts towards the end of the segment and a user may need to view or review a large chunk of the segment in order to get to the content that the\nuser desires to consume.  Similarly, in some embodiments, the media guidance application may delete an end portion of a prior segment after the prior event has ended.  It should be noted that these portions may be deleted or marked for deletion for later\nremoval.\n In some embodiments, the media guidance application may automatically delete all the segments marked for deletion.  Additionally or alternatively, the media guidance application may prompt a user to make the decision.  For example, when a user\nselects a media listing to consume the recorded media asset, the media guidance application may prompt the user to delete the segments marked for deletion.  In some embodiments, the media guidance application may provide an option to the user to hide the\nsegments that are marked for deletion by, for example, skipping them during playback.\n In some embodiments, the media guidance application may determine a first plurality of segments of the media asset that have an associated delete indicator and a second plurality of segments of the media asset that have an associated event\nindicator.  The media guidance application may make the determination by analyzing the metadata associated with each segment.  Additionally or alternatively, the media guidance application may analyze the segment map.  Thus, the segment map may be\nconfigured to store, in addition to the delete indicators, new event indicators.  The media guidance application may, based on the determining, update the media asset by deleting the first plurality of segments and insert a cue to indicate every new\nevent within the media asset.  For example, in addition to deleting all the segments that have associated delete indicators, the media guidance application may insert one or more frames to indicate to the user that a new event has started or is about to\nstart.\n In some embodiments, as described above, the first, the second, and the third segments may be contiguous.  In some embodiments, each of the first, second and third segment include a video frame.  In some embodiments, each segment is a video\nframe.\n The above-described embodiments of the present disclosure are presented for purposes of illustration and not of limitation, and the present disclosure is limited only by the claims that follow.  Furthermore, it should be noted that the features\nand limitations described in any one embodiment may be applied to any other embodiment herein, and flowcharts or examples relating to one embodiment may be combined with any other embodiment in a suitable manner, done in different orders, or done in\nparallel.  In addition, the systems and methods described herein may be performed in real time.  It should also be noted, the systems and/or methods described above may be applied to, or used in accordance with, other systems and/or methods.", "application_number": "15078436", "abstract": " Systems and methods are provided to record portions of media assets. User\n     request is received to record a media asset together with a criterion for\n     recording portions of that media asset. A content recognition algorithm\n     is executed against segments of the media asset to determine a set of\n     keywords associated with those segments. Separately a set of keywords\n     associated with the criterion is generated. Sets of keywords are compared\n     and segments that match the criterion are discovered. If it is determined\n     that a first segment and third segment each match the criterion and a\n     second segment does not, a delete indicator is added to the second\n     segment and the third and first segments are compared. If those segments\n     match the delete indicator is removed from the second segment.\n", "citations": ["20060215991", "20070212023", "20090028520", "20150262616"], "related": []}, {"id": "20170329582", "patent_code": "10303448", "patent_name": "Systems and methods for graph-based analysis of software", "year": "2019", "inventor_and_country_data": " Inventors: \nSteven; John (Boston, MA), Schmidt; Chris (Boston, MA), Thayer; Jordan Tyler (North Billerica, MA)  ", "description": "BACKGROUND\n Computer software has become an indispensable tool in many aspects of human life.  Day-to-day activities (e.g., shopping, banking, signing up for health insurance, etc.) are often conducted via web and mobile applications.  Virtually all\norganizations, both public and private, rely on software applications to process information and manage operations.  Many of these software applications handle sensitive information such as personal financial records, trade secrets, classified government\ninformation, etc. Safety-critical systems in infrastructure, transportation, medicine, etc. are increasingly being controlled by software.\n Every year, trillions of dollars are spent globally to develop and maintain software applications.  Yet system failures and data breaches are constantly in the news.  Decades of research has failed to produce scalable and accurate solutions for\nimproving reliability and security of software applications. DESCRIPTION OF DRAWINGS\n The accompanying drawings are not necessarily drawn to scale.  For clarity, not every component may be labeled in every drawing.\n FIG. 1 shows an illustrative system 100 for software verification, in accordance with some embodiments.\n FIG. 2 shows an illustrative model-view-controller (MVC) architecture 200 that may be modeled using one or more discovery queries, in accordance with some embodiments.\n FIG. 3 shows an illustrative analysis engine 300 programmed to generate an application architecture model based on program code and one or more framework models, in accordance with some embodiments.\n FIG. 4 shows an illustrative source program 400 and an illustrative discovery query 420, in accordance with some embodiments.\n FIG. 5 shows an illustrative source program 500 and illustrative property queries 510 and 515, in accordance with some embodiments.\n FIG. 6 shows an illustrative process 600 that may be performed by an analysis engine, in accordance with some embodiments.\n FIG. 7 shows an illustrative Abstract Syntax Tree (AST) 700 for an illustrative program 705, in accordance with some embodiments.\n FIG. 8 shows Backus Normal Form (BNF) definitions of some components of an illustrative query language, in accordance with some embodiments.\n FIG. 9 shows a transformation of an illustrative AST 900 to a transformed AST 905, in accordance with some embodiments.\n FIG. 10 shows an illustrative source program 1050 and an illustrative property query 1055, in accordance with some embodiments.\n FIG. 11 shows an illustrative property query 1100, in accordance with some embodiments.\n FIG. 12 shows an illustrative network 1200 of modules, in accordance with some embodiments.\n FIG. 13 shows an illustrate set of nouns that may be used in a query language for accessing components in an MVC architecture, in accordance with some embodiments.\n FIG. 14 shows an illustrative hierarchy 1400 of MVC components, in accordance with some embodiments.\n FIG. 15 shows an illustrative network 1500 of models that may be used to facilitate analysis of a software application, in accordance with some embodiments.\n FIG. 16 shows illustrative framework models 1600 and 1605, in accordance with some embodiments.\n FIG. 17 illustrates an approach for programming an analysis engine to perform a field and type analysis, in accordance with some embodiments.\n FIG. 18A shows an illustrative application 1800 and illustrative component models 1805 and 1810, in accordance with some embodiments.\n FIG. 18B shows illustrative groups 1815, 1820, and 1825 of security issues that may be checked by an analysis engine, in accordance with some embodiments.\n FIGS. 18C-E show an illustrative taxonomy of security responsibilities, in accordance with some embodiments.\n FIG. 18F-G show an illustrative mapping from application components to security responsibilities, in accordance with some embodiments.\n FIG. 19 shows a plurality of illustrative types of models that may be used by an analysis engine to check a property of interest, in accordance with some embodiments.\n FIG. 20 shows an illustrative mapping from types of properties to types of models, in accordance with some embodiments.\n FIG. 21 shows an illustrative process for selecting one or more types of property models and using property models of the selected types to analyze a software application, in accordance with some embodiments.\n FIG. 22 shows an illustrative application 2200 and an illustrative analysis of the application 2200, in accordance with some embodiments.\n FIG. 23 shows illustrative program code 2300 and an illustrative analysis of the program code 2300, in accordance with some embodiments.\n FIG. 24 shows illustrative program code 2400 and an illustrative analysis of the program code 2400, in accordance with some embodiments.\n FIG. 25 shows an illustrative application architecture model 2500, in accordance with same embodiments.\n FIG. 26A shows an illustrative application 2600 and an illustrative implementation 2605 of route functions in the application 2600, in accordance with some embodiments\n FIG. 26B shows an illustrative revised configuration model 2625 and an illustrative revised route model 2635, in accordance with some embodiments.\n FIG. 27 shows an illustrative system 2700 for providing guidance to a developer, in accordance with some embodiments.\n FIG. 28 shows an illustrative knowledge graph 2800, in accordance with some embodiments.\n FIG. 29 shows an illustrative implementation of a guidance engine, in accordance with some embodiments.\n FIG. 30 shows an illustrative guidance store 3000 for storing guidance information, in accordance with some embodiments.\n FIG. 31 shows an illustrative decision tree 3100 that may be used by a guidance engine to determine what guidance to render to a developer and/or how such guidance is to be rendered, in accordance with some embodiments.\n FIG. 32 shows, schematically, an illustrative computer 1000 on which any aspect of the present disclosure may be implemented.\n FIGS. 33-36 illustrate exemplary graph-based queries provided to and received from the guidance engine, in accordance with an embodiment.\nDETAILED DESCRIPTION\n The inventors have recognized and appreciated various disadvantages of existing approaches to software verification.  For instance, the inventors have recognized and appreciated that some existing approaches focus solely on testing, which\nhappens late in the system development life cycle, when an application or module has already been implemented to a large extent.  At that late stage, correcting problems such as security vulnerabilities may involve rewriting not only the portion of code\nthat directly gives rise to an identified problem, but also related portions of code.  In some instances, it may be impractical to reverse certain design decisions made during the development stage.  As a result, a developer may be forced to adopt a\nsuboptimal solution to an identified problem.\n Accordingly, in some embodiments, techniques are provided for detecting potential problems during the development stage, so that an identified problem may be corrected before additional code is written that depends on the problematic code, and a\ndeveloper may have greater freedom to implement an optimal solution to the identified problem.  For instance, a verification tool may be built into an integrated development environment (IDE) and may be programmed to analyze code as the code is being\nwritten by a developer.  Alternatively, or additionally, a verification tool may be accessed via a web user interface.  In either scenario, the verification tool may be able to provide feedback sufficiently quickly (e.g., within minutes or seconds) to\nallow the developer to make use of the feedback while the developer is still working on the code.\n The inventors have recognized and appreciated that some existing approaches of software verification may be unhelpful to software developers.  Software development teams are under pressure to deliver products on time and within budget.  When a\nproblem is identified through testing, a developer may be given little or no guidance on how to address the problem.  As a result, the developer's attempted fix may be ineffective, or may even create new problems.  This frustrating process may repeat\nuntil the developer stumbles upon a correct solution, often after spending valuable time searching online resources and consulting with peers.\n Accordingly, in some embodiments, techniques are provided for integrating training and quality assessment.  As an example, a verification tool may be programmed to link an identified problem to one or more targeted training modules.  As another\nexample, a verification tool may be programmed to analyze software code to understand a developer's intent and proactively suggest one or more training modules on common problems related to that intent.  As yet another example, a verification tool may be\nprogrammed to analyze code written by a developer for a particular type of quality issue (e.g., a particular security vulnerability) after the developer views, reads, or otherwise completes a training module on that type of quality issue.\n In some embodiments, techniques are provided for presenting verification results to a software developer.  The inventors have recognized and appreciated that it may be beneficial to present verification results in a streamlined fashion so that\nverification may become an integral part of a software developer's work, rather than an interruption.  As an example, a verification tool may be programmed to deliver results incrementally, for instance, by first delivering results from easy checks\n(e.g., syntactic pattern matching), while the system is still performing a deep analysis (e.g., model checking).  In this manner, the developer may immediately begin to review and address the results from the easy checks, without having to wait for the\ndeep analysis to be completed.\n The inventors have further recognized and appreciated that it may be beneficial to present suggested code transforms in an unobtrusive fashion, so that a software developer may come to view the verification tool as a helpful peer, rather than\njust an annoying issue-flagging feature.  For example, a verification tool may be programmed to analyze software code to understand a developer's intent and provide suggested code modifications based on the identified intent.  Additionally, or\nalternatively, the verification tool may allow the developer to test a piece of suggested code in a sandbox.\n In some embodiments, a verification tool may be programmed to select, from a variety of different modes, an appropriate mode for delivering guidance to a software developer.  For instance, the verification tool may select from static content\n(e.g., text, video, etc. retrieved from a content store), dynamically generated content (e.g., content that is customized based on current code context), coding suggestions (e.g., suggested fixes to identified problems, or best practice tips based on\nidentified intent), a suggested version of code to be tested in a sandbox, etc.\n It should be appreciated that the techniques introduced above and discussed in greater detail below may be implemented in any of numerous ways, as the techniques are not limited to any particular manner of implementation.  Examples of details of\nimplementation are provided herein solely for illustrative purposes.  Furthermore, the techniques disclosed herein may be used individually or in any suitable combination, as aspects of the present disclosure are not limited to the use of any particular\ntechnique or combination of techniques.\n I. Software Verification\n Some techniques have been developed to automatically analyze program behavior with respect to properties such as correctness, robustness, safety, and liveness.  For instance, static analysis techniques have been developed to analyze program code\nwithout executing the code, whereas dynamic analysis techniques have been developed to analyze program code by observing one or more executions of the code.  Some software verification tools use a combination of static and dynamic analysis techniques.\n Examples of static analysis techniques include, but are not limited to, control flow analysis, data flow analysis, abstract interpretation, type and effect analysis, and model checking.  An analysis engine implementing one or more of these\ntechniques may receive as input program code and one or more properties to be checked, and may output one or more results (e.g., indicating a property is violated).\n Model checking techniques were developed initially for hardware verification, and have been used to some extent for software verification, albeit with lesser success, as software systems tend to be significantly more complex than hardware\nsystems.  To verify a program for compliance with a certain specification, a formal model of the program may be constructed, and the model may be checked against the specification.  For instance, a model may be formulated as a finite state machine, and a\nproperty may be expressed as a formula in a suitable logic.  A state space of the finite state machine may be explored to check whether the property is satisfied.\n In some implementations, states in a finite state machine may be explicitly enumerated.  Alternatively, or additionally, states may be symbolically enumerated, by encoding sets of states into respective symbolic states.  In some implementations,\na symbolic execution technique may be used, where an interpreter may simulate how a program executes and maintain program state with symbolic data.\n II.  Programmable Analysis of Software Applications\n Many software applications are complex and difficult to analyze.  For instance, an application may include hundreds of modules and millions of lines of code, and may make use of external components (e.g., frameworks, libraries, middleware, etc.)\nthat may or may not be open sourced.  The inventors have recognized and appreciated that it may be beneficial to provide techniques for abstracting a software application in a manner that focuses on one or more properties of interest, and that it may\nalso be beneficial to provide techniques for abstracting a framework or library.\n The inventors have additionally recognized and appreciated various disadvantages of existing approaches for abstraction.  For instance, some approaches are purely syntactic, such as using a utility like grep to search through source code for a\nmatch of a regular expression, or rely on simple abstractions such as performing a data flow analysis (e.g., based on bit propagation) to abstract a program, and making Boolean marks on library functions in abstractions.  The inventors have recognized\nand appreciated that these approaches may fail to capture program semantics sufficiently, and hence may incur high inaccuracies (e.g., false positives).  Furthermore, behaviors of external components such as frameworks and libraries may be modeled\npoorly, if at all, and precise semantics of a programming language in which an application is written may not be taken into account.\n The inventors have further recognized and appreciated that some software verification tools rely on limited methods for specifying properties to be checked.  For instance, specification methods based on XML (Extensible Markup Language) or JSON\n(JavaScript Object Notation) may be cumbersome to use, and may allow only a limited set of constructs, so that many interesting properties cannot be expressed.  Furthermore, these methods may not allow a user to specify a modification to be made to an\napplication, for example, when a certain issue is identified.\n The inventors have recognized and appreciated that it may be beneficial to provide improved techniques for abstracting an application and/or external components such as frameworks and libraries, and for specifying properties to be checked and/or\nmodifications to be made to an application to satisfy the properties.  In some embodiments, a unified method may be provided to allow a user to program any one or more, or all, of the above aspects of a software analysis engine.  For example, a universal\nquery language may be provided to allow a user to: (1) model software components including code written by the user and/or external components such as frameworks and libraries, (2) specify properties to be checked, and/or (3) mutate programs to satisfy\nproperties.\n FIG. 1 shows an illustrative system 100 for software verification, in accordance with some embodiments.  In this example, the system 100 includes an analysis engine 105 and a guidance engine 110.  The analysis engine 105 may receive as input\nprogram code of a software application to be analyzed.  In some embodiments, the input program code may include source code.  Alternatively, or additionally, the input program code may include object code.  The analysis engine 105 may further receive as\ninput one or more properties to be checked, and may output one or more results of checking the one or more properties against the program code.  The one or more results may include a finding indicating whether a property is satisfied, an identification\nof one or more portions of the input program code that violate a property, and/or a suggested modification to the program code to satisfy a property.  For instance, if the program code does not satisfy a particular property, the analysis engine 105 may\nbe programmed to suggest a modification so that the modified program code will satisfy that property.\n In some embodiments, the analysis engine 105 may further receive as input one or more framework models.  As one example, the analysis engine 105 may be programmed to select and retrieve (e.g., from a database) one or more previously constructed\nframework models.  The selection may be based on any suitable information about the input program code, such as one or more programming languages in which the input program code is written, and/or one or more external components (e.g., frameworks,\nlibraries, and/or middleware) used by the input program code.  As another example, one or more framework models may be selected by a user and retrieved by the analysis engine 105 (e.g., from a database).  As yet another example, one or more framework\nmodels may be constructed by a user and provided to the analysis engine 105.\n In some embodiments, a framework model may include one or more discovery queries written in a query language.  The inventors have recognized and appreciated that a deep understanding of a software application, such as an architecture of the\napplication, high-level functionalities of various components in the architecture, and/or intrinsic connections among the components, may facilitate accurate and efficient analysis of the application.  Accordingly, in some embodiments, techniques are\nprovided for automatically discovering one or more aspects of a software application.  For instance, a discovery query may be applied to the application to discover one or more portions of code corresponding to a component in an architecture, one or more\nfunctionalities of the discovered component, and/or how the discovered component interact with one or more other components in the architecture.\n In some embodiments, discovery queries may be written by a user in a query language.  Alternatively, or additionally, discovery queries for particular external components (e.g., frameworks, libraries, and/or middleware) may be developed in\nadvance and retrieved on demand (e.g., from a database) when input program code is to be evaluated.\n In some embodiments, a discovery query may include one or more statements instructing the analysis engine 105 how to look for a portion of code that is relevant for a certain analysis (e.g., looking for security vulnerabilities in general, or\none or more specific types of security vulnerabilities).  Additionally, or alternatively, a discovery query may instruct the analysis engine 105 what information to extract from the program code and store in a model, once a relevant portion of code has\nbeen located.  Thus, a discovery query may be an executable program that takes as input the program code to be analyzed and produces as output one or more models.\n In some embodiments, the analysis engine 105 may be programmed to interpret discovery queries written in a query language.  For instance, the analysis engine 105 may execute one or more discovery queries according to semantics of the query\nlanguage, which may cause the analysis engine 105 to gather certain information from source code elements of a program to be analyzed.  However, that is not required, as in some embodiments discovery queries may be compiled into machine code and then the\nmachine code may be executed.\n In some embodiments, the analysis engine 105 may be programmed to apply one or more discovery queries to program code and output a model of the program code that is specific to such discovery queries.  The model thus represents only a subset of\nthe program code that is relevant to the discovery queries.  The analysis engine 105 may then analyze the model and/or a subset of the program code to determine if a certain property of interest is satisfied.  In some embodiments, this analysis of the\nmodel and/or the subset of the program code may be performed using property queries written in the same query language that is used for the discovery queries.\n With the above approach, particular portions of a large application program that are relevant to one or more issues of interest (e.g., security) may be identified and represented by a model, while irrelevant portions of the application may be\nignored.  The resulting model may then be evaluated, and/or be used to identify relevant portions of the program code that should be evaluated, using one or more property queries relating to the issue(s) of interest.  By employing such a\ndivide-and-conquer approach, a highly complex application may be effectively and efficiently evaluated for one or more specific issues of concern.\n The inventors have recognized and appreciated that discovery queries may provide a convenient way to capture knowledge regarding a programming language, framework, library, middleware, etc. For instance, a user who understands semantics of a\nprogramming language (or framework, library, middleware, etc.) may write discovery queries that help the analysis engine 105 identify portions of program code that are relevant for a certain analysis that is being performed (which may, although need not,\nbe a security analysis).  A model that results from applying a discovery query to program code may be an abstraction of the program code with respect to the analysis that is being performed.  In this manner, property checking may be performed more\nefficiently, because much of the program code may be irrelevant for the analysis that is being performed, and may simply be ignored.\n The inventors have further recognized and appreciated that framework models may be managed advantageously as reusable assets.  For example, once a discovery query is written by a user for a certain analysis on a program written in a certain\nprogramming language (or using a certain framework, library, middleware, etc.), the discovery query may be appropriately indexed and stored.  In this manner, when the same user or another user wishes to perform the same analysis on a different program\nwritten in the same programming language (or using the same framework, library, middleware, etc.), the previously written discovery query may be retrieved and applied.\n Returning to the example shown in FIG. 1, one or more results output by the analysis engine 105 may be consumed by the guidance engine 110.  The inventors have recognized and appreciated that it may beneficial to provide customized and\nactionable guidance to a developer when a problem is identified.  Accordingly, in some embodiments, the guidance engine 110 may be programmed to select, based on the one or more results output by the analysis engine 105, an appropriate modality for\naiding a user who wrote the input program code.  Additionally, or alternatively, the guidance engine 110 may be programmed to select, based on the one or more results, appropriate content from a content store.  For instance, if the one or more results\nincludes a finding indicative of a security vulnerability, the guidance engine 110 may present to the user a textual or video message explaining the vulnerability, and/or an in-depth training module.  Additionally, or alternatively, if the one or more\nresults includes a suggested modification to the input program code, the guidance engine 110 may present to the user a textual or video message explaining the suggested modification, and/or modified program code ready to be tested in a sandbox.\n In some embodiments, the guidance engine 110 may automatically determine and present to a user a suggested technique for solving a problem.  For example, the guidance engine 110 may determine a solution based on user preferences, an intended use\nfor a software application, and/or other context information about the software application.\n It should be appreciated that the system 100 is shown in FIG. 1 and described above solely for purposes of illustration.  A software verification tool embodying one or more of the inventive aspects described herein may be implemented in any of\nnumerous ways.  For instance, in some embodiments, one or more of the functionalities described above in connection with the analysis engine 105 may instead be implemented by the guidance engine 110, or vice versa.  In some embodiments, a software\nverification tool may be implemented with a single engine programmed to analyze program code and to render guidance to a developer.  In some embodiments, the analysis engine 105 and the guidance engine 110 may be independently implemented, each as a\nstand-alone tool.  Aspects of the present disclosure are not limited to the use of both the analysis engine 105 and the guidance engine 110.\n As discussed above, the inventors have recognized and appreciated that a deep understanding of a software application, such as an architecture of the application, high-level functionalities of various components in the architecture, and/or\nintrinsic connections among the components, may facilitate accurate and efficient analysis of the application.  In some embodiments, a software architecture may be represented using a framework model comprising one or more discovery queries.  By applying\nsuch discovery queries to program code, an application architecture model may be generated that includes models for individual components in the architecture.  The application architecture model may then be used to facilitate verification of the program\ncode with respect to one or more properties of interest.\n FIG. 2 shows an illustrative model-view-controller (MVC) architecture 200 that may be modeled using one or more discovery queries, in accordance with some embodiments.  An MVC architecture may be used to build a web application comprising\nvarious components having separate responsibilities.  In the example shown in FIG. 2, the MVC architecture 200 includes a web server component 210, a routes component 215, a dispatcher component 220, a controller component 225, a model component 230, a\ndatabase component 235, and a view component 240.  The web server component 210 may receive a web request from a browser 205 and the routes component 215 may map the request to one or more actions to be taken by the controller component 225.  The\ndispatcher component 220 may inform the controller component 225 of the one or more actions to be taken, and the controller component 225 may issue one or more commands to be executed by the model component 230.  The model component 230 may execute the\none or more commands according to logic of the web application and may manage data stored in the database component 235.  The controller component 225 may receive an execution result from the model component 230 and may cause the view component 240 to\ngenerate an updated view based on the execution result.  The controller component 225 may then cause the web server component 210 to respond to the browser 205 with the updated view.\n FIG. 3 shows an illustrative analysis engine 300 programmed to generate an application architecture model based on program code and one or more framework models, in accordance with some embodiments.  For instance, the analysis engine 300 may be\nan implementation of the illustrative analysis engine 105 shown in FIG. 1.\n In the example shown in FIG. 3, the analysis engine 300 includes a model construction component 305 and a property checking component 340.  The model construction component 305 may receive as input program code (which may include source code\nand/or object code) and one or more framework models.  In some embodiments, the one or more framework models may include one or more discovery queries.\n In some embodiments, a framework model may reflect a software architecture, such as the illustrative MVC architecture 200 shown in FIG. 2.  The model construction component 305 may be programmed to use the framework model to understand the input\nprogram code, for example, by extracting relevant information from the input program code and storing the information in one or more models.  In some embodiments, a model may correspond to a component in the software architecture captured by the\nframework model.\n For instance, in the example shown in FIG. 3, the model construction component 305 may be programmed by an MVC framework model to generate an application architecture model 310 that includes a controller model 315, a view model 320, a database\nmodel 325, and a route model 330, which may correspond, respectively, to the controller component 225, the view component 240, the database component 235, and the routes component 215 of the illustrative MCV architecture 200 shown in FIG. 2. \nAdditionally, or alternatively, the application architecture model 310 may include a configuration model 335, which may not correspond to any component in the illustrative MCV architecture 200, but may store configuration information extracted from the\ninput program code.  Examples of configuration information that may be extracted and stored, include, but are not limited to, session and cookie configurations in web server code.\n It should be appreciated that the MVC architecture 200 shown in FIG. 2 and the application architecture model 310 shown in FIG. 3 are provided solely for purposes of illustration, as the inventive aspects described herein may be used to model\nany software architecture.\n FIG. 4 shows an illustrative source program 400 and an illustrative discovery query 420, in accordance with some embodiments.  For instance, the source program 400 may be a portion of the input program code shown in FIG. 3, and the discovery\nquery 420 may be included in the MVC framework model shown in FIG. 3.\n In the example shown in FIG. 4, the discovery query 420 includes a PERFORM statement with a WHEN clause.  The PERFORM statement may specify one or more actions to be performed if a condition specified in the WHEN clause is satisfied.  In some\nembodiments, the WHEN clause may specify a pattern and the one or more actions specified in the PERFORM statement may be performed if the pattern specified in the WHEN clause is detected in the input program code.\n For instance, in the example shown in FIG. 4, the WHEN clause specifies a pattern including a call to @RequestMapping with a URL $1, an HTTP method $2, and a function $f. A model construction component (e.g., the illustrative model construction\ncomponent 305 shown in FIG. 3) may search through the input program code to identify a match of the pattern specified in the WHEN clause.  If a match is found, the PERFORM statement may be executed to extract relevant information and store the extracted\ninformation in a model (e.g., the illustrative route model 330 shown in FIG. 3).\n For example, in the source program 400 shown in FIG. 4, the URL $1 may be matched to the string \"/database\" at 405, the HTTP method $2 may be matched to the string \"get\" at 410, and the function $f may be matched to the declaration of injectable\nQuery at 415.  As shown at 435, the model construction component may execute the PERFORM statement and store the declaration of injectable Query in a resulting model at the following.  _model.routes[\"/database\"][\"get\"].callbacks\n In this manner, the model construction component may be programmable via a discovery query (e.g., the discovery query 420 tells the model construction component what to look for in the input program code and, once a relevant portion of code is\nfound, what information to extract).  For instance, one or more discovery queries (e.g., the illustrative discovery 420 shown in FIG. 4) may be written to model how a particular framework (e.g., a SpringMVC framework) interprets program annotations\n(e.g., @RequestMapping).  Thus, the one or more discovery queries may represent semantics given to such annotations by the particular framework.  One or more models (e.g., the illustrative model 435 shown in FIG. 4) that are constructed by applying the\none or more discovery queries may then replace source code of the particular framework for purposes of checking whether one or more properties are satisfied.\n It should be appreciated that the discovery query 420 is shown in FIG. 4 and described above solely for purposes of illustration.  In some embodiments, other types of conditions may be specified, in addition to, or instead of, syntactic pattern\nmatching.  Furthermore, aspects of the present disclosure are not limited to the use of a discovery query in a framework model.  For instance, in some embodiments (e.g., as shown in FIG. 16 and discussed below), a framework model may include a model that\nis written directly to replace framework source code.  Such a model need not be a result of applying one or more discovery queries.\n Returning to the example shown in FIG. 3, the application architecture model 310 may be analyzed by the property checking component 340 of the analysis engine 300 to determine if one or more properties are satisfied.  Any suitable combination of\none or more property checking techniques may be used, including, but not limited to, data flow analysis, control flow analysis, and/or model checking.  The property checking component 340 may then output one or more results, which may include a finding\nindicating an identified problem (e.g., a security vulnerability), a suggested modification to the input program code to fix an identified problem, an indication that the property checking component 340 is unable to reach a conclusion with respect to a\ncertain property, and/or any other observation of interest.  For instance, a result may flag a portion of code that, based on information available to the property checking component 340, does not yet amount to a problem but merits further investigation. In some embodiments, a result output by the property checking component 340 may be processed by a guidance engine, such as the illustrative guidance engine 110 shown in FIG. 1, to provide appropriate feedback advice to a user.\n FIG. 5 shows an illustrative source program 500 and illustrative property queries 510 and 515, in accordance with some embodiments.  For instance, the source program 500 may be a portion of the input program code shown in FIG. 3, and the\nproperty queries 510 and 515 may be included in the properties to be checked shown in FIG. 3.\n In the example shown in FIG. 5, the property query 505 includes a PERFORM statement with a WHEN clause.  The PERFORM statement may specify one or more actions to be performed if a condition specified in the WHEN clause is satisfied.  In some\nembodiments, the WHEN clause may specify a pattern and the one or more actions specified in the PERFORM statement may be performed if the pattern specified in the WHEN clause is detected in the input program code.\n For instance, in the example shown in FIG. 5, the WHEN clause specifies a pattern where an assignment of a variable $x includes a call to getStringParameter or getRawParameter.  A property checking component (e.g., the illustrative property\nchecking component 340 shown in FIG. 3) may search through the input program code to identify a match of the pattern specified in the WHEN clause.  If a match is found, the property checking component may perform the PERFORM statement to add a field\nnamed tainted to the matched variable and set the value of that field to be true.  In this manner, the property checking component may be programmable via a property query (e.g., the property query 510 tells the property checking component what to look\nfor in program code and, once a relevant portion of code is found, what information to maintain).\n For example, in the source program 500 shown in FIG. 5, the variable $x may be matched to accountName because the assignment of accountName at 520 includes a call to getRawParameter.  This may cause the property checking component to execute the\nPERFORM statement, adding the field accountName.tainted and setting the value of that field to be true.\n In some embodiments, a property checking component may be programmed to propagate the value of an added field such as accountName.tainted.  For instance, in the source program 500 at 525, the variable accountName is used in an assignment of the\nvariable query.  This may cause a field query.tainted to be added and the value of that field set to true.  Thus, in this example, the property checking component is programmed to analyze the source program 500 both syntactically (e.g., via syntactic\npattern matching on getRawParameter) and semantically (e.g., via data flow analysis on the field tainted).\n In some embodiments, a property checking component may be programmed to detect and maintain type information.  For instance, in the source program 500 at 530, an assignment of the variable statement includes an invocation of\nconnection.createStatement.  The property checking component may be programmed to determine type information based on this assignment and associate the type information with the variable statement.\n In the example shown in FIG. 5, the property query 515 includes a MATCH clause, a WHERE clause, and a REWRITE clause.  The REWRITE clause may specify one or more modifications to be made to the program code if a condition specified by the MATCH\nand WHERE clauses is satisfied.\n For instance, in the source program 500 shown in FIG. 5, the object $o may be matched to statement at 535, the method $f may be matched to execute Query at 540, and the parameter $1 may be matched to the variable query at 545.  The property\nchecking component may then use the type information associated with the variable statement to determine that the object $o, which is matched to statement, is an instance of java.sql.Statement.  The property checking component may further determine that\nthe name of the method $f, which is matched to execute Query, matches the regular expression \"execute.*,\" and that the value of the tainted field of the parameter $1, which is matched to the variable query, is true.  Since all of the conditions in the\nWHERE clause are satisfied, the property checking component may execute the REWRITE clause, which may replace the variable query with API.sanitize(query), so that the last line in the source program 500 may become: ResultSet\nresults=statement.executeQuery(API.sanitize(query)).\n Thus, in this example, the property query 515 programs the property checking component to use syntactic information (e.g., presence of the substring execute), data flow information (e.g., propagation of the field tainted), and type information\n(e.g., a type of the variable statement) to determine whether to make a particular modification to the input program code.\n It should be appreciated that the property queries 510 and 515 are shown in FIG. 5 and described above solely for purposes of illustration.  Aspects of the present disclosure are not limited to the use of syntactic analysis, data flow analysis,\nor type analysis.  Furthermore, aspects of the present disclosure are not limited to the use of a REWRITE clause, as a property checking component may sometimes report a finding without suggesting a modification to the input program code.\n FIG. 6 shows an illustrative process 600 that may be performed by an analysis engine, in accordance with some embodiments.  For example, the process 600 may be performed by the illustrative analysis engine 300 shown in FIG. 3 to construct the\nillustrative application architecture model 310 and check one or more properties.\n At act 605, the analysis engine may compile input program code into a suitable representation, such as an abstract syntax tree (AST).  FIG. 7 shows an illustrative AST 700 for an illustrative program 705, in accordance with some embodiments. \nThe inventors have recognized and appreciated that an AST may be used to capture the structure of a program and facilitate manipulations such as annotations and/or modifications.  However, it should be appreciated that aspects of the present disclosure\nare not limited to the use of an AST, or any representation at all.  Examples of representations that may be used instead of, or in additional to, ASTs include, but are not limited to, byte-code, machine code, control flow graphs, logic formulas modeling\nthe semantics, etc.\n At act 610, the analysis engine may select one or more discovery queries to be applied to the AST constructed at act 605.  For instance, in some embodiments, the analysis engine may be programmed to select and retrieve (e.g., from a database) a\npreviously constructed framework model that includes one or more discovery queries.  The selection may be based on any suitable information about the input program code, such as one or more programming languages in which the input program code is\nwritten, and/or one or more external components (e.g., frameworks, libraries, and/or middleware) used by the input program code.  Additionally, or alternatively, the analysis engine may be programmed to select and retrieve (e.g., from a database) one or\nmore discovery queries based on a type of analysis to be performed (e.g., looking for security vulnerabilities in general, or one or more specific types of security vulnerabilities).\n In some embodiments, the analysis engine may retrieve (e.g., from a database) a discovery query selected by a user.  Additionally, or alternatively, the analysis engine may receive, via a user interface, a discovery query written by a user.  In\nsome embodiments, the user interface may be part of an IDE, although that is not required.\n At act 615, the analysis engine may apply the one or more discovery selected at act 610 to the AST constructed at act 605.  An illustrative application of a discovery query is shown in FIG. 4 and discussed above.\n In some embodiments, the analysis engine may first apply one or more discovery queries to extract relevant information from the AST constructed at act 605, thereby constructing a reduced AST.  The analysis engine may then apply one or more\ndiscovery queries to the reduced AST to construct an application architecture model.  Alternatively, or additionally, the analysis engine may apply one or more discovery queries directly to the AST constructed at act 605 to construct an application\narchitecture model.  Any suitable method may be used to traverse an AST.  For instance, in some embodiments, AST nodes may be visited based on control flow, and relationships between the AST nodes may be examined to check a query.  In some embodiments,\nan analysis state may be maintained during such a traversal.  For example, when an AST node is visited, semantic information may be recorded in the analysis state, which may be made available when a next AST node is processed.  The query may then be\nchecked over the information stored in the analysis state.\n At act 620, the analysis engine may apply one or more property queries to the application architecture model constructed at act 615.  Additionally, or alternatively, the analysis engine may apply one or more property queries to the AST\nconstructed at 605, and/or any reduced AST constructed at act 605 (e.g., portions of the AST constructed at 605, and/or any reduced AST constructed at act 605, that correspond to component models in the application architecture model constructed at act\n615).  An illustrative application of property queries is shown in FIG. 5 and discussed above.\n At act 625, the analysis engine may determine if the application of one or more property queries at act 620 has resulted in any observation of interest.  If there is an observation of interest, the analysis engine may, at act 630, output one or\nmore results.  The one or more results may include an indication of an identified problem (e.g., a security vulnerability), a suggested modification to the input program code to fix an identified problem, an indication that the analysis engine is unable\nto reach a conclusion with respect to a certain property, a portion of code that merits further investigation, and/or any other observation of interest.\n At act 635, the analysis engine may determine if the application of one or more property queries at act 620 has resulted in a suggested modification to the input program code.  If there is a suggested modification to the input program code, the\nanalysis engine may, at act 640, transform the AST constructed at act 605.  For example, the analysis engine may execute a mutation query (e.g., with a REWRITE clause) to replace a portion of code (e.g., the variable query in the example of FIG. 5) with\nanother portion of code (e.g., sanitize(query) in the example of FIG. 5).\n At act 645, the analysis engine may use the transformed AST to modify the input program code and output the modified program code.  In some embodiments, a user interface may be provided to allow a user to authorize use of the modified program\ncode and/or to test the modified program code in a sandbox.  Alternatively, or additionally, a branch may be created in a version control system for the modified program code generated by the analysis engine.\n Upon outputting the modified program code, or if it is determined at act 635 that there is no suggested modification to the input program code, the analysis engine may return to act 625 to determine if there is any additional observation of\ninterest.  The inventors have recognized and appreciated that some property queries may take more computing time to answer.  Accordingly, in some embodiments, the analysis engine may be programmed to output results incrementally.  For example, the\nanalysis engine may first deliver results from easy checks (e.g., syntactic pattern matching), while the analysis engine is still performing a deep analysis (e.g., model checking).  In this manner, the user may immediately begin to review and address the\nresults from the easy checks, without having to wait for the deep analysis to be completed.\n It should be appreciated that details of implementation are described above solely for purposes of illustration, as aspects of the present disclosure are not limited to any particular manner of implementation.  For instance, in some embodiments,\na separate guidance engine may be provided that consumes outputs of the analysis engine and renders guidance to a developer based on the analysis engine's outputs.\n III.  Query Language\n The inventors have recognized and appreciated that it may be beneficial to provide a unified method for understanding, modeling, checking, and/or fixing software applications with respect to one or more properties of interest (e.g., security\nvulnerabilities in general, or one or more specific types of security vulnerabilities).\n In some embodiments, a query language may be provided to allow a user to program any one or more aspects of software verification, which may include, but are not limited to: modeling one or more external components (e.g., frameworks, libraries,\nand/or middleware) used by a software application; constructing models of the application that abstract away irrelevant information (e.g., information that is irrelevant for a certain type of analysis such as security analysis); specifying one or more\nproperties to be checked against the application; specifying how the application should be fixed if a problem is identified; and/or controlling how an analysis engine analyzes the application.\n In some embodiments, a query language may be provided that is more expressive than existing techniques for verifying software applications.  For example, the query language may be a superset of a full realistic programming language (e.g.,\nJavaScript).  In some embodiments, a query language may be provided that is more powerful than existing techniques for verifying software applications.  For example, the query language may be used to define semantic abstractions of a program and/or\nexternal components (e.g., frameworks, libraries, and/or middleware) used by the program.  Additionally, or alternatively, the query language may be used to query program semantics.  In some embodiments, a query language may be provided that is more\nconvenient to use than existing techniques for modeling software applications.  For example, the query language may have a succinct syntax and may allow modular definitions.\n FIG. 8 shows Backus Normal Form (BNF) definitions of some components of an illustrative query language, in accordance with some embodiments.  Such a query language may be used, for example, to write the illustrative discovery query 420 shown in\nFIG. 4 and the illustrative property queries 510 and 515 shown in FIG. 5.\n The inventors have recognized and appreciated that a query language having the illustrative constructs shown in FIG. 8 and/or described herein may advantageously provide an expressive, powerful, and convenient method for software verification. \nFor example, these constructs may allow different types of analyses (e.g., static scanning, data flow analysis, fuzzing, dynamic scanning, etc.) to be specified using the same query language, so that the different types of analyses may be combined in a\ndeep way.  Furthermore, these constructs may allow different data sources to be queried using the same query language, so that query results regarding the different data sources may be assessed collectively.\n However, it should be appreciated that aspects of the present disclosure are not limited to the use of a query language having all of the constructs shown in FIG. 8 and/or described herein.  In various embodiments, any one or more of these\nconstructs, and/or other constructs, may be included in a query language.\n A. Syntax Matching Blocks\n In some embodiments, a query language may include constructs for syntax matching blocks, flow operators, semantic predicates, side-effect statements, and/or application programming interface (API) functions for an analysis engine.  Syntax\nmatching blocks may be based on source code syntax for any one or more programming languages, such as JavaScript, Java, C/C++/Objective-C, SWIFT, ASP.NET, Python, Ruby, etc. Flow operators may be used to connect syntax matching blocks to describe flows\nbetween different portions of a program.  Semantic predicates may be built using first order logic and/or native constructs and may be used for semantics queries.  Side-effect statements may be used to instruct the analysis engine to perform specific\nactions, such as building models for a program and/or modifying input program code.  API functions may be used to access internal state of the analysis engine and/or program how the analysis engine performs an analysis.\n In some embodiments, a query language may be provided that uses source language syntax directly for syntax matching.  For instance, in the example shown in FIG. 8, the notation &lt;{&lt;source syntax&gt;}&gt; describes a syntax matching block\nfor matching a syntactic element in a source language, where free variables (which are prefixed by \"$\") are assigned if a match is found.  Thus, syntax matching in this query language may depend on the syntax of a source language (e.g., JavaScript, Java,\nC/C++/Objective-C, SWIFT, ASP.NET, Python, Ruby, etc.).\n As an example, if variable assignment is denoted by \"=\" in a source language, then the syntax matching block &lt;{a=$b}&gt; may match any assignment statement that assigns a value to the variable a. For instance, the syntax matching block\n&lt;{a=$b}&gt; may match the statement, a=a+x, where the syntactic element a+x may be assigned to the free variable $b.\n As another example, the following syntax matching block may be specific to the syntax of Java SpringMVC.\n TABLE-US-00001 &lt;{ @RequestMapping(value =$1, method = $2) $f }&gt;\n This syntax matching block may match a function declaration with an annotation of route information, where the route's URL may be assigned to the free variable $1, the name of the HTTP method may be assigned to the free variable $2, and the\nfunction declaration may be assigned to the free variable $f.\n In some embodiments, a syntax matching block may include OR as a syntax operator.  For instance, the syntax matching block &lt;{getStringParameter( ) OR getRawParameter( )}&gt; may match a function call to getStringParameter or getRawParameter.\n In some embodiments, a syntax matching block may include a character (e.g., \"_\") for a \"don't care\" element.  For instance, the following pattern may match any for loop regardless of the condition, as long as the body of the for loop matches.\n TABLE-US-00002 //pattern for (_) { if ($1) $2; }\n In some embodiments, a syntax matching block may include a syntax operator AS.  For instance, the syntax matching block &lt;{$f(_,$2)}&gt; AS $call may match a function call of two arguments.  When a match is found, the function name may be\nassigned to $f and the second argument may be assigned to $2, while the first argument may not be stored.  Because of the use of the AS operator, the entire function call information, including function name, function declaration, and/or one or more\nmatched arguments, may be stored in $call.\n In some embodiments, a syntax matching block may include multilayer static scopes.  For instance, nested scopes may be expressed using braces and may be matched according to the syntax of a source language (e.g., JavaScript, Java,\nC/C++/Objective-C, SWIFT, ASP.NET, Python, Ruby, etc.).  As an example, the illustrative for loop pattern above may have two matches in the following program code.\n TABLE-US-00003 // program if (b) { for (var i = 1; i &lt; 10; i++) { for (var k in [1,2,3]) { if (x &gt; A[i]) { if (b[k]) x = 1; } } } }\n In the first match, the syntactic element x&gt;A[i] is assigned to $1, and the syntactic element if (b[k]) x=1 is assigned to $2.  In the second match, the syntactic element b[k] is assigned to $1, and the syntactic element x=1 is assigned to\n$2.  In both matches, both scopes (i.e., for loop and if branch) are matched syntactically.\n B. Flow Operators\n In some embodiments, a query language may include one or more flow operators, for example, to describe relationships between syntactic elements.  For instance, one or more temporal operators may be used to describe how a syntactic element flows\nto another syntactic element.  In some embodiments, an analysis engine may match a flow statement through a finite state machine algorithm.  For instance, a finite state machine may be defined that includes at least two states.  At the first state, the\nanalysis engine may analyze portions of input program code, looking for a first syntactic element.  The analysis engine may stay in the first state until the first syntactic element is matched.  Once the first syntactic element is matched, the analysis\nengine may move to the second state, where the analysis engine may analyze further portions of the input program code, looking for a second syntactic element.\n In some embodiments, a basic flow operator (.fwdarw.) may be used to express that a syntactic element is followed by another syntactic element in at least one program path.  As one example, the flow statement &lt;{$f1($a1).fwdarw.$f2($a2)}&gt;\nmay be matched if one function call is followed by another function call in at least one program path, where the two function may be different, but each of the two functions has a signal argument.  The name of the function that is called earlier may be\nassigned to $f1, and the argument of that function may be assigned to $a1, while the name of the function that is called later may be assigned to $f2, and the argument of that function may be assigned to $a2.\n As another example, the following flow statement may be matched if there is at least one program path in which a method of an object is invoked on a variable which was previously assigned the return value of a call to getStringParameter or\ngetRawParameter.  The name of the variable may be assigned to $x, the name of the object may be assigned to $o2, and the name of the function of the object may be assigned to $f.\n &lt;{$x=_.getStringParameter( ) OR _.getRawParameter( ).fwdarw.$o2.$f($x)}&gt;\n In some embodiments, an all-path flow operator (-AP.fwdarw.) may be used to express that a syntactic element is followed by another syntactic element in all program paths.  For instance, the flow statement &lt;{$f1($a1)-AP.fwdarw.$f2($a2)}&gt;\nmay be matched if a call to a first function with a first argument is followed by a call to a second function with a second argument in all program paths.  The name of the first function may be assigned to $f1, and the first argument may be assigned to\n$a1, while the name of the second function may be assigned to $f2, and the second argument may be assigned to $a2.\n In some embodiments, an absence operator (MISSING) may be used to express that in no program path a first syntactic element happens between a second syntactic element and a third syntactic element.  For instance, the flow statement\n&lt;{$f1($a1).fwdarw.MISSING $a2=_.fwdarw.$f2($a2)}&gt; may be matched if there is a program path in which a first function call is followed by a second function call, and there is no assignment to the argument of the second function call between the two\nfunction calls.\n In some embodiments, operators FIRST and LAST may be used to match, respectively, the first and last occurrences of a syntactic element.  For instance, the flow statement &lt;{FIRST f1($a1).fwdarw.LAST f2($a2)}&gt; may be matched if the first\ncall to f1 precedes the last call to f2 in at least one program path, where other calls to f1 and f2 in that program path may be ignored.\n C. Semantic Predicates\n In some embodiments, a query language may be provided that includes one or more semantics predicates for expressing properties relating to variable values, types, etc. Unlike syntax matching blocks, which may be used to query the syntax of a\nprogram, semantic predicates may be used to query semantics of a program, such as values of variables, types of variables, and/or semantic relationships between variables.\n In some embodiments, semantic predicates may be built using first order logic and/or native constructs.  Examples of operators for building semantic predicates include, but are not limited to: arithmetic operators (e.g., +, -, *, /, %, etc.);\nrelational operators (e.g., &gt;=, &gt;, ==, etc.); propositional logic operators (e.g., AND, OR, NOT, IMPLY, etc.); first-order logic quantifiers (e.g., EXIST, FORALL, etc.); domain-specific operators (e.g., RegExp.match, string.indexOf, etc.); type\noperators (e.g., instanceof, ISCONSTANT, etc.); and/or flow operators (e.g., USE, CALL, etc.).\n In some embodiments, an existentially quantified expression EXIST v IN c:body may evaluate to true if there is a value v in the set c such that a condition specified in the body is true.  As one example, the expression EXIST x IN [1,2]:x&gt;0\nmay evaluate to true because there is a value x in the range [1,2] such that x is greater than 0.  As another example, the expression EXIST arg IN f.arguments:arg.taint==true may evaluate to true if there is an argument in the set of arguments/arguments\nsuch that the taint field of the argument is set to true.\n In some embodiments, a universally quantified expression FORALL v IN c:body may evaluate to true if for every value v in the set c, a condition specified in the body is true.  For example, the following expression may evaluate to true if for\nevery index y in the object _model.routes, the route indexed by y, _model.routes[y], is not null.\n FORALL y IN _model.routes:_model.routes[y] !=null\n In some embodiments, a data flow operator USE may be used to express that a value of a second syntactic element is used to compute a value of a first syntactic element.  For example, the expression $arg USE $input may evaluate to true if a value\nof the syntactic element assigned to $input is used to compute a value of the syntactic element assigned to $arg.\n In some embodiments, a control-flow operator CALL may be used to express that a call to a first function includes a call to a second function.  For example, the expression $f1 CALL $f2 may evaluate to true if a call to the function assigned to\n$f1 includes a call to the function assigned to $f2.\n D. Side-Effect Statements\n In some embodiments, a query language may be provided that includes one or more side-effect constructs.  For instance, a side-effect construct may be used to define a discovery query, such as the illustrative discovery query 420 shown in FIG. 4.\n In some embodiments, the following illustrative side-effect construct may be used, where the PERFORM statement may specify one or more actions to be performed if a condition specified in the WHEN clause is satisfied.\n PERFORM &lt;statement&gt; WHEN &lt;syntax matching block&gt;\n In some embodiments, the WHEN clause may specify a pattern and the one or more actions specified in the PERFORM statement may be performed if the pattern specified in the WHEN clause is detected in input program code.  For instance, the PERFORM\nstatement may include a piece of executable code, where the WHEN clause may include a syntax matching block (which may in turn include a semantic predicate).  In some embodiments, a query language may be a superset of the syntax of a high-level\nprogramming language (e.g., JavaScript), so the PERFORM statement may use any one or more constructs provided by the high-level programming language.\n For example, the following discovery query, when executed by an analysis engine, may cause the analysis engine to search input program code for a declaration of a route function in an MVC architecture, where the HTTP method in the declaration is\na method of a RequestMethod object.  The route's URL may be assigned to the free variable $1, the name of the method may be assigned to the free variable $2, the name of the route function may be assigned to the free variable $3, and the entire function\ndeclaration may be assigned to the free variable $f (using the AS operator).  The PERFORM statement may cause the analysis engine to store the function declaration in a route model (e.g., such as the illustrative route model 330 shown in FIG. 3). \nPERFORM .ltoreq.model.routes[$1][$2].callbacks=[$f] WHEN &lt;{@RequestMapping(value=$1, method=RequestMethod.  $2) function $3(_){ } AS $f}&gt;\n Additionally, or alternatively, a PERFORM statement may be used to inject data into an AST (e.g., an AST compiled directly from input program code, or a reduced AST constructed by removing certain information).  For instance, in the following\nillustrative PERFORM statement, the WHEN clause may specify a pattern where an assignment of a variable $x includes a call to getStringParameter or getRawParameter.  If an analysis engine finds a match of this pattern, the analysis engine may add a field\nnamed tainted to the matched variable and set the value of that field to be true.  In some embodiments, the analysis engine may be programmed to propagate the value of the tainted field.\n PERFORM $x.tainted=true\n WHEN &lt;{$x=getStringParameter( ) OR getRawParameter( )}\n In some embodiments, data maintained in an added field may be used to facilitate property checking and/or code editing.  For instance, the following illustrative property query may be used to check if an argument of a call to a method of an\nobject is tainted, and if so, replace the argument with a sanitized version of the argument.\n &lt;{$o.$f($1)}&gt;\n WHERE $1.tainted==true\n REWRITE $1&lt;=SanitizerAPI.sanitize($1)\n FIG. 9 shows a transformation of an illustrative AST 900 to a transformed AST 905, in accordance with some embodiments.  For instance, this transformation may be performed by an analysis engine in executing the illustrative property query\ndescribed above to syntactically replace a subtree 910 assigned to the free variable $1 with a different subtree at a node 915.  The new subtree may correspond to applying the sanitize function in the SanitizerAPI library to the argument $1, and may be\nconstructed by attaching the subtree 910 to the node 915 as the argument of SanitizerAPI.sanitize.\n FIG. 10 shows an illustrative source program 1050 and an illustrative property query 1055, in accordance with some embodiments.  In this example, the source program 1050 may implement a bitwise comparison between two bit strings, which may be\ncryptographic digests such as CRCs (cyclic redundancy checks) or HMACs (keyed-hash message authentication codes).  This particular implementation may be vulnerable to side-channel attacks because execution time of the for loop may be input dependent. \nFor instance, the for loop may exit early if a difference is detected early in the bit strings, and may run through the entire lengths of the bit strings if the bit strings are identical.  This type of comparison is sometimes called a \"fail fast\"\ncomparison.\n In some embodiments, a property query may program an analysis engine to detect \"fail fast\" comparisons.  For instance, in the example shown in FIG. 10, the property query 1055 may be written using a data flow operator USE, which may cause the\nanalysis engine to search for a function declaration that has two byte arrays as arguments ($a and $b) and includes a for loop with an if statement in the body of the for loop, where the condition ($1) of the if statement depends on both of the byte\narray arguments ($1 USE $a AND $2 USE $b).  Thus, the property query 1055 may cause the analysis engine to perform a combination of syntactic matching and data flow analysis to detect a \"fail fast\" comparison.\n In some embodiments, a property query may program an analysis engine to remove a vulnerability caused by a \"fail fast\" comparison.  FIG. 11 shows an illustrative property query 1100, in accordance with some embodiments.  Like the illustrative\nproperty query 1055 shown in FIG. 10, the property query 1100 may program an analysis engine to detect a \"fail fast\" comparison.  Additionally, the property query 1100 may cause the analysis engine to assign a syntactic element (e.g., a subtree in an\nAST) corresponding to the for loop to a free variable $body, for example, using an AS operator at 1110.  At 1115, the property query 1100 may cause the analysis engine to replace the syntactic element assigned to $body with a new body 1105, resulting in\na transformed function declaration.  The transformed for loop may not exit early, even if a difference has been detected, thereby removing the vulnerability to side-channel attacks.\n E. Analysis Engine API Functions\n In some embodiments, a query language may be provided that includes one or more API functions for accessing internal state of an analysis engine and/or programming how the analysis engine performs an analysis.  The inventors have recognized and\nappreciated that an analysis engine may maintain useful information, such as ASTs (e.g., ASTs compiled directly from input program code, and/or reduced ASTs constructed by removing certain information), variable values, variable types, analysis results,\ninternal data structures, relationships between internal data, etc. Accordingly, a query language may implement a protocol for exposing some or all of the information maintained by the analysis engine.\n For example, an analysis engine may maintain a function closure as an internal representation of a function in an AST.  This closure may include information such as an original AST, parent scope, type information, member declarations within a\nbody of the function body, etc. In some embodiments, an API construct, $f.ast, may be used to obtain an AST stored by the analysis engine for the syntactic element assigned to $f, and an API construct, $f.ast.name, may be used to obtain the function name\nin the AST.  Additionally, or alternatively, the statement, FORALL v IN $f:v instanceof String, may be used to enumerate all data members in a function closure that are of the type String.\n F. Aliases, Macros, and Modules\n In some embodiments, a query language may allow definitions of aliases, macros, and/or modules.  The inventors have recognized and appreciated that such definitions may be used to enhance reusability and modularization.  However, it should be\nappreciated that aspects of the present disclosure are not limited to the use of any alias, macro, or module.\n In some embodiments, a keyword let may be used to introduce an alias.  An example is as follows.\n let source=getStringParameter( ) OR getRawParameter( )\n With this illustrative alias, the following queries are equivalent.  PERFORM $x.tainted=true WHEN &lt;{$x=source&gt;} PERFORM $x.tainted=true WHEN &lt;{$x=getStringParameter( ) OR getRawParameter( )&gt;}\n In some embodiments, a keyword DEFINE may be used to introduce a macro.  An example is as follows.\n DEFINE isStatement(v) {v instanceof java.sql.Statement;}\n With this illustrative macro, the following queries are equivalent.\n MATCH &lt;{$o.$f($1)}&gt; WHERE isStatement($o) MATCH &lt;{$o.$f($1)}&gt; WHERE $0 instanceof java.sql.Statement;\n In some embodiments, a keyword IMPORT may be used to load one or more query definitions from a query source file.  This construct may advantageously allow query definitions to be modularized.\n FIG. 12 shows an illustrative network 1200 of modules, in accordance with some embodiments.  The network 1220 may include a node Module 1 corresponding to a first query source file 1205, a node Module 2 corresponding to a second query source\nfile 1210, and a node Module 3 corresponding to a third query source file 1215.  The first query source file 1205 may include a framework model for an MVC architecture, the second query source file 1210 may include a framework model for a Node.js runtime\nenvironment, and the third query source file 1215 may include a framework model for an Express framework.\n In example shown in FIG. 12, the first query source file 1205 may be imported into the second query source file 1210 via an IMPORT statement, so that queries in the Node.js framework model may make use of query definitions in the MVC framework\nmodel.  Similarly, the second query source file 1210 may be imported into the third query source file 1215 via an IMPORT statement, so that queries in the Express framework model may use of query definitions in the Node.js framework model and/or the MVC\nframework model.\n The inventors have recognized and appreciated that an organization of modules such as that shown in FIG. 12 may improve reusability of query definitions.  However, it should be appreciated that aspects of the present disclosure are not limited\nto the use of modules for organizing query definitions.\n F. Libraries and High-Level Queries\n The inventors have recognized and appreciated that it may be beneficial to store certain commonly used query definitions in a library, so that these definitions may be accessed by simply loading the library.  For example, query definitions for\ndiscovering and/or manipulating MVC components for web applications may be stored in a library, and definitions for discovering and/or manipulating MVC components for mobile apps (e.g., for an Android.TM.  operating system and/or an iOS.TM.  operating\nsystem) may be stored in the same or a different library.\n FIG. 13 shows an illustrate set of nouns that may be used in a query language for accessing components in an MVC architecture, in accordance with some embodiments.  In some embodiments, an MVC library may be provided that includes one or more\npredefined queries for discovering and/or manipulating MVC components.  The MVC library may allow a user to use the nouns shown in FIG. 13 as high-level keywords in the query language.\n In some embodiments, an MVC library may include one or more discovery queries that program an analysis engine to build MVC component models.  For instance, an analysis engine may run the discovery queries on input program code and build the\nfollowing illustrative model.\n TABLE-US-00004 _model = { config: { ...  }, MVC: [ {model: ..., controller: {action1: ..., action2: ...  }, view: ...}, model: ..., controller: {action1: ..., action2: ...  }, view: ...}, ] }\n FIG. 14 shows an illustrative hierarchy 1400 of MVC components, in accordance with some embodiments.  For example, the hierarchy 1400 may represent MVC components from the above illustrative model, where two actions have been discovered for\ncontroller1, but no action has been discovered for controller 2 yet.\n In some embodiments, the nouns shown in FIG. 13 may be used to access MVC component models such as those shown in FIG. 14.  Any suitable high-level language constructs may be used to query MVC nouns.  For example, a query may use Xpath, Jquery,\nor CSS-like search, and may conveniently return a set of one or more elements.\n As one example, the following high-level query written using an Xpath syntax may be used to select all routings implementing a method for a GET request.\n //route[@method=`get`]\n In some embodiments, this high-level query may be implemented as follows.\n TABLE-US-00005 var res = [ ]; for (var r of_model.route) { (if r[`get`] != null) res.push[r] ; } return res;\n As another example, the following high-level query written using an Xpath syntax may be used to select the last view in an application.  A low-level implementation may be similar to the illustrative implementation shown above for\n//route[@method=`get`].\n /app/view[last( )]\n As another example, the following high-level query written using an Xpath syntax may be used to select all views having a parent in an AST such that the parent has at least three child nodes.  A low-level implementation may be based on how an\nXpath interpreter processes such a query.\n //view[@ast.parent.children.num&gt;2]\n In some embodiments, relationships between nouns may be expressed using verbs, where a verb may be syntactic sugar for a low-level implementation.  As one example, a verb bound may have the following syntax.\n &lt;View(v)&gt;bound&lt;Controller(_)&gt;\n This statement may be implemented as follows.\n EXISTS c IN _model.controller:_model.controller[c].view==v\n As another example, a verb manipulate may have the following syntax.\n &lt;ViewResolver(_)&gt; manipulate &lt;View (v)&gt;\n This statement may be implemented as follows.\n EXISTS r IN _model.view[v]:_model.view[v][r].resolver !=null\n As another example, a verb call may have the following syntax.\n &lt;Request(r)&gt; call &lt;Function(f)&gt;\n This statement may be implemented as follows.\n _model.request[r].handler=f\n As another example, a verb phrase set .  . . to . . . may have the following syntax.\n &lt;Session&gt; set &lt;Field(f)&gt; to &lt;Value(v)&gt;\n This statement be implemented as follows.\n _model.session[f]=v\n The inventors have appreciated that, in some instances, nouns and verbs may be more convenient to use than the basic constructs of a query language.  However, it should be appreciated that aspects of the present disclosure are not limited to the\nuse of nouns or verbs to supplement the syntax of a query language.  Furthermore, the techniques described here may be applied to software architectures other than MVC, as aspects of the present disclosure are not so limited.\n IV.  Model-Based Analysis of Software Applications\n Scalable analysis of complex and large software applications has remained a challenge for a long time.  An application may contain many components, use various external components (e.g., frameworks, libraries, middleware, etc.), and exhibit a\ncomplex architecture.  The inventors have recognized and appreciated that there may be a tradeoff between scalability and accuracy.  Accurate analysis often involve detailed modeling and rigorous checking, which may provide a deep understanding of\nsemantics of an application, but may require significant time and effort (e.g., both for a human to formulate an analysis and for a machine to perform the analysis).  Accordingly, it may be beneficial to provide analysis techniques with improved\nscalability and accuracy.\n The inventors have recognized and appreciated that some solutions may sacrifice accuracy for scalability, while others may sacrifice scalability for accuracy.  For example, syntactic analysis (e.g., based on grep) may be used to retrieve\ninformation from source code, and data flow analysis (e.g., based on bit propagation) may be used to understand how data is used by an application.  The inventors have recognized and appreciated that these techniques may involve over-approximations,\nwhich may lead to false positives.\n On the other hand, dynamic analysis techniques may apply fewer approximations (e.g. on relationships between components or on variables values) and therefore may be more accurate.  However, the inventors have recognized and appreciated that\ndynamic analysis techniques may have low coverage (e.g., due to computational constraints), which may lead to false negatives.\n The inventors have recognized and appreciated that, as more external components such as frameworks and libraries are used in software applications, and as software architectures become more complex, it may be more difficult to achieve both\naccuracy and scalability.  Although a user may model and analyze various portions of an application separately, such an ad hoc approach may be not only tedious, but also unreliable, as interactions between the separately modeled portions may not be\nmodeled adequately.\n Accordingly, in some embodiments, techniques are provided for achieving a desirable balance between scalability and accuracy.  For example, one or more pieces of information, including, but not limited to, software architecture (e.g., presence\nof one or more components and/or connections between components), program semantics, domain knowledge (e.g., regarding one or more frameworks, libraries, middleware, etc.), may be used to focus an analysis engine on one or more portions of an application\nthat are relevant for a particular analysis.  In some embodiments, such information may be explicitly recorded in one or more models.\n In some embodiments, an analysis engine may be programmed to construct an application architecture model for a software application.  The application architecture model may include models for individual components in an architecture.  Given a\ncertain property of interest, the analysis engine may select one or more relevant component models.  The analysis engine may then check the property of interest against the selected component models.  Using such a divide-and-conquer approach, the amount\nof information analyzed by the analysis engine may be reduced, while the risk of missing some relevant information may also be reduced because the component models are constructed based on knowledge of the application's architecture.\n In some embodiments, an analysis engine may be programmed to perform incremental analysis as a software application evolves.  For example, when a portion of source code is revised or added, the analysis engine may determine one or more component\nmodels that are affected, and may re-generate and/or re-analyze only the affected component models.  This may significantly improve the analysis engine's response time and hence user acceptance.\n In some embodiments, an analysis engine may be programmed to analyze an application adaptively.  For instance, given a certain property of interest, the analysis engine may select one or more types of models that may be suitable for use in\nchecking that property.  The analysis engine may then construct and analyze one or more models of a selected type.  In some embodiments, a model may be constructed by abstracting away information that is irrelevant for the property to be checked, thereby\nimproving efficiency of the analysis engine.\n FIG. 15 shows an illustrative network 1500 of models that may be used to facilitate analysis of a software application, in accordance with some embodiments.  For instance, the illustrative models shown in FIG. 15 may be used by an analysis\nengine (e.g., the illustrative analysis engine 105 shown in FIG. 1) to check input program code 1505 with respect to one or more properties of interest.\n In the example shown in FIG. 15, the input program code 1505 may use one or more external components 1515.  Examples of external components include, but are not limited to, frameworks, libraries, middleware, etc. Framework models 1520 for the\nexternal components 1515 may be built using a query language (e.g., via discovery queries), and may represent abstractions of the external components 1515 (e.g., for purposes of security analysis) and/or interactions between the external components 1515. In some embodiments, framework models may be indexed and stored in a database, and may be retrieved as needed.\n In some embodiments, the input program code 1505 may be compiled into a suitable representation, such as an AST 1510.  A reduced AST 1525 may then be constructed by applying one or more discovery queries from the framework models 1520 to extract\nrelevant information from the AST 1510.  For instance, the discovery queries may be used to identify and extract information in the AST 1510 that is relevant for security analysis, and the extracted information may be stored in the reduced AST 1525.\n In the example shown in FIG. 15, the framework models 1520 and the reduced AST 1525 are used to construct an application architecture model 1530.  The application architecture model 1530 may include high-level information such as software\narchitecture (e.g., one or more components and/or connections between the components), program semantics, and/or domain knowledge (e.g., regarding one or more frameworks, libraries, middleware, etc.).  For example, the application architecture model 1530\nmay include models for individual components in a software architecture, such as component model 1, component model 2, component model 3, etc. shown in FIG. 15.\n In the example shown in FIG. 15, the network 1500 further includes a property model 1535.  In some embodiments, an analysis engine may receive as input a property query 1540, which may capture semantics of a property of interest (e.g., a certain\nsecurity property).  Based on the property query 1540, the analysis engine may select an appropriate property model type and construct a property model of the selected type.  For instance, the property model 1535 may be of the selected type, and may be\nderived by the analysis engine from the reduced AST 1525 and/or the application architecture model 1530.  The analysis engine may then check the property model 1535 to determine if the property of interest is satisfied.\n In some instances, the application architecture model 1530 may include sufficient high-level information to allow an analysis engine to determine if a certain property is satisfied, without analyzing low-level source code.  This may allow the\nanalysis engine to produce a result more quickly, thereby improving user experience.  For example, values of configuration parameters may be extracted from input program code and may be stored in the application architecture model 1530 (e.g., in a\ntable).  When one or more such values are needed, an analysis engine may simply retrieve the one or more needed values from the application architecture model 1530, without having to look for such values in the input program code.  However, it should be\nappreciated that aspects of the present disclosure are not limited to storing configuration parameter values in an application architecture model.\n It should be appreciated that details of implementation are shown in FIG. 15 and described above solely for purposes of illustration, as aspects of the present disclosure are not limited to any particular manner of implementation.  For instance,\naspects of the present disclosure are not limited to the use of any reduced AST.  In some embodiments, the AST 1510, instead of the reduced AST 1525, may be used to generate the application architecture model 1530.\n FIG. 16 shows illustrative framework models 1600 and 1605, in accordance with some embodiments.  The framework models 1600 and 1605 may be used by an analysis engine (e.g., the illustrative analysis engine 300 shown in FIG. 3) to generate an\napplication architecture model (e.g., the illustrative application architecture model 310 shown in FIG. 3).\n The inventors have recognized and appreciated that an external component used by a software application (e.g., framework, library, middleware, etc.) may include a large amount of code.  For example, the Express framework's source code includes\naround 12,000 lines of JavaScript code.  Therefore, it may be desirable to provide an abstraction that represents semantics of a resource in a concise way.  Without such an abstraction, an analysis engine may be unable to analyze a resource quickly\nenough to deliver results in real time.\n In some embodiments, a framework model may include a specification of relevant information about a resource.  For example, a framework model may be defined using a query language having one or more constructs such as the illustrative constructs\nshown in FIG. 8 and discussed above.\n In the example shown in FIG. 16, the framework models 1600 and 1605 represent semantics of the Express framework and the Express Session middleware, respectively.  For instance, the framework model 1600 may reflect how routes are defined. \nAdditionally, or alternatively, the framework model 1600 may define framework APIs.  In some embodiments, the framework model 1600 may include about 100 lines of code, which is a significant reduction from the actual size of the Express framework (about\n12,000 lines).\n FIG. 16 shows an illustrative source code fragment 1610 that uses the Express framework and the Express Session middleware.  In some embodiments, an analysis engine may be programmed to replace references to the Express framework and the Express\nSession middleware with references to the respective framework models, resulting in illustrative code fragment 1615.  In this manner, framework models (e.g., the illustrative framework models 1600 and 1605 shown in FIG. 6) may be loaded, rather than\nsource code of the Express framework and the Express Session middleware.\n FIG. 16 also shows an illustrative source code fragment 1620 that uses an HTTP middleware and a Path middleware.  The inventors have recognized and appreciated that some external components may not be relevant for a property of interest and\ntherefore a model for such a resource need not be defined or loaded.  This may reduce complexity and thereby improve performance of an analysis engine.\n In some embodiments, one or more of the following properties may be of interest.\n 1.  Is an httpOnly flag set to true in a session cookie?\n 2.  In any route related to /users, is there a JavaScript injection?\n 3.  In any route related to user signup, is a user name properly checked?\n For these properties, session cookie and routes may be relevant, whereas other middleware such as HTTP and Path may not be relevant.  Accordingly, in some embodiments, an analysis engine may be programmed to ignore references to the HTTP\nmiddleware and the Path middleware, as well as all subsequent code related to the HTTP middleware and the Path middleware.  For instance, a mapping between types of properties and relevant middleware may be defined based on domain knowledge, and the\nanalysis engine may be programmed to use the mapping to identify middleware that may be ignored.\n FIG. 17 illustrates an approach for programming an analysis engine to perform a field and type analysis, in accordance with some embodiments.  For example, a query language may be used to program the analysis engine to perform a field and type\nanalysis.  In some embodiments, the query language may include one or more constructs such as the illustrative constructs shown in FIG. 8 and discussed above.\n In some embodiments, a query language may be used to program an analysis engine to track names and types of fields in an object, and/or names and types of member functions in the object.  These names and types may be matched with known\nsignatures to infer a role of an object and/or a role of a function using the object.\n For instance, a route function in the Express framework may have the following signature, and a query language may be used to program an analysis engine to determine if a function matches this signature.\n function test(req, res, .  . . )\n The request object req may contain one or more of the following fields: body session etc.\n The response object res may contain one or more of the following functions: render, with argument type String.times.Object session, with argument type String etc.\n FIG. 17 shows illustrative function declarations 1700, 1705, and 1710.  In some embodiments, the analysis engine may be programmed to determine that in the illustrative declaration 1700, a login function has two arguments, req and res, where the\nobject res has a member function render with argument type String.times.Object.  This may match the above signature, and the analysis engine may infer that login is likely a route function.  Such an inference may be made even if there is not a perfect\nmatch.  For instance, the analysis engine may infer that login is a route function even though the object req does not contain any field.\n In some embodiments, the analysis engine may be programmed to determine that in the illustrative declaration 1705, a signup function has three arguments, req, res, and next, where req has a field body, and res has a member function render with\nargument type String.times.Object and a member function redirect of argument type String.  This may match the above signature (even though the name redirect does not match the name session).  Therefore, the analysis engine may infer that signup is a\nroute function.\n In some embodiments, the analysis engine may be programmed to determine that in the illustrative declaration 1710, a test function has three arguments, req, res, and next, where req has a field body, but res has no member function.  Therefore,\nthe analysis engine may determine it is unlikely that test is a route function.\n Below are examples of queries that may be used to program an analysis engine to perform a field and type analysis (e.g., by performing syntactic pattern matching).  Looking for a function of the form f(req*, res*).  PERFORM\n_model.routes[`/UNKNOWN`][`UNKNOWN`]=f WHEN function f($1, $2) WHERE $1.ast.name.startsWith(`req`) AND $2.ast.name.startsWith(`res`) Looking for a function with a first argument that has a member function session, body, or params, or a second argument\nthat has a member function render or redirect.  PERFORM _model.routes[`/UNKNOWN`][`UNKNOWN`]=f WHEN function f($1, $2) {$1.session OR $1.body OR $1.params OR $2.render OR $2.redirect}\n In some embodiments, an analysis engine may be programmed by a framework model to perform a field and type analysis to infer a role of an object and/or a role of a function using the object.  The framework model may include one or more queries\nwritten in a query language.  An inferred role for an object (or function) may be stored in an application architecture model in association with that object (or function).  For instance, one or more discovered routes may be stored in a route model.\n FIG. 18A shows an illustrative application 1800 and illustrative component models 1805 and 1810, in accordance with some embodiments.  In this example, the application 1800 is written using the Express framework.  In some embodiments, an\nanalysis engine may be programmed to apply a framework model for the Express framework (e.g., the illustrative framework model 1600 shown in FIG. 16) to construct an application architecture model for the application 1800.  The application architecture\nmodel may include one or more component models, such as the component models 1805 and 1810 shown in FIG. 18A.  The component model 1805 may be a configuration model, and the component model 1810 may be a route model.  For instance, in some embodiments,\nthe component models 1805 and 1810 may be generated using the illustrative framework models 1600 and 1605 shown in FIG. 16.  For example, the analysis engine may interpret the framework models 1600 and 1605 the source code 1800, thereby generating the\ncomponents models 1805 and 1810 as output.\n FIG. 18B shows illustrative groups 1815, 1820, and 1825 of security issues that may be checked by an analysis engine, in accordance with some embodiments.  The inventors have recognized and appreciated that by constructing models for individual\ncomponents in an architecture, an analysis engine may be able to quickly identify relevant information to be analyzed and safely disregard irrelevant information.  As one example, to check configuration-related issues 1815 such as Cross-Site Request\nForgery (CSRF), configuration, secure transportation, session cookie safety, etc., the analysis engine may focus on the configuration model 1805.  As another example, to check per-route issues 1825 such as invalidated redirect, SQL injections, JavaScript\ninjections, etc., the analysis engine may focus on the route model 1810.  By contrast, both the configuration model 1805 and the route model 1810 may be relevant for security issues in the group 1820, so the analysis engine may analyze both models when\nchecking an issue from the group 1820.\n In some embodiments, a mapping between types of properties and respective components may be defined based on domain knowledge, and the analysis engine may be programmed to use the mapping to select one or more relevant components for a certain\nproperty to be checked.  In this manner, the amount of information analyzed by the analysis engine may be reduced, which may improve the analysis engine's performance, while the risk of missing some relevant information may also be reduced because the\ncomponent models are constructed based on knowledge of the application's architecture.\n FIGS. 18C-E show an illustrative taxonomy of security responsibilities, in accordance with some embodiments.  Such a taxonomy may be created by a security expert and may be use to characterize functional responsibilities of components in various\nframeworks.\n FIG. 18F-G show an illustrative mapping from application components to security responsibilities, in accordance with some embodiments.  For example, the mapping shown in FIG. 18F-G may be created by a security expert based on knowledge of a\nparticular framework (e.g., an MVC framework) and may map each component in the framework to one or more security responsibilities (e.g., one or more of the illustrative security responsibilities shown in FIGS. 18C-E).  Such a mapping may, in some\nembodiments, be used to select one or more relevant components for a certain security property to be checked.\n It should be appreciated that the taxonomy shown in FIGS. 18C-E and the mapping shown in FIGS. 18F-G are provided solely for purposes of illustration, as aspects of the present disclosure are not limited to any particular taxonomy of security\nresponsibilities, or to any particular way to align application components with a taxonomy.\n FIG. 19 shows a plurality of illustrative types of property models that may be used by an analysis engine to check a property of interest, in accordance with some embodiments.  For instance, an analysis engine may be programmed to determine\nwhich one or more types of property models may be appropriate for use in checking a certain property of interest.  Additionally, or alternatively, the analysis engine may be programmed to generate a property model of a selected type for a software\napplication, and analyze the property model to determine whether the software application satisfies a property of interest.\n The inventors have recognized and appreciated that different types of property models may be suitable for investigating different types of properties.  As one example, a call graph may be used to capture function call relationships, whereas a\ndata flow graph may be used to capture data dependence information (e.g., how a tainted value is propagated).  As another example, a type system may be used to record types of variables and objects.  As another example, an abstract numeric value\nestimation may be used to estimate possible values of numeric variables, whereas a string value estimation may be used to estimate possible values of string variables.  As another example, a heap shape model may be used to capture pointer relationships\nbetween components in a heap.  As another example, predicate abstraction may be used to capture relationships between values of variables.  FIG. 20 shows an illustrative mapping from types of properties to types of property models, in accordance with\nsome embodiments.\n The inventors have further recognized and appreciated that different types of property models may offer different advantages.  For instance, as shown in FIG. 19, property model types at the top (e.g., call graph, data graph, and type system) may\nbe more abstract, and hence easier to compute but less precise.  By contrast, property model types at the bottom (e.g., abstract numeric value estimation and string value estimation) may be more detailed, and hence more precise but harder to compute. \nTherefore, it may be beneficial to provide techniques for selecting an appropriate type of property model to achieve a desired balance between efficiency and accuracy.\n FIG. 21 shows an illustrative process for selecting one or more property model types and using property models of the selected types to analyze a software application, in accordance with some embodiments.  For example, the process shown in FIG.\n21 may be used by an analysis engine (e.g., the illustrative analysis engine 105 shown in FIG. 1) to check input program code with respect to one or more properties of interest.  For instance, a set of keywords may be retrieved from a property query. \nThen, for each keyword, a set of one or more relevant component models may be analyzed to generate one or more property models.\n FIG. 21 shows an illustrative application architecture model 2100.  In some embodiments, the application architecture model 2100 may be built by applying one or more framework models to input program code (e.g., as discussed above in connection\nwith FIG. 15).  The application architecture model 2100 may include high-level information such as software architecture (e.g., one or more components and/or connections between the components), program semantics, and/or domain knowledge (e.g., regarding\none or more frameworks, libraries, middleware, etc.).  For example, the application architecture model 2100 may include models for individual components in a software architecture, such as component model A and component model B shown in FIG. 21.\n FIG. 21 also shows illustrative query 1 and illustrative query 2, which may each define a property to be checked.  In some embodiments, an analysis engine may be programmed to select one or more property model types for a query such as query 1\nor query 2.  For instance, a query may be defined using a query language having one or more constructs such as the illustrative constructs shown in FIG. 8 and discussed above.  The analysis engine may be programmed to parse the query based on a syntax of\nthe query language, and to identify one or more semantic predicates from the query.  In the example shown in FIG. 21, a semantic keyword set 1 is extracted from query 1, a semantic keyword set 2 is extracted from query 2, and so on.\n In some embodiments, the analysis engine may select one or more property model types based on the identified semantic predicates.  For instance, the analysis engine may use the identified semantic predicates to match the query to one of the\nillustrative property types shown in FIG. 20, and then use the illustrative mapping shown in FIG. 20 to determine an appropriate type of property model.\n In some embodiments, the analysis engine may identify, for a component model in the application architecture model 2100 (e.g., the component model A or the component model B), one or more property model types for which the component model is\nrelevant.  For instance, the analysis engine may determine, for each query and each property model type associated with the query, whether the component model is relevant to the property model type (e.g., using one or more techniques described above in\nconnection with FIGS. 18A-B).  If the component model is determined to be relevant to the property model type, a property model of that type may be built based on that component model, and the property model may be analyzed.  A result of that analysis\nmay be output as a result for the query.  In some embodiments, the analysis engine may group and/or prioritize analysis results from checking various property models.  However, that is not required, as in some embodiments grouping and/or prioritization\nmay be performed by a guidance engine, or may not be performed at all.\n The inventors have recognized and appreciated that the illustrative process shown in FIG. 21 may be used advantageously to improve efficiency of an analysis engine.  As one example, if a semantic predicate identified from a query is concerned\nwith only types and Boolean/numeric values of some variables, then only type system analysis and numeric value estimation may be performed, and only for the variables involved.\n The inventors have further recognized and appreciated that if a property is disproved using a more abstract model, then there may be no need to build and analyze a more detailed model.  Accordingly, in some embodiments, an analysis engine may be\nprogram to perform analysis adaptively, for example, beginning with more abstract models and using more detailed models only as needed.\n FIG. 22 shows an illustrative application 2200 and an illustrative analysis of the application 2200, in accordance with some embodiments.  In this example, the application 2200 is written using the Express framework.  In some embodiments, an\nanalysis engine may be programmed to apply a framework model for the Express framework (e.g., the illustrative framework model 1600 shown in FIG. 16) to construct an application architecture model for the application 2000.  The application architecture\nmodel may include one or more component models, such as the illustrative configuration model 2215 shown in FIG. 22.\n In some embodiments, a query may be specified based on the following property, and an analysis engine may be programmed to identify from the query a semantic predicate, such as the illustrative semantic predicate 2205 shown in FIG. 22.  Is an\nhttpOnly flag set to true in a session cookie? Illustrative semantic predicate in a query language: model.setting.cookie.httpOnly==true\n In some embodiments, the analysis engine may select, based on the semantic predicate 2205, one or more types of property models.  For example, the analysis engine may determine at 2210 (e.g., using one or more techniques described in connection\nwith FIG. 21) that Boolean or numeric value estimation is to be performed for fields in session cookie.  The analysis engine may further determine (e.g., using one or more techniques described in connection with FIG. 21) that the configuration model 2215\nis relevant for Boolean or numeric value estimation for fields in session cookie.  The analysis engine may then perform Boolean or numeric value estimation for fields in session cookie on the configuration model 2215 and output a result that the httpOnly\nflag is not set to true in session cookie.\n FIG. 23 shows illustrative program code 2300 and an illustrative analysis of the program code 2300, in accordance with some embodiments.  The program code 2300 may be an implementation of the illustrative application 2200 shown in FIG. 22.\n In some embodiments, a query may be specified based on the following property, and an analysis engine may be programmed to identify from the query a semantic predicate, such as the illustrative semantic predicate 2305 shown in FIG. 23.  In any\nroute related to/users, is there a JavaScript injection? Illustrative semantic predicate in a query language: &lt;{eval($1)}&gt; WHERE $1.tainted=true\n In some embodiments, the analysis engine may select, based on the semantic predicate 2305, one or more types of property models.  For example, the analysis engine may determine at 2310 (e.g., using one or more techniques described in connection\nwith FIG. 21) that data flow analysis is to be performed to calculate \"tainted\" values for route functions related to/users.  The analysis engine may then analyze the program code 2300 (or an AST of the program code 2300) and construct a data flow graph\n2315.  Using the data flow graph 2315, the analysis engine may determine that JavaScript injections are present at eval(body.preTax) and eval(body.afterTax), and may output a result at 2320 accordingly.\n FIG. 24 shows illustrative program code 2400 and an illustrative analysis of the program code 2400, in accordance with some embodiments.  The program code 2400 may be an implementation of the illustrative application 2200 shown in FIG. 22.\n In some embodiments, a query may be specified based on the following property, and an analysis engine may be programmed to identify from the query a semantic predicate, such as the illustrative semantic predicate 2405 shown in FIG. 24.  In any\nroute related to user signup, is a user name properly checked (e.g. can the user name be empty when the user name is used for redirecting a page)? Illustrative semantic predicate in a query language: &lt;{$0.redirect(_+$2)}&gt; WHERE $2==` `.\n In some embodiments, the analysis engine may select, based on the semantic predicate 2405, one or more types of property models.  For example, the analysis engine may determine at 2410 (e.g., using one or more techniques described in connection\nwith FIG. 21) that variable value estimation is to be performed for userName.  The analysis engine may then perform variable value estimation for userName and output a result that the user name must contain one to 20 characters.\n FIG. 25 shows an illustrative application architecture model 2500, in accordance with same embodiments.  Like the illustrative application architecture model 1530 shown in FIG. 15, the application architecture model 2500 in the example of FIG.\n25 includes models for individual components in a software architecture.  In some embodiments, the application architecture model 2500 may be an updated version of the application architecture model 1530.  For example, an analysis engine may be\nprogrammed to update the application architecture model 1530 based on code changes to generate the application architecture model 2500.\n The inventors have recognized and appreciated that when a developer modifies program code (e.g., by revising existing code and/or adding new code), regenerating the entire application architecture model 1530 may involve unnecessary computation. \nFor example, the code changes may affect only some, but not all, of the component models in the application architecture model 1530.  The inventors have recognized and appreciated that regenerating an unaffected component model may result in an identical\ncomponent model.  Accordingly, in some embodiments, techniques are provided for identifying one or more component models affected by certain changes and regenerating only the affected component models, which may improve an analysis engine's response time\nsignificantly.\n The inventors have further recognized and appreciated that when a developer modifies program code (e.g., by revising existing code and/or adding new code), re-checking a property that is unaffected by the code changes may involve unnecessary\ncomputation.  Accordingly, in some embodiments, techniques are provided for determining if a property is affected by certain code changes.  An analysis engine may re-check only properties that are affected, which may also improve the analysis engine's\nresponse time significantly.\n In the example shown in FIG. 25, code changes include code revision 2505.  An analysis engine may be programmed to identify one or more component models (e.g., component model 2) that are affected by the code revision 2505.  For example, if the\ncode revision 2505 involves changes to a certain function only, and the function relates to a route definition, then the analysis engine may re-analyze only that route.  Previous results relating to unchanged code may still be valid.\n In the example shown in FIG. 25, code changes include new code 2510.  In some embodiments, the analysis engine may be programmed to determine if the new code 2510 adds a component to the software application that is being analyzed.  If it is\ndetermined that the new code 2510 adds a component to the software application that is being analyzed, the analysis engine may generate a new component model N, as shown in FIG. 25.  The analysis engine may be further programmed to determine if any\nproperty is affected by the presence of the new component model N. If it is determined that a property is affected by the presence of the new component model N, the analysis engine may re-check that property.\n In some embodiments, one or more incremental analysis techniques, such as those described in connection with FIG. 25, may be used to construct an application architecture model asynchronously.  For example, different components in a software\napplication may become available at different times.  Whenever a new component becomes available, a new component model may be generated for that component, and affected properties may be re-checked.  In this manner, an analysis engine may be able to\nreturn results quickly at each incremental step, rather than doing all of the computations after all components have become available.\n FIG. 26A shows an illustrative application 2600 and an illustrative implementation 2605 of route functions in the application 2600, in accordance with some embodiments.  In this example, the application 2600 includes a revision at 2610 to an\nassignment of a variable b, and the implementation 2605 includes revisions at 2615 to assignments of two variables, preTax and afterTax, as well as a new route function logout at 2620.\n FIG. 26B shows an illustrative revised configuration model 2625 and an illustrative revised route model 2635, in accordance with some embodiments.  For instance, an analysis engine may be programmed to determine that the revision at 2610 of FIG.\n26A affects only the configuration model, and to generate the revised configuration model 2625 to reflect, at 2630, the revision to the assignment of the variable b. Furthermore, the analysis engine may be programmed to determine that only properties\n2650 are affected by a change in the configuration model.  Therefore, the analysis engine may check only the properties 2650 against the revised configuration model 2625.\n Similarly, the analysis engine may be programmed to determine that the revisions at 2615 and 2620 of FIG. 26A affect only the route model, and to generate the revised route model 2635 to reflect, at 2640, the new route function logout and, at\n2645, the revisions to the assignments of preTax and afterTax.  Furthermore, the analysis engine may be programmed to determine that only properties 2655 are affected by a change in the route model.  Therefore, the analysis engine may check only the\nproperties 2655 against the revised route model 2635.\n V. Graph-Based Analysis of Software Applications\n In some embodiments, one or more results output by an analysis engine may be consumed by a guidance engine programmed to provide customized and actionable guidance to a developer when a problem is identified.  For instance, the guidance engine\nmay be programmed to select, based on the one or more results output by the analysis engine, an appropriate modality for aiding a user who wrote the input program code.  Additionally, or alternatively, the guidance engine may be programmed to select,\nbased on the one or more results, appropriate content from a content store.  For example, if the one or more results includes a finding indicative of a security vulnerability, the guidance engine may present to the user a textual or video message\nexplaining the vulnerability, and/or an in-depth training module.  Additionally, or alternatively, if the one or more results includes a suggested modification to the input program code, the guidance engine may present to the user a textual or video\nmessage explaining the suggested modification, and/or modified program code ready to be tested in a sandbox.\n FIG. 27 shows an illustrative system 2700 for providing guidance to a developer, in accordance with some embodiments.  In this example, the system 2700 includes a guidance engine 2705, a knowledge base 2710, an information repository 2715, an\nanalysis engine 2720, and user profiles 2725.  In some embodiments, the guidance engine 2705 may be an implementation of the illustrative guidance engine 110 shown in FIG. 1, and the analysis engine 2720 may be an implementation of the illustrative\nanalysis engine 105 shown in FIG. 1.\n In some embodiments, the knowledge base 2710 may store a collection of information regarding software development.  For example, the knowledge base 2710 may store information regarding certain security vulnerabilities and/or how such\nvulnerabilities manifest in different types of software (e.g., software written using different languages, frameworks, libraries, etc.).  Additionally or alternatively, the knowledge base 2710 may store information indicating how certain security\nvulnerabilities may be patched (e.g., suggested code transformations to fix identified problems).  However, it should be appreciated that the techniques described herein may be used to provide guidance relating to any suitable type of properties (e.g.,\ncorrectness, robustness, safety, liveness, etc.) in addition to, or instead of, security properties.\n The information stored in the knowledge base 2710 may be represented in any suitable manner.  For instance, in some embodiments, the knowledge base 2710 may include a knowledge graph having one or more nodes and/or one or more edges.  Each node\nmay represent a certain concept, such as a code transformation, a condition, a framework, a piece of metadata, a constraint (e.g., a functionality to be preserved when fixing an identified problem), etc. Each edge may represent a relationship between a\nsource node and a target node, where the target node may be different from, or the same as, the source node.\n For example, one or more nodes in the knowledge graph may correspond, respectively, to one or more nouns in a query language (e.g., the illustrative nouns shown in FIG. 13).  An edge may correspond to a verb that expresses a relationship between\ntwo nouns.  However, it should be appreciated that aspects of the present disclosure are not limited to the use of a knowledge graph with nouns and verbs.\n FIG. 28 shows an illustrative knowledge graph 2800, in accordance with some embodiments.  In this example, the knowledge graph 2800 includes two nodes, 2805 and 2810.  The node 2805 may represent a software development framework (e.g., version\n1.0 of a framework X), and the node 2810 may represent another software development framework (e.g., version 1.1 of the framework X).  The knowledge graph 2800 may further include an edge 2815 from the node 2805 to the node 2810.  The edge 2815 may\nrepresent a \"Replace\" relationship.  For example, the edge 2815 may indicate that if a certain pattern (e.g., a vulnerability Y) is identified, then the framework represented by the node 2805 (e.g., version 1.0 of the framework X) should be replaced by\nthe framework represented by the node 2810 (e.g., version 1.1 of the framework X).\n It should be appreciated that the knowledge graph 2800 is shown in FIG. 28 and discussed above solely for purposes of illustration, as aspects of the present disclosure are not limited to the use of any particular knowledge graph, or any\nknowledge graph at all.\n Returning to the example of FIG. 27, the guidance engine 2705 may, in some embodiments, be programmed to submit questions to the knowledge base 2710, and the knowledge base 2710 may be programmed to provide answers to the guidance engine 2.  For\ninstance, the guidance engine 2705 may submit a query such as, \"how to fix the vulnerability Y if version 1.0 of the framework X is used?\" The knowledge base 2710 may return an answer such as, \"update to version 1.1 of the framework X.\"\n In some embodiments, the knowledge base 2710 may answer queries based on information stored in a knowledge graph (e.g., the illustrative knowledge graph 2800 shown in FIG. 28).  For instance, the knowledge base 2710 may be programmed to match\ninformation provided in a query (e.g., version 1.0 of the framework X being used, the vulnerability Y being identified, etc.) to one or more relevant nodes (e.g., the illustrative node 2805 shown in FIG. 28) and/or one or more relevant edges (e.g., the\nillustrative edge 2815 shown in FIG. 28).\n In some embodiments, the knowledge base 2710 may be dynamically updated.  For instance, the guidance engine 2705 may be programmed to provide feedback to the knowledge base 2710, which may cause the knowledge base 2710 to add, delete, and/or\nmodify one or more pieces of stored information.  Such feedback may be generated in any suitable manner, for example, based on input received from a developer (e.g., an instruction to fix a certain vulnerability in a certain way).  Alternatively, or\nadditionally, the knowledge base 2710 may be updated by one or more experts (e.g., security experts) based on new knowledge (e.g., newly discovered vulnerabilities).\n In the example of FIG. 27, the guidance engine 2705 is programmed to receive input via the information repository 2715.  For instance, in some embodiments, the guidance engine 2705 and the information repository 2715 may be implemented using a\nblackboard architecture.  For instance, the information repository 2715 may include a blackboard component for storing problems, solutions, suggestions, and/or other information, while the guidance engine 2705 may include one or more workers programmed\nto pull information from, and/or push information onto, the blackboard component of the information repository 2715.\n In some embodiments, the information repository 2715 may include information received from one or more sources other than the guidance engine 2705.  For instance, the information repository 2715 may store information received from the analysis\nengine 2720.  Examples of such information include, but are not limited to, one or more analysis results, one or more portions of source code and/or representations thereof (e.g., an abstract syntax tree), one or more models (e.g., an application\narchitecture model, a property model, etc.), etc.\n Additionally, or alternatively, the information repository 2715 may store information from one or more user profiles 2725.  Examples of such information include, but are not limited to, one or more preferences of a developer and/or contextual\ninformation associated with a software application.  Any suitable contextual information may be stored, such as an indication of an industry for which the software application is developed, whether the software application handles personal financial\ninformation (e.g., credit card numbers), etc. In some embodiments, such information may be used by the guidance engine 2705 to determine whether there is a problem, and/or how to fix the problem.  For instance, if the software application handles\npersonal financial information, the guidance engine 2705 may check whether an appropriate encryption algorithm is used in compliance with relevant regulation.\n FIG. 29 shows an illustrative implementation of the guidance engine 2705, in accordance with some embodiments.  In this example, the guidance engine includes a moderator 2800 programmed to moderate activities of a plurality of workers, such as\nevent worker 2905, knowledge worker 2910, impact worker 2915, etc. For instance, the moderator 2800 may be programmed to control when each worker accesses information from, and/or adds contributions to, the information repository 2715.  In this manner,\nthe moderator 2800 may facilitate a process by which the workers collectively solve a problem (e.g., identifying a security vulnerability and/or providing guidance to address the security vulnerability).  The moderator is also configured to determine\nwhether the result received from any expert is stale with respect to data being used by the expert to obtain that result--the moderator is capable of rejecting such stale results out of hand, in accordance with an embodiment.\n In some embodiments, a worker may be programmed to handle a particular aspect of a problem.  As one example, the event worker 2905 may be programmed to identify, from the information repository 2715, events that match a particular pattern.  For\ninstance, the event worker 2905 may be programmed to identify events that match a pattern indicative of a vulnerability Y. If such an event is identified, the event worker 2905 may add a new event to the information repository 2715, indicating that the\nvulnerability Y is identified.  In some embodiments, the new event may include information regarding how the vulnerability Y manifests in a portion of source code (e.g., function declaration, variable assignment, configuration parameter value, etc. that\ngive rise to the vulnerability).\n As another example, the knowledge worker 2910 may be programmed to analyze an event from the information repository 2715 and formulate a query to be submitted to the knowledge base 2710.  For instance, the knowledge worker 2910 may be programmed\nto analyze an event indicating that a certain vulnerability (e.g., the vulnerability Y) is identified in a certain software application.  The knowledge worker 2910 may be programmed to identify a framework using which the software application is\nimplemented (e.g., version 1.0 of a framework X), and to formulate a query based on the identified framework (e.g., \"how to fix the vulnerability Y if version 1.0 of the framework X is used?\").  The knowledge worker 2910 may submit the query to the\nknowledge base 2710 and receive an answer (e.g., \"update to version 1.1 of the framework X\").  The knowledge worker 2910 may then add the answer to the information repository 2715.\n As another example, the impact worker 2915 may be programmed to analyze a proposed code transformation and identify potential impact and/or mitigation strategy.  For instance, if the proposed code transformation includes encrypting user\ncredentials using a new algorithm, the impact worker 2915 may determine that previously stored credentials should be decrypted and then re-encrypted using the new algorithm, or existing users may not be able to log in. The impact worker 2915 may then add\nthe identified impact and/or mitigation strategy to the information repository 2715.\n FIG. 34 shows a visual representation of an exemplary query to be submitted to knowledge base 2710 by guidance engine 2705, in accordance with an embodiment.  In accordance with an embodiment, these queries are provided in accordance with SPARQL\n(SPARQL Protocol and RDF Query Language) language constructs, although one skilled in the art will appreciate that any language usable for querying may be substituted.  The query of FIG. 34 may be submitted, as discussed above by way of non-limiting\nexample, by a worker such as knowledge worker 2910 of FIG. 29.  FIGS. 33, 35, and 36 show a visual representation of a mitigation strategy (including code transformations) responsive to the query of FIG. 34, in accordance with an embodiment.  The\nresponsive mitigation strategies of FIGS. 33, 35, and 36 are themselves presented as a SPARQL query, in accordance with an embodiment.\n In accordance with an embodiment (including with SPARQL), the query is provided as a graph-based query.  The query specifies a portion of a graph, including the holes that the query engine of knowledge base 2710 would need to fill in to respond\nto the query.  One skilled in the relevant arts will appreciate that other structures may be used in order to present the queries of FIGS. 33-36, and the use of the graph-based query constructs shown therein is provided by way of non-limiting example.\n In FIGS. 33-36, solid lines are used for portions of the graph that must exist in a matching result.  Dotted lines are used to show elements of the query that may be matched against and returned.  The nodes within the graphs represent concepts,\nand edges between the nodes are relationships between those concepts (e.g., application \"_:Prg\" 3410 in FIG. 34 has a relationship \"usesFramework\" with framework \"Angular\" 3412).  In accordance with an embodiment, the relationships determine which type\nof worker (e.g., knowledge, event, etc.) is used in order to resolve the graph-based query.  If a node's label is prefixed with an underscore \"_\", this means that there is imprecision about the match at that point in the graph--it is known that there is\nan object that maintains that space and relationship with surrounding nodes, but its specific label or name is not known.  If the node's label is prefixed with a question mark \"?\", then not only is the name a variable, but the name is desired as part of\nthe graph provided in response to the query.  In accordance with an embodiment, edges may also follow a similar convention for variable matching in a query and for inclusion in the response graph.  A solution--the desired set of transforms--to the query\nis shown in the graphs by way of double lined arrows.\n By way of non-limiting example, the graphs may span multiple notional domains.  For example, in the graph shown in FIG. 34, three separate domains are considered by the query.  These include, in the exemplary embodiment of the query, the HTML\ndomain (e.g., the _:Page, Translate directive, and loadsApp relationship, among others), the modeling domain (e.g., programs like _:Prg, using frameworks like Angular, and the services provided by the frameworks), and static analysis domains (e.g., the\nnotions of JSON objects and members, values associated with those members, and the AST that supports that value assignment).  In accordance with an embodiment, the query is able to span any number of domains, including domains relating to source code,\nconfiguration files, and expert information, in order to provide the appropriate guidance.\n The exemplary structure of these queries allows ambiguity about things (e.g., edges and nodes of the graph) in a very specific way in order to be fully responsive to the query.  By way of non-limiting example, a match for \"_:Prg\" could be found\nin a few locations, such as in a &lt;script&gt;&lt;/script&gt; block of code, or defined in a separate JavaScript file and referenced from the HTML file, by way of non-limiting example.  That distinction should not matter from the perspective of either\nthe query to detect the issue nor in the query to correct the issue, and denoting the program as matching \"_:Prg\" allows the query to be responsive in either instance.  However, the distinction does matter at the point where the program is written back\nout from the data store, which represents a separate expert in accordance with an embodiment.\n Compartmentalizing the identification of matching programs and how the issue is corrected in this way improves the efficiency in how new checkers or fixers are plugged into the process, enabled by the ability to span the multiple relevant\ndomains.  For example, if looking for \"code that defines the sanitizer strategy provided by the Angular framework,\" it is beneficial for the query to encompass all implementations of the sanitizer strategy without the need to specifically enumerate a set\nof patterns to search for as part of the query.\n FIG. 34 illustrates the interplay between additional domains of knowledge by way of an exemplary query, in accordance with an embodiment.  For example, the query structure of FIG. 34, a select query, looks for any match on \"_:Page\" 3402, which\ncontains a TranslateDirective 3404 and a \"loadsApp\" relationship with any matching program \"_:Prg\" 3410.  In this example, the translate directive has any (wildcard) target 3406 with a property (hasProp) that is \"UserControlled\" 3408.  The\nTranslateDirective 3404 must be supported by translateProvider 3416.  Turning back to the matched program in \"_:Prg\" 3410, the match must use the Angular framework 3412, that has a version matching \"?ver\" 3414.\n As previously discussed, because \"?ver\" 3414 is denoted with a question mark, this indicates a node that the query should resolve and provide to the user.\n Separately, if the query resolution engine can resolve the hasMember relationship from translateProvider 3416 to a SanitizerStrategy 3418 (following from the previous relationships), it looks for such a response, and further optionally looks for\na matching value \"_:Sanitizer\" 3420.  If these matches are found, the query resolution engine obtains, if possible, and returns the matching \"?src\" source code 3422 that defines the matched \"_:Sanitizer\" 3420.\n FIG. 35 illustrates a responsive guidance graph providing a version transform, in accordance with an embodiment.  Following from the example of FIG. 34, the query returns a version number 3414, as part of the responsive graph, of the Angular\nframework 3412 used by a matching application \"_:Prg\" 3410.  The responsive guidance graph shows that there is a relationship between the returned version number of the Angular framework, denoted by node 3502 where the version number is \"1.0\", as denoted\nby the \"is\" relationship with node 3504 \"1.0\".  The guidance then indicates (denoted by the double arrows) that the version number of the framework should be transformed to version \"1.1\" at node 3506.\n FIG. 36 illustrates an additional responsive guidance graph provided by the guidance engine, in accordance with an embodiment.  Following from the example of FIG. 34, if information is available regarding source code \"?src\" 3422 that defines a\nmatching sanitizer \"_:Sanitizer\" 3420, then it is further checked for guidance from the guidance engine (as denoted by the question mark \"?\" preceding \"?src\").  If matching source code is found in this optional step, this means that a matching sanitizer\nstrategy has been implemented in accordance with the expected approach, such that it can be considered by the guidance engine.  In FIG. 36, assuming that matching source code is found, denoted by node 3602, then the worker looks for the source code that\nis a function declaration \"FunDecl:uuid\", denoted by the \"is\" relationship with node 3604.  If this function declaration of node 3604 is found in the source code of node 3602, then the guidance engine recommends a transform (denoted by the double arrows)\nto a different version of the function declaration \"FunDecl:uuid'\" at node 3606.\n FIG. 33 illustrates another responsive guidance graph provided by the guidance engine, in accordance with an embodiment.  The guidance provided by the guidance graph of FIG. 33 is similar to that of FIG. 36 in effect, and likewise follows from\nthe example of FIG. 34 where if information is available regarding source code \"?src\" 3422 that defines a matching sanitizer \"_:Sanitizer\" 3420, then it is further checked for guidance from the guidance engine (as denoted by the question mark \"?\"\npreceding \"?src\").  If matching source code is found in this optional step, this means that a matching sanitizer strategy has been implemented in accordance with the expected approach, such that it can be considered by the guidance engine.  In FIG. 33,\nassuming that matching source code is found, denoted by node 3302, then the worker looks for a symbolic relationship 3304 that is the left-hand side of an \"Assign:_\" 3306 assignment in the source code 3302.  In the example of FIG. 33, the symbol \"uuid\"\n3304 is a variable being assigned a value denoted by the right-hand side of the \"Assign:_\" 3306 assignment.  The responsive guidance graph indicates that the assignment should be replaced (as indicated by the wildcard \"_:_\") with a new function\ndeclaration \"FunDecl:uuid\" 3308.\n Whereas FIG. 36 describes a transform of in-line code, replacing code for FunDecl:uuid 3604 with code for FunDecl:uuid' 3606, FIG. 33 describes a transform of a similar function by replacing a reference to the function with an entirely new\nfunction 3308.  These separate approaches can each be returned by the guidance engine as appropriate depending on the manner in which the code, in this case the sanitizer code, is implemented.  In some circumstances, in-line code replacement is feasible,\nwhereas in other cases a replacement of the entire source code by having a function pointer refer to the new code is the appropriate implementation.\n The inventors have recognized and appreciated various advantages of a blackboard architecture.  For instance, in some embodiments, guidance information stored in the information repository 2715 may be reused, so that the guidance engine 2705 may\nbe able to respond to similar problems more quickly in the future.  For example, if a certain problem is identified in a first application and a certain patch is used to fix the problem, the guidance engine 2705 may suggest a similar patch when a similar\nproblem is identified in a second application developed for a similar type of organization.\n It should be appreciated that details of implementation as shown in FIGS. 27-29 and 33-36 and described above solely for purposes of illustration, as aspects of the present disclosure are not limited to any particular manner of implementation. \nFor instance, aspects of the present disclosure are not limited to the use of a blackboard architecture.  In some embodiments, a guidance engine may receive inputs directly from various sources such as an analysis engine.  Furthermore, aspects of the\npresent disclosure are not limited to the particular examples of workers shown in FIG. 29.  Any suitable worker may be used to provide any suitable expertise.  In some embodiments, one or more workers may be different instances of a same software agent. \nAdditionally, or alternatively, one or more workers may be implemented using a microservice architecture.  Such a worker may itself include a plurality of workers with different expertise.\n In some embodiments, a guidance engine may use an aspect-oriented programming (AOP) language to specify when to render a piece of advice to a developer.  For instance, a point cut expression may be written using the AOP language to specify a\nplurality of join points, where each join point may correspond to a respective pattern.  The guidance engine may be programmed to execute such a point cut expression and deliver the piece of advice to the developer when all of the patterns are matched.\n FIG. 30 shows an illustrative guidance store 3000 for storing guidance information, in accordance with some embodiments.  In some embodiments, a guidance engine (e.g., the illustrative guidance engine 2705 shown in FIG. 27) may access the\nguidance store 300 to determine what guidance to render to a developer and/or how such guidance is to be rendered.  Additionally, or alternatively, an analysis engine (e.g., the illustrative analysis engine 2720 shown in FIG. 27) may access the guidance\nstore 300 to retrieve one or more queries to be run on a software application.  In some embodiments, the guidance store 3000 may be used in addition to, or instead of, a knowledge base such as the illustrative knowledge base 2710 shown in FIG. 27.\n In the example shown in FIG. 30, the guidance store 3000 includes one or more guidance artifacts, where a guidance artifact may include a trigger specification and/or a corresponding guidance specification.  For instance, in the example shown in\nFIG. 30, a guidance artifact 3005 includes a trigger specification 3010 and a guidance specification 3020.  In some embodiments, the trigger specification 3010 may include one or more pieces of software code that, when executed by an analysis engine\n(e.g., the illustrative analysis engine 2720 shown in FIG. 27), cause the analysis engine to look for a particular problem in a software application.  The trigger specification 3010 may be written in any suitable language, such as JavaScript and/or a\nquery language having one or more of the illustrative constructs shown in FIG. 8 and described above.\n In some embodiments, the guidance specification 3020 may include one or more pieces of software code that, when executed by a guidance engine (e.g., the illustrative guidance engine 2705 shown in FIG. 27), cause the guidance engine to provide\nguidance to a developer.  For example, in response to a problem identified by executing the trigger specification 3010, the guidance engine may be programmed to execute the guidance specification 3020 and provide guidance on how to fix the problem.\n In some embodiments, the guidance artifact 3005 may include a test harness 3015 for testing the trigger specification 3010 and a test harness 3025 for the guidance specification 3020.  The test harness 3015 may include any suitable software code\nand/or test data for testing the trigger specification 3010.  Similarly, the test harness 3025 may include any suitable software code and/or test data for testing the guidance specification 3020.  However, it should be appreciated that aspects of the\npresent disclosure are not limited to the use of a test harness for a trigger specification, nor to the use of a test harness for a guidance specification.\n It should be appreciated that the guidance artifact 3005 is shown in FIG. 30 and described above solely for purposes of illustration, as aspects of the present disclosure are not limited to the use of any particular type of guidance artifact, or\nany guidance artifact at all.  For instance, in some embodiments, a guidance artifact may include only a trigger specification, without a guidance specification.  A finding identified upon executing the trigger specification may be recorded (e.g., on a\nblackboard component) as an observation of interest.  A guidance engine may render guidance as more information is collected that confirms existence of a problem, or may ultimately determine that the finding does not indicate a problem after all.\n FIG. 31 shows an illustrative decision tree 3100 that may be used by a guidance engine to determine what guidance to render to a developer and/or how such guidance is to be rendered, in accordance with some embodiments.  For instance, the\ndecision tree 3100 may represent program logic implemented by the illustrative guidance specification 3020 shown in FIG. 30.\n In the example shown in FIG. 31, the decision tree 3100 includes a plurality of decision nodes, such as nodes 3105, 3115, and 3125, and a plurality of action nodes, such as nodes 3110, 3120, and 3130.  At each decision node, a guidance engine\n(e.g., the illustrative guidance engine 2705 shown in FIG. 27) may be programmed to evaluate one or more conditions.  As one example, at the decision node 3105, the guidance engine may determine if a parameter InitBinder is declared.  If the parameter\nInitBinder is declared, the guidance engine may proceed to a next decision node.  Otherwise, the guidance engine may proceed to the action node 3010 to set up a sandbox for testing a Create function of the parameter InitBinder.\n As another example, at the decision node 3115, the guidance engine may determine if a blacklist is used.  If a blacklist is used, the guidance engine may proceed to the action node 3120 to recommend that the developer read an article on the use\nof whitelists vs.  blacklists (e.g., by presenting a link to the article).  If a blacklist is not used, the guidance engine may proceed to a different action node.\n As another example, at the decision node 3125, the guidance engine may determine if one or more missing fields in a whitelist are sensitive.  If no sensitive field is missing from the whitelist, the guidance engine may proceed to the action node\n3130 to notify the developer of the one or more missing fields.  If at least one missing field is sensitive, the guidance engine may not render any guidance.\n In some embodiments, the guidance engine may be programmed to analyze source code of a software application to evaluate a condition at a decision node.  For instance, a trigger specification (e.g., the illustrative trigger specification 3010\nshown in FIG. 30) may cause an analysis engine to store a relevant portion of source code in a shared repository (e.g., the illustrative information repository 2715 shown in FIG. 27).  The guidance engine may then retrieve the code from the shared\nrepository and evaluate the condition based on the retrieved code.  Additionally, or alternatively, the analysis engine may share one or more analysis results, and the guidance engine may evaluate the condition based on the one or more analysis results.\n It should be appreciated that the decision tree 3100 is shown in FIG. 31 and described above solely for purposes of illustration, as aspects of the present disclosure are not limited to the use of any particular decision tree, or any decision\ntree at all.  For instance, aspects of the present disclosure are not limited to traversing a single path through a decision tree to reach a single piece of guidance.  In some embodiments, a single trigger may cause multiple pieces of guidance to be\nrendered in different modalities.\n The inventors have recognized and appreciated that it may be beneficial to determine an appropriate timing for presenting guidance relating to a certain problem.  If the guidance is presented to a developer too early, the developer may ignore\nthe guidance because the developer may have more urgent issues to address.  On the other hand, if the guidance is presented too late, an impact footprint of the problem may have grown, and more effort may be needed to correct the problem.  Accordingly,\nin some embodiments, techniques are provided for measuring how important and/or urgent a problem is.  Such a measurement may be used by a guidance engine to determine when to present what guidance and/or how to present such guidance.\n In some embodiments, a priority measurement for a certain vulnerability may be based on a severity measurement for the vulnerability.  In turn, the severity measurement may be based on two measurements: a potential impact of an exploitation of\nthe vulnerability, and a likelihood of the vulnerability actually being exploited.  These measurements may be determined in any suitable manner.  In some embodiments, an impact measurement may be based on one or more of the following: Confidentiality\nThis measurement may indicate an extent to which a successful exploitation of the vulnerability may impact user confidentiality.  For instance, a measurement of 0 may indicate that no confidential information may be exposed, a measurement of 5 may\nindicate that some user information (e.g., names, email addresses, phone numbers, etc.) may be exposed, and a measurement of 10 may indicate that critical Personally Identifiable Information (PII) may be exposed (e.g., social security numbers, employee\nidentifiers, passwords, etc.).  Integrity This measurement may indicate an extent to which a successful exploitation of the vulnerability may impact data integrity.  For instance, a measurement of 0 may indicate that any exposure may be limited to\nread-only data, a measurement of 5 may indicate that some data may be changed but a scope of impact may not be critical, and a measurement of 10 may indicate all data may be compromised.  Availability This measurement may indicate an extent to which a\nsuccessful exploitation of the vulnerability may impact availability.  For instance, a measurement of 0 may indicate no impact on services, a measurement of 5 may indicate that some non-critical services may become unavailable, and a measurement of 10\nmay indicate unrecoverable downtime of all services.\n In some embodiments, a likelihood measurement may be based on one or more of the following: Accessibility This measurement may indicate how easily the vulnerability may be exploited.  For instance, a measurement of 0 may indicate that the\nvulnerability may be difficult to exploit due to constraints such as physical location (e.g., a USB drive must be physically plugged into a server in a secured datacenter), a measurement of 5 may indicate an attacker may need to overcome some constraints\nto exploit the vulnerability (e.g., via phishing emails, click-jacking attacks, etc.), and a measurement of 10 may indicate that few or no constraints may be in place to prevent an exploitation (e.g., via drive-by downloads, cross-site scripting, etc.). \nComplexity This measurement may indicate how complex a successful exploitation may be.  For instance, a measurement of 0 may indicate that little or no skill or knowledge of an application may be required to successfully exploit the vulnerability (e.g.,\nusing fully automated tools, script-kiddies, etc.), a measurement of 5 may indicate that some level of skill and/or knowledge of the application, framework, and/or environment may be needed (e.g., using some tools, custom scripts, social engineering,\netc.), and a measurement of 10 may indicate that full understanding of the application, framework, and environment, and a high level of skill may be needed (e.g., no automated exploitation or discovery tools, many custom scripts, in-person social\nengineering, physical compromise, etc.).  Authentication This measurement may indicate a level of authentication needed for a successful exploitation.  For instance, a measurement of 0 may indicate an attacker may need to be authenticated as a system or\nadministrative user and may only exploit the vulnerability against other system or administrative level users, a measurement of 0 may indicate an attacker may need to be an authenticated user and may exploit the vulnerability against any other\nauthenticated user, and a measurement of 10 may indicate that the vulnerability may be exploited anonymously against any user.\n In some embodiments, a priority measurement for a certain vulnerability may be based on a measurement of developer priority, in addition to, or instead of, severity.  A developer priority measurement may be based on one or more of the following:\nFunctionality This measurement may indicate how much impact a bug fix may have on one or more functionalities of an application.  For instance, storing passwords in the clear may be insecure, and a potential fix may be to replace the passwords with\ncorresponding cryptographic hashes.  This fix may prevent existing users from logging in, unless a corresponding change is made to a login function to compute an appropriate cryptographic hash of a password entered by the user.  Complexity This\nmeasurement may indicate how much complexity may be involved in implementing a bug fix.  This may include technical and/or business complexities.  For instance, to implement a new password storage policy, a developer may need to consult with a system\narchitect, a database administrator, a product manager, a business development person, and/or senior management.  Additionally, or alternatively, the developer may need to write code to check if a user's password has been changed since the new storage\npolicy was rolled out and, if not, force the user to reset the password.  Stability This measurement may indicate how much impact a bug fix may have on performance of an application or one or more parts of the application.  For instance, hashing a\npassword may add hundreds of milliseconds at each login.  Such degradation may not be significant for most applications.  However, some applications (e.g., electronic trading) may be extremely time sensitive, so that losing hundreds of milliseconds per\nauthentication may be unacceptable.  Testability This measurement may indicate how easily a solution intended to fix a bug may be tested to determine if the solution actually fixes the bug.  For instance, a solution may be adopted more easily if an\nautomated test is available.\n Accordingly, in some embodiments, a priority measurement may be calculated as follows, where average may be any suitable function for combining multiple measurements.\n TABLE-US-00006 function priority(confidentiality, integrity, availability, accessibility, complexity_0, authentication, functionality, complexity_0, stability, testability) { var impact = average (confidentiality, integrity, availability); var\nlikelihood = average(accessibility, complexity_0, authentication); var friction = average (functionality, complexity_1, stability, testability); return average (impact, likelihood, friction) }\n It should be appreciated that the above definition of priority is provided solely for purposes of illustration, as a priority measure may be calculated in any suitable manner.  For instance, priority, impact, likelihood, and friction need not be\ncalculated using the same average function.  Moreover, any suitable combination of one or more parameters may be used to calculate any of these measures, in addition to, or instead of, the illustrative parameters used in the above definition of priori\n In some embodiments, one or more weighted average functions may be used to combine measurements.  For instance, a weighted priority measurement may be calculated as follows.\n TABLE-US-00007 function priority(confidentiality, integrity, availability, accessibility, complexity_0, authentication, functionality, complexity_0, stability, testability) { var impact = average (confidentiality, integrity, availability); var\nlikelihood = average(accessibility, complexity_0, authentication); var friction = average (functionality, complexity_1, stability, testability); return weighted_priority(impact, likelihood, friction) } function weighted_priority(impact, likelihood,\nfriction, guidance_weighted=0, business_weighted=0, develoer_weighted=0) { guidance_weighted = guidance.weight(impact, likelihood, friction, guidance_weighted, business_weighted, developer_weighted); business_weighted = orgprofile.weight(impact,\nlikelihood, friction, guidance_weighted, business_weighted, developer_weighted); developer_weighted = userprofile.weight(impact, likelihood, friction, guidance_weighted, business_weighted, developer_weighted); return weighted_priority(impact, likelihood,\nfriction, guidance_weighted, business_weighted, developer_weighted); }\n In some embodiments, a guidance engine may be programmed to apply suitable weightings to the measurements impact, likelihood, and friction, for example, via the function guidance.weight in the illustrative definition weighted_priority above.\n Additionally, or alternatively, weightings may be applied to the measurements impact, likelihood, and friction, via the function orgprofile.weight in the illustrative definition weighted_priority above.  Such weightings may reflect how an\norganization for which the application is developed may evaluate these measurements.\n Additionally, or alternatively, weightings may be applied to the measurements impact, likelihood, and friction, via the function userprofile.weight in the illustrative definition weighted_priority above.  Such weightings may reflect how a\ndeveloper working on the application may evaluate these measurements.  For instance, the weightings may reflect the developer's understanding of friction, secure coding guidance customized for the developer, and/or the developer's reputation.\n Additionally, or alternatively, further weightings may be applied to the measurements impact, likelihood, and friction, via a recursive call to the function weighted_priority.  Such a recursive call may capture any additional information that\nmay have become relevant since a previous round of calculation.  Any suitable exit criterion may be used for the recursion.  For instance, the recursion may stop when there is no more relevant information to be captured.\n It should be appreciated that weightings may be determined in any suitable manner.  In some embodiments, one or more weights may be determined based on an application lifecycle stage.  For example, security may becoming increasing important as\nan application progresses through experimentation, proof of concept, alpha, beta, and general availability, and increasing weights may be applied to these stages (e.g., 0, 1, 3, 5, and 10, respectively).\n Additionally, or alternatively, one or more weights may be determined based on one or more environmental conditions.  For instance, one or more weights may be determined based on presence of one or more transparent environmental controls such as\nload balancer, identity providers, etc.\n FIG. 32 shows, schematically, an illustrative computer 1000 on which any aspect of the present disclosure may be implemented.  In the embodiment shown in FIG. 32, the computer 1000 includes a processing unit 1001 having one or more processors\nand a non-transitory computer-readable storage medium 1002 that may include, for example, volatile and/or non-volatile memory.  The memory 1002 may store one or more instructions to program the processing unit 1001 to perform any of the functions\ndescribed herein.  The computer 1000 may also include other types of non-transitory computer-readable medium, such as storage 1005 (e.g., one or more disk drives) in addition to the system memory 1002.  The storage 1005 may also store one or more\napplication programs and/or external components used by application programs (e.g., software libraries), which may be loaded into the memory 1002.\n The computer 1000 may have one or more input devices and/or output devices, such as devices 1006 and 1007 illustrated in FIG. 32.  These devices can be used, among other things, to present a user interface.  Examples of output devices that can\nbe used to provide a user interface include printers or display screens for visual presentation of output and speakers or other sound generating devices for audible presentation of output.  Examples of input devices that can be used for a user interface\ninclude keyboards and pointing devices, such as mice, touch pads, and digitizing tablets.  As another example, the input devices 1007 may include a microphone for capturing audio signals, and the output devices 1006 may include a display screen for\nvisually rendering, and/or a speaker for audibly rendering, recognized text.\n As shown in FIG. 32, the computer 1000 may also comprise one or more network interfaces (e.g., the network interface 1010) to enable communication via various networks (e.g., the network 1020).  Examples of networks include a local area network\nor a wide area network, such as an enterprise network or the Internet.  Such networks may be based on any suitable technology and may operate according to any suitable protocol and may include wireless networks, wired networks or fiber optic networks.\n Having thus described several aspects of at least one embodiment, it is to be appreciated that various alterations, modifications, and improvements will readily occur to those skilled in the art.  Such alterations, modifications, and\nimprovements are intended to be within the spirit and scope of the present disclosure.  Accordingly, the foregoing description and drawings are by way of example only.\n The above-described embodiments of the present disclosure can be implemented in any of numerous ways.  For example, the embodiments may be implemented using hardware, software or a combination thereof.  When implemented in software, the software\ncode can be executed on any suitable processor or collection of processors, whether provided in a single computer or distributed among multiple computers.\n Also, the various methods or processes outlined herein may be coded as software that is executable on one or more processors that employ any one of a variety of operating systems or platforms.  Additionally, such software may be written using\nany of a number of suitable programming languages and/or programming or scripting tools, and also may be compiled as executable machine language code or intermediate code that is executed on a framework or virtual machine.\n In this respect, the concepts disclosed herein may be embodied as a non-transitory computer-readable medium (or multiple computer-readable media) (e.g., a computer memory, one or more floppy discs, compact discs, optical discs, magnetic tapes,\nflash memories, circuit configurations in Field Programmable Gate Arrays or other semiconductor devices, or other non-transitory, tangible computer storage medium) encoded with one or more programs that, when executed on one or more computers or other\nprocessors, perform methods that implement the various embodiments of the present disclosure discussed above.  The computer-readable medium or media can be transportable, such that the program or programs stored thereon can be loaded onto one or more\ndifferent computers or other processors to implement various aspects of the present disclosure as discussed above.\n The terms \"program\" or \"software\" are used herein to refer to any type of computer code or set of computer-executable instructions that can be employed to program a computer or other processor to implement various aspects of the present\ndisclosure as discussed above.  Additionally, it should be appreciated that according to one aspect of this embodiment, one or more computer programs that when executed perform methods of the present disclosure need not reside on a single computer or\nprocessor, but may be distributed in a modular fashion amongst a number of different computers or processors to implement various aspects of the present disclosure.\n Computer-executable instructions may be in many forms, such as program modules, executed by one or more computers or other devices.  Generally, program modules include routines, programs, objects, components, data structures, etc. that perform\nparticular tasks or implement particular abstract data types.  Typically the functionality of the program modules may be combined or distributed as desired in various embodiments.\n Also, data structures may be stored in computer-readable media in any suitable form.  For simplicity of illustration, data structures may be shown to have fields that are related through location in the data structure.  Such relationships may\nlikewise be achieved by assigning storage for the fields with locations in a computer-readable medium that conveys relationship between the fields.  However, any suitable mechanism may be used to establish a relationship between information in fields of\na data structure, including through the use of pointers, tags or other mechanisms that establish relationship between data elements.\n Various features and aspects of the present disclosure may be used alone, in any combination of two or more, or in a variety of arrangements not specifically discussed in the embodiments described in the foregoing and is therefore not limited in\nits application to the details and arrangement of components set forth in the foregoing description or illustrated in the drawings.  For example, aspects described in one embodiment may be combined in any manner with aspects described in other\nembodiments.\n Also, the concepts disclosed herein may be embodied as a method, of which an example has been provided.  The acts performed as part of the method may be ordered in any suitable way.  Accordingly, embodiments may be constructed in which acts are\nperformed in an order different than illustrated, which may include performing some acts simultaneously, even though shown as sequential acts in illustrative embodiments.\n Use of ordinal terms such as \"first,\" \"second,\" \"third,\" etc. in the claims to modify a claim element does not by itself connote any priority, precedence, or order of one claim element over another or the temporal order in which acts of a method\nare performed, but are used merely as labels to distinguish one claim element having a certain name from another element having a same name (but for use of the ordinal term) to distinguish the claim elements.\n Also, the phraseology and terminology used herein is for the purpose of description and should not be regarded as limiting.  The use of \"including,\" \"comprising,\" \"having,\" \"containing,\" \"involving,\" and variations thereof herein, is meant to\nencompass the items listed thereafter and equivalents thereof as well as additional items.", "application_number": "15595683", "abstract": " Disclosed herein are methods, systems, and computer program products\n     directed to a guidance engine. The guidance engine is configured to query\n     a knowledge base for guidance with respect to a property of a software\n     application. The guidance engine receives a responsive query from the\n     knowledge base that is based on the property. The responsive query\n     informs a user of the guidance engine how to address a vulnerability\n     within the software application by performing a transform with respect to\n     a property of the software application.\n", "citations": ["6553563", "6604110", "6732095", "7448022", "20050050046", "20050234755", "20060225052", "20070250825", "20090327809", "20140067781", "20140095535", "20150026158"], "related": ["62336735"]}, {"id": "20170351762", "patent_code": "10346491", "patent_name": "Generating exemplar electronic documents using semantic context", "year": "2019", "inventor_and_country_data": " Inventors: \nLecue; Freddy (Dublin, IE), Brazil; Caroline (Offaly, IE), Wan; Dadong (San Jose, CA)  ", "description": "BACKGROUND\n Enterprises may require individuals to plan and/or purchase goods and/or services.  In one example, individuals may need to book travel.  For example, travel arrangements are made on an individual basis, where individuals book flights, hotels,\nand other travel accommodations based on their own needs.  As another example, a travel agent books travel arrangements for individuals.  In such examples, an absence of contextual information can limit insight into appropriate travel accommodations for\na particular individual and/or a particular trip.  Further, it can be a relatively burdensome, manual effort to identify which hotel, which flight, which public transportation (if needed), and the like.  In some instances, the travel arrangements are\nsubsequently scrutinized for conformance to enterprise policies, and/or regulatory schema.  Non-conforming travel arrangements can be resource-burdensome to audit.\nSUMMARY\n Implementations of the present disclosure are generally directed to generating exemplar electronic documents based contextual information.  In some implementations, actions include receiving input from a user, the input including data that is at\nleast partially representative of a subject, receiving a plurality of stored subjects, each stored subject including data that is at least partially representative of the respective stored subject, and that is provided in a knowledge graph, processing\nthe input in view of each of the stored subjects based on semantic comparison between the input and each of the stored subjects to provide a set of semantic differences, each semantic difference representing the input and a respective stored subject,\nprocessing a user profile of the user in view of each of a plurality of other user profiles to provide a set of semantic correlations, each semantic correlation representing the user profile and a respective other user profile, and providing the exemplar\nelectronic document based on the set of semantic differences and the set of semantic correlations, the exemplar electronic document including at least a portion of the input, and at least respective portions of each of a plurality of stored subjects\nbased on respective scores.  Other implementations of this aspect include corresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices.\n These and other implementations can each optionally include one or more of the following features: processing the input in view of each of the stored subjects includes semantically comparing features of the input to respective features of each\nof the stored subjects at least partially based on one or more hierarchical ontologies; semantically comparing includes, for each input and stored subject pair, determining a semantic distance as a weighted average of a number of edges in the knowledge\ngraph between features of the input and features of the stored subject; processing a user profile of the user in view of each of a plurality of other user profiles includes semantically correlating attributes of the user profile to respective attributes\nof each of the plurality of other user profiles using graph subsumption; the respective scores are at least partially determined based on the set of semantic differences and the set of semantic correlations; the exemplar electronic document is a travel\nitinerary; actions further include: transmitting the exemplar electronic document to one or more computer-implemented booking services, and receiving a modified exemplar electronic document from the one or more computer-implemented services, the modified\nexemplar electronic document including booking data; the subject includes a trip and the data of the input includes trip details; data of the stored subjects represents travel possibilities for the trip; data of a stored subject describes a previous trip\nperformed by another user; data of the stored subject describes respective previous trips of other users determined to be similar to the user; data of a stored subject relates to a travel policy of an entity; and providing the exemplar electronic\ndocument includes pre-populating trip details in the exemplar electronic document.\n Implementations of the present disclosure provide one or more of the following advantages.  In some examples, implementations enable semantic identification of preferred contexts, for example, travel trips of user trip, or travel profile for\nuser profile.  In some examples, implementations enable semantic aggregation of preferred context and content, for example, aggregation of travel trips or profiles.  In some examples, implementations enable semantic reasoning for context combination, for\nexample, consistency checking of any combination of travel trips or profiles.  Accordingly, implementations of the present disclosure provide optimized identification of exemplars based on semantic context, and enable the efficient production of a\nrelevant, accurate exemplar electronic document.  In this manner, computer resources (e.g., processors, memory, bandwidth) are conserved.\n The present disclosure also provides a computer-readable storage medium coupled to one or more processors and having instructions stored thereon which, when executed by the one or more processors, cause the one or more processors to perform\noperations in accordance with implementations of the methods provided herein.\n The present disclosure further provides a system for implementing the methods provided herein.  The system includes one or more processors, and a computer-readable storage medium coupled to the one or more processors having instructions stored\nthereon which, when executed by the one or more processors, cause the one or more processors to perform operations in accordance with implementations of the methods provided herein.\n It is appreciated that methods in accordance with the present disclosure can include any combination of the aspects and features described herein.  That is, methods in accordance with the present disclosure are not limited to the combinations of\naspects and features specifically described herein, but also include any combination of the aspects and features provided.\n The details of one or more implementations of the present disclosure are set forth in the accompanying drawings and the description below.  Other features and advantages of the present disclosure will be apparent from the description and\ndrawings, and from the claims. BRIEF DESCRIPTION OF DRAWINGS\n FIG. 1 depicts an example system that can execute implementations of the present disclosure.\n FIG. 2 depicts an example module architecture in accordance with implementations of the present disclosure.\n FIG. 3 depicts an example portion of an example knowledge graph.\n FIG. 4 depicts a graphical representation of example semantic identification of subjects in accordance with implementations of the present disclosure.\n FIG. 5 depicts a graphical representation of example semantic correlation of users in accordance with implementations of the present disclosure.\n FIG. 6 depicts an example exemplar in accordance with implementations of the present disclosure.\n FIGS. 7-9 depict example processes that can be executed in implementations of the present disclosure.\nDETAILED DESCRIPTION\n Implementations of the present disclosure are generally directed to automatically generating exemplar electronic documents based on semantic context.  More particularly, implementations of the present disclosure are directed to automatically\ngenerating exemplar electronic documents based on a knowledge graph (e.g., a data structure relating entities through semantic properties), semantic identification of subjects, semantic correlation of users, and content and context aggregation.  In some\nimplementations, an exemplar electronic document is provided as an electronic document that is representative of a subject, and that includes one or more features of the subject.  Example subjects can include an event, a set of events, an item, a\nproduct, a service, and the like.  In some implementations, one or more actions are automatically executed based on the exemplar electronic document.\n Implementations of the present disclosure will be described in further detail herein with reference to an example context.  The example context includes enterprise travel planning and booking (e.g., for an employee of the enterprise).  In the\nexample context, the exemplar electronic document is provided as a travel itinerary for a trip that a particular employee is to take; the trip is provided as the subject, and example features of the subject include dates, reason for travel, to\nlocation(s), from location(s), conveyance(s) (e.g., planes, trains, automobiles, ferries), hotels, events, dinners, lunches, and the like.  It is contemplated, however, the implementations of the present disclosure can be realized in any appropriate\ncontext.\n FIG. 1 depicts an example system 100 that can execute implementations of the present disclosure.  The example system 100 includes computing devices 102, 104, 106, a back-end system 108, and a network 110.  In some examples, the network 110\nincludes a local area network (LAN), wide area network (WAN), the Internet, or a combination thereof, and connects web sites, devices (e.g., the computing device 102, 104, 106), and back-end systems (e.g., the back-end system 108).  In some examples, the\nnetwork 110 can be accessed over a wired and/or a wireless communications link.  For example, mobile computing devices, such as smartphones can utilize a cellular network to access the network 110.\n In the depicted example, the back-end system 108 includes at least one server system 112, and data store 114 (e.g., database and knowledge graph structure).  In some examples, the at least one server system 112 hosts one or more\ncomputer-implemented services that users can interact with using computing devices.  For example, the server system 112 can host a computer-implemented service for generating exemplar electronic documents in accordance with implementations of the present\ndisclosure.\n In some examples, the computing devices 102, 104, 106 can each include any appropriate type of computing device such as a desktop computer, a laptop computer, a handheld computer, a tablet computer, a personal digital assistant (PDA), a cellular\ntelephone, a network appliance, a camera, a smart phone, an enhanced general packet radio service (EGPRS) mobile phone, a media player, a navigation device, an email device, a game console, or an appropriate combination of any two or more of these\ndevices or other data processing devices.\n In the depicted example, the computing devices 102, 104, 106 are each provided as a desktop computer that is used by respective users 120, 122, 124.  In the example context, the users 120, 122, 124 are employees of an enterprise.  For example,\nthe user 120 can include an employee \"Jamie,\" who is planning travel for work, the user 122 can include an employee \"Paul,\" who had previously traveled for work, and the user 124 can include an employee \"Tom,\" who had also previously traveled for work. \nIn some examples, each user 120, 122, 124 is associated with a user profile maintained by the enterprise.  In some examples, the user profile includes one or more attributes (e.g., name, group, role, contact information, etc.).  In some implementations,\nthe enterprise maintains a knowledge base of information associated with each user.  For example, the enterprise maintains a knowledge base of information associated with travel (e.g., duration, location, type, hotels, travel conveyances) each of the\nusers has undertaken as part of their employment.\n In accordance with implementations of the present disclosure, and as introduced above, exemplar electronic documents are automatically generated based on a knowledge graph, semantic identification of subjects, semantic correlation of users, and\ncontent and context aggregation.  In some implementations, the knowledge graph is provided based on data associated with one or more subjects.  In some examples, the data is provided as historical data that reflects one or more subjects that are\nassociated with the enterprise.  In the example context, the knowledge graph is provided based on data associated with travel that has been performed by users (e.g., users 120, 122, 124) of the enterprise.\n In some examples, a knowledge graph is a collection of data and related based on a schema representing entities and relationships between entities.  The data can be logically described as a graph (even though also provided in table form), in\nwhich each distinct entity is represented by a respective node, and each relationship between a pair of entities is represented by an edge between the nodes.  Each edge is associated with a relationship and the existence of the edge represents that the\nassociated relationship exists between the nodes connected by the edge.  For example, if a node A represents a person Alpha, a node B represents a person Beta, and an edge E is associated with the relationship \"is the father of,\" then having the edge E\nconnect the nodes in the direction from node A to node B in the graph represents the fact that Alpha is the father of Beta.  In some examples, the knowledge graph can be enlarged with schema-related knowledge (e.g., Alpha is a concept Person, Beta is a\nconcept Person, and \"is the father of\" is a property or relationship between two entities/instances of concept Person).  Adding schema-related information supports evaluation of reasoning results, such as subsumption or graph pattern matching.\n A knowledge graph can be represented by any of a variety of physical data structures.  For example, a knowledge graph can be represented by triples that each represent two entities in order, and a relationship from the first to the second\nentity; for example, [alpha, beta, is the father of], or [alpha, is the father of, beta], are alternative ways of representing the same fact.  Each entity and each relationship can be, and generally will be, included in multiple triples.\n In some examples, each entity can be stored as a node once, as a record or an object, for example, and linked through a linked list data structure to all the relationships the entity has, and all the other entities to which the entity is\nrelated.  More specifically, a knowledge graph can be stored as an adjacency list in which the adjacency information includes relationship information.  In some examples, each distinct entity and each distinct relationship are represented with\nrespective, unique identifiers.\n The entities represented by a knowledge graph need not be tangible things or specific people.  The entities can include particular people, places, things, artistic works, concepts, events, or other types of entities.  Thus, a knowledge graph can\ninclude data defining relationships between people (e.g., co-stars in a movie); data defining relationships between people and things (e.g., a particular singer recorded a particular song); data defining relationships between places and things (e.g., a\nparticular type of wine comes from a particular geographic location); data defining relationships between people and places (e.g., a particular person was born in a particular city); and other kinds of relationships between entities.\n In some implementations, each node has a type based on the kind of entity the node represents; and the types can each have a schema specifying the kinds of data that can be maintained about entities represented by nodes of the type and how the\ndata should be stored.  For example, a node of a type for representing a person could have a schema defining fields for information such as birth date, birth place, and so on.  Such information can be represented by fields in a type-specific data\nstructure, or by triples that look like node-relationship-node triples (e.g., [person identifier, was born on, date]), or in any other convenient predefined way.  In some examples, some or all of the information specified by a type schema can be\nrepresented by links to nodes in the knowledge graph; for example, [one person identifier, child of, another person identifier], where the other person identifier is a node in the graph.\n In accordance with the example context, and as described in further detail herein, a knowledge graph can be provided as a collection of data and a schema representing subjects and relationships between subjects, and/or features of subjects.  For\nexample, a knowledge graph, or at least a portion thereof, can be provided as a collection of data representing a trip and features of a trip.\n FIG. 2 depicts an example module architecture 200 in accordance with implementations of the present disclosure.  The example module architecture 200 includes an exemplar electronic document generation module 202 that receives input data 204, and\nprovides an exemplar electronic document 206.  In the depicted example, the exemplar generation module 202 includes a semantic identification module 208, a semantic correlation module 210, and a content and context aggregation 212.  The example module\narchitecture 200 also includes a knowledge base 214, and a user profile database 216.  In some examples, each of the modules is provided as one or more computer-executable programs executed by one or more computing devices (e.g., of the back-end system\n108 of FIG. 1).\n In some implementations, the knowledge base 214 stores historical data associated with one or more subjects.  In the examples context, the knowledge base 214 stores data indicative of user profiles (e.g., unique user identifiers associated with\nrespective user profiles), trips, and features of trips.  In some examples, the data is stored as a knowledge graph, described in further detail herein.  In some examples, data in the knowledge base 214 is organized based on an ontology for semantic\nquerying, ordering, and ranking, described in further detail herein.  For example, and in the example context, justifications for trips can be ordered in a hierarchy (e.g., an Acquisition Visit is a Client Visit, and a Client Visit is an Event).  In some\nexamples, the user profile database 216 stores user profiles of respective users of the enterprise.  For example, each user profile is associated with a unique identifier that identifies a respective user (e.g., an employee ID assigned to, and unique to\nthe user).  In some examples, each user profile stored in the user profile database 216 includes one or more attributes (e.g., name, group, role, contact information, etc.) of the respective user.\n In some examples, the input 204 includes data defining a subject and one or more features of a subject.  In the example context, the input 204 includes data defining a trip to be taken by a user, and features of the trip.  For example, the user\nJamie (e.g., the user 120 of FIG. 1) is planning a trip and provides the following example input defining the trip: Location: Austin, Tex., USA Duration: 2 days Date: 2016 Feb.  16 Justification: Acquisition Visit Weekend External Event: Live Music\n In some examples, the input is associated with the unique identifier that identifies the user providing the input (e.g., an employee ID assigned to, and unique to Jamie).\n In accordance with implementations of the present disclosure, existing, already conducted travel trips are semantically identified based on the input 204.  For example, all trips stored in the knowledge base 214 are compared to the input 204 to\nidentify semantic equivalents of the trip represented by the input 204.  In some examples, the semantic identification is based on determining one or more concept distances based on data stored in the knowledge graph.\n FIG. 3 depicts an example portion 300 of an example knowledge graph 302 in accordance with the example context.  In the depicted example, a node Trip is related to a plurality of other nodes (e.g., Austin, Acquisition Visit, Weekend,\nTransportation, Flight), by respective edges.  In some examples, each edge represents a relationship between nodes.  For example, the edge between Trip and Austin represents a relationship of [travel to], and the edge between Trip and Acquisition Visit\nrepresents a relationship of [justification for].\n To assist in illustrating implementations of the present disclosure, the knowledge base 214 stores data associated with trips performed by other users, such as Paul and Tom (e.g., the users 122, 124, respectively, of FIG. 1).  For example, the\nknowledge base 214 stores data for the following example trips (among, potentially, tens, hundreds, thousands of other trips): Location: Dublin, IE.fwdarw.Austin, Tex., USA Duration: 4 days Date: 2015 Aug.  15 Justification: Conference Week Public\nTransportation USA Airways (Dublin.fwdarw.Chicago.fwdarw.Austin) External Event: none and Location: Dublin, IE.fwdarw.Boston, Mass., USA Duration: 12 days Date: 2015 Feb.  11 Justification: Client Visit Weekend Car Rental USA Airways External Event:\nComputer Science Conference Restaurant: Trio Chain\n FIG. 4 depicts a graphical representation of example semantic identification of subjects in accordance with implementations of the present disclosure.  In the depicted example, a hierarchical ontology is provided in view of the example context. \nFor example, a hierarchical (or taxonomy-based) ontology for Trip is provided, where State and International are both types of trip, TX and NY are both types of trips under State, etc. As another example, a hierarchical ontology for Season is provided,\nwhere Summer and Winter are types of season.  As another example, a hierarchical ontology for Event is provided, where Conference and Client Visit are both types of event, and Acquisition Visit is a Client Visit.\n In some implementations, semantic identification is provided by comparing features of the input subject (e.g., the input 204) to features of previously conducted subjects.  Continuing with the examples above, the input provided by Jamie is\nsemantically compared to subjects stored in the knowledge base 214.  In some examples, a semantic delta (.DELTA..sub.SEM) is determined between the input subject and each stored subject.  In the example context, the input trip (i.e., Jamie's\nto-be-planned trip to Austin) is semantically compared to all stored trips (i.e., Houston.fwdarw.Austin, Dublin.fwdarw.Boston, among many others).  In some examples, spatial correlations and/or temporal correlations are provided between features of\nsubjects.  For example, a spatial correlation can be provided based on location, and a temporal correlation can be provide based on date.\n In some implementations, .DELTA..sub.SEM is determined as a semantic distance between the input subject (e.g., planned trip) and a stored subject (e.g., stored trip).  In some examples, the semantic distance is determined as a weighted average\nof the number of edges in the knowledge graph (or relevant portion of a knowledge graph) between all features of the respective input subject and stored subject.  For example, the more edges between two features, the more distant the features are from\none another, and the less semantically similar the features are.  On the other hand, the fewer edges between two features, the less distant the features are from one another, and the more semantically similar the features are.\n In some implementations, .DELTA..sub.SEM is determined based on the following example relationship:\n .DELTA..fwdarw..function..times..times..alpha..function..function..times.- .times..alpha.  ##EQU00001## where: S.sub.I=the input subject (e.g., the trip being planned) i=particular trip feature; n=number of trip features S.sub.j.sup.i=feature of\nsubject j; S.sub.j=vector of feature of subject j (e.g., stored trip being compared to S.sub.I); .alpha.=weighted factor with value in [0,1]; and d=semantic similarity of features e.g., the number of edges between the features.\n Continuing with the examples above, and as depicted in the example of FIG. 4, .DELTA..sub.SEM for Jamie's planned trip to Austin, and the stored trip Houston.fwdarw.Austin can be calculated as 0.9 (e.g., both trips are to Austin for an Event,\nwhere Conference and Client Visit are both Events).  As another example, .DELTA..sub.SEM for Jamie's planned trip to Austin, and the stored trip Dublin.fwdarw.Boston can be calculated as 0.7 (e.g., both trips are in Winter for an Event).  A set of\n.DELTA..sub.SEM'S is provided and includes one or more .DELTA..sub.SEM's, each .DELTA..sub.SEM corresponding to a respective semantic comparison between the input subject and a stored subject.  For example, if the input subject is compared to 50 stored\ntrips, the set of .DELTA..sub.SEM's includes 50 .DELTA..sub.SEM values.\n In some implementations, a set of user profiles is provided based on the semantic comparisons.  In some examples, each stored subject is associated with a respective user profile (e.g., the user profile of the user that took the trip).  In some\nexamples, the set of user profiles includes unique user profiles.  For example, if a user is associated with multiple subjects included in the semantic comparison, the user profile of the user is only included once in the set of user profiles (e.g., Tom\ntook 10 trips of the 50 stored trips semantic comparison was provided for, but Tom's profile is only included once in the set of user profiles).\n Continuing with the examples above, Paul can be associated with the trip Houston.fwdarw.Austin, and Tom can be associated with the trip Dublin.fwdarw.Boston.  Consequently, a set of user profiles is provided and includes Paul's user profile and\nTom's user profiles (among, potentially, many others).\n FIG. 5 depicts a graphical representation of example semantic correlation of users in accordance with implementations of the present disclosure.  In some implementations, the semantic correlation of users is provided based on user profiles\n(e.g., stored in the user profile database 216).  For example, the user profile of the user that submitted the input 204 (e.g., Jamie) can be retrieved from the user profile database 216 based on the unique identifier (e.g., employee ID), and can be\ncompared to one or more other user profiles stored in the user profile database 216 (among, potentially, tens, hundreds, thousands of other user profiles).  More particularly, the user profile can be compared to user profiles in the set of user profiles\nthat is provide based on the semantic comparison of subjects, as described above.\n In some implementations, the semantic correlation is provided based on graph subsumption, or sub-graph pattern matching of the user profiles.  For example, the semantic correlation between a first user profile (e.g., the user profile of the user\nthat submitted the input 204) and a second user profile stored in the user profile database 216 is evaluated based on the maximum number of features that characterizes the first profile with respect to the second profile through graph subsumption\n(specification relation).  In some examples, graph subsumption is provided as an instance of graph isomorphism, in which isomorphism exists when two graphs are equivalent.  For example, two graphs having edges and vertices, and which contain the same\nnumber of vertices connected in the same way are said to be isomorphic.  Subsumption aims at identifying specification-based relationships of entities.  For example, TX is subsumed by State in FIG. 4, because TX is more specific than State.  Note that\nsubsumption is not a symmetric function.  For instance State is not subsumed by TX.\n In some implementations, the semantic correlation (C.sub.SEM) is determined based on the following example relationship:\n .fwdarw..function..times..times..function.  ##EQU00002## where: P.sub.i=the input profile (e.g., the user profile of the user that provided the input).  P.sub.i.sup.k is the feature k of user profile P.sub.i.  P.sub.j=a potential profile that is\nevaluated against P.sub.i (e.g., the user profile of an other user).  P.sub.j.sup.k is the feature j of user profile P.sub.j.  i=1, .  . . , n; j=1, .  . . , n; n=number of other user profiles in the set of user profiles; m=number of attributes in\nprofile; and Subsume.fwdarw.Return 1 if P.sub.i.sup.k is more specific than P.sub.j.sup.k, otherwise return 0.\n Continuing with the examples above, and as depicted in the example of FIG. 5, C.sub.SEM between Jamie and Paul, and between Jamie and Tom can be determined.  In the example of FIG. 5, a comparison between Jamie's user profile and Paul's user\nprofile is graphically depicted.  In some examples, attributes of the user profiles are grouped into one or more groups.  Example groups include Domain Expertise and Client Impact.  In some examples, Domain Expertise includes the example attributes\nWorkgroup and Years of Experience, and Client Impact includes Title and Number of Client Visits.  In the Example of FIG. 5, and with respect to Domain Expertise, Jamie is in the Client Marketing Workgroup and has 1 Years of Experience, while Paul is in\nthe Client Workgroup and has 4 Years of Experience.  In the Example of FIG. 5, and with respect to Client Impact, Jamie has the title of Senior Manager at a Level 6 and has 55 Client Visits, while Paul has a title of Manager at Level 7 and has 31 Client\nVisits.  In some examples, C.sub.SEM between Jamie and Paul can be provided as 0.5, and C.sub.SEM between Jamie and Tom can be provided as 0.4.  For example, Jamie can be determined to be closer to Paul based on Domain Expertise, and closer to Tom based\non Client Impact.\n A set of C.sub.SEM's is provided and includes one or more C.sub.SEM's, each C.sub.SEM corresponding to a respective semantic correlation between the user profile of the user that provided the input and the user profile of another user.  For\nexample, if the user profile is compared to 30 other user profiles, the set of C.sub.SEM's includes 30 C.sub.SEM values.  The C.sub.SEM values with the highest score are considered.\n In accordance with implementations of the present disclosure, content and context aggregation is performed based on the semantic comparisons and the semantic correlations.  More particularly, a score (Sim) is provided based on combining the\nsemantic correlation of user profiles with the semantic comparison of subjects (e.g., trips).  In some examples, the score is weighted based on one or more features of the subjects.  In some examples, the score is determined based on the following\nexample relationship:\n .function..times..times..times..alpha..beta..times..alpha..times..times..- fwdarw..function..beta..DELTA..fwdarw..function.  ##EQU00003## where: User of P.sub.i is the same as user of S.sub.k User of P.sub.j is the same as user of S.sub.l\nP.sub.i=the input profile (e.g., the user profile of the user that provided the input) P.sub.j=a potential profile that is evaluated against P.sub.i (e.g., the user profile of an other user).  S.sub.k=the input subject (e.g., the trip being planned)\nS.sub.l=vector of feature of subject j (e.g., stored trip being compared to S.sub.l); i=1, .  . . , n; k=1, .  . . , n; j=1, .  . . , m; l=1, .  . . , m; value=the maximum value to be returned .alpha.=weighted factor with value in [0,1] such that\n.alpha.+.beta.=1; .beta.=weighted factor with value in [0,1].alpha.+.beta.=1; n=number of other user profiles in the set of user profiles; m=number of trip features; t=the maximum number of features considered; and Subsume.fwdarw.Return 1 if P.sub.I is\nmore general than P.sub.k, otherwise return 0.\n Continuing with the examples above, a score as between Jamie's planned trip to Austin, and Paul's stored trip to Austin can be provided as 0.84, and a score as between Jamie's planned trip to Austin, and Tom's stored trip to Boston can be\nprovided as 0.62.  Accordingly, a data set can be provided as (Jamie, {(Paul's trip, 0.84), (Tom's trip, 0.62)}.\n In some implementations, the stored subjects are ranked based on the scores (e.g., highest to lowest), and the exemplar electronic document is provided based on values of the stored subjects in view of the scores.  In some examples, an exemplar\nelectronic document can be provided as a template that includes a plurality of features in respective fields, values of the features to be provided based on the input, and one or more of the stored subjects.  For example, and in the example context, a\ntemplate electronic document can include fields for Location, Duration, Date, Justification, Flights, Transportation, Restaurant, External Event, and the like.  In some examples, one or more fields are populated based on the input (e.g., the input\nprovided by Jamie to trigger trip planning).  For example, Location: Austin, Tex., USA, Duration: 2 days, Date: 2016 Feb.  16, Justification: Acquisition Visit.  In some examples, one or more fields are populated based on features provided from the\nstored subjects.  For example, Transportation: Public Transportation (from Paul's trip to Austin), Restaurant: Trio Chain (from Tom's trip to Boston), Airline: USA Airways (from Paul's and Tom's respective trips), and Flights:\nDublin.fwdarw.Chicago.fwdarw.Austin (from Paul's trip to Austin).\n In some implementations, the exemplar electronic document is populated based on a ranked list of semantically similar stored subjects (e.g., shortlisted travel trips).  In some examples, the stored subjects are ranked based on the Sim score,\ndescribed above, the higher the score, the higher the ranking.  Selection of values of particular fields to populate the exemplar electronic document is based on retrieving values of fields of stored subjects starting with the highest ranked stored\nsubject.  In some examples, the system iteratively goes through all fields required (e.g., Transportation, Airlines, etc.).  If available from a currently considered, stored subject, the system select values to populate the fields (e.g., Public\ntransportation, USA Airways).  If not available from the currently considered, stored subject, the system goes through the next best match (e.g., the next stored subject in the ranked list of stored subjects) to populate fields missing from higher ranked\nstored subjects (e.g., Restaurant Trio Chain).\n In some implementations, the exemplar electronic document is populated as described above, and is displayed to the user.  For example, the exemplar electronic document can be displayed to the user on a computing device.  In some examples, the\nuser can edit one or more features of the subject captured in the electronic document.  For example, the user can add, delete, or edit (modify) one or more features.\n In some implementations, the exemplar electronic document can be provided to one or more services for booking the subject.  For example, the exemplar electronic document can be provided to a hotel booking service, a flight booking service, a car\nrental booking service, a restaurant booking service, and the like.  In some examples, a single service can attend to one or more of hotel, flight, car rental, restaurant booking.  In some examples, booking services are provided as one or more\ncomputer-implemented services.  For example, a computer-implemented booking service can receive the exemplar electronic document, and can provide booking information based on the values provided in the exemplar electronic document.  For example, and in\nthe example context, a flight booking service can receive the exemplar electronic document, which indicates USA Airways is to be flown from Dublin.fwdarw.Chicago.fwdarw.Austin on 2016 Feb.  16, returning on 2016 Feb.  18, and can process this information\nto identify flights.  In some examples, the flight booking service can automatically book the flights on behalf of the traveler (e.g., Jamie) using stored information (e.g., in the user profile).  In some examples, the flight booking service can provide\npotential flights to the user, which the user can select from, and the flights selected by the user are booked.  This can occur for hotels, car rentals, and the like (e.g., automatically booked, or booked based on suggestions provided to the user and the\nuser selecting from the suggestions).  In some implementations, whether automatically booked and/or booked based on user selections, an exemplar electronic document is provided with booking information.\n FIG. 6 depicts an example exemplar electronic document 600 in accordance with implementations of the present disclosure.  In accordance with the example context, the example exemplar electronic document 600 includes a travel itinerary with\nbooking details for flights, car rental, hotel, and the like.\n FIG. 7 depicts an example process 700 that can be executed in implementations of the present disclosure.  In some examples, the example process 700 is provided using one or more computer-executable programs executed by one or more computing\ndevices (e.g., the back-end system 108 of FIG. 1).  The example process 700 can be executed to provide an exemplar, as described herein.\n User input is received (702).  For example, a user provides input 204 to the exemplar electronic document generation module 202 of FIG. 2.  In some examples, the user input provides data indicative of a subject.  A set of stored subjects is\ndetermined based on semantic identification (704).  For example, the input subject is semantically compared to a plurality of stored subjects (e.g., stored in the knowledge base 214 of FIG. 2 as at least a portion of a knowledge graph).  In some\nexamples, the set of stored subjects is provided as, or includes a set of .DELTA..sub.SEM's, as described herein.  A set of user profiles is determined based on semantic correlation (706).  For example, a user profile of the user that submitted the input\nis semantically correlated to a plurality of other user profiles.  In some examples, the other user profiles are provided in a set of user profiles that is determined based on the stored subjects considered during the semantic identifications.  In some\nexamples, the set of user profiles provided based on semantic correlation is provided as, or includes a set of C.sub.SEM's, as described herein.\n Content and context are aggregated (708).  In some examples, and as described herein, content and context are aggregated based on the set of .DELTA..sub.SEM's and the set of C.sub.SEM's to provide respective scores associated with subject and\nuser tuples, as described herein.  For example, (Jamie, {(Paul's trip, 0.84), (Tom's trip, 0.62)}).  An exemplar electronic document is provided (710).  For example, features provided from the input and one or more stored subjects are used to populate\nfields of the exemplar electronic document, as described herein.  One or more actions are automatically performed based on the exemplar electronic document (712).  For example, the exemplar electronic document is automatically forwarded to one or more\ncomputer-implemented booking services.  In some examples, the one or more computer-implemented booking services automatically book respective features (e.g., hotel, flight, restaurant), and/or automatically provides suggestions to the user, and books a\nsuggestion selected by the user.\n FIG. 8 depicts an example process 800 that can be executed in implementations of the present disclosure.  In some examples, the example process 800 is provided using one or more computer-executable programs executed by one or more computing\ndevices (e.g., the back-end system 108 of FIG. 1; the semantic identification module 208 of FIG. 2).  The example process 800 is executed to provide a set of stored subjects based on semantic identification.\n User input is received (802).  For example, a user provides input 204 to the exemplar electronic document generation module 202 of FIG. 2.  In some examples, the user input provides data indicative of a subject.  A counter j is set equal to 1\n(804).  Features of a stored subject S.sub.j are received (806).  For example, features of S.sub.j are received by the semantic identification module 208 from the knowledge base 214.  .DELTA..sub.SEM,I.fwdarw.j is determined (808).  For example, the\nsemantic identification module 208 semantically compares features of the input subject (S.sub.I) to the stored subject (S.sub.j) to provide .DELTA..sub.SEM,I.fwdarw.j, as described herein.  It is determined whether there are more stored subjects (810). \nFor example, it is determined whether all relevant stored subjects stored in the knowledge base 214 have been evaluated.  If not, the counter j is incremented (812), and the example process loops back.  If so, a set of .DELTA..sub.SEM's is provided\n(814).\n FIG. 9 depicts an example process 900 that can be executed in implementations of the present disclosure.  In some examples, the example process 900 is provided using one or more computer-executable programs executed by one or more computing\ndevices (e.g., the back-end system 108 of FIG. 1; the semantic correlation module 210 of FIG. 2).  The example process 900 is executed to provide a set of user profiles based on semantic correlation.\n A user profile (P.sub.I) is received (902).  For example, the semantic correlation module 210 receives the user profile from the user profile database 216 based on a user identifier associated with the user that provided the input 204.  A\ncounter k is set equal to 1 (904).  Attributes of a user profiled (P.sub.k) are received (906).  For example, the user profile (P.sub.k) and its associated attributes are received from the user profile database 216.  A semantic correlation\nC.sub.SEM,I.fwdarw.k is determined (908).  For example, the semantic correlation module 210 semantically correlates attributes of the input user profile (P.sub.I) to another user profile (P.sub.k) to provide C.sub.SEM,I.fwdarw.k, as described herein.  It\nis determined whether there are more user profiles for semantic correlation (910).  For example, it is determined whether all relevant user profiles stored in the user profile database 216 have been evaluated.  If not, the counter k is incremented (912),\nand the example process loops back.  If so, a set of C.sub.SEM's is provided (914).\n Implementations and all of the functional operations described in this specification may be realized in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and\ntheir structural equivalents, or in combinations of one or more of them.  Implementations may be realized as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a computer readable medium for\nexecution by, or to control the operation of, data processing apparatus.  The computer readable medium may be a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter effecting a machine-readable\npropagated signal, or a combination of one or more of them.  The term \"computing system\" encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or\ncomputers.  The apparatus may include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating\nsystem, or a combination of one or more of them.  A propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable\nreceiver apparatus.\n A computer program (also known as a program, software, software application, script, or code) may be written in any appropriate form of programming language, including compiled or interpreted languages, and it may be deployed in any appropriate\nform, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.  A computer program does not necessarily correspond to a file in a file system.  A program may be stored in a\nportion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub\nprograms, or portions of code).  A computer program may be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.\n The processes and logic flows described in this specification may be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output.  The processes\nand logic flows may also be performed by, and apparatus may also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).\n Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any appropriate kind of digital computer.  Generally, a processor will\nreceive instructions and data from a read only memory or a random access memory or both.  Elements of a computer can include a processor for performing instructions and one or more memory devices for storing instructions and data.  Generally, a computer\nwill also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.  However, a computer need not have such devices. \nMoreover, a computer may be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name just a few.  Computer readable media suitable for storing\ncomputer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks\nor removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.  The processor and the memory may be supplemented by, or incorporated in, special purpose logic circuitry.\n To provide for interaction with a user, implementations may be realized on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a\npointing device, e.g., a mouse or a trackball, by which the user may provide input to the computer.  Other kinds of devices may be used to provide for interaction with a user as well; for example, feedback provided to the user may be any appropriate form\nof sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any appropriate form, including acoustic, speech, or tactile input.\n Implementations may be realized in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client\ncomputer having a graphical user interface or a Web browser through which a user may interact with an implementation, or any appropriate combination of one or more such back end, middleware, or front end components.  The components of the system may be\ninterconnected by any appropriate form or medium of digital data communication (e.g., a communication network).  Examples of communication networks include a local area network (\"LAN\") and a wide area network (\"WAN\"), e.g., the Internet.\n The computing system may include clients and servers.  A client and server are generally remote from each other and typically interact through a communication network.  The relationship of client and server arises by virtue of computer programs\nrunning on the respective computers and having a client-server relationship to each other.\n While this specification contains many specifics, these should not be construed as limitations on the scope of the disclosure or of what may be claimed, but rather as descriptions of features specific to particular implementations.  Certain\nfeatures that are described in this specification in the context of separate implementations may also be implemented in combination in a single implementation.  Conversely, various features that are described in the context of a single implementation may\nalso be implemented in multiple implementations separately or in any suitable sub-combination.  Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed\ncombination may in some cases be excised from the combination, and the claimed combination may be directed to a sub-combination or variation of a sub-combination.\n Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations\nbe performed, to achieve desirable results.  In certain circumstances, multitasking and parallel processing may be advantageous.  Moreover, the separation of various system components in the implementations described above should not be understood as\nrequiring such separation in all implementations, and it should be understood that the described program components and systems may generally be integrated together in a single software product or packaged into multiple software products.\n A number of implementations have been described.  Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure.  For example, various forms of the flows shown above may\nbe used, with steps re-ordered, added, or removed.  Accordingly, other implementations are within the scope of the following claims.", "application_number": "15170030", "abstract": " Implementations are directed to providing an exemplar electronic document\n     (EED) with actions including receiving input that is at least partially\n     representative of a subject, receiving a plurality of stored subjects,\n     each including data representative of a respective stored subject, and\n     provided in a knowledge graph, processing the input based on semantic\n     comparison between the input and each of the stored subjects to provide a\n     set of semantic differences, each semantic difference representing the\n     input and a respective stored subject, processing a profile in view of\n     each of a plurality of other profiles to provide a set of semantic\n     correlations, each semantic correlation representing the profile and a\n     respective other profile, and providing the EED based on the sets of\n     semantic differences and semantic correlations, the EED including at\n     least a portion of the input, and respective portions of each of a\n     plurality of stored subjects based on respective scores.\n", "citations": ["7571177", "20070239697", "20080120257", "20100305984", "20120117455", "20140351261", "20150206072", "20150254311"], "related": []}, {"id": "20170359236", "patent_code": "10324973", "patent_name": "Knowledge graph metadata network based on notable moments", "year": "2019", "inventor_and_country_data": " Inventors: \nCirclaeys; Eric (Paris, FR), Bessiere; Kevin (San Francisco, CA), Aujoulet; Kevin (Paris, FR), Huyghe; Killian (Paris, FR), Vergnaud; Guillaume (Tokyo, JP), Hirmer; Benedikt (Paris, FR)  ", "description": "FIELD\n Embodiments described herein relate to digital asset management (also referred to as DAM).  More particularly, embodiments described herein relate to generating a knowledge graph metadata network (also referred to as a metadata network) based on\none or more notable moments in a collection of the digital assets (also referred to as a DA collection) that can assist with managing the DA collection.\nBACKGROUND\n Modern consumer electronics have enabled users to create, purchase, and amass considerable digital assets (also referred to as DAs).  For example, a computing system (e.g., a smartphone, a stationary computer system, a portable computer system,\na media player, a tablet computer system, a wearable computer system or device, etc.) can store or have access to a collection of digital assets (also referred to as a DA collection) that includes hundreds or thousands of DAs (e.g., images, videos,\nmusic, etc.).\n Managing a DA collection can be a resource-intensive exercise for users.  For example, retrieving multiple DAs representing a sentimental moment in a user's life from a sizable DA collection can require the user to sift through many irrelevant\nDAs.  This process can be arduous and unpleasant for many users.  A digital asset management (DAM) system can assist with managing a DA collection.  A DAM system represents an intertwined system incorporating software, hardware, and/or other services in\norder to manage, store, ingest, organize, and retrieve DAs in a DA collection.  An important building block for at least one commonly available DAM system is a database.  Databases are commonly known as data collections that are organized as schemas,\ntables, queries, reports, views, and other objects.  Exemplary databases include relational databases (e.g., tabular databases, etc.), distributed databases that can be dispersed or replicated among different points in a network, and object-oriented\nprogramming databases that can be congruent with the data defined in object classes and subclasses.\n One problem associated with using databases for digital asset management (DAM) is that the DAM system can become resource-intensive.  That is, substantial computational resources may be needed to manage the DAs in the DA collection (e.g.,\nprocessing power for performing queries or transactions, storage memory space for storing the necessary databases, etc.).  This requirement may assist with reducing the processing power available for other tasks.  Another related problem associated with\nusing databases is that digital asset management (DAM) cannot be easily implemented on a computing system with limited storage capacity without managing the assets directly (e.g., a portable device such as a smartphone or a wearable device). \nConsequently, a DAM system's functionality is generally provided by a remote device (e.g., an external data store, an external server, etc.) where copies of the DAs are stored and the results are transmitted back to the computing system having limited\nstorage capacity.  Requiring external data stores and/or servers in order to use databases for managing a large DA collection can assist with making digital asset management (DAM) resource-intensive.  This requirement may also assist with reducing the\nprocessing power available for other tasks on the local device.  At least one currently available DAM system uses metadata associated with a DA collection--such as spatiotemporal metadata (e.g., time metadata, location metadata, etc.)--to organize DAs in\nthe DA collection into multiple events.  These currently available DAM system(s), however, organize the metadata associated with the DA collection using databases, which can contribute to making digital asset management (DAM) a resource-intensive\nendeavor as explained above.\nSUMMARY\n Methods, apparatuses, and systems for generating a knowledge graph metadata network (also referred to as a metadata network) based on a collection of the digital assets (DA collection) are described.  Such embodiments can enable digital asset\nmanagement (DAM) of digital assets (DAs) without using traditional databases.\n For one embodiment, a DAM logic/module obtains or generates a knowledge graph metadata network (metadata network) associated with a collection of digital assets (DA collection).  The metadata network can comprise correlated metadata assets\ndescribing characteristics associated with digital assets (DAs) in the DA collection.  Each metadata asset can describe a characteristic associated with one or more digital assets (DAs) in the DA collection.  For a non-limiting example, a metadata asset\ncan describe a characteristic associated with multiple DAs in the DA collection.  Each metadata asset can be represented as a node in the metadata network.  A metadata asset can be correlated with at least one other metadata asset.  Each correlation\nbetween metadata assets can be represented as an edge in the metadata network that is between the nodes representing the correlated metadata assets.\n For one embodiment, the DAM logic/module identifies a first metadata asset in the metadata network.  The DAM logic/module can also identify a second metadata asset based on at least the first metadata asset.  For one embodiment, the DAM\nlogic/module causes one or more DAs associated with the first and/or second metadata assets to be presented via an output device.\n Other features or advantages attributable to the embodiments described herein will be apparent from the accompanying drawings and from the detailed description that follows below. BRIEF DESCRIPTION OF THE DRAWINGS\n Embodiments described herein are illustrated by examples and not limitations in the accompanying drawings, in which like references indicate similar features.  Furthermore, in the drawings some conventional details have been omitted so as not to\nobscure the inventive concepts described herein.\n FIG. 1A illustrates, in block diagram form, an asset management processing system that includes electronic components for performing digital asset management (DAM) in accordance with an embodiment.\n FIG. 1B illustrates, in block diagram form, an exemplary knowledge graph metadata network (also referred to as a metadata network) in accordance with one embodiment.  The exemplary metadata network illustrated in FIG. 1B can be generated and/or\nused by the DAM processing system illustrated in FIG. 1A in accordance with an embodiment.\n FIG. 2 is a flowchart representing an operation to perform DAM according to an embodiment.\n FIG. 3A illustrates, in flowchart form, an operation to generate an exemplary metadata network in accordance with an embodiment.\n FIGS. 3B-3C illustrate, in flowchart form, an operation to generate an exemplary metadata network in accordance with an embodiment.  FIGS. 3B-3C provides additional details about the operation illustrated in FIG. 3A.\n FIG. 3D illustrates, in flowchart form, an operation to generate one or more edges between nodes in a metadata network in accordance with an embodiment.  FIG. 3D provides additional details about the operation illustrated in FIGS. 3B-3C.\n FIG. 4 is a flowchart representing an operation to relate and present at least two digital assets (DAs) from a collection of DAs (DA collection) according to one embodiment.\n FIG. 5 is a flowchart representing an operation to determine and present at least two digital assets (DAs) from a DA collection based on a predetermined criterion in accordance with one embodiment.\n FIG. 6 is a flowchart representing an operation to determine and present representative digital assets (DAs) for a moment according to one embodiment.\n FIG. 7 illustrates an exemplary processing system for DAM according to one or more embodiments described herein.\nDETAILED DESCRIPTION\n Methods, apparatuses, and systems for generating a knowledge graph metadata network (also referred to as a metadata network) based on a collection of digital assets (also referred to as a DA collection) are described.  Such embodiments can\nenable digital asset management (DAM) for digital assets (also referred to as DAs) in the DA collection without using traditional databases.\n Embodiments set forth herein can assist with improving computer functionality by enabling computing systems that use one or more embodiments of the metadata network described herein for digital asset management (DAM).  Such computing systems can\nimplement DAM to assist with reducing or eliminating the need to use databases for digital asset management (DAM).  This reduction or elimination can, in turn, assist with minimizing wasted computational resources (e.g., memory, processing power,\ncomputational time, etc.) that may be associated with using databases for DAM.  For example, DAM via databases may include external data stores and/or remote servers (as well as networks, communication protocols, and other components required for\ncommunicating with external data stores and/or remote servers).  In contrast, DAM performed as described herein can occur locally on a device (e.g., a portable computing system, a wearable computing system, etc.) without the need for external data\nstores, remote servers, networks, communication protocols, and/or other components required for communicating with external data stores and/or remote servers.  Consequently, at least one embodiment of DAM described herein can assist with reducing or\neliminating the additional computational resources (e.g., memory, processing power, computational time, etc.) that may be associated with using databases for DAM.\n FIG. 1A illustrates, in block diagram form, a processing system 100 that includes electronic components for performing digital asset management (DAM) in accordance with this disclosure.  The system 100 can be housed in single computing system,\nsuch as a desktop computer system, a laptop computer system, a tablet computer system, a server computer system, a mobile phone, a media player, a personal digital assistant (PDA), a personal communicator, a gaming device, a network router or hub, a\nwireless access point (AP) or repeater, a set-top box, or a combination thereof.  Components in the system 100 can be spatially separated and implemented on separate computing systems that are connected by the communication technology 110, as described\nin further detail below.\n For one embodiment, the system 100 may include processing unit(s) 130, memory 160, a DA capture device 120, sensor(s) 191, and peripheral(s) 190.  For one embodiment, one or more components in the system 100 may be implemented as one or more\nintegrated circuits (ICs).  For example, at least one of the processing unit(s) 130, the communication technology 110, the DA capture device 120, the peripheral(s) 190, the sensor(s) 191, or the memory 160 can be implemented as a system-on-a-chip (SoC)\nIC, a three-dimensional (3D) IC, any other known IC, or any known IC combination.  For another embodiment, two or more components in the system 100 are implemented together as one or more ICs.  For example, at least two of the processing unit(s) 130, the\ncommunication technology 110, the DA capture device 120, the peripheral(s) 190, the sensor(s) 191, or the memory 160 are implemented together as an SoC IC.  Each component of system 100 is described below.\n As shown in FIG. 1A, the system 100 can include processing unit(s) 130, such as CPUs, GPUs, other integrated circuits (ICs), memory, and/or other electronic circuitry.  For one embodiment, the processing unit(s) 130 manipulate and/or process\nmetadata 170 or optional data 180 associated with digital assets (e.g., manipulate computer graphics, perform image processing, manipulate audio files, any other known processing operations performed on DAs, etc.).  The processing unit(s) 130 may include\na digital asset management (DAM) module/logic 140 for performing one or more embodiments of DAM, as described herein.  For one embodiment, the DAM module/logic 140 is implemented as hardware (e.g., electronic circuitry associated with the processing\nunit(s) 130, circuitry, dedicated logic, etc.), software (e.g., one or more instructions associated with a computer program executed by the processing unit(s) 130, software run on a general-purpose computer system or a dedicated machine, etc.), or a\ncombination thereof.\n The DAM module/logic 140 can enable the system 100 to generate and use a knowledge graph metadata network (metadata network) 175 of the DA metadata 170 as a multidimensional network.  Metadata networks and multidimensional networks are described\nbelow.  FIG. 1B (which is described below) provides additional details about generating the metadata network 175.  For one embodiment, the DAM module/logic 140 can perform one or more of the following: (i) generate the metadata network 175; (ii) relate\nand/or present at least two DAs based on the metadata network 175; (iii) determine and/or present interesting DAs in the DA collection based on the metadata network 175 and predetermined criterion; and (iv) select and/or present representative DAs to\nsummarize a moment's DAs based on input specifying the representative group's size.  Additional details about the immediately preceding operations performed by the DAM logic/module 140 are described below in connection with FIGS. 1B-6.\n The DAM module/logic 140 can obtain or receive a collection of DA metadata 170 associated with a DA collection.  As used herein, a \"digital asset,\" a \"DA,\" and their variations refer to data that can be stored in or as a digital form (e.g., a\ndigital file etc.).  This digitalized data includes, but is not limited to, the following: image media (e.g., a still or animated image, etc.); audio media (e.g., a song, etc.); text media (e.g., an E-book, etc.); video media (e.g., a movie, etc.); and\nhaptic media (e.g., vibrations or motions provided in connection with other media, etc.).  The examples of digitalized data above can be combined to form multimedia (e.g., a computer animated cartoon, a video game, etc.).  A single DA refers to a single\ninstance of digitalized data (e.g., an image, a song, a movie, etc.).  Multiple DAs or a group of DAs refers to multiple instances of digitalized data (e.g., multiple images, multiple songs, multiple movies, etc.).  Throughout this disclosure, the use of\n\"a DA\" refers to \"one or more DAs\" including a single DA and a group of DAs.  For brevity, the concepts set forth in this document use an operative example of a DA as one or more images.  It is to be appreciated that a DA is not so limited and the\nconcepts set forth in this document are applicable to other DAs (e.g., the different media described above, etc.).\n As used herein, a \"digital asset collection,\" a \"DA collection,\" and their variations refer to multiple DAs that may be stored in one or more storage locations.  The one or more storage locations may be spatially or logically separated as is\nknown.\n As used herein, \"metadata,\" \"digital asset metadata,\" \"DA metadata,\" and their variations collectively refer to information about one or more DAs.  Metadata can be: (i) a single instance of information about digitalized data (e.g., a time stamp\nassociated with one or more images, etc.); or (ii) a grouping of metadata, which refers to a group comprised of multiple instances of information about digitalized data (e.g., several time stamps associated with one or more images, etc.).  There are\ndifferent types of metadata.  Each type of metadata (also referred to as \"metadata type\") describes one or more characteristics or attributes associated with one or more DAs.  Each metadata type can be categorized as primitive metadata or inferred\nmetadata, as described in further detail below.\n For one embodiment, the DAM module/logic 140 can identify primitive metadata associated with one or more DAs within the DA metadata 170.  For a further embodiment, the DAM module/logic 140 may determine inferred metadata based at least on the\nprimitive metadata.\n As used herein, \"primitive metadata\" refers to metadata that describes one or more characteristics or attributes associated with one or more DAs.  That is, primitive metadata includes acquired metadata describing one or more DAs.  In some\nscenarios, primitive metadata can be extracted from inferred metadata, as described in further detail below.  In accordance with this disclosure, there are two categories of primitive metadata--(i) primary primitive metadata; and (ii) auxiliary primitive\nmetadata.\n Primary primitive metadata can include one or more of: time metadata; geo-position metadata; geolocation metadata; people metadata; scene metadata; content metadata; object metadata; and sound metadata.  Time metadata refers to a time associated\nwith one or more DAs (e.g., a timestamp associated with a DA, a time the DA is generated, a time the DA is modified, a time the DA is stored, a time the DA is transmitted, a time the DA is received, etc.).  Geo-position metadata refers to geographic or\nspatial attributes associated with one or more DAs using a geographic coordinate system (e.g., latitude, longitude, and/or altitude, etc.).  Geolocation metadata refers to one or more meaningful locations associated with one or more DAs rather than\ngeographic coordinates associated with the DA(s).  Examples include a beach (and its name) a street address, a country name, a region, a building, a landmark, etc. Geolocation metadata can, for example, be determined by processing geo-position\ninformation together with data from a map application to determine that the geolocation for a scene in a group of images.  People metadata refers to at least one detected or known person associated with one or more DAs (e.g., a known person in an image\ndetected through facial recognition techniques, etc.).  Scene metadata refers to an overall description of an activity or situation associated with one or more DAs.  For example, if a DA includes a group of images, then scene metadata for the group of\nimages can be determined using detected objects in images.  For a more specific example, the presence of a large cake with candles and balloons in at least two images in the group can be used to determine that the scene for the group of images is a\nbirthday celebration.  Object metadata refers to one or more detected objects associated with one or more DAs (e.g., a detected animal, a detected company logo, a detected piece of furniture, etc.).  Content metadata refers to the features of a DA (e.g.,\npixel characteristics, pixel intensity values, luminance values, brightness values, loudness levels, etc., etc.).  Sound metadata refers to one or more detected sounds associated with one or more DAs (e.g., a detected sound is a human's voice, a detected\nsound is a fire truck's siren, etc.).\n Auxiliary primitive metadata includes, but is not limited to, the following: (i) a condition associated with capturing one or more DAs; (ii) a condition associated with modifying one or more DAs; and (iii) a condition associated with storing or\nretrieving one or more DAs.  Examples of a condition associated with capturing a DA include, but are not limited to, an image sensor or other electronic component used to generate a DA.  Examples of a condition associated with modifying a DA include, but\nare not limited to an algorithm or operation performed on a DA to convert it from one format to another, and an algorithm or operation performed on a DA to edit the DA's characteristics.  Examples of a condition associated with storing or retrieving a DA\ninclude, but are not limited to, a memory cell's logical address, a storage element's logical address, a network host at which the DA resides, and a physical address represented as a binary number on the address bus circuitry in order to enable a data\nbus to access a particular storage cell or a register in a memory mapped I/O device.\n For an illustrative example, primitive metadata associated with a DA (e.g., one or more images, etc.) can include the following: a capture time associated with the one or more images; a modification time associated with the one or more images; a\nstorage time associated with the one or more images; a storage location associated with the one or more images; an image processing operation performed on the one or more images; pixel values describing pixel intensities in the one or more images; a\ncategory/name of an imaging sensor used to capture the one or more images; and a geographic or spatial location (e.g., latitude, longitude, altitude, etc.) associated with capture, modification, storage, or processing of the one or more images as\nobtained from a global positioning system (GPS) or other known tracking device.\n As used herein, \"inferred metadata\" refers to additional information about one or more DAs that is beyond the information provided by primitive metadata.  One difference between primitive metadata and inferred metadata is that primitive metadata\nrepresents an initial set of descriptions of one or more DA while inferred metadata provides additional descriptions of the one or more DAs based on processing one or more of the primitive metadata (i.e., the initial set of descriptions) and contextual\ninformation.  For example, primitive metadata may identify two detected persons in a group of images as John Doe and Jane Doe, while inferred metadata may identify John Doe and Jane Doe as a married couple based on processing one or more of the primitive\nmetadata (i.e., the initial set of descriptions) and contextual information.  For one embodiment, inferred metadata is formed from at least one of: (i) a combination of different types of primitive metadata; (ii) a combination of different types of\ncontextual information; or (iii) a combination of primitive metadata and contextual information.\n As used herein, \"context\" and its variations refer to any or all attributes of a user's device that includes or has access to a DA collection associated with the user, such as physical, logical, social, and other contextual information.  As used\nherein, \"contextual information\" and its variations refer to metadata that describes or defines a user's context or a context of a user's device that includes or has access to a DA collection associated with the user.  Exemplary contextual information\nincludes, but is not limited to, the following: a predetermined time interval; an event scheduled to occur in a predetermined time interval; a geolocation to be visited in a predetermined time interval; one or more identified persons associated with a\npredetermined time; an event scheduled for a predetermined time, or a geolocation to be visited at predetermined time; weather metadata describing weather associated with a particular period in time (e.g., rain, snow, sun, temperature, etc.); season\nmetadata describing a season associated with capture of the image.  For some embodiments, the contextual information can be obtained from external sources, a social networking application, a weather application, a calendar application, an address book\napplication, any other type of application, or from any type of data store accessible via a wired or wireless network (e.g., the Internet, a private intranet, etc.).\n Two categories of inferred metadata are set forth herein--(i) primary inferred metadata; and (ii) auxiliary inferred metadata.  Primary inferred metadata can include event metadata describing one or more events associated with one or more DAs. \nFor example, if a DA includes one or more images, the primary inferred metadata can include event metadata describing one or more events where the one or more images were captured (e.g., a vacation, a birthday, a sporting event, a concert, a graduation\nceremony, a dinner, a project, a work-out session, a traditional holiday, etc.).  Primary inferred metadata can, in some embodiments, be determined by clustering one or more of primary primitive metadata, auxiliary primitive metadata, and contextual\nmetadata.\n Auxiliary inferred metadata includes, but is not limited to, the following: (i) geolocation relationship metadata; (iii) person relationship metadata; (iii) object relationship metadata; and (iv) sound relationship metadata.  Geolocation\nrelationship metadata refers to a relationship between one or more known persons associated with one or more DAs and one or more meaningful locations associated with the one or more DAs.  For example, an analytics engine or data mining technique can be\nused to determine that a scene associated with one or more images of John Doe represents John Doe's home.  Person relationship metadata refers to a relationship between one or more known persons associated with one or more DAs and one or more other known\npersons associated with the one or more DAs.  For example, an analytics engine or data mining technique can be used to determine that Jane Doe (who appears in one or more images with John Doe) is John Doe's wife.  Object relationship metadata refers to a\nrelationship between one or more known objects associated with one or more DAs and one or more known persons associated with the one or more DAs.  For example, an analytics engine or data mining technique can be used to determine that a boat appearing in\none or more images with John Doe is owned by John Doe.  Sound relationship metadata refers to a relationship between one or more known sounds associated with one or more DAs and one or more known persons associated with the one or more DAs.  For example,\nan analytics engine or data mining technique can be used to determine that a voice that appears in one or more videos with John Doe is John Doe's voice.\n As explained above, inferred metadata may be determined or inferred from primitive metadata and/or contextual information by performing at least one of the following: (i) data mining the primitive metadata and/or contextual information; (ii)\nanalyzing the primitive metadata and/or contextual information; (iii) applying logical rules to the primitive metadata and/or contextual information; or (iv) any other known methods used to infer new information from provided or acquired information. \nAlso, primitive metadata can be extracted from inferred metadata.  For a specific embodiment, primary primitive metadata (e.g., time metadata, geolocation metadata, scene metadata, etc.) can be extracted from primary inferred metadata (e.g., event\nmetadata, etc.).  Techniques for determining inferred metadata and/or extracting primitive metadata from inferred metadata can be iterative.  For a first example, inferring metadata can trigger the inference of other metadata and so on.  For a second\nexample, extracting primitive metadata from inferred metadata can trigger inference of additional inferred metadata or extraction of additional primitive metadata.\n Referring again to FIG. 1A, the primitive metadata and the inferred metadata described above are collectively referred to as the DA metadata 170.  For one embodiment, the DAM module/logic 140 uses the DA metadata 170 to generate a metadata\nnetwork 175.  As shown in FIG. 1A, all or some of the metadata network 175 can be stored in the processing unit(s) 130 and/or the memory 160.  As used herein, a \"knowledge graph,\" a \"knowledge graph metadata network,\" a \"metadata network,\" and their\nvariations refer to a dynamically organized collection of metadata describing one or more DAs (e.g., one or more groups of DAs in a DA collection, one or more DAs in a DA collection, etc.) used by one or more computer systems for deductive reasoning.  In\na metadata network, there is no DA--only metadata (e.g., metadata associated with one or more groups of DAs, metadata associated with one or more DAs, etc.).  Metadata networks differ from databases because, in general, a metadata network enables deep\nconnections between metadata using multiple dimensions, which can be traversed for additionally deduced correlations.  This deductive reasoning generally is not feasible in a conventional relational database without loading a significant number of\ndatabase tables (e.g., hundreds, thousands, etc.).  As such, conventional databases may require a large amount of computational resources (e.g., external data stores, remote servers, and their associated communication technologies, etc.) to perform\ndeductive reasoning.  In contrast, a metadata network may be viewed, operated, and/or stored using fewer computational resource requirements than the preceding example of databases.  Furthermore, metadata networks are dynamic resources that have the\ncapacity to learn, grow, and adapt as new information is added to them.  This is unlike databases, which are useful for accessing cross-referred information.  While a database can be expanded with additional information, the database remains an\ninstrument for accessing the cross-referred information that was put into it.  Metadata networks do more than access cross-referred information--they go beyond that and involve the extrapolation of data for inferring or determining additional data.\n As explained in the preceding paragraph, a metadata network enables deep connections between metadata using multiple dimensions in the metadata network, which can be traversed for additionally deduced correlations.  Each dimension in the\nmetadata network may be viewed as a grouping of metadata based on metadata type.  For example, a grouping of metadata could be all time metadata assets in a metadata collection and another grouping could be all geo-position metadata assets in the same\nmetadata collection.  Thus, for this example, a time dimension refers to all time metadata assets in the metadata collection and a geo-position dimension refers to all geo-position metadata assets in the same metadata collection.  Furthermore, the number\nof dimensions can vary based on constraints.  Constraints include, but are not limited to, a desired use for the metadata network, a desired level of detail, and/or the available metadata or computational resources used to implement the metadata network. For example, the metadata network can include only a time dimension, the metadata network can include all types of primitive metadata dimensions, etc. With regard to the desired level of detail, each dimension can be further refined based on specificity\nof the metadata.  That is, each dimension in the metadata network is a grouping of metadata based on metadata type and the granularity of information described by the metadata.  For a first example, there can be two time dimensions in the metadata\nnetwork, where a first time dimension includes all time metadata assets classified by week and the second time dimension includes all time metadata assets classified by month.  For a second example, there can be two geolocation dimensions in the metadata\nnetwork, where a first geolocation dimension includes all geolocation metadata assets classified by type of establishment (e.g., home, business, etc.) and the second geolocation dimension includes all geolocation metadata assets classified by country. \nThe preceding examples are merely illustrative and not restrictive.  It is to be appreciated that the level of detail for dimensions can vary depending on designer choice, application, available metadata, and/or available computational resources.\n The DAM module/logic 140 may generate the metadata network 175 as a multidimensional network of the DA metadata 170.  As used herein, a \"multidimensional network\" and its variations refer to a complex graph having multiple kinds of\nrelationships.  A multidimensional network generally includes multiple nodes and edges.  For one embodiment, the nodes represent metadata, and the edges represent relationships or correlations between the metadata.  Exemplary multidimensional networks\ninclude, but are not limited to, edge-labeled multigraphs, multipartite edge-labeled multigraphs, and multilayer networks.\n For one embodiment, the nodes in the metadata network 175 represent metadata assets found in the DA metadata 170.  For example, each node represents a metadata asset associated with one or more DAs in a DA collection.  For another example, each\nnode represents a metadata asset associated with a group of DAs in a DA collection.  As used herein, a \"metadata asset\" and its variations refer to metadata (e.g., a single instance of metadata, a group of multiple instances of metadata, etc.) describing\none or more characteristics of one or more DAs in a DA collection.  As such, there can be a primitive metadata asset, an inferred metadata asset, a primary primitive metadata asset, an auxiliary primitive metadata asset, a primary inferred metadata\nasset, and/or an auxiliary inferred metadata asset.  For a first example, a primitive metadata asset refers to a time metadata asset describing a time interval between Jun.  1, 2016 and Jun.  3, 2016 when one or more DAs were captured.  For a second\nexample, a primitive metadata asset refers to a geo-position metadata asset describing one or more latitudes and/or longitudes where one or more DAs were captured.  For a third example, an inferred metadata asset refers to an event metadata asset\ndescribing a vacation in Paris, France between Jun.  5, 2016 and Jun.  30, 2016 when one or more DAs were captured.\n For one embodiment, the metadata network 175 includes two types of nodes--(i) moment nodes; and (ii) non-moments nodes.  As used herein, a \"moment\" refers a single event (as described by an event metadata asset) that is associated with one or\nmore DAs.  For example, a moment refers to a vacation in Paris, France that lasted between Jun.  1, 2016 and Jun.  9, 2016.  For this example, the moment can be used to identify one or more DAs (e.g., one image, a group of images, a video, a group of\nvideos, a song, a group of songs, etc.) associated with the vacation in Paris, France that lasted between Jun.  1, 2016 and Jun.  9, 2016 (and not with any other event).\n As used herein, a \"moment node\" refers to a node in a multidimensional network that represents a moment (which is described above).  Thus, a moment node refers to a primary inferred metadata asset representing a single event associated with one\nor more DAs.  Primary inferred metadata is described above.  As used herein, a \"non-moment node\" refers a node in a multidimensional network that does not represent a moment.  Thus, a non-moment node refers to at least one of the following: (i) a\nprimitive metadata asset associated with one or more DAs; or (ii) an inferred metadata asset associated with one or more DAs that is not a moment (i.e., not an event metadata asset).\n As used herein, an \"event\" and its variations refer to a situation or an activity occurring at one or more locations during a specific time interval.  An event includes, but is not limited to the following: a gathering of one or more persons to\nperform an activity (e.g., a holiday, a vacation, a birthday, a dinner, a project, a work-out session, etc.); a sporting event (e.g., an athletic competition, etc.); a ceremony (e.g., a ritual of cultural significance that is performed on a special\noccasion, etc.); a meeting (e.g., a gathering of individuals engaged in some common interest, etc.); a festival (e.g., a gathering to celebrate some aspect in a community, etc.); a concert (e.g., an artistic performance, etc.); a media event (e.g., an\nevent created for publicity, etc.); and a party (e.g., a large social or recreational gathering, etc.).\n For one embodiment, the edges in the metadata network 175 between nodes represent relationships or correlations between the nodes.  For one embodiment, the DAM module/logic 140 updates the metadata network 175 as the DAM module/logic 140 obtains\nor receives new primitive metadata 170 and/or determines new inferred metadata 170 based on the new primitive metadata 170.\n The DAM module/logic 140 can manage DAs associated with the DA metadata 170 using the metadata network 175.  For a first example, DAM module/logic 140 can use the metadata network 175 to relate multiple DAs based on the correlations (i.e., the\nedges in the metadata network 175) between the DA metadata 170 (i.e., the nodes in the metadata network 175).  For this first example, the DAM module/logic 140 relates the a first group of one or more DAs with a second group of one or more DAs based on\nthe metadata assets that are represented as moment nodes in the metadata network 175.  For a second example, DAM module/logic 140 uses the metadata network 175 to locate and present interesting groups of one or more DAs in DA collection based on the\ncorrelations (i.e., the edges in the metadata network 175) between the DA metadata (i.e., the nodes in the metadata network 175) and predetermined criterion.  For this second example, the DAM module/logic 140 selects the interesting DAs based on moment\nnodes in the metadata network 175.  Furthermore, and for this second example, the predetermined criterion refers to contextual information (which is described above).  The predetermined time interval can be a current time interval or a future time\ninterval.  For a third example, the DAM module/logic 140 uses the metadata network 175 to select and present a representative group of one or more DAs that summarize a moment's DAs based on the correlations (i.e., the edges in the metadata network 175)\nbetween the DA metadata (i.e., the nodes in the metadata network 175) and input specifying the representative group's size.  For this third example, the DAM module/logic 140 selects the representative DAs based on an event metadata asset.  The event\nmetadata asset can, but is not required to, be a moment node in the metadata network 175 associated with one or more DAs.\n The system 100 can also include memory 160 for storing and/or retrieving metadata 170, the metadata network 175, and/or optional data 180 described by or associated with the metadata 170.  The metadata 170, the metadata network 175, and/or the\noptional data 180 can be generated, processed, and/or captured by the other components in the system 100.  For example, the metadata 170, the metadata network 175, and/or the optional data 180 includes data generated by, captured by, processed by, or\nassociated with one or more peripherals 190, the DA capture device 120, or the processing unit(s) 130, etc. The system 100 can also include a memory controller (not shown), which includes at least one electronic circuit that manages data flowing to\nand/or from the memory 160.  The memory controller can be a separate processing unit or integrated in processing unit(s) 130.\n The system 100 can include a DA capture device 120 (e.g., an imaging device for capturing images, an audio device for capturing sounds, a multimedia device for capturing audio and video, any other known DA capture device, etc.).  Device 120 is\nillustrated with a dashed box to show that it is an optional component of the system 100.  Nevertheless, the DA capture device 120 is not always an optional component of the system 100--some embodiments of the system 100 may require the DA capture device\n120 (e.g., a camera, a smartphone with a camera, etc.).  For one embodiment, the DA capture device 120 can also include a signal processing pipeline that is implemented as hardware, software, or a combination thereof.  The signal processing pipeline can\nperform one or more operations on data received from one or more components in the device 120.  The signal processing pipeline can also provide processed data to the memory 160, the peripheral(s) 190, and/or the processing unit(s) 130.\n The system 100 can also include peripheral(s) 190.  For one embodiment, the peripheral(s) 190 can include at least one of the following: (i) one or more input devices that interact with or send data to one or more components in the system 100\n(e.g., mouse, keyboards, etc.); (ii) one or more output devices that provide output from one or more components in the system 100 (e.g., monitors, printers, display devices, etc.); or (iii) one or more storage devices that store data in addition to the\nmemory 160.  Peripheral(s) 190 is illustrated with a dashed box to show that it is an optional component of the system 100.  Nevertheless, the peripheral(s) 190 is not always an optional component of the system 100--some embodiments of the system 100 may\nrequire the peripheral(s) 190 (e.g., a smartphone with media recording and playback capabilities, etc.).  The peripheral(s) 190 may also refer to a single component or device that can be used both as an input and output device (e.g., a touch screen,\netc.).  The system 100 may include at least one peripheral control circuit (not shown) for the peripheral(s) 190.  The peripheral control circuit can be a controller (e.g., a chip, an expansion card, or a stand-alone device, etc.) that interfaces with\nand is used to direct operation(s) performed by the peripheral(s) 190.  The peripheral(s) controller can be a separate processing unit or integrated in processing unit(s) 130.  The peripheral(s) 190 can also be referred to as input/output (I/O) devices\n190 throughout this document.\n The system 100 can also include one or more sensors 191, which are illustrated with a dashed box to show that the sensor can be optional components of the system 100.  Nevertheless, the sensor(s) 191 are not always optional components of the\nsystem 100--some embodiments of the system 100 may require the sensor(s) 191 (e.g., a camera that includes an imaging sensor, etc.).  For one embodiment, the sensor(s) 191 can detect a characteristic of one or more environs.  Examples of a sensor\ninclude, but are not limited to, a light sensor, an imaging sensor, an accelerometer, a sound sensor, a barometric sensor, a proximity sensor, a vibration sensor, a gyroscopic sensor, a compass, a barometer, a heat sensor, a rotation sensor, a velocity\nsensor, and an inclinometer.\n For one embodiment, the system 100 includes communication mechanism 110.  The communication mechanism 110 can be a bus, a network, or a switch.  When the technology 110 is a bus, the technology 110 is a communication system that transfers data\nbetween components in system 100, or between components in system 100 and other components associated with other systems (not shown).  As a bus, the technology 110 includes all related hardware components (wire, optical fiber, etc.) and/or software,\nincluding communication protocols.  For one embodiment, the technology 110 can include an internal bus and/or an external bus.  Moreover, the technology 110 can include a control bus, an address bus, and/or a data bus for communications associated with\nthe system 100.  For one embodiment, the technology 110 can be a network or a switch.  As a network, the technology 110 may be any network such as a local area network (LAN), a wide area network (WAN) such as the Internet, a fiber network, a storage\nnetwork, or a combination thereof, wired or wireless.  When the technology 110 is a network, the components in the system 100 do not have to be physically co-located.  When the technology 110 is a switch (e.g., a \"cross-bar\" switch), separate components\nin system 100 may be linked directly over a network even though these components may not be physically located next to each other.  For example, two or more of the processing unit(s) 130, the communication technology 110, the memory 160, the\nperipheral(s) 190, the sensor(s) 191, and the DA capture device 120 are in distinct physical locations from each other and are communicatively coupled via the communication technology 110, which is a network or a switch that directly links these\ncomponents over a network.\n FIG. 1B illustrates, in block diagram form, an exemplary metadata network 175 in accordance with one embodiment.  The exemplary metadata network 175 illustrated in FIG. 1B can be generated and used by the processing system 100 illustrated in\nFIG. 1A to perform DAM in accordance with an embodiment.  For one embodiment, the metadata network 175 illustrated in FIG. 1B is similar to or the same as the metadata network 175 described above in connection with FIG. 1A.  It is to be appreciated that\nthe metadata network 175 described in FIG. 1B is exemplary and that every node that can be generated by the DAM module/logic 140 is not shown.  For example, even though every possible node is not illustrated in FIG. 1B, the DAM module/logic 140 can\ngenerate a node to represent each metadata asset illustrated in boxes 205-210 of FIG. 1B.\n In the metadata network 175 illustrated in FIG. 1B, nodes representing metadata are illustrated as circles and edges representing correlations between the metadata are illustrated as labeled connections between circles.  Furthermore, moment\nnodes are represented as circles with thickened boundaries while other non-moment nodes lack the thickened boundaries.  In addition, the metadata assets shown in boxes 205, 210, and 215 can be represented as non-moment nodes in the metadata network 175.\n Generating the metadata network 175, by the DAM module/logic 140, can include defining nodes based on the primitive metadata and/or the inferred metadata associated with one or more DAs in a DA collection.  As the DAM module/logic 140 identifies\nmore primitive metadata within the metadata associated with a DA collection and/or infers metadata from at least the primitive metadata, the DAM module/logic 140 can generate additional nodes to represent the primitive metadata and/or the inferred\nmetadata.  Furthermore, as the DAM module/logic 140 determines correlations between nodes, the DAM module/logic 140 can create edges between the nodes.  Two generation processes can be used to create the metadata network 175.  The first generation\nprocess is initiated using a metadata asset that does not describe a moment (e.g., primary primitive metadata asset, an auxiliary primitive metadata asset, an auxiliary inferred metadata asset etc.).  The second generation process is initiated using a\nmetadata asset that describes a moment (e.g., an event metadata).  Each of these generation processes is described below.\n For the first generation process, the DAM module/logic 140 can generate a non-moment node 223 to represent metadata associated with a user, a consumer, or an owner of a DA collection associated with the metadata network 175.  As illustrated in\nFIG. 1B, a user is identified as Jean Dupont.  For one embodiment, the DAM module/logic 140 generates the non-moment node 223 to represent the metadata 210 provided by the user (e.g., Jean Dupont, etc.) via an input device.  For example, the user can add\nat least some of the metadata 210 about herself or himself to the metadata network 175 via an input device.  In this way, the DAM module/logic 140 can use the metadata 210 to correlate the user with other metadata acquired from a DA collection.  For\nexample, and as shown in FIG. 1B, the metadata 210 provided by the user Jean Dupont can include one or more of his name, his birthplace (which is Paris, France), his birthdate (which is May 27, 1991), his gender (which is male), his relationship status\n(which is married), his significant other or spouse (which is Marie Dupont), and his current residence (which is in Key West, Fla., USA).\n Still with regard to the first generation process, at least some of the metadata 210 can be predicted based on processing performed by the DAM module/logic 140.  The DAM module/logic 140 may predict metadata 210 based on an analysis of metadata\naccessed via an application or metadata in a data store (e.g., memory 160 of FIG. 1, etc.).  For example, the DAM module/logic 140 may predict the metadata 210 based on analyzing information acquired by accessing the user's contacts (via a contacts\napplication), activities (via a calendar application or an organization application), contextual information (via sensor(s) 191 and/or peripheral(s) 190), and/or social networking data (via a social networking application).\n For one embodiment, the metadata 210 includes, but is not limited to, other metadata, such as the user's relationships with other others (e.g., family members, friends, co-workers, etc.), the user's workplaces (e.g., past workplaces, present\nworkplaces, etc.), the user's interests (e.g., hobbies, DAs owned, DAs consumed, DAs used, etc.), places visited by the user (e.g., previous places visited by the user, places that will be visited by the user, etc.).  For one embodiment, the metadata 210\ncan be used alone or in conjunction with other data to determine or infer at least one of the following: (i) vacations or trips taken by Jean Dupont (e.g., nodes 231, etc.); days of the week (e.g., weekends, holidays, etc.); locations associated with\nJean Dupont (e.g., nodes 231, 233, 235, etc.); Jean Dupont's social group (e.g., his wife Marie Dupont represented in node 227, etc.); Jean Dupont's professional or other groups (e.g., groups based on his occupation, etc.); types of places visited by\nJean Dupont (e.g., Prime 114 restaurant represented in node 229, Home represented by node 225, etc.); activities performed (e.g., a work-out session, etc.); etc. The preceding examples are illustrative and not restrictive.\n For the second generation process in FIG. 1B, the metadata network 175 may include at least one moment node--for example, the moment node 220A and moment node 220B.  Other embodiments of the metadata network 175, however, are not so limited. \nFor example, the metadata network 175 can include less than two moment nodes or more than two moment nodes.  For this second generation process, the DAM module/logic 140 generates the moment node 220A and the moment node 220B to represent one or more\nprimary inferred metadata assets (e.g., an event metadata asset, etc.).  The DAM module/logic 140 can determine or infer the primary inferred metadata (e.g., an event metadata asset, etc.) from one or more of the information 210, the metadata 205, the\nmetadata 215, and other data received from external sources (e.g., weather application, calendar application, social networking application, address book application, etc.).  Also, the DAM module/logic 140 may receive the primary inferred metadata\nassets, generate this metadata as the moment node 220A and the moment node 220B, and extract primary primitive metadata 205 and 215 from the primary inferred metadata assets represented as the moment node 220A and the moment node 220B.  The primary\nprimitive metadata assets illustrated in boxes 205 and 215 can include more or less than the metadata assets illustrated in FIG. 1B.  For example, primary primitive metadata can also include altitude, relative geographical coordinates, week of the year,\nday of the week, month of the year, season, relative time, additional objects, additional scene descriptions, etc.\n For one embodiment, the metadata network 175 also includes non-moment nodes 223, 225, 227, 229, 231, 233, 235, and 237.  The DAM module/logic 140 can generate additional nodes based on moment nodes as follows: (i) the DAM module/logic 140\ndetermines auxiliary primitive metadata assets associated with the moment nodes 220A-B by cross-referencing the auxiliary primitive metadata assets with primary primitive metadata assets and/or primary inferred metadata assets in a metadata collection;\n(ii) the DAM module/logic 140 determines or infers auxiliary inferred metadata assets associated with the moment nodes 220A-B based on the auxiliary primitive metadata assets, the primary primitive metadata assets, and/or the primary inferred metadata\nassets; and (iii) the DAM module/logic 140 generates a node for each auxiliary inferred metadata asset, each auxiliary primitive metadata asset, each primary primitive metadata asset, and/or each primary inferred metadata asset.  For a first example, and\nas illustrated in FIG. 1B, the DAM module/logic 140 generates non-moment nodes 233, 231, 229, 235, and 237 after determining and/or inferring metadata assets associated with the moment node 220A.  For a second example, the DAM module/logic 140 generates\nnodes 225 and 227 after determining and/or inferring metadata assets associated with the moment node 220B.\n For one embodiment, the DAM module/logic 140 can refine each metadata asset associated with the moment nodes 220A-B based on a probability distribution (e.g., a discrete probability distribution, a continuous probability distribution, etc.). \nFor example, a Gaussian distribution may be used to determine a distribution of the primary primitive metadata assets.  For this example, the distribution may be used to ascertain a mean, a median, a mode, a standard deviation, and/or a variance\nassociated with the distribution of the primary primitive metadata assets.  The DAM module/logic 140 can use the Gaussian distribution to select or filter out a sub-set of the primary primitive metadata assets that is within a predetermined criterion\n(e.g., 1 standard deviation (68%), 2 standard deviations (95%), or 3 standard deviations (99.7%), etc.).  Hence, this selection/filtering operation can assist with identifying relevant primary primitive metadata assets for DAM and with filtering out\nnoise or unreliable primary primitive metadata assets.  Consequently, all the other types of metadata (e.g., auxiliary primitive metadata assets, primary inferred metadata assets, auxiliary inferred metadata assets, etc.) that are associated with,\ndetermined from, or inferred from the primary primitive metadata assets may also be relevant and relatively noise-free.  For a second example, a Gaussian distribution may be used to determine a distribution of the primary inferred metadata assets (i.e.,\nmoment nodes).  For this example, the distribution may be used to ascertain a mean, a median, a mode, a standard deviation, and/or a variance associated with the distribution of the moments.  The DAM module/logic 140 can use the Gaussian distribution to\nselect or filter out a sub-set of the primary inferred metadata assets (i.e., moment nodes) that is within a predetermined criterion (e.g., 1 standard deviation (68%), 2 standard deviations (95%), or 3 standard deviations (99.7%), etc.).  Hence, this\nselection/filtering operation can assist with identifying relevant primary inferred metadata assets (i.e., moment nodes) for DAM and with filtering out noise or unreliable primary inferred metadata assets.  Consequently, all the other types of metadata\n(e.g., primary primitive metadata assets, auxiliary primitive metadata assets, auxiliary inferred metadata assets, etc.) that are associated with, determined from, or extracted from the primary inferred metadata assets may also be relevant and relatively\nnoise-free.\n Noise can occur due to primary primitive metadata assets that are associated one or more irrelevant DAs.  Such DAs can be determined based on the number of DAs associated with a primary primitive metadata asset.  For example, a primary primitive\nmetadata asset associated with two or less DAs can be designated as noise.  This is because such metadata assets (and their DAs) may be irrelevant given the little information they provide.  For example, the more important or significant an event is to a\nuser, the higher the likelihood that the event is captured using a large number of images (e.g., three or more, etc.).  For this example, the probability distribution described above can enable selecting the primary primitive metadata asset associated\nwith these DAs.  This is because the number of DAs associated with the event may suggest an importance or relevance of the primary primitive metadata asset.  In contrast, insignificant events may have only one or two images, and the corresponding primary\nprimitive metadata asset may not add much to DAM based on the metadata network described herein.  The immediately preceding examples are also applicable to the primary inferred metadata, the auxiliary primitive metadata, and the auxiliary inferred\nmetadata.\n For one embodiment, the DAM module/logic 140 determines a confidence weight and/or a relevance weight for at least some, and possibly each, of the primary primitive metadata assets, the primary inferred metadata assets, the auxiliary primitive\nmetadata assets, and the auxiliary inferred metadata assets associated with the moment node 220A-B.\n As used herein, a \"confidence weight\" and its variations refer to a value (e.g., an integer, etc.) used to describe a certainty that some metadata correctly identifies a feature or characteristic of one or more DAs associated with a moment.  For\nexample, a confidence weight of 0.6 (out of a maximum of 1.0) can be used to indicate a 60% confidence level that a feature in one or more digital images associated with a moment is a dog.\n As used herein, a \"relevance weight\" and its variations refer to a value (e.g., an integer, etc.) used to describe an importance assigned to a feature or characteristic of one or more DAs associated with a moment as identified by a metadata\nasset.  For example, a first relevance weight of 0.85 (out of a maximum of 1.0) can be used indicate that a first identified feature in a digital image (e.g., a person) is very important while a second relevance weight of 0.50 (out of a maximum of 1.0)\ncan be used indicate that a second identified feature in a digital image (e.g., a dog) is not very important.\n As shown in FIG. 1B, and for one example, the DAM module/logic 140 estimates that one or more metadata assets associated with the moment node 220A describe Jean Dupont's birthday.  For this example, the confidence weight 239 is assigned a value\nof 0.8 to indicate an 80% confidence level that Jean Dupont's birthday is described by one or more metadata assets illustrated in box 205.  Furthermore, and for this example, a relevance weight 239 is assigned a value of is 0.9 (out of a maximum of 1.0)\nto indicate that Jean Dupont's birthday is an important feature in the metadata asset(s) illustrated in box 205.  For this example, the important metadata asset illustrated in box 205 can include the date associated with moment 220A, which is illustrated\nas May 27, 2016.  The DAM module/logic 140 can compare the data shown in box 205 with Jean Dupont's known birthday 233 of May 27, 1991 to determine the confidence weight 235 and the relevance weight 235.  For another example, the DAM module/logic 140 may\ncompare Jean Dupont's known birthday 233 against some or all metadata assets of a date type until a moment (e.g., moment 220A) that includes time metadata with the same or similar date as Jean Dupont's known birthday 233 is found (e.g., the time metadata\nasset shown in box 205, etc.).\n With specific regard to images, confidence weights and relevance weights may be detected via feature detection techniques that include analyzing metadata associated with one or more images.  For one embodiment, the DAM module/logic 140 can\ndetermine confidence levels and relevance weights using metadata associated with one or more DAs by applying known feature detection techniques.  Relevance can be statically defined in the metadata network from external constraints.  For example,\nrelevance can be based on information acquired from other sources, like social networking data, calendar data, etc. Also, relevance may be based on internal constraints.  That is, as more detections of a metadata asset are made, its relevance can be\nincreased.  Relevance can also retard as fewer detections are made.  For example, as more detections of Marie Dupont 227 are made over a predetermined period of time (e.g., an hour, a day, a week, a year, etc.), her relevance is increased to indicate her\nimportance to Jean Dupont.  Confidence can be dynamically generated based on the ingest of metadata in the metadata network.  For instance, a detected person in an image may be linked with information about that person from a contacts application, a\ncalendar application, social networking application, or other application to determine a level of confidence that the detected person is correctly identified.  For a further example, the overall description of a scene in the image may be linked with\ngeo-position information acquired from primary inferred metadata associated with the detected person to determine the level of confidence.  Other examples are possible.  In addition, confidence can be based on internal constraints.  That is, as more\ndetections of a metadata asset are made, its identification confidence is increased.  Confidence can also retard as fewer detections are made.\n The DAM module/logic 140 can generate edges representing correlations between nodes (i.e., the metadata assets) in the metadata network 175.  For one embodiment, the DAM module/logic 140 determines correlations between the nodes in the metadata\nnetwork 175 based on the confidence weights and the relevance weights.  For a further embodiment, the DAM module/logic 140 determines correlations between nodes in the metadata network 175 based on the confidence weight between two nodes being greater\nthan or equal to a confidence threshold and/or the relevance weight between two nodes being greater than or equal to a relevance threshold.  For one embodiment, the correlation between the two nodes is determined based on a combination of the confidence\nweight and the relevance weight between the two nodes being equal to or greater than a threshold correlation.  For example, and as shown in FIG. 1B, the DAM module/logic 140 can generate an edge 239 to indicate a correlation between the metadata asset\nrepresented by a node 233, which describes Jean Dupont's birthday and the metadata asset represented by the moment node 220A.  For this example, the DAM module/logic 140 can generate the edge 239 based on the DAM module/logic 140 determining that the\nconfidence weight associated with the edge 239 is greater than or equal to a confidence threshold and/or that the relevance weight associated with the edge 239 is greater than or equal to a relevance threshold.\n Referring now to FIG. 2, which is a flowchart representing an operation 200 to perform DAM according to an embodiment.  Operation 200 can be performed by a DAM logic/module (e.g., the DAM module/logic 140 described above in connection with FIGS.\n1A-1B).  Operation 200 begins at block 291, where a metadata network is received or generated.  The metadata network can be similar to or the same as the metadata network 175 described above in connection with FIGS. 1A-1B.  The metadata network can be\nobtained from memory (e.g., memory 160 described above in connection with FIG. 1A).  Additionally, or alternatively, the metadata network can be generated by processing unit(s) (e.g., the processing unit(s) 130 described above in connection with FIGS.\n1A-1B.  Block 291 can be performed according to one or more descriptions provided above in connection with FIGS. 1A-1B.  Operation 200 proceeds to block 293, where a first metadata asset (e.g., a moment node, a non-moment node, etc.) is identified in the\nmultidimensional network representing the metadata network.  For one embodiment, the first metadata asset is represented as a moment node.  For this embodiment, the first metadata asset represents a first event associated with one or more DAs.  At block\n295, a second metadata asset is identified or detected based at least on the first metadata asset.  The second metadata asset may be identified or detected in the metadata network as a second node (e.g., a moment node, a non-moment node, etc.) based on\nthe first node used to represent the first metadata asset.  For one embodiment, the second metadata asset is represented as a second moment node that differs from the first moment node.  This is because the first moment node represents a first event\nmetadata asset that describes a first event associated with one or more DAs while the second moment node represents a second event metadata asset that describes a second event associated with one or more DAs.\n For one embodiment, identifying the second metadata asset (e.g., a moment node, etc.) based on the first metadata asset (e.g., a moment node, etc.) is performed by determining that the first and second metadata assets share a primary primitive\nmetadata asset, a primary inferred metadata asset, an auxiliary primitive metadata asset, and/or an auxiliary inferred metadata asset even though some of their metadata differ.  For one embodiment, the shared metadata assets between the first and second\nmetadata assets may be selected based on the confidence and/or relevance weights between the metadata assets.  The shared metadata assets between the first and second metadata asset may be selected based on the confidence and/or relevance weights being\nequal to or greater than a threshold level of confidence and/or relevance.\n For one example, a first moment node could represent a first event metadata asset associated with multiple images that were taken at a public park in Houston, Tex.  between Jun.  1, 2016 and Jun.  3, 2016.  For this example, a second moment node\nthat represents a second moment node associated with multiple images could be identified based on the first moment node.  The second moment node could be identified by determining one or more other nodes (i.e., other metadata assets) that are associated\nwith one or more images that were taken at the same public park in Houston, Tex.  but on different dates (i.e., not between Jun.  1, 2016 and Jun.  3, 2016).  For a variation of this example, the second moment node could be identified based on the first\nmoment node by determining one or more other nodes (i.e., other metadata assets) associated with one or more images that were taken at another public park in Houston, Tex.  but on different dates (i.e., not between Jun.  1, 2016 and Jun.  3, 2016).  For\nyet another variation of this example, the second moment node could be identified based on the first moment node by determining one or more other nodes (i.e., other metadata assets) associated with one or more images that were taken at another public\npark outside Houston, Tex.  but on different dates (i.e., not between Jun.  1, 2016 and Jun.  3, 2016).  Operation 200 can proceed to block 297, where at least one DA associated with the first metadata asset or the second metadata asset is presented via\nan output device.  For example, one or more images of the identified public park in Houston, Tex.  can be presented on a display device.\n FIG. 3A illustrates, in flowchart form, an operation 300 to generate an exemplary metadata network for DAM in accordance with an embodiment.  Operation 300 can be performed by a DAM logic/module (e.g., the DAM logic/module described above in\nconnection with FIGS. 1A-1B, etc.).  Each of blocks 301-305B can be performed in accord with descriptions provided above in connection with FIGS. 1A-2.\n Operation 300 begins at block 301, where DA metadata associated with a DA collection (hereinafter \"a metadata collection\") is obtained or received.  The metadata collection can be received or obtained from a memory (e.g., memory 160 described\nabove in connection with FIG. 1A, etc.).  For one embodiment, the metadata collection includes at least one of the following: (i) one or more primary primitive metadata assets associated with one or more DAs in the DA collection; (ii) one or more\nauxiliary primitive metadata assets associated with one or more DAs in the DA collection; or (iii) one or more primary inferred metadata assets associated with one or more DAs in the DA collection.\n At block 303, the metadata collection is analyzed for primary primitive metadata assets, auxiliary primitive metadata assets, primary inferred metadata assets, and auxiliary inferred metadata assets.  The analysis at block 303 can begin by\nidentifying primary primitive metadata asset(s) and/or primary inferred metadata asset(s) in the metadata collection.  When the metadata collection includes primary primitive metadata asset(s), such asset(s) can be used to infer at least one primary\ninferred metadata asset.  Alternatively or additionally, when the metadata collection includes the primary inferred metadata asset(s), at least one primary metadata asset can be extracted from the primary inferred metadata asset(s).  For one embodiment,\nthe identified primary primitive metadata asset(s) and/or the identified primary inferred metadata asset(s) may be used to determine at least one auxiliary primary metadata asset or infer at least one auxiliary inferred metadata asset.\n For an embodiment, the auxiliary primitive metadata asset(s) in the metadata collection may be determined by cross-referencing the primary primitive metadata asset(s) and/or the primary inferred metadata asset(s) with auxiliary primitive\nmetadata asset(s) in the same metadata collection.  For example, auxiliary primitive metadata asset(s) can be determined by cross-referencing the primary primitive metadata asset(s) and/or the primary inferred metadata asset(s) with some or all other\nmetadata assets in the metadata collection and excluding any metadata asset in the metadata collection that is not an auxiliary primitive metadata asset until one or more auxiliary primitive metadata assets are found.  For a specific example, a primary\nprimitive metadata asset that represents a time metadata asset in a metadata collection can be used to determine an auxiliary primitive metadata asset in the same metadata collection that represents a condition associated with capturing a DA.  For this\nexample, the condition can include determining a working condition of an image sensor used to capture the DA at the specific time represented by the time metadata asset, which is determined by cross-referencing the time metadata asset with some or all\nother metadata assets in the metadata collection and excluding any metadata asset in the metadata collection that is not an auxiliary primitive metadata asset until one or more auxiliary primitive metadata assets are found.  For this example, the located\nauxiliary primitive metadata assets include the auxiliary primitive metadata asset that represents the working condition of the image sensor used to capture the DA\n For one embodiment, the auxiliary inferred metadata asset(s) in the metadata collection may be determined or inferred based on the auxiliary primitive metadata asset(s), the primary primitive metadata asset(s), and/or the primary inferred\nmetadata asset(s) in the same metadata collection.  For one embodiment, the auxiliary inferred metadata asset(s) in the metadata collection is determined by clustering auxiliary primitive metadata asset(s), the primary primitive metadata asset(s), and/or\nthe primary inferred metadata asset(s) in the same metadata collection with contextual or other information received from other sources.  For example, clustering multiple geo-position metadata assets in a metadata collection with information from a\ngeographic map received from a map application can be used determine a geolocation metadata asset.  For another embodiment, the auxiliary inferred metadata asset(s) in the metadata collection may be determined by cross-referencing the auxiliary primitive\nmetadata asset(s), the primary primitive metadata asset(s), and/or the primary inferred metadata asset(s) in the same metadata collection with some or all other metadata assets in the same metadata collection and excluding any metadata asset in the\nmetadata collection that is not an auxiliary inferred metadata asset(s) until one or more auxiliary inferred metadata assets are found.  It is to be appreciated that the two embodiments can be combined.\n Operation 300 can proceed to blocks 305A-B where, a metadata network is generated.  At blocks 305A-B, the generated metadata network can be a multidimensional network that includes nodes and edges.  For one embodiment, and with specific regard\nto block 305A, each node represents an auxiliary inferred metadata asset, an auxiliary primitive metadata asset, a primary primitive metadata asset, or a primary inferred metadata asset (i.e., a moment).  For another embodiment of block 305A, each node\nrepresenting a primary inferred metadata asset may be designated as a moment node.  At block 305B, the metadata network can determine and generate an edge for one or more pairs of nodes.  For one embodiment, each edge indicates a correlation between its\npair of metadata assets (i.e., nodes).\n FIGS. 3B-3C illustrate, in flowchart form, an operation 350 to generate an exemplary metadata network for DAM in accordance with an embodiment.  FIGS. 3B-3C provide additional details about the operation 300 illustrated in FIG. 3A.  Operation\n350 can be performed by a DAM logic/module (e.g., the module/logic 140 described above in connection with FIGS. 1A-1B).  For one embodiment, portions of the operation 300 and 350 may be combined or omitted as desired.\n Referring now to FIG. 3B, operation 350 begins at block 347 and proceeds to block 349, where a metadata collection associated with a DA collection is obtained or received.  Block 349 in FIG. 3B is similar to or the same as block 301 in FIG. 3A,\nwhich is described above in connection with FIG. 3A.  For brevity, this block is not described again.\n As shown in FIGS. 3B-3C, there can be N number of groups, where N refers to the number of one or more DAs in the collection having their own distinct primary inferred metadata asset (i.e., moment node).  For one embodiment, each group of blocks\n351A-N, 353A-N, 355A-N, 357A-N, and 359A-N may be performed in parallel (as opposed to sequentially).  For example, the group of blocks 351A, 353A, 355A, 357A, and 359A may be performed in parallel with the group of 351N, 353N, 355N, 357N, and 359N. \nFurthermore, performing the groups of blocks in parallel does not mean that each group (e.g., the group of 351A, 353A, 355A, 357A, and 359A, etc.) begins and/or ends at the same time as another group (e.g., the group of 351B, 353B, 355B, 357B, and 359B,\netc.).  In addition, the time taken to complete each group (e.g., the group of 351A, 353A, 355A, 357A, and 359A, etc.) can be different from the time taken to complete another group (e.g., the group of 351B, 353B, 355B, 357B, and 359B, etc.).  For\nbrevity, only the group of 351A, 353A, 355A, 357A, and 359A will be discussed below in connection with FIGS. 3B-3C.\n Referring again to FIG. 3B, operation 350 proceeds to blocks 351A.  At this block, a DAM module/logic performing operation 350 identifies one or more first primary primitive metadata assets.  For one embodiment, the first primary primitive\nmetadata asset(s) may be selected from the metadata collection that is obtained/received in block 349.  Primary primitive metadata is described above in connection with FIGS. 1A-2.\n Next, operation 350 proceeds to block 353A in FIG. 3B.  Here, a DAM module/logic performing operation 350 determines a first primary inferred metadata asset (i.e., the first event metadata asset) associated with one or more first DAs based on\nthe first primary primitive metadata asset(s) associated with the one or more first DAs.  Primary inferred metadata is described above in connection with FIGS. 1A-2.  Operation 350 proceeds to block 355A in FIG. 3B, where a first moment node is generated\nbased on the first primary inferred metadata asset (e.g., the first event metadata asset, etc.).\n Referring now to FIG. 3C, process 350 proceeds to block 357A.  Here, one or more first auxiliary primitive metadata assets are determined or inferred from the metadata collection associated with the DA collection.  For one embodiment, block 357A\nis performed in accordance with one or more of FIGS. 1-3B, which are described above.\n At block 359A, one or more first auxiliary inferred metadata assets may be determined or inferred based on the first auxiliary primitive metadata asset(s), the first primary primitive metadata asset(s), and/or the first primary inferred metadata\nasset.  Next, operation 350 proceeds to block 361.  Here, a DAM module/logic performing operation 350 may generate a node for each primary primitive metadata asset, each auxiliary primitive metadata asset, and each auxiliary inferred metadata asset. \nThat is, for each Nth group, a node may be generated for each primary primitive metadata asset, each auxiliary primitive metadata asset, and each auxiliary inferred metadata asset.  Also, at block 363 of FIG. 3C, an edge representing a correlation\nbetween two metadata assets (i.e., two nodes) may be determined and generated.  For one embodiment, the edge is determined and generated as described in connection with at least FIG. 1B and FIG. 3D.  For one embodiment, operation 350 is performed\niteratively and ends at block 365 after no additional nodes can be generated and no additional edges can be generated.\n FIG. 3D illustrates, in flowchart form, an operation 390 to generate one or more edges between nodes in a metadata network for DAM in accordance with an embodiment.  FIG. 3D provides additional details about the block 363 of operation 350\ndescribed above in connection with FIGS. 3B-3C.  Operation 390 can be performed by a DAM logic/module (e.g., the module/logic 140 described above in connection with FIGS. 1A-1B).  For one embodiment, operation 390 begins at block 391 and proceeds to\nblocks 393A-N, where N refers to the number of one or more DAs in the DA collection having their own distinct primary inferred metadata asset (i.e., moment node).  For brevity, only block 393A is described below in connection with FIG. 3C.  Block 393A\nrequires determining confidence weights and relevance weights for each of the first primitive metadata assets (i.e., the primary primitive metadata asset(s) and the auxiliary primitive metadata asset(s), etc.) and each of the first inferred metadata\nassets (i.e., the primary inferred metadata asset and the auxiliary inferred metadata asset(s), etc.).  Confidence weights and relevance weights are described above in connection with one or more of FIGS. 1A-3B.\n At block 395 of FIG. 3D, a DAM logic/module performing operation 390 may determine, for each pair of nodes, whether a correlation exists between the two nodes.  For one embodiment, this determination includes determining that a set of two nodes\nis correlated when at least one of the following occurs: (i) the confidence weight between the two nodes exceeds a threshold confidence; (ii) the relevance weight between the at least two nodes exceeds a threshold relevance; or (iii) a combination of the\nconfidence weight and the relevance weight exceeds a threshold correlation.  Combinations of the confidence and relevance weights include, but are not limited to, a sum of the two weights, a product of the two weights, an average of the two weights, a\nmedian of the two weights, and a difference between the two weights.  Next, operation 390 proceeds to block 397, where a DAM logic/module performing operation 390 generates an edge between the correlated nodes in the multidimensional network representing\nthe KB.  For one embodiment, operation 390 is performed iteratively and ends at block 399 when no more additional edges can be generated between two nodes.\n One or more of operations 300, 350, and 390 described above in connection with FIGS. 3A-3D, respectively can be used to update the metadata network 175 described above in connection with FIGS. 1A-2.  For example, a DAM module/logic 140 updates\nthe metadata network 175 using one or more of operations 300, 350, and 390 as the DAM module/logic 140 obtains or receives new primitive metadata 170 and/or as the DAM module/logic 140 determines or infers new inferred metadata 170 based on the new\nprimitive metadata 170.\n Referring now to FIG. 4, which is a flowchart representing one embodiment of an operation 400 to relate and/or present at least two digital assets (DAs) from a collection of DAs (DA Collection) in accord with one embodiment.  Operation 400 can\nbe performed by a DAM logic/module (e.g., the module/logic 140 described above in connection with FIGS. 1A-1B).  Operation 400 begins at block 401, where a metadata network is obtained or received as described above in connection with FIGS. 1A-3C.\n Operation 400 proceeds to block 403, where a DAM logic/module performing operation 400 may select a first metadata asset that is represented as a node in the metadata network.  The first metadata asset may be a non-moment node or a moment node. \nFor one embodiment, the first metadata asset (i.e., the selected node) can represent a primary primitive metadata asset, a primary inferred metadata asset, an auxiliary primitive metadata asset, or an auxiliary inferred metadata asset associated one or\nmore DAs in a DA collection.  For example, when a user is consuming or perceiving a DA (e.g., a single DA, a group of DAs, etc.) via an output device (e.g., a display device, an audio output device, etc.), then a user-input indicating a selection of the\nDA can trigger a selection of a specific metadata asset associated with the DA in the metadata network.  Alternatively, or additionally, a user interface may be provided to the user to enable the user to select a specific metadata asset associated with\none or more DAs from a group of metadata assets associated with the one or more DAs.  Exemplary user interfaces include, but are not limited to, graphical user interfaces, voice user interfaces, object-oriented user interfaces, intelligent user\ninterfaces, hardware interfaces, touch user interfaces, touchscreen devices or systems, gesture interfaces, motion tracking interfaces, and tangible user interfaces.  The user interface may be presented to the user in response to the user selecting the\nspecific DA.  One or more specific examples of a user interface can be found in U.S.  Provisional Patent Application No. 62/349,109, entitled \"USER INTERFACES FOR RETRIEVING CONTEXTUALLY RELEVANT MEDIA CONTENT,\" filed Jun.  12, 2016, which is\nincorporated by reference in its entirety.\n For one embodiment, operation 400 includes block 405.  At this block, a determination may be made that the first metadata asset (i.e., the selected node) is associated with a second metadata asset that is represented as a second node in the\nmetadata network.  The second node can be a moment node or a non-moment node.  For example, the second metadata asset can be a first moment node.  For this example, the determination may include determining that at least one of the primary primitive\nmetadata asset(s), the auxiliary primitive metadata asset(s), or the auxiliary inferred metadata asset(s) represented by the selected node (i.e., the first metadata asset) corresponds to the second metadata asset (i.e., the first moment node).\n At block 407, a third metadata asset can be identified based on the first metadata asset (i.e., the selected node) and/or the second metadata asset (i.e., the second node).  The third metadata asset can be represented as a third node in the\nmetadata network.  The third node may be a moment node or a non-moment node.  For example, the third metadata asset can be represented as a second moment node that is different from the first moment node in the immediately preceding example (i.e., the\nsecond metadata asset).  At block 409, at least one DA associated with the third metadata asset (e.g., the second moment node in the metadata network, etc.) may be presented via an output device.  In this way, operation 400 can assist with relating and\npresenting one or more DAs in a DA collection based on their metadata.\n FIG. 5 is a flowchart representing an operation 500 to determine and present at least two digital assets (DAs) from a DA collection based on a predetermined criterion in accordance with one embodiment.  A DAM logic/module can perform operation\n500 (e.g., the module/logic 140 described above in connection with FIGS. 1A-1B, etc.).  For one embodiment, a DAM logic/module performs operation 500 to determine and/or present one or more DAs based on a predetermined criterion and one or more notable\nmoments (i.e., one or more event metadata assets).  For example, if the predetermined criterion requires a date from one or more previous years that share the same day as today, then a DAM logic/module performs operation 500 to determine and/or present\none or more DAs associated with one or more notable moments (i.e., one or more event metadata assets) that share the same day as today.  For one embodiment, the predetermined criterion includes contextual information.\n Operation 500 begins at block 501, where a DAM logic/module performing operation 500 obtains or receives a metadata network.  One or more embodiments of metadata networks are described above in connection with FIGS. 1A-4.  At block 503, a\npredetermined criterion is received.  For one embodiment, the predetermined criterion may be based on contextual information.  Context and contextual information are described above.  Process 500 proceeds to block 505, where a DAM logic/module performing\noperation 500 may determine that one or more metadata assets that are represented as nodes in the metadata network satisfy the predetermined criterion.  The nodes that satisfy the predetermined criterion can be moment nodes or non-moment nodes.  For one\nembodiment, the identified nodes match the criterion.  For example, the predetermined criterion can include a geolocation that will be visited by a user during a future time period.  Thus, for this example, one or more nodes that include the geolocation\nspecified by the predetermined criterion can be identified in the metadata network.\n For one embodiment, the predetermined criterion can be based on one or more metadata assets that represent a break in a user's habits.  For this embodiment, the predetermined criterion can be determined by identifying one or more metadata assets\nhaving a low rate of occurrence based on an analysis of metadata assets of that metadata type.  For example, a count and/or comparison of all time metadata assets in a metadata collection reveals that the lowest number of time metadata assets are those\nhaving times between 12:00 AM and 5:00 AM every day.  Consequently, and for this example, the times between 12:00 AM and 5:00 AM every day can be specified as the predetermined criterion.  Using the predetermined criterion described above to identify a\nbreak in a user's habits can identify metadata assets associated with one or more interesting DAs (e.g., one or more images that represent a break in a user's daily routine, etc.).  Exemplary predetermined criterion representing a break in a user's\nhabits include, but are not limited to, visiting a geolocation that has never been visited before (e.g., a first day in Hawaii, etc.), visiting a geolocation that has not been visited in an extended time (e.g., a trip to your birthplace after being away\nfor more than a month, a year, 6 months, etc.), and an outing with one or more identified persons that have not been interacted with for an extended time (e.g., a dinner with childhood friends you haven't seen in over a month, a year, 6 months, etc.).\n Operation 500 proceeds to block 507.  At this block, a determination may be made that the identified metadata data asset(s), which are represented as node(s) in the metadata network, are associated with one or more other metadata data asset(s). \nThese other metadata asset(s) could be moment nodes or non-moment nodes that are represented in the metadata network.  For one embodiment, the identified node(s) in block 505 can be used to determine one or more moment nodes in block 507.  For example,\none of the identified node(s) in block 505 can represent a metadata asset that describes a geolocation to be attended by the user.  Thus, for this example, one or more moments nodes that represent event metadata asset(s) associated with the geolocation\nspecified by a predetermined criterion can be determined in the metadata network at block 507.  The determined metadata asset(s) in block 507 can be used to identify one or more DAs in the DA collection.  At block 509, the identified DA(s) associated\nwith the determined metadata asset(s) in block 507 can be presented via an output device (e.g., a display device, an audio output device, etc.) for consumption by a user of the device.\n FIG. 6 is a flowchart representing an operation 600 to determine and present a representative set of digital assets (DAs) for a moment according to one embodiment.  For one embodiment, operation 600 is performed on metadata assets associated\nwith a group of DAs that share the same event metadata.  Thus, for this embodiment, the metadata networks described above are not always required.  Other embodiments, however, perform operation 600 on one or more moment nodes in a metadata network.  For\nbrevity, operation 600 will be described in connection with a moment (i.e., an event metadata asset) in a metadata network.\n Operation 600 can be performed by a DAM logic/module to curate one or more representative DAs associated with an event metadata asset that is represented as a moment node in a metadata network.  As used herein, \"curation\" and its variations\nrefer to determining and/or presenting a representative set of DAs for summarizing the one or more DAs associated with a moment.  For example, if there are fifty images associated with a moment, then a curation of the moment can include determining\nand/or presenting ten images summarizing the fifty DAs associated with the moment.\n Operation 600 begins at block 605, where a DAM logic/module performing operation 600 obtains or receives a maximum number of DAs to be used for representing the DAs associated with a moment (i.e., an event metadata asset) that is represented as\na moment node in a metadata network and a minimum number of DAs to be used for representing the DAs associated with the moment (i.e., the event metadata asset) that is represented as the moment node in the metadata network.  For one embodiment, the\nmaximum and minimum numbers can be received via user input provided through an input device (e.g., peripheral(s) 190 described above in connection with FIG. 1A, input device(s) 706 described below in connection with FIG. 7, etc.).  For another\nembodiment, the maximum and minimum numbers can be predetermined numbers that are applied automatically by the DAM logic/module performing operation 600.  These predetermined numbers can be set when developing the DAM logic/module that performs operation\n600 or through an input provided via a user interface (e.g., through a user preferences setting, etc.).  For one embodiment, the maximum and minimum numbers can be determined dynamically based on processing operations performed by computational resources\nassociated with the DAM logic/module.  For example, as more computational resources become available, the maximum and minimum numbers can be increased or decreased.\n At block 607, one or more other metadata assets associated with the selected moment may be identified and further classified into multiple sub-clusters.  The one or more other metadata assets may include primary primitive metadata assets,\nauxiliary primitive metadata assets, and/or auxiliary inferred metadata assets that correspond to the moment (i.e., the event metadata asset) that is represented as the moment node in the metadata network.  For one embodiment, the one or more other\nmetadata assets are identified using their corresponding nodes in the metadata network.  For one embodiment, block 607 also includes determining a time period spanned by the other metadata assets associated with the selected moment and determining\nwhether this time period is greater than or equal to a predetermined threshold.  This predetermined threshold is used to differentiate collections of metadata assets that represent a short moment (e.g., a birthday party spanning three hours, etc.) from\ncollections of metadata assets that represent a longer moment (e.g., a vacation trip spanning a week, etc.).  Curation settings can be used to select representative DAs for collections of metadata assets that represent longer moments.  When the time\nperiod spanned by the other metadata assets associated with the selected moment is greater than or equal to a predetermined threshold, the other metadata assets associated with the selected moment may be considered a dense cluster.  Alternatively, when a\ntime period spanned by the other metadata assets associated with the selected moment fails to exceed the predetermined threshold, the other metadata assets associated with the selected moment may be considered a diffused or sparse cluster.  For one\nembodiment, when a dense cluster is determined, operation 500 (as described above) may be used to select and present the DAs associated with selected moment via an output device.  In contrast, when a diffused or sparse cluster is determined, the other\nmetadata assets associated with the selected moment may be ordered sequentially.  For one embodiment, sequentially ordering the other metadata assets may be based on at least one a capture time, a modification time, or a save time.  After the other\nmetadata assets associated with the selected moment are ordered, block 607 includes applying a clustering technique based on time and spatial distances between the selected moment's metadata assets (i.e., the other metadata assets).  Examples of such\nclustering techniques include, but are not limited to, exclusive clustering algorithms, overlapping clustering algorithms, hierarchical clustering, and probabilistic clustering algorithms.  For one embodiment, time may be the base vector used for the\nclustering technique and the spatial distances between the selected moment's metadata assets may be a function of the time.\n For one embodiment, block 607 may include iteratively applying a first density-based data clustering algorithm to the results of the clustering technique described above.  For one embodiment, the first density-based data clustering algorithm\nincludes the \"density-based spatial clustering of applications with noise\" or DBSCAN algorithm.  For one embodiment, the DBSCAN algorithm may be applied to determine or infer sub-clusters of the selected moment's metadata assets while avoiding outlier\nmetadata assets.  Such outliers typically lie in low density regions.  For one embodiment, block 607 may also include applying a second density-based data-clustering algorithm to the results of the first density-based data-clustering algorithm.  For one\nembodiment, the second density-based data-clustering algorithm can include the \"ordering points to identify the clustering structure\" or OPTICS algorithm.  For one embodiment, the OPTICS algorithm may be applied to results of the DBSCAN algorithm to\ndetect meaningful sub-clusters of the other metadata assets associated with the selected moment.  The OPTICS algorithm linearly orders the other metadata assets associated with the selected moment such that metadata assets that are spatially closest to\neach other become neighbors.  Additionally, a special distance may be stored for each sub-cluster of the other metadata assets.  This special distance can represent the maximum spatial distance between two metadata assets that needs to be accepted for a\nsub-cluster in order to have two or more metadata assets be deemed as belonging to that sub-cluster.  That is, any two metadata assets whose spatial distance exceeds the special distance are not considered part of the same sub-cluster.  For one\nembodiment, block 607 also includes applying a weight to each metadata asset in each sub-cluster that results from applying the OPTICS algorithm.  For example, the weight can be a score between 0.0 and 1.0, where each metadata asset in each sub-cluster\nhas a starting score of 0.5.  Block 607 may further include applying at least one heuristic function to determine a representative weight for each determined sub-cluster based on the individual weights within each sub-cluster.\n Operation 600 proceeds to block 609, where metadata assets are selected from the identified sub-cluster(s).  The selected metadata assets correspond to or identify the representative DAs.  For one embodiment, block 609 includes applying an\nadaptive election algorithm to select or filter a sub-set of the sub-clusters determined in block 607.  The number of sub-clusters in the sub-set may be equal to the maximum number described above in connection with block 605.  Block 609 can also include\ndetermining a percentage of representative DAs that can be contributed by each sub-cluster in the sub-set to the maximum number described above in connection with block 605.  For example, if there are two sub-clusters in the sub-set and the first\nsub-cluster has metadata assets associated with 20 DAs while the second sub-cluster has metadata assets associated with 10 DAs, then the first sub-cluster can contribute 75% of its DAs to the maximum number of representative DAs and the second\nsub-cluster can contribute 25% of its DAs to the maximum number of representative DAs.  For one embodiment, when the number of representative DAs a sub-cluster can contribute to the representative DAs is less than the minimum number described above in\nconnection with block 605, that sub-cluster may be removed from consideration.  Thus, and with regard to the immediately preceding example, if 25% of the DAs that can be contributed by the second sub-cluster is less than the minimum number described\nabove in connection with block 605, then the second sub-cluster may be removed from consideration.  For one embodiment, determining the maximum number that each sub-cluster in the sub-set can contribute to the number of representative DAs may be\nperformed iteratively until each sub-cluster can contribute at least the minimum number described above in connection with block 605.\n At block 609, hierarchical cluster analysis (e.g., agglomerative clustering, divisive clustering, etc.) can be performed on the sub-clusters that can contribute a number of their DAs to the representative DAs.  Exemplary agglomerative clustering\ntechniques include, but are not limited to, hierarchical agglomerative clustering (HAC) techniques.  Exemplary divisive clustering techniques include, but are not limited to, k-mean clustering techniques (where k is equal to the number of DAs associated\nwith a sub-cluster that can be contributed to the total number of representative DAs and where k is at least equal to the minimum number described above in connection with block 605).  For one embodiment, the selected metadata assets associated with DAs\nin a sub-cluster that can be contributed to the total number of representative DAs are then filtered for redundancies and noise.  Here, noisy metadata assets may be assets that have incomplete information or are otherwise not associated with the selected\nmoment.  After the redundant and noisy metadata assets are removed, the DAs associated with the unremoved metadata assets may be deemed the total number of representative DAs.  For one embodiment, this total number of the one or more representative DAs\nis (i) less than or equal to the maximum number from block 605 and (ii) greater than or equal to the minimum number from block 605.  As shown in block 611, the DAs associated with the unremoved metadata assets can be presented on an output device as the\nrepresentative DAs.\n FIG. 7 is a block diagram illustrating an exemplary data processing system 700 that may be used with one or more of the described embodiments.  For example, the system 700 may represent any data processing system (e.g., one or more of the\nsystems described above performing any of the operations or methods described above in connection with FIGS. 1A-6, etc.).  System 700 can include many different components.  These components can be implemented as integrated circuits (ICs), portions\nthereof, discrete electronic devices, or other modules adapted to a circuit board such as a motherboard or add-in card of a computer system, or as components otherwise incorporated within a chassis of a computer system.  Note also that system 700 is\nintended to show a high-level view of many, but not all, components of the computer system.  Nevertheless, it is to be understood that additional components may be present in certain implementations and furthermore, different arrangements of the\ncomponents shown may occur in other implementations.  System 700 may represent a desktop computer system, a laptop computer system, a tablet computer system, a server computer system, a mobile phone, a media player, a personal digital assistant (PDA), a\npersonal communicator, a gaming device, a network router or hub, a wireless access point (AP) or repeater, a set-top box, or a combination thereof.  Further, while only a single machine or system is illustrated, the term \"machine\" or \"system\" shall also\nbe taken to include any collection of machines or systems that individually or jointly execute instructions to perform any of the methodologies discussed herein.\n For one embodiment, system 700 includes processor(s) 701, memory 703, devices 705-709, and device 711 via a bus or an interconnect 710.  System 700 also includes a network 712.  Processor(s) 701 may represent a single processor or multiple\nprocessors with a single processor core or multiple processor cores included therein.  Processor(s) 701 may represent one or more general-purpose processors such as a microprocessor, a central processing unit (CPU), graphics processing unit (GPU), or the\nlike.  More particularly, processor(s) 701 may be a complex instruction set computer (CISC), a reduced instruction set computer (RISC) or a very long instruction word (VLIW) computer architecture processor, or processors implementing a combination of\ninstruction sets.  Processor(s) 701 may also be one or more special-purpose processors such as an application specific integrated circuit (ASIC), an application-specific instruction set processor (ASIP), a cellular or baseband processor, a field\nprogrammable gate array (FPGA), a digital signal processor (DSP), a physics processing unit (PPU), an image processor, an audio processor, a network processor, a graphics processor, a graphics processing unit (GPU), a network processor, a communications\nprocessor, a cryptographic processor, a co-processor, an embedded processor, a floating-point unit (FPU), or any logic that can process instructions.\n Processor(s) 701, which may be a low power multi-core processor socket such as an ultra-low voltage processor, may act as a main processing unit and central hub for communication with the various components of the system.  Such processor(s) can\nbe implemented as one or more system-on-chip (SoC) integrated circuits (ICs).  A digital asset management (DAM) logic/module 728A may reside, completely or at least partially, within processor(s) 701.  In one embodiment, the DAM logic/module 728A enables\nthe processor(s) 701 to perform any or all of the operations or methods described above in connection with FIGS. 1A-6.  Additionally or alternatively, the processor(s) 701 may be configured to execute instructions for performing the operations and\nmethodologies discussed herein.\n System 700 may further include a graphics interface that communicates with optional graphics subsystem 704, which may include a display controller, a graphics processing unit (GPU), and/or a display device.  Processor(s) 701 may communicate with\nmemory 703, which in one embodiment can be implemented via multiple memory devices to provide for a given amount of system memory.  Memory 703 may include one or more volatile storage (or memory) devices such as random access memory (RAM), dynamic RAM\n(DRAM), synchronous DRAM (SDRAM), static RAM (SRAM), or other types of storage devices.  Memory 703 may store information including sequences of instructions that are executed by processor(s) 701 or any other device.  For example, executable code and/or\ndata from a variety of operating systems, device drivers, firmware (e.g., input output basic system or BIOS), and/or applications can be loaded in memory 703 and executed by processor(s) 701.  An operating system can be any kind of operating system.  A\nDAM logic/module 728D may also reside, completely or at least partially, within memory 703.\n For one embodiment, the memory 703 includes a DAM logic/module 728B as executable instructions.  For another embodiment, when the instructions represented by DAM logic/module 728B are executed by the processor(s) 701, the instructions cause the\nprocessor(s) 701 to perform any, all, or some of the operations or methods described above in connection with FIGS. 1A-6.\n System 700 may further include I/O devices such as devices 705-708, including network interface device(s) 705, optional input device(s) 706, and other optional I/O device(s) 707.  Network interface device 705 may include a wired or wireless\ntransceiver and/or a network interface card (NIC).  The wireless transceiver may be a WiFi transceiver, an infrared transceiver, a Bluetooth transceiver, a WiMax transceiver, a wireless cellular telephony transceiver, a satellite transceiver (e.g., a\nglobal positioning system (GPS) transceiver), or other radio frequency (RF) transceivers, or a combination thereof.  The NIC may be an Ethernet card.\n Input device(s) 706 may include a mouse, a touch pad, a touch sensitive screen (which may be integrated with display device 704), a pointer device such as a stylus, and/or a keyboard (e.g., a physical keyboard or a virtual keyboard displayed as\npart of a touch sensitive screen).  For example, input device 706 may include a touch screen controller coupled to a touch screen.  The touch screen and touch screen controller can, for example, detect contact and movement or a break thereof using one or\nmore touch sensitivity technologies, including but not limited to capacitive, resistive, infrared, and surface acoustic wave technologies, as well as other proximity sensor arrays or other elements for determining one or more points of contact with the\ntouch screen.\n I/O devices 707 may include an audio device.  An audio device may include a speaker and/or a microphone to facilitate voice-enabled functions, such as voice recognition, voice replication, digital recording, and/or telephony functions.  Other\nI/O devices 707 may include universal serial bus (USB) port(s), parallel port(s), serial port(s), a printer, a network interface, a bus bridge (e.g., a PCI-PCI bridge), sensor(s) (e.g., a motion sensor such as an accelerometer, gyroscope, a magnetometer,\na light sensor, compass, a proximity sensor, etc.), or a combination thereof.  Device(s) 707 may further include an imaging processing subsystem (e.g., a camera), which may include an optical sensor, such as a charged coupled device (CCD) or a\ncomplementary metal-oxide semiconductor (CMOS) optical sensor, utilized to facilitate camera functions, such as recording photographs and video clips.  Certain sensors may be coupled to interconnect 710 via a sensor hub (not shown), while other devices\nsuch as a keyboard or thermal sensor may be controlled by an embedded controller (not shown), dependent upon the specific configuration or design of system 700.\n To provide for persistent storage for information such as data, applications, one or more operating systems and so forth, a mass storage device or devices (not shown) may also coupled to processor(s) 701.  For various embodiments, to enable a\nthinner and lighter system design as well as to improve system responsiveness, this mass storage may be implemented via a solid state device (SSD).  However in other embodiments, the mass storage may primarily be implemented using a hard disk drive (HDD)\nwith a smaller amount of SSD storage to act as a SSD cache to enable non-volatile storage of context state and other such information during power down events so that a fast power up can occur on re-initiation of system activities.  In addition, a flash\ndevice may be coupled to processor(s) 701, e.g., via a serial optional peripheral interface (SPI).  This flash device may provide for non-volatile storage of system software, including a basic input/output software (BIOS) and other firmware.\n A DAM logic/module 728C may be part of a specialized stand-alone computing system/device 711 that is formed from hardware, software, or a combination thereof.  For one embodiment, the DAM logic/module 728C performs any, all, or some of the\noperations or methods described above in connection with FIGS. 1A-6.\n Storage device 708 may include computer-accessible storage medium 709 (also known as a machine-readable storage medium or a computer-readable medium) on which is stored one or more sets of instructions or software--e.g., a DAM logic/module 728D.\n For one embodiment, the instruction(s) or software stored on storage medium 709 embody one or more methodologies or functions described above in connection with FIGS. 1A-6.  For another embodiment, the storage device 708 includes a DAM\nlogic/module 728D as executable instructions.  When the instructions represented by a DAM logic/module 728D are executed by the processor(s) 701, the instructions cause the system 700 to perform any, all, or some of the operations or methods described\nabove in connection with FIGS. 1A-6.\n Computer-readable storage medium 709 can store some or all of the software functionalities of a DAM logic/module 728A-D described above persistently.  While computer-readable storage medium 709 is shown in an exemplary embodiment to be a single\nmedium, the term \"computer-readable storage medium\" should be taken to include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that store the one or more sets of instructions.  The\nterms \"computer-readable storage medium\" shall also be taken to include any medium that is capable of storing or encoding a set of instructions for execution by the system 700 and that cause the system 700 to perform any one or more of the disclosed\nmethodologies.  The term \"computer-readable storage medium\" shall accordingly be taken to include, but not be limited to, solid-state memories, and optical and magnetic media, or any other non-transitory machine-readable medium.\n Note that while system 700 is illustrated with various components of a data processing system, it is not intended to represent any particular architecture or manner of interconnecting the components; as such, details are not germane to the\nembodiments described herein.  It will also be appreciated that network computers, handheld computers, mobile phones, servers, and/or other data processing systems, which have fewer components or perhaps more components, may also be used with the\nembodiments described herein.\n In the foregoing description, numerous specific details are set forth, such as specific configurations, dimensions and processes, etc., in order to provide a thorough understanding of the embodiments.  In other instances, well-known processes\nand manufacturing techniques have not been described in particular detail in order to not unnecessarily obscure the embodiments.  Reference throughout this specification to \"one embodiment,\" \"an embodiment,\" \"another embodiment,\" \"other embodiments,\"\n\"some embodiments,\" and their variations means that a particular feature, structure, configuration, or characteristic described in connection with the embodiment is included in at least one embodiment.  Thus, the appearances of the phrase \"for one\nembodiment,\" \"for an embodiment,\" \"for another embodiment,\" \"in other embodiments,\" \"in some embodiments,\" or their variations in various places throughout this specification are not necessarily referring to the same embodiment.  Furthermore, the\nparticular features, structures, configurations, or characteristics may be combined in any suitable manner in one or more embodiments.\n In the following description and claims, the terms \"coupled\" and \"connected,\" along with their derivatives, may be used.  It should be understood that these terms are not intended as synonyms for each other.  \"Coupled\" is used to indicate that\ntwo or more elements or components, which may or may not be in direct physical or electrical contact with each other, co-operate or interact with each other.  \"Connected\" is used to indicate the establishment of communication between two or more elements\nor components that are coupled with each other.\n Some portions of the preceding detailed description have been presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory.  These algorithmic descriptions and representations are the ways\nused by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art.  An algorithm is here, and generally, conceived to be a self-consistent sequence of operations leading to a desired\nresult.  The operations are those requiring physical manipulations of physical quantities.  It should be borne in mind, however, that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient\nlabels applied to these quantities.  Unless specifically stated otherwise as apparent from the above discussion, it is appreciated that throughout the description, discussions utilizing terms such as those set forth in the claims below, refer to the\naction and processes of a computer system, or similar electronic computing system, that manipulates and transforms data represented as physical (electronic) quantities within the computer system's registers and memories into other data similarly\nrepresented as physical quantities within the computer system memories or registers or other such information storage, transmission or display devices.\n Embodiments described herein can relate to an apparatus for performing a computer program (e.g., the operations described herein, etc.).  Such a computer program is stored in a non-transitory computer readable medium.  A machine-readable medium\nincludes any mechanism for storing information in a form readable by a machine (e.g., a computer).  For example, a machine-readable (e.g., computer-readable) medium includes a machine (e.g., a computer) readable storage medium (e.g., read only memory\n(\"ROM\"), random access memory (\"RAM\"), magnetic disk storage media, optical storage media, flash memory devices).\n Although operations or methods are described above in terms of some sequential operations, it should be appreciated that some of the operations described may be performed in a different order.  Moreover, some operations may be performed in\nparallel rather than sequentially.  Embodiments described herein are not described with reference to any particular programming language.  It will be appreciated that a variety of programming languages may be used to implement the various embodiments of\nthe disclosed subject matter.  In utilizing the various aspects of the embodiments described herein, it would become apparent to one skilled in the art that combinations, modifications, or variations of the above embodiments are possible for managing\ncomponents of a processing system to increase the power and performance of at least one of those components.  Thus, it will be evident that various modifications may be made thereto without departing from the broader spirit and scope of at least one of\nthe disclosed concepts set forth in the following claims.  The specification and drawings are, accordingly, to be regarded in an illustrative sense rather than a restrictive sense.\n In the development of any actual implementation of one or more of the disclosed concepts (e.g., such as a software and/or hardware development project, etc.), numerous decisions must be made to achieve the developers' specific goals (e.g.,\ncompliance with system-related constraints and/or business-related constraints).  These goals may vary from one implementation to another, and this variation could affect the actual implementation of one or more of the disclosed concepts set forth in the\nembodiments described herein.  Such development efforts might be complex and time-consuming, but may still be a routine undertaking for a person having ordinary skill in the art in the design and/or implementation of one or more of the inventive concepts\nset forth in the embodiments described herein.\n One aspect of the present technology is the gathering and use of data available from various sources to improve the operation of the metadata network.  The present disclosure contemplates that in some instances, this gathered data may include\npersonal information data that uniquely identifies a specific person.  Such personal information data can include demographic data, location-based data, telephone numbers, email addresses, twitter ID's, home addresses, or any other identifying\ninformation.\n The present disclosure recognizes that the use of such personal information data, in the present technology, can be used to the benefit of users.  For example, the personal information data can be used to improve the metadata assets and enable\nidentifying correlation between metadata nodes.  Further, other uses for personal information data that benefit the user are also contemplated by the present disclosure.\n The present disclosure further contemplates that the entities responsible for the collection, analysis, disclosure, transfer, storage, or other use of such personal information data will comply with well-established privacy policies and/or\nprivacy practices.  In particular, such entities should implement and consistently use privacy policies and practices that are generally recognized as meeting or exceeding industry or governmental requirements for maintaining personal information data\nprivate and secure.  For example, personal information from users should be collected for legitimate and reasonable uses of the entity and not shared or sold outside of those legitimate uses.  Further, such collection should occur only after receiving\nthe informed consent of the users.  Additionally, such entities would take any needed steps for safeguarding and securing access to such personal information data and ensuring that others with access to the personal information data adhere to their\nprivacy policies and procedures.  Further, such entities can subject themselves to evaluation by third parties to certify their adherence to widely accepted privacy policies and practices.\n Despite the foregoing, the present disclosure also contemplates embodiments in which users selectively block the use of, or access to, personal information data.  That is, the present disclosure contemplates that hardware and/or software\nelements can be provided to prevent or block access to such personal information data.  For example, in the case of the present metadata network, the present technology can be configured to allow users to select to \"opt in\" or \"opt out\" of participation\nin the collection of personal information data for use as metadata assets in the metadata network.\n Therefore, although the present disclosure broadly covers use of personal information data to implement one or more various disclosed embodiments, the present disclosure also contemplates that the various embodiments can also be implemented\nwithout the need for accessing such personal information data.  That is, the various embodiments of the present technology are not rendered inoperable due to the lack of all or a portion of such personal information data.\n As used in the description above and the claims below, the phrase \"at least one of A, B, or C\" includes A alone, B alone, C alone, a combination of A and B, a combination of B and C, a combination of A and C, and a combination of A, B, and C.\nThat is, the phrase \"at least one of A, B, or C\" means A, B, C, or any combination thereof such that one or more of a group of elements consisting of A, B and C, and should not be interpreted as requiring at least one of each of the listed elements A, B\nand C, regardless of whether A, B and C are related as categories or otherwise.  Furthermore, the use of the article \"a\" or \"the\" in introducing an element should not be interpreted as being exclusive of a plurality of elements.  Also, the recitation of\n\"A, B and/or C\" is equal to \"at least one of A, B or C.\"\n Also, the use of \"a\" refers to \"one or more\" in the present disclosure.  For example, \"a DA\" refers to \"one or more DAs.\"", "application_number": "15391276", "abstract": " Techniques of generating a knowledge graph metadata network (metadata\n     network) for digital asset management (DAM) are described. A DAM\n     logic/module can obtain one or more first metadata assets describing\n     characteristics associated with digital assets (DAs) in the DA\n     collection. The DAM logic/module can also determine second metadata\n     asset(s) and third metadata asset(s) describing characteristics\n     associated with DAs in the DA collection based on the first metadata\n     asset(s). The DAM logic/module can generate at least some of the metadata\n     assets as nodes in a metadata network associated with the DA collection.\n     The DAM logic/module can also determine, for at least two of the metadata\n     assets, a correlation between the at least two metadata assets. The DAM\n     logic/module can generate an edge in the metadata network between the\n     nodes that represent the at least two metadata assets to represent the\n     determined correlation.\n", "citations": ["5416895", "5565888", "5604861", "5677708", "5757368", "5784061", "5825349", "5956035", "5973694", "6073036", "6237010", "6252596", "6279018", "6301586", "6334025", "6351556", "6441824", "6452597", "6477117", "6606411", "6686938", "6741268", "6784925", "6915011", "6920619", "7015910", "7139982", "7164410", "7178111", "7325198", "7421449", "7434177", "7587671", "7627828", "7636733", "7680340", "7716194", "7747625", "7788592", "7823080", "7831100", "7843454", "7865215", "7991234", "8024658", "8028249", "8106856", "43260", "8200669", "8305355", "8339420", "8352471", "8406473", "8571331", "8698762", "9042646", "9123086", "9143601", "9411506", "20020021758", "20020054233", "20020093531", "20020168108", "20030033296", "20030048291", "20030090504", "20030122787", "20040046886", "20040119758", "20040125150", "20040143590", "20040167898", "20040205504", "20040207722", "20040212617", "20050020317", "20050041035", "20050044066", "20050052427", "20050062130", "20050071736", "20050071767", "20050073601", "20050076056", "20050102635", "20050104848", "20050128305", "20050134945", "20050160377", "20050183026", "20050195221", "20050275636", "20060001652", "20060017692", "20060025218", "20060026521", "20060026536", "20060036960", "20060072028", "20060077266", "20060080386", "20060088228", "20060090141", "20060136839", "20060155757", "20060156237", "20060156245", "20060156246", "20060265643", "20070008321", "20070016868", "20070081740", "20070115373", "20070136778", "20070152984", "20070204225", "20070229678", "20070245236", "20080030456", "20080057941", "20080059888", "20080062141", "20080091637", "20080133697", "20080152201", "20080168349", "20080168402", "20080256577", "20080282202", "20080309632", "20090006965", "20090021576", "20090063542", "20090113350", "20090132921", "20090161962", "20090210793", "20090216806", "20090278806", "20090282371", "20090287470", "20090300146", "20090307623", "20090319472", "20090325701", "20100045828", "20100046842", "20100076976", "20100083173", "20100103321", "20100110228", "20100114891", "20100125786", "20100150456", "20100207892", "20100287053", "20100302179", "20110025719", "20110035700", "20110050564", "20110050640", "20110099199", "20110099478", "20110126148", "20110145275", "20110145327", "20110191661", "20110246463", "20110267368", "20110282867", "20110320938", "20120110438", "20120243735", "20130022282", "20130040660", "20130061175", "20130156275", "20130198176", "20140046914", "20140055495", "20140064572", "20140082533", "20140089330", "20140143693", "20140157321", "20140181089", "20140189584", "20140198234", "20140218371", "20140222809", "20140236882", "20140250126", "20140250374", "20140282011", "20140289222", "20140337324", "20140341476", "20140351720", "20150005013", "20150078680", "20150082250", "20150091896", "20150106752", "20150130719", "20150143234", "20150213604", "20150227611", "20150287162", "20150363409", "20160019388", "20160140146", "20160358311", "20170019587", "20170244959", "20170357409", "20180091732"], "related": ["62349109", "62349092", "62349094", "62349099"]}, {"id": "20180011903", "patent_code": "10346401", "patent_name": "Query rewriting in a relational data harmonization framework", "year": "2019", "inventor_and_country_data": " Inventors: \nAbolhassani; Neda (Athens, GA), Tung; Teresa Sheausan (San Jose, CA), Gomadam; Karthik (San Jose, CA)  ", "description": "TECHNICAL FIELD\n This disclosure relates to database systems, and to queries executed in database systems.\nBACKGROUND\n The processing power, memory capacity, available disk space, and other resources available to processing systems have increased exponentially in recent years.  Database systems in particular have grown in capacity and capability to power\nextremely complex and sophisticated analyses on immense datasets that discover useful information, suggest conclusions, and support decision-making.  Improvements in database systems will further advance database capabilities. BRIEF DESCRIPTION\nOF THE DRAWINGS\n FIG. 1 shows a query rewriting architecture and query rewriting logic.\n FIG. 2 illustrates a semantic model for an example database.\n FIG. 3 shows another example database.\n FIG. 4 shows a corresponding semantic model for the database in FIG. 3.\n FIG. 5 shows a query tree.\n FIG. 6 shows a query rewriting processor.\nDETAILED DESCRIPTION\n Enterprises store their data in multiple different databases.  The databases are established for many reasons and according to many different factors such as data structure, volatility, data type, volume, and security level.  With multiple\ndifferent databases, combining data from varied sources into integrated, unambiguous and consistent information is a requirement for the enterprise data management plan.  Data Virtualization (DV) harmonizes and integrates data from diverse sources,\nlocations, and structures and offers an interface to the user which hides the technical details of stored data, such as access language and storage technologies.  DV provides an abstraction layer that has the advantage of not replicating data in a giant\ndata warehouse, and allowing access to data without requiring applications to know technical details about the data, such as how the data is formatted, where it is physically stored, or even in what databases or tables the data resides.\n The query rewriting processor (\"processor\") described below captures database interactions in a semantic model (e.g., a Resource Description Framework (RDF) model).  The semantic model may employ a labeled, directed acyclic graph structure.  The\nsemantic models are mappings from the relational database structure, including interconnections, to a description framework (e.g., RDF).  The processor provides a comprehensive approach for query rewriting in a data harmonization framework where the data\nis located in relational data stores.  The processor thereby improves underlying computer system implementations that include relational databases.  In particular, the processor allows a system to accurately and efficiently generate and issue structured\nquery language (SQL) queries, e.g., through a DV layer, with the SQL query automatically enhanced with knowledge about the database interactions that the processor discovers from the initial model query and the semantic models.\n Expressed another way, in one implementation the processor generates, references, or obtains database semantic models (e.g., RDF knowledge graphs).  The semantic models capture the structure and interconnections (e.g., foreign and primary key\nlinks to other tables) present in relational databases.  The purpose of the initial model query (e.g., a SPARQL query) is to interrogate the semantic model for structural information about the relational databases.  This is often done as a first step in\nultimately obtaining information from the databases, given that the DV layer hides the technical implementation details about the data.\n The processor generates the enriched model query and translates the enriched model query to obtain a custom relational database query (e.g., an SQL query).  The processor may then pass the custom relational database query to a DV layer for\nexecution against the individual relational databases.  Said another way, the processor automatically translates queries for information about the relational database structure to a corresponding or matching query to actually obtain the relevant data\nfrom the relational database structure.\n The processor may be added as a component to a very wide range of systems and enterprises that use any type of data.  One example is provided for discussion purposes below, and many others are possible.  In the example, a university is\ninterested in implementing a student registration system that involves multiple tables and databases.  Assume that the tables are: Student, Department, Course, and Enrolled.  Any query source may ask for information about a student without knowing in\nwhich database or table the data resides.  Hard coding all the database paths for each of the queries in the program would give rise to difficult maintenance issues.  One technical advantage and system improvement achieved by the processor is the\nelimination of hard coding.\n The processor maintains metadata for the databases in the semantic models, e.g., in RDF knowledge graphs.  The initial model query is prepared for submission against the knowledge graphs in order to retrieve the database structure and\ninterconnection information as a first step in obtaining data of interest.  The processor dynamically translates the initial model query (e.g., a SPARQL query) to a corresponding relational query (e.g., an SQL query).  The processor thereby accelerates\nobtaining the data of interest in an accurate and efficient manner.\n FIG. 1 shows an example of a query rewriting architecture 100 and query rewriting logic 150.  As noted above, the interconnections between database objects are represented in semantic models, such as RDF knowledge graphs, and may be stored in a\nlibrary of semantic models, e.g., in the semantic model database 102.  The initial model query 104 has no information about the location of the desired data, and is received by the processor system, e.g., as an SPARQL query (152).\n The processor extracts the projected attributes of the initial model query 104 from the initial model query 104 (154).  The column names that are written after the SELECT query keyword are the projected attributes.  In the student registration\nsystem example, an example of a projected attribute in the initial model query is \"?name\".  In the translation process, the processor projects \"?name\" as, for instance, \"Course.Name\" into the relational database query.  The processor scans the tuple\nstatements in the model query, and identifies those individual statements that are valid semantic model queries, and also identifies combinations of individual statements that together form a valid model queries (165).  These individual statements and\ncombination of statements that form valid queries are the `n` different sub-queries 106 shown in FIG. 1.\n The sub-queries were written to query the knowledge graph in order to find the path of each database object (158) needed to actually obtain the data of interest.  The processor executes the analysis rules to generate an enriched model query with\nthe combination of the projected attributes 108 and the result of the sub-queries (160).  The processor also translates the enriched model query to a relational database query 110 with the conditions and database information obtained after analysis and\nenrichment of the initial model query 104 (162).  The processor may transmit the relational database query to another or system or process for further action (164), including, e.g., to a DV interface which will execute the relational query against the\ndatabases to actually retrieve the data of interest.\n Semantic Model Generation\n A semantic model may be implemented as a knowledge graph that captures a reference schema for database entities, their interconnections, and other and information about them.  The processor may work with pre-generated knowledge graphs or may\ngenerate its own knowledge graphs.  The knowledge graphs capture the schema of the relational databases.  Generating the knowledge graphs may include scanning each relational schema to determine name, type and referential integrity constraints of the\ntables and their columns.  Referential integrity may be implemented and enforced by primary and foreign key combinations.  The semantic model captures the extracted information as an RDF graph with resources pointing to the database table, column names\nand their relationships.\n Table 1, below, shows example semantic model statements for capturing a relational database schema.\n TABLE-US-00001 TABLE 1 Semantic model statements for capturing the database schema Subject Predicate Object :Database :has_Table :Table :Table :has_Attribute_attr :Attribute :Table :has_pk :PrimaryKeyAttribute :Table :has_ReferencedTable_tbl\n:ForeignKeyAttribute :ReferredTable :refer_ReferredTable_tbl :ForeignKeyAttribute\n FIG. 2 shows an example semantic model 200 for the example university database 201.  In one implementation, the semantic model stores the metadata information in RDF &lt;subject, predicate, object&gt; tuples in the following manner: the links\nare predicates and their ending nodes are subjects and objects of each RDF statement.  There are links between a database schema and its tables, denoted by the \"has_Table\" constructs 202, 204, 206, and 208.  As demonstrated in the example semantic model\n200 in FIG. 2, the four tables of this example are connected to their column names with a link formed by the \"has_Attribute_attr\" construct, e.g., the \"has_age_attr\" construct 210.  As can be seen in FIG. 2, the Student Table that has Age, Name, and ID\ncolumns, with the ID column being a primary key.  The semantic model 200 also includes the links between tables and columns in the database for primary key attributes of the tables, using the \"has_pk\" construct, e.g., the \"has_pk\" construct 212 that\ncaptures the primary key in the ID column.\n Referential integrity constraints are shown with dotted lines in FIG. 2, e.g., the referential integrity constraint 214.  In this example, the Enrolled table has foreign keys to the Course and Student tables and the Enrolled table is connected\nto its foreign keys with referenced table links, denoted by the has_ReferencedTableName_tbl construct, e.g., the \"has_STUDENT_tbl\" construct 216.  The referenced tables are also connected to these keys with referred table links, denoted by the\nrefer_ReferencedTableName_tbl construct, e.g., the \"refer_STUDENT_tbl\" construct 218.  The processor reads these links to determine constraints present in the database schema.\n Translation Rules for Translating the Model Query to a Relational Query\n The processor may implement any number of translation rules for converting a model query to a relational database query.  Example translation rules are given below.  The processor executes the translation rules to generate a relational query\nbased on a model query (e.g., the initial model query).  Note that the processor recognizes and handles situations in which the initial model query originally lacks sufficient tuple statements for a complete translation.  For instance, assume a search is\nlooking for the department of a student whose name is `John`.  The initial model query for this request in SPARQL is shown in Table 2.\n TABLE-US-00002 TABLE 2 Query 1.  SPARQL input query SELECT ?name WHERE { :Student :has_Name_attr `John`.  :Student :has_Department_tbl ?d. ?d :has_Name_attr ?name.  }\n The processor may extract, from the initial model query of subject, predicate, object tuples, the tables and projected attributes.  If one of the subjects is a uniform resource indicator (URI) and it belongs to a database according to a\nhas_table link, then the processor may extract the subject as a table.  Moreover, if multiple databases are involved in the model query which have the same table, the processor may translate that to a SQL statement including a Union for the extracted\ntables of the databases.  The processor may implement, for example, the logic in Algorithm 1 for this processing.\n TABLE-US-00003 Algorithm 1 Tables and Attributes Extraction 1: attributesList .rarw.  projected attributes 2: while (subject is URI and hasResult(?database has_table subject)) or (object is URI and hasResult(?database :has_table object)) or\nisTrue(predicate contains a table) do 3: tablesList .rarw.  corresponding subject, object or 4: predicate 5: if ?database has more than one value then 6: create a Union for multiple databases with the extracted table 7: end if 8: end while\n The processor may also detect query conditions after extracting the tables and attributes.  If the processor finds a literal as the object of the model query, then the processor treats the literal as a condition in the where clause of the\ngenerated SQL query.  The processor may generate the attribute which holds the condition by implementing and executing the logic in Algorithm 2.  For the given example, Department and Student are added to the tables list and they both belong to only one\ndatabase.  In addition, `John` is detected as a literal.\n TABLE-US-00004 Algorithm 2 Literal in the Condition 1: while (object is literal) do 2: table .rarw.  subject 3: attribute .rarw.  predicate 4: condition .rarw.  table.attribute = literal 5: end while\n The processor may also detect joins by detecting when there is a co-referencing in the tuple statements.  The processor detects co-referencing when the subject variable of a statement is the same as the object variable of another statement.  In\naddition, the statement which contains the object variable has a &lt;predicate&gt; that has a table name (T) in its pattern (e.g., \"Course\" in \"has_Course_tbl\").  If the tuple statement which contains the object variable of co-referencing can\nindividually be treated as a correct model query, then the tables involved in the join have a 1-to-many relationship.  Otherwise, the tables have a many-to-many relationship and there is another table involved in the model query which is not indicated in\nthe model query (recall that the query source does not have information about the actual interconnections of the database system objects).\n Considering the initial model query of the student department example shown in Table 2, variable \"?ed\" is present as a &lt;subject&gt; and an &lt;object&gt; variable.  The &lt;predicate&gt; of the second tuple statement which has \"?ed\" as its\n&lt;object&gt; variable has the Department table in its pattern as well.  This pattern indicates a co-referencing.  Since the second tuple statement is a correct model query based on the given semantic model, this is a 1-to-many relationship.\n In the case of a 1-many relationship, the processor modifies the statement containing the subject variable of the co-referencing and adds a new statement to the model query which has the refer_T_tbl predicate.  Continuing the example above, the\nprocessor enriches the initial model query as shown in Table 3.\n TABLE-US-00005 TABLE 3 Query 2.  Enriched SPARQL query SELECT ?name WHERE { :Student :has_Name_attr `John` .  :Student :has_Department_tbl ?d .  ?dprt :refer_Department_tbl ?d .  ?dprt :has_Name_attr ?name .  }\n In particular, the processor has modified the last two statements of the initial model query as shown in enriched model query in Table 2 to have the same subject variables (\"?dprt\").  The processor translates the statements by adding a join in\nthe translated SQL query that the processor prepares.  The processor queries the semantic model to find the join condition based on the primary key of the referred table and the foreign key of T. Using this additional information and the enriched model\nquery, the processor translates the initial model query to obtain the relational query shown in Table 4.\n TABLE-US-00006 TABLE 4 Query 3.  Translated SQL query.  SELECT D .  name FROM Student AS S JOIN Department AS D ON D.id = S .  departmentid WHERE S .  name = `John`;\n In a many-to-many relationship scenario, the processor enriches the model query by adding the mutual table between the two tables to the previously extracted tables list.  Moreover, the processor may enrich the model query by changing the\nstatement that has the object variable of the co-referencing and add new statements to impose two joins between the three tables.  The processor may accomplish this with, e.g., three added statements that enrich the model query.  The processor may\nimplement and execute the join detection explained in detail in Algorithm 3 to handle the 1-to-many and the many-to-many relationships.\n TABLE-US-00007 Algorithm 3 Co-referencing, Joining 1-to-Many and Many-to-Many Relationships 1: for i = 1 to number of query statements do 2: subjectList .rarw.  subject of ith statement 3: objectList .rarw.  object of ith statement 4:\npredicateList .rarw.  predicate of ith statement 5: end for 6: for i = 1 to number of statements do 7: for j = 1 to number of statements do 8: %%%Co-referencing Detection%%% 9: if subjectList[j] = objectList[i] and isTrue(predicateList[i] contains a\nTableName) then 10: result .rarw.  SELECT objectList[i] 11: WHERE {statement[i]} 12: if result is null then 13: %%%Many-Many Relationship%%% 14: tablesList .rarw.  SELECT ?table 15: WHERE{?table : has_TableName ?o1.  16: ?table : has_subjectList[i] ?o2.}\n17: ith statement .rarw.  subjectList[i] : refer_subjectList[i]_tbl ?newObject1 18: first new statement .rarw.?newSubject : has_subjectList[i]_tbl ?newObject1 19: second new statement.rarw.?newSubject1 : has_TableName_tbl ?newObject2 20: third new\nstatement.rarw.?subjectList[j] : refer_TableName_tbl ?newObject2 21: else 22: %%%1-Many Relationship%%% 23: new statement .rarw.?newSubject : refer_TableName_tbl ?objectList[i] 24: jth statement .rarw.  ?newSubject predicateList[j] ?objectList[j] 25: end\nif 26: end if 27: end for 28: end for\n FIG. 3 shows another example database 300 analyzed in connection with a further example of the processor processing below.  The database 300 includes a Student table 302, an Enrolled table 304, and a Course table 306.  Primary keys are denoted\n[PK] and foreign keys [FK].  FIG. 4 shows a corresponding semantic model 400 for the database 300.\n In this example, a query source asks for the course names (\"?name\") that a student whose name is `Neda` has taken.  Query 4 in Table 5 below is the initial model query.\n TABLE-US-00008 TABLE 5 Query 4.  SPARQL input query.  SELECT ? name WHERE { :Student :has_Name_attr `Neda`.  :Student :has_Course_tbl ?c. ?c :has_Name_attr ?name.  }\n Note that the initial model query in Table 4 lacks sufficient query statements to provide all of the information for the translation to the relational query.  The initial model query does not return any result when executed against the semantic\nmodel 400 for the database 300.\n The processor updates and enriches the initial model query according to the techniques described above.  Specifically, the processor finds the many-to-many relationship between the Student.fwdarw.Enrolled and Course.fwdarw.Enrolled tables.  The\nprocessor also executes the co-referencing rule in testing the second tuple statement to detect a mutual table.  Table 6 shows the enriched model query generated by the processor.  The enriched model query shows that the processor identifies and includes\nthe Enrolled table, even though the query source was unaware of that (and all other) tables.\n TABLE-US-00009 TABLE 6 Query 5.  Enriched SPARQL query SELECT ?name WHERE { :Student :has_Name_attr `John`.  :Student :refer_Student_tbl ?sid.  ?e :has_Student_tbl ?sid.  ?e :has_Course_tbl ?cid .  ?course :refer_Course_tbl ?cid.  ?course\n:has_Name_attr ?name.  }\n Tables 7-12 below show intermediate the processor makes along the way from the initial model query in Table 5 to the enriched model query shown in Table 6.  Tables 7-12 also illustrate in parallel the translated relational query, as the\nprocessor determines its components, starting from empty Select, From, and Where statements that will form the translated relational query.  The processor updates and enriches the initial model query by executing the rules described above.  As part of\nthe translation process, the processor discovers the tables Student, Course, and Enrolled.  In addition, the processor finds the many-to-many relationship between the Student.fwdarw.Enrolled and Course.fwdarw.Enrolled tables.  The processor also executes\nthe co-referencing rule in testing the second tuple statement regarding \"?sid\" to detect a mutual table.\n TABLE-US-00010 TABLE 7 Processor discovers tables Student and Course Input SPARQL query: Output SQL query: SELECT ?name SELECT WHERE { FROM :Student :has_name_attr `Neda ` .  WHERE :Student :has_Course_tbl ?c .  ?c :has_name_attr ?name .  }\nTables List={Student, Course} Note that the processor located the Course table by extracting \"Course\" from the \"has_Course_tbl\" pattern.\n TABLE-US-00011 TABLE 8 Processor identifies literal `Neda` as the student of interest and prepares the translated relational query to include a corresponding selection from the Student table for the name `Neda`.  Input SPARQL query: Output SQL\nquery: SELECT ?name SELECT WHERE { FROM Student as S :Student :has_name_attr `Neda ` .  WHERE S.name = `Neda` :Student :has_Course_tbl ?c .  ?c :has_name_attr ?name .  } Tables List={Course} Note that the processor added the Student table to the output\nSQL query and it is not in the tables list anymore.\n TABLE-US-00012 TABLE 9 Processor finds the variable \"?c\" as a Subject in a tuple statement, and \"?c\" is not a URI Input SPARQL query: Output SQL query: SELECT ?name SELECT WHERE { FROM Student as S :Student :has_name_attr `Neda ` .  WHERE S.name\n= `Neda` :Student :has_Course_tbl ?c .  ?c :has_name_attr ?name .  } Tables List={Course}\n TABLE-US-00013 TABLE 10 Processor determines that running the query \":Student :has_Course_tbl ?c.\" against the semantic model returns no result because the Student table 302 does not refer to the Course table 306 directly.  Input SPARQL query:\nOutput SQL query: SELECT ?name SELECT WHERE { FROM Student as S :Student :has_name_attr `Neda `.  WHERE S.name = `Neda` :Student :has_Course_tbl ?c. ?c :has_name_attr ?name.  } Tables List={Course}\n TABLE-US-00014 TABLE 11 Processor identifies many-to-many relationship (Algorithm 3), and executes a custom model query against the semantic model to identify the intermediate table involved: Enrolled.  Input SPARQL query: Output SQL query:\nSELECT ?name SELECT WHERE { FROM Student AS S :Student :has_name_attr `Neda ` .  WHERE S.name = `Neda` :Student :has_Course_tbl ?c .  ?c :has_name_attr ?name .  } Tables List={Course, Enrolled} Many-to-Many .fwdarw.  SELECT ?table .fwdarw.  Enrolled\nRelationship WHERE{ ?table :has_Course_tbl ?c. ?table :has_Student_tbl ?s. }\n TABLE-US-00015 TABLE 12 Processor determines what is the key in the referenced table: ID.  Input SPARQL query: Output SQL query: SELECT ?name SELECT WHERE { FROM Student AS S :Student :has_name_attr `Neda ` .  JOIN Enrolled AS E :Student\n:refer_Student_tbl ?sid .  ON E.studentid = S.id ?e :has_Student_tbl ?sid .  WHERE S.name = `Neda` ?c :has_name_attr ?name.  } Tables List={Course} What is the key .fwdarw.  SELECT ?key .fwdarw.  ID in the referenced WHERE{ table? :Student :has_pk ?key. \n}\n TABLE-US-00016 TABLE 13 Final enriched model query and translated relational query Enriched model query Translated relational query SELECT ?name SELECT C.name WHERE { FROM Student AS S :Student :has_Name_attr `John`.  JOIN Enrolled AS E :Student\n:refer_Student_tbl ?sid.  ON S.id = E .studentid ?e :has_Student_tbl ?sid.  JOIN Course AS C ?e :has_Course_tbl ?cid .  ON C .  id = E .  courseid ?course :refer_Course_tbl ?cid.  WHERE S .  name = `Neda` ; ?course :has_Name_attr ?name.  }\n In Table 13, note that a selection of the student name is shown in the first tuple statement of the enriched model query.  The statements with ?sid object variables show a join between the Student table 302 and Enrolled table 304.  Moreover, the\nstatements with ?cid object variables show the second join on the Course table 306 and the Enrolled table 304.  The final tuple statement shows the source of the projected attribute which is a course name.\n FIG. 5 shows a query tree 500 used for testing and evaluation of the translated relational query in Table 6.  The query tree 500 demonstrates the relational algebra expression for the translated relational query, and it is identical to the\nrelational algebra for the enriched model query in Table 5.  In other words, the query tree 500 serves as validation that the translated relational query properly matches the enriched model query.\n FIG. 6 shows an example implementation of a query rewriting processor (processor) 600.  The processor 600 includes communication interfaces 602, system circuitry 604, input/output (I/O) interfaces 606, and display circuitry 608 that generates\nmachine interfaces 610 locally or for remote display, e.g., in a web browser running on a local or remote machine.  The communication interfaces 602 may include wireless transmitters and receivers (\"transceivers\") 612 and any antennas 614 used by the\ntransmit and receive circuitry of the transceivers 612.  The transceivers 212 and antennas 614 may support WiFi network communications, for instance, under any version of IEEE 802.11, e.g., 802.11n or 802.11ac.  The communication interfaces 602 may also\ninclude physical medium transceivers 616.  The physical medium transceivers 616 may provide physical layer interfaces for any of a wide range of communication protocols, such as any type of Ethernet, data over cable service interface specification\n(DOCSIS), digital subscriber line (DSL), Synchronous Optical Network (SONET), or other protocol.\n The system circuitry 604 may include hardware, software, firmware, or other circuitry in any combination.  The system circuitry 604 may be implemented, for example, with one or more systems on a chip (SoC), application specific integrated\ncircuits (ASIC), microprocessors, discrete analog and digital circuits, and other circuitry.  The system circuitry 604 is part of the implementation of any desired functionality in the processor 600, including the translation rules and semantic models. \nAs just one example, the system circuitry 604 may include one or more instruction processors 618 and memories 620.  The memory 620 stores, for example, control instructions 622 and an operating system 624.  In one implementation, the processor 618\nexecutes the control instructions 622 and the operating system 624 to carry out any desired functionality for the processor 600.  The control parameters 626 provide and specify configuration and operating options for the control instructions 622,\noperating system 624, and other functionality of the processor 600.\n The processor 600 may connect to and interact with any number of local or remote databases 632, e.g., via a data virtualization layer 634 or database management system 636.  The databases 632 define and store database table structures that the\ncontrol instructions 622 access to perform the functionality implemented in the control instructions 622.  The processor 600 may execute the control instructions 622 to perform the query rewriting processing noted above, including accessing the semantic\nmodels 638, and executing the translation rules 640 to enrich semantic model queries 642 and translate the semantic model queries 642 into translated relational queries 644.\n The semantic models 638, translation rules 640, and control instructions 622 improve the functioning of the underlying computer hardware itself.  That is, these features (among others described above) are specific improvements in way that the\nunderlying system operates.  The improvements facilitate more efficient, accurate, and precise execution of database queries received from any query source 646, whether locally or over any interface or network(s) 648.  The query source 646 provides,\ne.g., the initial model query 650, and the processor 600 performs the processing noted above to enrich the model query, generate a translated relational query 652, and obtain corresponding database data 654 by executing the relational query 652 against\nrelational databases.  The improvements are of particular relevance in, e.g., complex data virtualization environments, to allow database details to be abstracted to avoid, e.g., hard coding and other undesirable database access techniques.\n The methods, devices, processing, circuitry, and logic described above may be implemented in many different ways and in many different combinations of hardware and software.  For example, all or parts of the implementations may be circuitry that\nincludes an instruction processor, such as a Central Processing Unit (CPU), microcontroller, or a microprocessor; or as an Application Specific Integrated Circuit (ASIC), Programmable Logic Device (PLD), or Field Programmable Gate Array (FPGA); or as\ncircuitry that includes discrete logic or other circuit components, including analog circuit components, digital circuit components or both; or any combination thereof.  The circuitry may include discrete interconnected hardware components or may be\ncombined on a single integrated circuit die, distributed among multiple integrated circuit dies, or implemented in a Multiple Chip Module (MCM) of multiple integrated circuit dies in a common package, as examples.\n Accordingly, the circuitry may store or access instructions for execution, or may implement its functionality in hardware alone.  The instructions may be stored in a tangible storage medium that is other than a transitory signal, such as a flash\nmemory, a Random Access Memory (RAM), a Read Only Memory (ROM), an Erasable Programmable Read Only Memory (EPROM); or on a magnetic or optical disc, such as a Compact Disc Read Only Memory (CDROM), Hard Disk Drive (HDD), or other magnetic or optical\ndisk; or in or on another machine-readable medium.  A product, such as a computer program product, may include a storage medium and instructions stored in or on the medium, and the instructions when executed by the circuitry in a device may cause the\ndevice to implement any of the processing described above or illustrated in the drawings.\n The implementations may be distributed.  For instance, the circuitry may include multiple distinct system components, such as multiple processors and memories, and may span multiple distributed processing systems.  Parameters, databases, and\nother data structures may be separately stored and managed, may be incorporated into a single memory or database, may be logically and physically organized in many different ways, and may be implemented in many different ways.  Example implementations\ninclude linked lists, program variables, hash tables, arrays, records (e.g., database records), objects, and implicit storage mechanisms.  Instructions may form parts (e.g., subroutines or other code sections) of a single program, may form multiple\nseparate programs, may be distributed across multiple memories and processors, and may be implemented in many different ways.  Example implementations include stand-alone programs, and as part of a library, such as a shared library like a Dynamic Link\nLibrary (DLL).  The library, for example, may contain shared data and one or more shared programs that include instructions that perform any of the processing described above or illustrated in the drawings, when executed by the circuitry.\n Various implementations have been specifically described.  However, many other implementations are also possible.", "application_number": "15227605", "abstract": " A query rewriting processor (processor) analyzes database semantic models\n     (e.g., RDF knowledge graphs) that capture the interconnections (e.g.,\n     foreign and primary key links to other tables) present in a relational\n     database. The processor generates an enriched model query given an\n     initial model query (e.g., a SPARQL query) against the semantic model.\n     The processor generates the enriched model query and translates the\n     enriched model query into a relational database query (e.g., an SQL\n     query). The processor may then pass the relational database query to\n     another system or process (e.g., a data virtualization layer) for\n     execution against the individual relational databases. In this manner,\n     the processor automatically translates queries for information about the\n     relational database structure to a corresponding or matching query for\n     data from the relational database structure.\n", "citations": ["20020143754", "20090094216", "20090138437", "20090300002", "20100241637", "20130124500", "20150269223", "20160103931"], "related": ["62359547"]}, {"id": "20180048662", "patent_code": "10313365", "patent_name": "Cognitive offense analysis using enriched graphs", "year": "2019", "inventor_and_country_data": " Inventors: \nJang; Jiyong (White Plains, NY), Kirat; Dhilung Hang (White Plains, NY), Park; Youngja (Princeton, NJ), Stoecklin; Marc Philippe (White Plains, NY)  ", "description": "BACKGROUND\nTechnical Field\n This disclosure relates generally to cybersecurity offense analytics.\nBackground of the Related Art\n Today's networks are larger and more complex than ever before, and protecting them against malicious activity is a never-ending task.  Organizations seeking to safeguard their intellectual property, protect their customer identities, avoid\nbusiness disruptions, and the like, need to do more than just monitor logs and network flow data; indeed, many organizations create millions, or even billions, of events per day, and distilling that data down to a short list of priority offenses can be\ndaunting.\n Known security products include Security Incident and Event Management (SIEM) solutions, which are built upon rule-based mechanisms to evaluate observed security events.  SIEM systems and methods collect, normalize and correlate available\nnetwork data.  One such security intelligence product of this type is IBM.RTM.  QRadar.RTM.  SIEM, which provides a set of platform technologies that inspect network flow data to find and classify valid hosts and servers (assets) on the network, tracking\nthe applications, protocols, services and ports they use.  The product collects, stores and analyzes this data, and it performs real-time event correlation for use in threat detection and compliance reporting and auditing.  Using this platform, billions\nof events and flows can therefore be reduced and prioritized into a handful of actionable offenses, according to their business impact.  While SIEM-based approaches provide significant advantages, the rules are either hard coded or parameterized with a\nthreat feed with concrete indicators of compromise (IoCs).  Thus, typically these solutions are able to detect only known threats, but for unknown threats, e.g., detected by means of a behavior based rule, are unable to identify root cause and assist the\nsecurity analyst.  Moreover, these systems can present implementation challenges, as they often rely on manual curation of any semi-structured and unstructured threat feeds, i.e., natural language text, by means of security professionals reading threat\nadvisories and extracting IoCs.\n Security Operations Center (SOC) analysts who use such systems are confronted with a large number of offenses every day.  The majority of their time is spent to understand and analyze these offenses, confirm their validity, find related\ninformation, and attempt to find appropriate actions to resolve them.  Typically, SOC analysts attempt to find relevant cybersecurity intelligence reports and/or vulnerability reports for the target offenses from various data sources.  To this end,\nmostly they use web search engines to query and manually browse threat and security intelligence Internet services.  Given the widely-disparate information sources, an analyst often is faced with many, often conflicting, data sources and hypotheses to\nread and process to draw a conclusion.\n Presently, there are no automated systems or tools to do search, filtering, and prioritization of hypotheses for security offenses.  The subject matter of this disclosure addresses this need.\nBRIEF SUMMARY\n According to this disclosure, a method, apparatus and computer program product for cybersecurity offense analytics uses a cognitive methodology to automatically analyze and enrich an offense on behalf of a security analyst by collecting relevant\ncontextual data.\n According to a first embodiment, an automated method begins by receiving from a security system (e.g., a SIEM) information representing an offense.  Based in part on context data extracted from the offense, an offense context graph is then\nbuilt.  The offense context graph comprises nodes and edges, with an edge therein representing a relationship between a pair of nodes, at least one of the nodes being a root node representing an entity associated with the offense.  The method then\ncontinues by mining information about one or more other events that are determined to share a local contextual relationship with the offense represented by the offense context graph.  This operation generates an enriched offense context graph.  The\nenriched offense context graph is then pruned to identify an offense context for further examination.  Pruning the offense context graph produces a pruned offense context graph and may involve various types of operations, such as applying a metric to one\nor more events associated with the offense and removing nodes that, based on evaluation of the metric, do not contribute to the offense.  Other pruning techniques that may be leveraged include consolidating nodes that represent redundant information,\nremoving nodes and edges that are found to be outside paths between the root node and nodes determined or known to be malicious, summarizing subgraphs to a higher-level abstraction, hiding or removing irrelevant intermediate nodes on paths, clustering of\nsimilar nodes connected to a given node, and replacing nodes that share a common semantic meaning or relevance with a summary node.\n According to a second aspect of this disclosure, an apparatus for processing security event data is described.  The apparatus comprises a processor, and computer memory holding computer program instructions executed by processor to perform a set\nof operations such as described above.\n According to a third aspect of this disclosure, a computer program product in a non-transitory computer readable medium for use in a data processing system for processing security event data is described.  The computer program product holds\ncomputer program instructions executed in the data processing system and operative to perform operations such as described above.\n The foregoing has outlined some of the more pertinent features of the subject matter.  These features should be construed to be merely illustrative.  Many other beneficial results can be attained by applying the disclosed subject matter in a\ndifferent manner or by modifying the subject matter as will be described. BRIEF DESCRIPTION OF THE DRAWINGS\n For a more complete understanding of the present invention and the advantages thereof, reference is now made to the following descriptions taken in conjunction with the accompanying drawings, in which:\n FIG. 1 depicts an exemplary block diagram of a distributed data processing environment in which exemplary aspects of the illustrative embodiments may be implemented;\n FIG. 2 is an exemplary block diagram of a data processing system in which exemplary aspects of the illustrative embodiments may be implemented;\n FIG. 3 illustrates a security intelligence platform in which the techniques of this disclosure may be practiced;\n FIG. 4 depicts a high level process flow of the cognitive analysis technique of this disclosure;\n FIG. 5 depicts the cognitive analysis technique in additional detail; and\n FIG. 6 depicts how an offense context graph is augmented using a security knowledge graph according to this disclosure;\n FIG. 7 depicts a representative example scenario and, in particular, a partial set of findings that are output from the SIEM about a potential offense;\n FIG. 8 depicts a representative offense context graph that is generated from the findings;\n FIG. 9 depicts a representative offense graph after enrichment with nodes and relationships from the security knowledge graph;\n FIG. 10 depicts a close-up view of the enriched offense graph; and\n FIG. 11 depicts how a knowledge graph finding (the subgraph) is merged into the offense context graph and then scored and pruned according to the technique of this disclosure.\nDETAILED DESCRIPTION OF AN ILLUSTRATIVE EMBODIMENT\n With reference now to the drawings and in particular with reference to FIGS. 1-2, exemplary diagrams of data processing environments are provided in which illustrative embodiments of the disclosure may be implemented.  It should be appreciated\nthat FIGS. 1-2 are only exemplary and are not intended to assert or imply any limitation with regard to the environments in which aspects or embodiments of the disclosed subject matter may be implemented.  Many modifications to the depicted environments\nmay be made without departing from the spirit and scope of the present invention.\n With reference now to the drawings, FIG. 1 depicts a pictorial representation of an exemplary distributed data processing system in which aspects of the illustrative embodiments may be implemented.  Distributed data processing system 100 may\ninclude a network of computers in which aspects of the illustrative embodiments may be implemented.  The distributed data processing system 100 contains at least one network 102, which is the medium used to provide communication links between various\ndevices and computers connected together within distributed data processing system 100.  The network 102 may include connections, such as wire, wireless communication links, or fiber optic cables.\n In the depicted example, server 104 and server 106 are connected to network 102 along with storage unit 108.  In addition, clients 110, 112, and 114 are also connected to network 102.  These clients 110, 112, and 114 may be, for example,\npersonal computers, network computers, or the like.  In the depicted example, server 104 provides data, such as boot files, operating system images, and applications to the clients 110, 112, and 114.  Clients 110, 112, and 114 are clients to server 104\nin the depicted example.  Distributed data processing system 100 may include additional servers, clients, and other devices not shown.\n In the depicted example, distributed data processing system 100 is the Internet with network 102 representing a worldwide collection of networks and gateways that use the Transmission Control Protocol/Internet Protocol (TCP/IP) suite of\nprotocols to communicate with one another.  At the heart of the Internet is a backbone of high-speed data communication lines between major nodes or host computers, consisting of thousands of commercial, governmental, educational and other computer\nsystems that route data and messages.  Of course, the distributed data processing system 100 may also be implemented to include a number of different types of networks, such as for example, an intranet, a local area network (LAN), a wide area network\n(WAN), or the like.  As stated above, FIG. 1 is intended as an example, not as an architectural limitation for different embodiments of the disclosed subject matter, and therefore, the particular elements shown in FIG. 1 should not be considered limiting\nwith regard to the environments in which the illustrative embodiments of the present invention may be implemented.\n With reference now to FIG. 2, a block diagram of an exemplary data processing system is shown in which aspects of the illustrative embodiments may be implemented.  Data processing system 200 is an example of a computer, such as client 110 in\nFIG. 1, in which computer usable code or instructions implementing the processes for illustrative embodiments of the disclosure may be located.\n With reference now to FIG. 2, a block diagram of a data processing system is shown in which illustrative embodiments may be implemented.  Data processing system 200 is an example of a computer, such as server 104 or client 110 in FIG. 1, in\nwhich computer-usable program code or instructions implementing the processes may be located for the illustrative embodiments.  In this illustrative example, data processing system 200 includes communications fabric 202, which provides communications\nbetween processor unit 204, memory 206, persistent storage 208, communications unit 210, input/output (I/O) unit 212, and display 214.\n Processor unit 204 serves to execute instructions for software that may be loaded into memory 206.  Processor unit 204 may be a set of one or more processors or may be a multi-processor core, depending on the particular implementation.  Further,\nprocessor unit 204 may be implemented using one or more heterogeneous processor systems in which a main processor is present with secondary processors on a single chip.  As another illustrative example, processor unit 204 may be a symmetric\nmulti-processor (SMP) system containing multiple processors of the same type.\n Memory 206 and persistent storage 208 are examples of storage devices.  A storage device is any piece of hardware that is capable of storing information either on a temporary basis and/or a permanent basis.  Memory 206, in these examples, may\nbe, for example, a random access memory or any other suitable volatile or non-volatile storage device.  Persistent storage 208 may take various forms depending on the particular implementation.  For example, persistent storage 208 may contain one or more\ncomponents or devices.  For example, persistent storage 208 may be a hard drive, a flash memory, a rewritable optical disk, a rewritable magnetic tape, or some combination of the above.  The media used by persistent storage 208 also may be removable. \nFor example, a removable hard drive may be used for persistent storage 208.\n Communications unit 210, in these examples, provides for communications with other data processing systems or devices.  In these examples, communications unit 210 is a network interface card.  Communications unit 210 may provide communications\nthrough the use of either or both physical and wireless communications links.\n Input/output unit 212 allows for input and output of data with other devices that may be connected to data processing system 200.  For example, input/output unit 212 may provide a connection for user input through a keyboard and mouse.  Further,\ninput/output unit 212 may send output to a printer.  Display 214 provides a mechanism to display information to a user.\n Instructions for the operating system and applications or programs are located on persistent storage 208.  These instructions may be loaded into memory 206 for execution by processor unit 204.  The processes of the different embodiments may be\nperformed by processor unit 204 using computer implemented instructions, which may be located in a memory, such as memory 206.  These instructions are referred to as program code, computer-usable program code, or computer-readable program code that may\nbe read and executed by a processor in processor unit 204.  The program code in the different embodiments may be embodied on different physical or tangible computer-readable media, such as memory 206 or persistent storage 208.\n Program code 216 is located in a functional form on computer-readable media 218 that is selectively removable and may be loaded onto or transferred to data processing system 200 for execution by processor unit 204.  Program code 216 and\ncomputer-readable media 218 form computer program product 220 in these examples.  In one example, computer-readable media 218 may be in a tangible form, such as, for example, an optical or magnetic disc that is inserted or placed into a drive or other\ndevice that is part of persistent storage 208 for transfer onto a storage device, such as a hard drive that is part of persistent storage 208.  In a tangible form, computer-readable media 218 also may take the form of a persistent storage, such as a hard\ndrive, a thumb drive, or a flash memory that is connected to data processing system 200.  The tangible form of computer-readable media 218 is also referred to as computer-recordable storage media.  In some instances, computer-recordable media 218 may not\nbe removable.\n Alternatively, program code 216 may be transferred to data processing system 200 from computer-readable media 218 through a communications link to communications unit 210 and/or through a connection to input/output unit 212.  The communications\nlink and/or the connection may be physical or wireless in the illustrative examples.  The computer-readable media also may take the form of non-tangible media, such as communications links or wireless transmissions containing the program code.  The\ndifferent components illustrated for data processing system 200 are not meant to provide architectural limitations to the manner in which different embodiments may be implemented.  The different illustrative embodiments may be implemented in a data\nprocessing system including components in addition to or in place of those illustrated for data processing system 200.  Other components shown in FIG. 2 can be varied from the illustrative examples shown.  As one example, a storage device in data\nprocessing system 200 is any hardware apparatus that may store data.  Memory 206, persistent storage 208, and computer-readable media 218 are examples of storage devices in a tangible form.\n In another example, a bus system may be used to implement communications fabric 202 and may be comprised of one or more buses, such as a system bus or an input/output bus.  Of course, the bus system may be implemented using any suitable type of\narchitecture that provides for a transfer of data between different components or devices attached to the bus system.  Additionally, a communications unit may include one or more devices used to transmit and receive data, such as a modem or a network\nadapter.  Further, a memory may be, for example, memory 206 or a cache such as found in an interface and memory controller hub that may be present in communications fabric 202.\n Computer program code for carrying out operations of the present invention may be written in any combination of one or more programming languages, including an object-oriented programming language such as Java.TM., Smalltalk, C++ or the like,\nand conventional procedural programming languages, such as the \"C\" programming language or similar programming languages.  The program code may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package,\npartly on the user's computer and partly on a remote computer, or entirely on the remote computer or server.  In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area network\n(LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).\n Those of ordinary skill in the art will appreciate that the hardware in FIGS. 1-2 may vary depending on the implementation.  Other internal hardware or peripheral devices, such as flash memory, equivalent non-volatile memory, or optical disk\ndrives and the like, may be used in addition to or in place of the hardware depicted in FIGS. 1-2.  Also, the processes of the illustrative embodiments may be applied to a multiprocessor data processing system, other than the SMP system mentioned\npreviously, without departing from the spirit and scope of the disclosed subject matter.\n As will be seen, the techniques described herein may operate in conjunction within the standard client-server paradigm such as illustrated in FIG. 1 in which client machines communicate with an Internet-accessible Web-based portal executing on a\nset of one or more machines.  End users operate Internet-connectable devices (e.g., desktop computers, notebook computers, Internet-enabled mobile devices, or the like) that are capable of accessing and interacting with the portal.  Typically, each\nclient or server machine is a data processing system such as illustrated in FIG. 2 comprising hardware and software, and these entities communicate with one another over a network, such as the Internet, an intranet, an extranet, a private network, or any\nother communications medium or link.  A data processing system typically includes one or more processors, an operating system, one or more applications, and one or more utilities.  The applications on the data processing system provide native support for\nWeb services including, without limitation, support for HTTP, SOAP, XML, WSDL, UDDI, and WSFL, among others.  Information regarding SOAP, WSDL, UDDI and WSFL is available from the World Wide Web Consortium (W3C), which is responsible for developing and\nmaintaining these standards; further information regarding HTTP and XML is available from Internet Engineering Task Force (IETF).  Familiarity with these standards is presumed.\n Security Intelligence Platform with Incident Forensics\n A representative security intelligence platform in which the techniques of this disclosure may be practiced is illustrated in FIG. 3.  Generally, the platform provides search-driven data exploration, session reconstruction, and forensics\nintelligence to assist security incident investigations.  In pertinent part, the platform 300 comprises a set of packet capture appliances 302, an incident forensics module appliance 304, a distributed database 306, and a security intelligence console\n308.  The packet capture and module appliances are configured as network appliances, or they may be configured as virtual appliances.  The packet capture appliances 302 are operative to capture packets off the network (using known packet capture (pcap)\napplication programming interfaces (APIs) or other known techniques), and to provide such data (e.g., real-time log event and network flow) to the distributed database 306, where the data is stored and available for analysis by the forensics module 304\nand the security intelligence console 308.  A packet capture appliance operates in a session-oriented manner, capturing all packets in a flow, and indexing metadata and payloads to enable fast search-driven data exploration.  The database 306 provides a\nforensics repository, which distributed and heterogeneous data sets comprising the information collected by the packet capture appliances.  The console 308 provides a web- or cloud-accessible user interface (UI) that exposes a \"Forensics\" dashboard tab\nto facilitate an incident investigation workflow by an investigator.  Using the dashboard, an investigator selects a security incident.  The incident forensics module 304 retrieves all the packets (including metadata, payloads, etc.) for a selected\nsecurity incident and reconstructs the session for analysis.  A representative commercial product that implements an incident investigation workflow of this type is IBM.RTM.  Security QRadar.RTM.  Incident Forensics V7.2.3 (or higher).  Using this\nplatform, an investigator searches across the distributed and heterogeneous data sets stored in the database, and receives a unified search results list.  The search results may be merged in a grid, and they can be visualized in a \"digital impression\"\ntool so that the user can explore relationships between identities.\n In particular, a typical incident forensics investigation to extract relevant data from network traffic and documents in the forensic repository is now described.  According to this approach, the platform enables a simple, high-level approach of\nsearching and bookmarking many records at first, and then enables the investigator to focus on the bookmarked records to identify a final set of records.  In a typical workflow, an investigator determines which material is relevant.  He or she then uses\nthat material to prove a hypothesis or \"case\" to develop new leads that can be followed up by using other methods in an existing case.  Typically, the investigator focuses his or her investigation through course-grained actions at first, and then\nproceeds to fine-tune those findings into a relevant final result set.  The bottom portion of FIG. 3 illustrates this basic workflow.  Visualization and analysis tools in the platform may then be used to manually and automatically assess the results for\nrelevance.  The relevant records can be printed, exported, or submitted processing.\n As noted above, the platform console provides a user interface to facilitate this workflow.  Thus, for example, the platform provides a search results page as a default page on an interface display tab.  Investigators use the search results to\nsearch for and access documents.  The investigator can use other tools to further the investigation.  One of these tools is a digital impression tool.  A digital impression is a compiled set of associations and relationships that identify an identity\ntrail.  Digital impressions reconstruct network relationships to help reveal the identity of an attacking entity, how it communicates, and what it communicates with.  Known entities or persons that are found in the network traffic and documents are\nautomatically tagged.  The forensics incident module 304 is operative to correlate tagged identifiers that interacted with each other to produce a digital impression.  The collection relationships in a digital impression report represent a\ncontinuously-collected electronic presence that is associated with an attacker, or a network-related entity, or any digital impression metadata term.  Using the tool, investigators can click any tagged digital impression identifier that is associated\nwith a document.  The resulting digital impression report is then listed in tabular format and is organized by identifier type.\n Generalizing, a digital impression reconstructs network relationships to help the investigator identify an attacking entity and other entities that it communicates with.  A security intelligence platform includes a forensics incident module that\nis operative to correlate tagged identifiers that interacted with each other to produce a digital impression.  The collection relationships in a digital impression report represent a continuously-collected electronic presence that is associated with an\nattacker, or a network-related entity, or any digital impression metadata term.  Using the tool, investigators can click any tagged digital impression identifier that is associated with a document.  The resulting digital impression report is then listed\nin tabular format and is organized by identifier type.\n Typically, an appliance for use in the above-described system is implemented is implemented as a network-connected, non-display device.  For example, appliances built purposely for performing traditional middleware service oriented architecture\n(SOA) functions are prevalent across certain computer environments.  SOA middleware appliances may simplify, help secure or accelerate XML and Web services deployments while extending an existing SOA infrastructure across an enterprise.  The utilization\nof middleware-purposed hardware and a lightweight middleware stack can address the performance burden experienced by conventional software solutions.  In addition, the appliance form-factor provides a secure, consumable packaging for implementing\nmiddleware SOA functions.  One particular advantage that these types of devices provide is to offload processing from back-end systems.  A network appliance of this type typically is a rack-mounted device.  The device includes physical security that\nenables the appliance to serve as a secure vault for sensitive information.  Typically, the appliance is manufactured, pre-loaded with software, and then deployed within or in association with an enterprise or other network operating environment;\nalternatively, the box may be positioned locally and then provisioned with standard or customized middleware virtual images that can be securely deployed and managed, e.g., within a private or an on premise cloud computing environment.  The appliance may\ninclude hardware and firmware cryptographic support, possibly to encrypt data on hard disk.  No users, including administrative users, can access any data on physical disk.  In particular, preferably the operating system (e.g., Linux) locks down the root\naccount and does not provide a command shell, and the user does not have file system access.  Typically, the appliance does not include a display device, a CD or other optical drive, or any USB, Firewire or other ports to enable devices to be connected\nthereto.  It is designed to be a sealed and secure environment with limited accessibility and then only be authenticated and authorized individuals.\n An appliance of this type can facilitate Security Information Event Management (SIEM).  For example, and as noted above, IBM.RTM.  Security QRadar.RTM.  STEM is an enterprise solution that includes packet data capture appliances that may be\nconfigured as appliances of this type.  Such a device is operative, for example, to capture real-time Layer 4 network flow data from which Layer 7 application payloads may then be analyzed, e.g., using deep packet inspection and other technologies.  It\nprovides situational awareness and compliance support using a combination of flow-based network knowledge, security event correlation, and asset-based vulnerability assessment.  In a basic QRadar STEM installation, the system such as shown in FIG. 3 is\nconfigured to collect event and flow data, and generate reports.  As noted, a user (e.g., an SOC analyst) can investigate offenses to determine the root cause of a network issue.\n Generalizing, Security Information and Event Management (SIEM) tools provide a range of services for analyzing, managing, monitoring, and reporting on IT security events and vulnerabilities.  Such services typically include collection of events\nregarding monitored accesses and unexpected occurrences across the data network, and analyzing them in a correlative context to determine their contribution to profiled higher-order security events.  They may also include analysis of firewall\nconfigurations, network topology and connection visualization tools for viewing current and potential network traffic patterns, correlation of asset vulnerabilities with network configuration and traffic to identify active attack paths and high-risk\nassets, and support of policy compliance monitoring of network traffic, topology and vulnerability exposures.  Some SIEM tools have the ability to build up a topology of managed network devices such as routers, firewalls, and switches based on a\ntransformational analysis of device configurations processed through a common network information model.  The result is a locational organization which can be used for simulations of security threats, operational analyses of firewall filters, and other\napplications.  The primary device criteria, however, are entirely network- and network-configuration based.  While there are a number of ways to launch a discovery capability for managed assets/systems, and while containment in the user interface is\nsemi-automatically managed (that is, an approach through the user interface that allows for semi-automated, human-input-based placements with the topology, and its display and formatting, being data-driven based upon the discovery of both initial\nconfigurations and changes/deletions in the underlying network), nothing is provided in terms of placement analytics that produce fully-automated placement analyses and suggestions.\n Cognitive Offense Analysis Using Contextual Data and Knowledge Graphs\n With the above as background, the techniques of this disclosure are now described.\n The basic approach of this disclosure involves processing security event data in association with a cybersecurity knowledge graph (\"KG\").  The cybersecurity knowledge graph is derived one or more data sources and includes a set of nodes, and a\nset of edges.  The method preferably is automated and begins upon receipt of information from a security system (e.g., a STEM) representing an offense.  Based on the offense type, context data about the offense is extracted, and an initial offense\ncontext graph is built.  The initial offense context graph typically comprises a set of nodes, and a set of edges, with an edge representing a relationship between a pair of nodes in the set.  At least one of the set of nodes in the offense context graph\nis a root node representing an offending entity that is determined as a cause of the offense.  The initial offense context graph also includes one or more activity nodes connected to the root node either directly or through one or more other nodes of the\nset, wherein at least one activity node has associated therewith data representing an observable.  The root node and its one or more activity nodes associated therewith (and the observables) represent a context for the offense.  According to the method,\nthe knowledge graph and potentially other data sources are then examined to further refine the initial offense context graph.\n In particular, preferably the knowledge graph is explored by locating the observables (identified in the initial offense graph) in the knowledge graph.  Based on the located observables and their connections being associated with one or more\nknown malicious entities as represented in the knowledge graph, one or more subgraphs of the knowledge graph are then generated.  A subgraph typically has a hypothesis (about the offense) associated therewith.  Using a hypothesis, the security system (or\nother data source) is then queried to attempt to obtain one or more additional observables (i.e. evidence) supporting the hypothesis.  Then, a refined offense context graph is created, preferably by merging the initial offense context graph, the one or\nmore sub-graphs derived from the knowledge graph exploration, and the additional observables mined from the one or more hypotheses.  The resulting refined offense context graph is then provided (e.g., to a SOC analyst) for further analysis.\n An offense context graph that has been refined in this manner, namely, by incorporating one or more subgraphs derived from the knowledge graph as well as additional observables mined from examining the subgraph hypotheses, provides for a refined\ngraph that reveals potential causal relationships more readily, or otherwise provides information that reveals which parts of the graph might best be prioritized for further analysis.  The approach herein thus greatly simplifies the further analysis and\ncorrective tasks that must then be undertaken to address the root cause of the offense.\n With reference now to FIG. 4, a high level process flow of the technique of this disclosure is provided The routine begins at step 400 with offense extraction and analysis.  In this step, an offense is extracted from a SIEM system, such as IBM\nQRadar, for deep investigation.  Typically, a detected offense may include many different entities, such as offense types, fired rules, user names, and involved indicators of compromise.\n At step 402, the process continues with offense context extraction, enrichment and data mining.  Here, offense context is extracted and enriched based on various information or factors such as, without limitation, time, an offense type, and a\ndirection.  This operation typically involves data mining around the offense to find potentially related events.  The process then continues at step 404 to build an offense context graph, preferably with the offending entity as the center node and\ncontextual information gradually connected to the center node and its children.  Examples of contextual information can be represented by activity nodes in the graph.  Typically, an activity comprises one or more observables, which are then connected to\nthe respective activity, or directly to the center node.\n The process then continues at step 406.  In particular, at this step a knowledge graph is explored, preferably using a set of observables extracted from the offense context graph.  This exploration step identifies related and relevant pieces of\ninformation or entities available from the knowledge graph.  A primary goal in this operation is to find out how strongly the input observables are related to malicious entities in the knowledge graph.  If the event related entities are strong malicious\nindicators, a hypothesis (represented by a subgraph in the knowledge graph) is generated.  The process then continues at step 408.  At this step, the resulting subgraph (generated in step 406) is mapped into the original offense context graph and scored. To reinforce the hypothesis (represented by the subgraph), additional evidence may be obtained (and built into the offense context graph) by querying local STEM data for the presence of activities that are related to the hypothesis that is returned by\nthe KG exploration in step 406.  Additional findings as part of the hypothesis scoring may also be used to extend the offense context graph further and/or to trigger new knowledge graph explorations.  Thus, step 408 represents an evidence-based scoring\nof the threat hypothesis.\n The process then continues at step 410 with an offense investigation.  At this point, the offense hypothesis includes the original offense IOCs (indicators of compromise), knowledge graph enrichment, evidence, and scores.  The extended offense\ncontext graph is then provided to the SOC analyst (user) for offense investigation.  The SOC user reviews the hypothesis that has been weighted in the manner described, and can then choose the right hypothesis that explains the given offense.  There may\nbe multiple hypotheses.\n If additional or further exploration and more evidence are needed to make a decision, the SOC user can elect to nodes or edges in the offense context graph and repeat steps 406 and 408 as needed.  This iteration is depicted in the drawing.  This\ncompletes the high level process flow.\n FIG. 5 depicts a modeling diagram showing the various entities involved in the technique and their interactions.  As depicted, these entities include the SOC user 500, the SIEM system 502, the (offense) context graph 504, a knowledge graph 506,\nand a maintenance entity 508.  Viewing the interactions from top to bottom, the knowledge graph 506 may be updated with new data/records 510 periodically; this operation is shown as an off-line operation (above the dotted line).  The remainder of the\nfigure depicts the process flow referenced above.  Thus, the new offense 505 is identified by the SIEM system 502 and used together with the offense details 510 and data mining 512 to generate the context graph 504 via the offense extraction and analysis\n514 and context graph building 516 operations.  Once built, the knowledge graph 506 is explored 518 to identify one or more subgraphs.  The evidence-based threat hypothesis scoring uses the subgraphs at operation 520, and the process may iterate\n(operation 522) as previously described.  After evidence validation and IOC mining 524, the offense investigation 526 is then carried out, typically by the SOC user 500.\n FIG. 6 depicts the offense context graph 600 augmented by the knowledge graph 602.  In general, the offense context graph 600 depicts local kinetics, e.g., events and intelligence related to an offense, e.g., SIEM offense data, log events and\nflows, and such information preferably is augmented from the information derived from the knowledge graph 602.  The knowledge graph is global in nature and scope, as it preferably depicts external cyber security and threat intelligence, cyber security\nconcepts, and the like.  Typically, the knowledge graph is informed by combining multiple structured and unstructured data sources.  As shown, the offense context graph is centered around a root node 604 that has child nodes 606 within the \"offense\" 605. The \"offense context\" 607 includes still other nodes of relevance.  There may also be a set of device activities 609 that include relevant device nodes 608.  As depicted by the arrow 610, augmenting the context graph 600 using the knowledge graph 602\nexamines whether there is any path (such as one or more of paths 611, 613 or 615) from a node in the set of offense context nodes 607 to a node in the set of device activities 609 that passes through one or more nodes of the knowledge graph 602 (to which\na threat activity is attached)? In the example shown, there is one or more such paths (611, 613 and 615), and the relevant subgraph 617 in the knowledge graph thus is captured and used to augment the offense context graph.\n Thus, in the approach, details of an offense are extracted from a STEM system, such as QRadar.  The details typically include offense types, rules, categories, source and destination IP addresses, and user names.  For example, an offense may be\na malware category offense that indicates that malicious software is detected on a machine.  Accordingly, activities of the machine around the offense need to be examined to determine infection vectors and potential data leakage.  Of course, the nature\nof the activities that will need to be investigated will depend on the nature of the offense.\n According to a further aspect of the approach herein, offense context related to an identified offense is then extracted and enriched depending on various factors, such as time, an offense type, and a direction.  For example, if an offense type\nis a source IP, system and network activities of the same source IP (which may or may not be captured at other offenses) may then be collected.  This collected context depicts potential casual relationships among events, and this information then\nprovides a basis for investigation of provenance and consequences of an offense, e.g., Markov modeling to learn their dependencies.  Of course, the nature of the offense context extraction and enrichment also depends on the nature of the offense.\n From the contextual data extracted (as described above), an initial offense \"context graph\" 600 in FIG. 6 is built, preferably depending on offense types, such that a main offense source becomes a root 604 of an offense context graph, and\noffense details are then linked together around the root node.  As noted above, the initial context graph preferably is then enriched and, in particular, by correlating local context, to further identify potential causal relationships among events.  This\nhelps analysts perform deep, more fine-grained investigation of provenance and consequences of the offense.\n In a preferred embodiment, provenance context preferably is extracted by identifying other offenses wherein the offense source is a target, e.g., an exploit target.  Similarly, consequence context is extracted, preferably by finding other\noffenses wherein the offense source also is a source, e.g., a stepping stone.  Similarly, consequence context is extracted by finding other offenses.  Thus, this graph typically contains the offending entity (e.g., computer system, user, etc.) as the\ncenter (root) node of the graph, and contextual information is gradually connected to the node and its children.  The result is the offense context 607 in FIG. 6.  Examples of contextual information will depend on the nature of the offense; such\ninformation can be represented by activity nodes that include, without limitation, network activity, user activity, system activity, application activity, and so forth.  Preferably, an activity comprises one or more observables, which are then connected\nto the respective activity nodes or directly to the center node.  Further, the context graph can be extended with additional nodes representing information that does not directly relate to the original offense.  For example, and by means of data mining\n(e.g., behavior-based anomaly detection, sequence mining, rule-based data extraction, and the like) of security-related events in temporal vicinity to the offense, additional activities of interest can be extracted and added to the context graph.  This\noperation is represented in the graph by device activities 606.\n Thus, in the approach as outlined so far, details of an offense are extracted from a SIEM system.  The details include (but are not limited to) offense types, rules, categories, source and destination IPs, and user names.  An initial offense\ncontext graph is built depending on offense types, such that the main offense source becomes the root of an offense context graph and offense details are linked together around the root node.  The initial context graph is then enriched by correlating\nlocal context to further identify potential casual relationships among events, which helps analysts perform deep investigation of provenance and consequences of the offense.  Provenance context is extracted by identifying other offenses where the offense\nsource is a target, e.g., an exploit target.  Similarly, consequence context is extracted by finding other offenses where the offense target is a source, e.g., a stepping stone.  The enriched (and potentially dense) offense context graph is then pruned\nto highlight critical offense context for the SOC analyst's benefit.  Typically, pruning is applied based on several metrics, such as weight, relevance, toxicity, and time.  For example, it may be desirable to assign weight to each event detail based on\noffense rules and categories to thereby indicate key features contributing to an offense.\n Once the initial offense context graph is built, preferably that context graph is further enriched, validated and/or augmented based on information derived from a cybersecurity knowledge graph (KG) 602, which preferably is a source of domain\nknowledge.  The knowledge graph, like the initial offense context graph, comprises nodes and edges.  The cybersecurity knowledge graph can be constructed in several ways.  In one embodiment, one or more domain experts build a KG manually.  In another\nembodiment, a KG 602 is built automatically or semi-automatically, e.g., from structured and unstructured data sources.  As noted above, the context extraction and analysis processes provide a list of observables related to the given offense.  According\nto this operation, the observables preferably are then enriched using the in-depth domain knowledge in the KG.  This enrichment (or knowledge graph exploration) is now described.\n In particular, this knowledge graph (KG) enrichment operation can be done in several different ways.  In one approach, enrichment involves building sub-graphs related to the observables.  To this end, the system locates the observables in the KG\nand discovers the connections among them.  This discovery may yield one or more subgraphs (such as 617 in FIG. 6) showing the relationships of the given observables with other related security objects such as observables and threats.  These subgraphs can\nprovide a broader view on the given offense.\n In another enrichment scenario, a SOC analyst can perform the query knowledge graph (KG) exploration step receives a set of observables, such as IP, URL, and files hashes, extracted from the SIEM offense.  This exploration step seeks to identify\nall related and relevant pieces of information or entities available in the knowledge graph.  The main goal is to find out how strongly the input observables are related to malicious entities in the knowledge graph.  Some of the related entities can be\nstrong malicious indicators, and thus a hypothesis about the offense can be generated.  The related malicious entities might be strongly related among themselves, which also creates a hypothesis.  Generalizing, an output of this step is a set of one or\nmore hypotheses, which are consumed during the evidence-based threat hypothesis scoring operation where they are evaluated against local SIEM data.  Preferably, and as noted above, the extraction of related entities is performed by traversing the\nknowledge graph, preferably starting from the input observables and extracting the subgraph.  In general, unconstrained subgraph extraction may result in a very large and noise graph.  Thus, and as will be further described below, preferably one or more\ntraversal algorithms that focus on finding different types of related information by exploring the graph and pruning less relevant entities from the result may be deployed.  One or more of these pruning algorithms may be run serially, in parallel, or\notherwise.  In addition, where possible coefficients of the graph entities are precomputed to enhance the efficiency of the graph traversal.\n The following describes additional details of the evidence-based threat hypothesis scoring.  Preferably, the knowledge graph exploration step returns a subgraph of observables, along with one or more annotations associated with the hypotheses. \nThis subgraph preferably is then mapped into the original context graph.  To reinforce the hypotheses, it may be desirable to build further relevant evidence, e.g., by querying local SIEM data for the presence of activities that are related to the\nhypotheses returned by the knowledge graph exploration.  These activities may not have been flagged before by a simple rule-based offense monitor.  This operation thus builds a merged graph that includes input from three sources, the original context\ngraph, the knowledge graph exploration subgraph, and the additional observables queried for building the evidence for the hypotheses.\n As also described, the final operation typically is offense investigation.  Based on the prior operations described, the offense hypotheses now include the original offense IOCs, knowledge graph enrichment and supporting evidences, and their\nscores.  This extended graph then is provided to an SOC analyst for an offense investigation.  The SOC analyst reviews the weighted hypotheses and chooses the right hypothesis that explains the given offense.  The selection itself may be automated, e.g.,\nvia machine learning.  If further exploration and more evidence are needed to make a decision, the SOC can choose the nodes and/or edges of interest in the hypothesis graphs, and then repeat the above-described steps of knowledge graph exploration and\nevidence-based threat hypotheses scoring.  During the hypothesis review process, the SOC may learn new facts and insights about the offense and, thus, add additional queries (e.g. observables or relationship) in a next iteration.  The SOC analyst thus\ncan use this iterative knowledge enrichment, evidence generation and hypothesis scoring to gain a deep understanding of the offense and actionable insights that may then be acted upon.\n Thus, the basic notion of this approach is to use an autonomic mechanism to extract what is known about an offense (or attack), reason about the offense based on generalized knowledge (as represented by the knowledge graph), and thereby arrive\nat a most probable diagnosis about the offense and how to address it.\n FIG. 7 depicts a representative example scenario and, in particular, a partial set of findings that are output from the STEM about a potential offense (in this example, a ransom-ware exploit).  FIG. 8 depicts a representative offense context\ngraph that is generated from the findings.  Nodes in the graph indicate observable properties of the offense events, including information such as IP addresses, domain names, URLs, malware hashes.  Edges represent semantic relationships between the\nnodes.  FIG. 9 depicts a representative offense context graph after enrichment with information from the security knowledge graph.  The enrichment may add multiple additional nodes to the existing offense context graph, which are deemed to be related to\nthe initial offense's nodes.  FIG. 10 depicts a close-up view of the enriched offense graph in FIG. 9, showing the relationship types in more detail.  An edge marked as \"LINKS\" between a file and a URL means that the file was downloaded from the URL\n(download URL), and an edge marked as \"CONNECT\" means the file made an HTTP request to URL (e.g., post-infection).  FIG. 11 depicts how a KG-determined subgraph finding is merged into the offense graph and then pruned and scored to identity that several\nactivities of this offense all relate to a single attack family.\n As described above, after generating the enriched offense context graph, preferably this graph is pruned, e.g., to reduce its size to a more manageable level.  The following provides additional details regarding pruning of the enriched offense\ncontext graph.\n In particular, pruning can be accomplished in various ways including, without limitation, consolidating nodes that represent redundant information, removing nodes and edges that are found to be outside any path from an input node (e.g.,\nrepresenting an observable associated with the offense under examination) to one or more nodes that are known to be malicious, summarizing subgraphs to a higher-level abstraction, and hiding irrelevant intermediate nodes on paths.  According to a\nparticular pruning technique, a metric (e.g., weight, relevance, distance, degree, toxicity, time, or the like) is applied to one or more events associated with the offense.  Then, and based on one or more rules (and categories) that indicate key\nfeatures and characteristics of the offense, nodes are scored according to the metric(s), and nodes with scores below a threshold are removed.  Additional information, such as network structures and connectivity, and identity of high value assets, may\nalso be leveraged to tune the metric(s).  In addition, paths between the root node and malicious nodes are determined, and nodes outside of any such paths preferably are marked for removal.  Clustering of the same type of nodes (e.g., anti-virus\nsignatures, file names, reputation, and URLs) connected to a given node is performed to summarize into a representative placeholder potentially redundant and overlapping information.  In addition, subgraphs and intermediate nodes preferably are analyzed\nfor their semantic meaning and relevance and, where appropriate, replaced by a summary node and/or removed.  The result is sometimes referred to herein as a pruned offense context graph (or a pruned context graph).\n As noted above, it may be desirable to explore the knowledge graph, e.g., once again to reduce its size to a more manageable level.  To this end, a signal flow analysis-based exploration of security knowledge represented in the graph structure\nmay be implemented.  In this approach, \"conductance\" values are associated to each of a set of edges.  Each node has an associated \"toxicity\" value representing a degree of maliciousness associated with the node.  The conductance value associated with an\nedge is a function of at least the toxicity values of the nodes to which the edge is incident.  A signal flow analysis is conducted with respect to an input node representing an observable associated with an offense.  The flow analysis seeks to identify\na subset of the nodes that, based on their conductance values, are reached by flow of a signal representing a threat, wherein signal flow over a path in the graph continues until a signal threshold is met.  Based on the analysis, nodes within the subset\nare designated as hypothesis nodes for further examination.  Entities that are not in the path from an input node to the hypothesis nodes preferably are pruned.\n The technique of this disclosure provides significant advantages.  The technique builds an enriched offense context graph that reveals potential causal relationships between security events and offenses, thereby helping the analyst comprehend an\noffense more thoroughly.  The approach enables the analyst to prioritize which parts of the graph to be investigated first, thereby leading to faster solution.  The approach provides security analysts with more comprehensive context from a variety of\nkinetics data imported into a SIEM system.  For deep and efficient investigation, the described approach leverages a comprehensive set of rules, and it offers enriched relevant context of an offense.  The approach enables efficient mining of offense\ncontext (e.g., activities, device event details, offense rules and categories, etc.) and to provide a comprehensive context graph for follow-on deep investigation and analysis.\n More generally, the approach herein provides for an enhanced data mining process on security data (e.g., a cybersecurity incident) to extract contextual data related to the incident, and to translate this information into a graph representation\nfor investigation by a security analyst.  The approach, being automated, is highly efficient, and it greatly eases the workflow requirements for the SOC analyst.\n The technique herein also provides for enhanced automated and intelligent investigation of a suspicious network offense so that corrective action may be taken.  The nature of the corrective action is not an aspect of the described methodology,\nand any known or later-developed technologies and systems may be used for this purpose.\n One of ordinary skill in the art will further appreciate that the technique herein automates the time-consuming and often difficult research and investigation process that has heretofore been the province of the security analyst.  The approach\nretrieves knowledge about the IOCs using a knowledge graph preferably extracted from public and/or private structured and unstructured data sources, and then extends that knowledge even further, thereby greatly reducing the time necessary for the analyst\nto determine cause and effect.\n The approach herein is designed to be implemented in an automated manner within or in association with a security system, such as a SIEM.\n The knowledge graph may be a component of the system, or such a graph may be used by the system.\n The functionality described above may be implemented as a standalone approach, e.g., a software-based function executed by a processor, or it may be available as a managed service (including as a web service via a SOAP/XML interface).  The\nparticular hardware and software implementation details described herein are merely for illustrative purposes are not meant to limit the scope of the described subject matter.\n More generally, computing devices within the context of the disclosed subject matter are each a data processing system (such as shown in FIG. 2) comprising hardware and software, and these entities communicate with one another over a network,\nsuch as the Internet, an intranet, an extranet, a private network, or any other communications medium or link.  The applications on the data processing system provide native support for Web and other known services and protocols including, without\nlimitation, support for HTTP, FTP, SMTP, SOAP, XML, WSDL, UDDI, and WSFL, among others.  Information regarding SOAP, WSDL, UDDI and WSFL is available from the World Wide Web Consortium (W3C), which is responsible for developing and maintaining these\nstandards; further information regarding HTTP, FTP, SMTP and XML is available from Internet Engineering Task Force (IETF).  Familiarity with these known standards and protocols is presumed.\n The scheme described herein may be implemented in or in conjunction with various server-side architectures including simple n-tier architectures, web portals, federated systems, and the like.  The techniques herein may be practiced in a\nloosely-coupled server (including a \"cloud\"-based) environment.\n Still more generally, the subject matter described herein can take the form of an entirely hardware embodiment, an entirely software embodiment or an embodiment containing both hardware and software elements.  In a preferred embodiment, the\nfunction is implemented in software, which includes but is not limited to firmware, resident software, microcode, and the like.  Furthermore, as noted above, the identity context-based access control functionality can take the form of a computer program\nproduct accessible from a computer-usable or computer-readable medium providing program code for use by or in connection with a computer or any instruction execution system.  For the purposes of this description, a computer-usable or computer readable\nmedium can be any apparatus that can contain or store the program for use by or in connection with the instruction execution system, apparatus, or device.  The medium can be an electronic, magnetic, optical, electromagnetic, infrared, or a semiconductor\nsystem (or apparatus or device).  Examples of a computer-readable medium include a semiconductor or solid state memory, magnetic tape, a removable computer diskette, a random access memory (RAM), a read-only memory (ROM), a rigid magnetic disk and an\noptical disk.  Current examples of optical disks include compact disk-read only memory (CD-ROM), compact disk-read/write (CD-R/W) and DVD.  The computer-readable medium is a tangible item.\n The computer program product may be a product having program instructions (or program code) to implement one or more of the described functions.  Those instructions or code may be stored in a computer readable storage medium in a data processing\nsystem after being downloaded over a network from a remote data processing system.  Or, those instructions or code may be stored in a computer readable storage medium in a server data processing system and adapted to be downloaded over a network to a\nremote data processing system for use in a computer readable storage medium within the remote system.\n In a representative embodiment, the graph generation techniques are implemented in a special purpose computer, preferably in software executed by one or more processors.  The software is maintained in one or more data stores or memories\nassociated with the one or more processors, and the software may be implemented as one or more computer programs.  Collectively, this special-purpose hardware and software comprises the functionality described above.\n Further, any authentication or authorization functionality required herein may be implemented as an adjunct or extension to an existing access manager or policy management solution.\n While the above describes a particular order of operations performed by certain embodiments of the invention, it should be understood that such order is exemplary, as alternative embodiments may perform the operations in a different order,\ncombine certain operations, overlap certain operations, or the like.  References in the specification to a given embodiment indicate that the embodiment described may include a particular feature, structure, or characteristic, but every embodiment may\nnot necessarily include the particular feature, structure, or characteristic.\n Finally, while given components of the system have been described separately, one of ordinary skill will appreciate that some of the functions may be combined or shared in given instructions, program sequences, code portions, and the like.\n The techniques herein provide for improvements to another technology or technical field, namely, security incident and event management (SIEM) systems, as well as improvements to automation-based knowledge graph-based analytics.\n A refined offense context graph as described herein may be rendered for visual display to the SOC analyst to facilitate the follow-on analysis.", "application_number": "15236582", "abstract": " An automated method for processing security events begins upon receipt of\n     information representing an offense. Based in part on context data\n     extracted from the offense, an offense context graph is built. The\n     offense context graph comprises nodes and edges, with an edge therein\n     representing a relationship between a pair of nodes, at least one of the\n     nodes being a root node representing an entity associated with the\n     offense. The method then continues by mining information about other\n     events that are determined to share a local contextual relationship with\n     the offense represented by the offense context graph. This operation\n     generates an enriched offense context graph. The enriched offense context\n     graph is then pruned to identify an offense context for further\n     examination. Pruning may involve applying a metric to events associated\n     with the offense and removing nodes that, based on evaluation of the\n     metric, do not contribute to the offense.\n", "citations": ["6038538", "6275238", "7003779", "7653059", "8150783", "8943154", "9166997", "9202052", "9210185", "9276948", "20020022952", "20050108049", "20070192859", "20070209074", "20070226796", "20090030756", "20100325412", "20130097183", "20130097463", "20140074764", "20140136707", "20140172919", "20140173742", "20140337306", "20150222730", "20150229662", "20150379409", "20160065610", "20160205122", "20160335348", "20160357872", "20170039500", "20170063912", "20170075749", "20170134415", "20170213127", "20170280351", "20180034685", "20180205793"], "related": []}, {"id": "20180067923", "patent_code": "10366163", "patent_name": "Knowledge-guided structural attention processing", "year": "2019", "inventor_and_country_data": " Inventors: \nChen; Yun-Nung (Bellevue, WA), Hakkani-Tur; Dilek Z. (Los Altos, CA), Tur; Gokhan (Redmond, WA), Celikyilmaz; Asli (Kirkland, WA), Gao; Jianfeng (Woodinville, WA), Deng; Li (Redmond, WA)  ", "description": "CROSS-REFERENCE TO RELATED APPLICATION\n This application is related to co-pending and commonly assigned U.S.  patent application Ser.  No. 15/229,039, filed Jun.  23, 2016, entitled \"End-to-End Memory Networks for Contextual Language Understanding,\" which is incorporated by reference.\nBACKGROUND\n Tagging of word and sentence sequences with semantic classes is crucial for natural language processing.  Tagging is the identification of meaning and semantics of words and sequences in a \"turn\", spoken language that is processed as a discrete\ninstance.  Recently, recurrent neural networks (RNNs) with long short-term memory (LSTM) cell structure demonstrated strong results on sequence tagging tasks in natural language processing due to their ability of preserving sequential information from\nmultiple turns of spoken language over time.  Generally, however, these RNNs assign tags to sequences considering only their flat structures, i.e., a linear chain of words/phrases.  However, natural language exhibits inherent syntactic properties that\nprovide rich, structured tree or tree-like information, which should help computer systems with for better understanding of natural language sentences or phrases, spoken or textual.\nSUMMARY\n The following Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description.  The Summary is not intended to identify key features or essential features of the claimed\nsubject matter, nor is it intended to be used to limit the scope of the claimed subject matter.\n According to aspects of the disclosed subject matter, systems and methods for determining knowledge-guided information for a recurrent neural networks (RNN) to guide the RNN in semantic tagging of an input phrase are presented.  A knowledge\nencoding module of a Knowledge-Guided Structural Attention Process (K-SAP) receives an input phrase and, in conjunction with additional sub-components or cooperative components generates a knowledge-guided vector that is provided with the input phrase to\nthe RNN for linguistic semantic tagging.  Generating the knowledge-guided vector comprises at least parsing the input phrase and generating a corresponding hierarchical linguistic structure comprising one or more discrete sub-structures.  The\nsub-structures may be encoded into vectors along with attention weighting identifying those sub-structures that have greater importance in determining the semantic meaning of the input phrase.\n According to aspects of the disclosed subject matter, a computer-implemented method for providing structural linguistic knowledge to a semantic tagging process is presented.  The method includes receiving an input phrase to be semantically\ntagged.  A hierarchical structure of the input phrase is generated, where the hierarchical structure comprises one or more discrete sub-structures.  For each of the discrete sub-structures, an attention weight is determined according to the linguistic\nimportance of the sub-structure, and the determined attention weight is associated to or with each discrete sub-structure.  A weighted sum vector is generated according to the content of each discrete sub-structure and according to the corresponding\nattention weight of the discrete sub-structures.  The weighted sum vector is combined with an input vector based on the subject matter of the input phrase, thereby generating a knowledge-guided vector corresponding to the input phrase.  The\nknowledge-guided vector and the input phrase are then to a recurrent neural network (RNN) for semantic tagging.\n According to additional aspects of the disclosed subject matter, a computer system for providing structural linguistic knowledge to a semantic tagging process is presented.  The computer system includes a processor and a memory, where the\nprocessor executes instructions in the memory as part of or in conjunction with additional components in providing structural linguistic knowledge to a semantic tagging process.  The additional components include a knowledge encoding module that receives\nan input phrase, determines a knowledge-guided vector for the input phrase, and provides the knowledge-guided vector with the input phrase to a recurrent neural network (RNN) for semantic tagging of the input phrase. BRIEF DESCRIPTION OF THE\nDRAWINGS\n The foregoing aspects and many of the attendant advantages of the disclosed subject matter will become more readily appreciated as they are better understood by reference to the following description when taken in conjunction with the following\ndrawings, wherein:\n FIG. 1 is a block diagram illustrating an exemplary executable components of a knowledge-guided structural attention processing (K-SAP) framework configured according to aspects of the disclosed subject matter;\n FIG. 2 is a block diagram illustrating the processing of an input phrase, \"Show me the flights from Seattle to San Francisco,\" by a components/modules of the K-SAP framework;\n FIG. 3 is a flow diagram illustrating an exemplary routine for generating knowledge-guided information, in the form of a knowledge-guided vector, for use in an RNN tagging process;\n FIG. 4 is a block diagram illustrating an exemplary computer readable medium encoded with instructions to generate metadata in regard to an image, respond to an image request with the image and metadata, and/or request an image from an online\nimage service; and\n FIG. 5 is a block diagram illustrating an exemplary computing device configured to generate and provide a knowledge-guided vector with the input phrase to guide the tagging process of an RNN tagging module/service.\nDETAILED DESCRIPTION\n For purposes of clarity and definition, the term \"exemplary,\" as used in this document, should be interpreted as serving as an illustration or example of something, and it should not be interpreted as an ideal or a leading illustration of that\nthing.  Stylistically, when a word or term is followed by \"(s)\", the meaning should be interpreted as indicating the singular or the plural form of the word or term, depending on whether there is one instance of the term/item or whether there is one or\nmultiple instances of the term/item.  For example, the term \"user(s)\" should be interpreted as one or more users.\n As indicated above and by way of definition, the term \"turn\" should be interpreted as being a discrete spoken sentence or utterance, as an instance of spoken dialog to be processed by spoken language understanding (SLU) component.  By way of\nillustration, dialogue or conversations typically involve multiple turns.  Additionally, the term \"semantic parsing\" should be interpreted as the parsing of input, e.g., one or more turns or a query, to identify parts and/or characteristics of the input. Semantic parsing can include identifying a domain and an intent of the input, and assigning words of the input to slots, though other terminologies may also be included in semantic parsing, including domains, dialogues, constraint slots, and requested\nslots, combination of action and requested slots, and equivalents thereof.  The term \"domain\" represents a category identified for an input from semantic parsing, and the term \"slot\" represents a constraint for a query or argument for an API for semantic\nparsing.\n Recurrent neural networks (RNNs), such as those described in co-pending and commonly assigned U.S.  patent application Ser.  No. 15/229,039, entitled \"End-to-End Memory Networks for Contextual Language Understanding,\" can concisely capture\ninformation/knowledge from processing turns due to the feedback connections from one time step to another, making them suitable for modeling sequential data with temporal dependencies.  Indeed, they have demonstrated success in processing and sequencing\ntagging tasks with regard to language modeling, particularly spoken language understanding (SLU).  Unfortunately, RNNs consider the models of the various turns as flat structures--i.e., without hierarchical/structural semantic relationships and\ninformation, thus ignoring structured information that is typically present in natural language sequence.  Indeed, hierarchical structures and semantic relationships contain linguistic characteristics of input word sequences that form sentences.  This\ninformation (the hierarchical structures and/or semantic relationships) would be valuable in tagging sequences, especially when tagging previously un-encountered sequences.  In contrast and according to aspects of the disclosed subject matter,\nknowledge-guided structural attention processing (K-SAP) incorporates non-flat network topologies guided by prior knowledge and end-to-end knowledge-guided structural learning, and provides for generalization for different knowledge bases and efficacy of\nprocessing.  In particular, in regard to incorporating non-flat network topologies guided by prior knowledge, the disclosed subject matter identifies important substructures captured from small training data, and allows the model to generalize\ninformation to previously unseen test data.  Also the model automatically figures out the important substructures that are essential to predict the tags of the sequence of words and/or sentences, so that the understanding performance can be improved. \nRegarding the generalization for different knowledge bases, there is no required schema of knowledge, and different types of parsing results, such as dependency relations, knowledge graph-specific relations, and parsing output of hand-crafted grammars,\ncan serve as the knowledge guidance in this model.\n In regard to a language understanding (LU) task and according to aspects of the disclosed subject matter, for any given utterance with a sequence of words/token s=w.sub.1, .  . . , w.sub.T, the K-SAP model predicts corresponding semantic tags\ny=y.sub.1, .  . . , y.sub.T for each word/token by incorporating knowledge-guided structures.  Turning, then, to FIGS. 1 and 2, FIG. 1 is a block diagram illustrating an exemplary executable components of a knowledge-guided structural attention\nprocessing (K-SAP) framework 100 configured according to aspects of the disclosed subject matter.  FIG. 2 is a block diagram illustrating the processing 200 of an input phrase, \"Show me the flights from Seattle to San Francisco,\" by a knowledge encoding\nmodule 102 of the K-SAP framework.\n As shown in FIG. 1, the executable knowledge encoding module 102 includes various executable sub-components that cooperatively generate a knowledge-guided vector 116 that corresponds to the input phrase 101.  This knowledge-guided vector 116 is\nthen used by the RNN tagger 120 in tagging the various words and tokens of the input phrase.  These sub-components include, by way of illustration and not limitation, a parsing module 104, a vector encoding module 106, an attention weighting module 108,\nand a vector combination module 110.\n The parsing module 104, in execution, generates a linguistic, hierarchical structure of an input phrase.  For example and with regard to the input phrase 101 (\"Show me the flights from Seattle to Son Francisco\"), the parsing module generates a\nhierarchical structure 200 comprising a discrete set of knowledge-guided substructures 232-238.  According to aspects of the disclosed subject matter, the knowledge encoding module 102/parsing module 104 may rely upon external and local knowledge sources\nto identify and parse the input phrase into the hierarchical arrangement.  In regard to the hierarchical structure, each sub-structure begins with the root node 202 (\"Show\") and follows a path down the structure to a leaf node, such as leaf nodes 204 and\n208, including intermediate nodes, such as intermediate node 206 (\"flights\"), that are passed through.\n Regarding the parsing module 104, the input phrase is parsed to identify paths from the root (the general purpose of the phrase) to the various leaves (individual words/terms.) The number of substructures that are identified may be less than the\nnumber of words in a given input phrase, such as input phrase 101, because non-leaf nodes do not have corresponding substructures to reduce the duplicated information in the model.\n After generating the hierarchical structure 200, the discrete knowledge-guided substructures are encoded into corresponding vector representations by way of a vector encoding module 106, resulting in a generated set of vectors for the input\nphrase.  For example, each of the various substructures 232-238 of the hierarchical structure 200 are encoded by the encoding module 106 into a continuous space vector, including encoded vectors 210 and 212.  According to aspects of the disclosed subject\nmatter, the vector encoding module 106 may utilize multiple models for encoding, alone or in combination.  These encoding models include fully-connected neural networks (NNs) with linear activation, recurrent neural networks (RNNs), and/or convolutional\nneural networks (CNNs).\n According to aspects of the disclosed subject matter, the K-SAP framework learns a phrase/sentence meaning/representation according to the various substructures, as represented by the various generated vectors.  Further, in learning the meaning\nof a particular phrase according to the various substructures of the phrase, emphasis or weight is given (i.e., greater attention) to certain substructures over others by an attention weighting module 108.  This greater attention is directed to those\nsubstructures that are viewed as more important to the sentence/phase in determining its meaning and encoded into the corresponding vectors.  By way of illustration, attention levels are represented by the amount of shading in the encoded vectors\n202-208, with the darker vectors indicating those substructures that should be afforded greater attention.\n According to aspects of the disclosed subject matter, the levels of attention afforded to individual substructures (as captured in the corresponding vectors) are determined according to a variety of resources, including external resources. \nThese resources may include information sources that indicate dependency relations in phrases, knowledge bases, as well as data regarding the person from whom the phrase originates.  This prior knowledge provides richer information to help the RNN\ndetermine semantic tags to an input utterance, such as input phrase 101.\n After weighting the various encoded vectors of the various sub-structures, a vector combination module 110 combines the encoded vectors into a weighted sum vector 112, the weighted sum vector representing the knowledge and attention weights to\nbe applied to the input phrase.  Indeed, the weighted sum vector 112 represents the attention to be paid to different sub-structures guided by external knowledge.\n In addition to encoding the various sub-structures of the input phrase and generated the weighted sum vector 112, and according to aspects of the disclosed subject matter, the entire input phrase 101 is also encoded as an input vector 114 and\ncombined with the weighed sum vector 112 to generate a knowledge-guided vector 116 to be used by an RNN tagger in tagging the words and tokens of the input phrase.\n In regard to the weights associated with the various sub-structures of the input phrase, in one embodiment a matching score is determined between the encoded vectors of a substructure and the entire utterance (as represented by the input vector\n114).  This matching score may be determined according to the inner product of the two vectors followed by a softmax function (also referred to as a normalized exponential function), where the result can be viewed as the attention distribution for\nmodeling input sub-structures from external knowledge in order to understand the current utterance.  As will be readily appreciated, a softmax function is a gradient-log-normalizer of the categorical probability distribution.  Indeed, the softmax\nfunction is often implemented at the final layer of a network used for classification.  In various embodiments, a softmax function maps a vector and a specific index i to a real value.\n Turning to FIG. 3, FIG. 3 is a flow diagram illustrating an exemplary routine 300 for generating knowledge-guided information, in the form of a knowledge-guided vector 116, for use in an RNN tagging process.  Beginning at block 302, an input\nphrase to be tagged by an RNN tagging process is received.  At block 304, a parsing module 104 of a knowledge encoding module 102 (or operating in cooperation with the knowledge encoding module) parses the input phrase into a hierarchical arrangement of\nsub-structures, as illustrated in FIG. 2.  As indicated above, the hierarchical arrangement comprises one or more sub-structures.\n At block 306, the subject matter of the various discrete sub-structures of the hierarchical arrangement are encoded into corresponding encoded vectors.  At block 308, a determination as to the importance, and therefore the weighting, of each of\nthe sub-structures is determined.  At block 310, each of the encoded vectors is updated to reflect the determined importance of the corresponding sub-structure.\n At block 312, the weighted, encoded vectors are then combined into a weighted sum vector 112, as described above.  At block 314, the subject matter of the entire input vector is encoded as an input vector 114 and, at block 316, the input vector\n114 and the weighted sum vector 112 are combined to generate a knowledge-guided vector 116.\n The knowledge-guided vector 116 is then provided, with the input phrase 101, to a suitably configured recurrent neural network (RNN) for tagging of the input phrase, where in the RNN suitably configured to also accept the knowledge-guided vector\n116 with the input phrase to guide the tagging process.  Thereafter, the routine 300 terminates.\n Regarding routine 300 described above, as well as other processes describe herein, while the routines/processes are expressed in regard to discrete steps, these steps should be viewed as being logical in nature and may or may not correspond to\nany specific actual and/or discrete steps of a given implementation.  Also, the order in which these steps are presented in the various routines and processes, unless otherwise indicated, should not be construed as the only order in which the steps may\nbe carried out.  Moreover, in some instances, some of these steps may be combined and/or omitted.  Those skilled in the art will recognize that the logical presentation of steps is sufficiently instructive to carry out aspects of the claimed subject\nmatter irrespective of any particular development or coding language in which the logical instructions/steps are encoded.\n Of course, while the routines and/or processed include various novel features of the disclosed subject matter, other steps (not listed) may also be carried out in the execution of the subject matter set forth in these routines.  Those skilled in\nthe art will appreciate that the logical steps of these routines may be combined together or be comprised of multiple steps.  Steps of the above-described routines may be carried out in parallel or in series.  Often, but not exclusively, the\nfunctionality of the various routines is embodied in software (e.g., applications, system services, libraries, and the like) that is executed on one or more processors of computing devices, such as the computing device described in regard FIG. 6 below. \nAdditionally, in various embodiments all or some of the various routines may also be embodied in executable hardware modules including, but not limited to, system on chips (SoC's), codecs, specially designed processors and or logic circuits, and the like\non a computer system.\n As suggested above, these routines and/or processes are typically embodied within executable code modules comprising routines, functions, looping structures, selectors and switches such as if-then and if-then-else statements, assignments,\narithmetic computations, and the like that, in execution, configure a computing device to operate in accordance with the routines/processes.  However, as suggested above, the exact implementation in executable statement of each of the routines is based\non various implementation configurations and decisions, including programming languages, compilers, target processors, operating environments, and the linking or binding operation.  Those skilled in the art will readily appreciate that the logical steps\nidentified in these routines may be implemented in any number of ways and, thus, the logical descriptions set forth above are sufficiently enabling to achieve similar results.\n While many novel aspects of the disclosed subject matter are expressed in routines embodied within applications (also referred to as computer programs), apps (small, generally single or narrow purposed applications), and/or methods, these\naspects may also be embodied as computer-executable instructions stored by computer-readable media, also referred to as computer-readable storage media, which are articles of manufacture.  As those skilled in the art will recognize, computer-readable\nmedia can host, store and/or reproduce computer-executable instructions and data for later retrieval and/or execution.  When the computer-executable instructions that are hosted or stored on the computer-readable storage devices are executed by a\nprocessor of a computing device, the execution thereof causes, configures and/or adapts the executing computing device to carry out various steps, methods and/or functionality, including those steps, methods, and routines described above in regard to the\nvarious illustrated routines and/or processes.  Examples of computer-readable media include, but are not limited to: optical storage media such as Blu-ray discs, digital video discs (DVDs), compact discs (CDs), optical disc cartridges, and the like;\nmagnetic storage media including hard disk drives, floppy disks, magnetic tape, and the like; memory storage devices such as random access memory (RAM), read-only memory (ROM), memory cards, thumb drives, and the like; cloud storage (i.e., an online\nstorage service); and the like.  While computer-readable media may reproduce and/or cause to deliver the computer-executable instructions and data to a computing device for execution by one or more processors via various transmission means and mediums,\nincluding carrier waves and/or propagated signals, for purposes of this disclosure computer readable media expressly excludes carrier waves and/or propagated signals.\n Turning to FIG. 4, FIG. 4 is a block diagram illustrating an exemplary computer readable medium encoded with instructions to generate metadata in regard to an image, respond to an image request with the image and metadata, and/or request an\nimage from an online image service as described above.  More particularly, the implementation 400 comprises a computer-readable medium 408 (e.g., a CD-R, DVD-R or a platter of a hard disk drive), on which is encoded computer-readable data 406.  This\ncomputer-readable data 406 in turn comprises a set of computer instructions 404 configured to operate according to one or more of the principles set forth herein.  In one such embodiment 402, the processor-executable instructions 404 may be configured to\nperform a method, such as at least some of the exemplary method 300, for example.  In another such embodiment, the processor-executable instructions 404 may be configured to implement a system, such as at least some of the exemplary system 500, as\ndescribed below.  Many such computer-readable media may be devised, by those of ordinary skill in the art, which are configured to operate in accordance with the techniques presented herein.\n Turning now to FIG. 5, FIG. 5 is a block diagram illustrating an exemplary computing device 500 configured to generate and provide a knowledge-guided vector 116 with the input phrase to guide the tagging process of an RNN tagging module/service. The exemplary computing device 500 includes one or more processors (or processing units), such as processor 502, and a memory 504.  The processor 502 and memory 504, as well as other components, are interconnected by way of a system bus 510.  The memory\n504 typically (but not always) comprises both volatile memory 506 and non-volatile memory 508.  Volatile memory 506 retains or stores information so long as the memory is supplied with power.  In contrast, non-volatile memory 508 is capable of storing\n(or persisting) information even when a power supply is not available.  Generally speaking, RAM and CPU cache memory are examples of volatile memory 506 whereas ROM, solid-state memory devices, memory storage devices, and/or memory cards are examples of\nnon-volatile memory 508.\n As will be appreciated by those skilled in the art, the processor 502 executes instructions retrieved from the memory 504 (and/or from computer-readable media, such as computer-readable media 400 of FIG. 4) in carrying out various functions of\nproviding a knowledge-guided vector 116 with the input phrase to guide the tagging process of an RNN tagging module/service as set forth above.  The processor 502 may be comprised of any of a number of available processors such as single-processor,\nmulti-processor, single-core units, and multi-core units.\n Further still, the illustrated computing device 500 includes a network communication component 512 for interconnecting this computing device with other devices and/or services over a computer network, including other user devices.  The network\ncommunication component 512, sometimes referred to as a network interface card or NIC, communicates over a network using one or more communication protocols via a physical/tangible (e.g., wired, optical, etc.) connection, a wireless connection, or both. \nAs will be readily appreciated by those skilled in the art, a network communication component, such as network communication component 512, is typically comprised of hardware and/or firmware components (and may also include or comprise executable\nsoftware components) that transmit and receive digital and/or analog signals over a transmission medium (i.e., the network.)\n The illustrated computing device 500 also includes a knowledge encoding module 102.  As set forth above, the knowledge encoding module 102, in conjunction with the various sub-component or cooperative components, generates a knowledge-guided\nvector 116 that is used by an RNN tagger 120 in tagging the various words and tokens of an input phrase, such as input phrase 101.  These additional and/or cooperative components includes a parsing module 104, a vector encoding module 106, an attention\nweighting module 108, and a vector combination module 110.\n As indicated above, the parsing module 104, in execution, causes the computing device to generate a linguistic, hierarchical structure of an input phrase, such as input phrase 101.  More particularly, the parsing module 104 generates a\nhierarchical structure, such as hierarchical structure 200 of input phrase 101, comprising a discrete set of knowledge-guided substructures.\n As set forth above, the vector encoding module 106 encodes the discrete knowledge-guided sub-structures generated by the parsing module 104 into corresponding vector representations.  The attention weighting module 108 determines and assigns\nemphasis or weight to the various encoded vectors generated by the encoding module 106.  As indicated above, the attention weighting module 108 assigns the emphasis/weight according to linguistic knowledge information, both local and/or external, such as\nmay be found in linguistic knowledge base 522.\n The vector combination module 110 combines the various encoded, weighted vectors into a single weighted sum vector 112.  Also, the vector combination module 110 is configured, in execution, to combine an input vector (an encoded vector of the\nsubject matter of the entire input phrase) with the weighted sum vector 112 to generate a knowledge-guided vector 116 that can be used by the suitably configured RNN tagging component 120 in tagging the input phrase according to the encoded knowledge in\nthe knowledge-guided vector.\n Regarding the various components of the exemplary computing device 500, those skilled in the art will appreciate that many of these components may be implemented as executable software modules stored in the memory of the computing device, as\nexecutable hardware modules and/or components (including SoCs--system on a chip), or a combination of the two.  Indeed, components may be implemented according to various executable embodiments including executable software modules that carry out one or\nmore logical elements of the processes described in this document, or as a hardware and/or firmware components that include executable logic to carry out the one or more logical elements of the processes described in this document.  Examples of these\nexecutable hardware components include, by way of illustration and not limitation, ROM (read-only memory) devices, programmable logic array (PLA) devices, PROM (programmable read-only memory) devices, EPROM (erasable PROM) devices, and the like, each of\nwhich may be encoded with instructions and/or logic which, in execution, carry out the functions described herein.\n Moreover, in certain embodiments each of the various components of the exemplary computing device 500 may be implemented as an independent, cooperative process or device, operating in conjunction with or on one or more computer systems and or\ncomputing devices.  It should be further appreciated, of course, that the various components described above should be viewed as logical components for carrying out the various described functions.  As those skilled in the art will readily appreciate,\nlogical components and/or subsystems may or may not correspond directly, in a one-to-one manner, to actual, discrete components.  In an actual embodiment, the various components of each computing device may be combined together or distributed across\nmultiple actual components and/or implemented as cooperative processes on a computer network as in known in the art.\n While various novel aspects of the disclosed subject matter have been described, it should be appreciated that these aspects are exemplary and should not be construed as limiting.  Variations and alterations to the various aspects may be made\nwithout departing from the scope of the disclosed subject matter.", "application_number": "15258639", "abstract": " Systems and methods for determining knowledge-guided information for a\n     recurrent neural networks (RNN) to guide the RNN in semantic tagging of\n     an input phrase are presented. A knowledge encoding module of a\n     Knowledge-Guided Structural Attention Process (K-SAP) receives an input\n     phrase and, in conjunction with additional sub-components or cooperative\n     components generates a knowledge-guided vector that is provided with the\n     input phrase to the RNN for linguistic semantic tagging. Generating the\n     knowledge-guided vector comprises at least parsing the input phrase and\n     generating a corresponding hierarchical linguistic structure comprising\n     one or more discrete sub-structures. The sub-structures may be encoded\n     into vectors along with attention weighting identifying those\n     sub-structures that have greater importance in determining the semantic\n     meaning of the input phrase.\n", "citations": ["5832429", "6615178", "9239828", "9836671", "20040167778", "20040186730", "20050055209", "20050105712", "20060074634", "20060129396", "20060253273", "20070010900", "20070094006", "20070225977", "20070260450", "20090030686", "20090063473", "20100030552", "20110054883", "20110087483", "20120197631", "20120239378", "20130268260", "20130325436", "20140113263", "20140236577", "20140278355", "20150066496", "20150161101", "20150178265", "20150199333", "20150269949", "20150332049", "20150347521", "20160124943", "20160350655", "20170270919", "20170372200"], "related": []}, {"id": "20180082183", "patent_code": "10303999", "patent_name": "Machine learning-based relationship association and related discovery and\n     search engines", "year": "2019", "inventor_and_country_data": " Inventors: \nHertz; Shai (Tel Aviv, IL), Olof-Ors; Mans (Lucerne, CH), Weinreb; Enav (Petah Tikva, IL), Hazai; Oren (Tel Aviv, IL), Horrell; Geoff (London, GB), Lindman; Yael (Shoham, IL), Mataraso; Yehonatan (New Hyde Park, NY), Nivarthi; Phani (West New York, NJ)  ", "description": "TECHNICAL FIELD\n The invention relates generally to natural language processing, information extraction, information retrieval and text mining and more particularly to entity associations and to systems and techniques for identifying and measuring entity\nrelationships and associations.  The invention also relates to discovery and search interfaces to enhance linked data used in generating results for delivery in response to user input.\nBACKGROUND\n With computer-implemented word processing and mass data storage, the amount of information generated by mankind has risen dramatically and with an ever-quickening pace.  As a result, there is a continuing and growing need to collect and store,\nidentify, track, classify and catalogue, and link for retrieval and distribution this growing sea of information.\n Much of the world's information or data is in the form of text, the majority of which is unstructured (without metadata or in that the substance of the content is not asymmetrical and unpredictable, i.e., prose, rather than formatted in\npredictable data tables).  Much of this textual data is available in digital form [either originally created in this form or somehow converted to digital--by means of OCR (optical character recognition), for example] and is stored and available via the\nInternet or other networks.  Unstructured text is difficult to effectively handle in large volumes even when using state of the art processing capabilities.  Content is outstripping the processing power needed to effectively manage and assimilate\ninformation from a variety of sources for refinement and delivery to users.  Although advances have made it possible to investigate, retrieve, extract and categorize information contained in vast repositories of documents, files, or other text\n\"containers,\" systems are needed to more efficiently manage and classify the ever-growing volume of data generated daily and to more effectively deliver such information to consumers.\n This proliferation of text-based information in electronic form has resulted in a growing need for tools that facilitate organization of the information and allow users to query systems for desired information.  One such tool is information\nextraction software that, typically, analyzes electronic documents written in a natural language and populates a database with information extracted from such documents.  Applied against a given textual document, the process of information extraction\n(IE) is used to identify entities of predefined types appearing within the text and then to list them (e.g., people, companies, geographical locations, currencies, units of time, etc.).  IE may also be applied to extract other words or terms or strings\nof words or phrases.\n Knowledge workers, such as scientists, lawyers, traders or accountants, have to deal with a greater than ever amount of data with an increased level of variety.  Their information needs are often focused on entities and their relations, rather\nthan on documents.  To satisfy these needs, information providers must pull information from wherever it happens to be stored and bring it together in a summary result.  As a concrete example, suppose a user is interested in companies with the highest\noperating profit in 2015 currently involved in Intellectual Property (IP) lawsuits.  In order to answer this query, one needs to extract company entities from free text documents, such as financial reports and court documents, and then integrate the\ninformation extracted from different documents about the same company together.\n Content and enhanced experience providers, such as Thomson Reuters Corporation, identify, collect, analyze and process key data for use in generating content, such as news articles and reports, financial reports, scientific reports and studies,\nlaw related reports, articles, etc., for consumption by professionals and others.  The delivery of such content and services may be tailored to meet the particular interests of certain professions or industries, e.g., wealth managers and advisors, fund\nmanagers, financial planners, investors, scientists, lawyers, etc. Professional services companies, like Thomson Reuters, continually develop products and services for use by subscribers, clients and other customers and with such developments distinguish\ntheir products and services over those offered by their competition.\n Companies, such as Thomson Reuters--with many businesses involved in delivery of content and research tools to aid a wide variety of research and professional service providers--generate, collect and store a vast spectrum of documents, including\nnews, from all over the world.  These companies provide users with electronic access to a system of databases and research tools.  Professional services providers also provide enhanced services through various techniques to augment content of documents\nand to streamline searching and more efficiently deliver content of interest to users.  For example, Thomson Reuters structures documents by tagging them with metadata for use in internal processes and for delivery to users.\n \"Term\" refers to single words or strings of highly-related or linked words or noun phrases.  \"Term extraction\" (also term recognition or term mining) is a type of IE process used to identify or find and extract relevant terms from a given\ndocument, and therefore have some relevance, to the content of the document.  Such activities are often referred to as \"Named Entity Extraction\" and \"Named Entity Recognition\" and \"Named Entity Mining\" and in connection with additional processes, e.g.,\nCalais \"Named Entity Tagging\" (or more generally special noun phrase tagger) and the like.  There are differences in how these activities are performed.  For example, term recognition might only require setting a flag when a certain expression is\nidentified in a text span, while term extraction would be identifying it and its boundaries and writing it out for storage in, for example, a database, noting exactly where in the text it came from.  Techniques employed in term extraction may include\nlinguistic or grammar-based techniques, natural language or pattern recognition, tagging or structuring, data visualizing and predictive formulae.  For example, all names of companies mentioned in the text of a document can be identified, extracted and\nlisted.  Similarly, events (e.g., Exxon-Valdez oil spill or BP Horizon explosion), sub-events related to events (e.g., cleanup effort associated with Exxon Valdez oil spill or BP Horizon explosion), names of people, products, countries, organizations,\ngeographic locations, etc., are additional examples of \"event\" or \"entity\" type terms that are identified and may be included in a list or in database records.  This IE process may be referred to as \"event or entity extraction\" or \"event or entity\nrecognition.\" As implemented, known IE systems may operate in terms of \"entity\" recognition and extraction wherein \"events\" are considered a type of entity and are treated as an entity along with individuals, companies, industries, governmental entities,\netc.\n There are a variety of methods available for automatic event or entity extraction, including linguistic or semantic processors to identify, based on known terms or applied syntax, likely noun phrases.  Filtering may be applied to discern true\nevents or entities from unlikely events or entities.  The output of the IE process is a list of events or entities of each type and may include pointers to all occurrences or locations of each event and/or entity in the text from which the terms were\nextracted.  The IE process may or may not rank the events/entities, process to determine which events/entities are more \"central\" or \"relevant\" to the text or document, compare terms against a collection of documents or \"corpus\" to further determine\nrelevancy of the term to the document.\n Systems and methods for identifying risks, entities, relationships, supply chains, and for generating visualizations related to risks, entities, relationships, and supply chains are described in at least: SYSTEMS, METHODS, AND SOFTWARE FOR\nENTITY EXTRACTION AND RESOLUTION COUPLED WITH EVENT AND RELATIONSHIP EX FRACTION, U.S.  patent application Ser.  No. 12/341,926, filed Dec.  22, 2008, Light et al.; SYSTEMS, METHODS, SOFTWARE AND INTERFACES FOR ENTITY EXTRACTION AND RESOLUTION AND\nTAGGING, U.S.  patent application Ser.  No. 12/806,116, filed Aug.  5, 2010, issued as U.S.  Pat.  No. 9,501,467, on Nov.  11, 2016, Light et al.; FINANCIAL EVENT AND RELATIONSHIP EXTRACTION, U.S.  patent application Ser.  No. 12/363,524, filed Jan.  30,\n2009, Schilder et al.; SYSTEMS, METHODS, AND SOFTWARE FOR ENTITY RELATIONSHIP RESOLUTION, U.S.  patent application Ser.  No. 12/341,913, filed Dec.  22, 2008, issued as U.S.  Pat.  No. 9,600,509, on Mar.  1, 2017, Conrad et al.; METHODS AND SYSTEMS FOR\nMANAGING SUPPLY CHAIN PROCESSES AND INTELLIGENCE, U.S.  patent application Ser.  No. 13/594,864, filed Aug.  26, 2012, Siig et al.; METHODS AND SYSTEMS FOR GENERATING SUPPLY CHAIN REPRESENTATIONS, U.S.  patent application Ser.  No. 13/795,022, filed Mar. 12, 2013, Leidner et al.; and RISK IDENTIFICATION AND RISK REGISTER GENERATION SYSTEM AND ENGINE, U.S.  patent application Ser.  No. 15/181,194, filed Jun.  13, 2016, Leidner et al.; each and all of which are incorporated herein by reference in their\nentirety.\n Thomson Reuters' Text Metadata Services group (\"TMS\") formerly known as ClearForest prior to acquisition in 2007, is one exemplary IE-based solution provider offering text analytics software used to \"tag,\" or categorize, unstructured information\nand to extract facts about people, organizations, places or other details from news articles, Web pages and other documents.  TMS's Calais is a web service that includes the ability to extract entities such as company, person or industry terms along with\nsome basic facts and events.  OpenCalais is an available community tool to foster development around the Calais web service.  APIs (Application Programming Interfaces) are provided around an open rule development platform to foster development of\nextraction modules.  Other providers include Autonomy Corp., Nstein and Inxight.  Examples of Information Extraction software in addition to OpenCalais include: AlchemyAPI; CRF++; LingPipe; TermExtractor; TermFinder; and TextRunner.  IE may be a separate\nprocess or a component or part of a larger process or application, such as business intelligence software.\n Currently, the dominant technology for providing nontechnical users with access to Linked Data is keyword-based search.  This is problematic because keywords are often inadequate as a means for expressing user intent.  In addition, while a\nstructured query language can provide convenient access to the information needed by advanced analytics, unstructured keyword-based search cannot meet this extremely common need.  This makes it harder than necessary for non-technical users to generate\nanalytics.\n What is needed is a natural language-based system that utilizes the benefits of structured query language capabilities to allow non-technical users to create well-formed questions.\n Today, investment decisions in the financial markets require careful analysis of information available from multiple sources.  To meet this challenge, financial institutions typical maintain very large datasets that provide a foundation for this\nanalysis.  For example, forecasting stock market, currency exchange rate, bank bankruptcies, understanding and managing financial risk, trading futures, credit rating, loan management, bank customer profiling, and money laundering analyses all require\nlarge datasets of information for analysis.  The datasets of information can be structured datasets as well as unstructured data sets.\n Typically, the datasets of information are used to model one or more different entities, each of which may have a relationship with other entities.  For example, a company entity may be impacted by, and thereby have a relationship with, any of\nthe following entities: a commodity or natural resource (e.g., aluminum, corn, crude oil, sugar, etc.), a source of the commodity or natural resource, a currency (e.g., euro, sterling, yen, etc.), and one or more competitor, supplier or customer.  Any\nchange in one entity can have an impact on another entity.  For example, rising crude oil prices can impact a transportation company's revenues, which can affect the company's valuation.  In another example, an acquisition of a supplier by a competitor\nputs an entity's supply chain at risk, as would political upheaval or natural disaster (e.g., tsunami, earthquake) affecting availability or operations of a supplier.\n Given the quantity and nature of these datasets, each modeled entity tends to have multiple relationships with a large number of other entities.  As such, it is difficult to identify which entities are more significant than others for a given\nentity.\n Accordingly, there is a need for systems and techniques to automatically analyze all available supply chain related data to identify relationships and assign significance scores to entity relationships.\n Event detection and relation extraction is an active field of academic research.  State of the art systems employ statistical machine learning models to identify and classify relations between entities mentioned in natural language texts. \nRecently, deep learning-based systems have been shown to achieve similar quality, requiring less feature engineering.  Knowledge base building systems make use of known machine learning models to create or augment knowledge graphs, depicting relations\nbetween entities.\n What is needed is, a system configured to be applied to the identification of supply chain relationship between companies.  Supply chain identification is still based on manual work and on extracting relations from structured data (financial\nreports, piers records etc.).\n Supplier--Customer relations are very valuable to investors, among other interested classes of users, but are oftentimes hard to detect.  Some information is available in structured data, but many more indications are available only in\nunstructured data, such as news stories, company SEC filings, blogs and company and other web sites.  A lot of highly informative data is publicly available, but is too voluminous and unfeasible for manual processing to systematically identify supply\nchain relations.\n Accordingly, what is needed is an automated system capable of processing the large volumes of available data to detect indications for supply chain relationship between companies and aggregate these indications across data sources to generate a\nsingle confidence score for the relation between such companies.\nSUMMARY\n Over the past few decades the amount of electronic data has grown to massive levels and the desire to search, manipulate, assimilate and otherwise make full use of such data has grown in kind.  Such growth will only increase over the foreseeable\nfuture with sources of data growing rapidly.  Not all data is in the same format or language and some data is structured (including metadata, i.e., data concerning or about the document, subjects of the document, source of data, field descriptors,\nsignature data, etc.) and some data is unstructured, e.g., free text.  Given data reaching an unprecedented amount, coming from diverse sources, and covering a variety of domains in heterogeneous formats, information providers are faced with the critical\nchallenge to process, retrieve and present information to their users to satisfy their complex information needs.  In one manner of implementation, the present invention is used in a family of services for building and querying an enterprise knowledge\ngraph in order to address this challenge.  We first acquire data from various sources via different approaches.  Furthermore, we mine useful information from the data by adopting a variety of techniques, including Named Entity Recognition (NER) and\nRelation Extraction (RE); such mined information is further integrated with existing structured data (e.g., via Entity Linking (EL) techniques) to obtain relatively comprehensive descriptions of the entities.  Modeling the data as an RDF graph model\nenables easy data management and embedding of rich semantics in processed data.  Finally, to facilitate the querying of this mined and integrated data, i.e., the knowledge graph, the invention is described with a natural language interface, e.g., Thomson\nReuters Discover, that allows users to ask questions of the knowledge graph in their own words; these natural language questions are translated into executable queries for answer retrieval.\n The present invention provides a system configured to automatically and systematically access numerous data sources and process large volumes of natural unstructured texts to identify supply chain relations between companies.  In addition to\nNatural Language Processing (NLP) features, as typically used in academic relation extraction works, the present invention includes processes adapted to consider additional information, such as from available knowledge graphs, to enhance accuracy and\nefficiency.  Knowledge graphs are known and offered by several companies with some being public facing and others private or proprietary or available as part of a fee-based service.  A knowledge graph comprises semantic-search information from a variety\nof sources, including public and private sources, and often is used as part of a search engine/platform.  A knowledge graph is dynamic in that it is updated, preferably in real time, upon entity/member profile changes and upon identifying and adding new\nentities/members.\n For example, Thomson Reuters includes as part of its service offerings a Knowledge Graph facility that may be used by the present invention in connection with delivery of services, such as via Thomson Reuters Eikon platform.  In this manner, the\npresent invention may be used in a system to build supply chain graphs to feed Eikon value chain offering by using proprietary, authority information, e.g., industries and past information about supply chain between a set of companies (either from\nevidence previously discovered by the system or from manually curated data), to reliably compute a confidence score.  The invention may be used to extract supplier-customer relations from news stories, newsroom sources, blogs, company web sites, and\ncompany SEC filings, building a knowledge graph and exposing it via Eikon.  The invention is used in a system preferably capable of being scaled to handle additional/different document sources and aggregate multiple evidences to one confidence score.  A\nsearch engine may be used as a vehicle to allow users to enter company names of interest and to yield a set of supply chain related relationship data of interest to the user.  Other companies that have knowledge graph facilities include Google, Microsoft\nBing Satori, Yahoo!, Baidu, LinkedIn, Yandex Object Answer, and others.\n Systems and techniques for determining significance between entities are disclosed.  The systems and techniques identify a first entity having a relationship or an association with a second entity, apply a plurality of relationship or\nassociation criteria to the relationship/association, weight each of the criteria based on defined weight values, and compute a significance score for the first entity with respect to the second entity based on a sum of a plurality of weighted criteria\nvalues.  The system identifies text representing or signifying a connection between two or more entities and in particular in the context of a supply chain environment.  As used herein the terms \"association\" and \"relationship\" include their respective\nordinary meanings and as used include the meaning of one within the other.  The systems and techniques, including deep learning and machine learning processes, utilize information, including unstructured text data from disparate sources, to create one or\nmore uniquely powerful informational representations including in the form of signals, feed, knowledge graphs, supply chain graphical interfaces and more.  The systems and techniques disclosed can be used to identify and quantify the significance of\nrelationships (e.g., associations) among various entities including, but not limited to, organizations, people, products, industries, geographies, commodities, financial indicators, economic indicators, events, topics, subject codes, unique identifiers,\nsocial tags, industry terms, general terms, metadata elements, classification codes, and combinations thereof.\n The present invention provides a method and system to automatically identify supply chain relationships between companies and/or entities, based on, among other things, unstructured text corpora.  The system combines Machine Learning and/or deep\nlearning models to identify sentences mentioning or referencing or representing a supply chain connection between two companies (evidence).  The present invention also applies an aggregation layer to take into account the evidence found and assign a\nconfidence score to the relationship between companies.  This supply chain relationship information and aggregation data may be used to build and present one or more supply chain graphical representations and/or knowledge graphs.\n The invention may use specific Machine Learning features and make use of existing supply chain knowledge and other information in generating and presenting knowledge graphs, e.g., in connection with an enterprise content platform such as Thomson\nReuters Eikon.  The invention identifies customer-supplier relations, which feeds the Eikon value chain module and allows Eikon users to investigate relations which might affect companies of interest and generate a measure of performance on a\nrisk-adjusted basis \"Alpha.\" The invention may also be used in connection with other technical risk ratios or metrics, including beta, standard deviation, R-squared, and the Sharpe ratio.  In this manner, the invention may be used, particularly in the\nsupply chain/distribution risk environment, to provide or enhance statistical measurements used in modern portfolio theory to help investors determine a risk-return profile.\n The present invention provides, in one exemplary manner of operation, a Supply Chain Analytics & Risk \"SCAR\" (aka \"Value Chains\") engine or application adapted to exploit vast amounts of structured and unstructured data across news, research,\nfilings, transcripts, industry classifications, and economics.  The Machine Learning and aggregating features of the present invention may be used to fine-tune existing text analytics technologies (e.g., Thomson Reuters Eikon and DataScope data and\nanalytics platforms) to develop an improved Supply Chain Analytics and Risk offering within such platforms.  The present invention utilizes supply chain data to deliver enhanced supply chain relationship feeds and tools to professionals for use in\nadvising clients and making decisions.  For example, the invention may be used to deliver information and tools to financial professionals looking for improved insights in their search for investment opportunities and returns, while better understanding\nrisk in their portfolios.  Supply chain data can create value for several different types of users and use cases.  In one example, the invention enables research analysts on both buy and sell sides to leverage supply chain data to gain insights into\nrevenue risks based on relationships and geographic revenue distribution.  Also, the invention provides portfolio managers with a new insightful view of risks and returns of their portfolio by providing \"supply chain\" driven views of their holdings.  In\naddition, the invention enables quant analysts and Hedge Funds to leverage supply chain data to build predictive analytics on performance of companies based on overall supply chain performance.  Traders can use information and tools delivered in\nconnection with the invention to, for example, track market movement of prices by looking at intra-supply arbitrage opportunities (e.g., effect of revenue trends from suppliers through distributors) and second-order impact of breaking news.\n In a first embodiment, the present invention provides a system for providing remote users over a communication network supply-chain relationship data via a centralized Knowledge Graph user interface, the system comprising: a Knowledge Graph data\nstore comprising a plurality of Knowledge Graphs, each Knowledge Graph related to an associated entity, and including a first Knowledge Graph associated with a first company and comprising supplier-customer data; an input adapted to receive electronic\ndocuments from a plurality of data sources via a communications network, the received electronic documents including unstructured text; a pre-processing interface adapted to perform one or more of named entity recognition, relation extraction, and entity\nlinking on the received electronic documents and generate a set of tagged data, and further adapted to parse the electronic documents into sentences and identify a set of sentences with each identified sentence having at least two identified companies as\nan entity-pair; a pattern matching module adapted to perform a pattern-matching set of rules to extract sentences from the set of sentences as supply chain evidence candidate sentences; a classifier adapted to utilize natural language processing on the\nsupply chain evidence candidate sentences and calculate a probability of a supply-chain relationship between an entity-pair associated with the supply chain evidence candidate sentences; and an aggregator adapted to aggregate at least some of the supply\nchain evidence candidates based on the calculated probability to arrive at an aggregate evidence score for a given entity-pair, wherein a Knowledge Graph associated with at least one company from the entity-pair is generated or updated based at least in\npart on the aggregate evidence score.\n The system of the first embodiment may also be characterized in one or more of the following ways.  The system may further comprise a user interface adapted to receive an input signal from a remote user-operated device, the input signal\nrepresenting a user query, wherein an output is generated for delivery to the remote user-operated device and related to a Knowledge Graph associated with a company in response to the user query.  The system may further comprise a query execution module\nadapted to translate the user query into an executable query set and execute the executable query set to generate a result set for presenting to the user via the remote user-operated device.  The system may further comprise a graph-based data model for\ndescribing entities and relationships as a set of triples comprising a subject, predicate and object and stored in a triple store.  The graph-based data model may be a Resource Description Framework (RDF) model.  The triples may be queried using SPARQL\nquery language.  The system may further comprise a fourth element added to the set of triples to result in a quad.  The system may further comprise a machine learning-based algorithm adapted to detect relationships between entities in an unstructured\ntext document.  The classifier may predict a probability of a relationship based on an extracted set of features from a sentence.  The extracted set of features may include context-based features comprising one or more of n-grams and patterns.  The\nsystem may further comprise wherein updating the Knowledge Graph is based on the aggregate evidence score satisfying a threshold value.  The pre-processing interface may further be adapted to compute significance between entities by: identifying a first\nentity and a second entity from a plurality of entities, the first entity having a first association with the second entity, and the second entity having a second association with the first entity; weighting a plurality of criteria values assigned to the\nfirst association, the plurality of criteria values based on a plurality of association criteria selected from the group consisting essentially of interestingness, recent interestingness, validation, shared neighbor, temporal significance, context\nconsistency, recent activity, current clusters, and surprise element; and computing a significance score for the first entity with respect to the second entity based on a sum of the plurality of weighted criteria values for the first association, the\nsignificance score indicating a level of significance of the second entity to the first entity.\n In a second embodiment, the present invention provides A method for providing remote users over a communication network supply-chain relationship data via a centralized Knowledge Graph user interface, the method comprising: storing at a\nKnowledge Graph data store a plurality of Knowledge Graphs, each Knowledge Graph related to an associated entity, and including a first Knowledge Graph associated with a first company and comprising supplier-customer data; receiving, by an input,\nelectronic documents from a plurality of data sources via a communications network, the received electronic documents including unstructured text; performing, by a pre-processing interface, one or more of named entity recognition, relation extraction,\nand entity linking on the received electronic documents and generate a set of tagged data, and further adapted to parse the electronic documents into sentences and identify a set of sentences with each identified sentence having at least two identified\ncompanies as an entity-pair; performing, by a pattern matching module, a pattern-matching set of rules to extract sentences from the set of sentences as supply chain evidence candidate sentences; utilizing, by a classifier, natural language processing on\nthe supply chain evidence candidate sentences and calculate a probability of a supply-chain relationship between an entity-pair associated with the supply chain evidence candidate sentences; and aggregating, by an aggregator, at least some of the supply\nchain evidence candidates based on the calculated probability to arrive at an aggregate evidence score for a given entity-pair, wherein a Knowledge Graph associated with at least one company from the entity-pair is generated or updated based at least in\npart on the aggregate evidence score.\n The method of the second embodiment may further comprise receiving, by a user interface, an input signal from a remote user-operated device, the input signal representing a user query, wherein an output is generated for delivery to the remote\nuser-operated device and related to a Knowledge Graph associated with a company in response to the user query; and translating, by a query execution module, the user query into an executable query set and execute the executable query set to generate a\nresult set for presenting to the user via the remote user-operated device.  The method may further comprise describing, by a graph-based data model, entities and relationships as a set of triples comprising a subject, predicate and object and stored in a\ntriple store.  The graph-based data model may be a Resource Description Framework (RDF) model.  The triples may be queried using SPARQL query language.  The method may further comprise a fourth element added to the set of triples to result in a quad. \nThe method may further comprise detecting, by a machine learning-based algorithm, relationships between entities in an unstructured text document.  The predicting, by the classifier, may further comprise a probability of a relationship is based on an\nextracted set of features from a sentence.  The extracted set of features may include context-based features comprising one or more of n-grams and patterns.  The updating the Knowledge Graph may be based on the aggregate evidence score satisfying a\nthreshold value.  The method may further comprise: identifying, by the pre-processing interface, a first entity and a second entity from a plurality of entities, the first entity having a first association with the second entity, and the second entity\nhaving a second association with the first entity; weighting, by the pre-processing interface, a plurality of criteria values assigned to the first association, the plurality of criteria values based on a plurality of association criteria selected from\nthe group consisting essentially of interestingness, recent interestingness, validation, shared neighbor, temporal significance, context consistency, recent activity, current clusters, and surprise element; and computing, by the pre-processing interface,\na significance score for the first entity with respect to the second entity based on a sum of the plurality of weighted criteria values for the first association, the significance score indicating a level of significance of the second entity to the first\nentity.\n In a third embodiment, the present invention provides a system for automatically identifying supply chain relationships between companies based on unstructured text and for generating Knowledge Graphs.  The system comprises: a Knowledge Graph\ndata store comprising a plurality of Knowledge Graphs, each Knowledge Graph related to an associated company, and including a first Knowledge Graph associated with a first company and comprising supplier-customer data; a machine-learning module adapted\nto identify sentences containing text data representing at least two companies, to determine a probability of a supply chain relationship between a first company and a second company, and to generate a value representing the probability; an aggregation\nmodule adapted to aggregate a set of values determined by the machine-learning module representing a supply chain relationship between the first company and the second company and further adapted to generate and aggregate evidence score representing a\ndegree of confidence in the existence of the supply chain relationship.\n Additional systems, methods, as well as articles that include a machine-readable medium storing machine-readable instructions for implementing the various techniques, are disclosed.  Details of various implementations are discussed in greater\ndetail below. BRIEF DESCRIPTION OF THE DRAWINGS\n FIG. 1 is a schematic of an exemplary computer-based system for computing connection significance between entities.\n FIG. 2 illustrates an exemplary method for determining connection significance between entities according to one embodiment of the invention.\n FIG. 3 is a schematic of an exemplary directed graph.\n FIG. 4 illustrates exemplary interestingness measures.\n FIG. 5 is an exemplary process flow according to the present invention.\n FIG. 6 is a is a schematic diagram representing in more detail an exemplary architecture according to the present invention\n FIG. 7 provides an overall architecture of an exemplary embodiment of the SCAR system according to the present invention.\n FIG. 8 is a flow diagram demonstrating an example of NER, entity linking, and relation extraction processes according to the present invention.\n FIG. 9 is an exemplary ontology snippet of an exemplary Knowledge Graph in connection with an operation of the present invention.\n FIGS. 10(a)-10(c) provide graphical user interface elements illustrating a question building process according to the present invention.\n FIG. 10(d) is an exemplary user interface providing a question built by the question building process and the answers retrieved by executing the question as a query according to the present invention.\n FIG. 11 is a Parse Tree for the First Order Logic (FOL) of the question \"Drugs developed by Merck\" according to the present invention.\n FIG. 12 is a flowchart illustrating a supply chain communication process according to the present invention.\n FIG. 13 is a flowchart illustrating a relationship finder process according to the present invention.\n FIG. 14 provides three graphs (a), (b), and (c) that show the runtime of natural language parsing according to the present invention.\n FIG. 15 is a flowchart illustrating a method for identifying supply chain relationships according to the present invention.\n FIG. 16 provides two graphs illustrating the runtime of named entity recognition and entity linking processes according to the present invention\n Like reference symbols in the various drawings indicate like elements.\nDETAILED DESCRIPTION\n Turning now to FIG. 1, an example of a suitable computing system 10 within which embodiments of the present invention may be implemented is disclosed.  The computing system 10 is only one example and is not intended to suggest any limitation as\nto the scope of use or functionality of the invention.  Neither should the computing system 10 be interpreted as having any dependency or requirement relating to any one or combination of illustrated components.\n For example, the present invention is operational with numerous other general purpose or special purpose computing consumer electronics, network PCs, minicomputers, mainframe computers, laptop computers, as well as distributed computing\nenvironments that include any of the above systems or devices, and the like.\n The invention may be described in the general context of computer-executable instructions, such as program modules, being executed by a computer.  Generally, program modules include routines, programs, objects, components, data structures, loop\ncode segments and constructs, etc. that perform particular tasks or implement particular abstract data types.  The invention can be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked\nthrough a communications network.  In a distributed computing environment, program modules are located in both local and remote computer storage media including memory storage devices.  Tasks performed by the programs and modules are described below and\nwith the aid of figures.  Those skilled in the art can implement the description and figures as processor executable instructions, which can be written on any form of a computer readable media.\n In one embodiment, with reference to FIG. 1, the system 10 includes a server device 12 configured to include a processor 14, such as a central processing unit (`CPU`), random access memory (`RAM`) 16, one or more input-output devices 18, such as\na display device (not shown) and keyboard (not shown), and non-volatile memory 20, all of which are interconnected via a common bus 22 and controlled by the processor 14.\n As shown in the FIG. 1 example, in one embodiment, the non-volatile memory 20 is configured to include an identification module 24 for identifying entities from one or more sources.  The entities identified may include, but are not limited to,\norganizations, people, products, industries, geographies, commodities, financial indicators, economic indicators, events, topic codes, subject codes, unique identifiers, social tags, industry terms, general terms, metadata elements, and classification\ncodes.  An association module 26 is also provided for computing a significance score for an association between entities, the significance score being an indication of the level of significance a second entity to a first entity.\n In one embodiment, a context module 28 is provided for determining a context (e.g., a circumstance, background) in which an identified entity is typically referenced in or referred to, a cluster module 30 for clustering (e.g., categorizing)\nidentified entities, and a signal module 31 for generating and transmitting a signal associated with the computed significance score.  Additional details of these modules 24, 26, 28, 30 and 32 are discussed in connection with FIGS. 2, 3 and 4.\n In a further embodiment, Server 12 may include in non-volatile memory 20 a Supply Chain Analytics & Risk \"SCAR\" (aka \"Value Chains\") engine 23, as discussed in detail hereinbelow, in connection with determining supply chain relationships among\ncompanies and providing other enriching data for use by users.  SCAR 23 includes, in this example, a training/classifier module 25, Natural Language Interface/Knowledge Graph Interface Module 27 and Evidence Scoring Module 29 for generating and updating\nKnowledge Graphs associated with companies.  The training/classifier module 25 may be a machine-learning classifier configured to predict the probability of possible customer/supplier relationships between an identified company-pair.  The classifier may\nuse set(s) of patterns as filters and extract feature sets at a sentence-level, e.g., context-based features such as token-level n-grams and patterns.  Other features based on transformations and normalizations and/or information from existing Knowledge\nGraph data may be applied at the sentence-level.  Evidence Scoring Module 29 may be used to score the detected and identified supply-chain relationship candidate sentence/company pair and may include an aggregator, discussed in detail below, to arrive at\nan aggregate evidence score.  The SCAR 25 may then update the Knowledge Graph(s) associated with one or both of the companies of the subject company-pair.  In one exemplary manner of operation, the SCAR 23 may be accessed by one or more remote access\ndevice 43.  A user interface 44 operated by a user at access device 43 may be used for querying or otherwise interrogating the Knowledge Graph via Natural Language Interface/Knowledge Graph Interface Module 27 for responsive information, e.g., use of\nSPARQL query techniques.  Responsive data outputs may be generated at the Server 12 and returned to the remote access device 43 and presented and displayed to the associated user.  FIG. 7 illustrates several exemplary input/output scenarios.\n As shown in FIG. 1, in one embodiment, a network 32 is provided that can include various devices such as routers, server, and switching elements connected in an Intranet, Extranet or Internet configuration.  In one embodiment, the network 32\nuses wired communications to transfer information between an access device (not shown), the server device 12, and a data store 34.  In another embodiment, the network 32 employs wireless communication protocols to transfer information between the access\ndevice, the server device 12, and the data store 34.  In yet other embodiments, the network 32 employs a combination of wired and wireless technologies to transfer information between the access device, the server device 12, and the data store 34.\n The data store 34 is a repository that maintains and stores information utilized by the before-mentioned modules 24, 26, 28, 30 and 31.  In one embodiment, the data store 34 is a relational database.  In another embodiment, the data store 34 is\na directory server, such as a Lightweight Directory Access Protocol (`LDAP`).  In yet another embodiment, the data store 34 is an area of non-volatile memory 20 of the server 12.\n In one embodiment, as shown in the FIG. 1 example, in one embodiment, the data store 34 includes a set of documents 36 that are used to identify one or more entities.  As used herein, the words `set` and `sets` refer to anything from a null set\nto a multiple element set.  The set of documents 36 may include, but are not limited to, one or more papers, memos, treatises, news stories, articles, catalogs, organizational and legal documents, research, historical documents, policies and procedures,\nbusiness documents, and combinations thereof.  In another embodiment, the data store 34 includes a structured data store, such as a relational or hierarchical database, that is used to identify one or more entities.  In yet another embodiment, sets of\ndocuments and structured data stores are used to identify one or more entities.\n A set of association criteria 38 is provided that comprises contingency tables used by the association module 26 to compute a significance score for an identified relationship between entities.  In one embodiment, the contingency tables are\nassociated with a set of interestingness measures that are used by the association module 26 to compute the significance score.  An example of interestingness measures, along with each respective formulation, is shown in connection with FIG. 4.\n The data store 34 also includes a set of entity pairs 40.  Each pair included in the set of entity pairs 40 represents a known relationship existing between at least two identified entities.  In one embodiment, the relationship is identified by\nan expert upon reviewing one of the set of documents 36.  In another embodiment, the relationship is identified from the one or more set of documents 36 using a computer algorithm included in the context module 28.  For example, upon reviewing a news\nstory, an expert and/or the context module 28 may identify the presence of two entities occurring in the same news story,\n As shown in FIG. 1, in one embodiment, a set of context pairs 42 are also provided.  Each of the set of context pairs 42 represents a context that exists between at least two entities.  For example, whenever a particular topic or item is\ndiscussed in a news story, the two entities also are mentioned in the same news story.  Similar to the set of entity pairs 40 discussed previously, the set of context pairs may also be identified by an expert, or a computer algorithm included in the\ncontext module 28.  Additional details concerning information included in the data store 34 are discussed in greater detail below.\n In the further embodiment of Server 12 having SCAR 23, data store 34 also includes Knowledge Graph store 37, Supply Chain Relationship Pattern store 39 and Supply Chain Company Pair store 41.  Documents store 36 receives document data from a\nvariety of sources and types of sources including unstructured data that may be enhanced and enriched by SCAR 23.  For example, data sources 35 may include documents from one or more of Customer data, Data feeds, web pages, images, PDF files, etc., and\nmay involve optical character recognitions, data feed consumption, web page extraction, and even manual data entry or curation.  SCAR 23 may then pre-process the raw data from data sources including, e.g., application of OneCalais or other Named Entity\nRecognition (NER), Relation Extraction (ER), or Entity Linking (EL), processes.  These processes are described in detail below.\n Although the data store 34 shown in FIG. 1 is connected to the network 32, it will be appreciated by one skilled in the art that the data store 34 and/or any of the information shown therein, can be distributed across various servers and be\naccessible to the server 12 over the network 32, be coupled directly to the server 12, or be configured in an area of non-volatile memory 20 of the server 12.\n Further, it should be noted that the system 10 shown in FIG. 1 is only one embodiment of the disclosure.  Other system embodiments of the disclosure may include additional structures that are not shown, such as secondary storage and additional\ncomputational devices.  In addition, various other embodiments of the disclosure include fewer structures than those shown in FIG. 1.  For example, in one embodiment, the disclosure is implemented on a single computing device in a non-networked\nstandalone configuration.  Data input and requests are communicated to the computing device via an input device, such as a keyboard and/or mouse.  Data output, such as the computed significance score, of the system is communicated from the computing\ndevice to a display device, such as a computer monitor.\n Turning now to FIG. 2, an example method for determining connection significance between entities is disclosed.  As shown in the FIG. 2 example, at step 44, the identification module 24 first generates a directed graph to represent entities\nidentified in each of the set of documents 36.  In one embodiment, the identification module 24 determines a frequency and co-occurrence of each entity in each of the set of documents 36, and then generates a contingency table to record and determine\nassociations.  The set of documents may be structured documents, including but not limited to eXtensible Markup Language (XML) files, as well as unstructured documents including, but not limited to articles and news stories.  As described previously, the\npresent invention is not limited to only using a set of documents to identify entities.  For example, the present invention may use structured data stores including, but not limited to, relational and hierarchical databases, either alone or in\ncombination with the set of documents to identify entities.\n Further, it will be appreciated that the present invention is not limited to a directed graph implementation, and that other computer-implemented data structures capable of modeling entity relationships may be used with the present invention,\nsuch as a mixed graph and multi graph.\n A schematic of an exemplary directed graph generated by the identification module 24 is shown in connection with FIG. 3.  Each node 60, 62, 64, 66, 68, 70 and 72 of the graph represents an entity identified from one or more of the set of\ndocuments, and vertices (e.g., edges) of each node represent an association (e.g., relationship) between entities.  For example, as shown in the FIG. 3 example, Entity A 60 has a first association 60A with Entity B 62 indicating a level of significance\nof Entity B 62 to Entity A 60, and a second association 60B with Entity B 62 indicating a level of significance of Entity A 60 to Entity B 62.\n Referring back to FIG. 2, at step 46, the identification module 24 next identifies a first entity and at least one second entity from the directed graph.  In one embodiment, the first entity is included in a user request and the second entity is\ndetermined by the identification module 24 using a depth-first search of the generated graph.  In another embodiment, the identification module 24 uses the depth-first search on each node (e.g., first entity) of the graph to determine at least one other\nnode (e.g., second entity).\n Next, at step 48, once the first entity and second entity are identified, the association module 26 applies a plurality of association criteria 38 to one of the associations between the first entity and the second entity.  The plurality of\nassociation criteria 38 include, but are not limited to, the following set of criteria: interestingness, recent interestingness, validation, shared neighbor, temporal significance, context consistency, recent activity, current clusters, and surprise\nelement.  Once the association criteria are applied, the association module 28 assigns criteria values to each of the association criteria.\n For example, in one embodiment, the association module 26 may apply the interestingness criteria to the first association.  Interestingness criteria are known to one skilled in the art and as a general concept, may emphasize conciseness,\ncoverage, reliability, peculiarity, diversity, novelty, surprisingness, utility, and actionability of patterns (e.g., relationships) detected among entities in data sets.  In one embodiment, the interestingness criteria is applied by the association\nmodule 26 to all associations identified from the set of documents 36 and may include, but is not limited to, one of the following interestingness measures: correlation coefficient, Goodman-Kruskal's lambda (.lamda.), Odds ratio (.alpha.), Yule's Q,\nYule's Y, Kappa (.kappa.), Mutual Information (M), J-Measure (J), Gini-index (G), Support (s), Confidence (c), Laplace (L), Conviction (V), Interest (I), cosine (IS), Piatetsky-shaporo's (PS), Certainty factor (F), Added Value (AV), Collective Strength\n(S), Jaccard Index, and Klosgen (K).  Once the interestingness criteria is applied to the first association, the association module 26 assigns a value to the interestingness criteria based on the interestingness measure.\n A list of example interestingness measures with accompanied formulas used by the association module 26 is shown is shown in connection with FIG. 4.  As shown in the FIG. 4 example, one of the interestingness measures includes a correlation\ncoefficient (.PHI.-coefficient) that measures the degree of linear interdependency between a pair of entities, represented by A and B in FIG. 4, respectively.  The correlation coefficient is defined by the covariance between two entities divided by their\nstandard deviations.  The correlation coefficient equals zero (0) when entity A and entity B are independent and may range from minus one (-1) to positive one (+1).\n In one embodiment, the association module 26 applies the recent interestingness criteria to the first association.  The recent interestingness criteria may be applied by the association module 26 to associations identified from a portion of the\nset of documents 36 and/or a portion of a structured data store.  The portion may be associated with a configurable pre-determined time interval.  For example, the association module 26 may apply the recent interestingness criteria to only associations\nbetween entities determined from documents not older than six (6) months ago.  Similar to the before-mentioned interestingness criteria, the recent interestingness criteria may include, but is not limited to, one of the following interestingness\nmeasures: correlation coefficient, Goodman-Kruskal's lambda (.lamda.), Odds ratio (.alpha.), Yule's Q, Yule's Y, Kappa (.kappa.), Mutual Information (M), J-Measure (J), Gini-index (G), Support (s), Confidence (c), Laplace (L), Conviction (V), Interest\n(I), cosine (IS), Piatetsky-shaporo's (PS), Certainty factor (F), Added Value (AV), Collective Strength (S), Jaccard Index, and Klosgen (K).  Once the recent interestingness criteria is applied to the first association, the association module 26 assigns\na value to the recent interestingness criteria based on the interestingness measure.\n The association module 26 may apply the validation criteria to the first association.  In one embodiment, the association module 26 determines whether the first entity and the second entity co-exist as an entity pair in the set of entity pairs\n40.  As described previously, each of the entity pairs defined in the set of entity pairs 40 may be previously identified as having a relationship with one another.  Based on the determination, the association module 26 assigns a value to the validation\ncriteria indicating whether or not the first entity and the second entity exist as pair entities in the set of entity pairs 40.\n The association module 26 may apply the shared neighbor criteria to the first association.  In one embodiment, the association module 26 determines a subset of entities having edges extending a pre-determined distance from the first entity and\nthe second entity.  The subset of entities represents an intersection of nodes neighboring the first and second entity.  The association module 26 then computes an association value based at least in part on a number of entities included in the subset of\nentities, and assigns a value to the shared neighbor criteria based on the computed association value.\n For example, referring to FIG. 3 and assuming a pre-determined distance (e.g., a hop) of one (I) between entities in the graph, the shared entities (e.g., neighboring entities) between Entity A 60 and Entity B 62 are Entity C 64 and Entity D,\nresulting in a computed association value of two (2) which is assigned to the shared neighbor criteria.  As shown in the FIG. 3 example, Entity E 68 and Entity F 70 are more than the pre-determined distance from Entity A 60, and Entity G 72 is more than\nthe predetermined distance from Entity B 62.\n Referring back to FIG. 2, at step 48, the association module 26 may apply the temporal significance criteria to the first association.  In one embodiment, the association module 26 applies interestingness criteria to the first association as\ndetermined by a first portion of the set of documents and/or a first portion of a structured data store.  The first portion is associated with a first time interval.  The association module 26 then applies interestingness criteria to the first\nassociation as determined by a second portion of the set of documents and/or a second portion of the structured data store.  The second portion associated with a second time interval different from the first time interval.  The interestingness criteria\nmay include, but is not limited to, one of the following interestingness measures: correlation coefficient, Goodman-Kruskal's lambda (i), Odds ratio (a), Yule's Q, Yule's Y, Kappa (K), Mutual Information (M), i-Measure (J), Gini-index (G), Support (s),\nConfidence (c), Laplace (L), Conviction (V), Interest (I), cosine (IS), Piatetsky-shaporo's (PS), Certainty factor (F), Added Value (AV), Collective Strength (S), Jaccard index, and Klosgen (K).\n Once the temporal significance criteria is applied, the association module 26 determines a difference value between a first interestingness measure associated with the first time interval and a second interestingness measure associated with the\nsecond time interval.  The association module 26 then assigns a value to the temporal significance criteria based on the determined difference value.\n The association module 26 may apply the context consistency criteria to the first association.  In one embodiment, the association module 26 determines a frequency of the first entity and the second entity occurring in a context of each document\nof the set of documents 36.  The context may include, but is not limited to, organizations, people, products, industries, geographies, commodities, financial indicators, economic indicators, events, topics, subject codes, unique identifiers, social tags,\nindustry terms, general terms, metadata elements, classification codes, and combinations thereof.  The association module 26 then assigns a value to the context consistency criteria based on the determined frequency.\n The association module 26 also may apply the recent activity criteria to the first association.  For example, in one embodiment, the association module 26 computes an average of occurrences of the first entity and the second entity occurring in\none of the set of documents 36 and/or the structured data store.  The association module 26 then compares the computed average of occurrences to an overall occurrence average associated with other entities in a same geography or business.  One the\ncomparison is completed, the association module 26 assigns a value to the recent activity criteria based on the comparison.  In various embodiments, the computed average of occurrences and/or the overall occurrence average are seasonally adjusted.\n The association module 26 may also apply the current clusters criteria to the first association.  In one embodiment, identified entities are clustered together using the clustering module 30.  The clustering module 30 may implement any\nclustering algorithm known in the art.  Once entities are clustered, the association module 26 determines a number of clusters that include the first entity and the second entity.  The association module 26 then compares the determined number of clusters\nto an average number of clusters that include entity pairs from the set of context pairs 42 and which do not include the first entity and the second entity as one of the entity pairs.  In one embodiment, the defined context is an industry or geography\nthat is applicable to both the first entity and the second entity.  The association module 26 then assigns a value to the current cluster criteria based on the comparison.\n The association module 26 may also apply the surprise element criteria to the first association.  In one embodiment, the association module 26 compares a context in which the first entity and the second entity occur in a prior time interval\nassociated with a portion of the set of documents and/or a portion of the structured data store, to a context in which the first entity and the second entity occur in a subsequent time interval associated with a different portion of the set of documents\nand/or the structured data store.  The association module 26 then assigns a value to the surprise element criteria based on the comparison.\n Referring to FIG. 2, once the plurality of criteria are applied to the first association, at step 50, the association module 26 weights each of the plurality of criteria values assigned to the first association.  In one embodiment, the\nassociation module 26 multiplies a user-configurable value associated with each of the plurality of criteria with each of the plurality of criteria values, and then sums the plurality of multiplied criteria values to compute a significance score.  As\ndiscussed previously, the significance score indicates a level of significance of the second entity to the first entity.  In another embodiment, the association module 26 multiplies a pre-defined system value associated with each of the plurality of\ncriteria, and then sums the plurality of multiplied criteria values to compute the significance score.\n Once the significance score is computed, at step 54, the signal module 32 generates a signal including the computed significance score.  Lastly, at step 56, the signal module 32 transmits the generated signal.  In one embodiment, the signal\nmodule 32 transmits the generated signal in response to a received request.\n A further invention aspect provides a SCAR comprising at the core an automated (machine learning based) relation extraction system that automatically identifies pairs of companies that are related in a supplier-customer relationship and also\nidentifies the supplier and the customer in the pair.  The system then feeds this information to the Thomson Reuters knowledge graph.  Currently, the system extracts these pairs from two sources of text data, namely:\n 1) News\n 2) SEC Filings\n FIG. 5 illustrates an exemplary process flow 500 of the present invention characterized by 1) value/supply chains: supplier-customer relationship 502; 2) machine learning-based system 504; 3) classification 506--identify a pair of companies or\nsets of companies in a sentence and identify direction, e.g., A supplying B or B supplying A. More specifically, the process may include as Step 1: 1) Named Entity Recognition, e.g., applying TR OneCalais Engine 508 to extract company names--Denso Corp\nand Honda 510, 2) break textual information from a document or source into discrete sentences, 3) mark only those sentences that have at least two companies; 4) anaphora resolution like `we`, `the company`, etc. For example, **Apple** announced its 3rd\nquarter results yesterday--excluded; **Toyota Corp** is an important Client of **GoodYear Inc**--included.\n The SCAR process may further include as Step 2--Patterns identification (High recall low precision), which may include: 1) use patterns to extract sentences that are potentials for identifying value chains; 2) `supply`, `has sold`,\n`customers(\\s+) include`, `client`, `provided`, etc.; 3) removes lot of noise; and 4) retain only those sentences that have two companies and at least one pattern matched.  Examples of treatment of three identified sentences: 1) Prior to **Apple**, he\nserved as Vice President, Client Experience at **Yahoo**--included; 2) **Toyota Corp** is an important Client of **GoodYear Inc**--included; 3) **Microsoft** share in the smartphone market is significantly less than **Google**--excluded.\n The SCAR process may further include as Step 3--Run a Classifier to identify value chains and may include: 1) train a classifier that classifies each sentence; 2) prefer higher precision over recall; and 3) classifier: Logistic Regression. \nExamples of this operation follow: 1) Prior to **Apple**, he served as Vice President, Client Experience at **Yahoo**: 0.005; and 2) **Toyota Corp** is an important Client of **GoodYear Inc**: 0.981.  The machine learning (ML)-based classifier may\ninvolve use of positive and negative labeled documents for training purposes.  Training may involve nearest neighbor type analysis based on computed similarity of terms or words determined as features to determine positiveness or negativeness.  Inclusion\nor exclusion may be based on threshold values.  A training set of documents and/or feature sets may be used as a basis for filtering or identifying supply-chain candidate documents and/or sentences.  Training may result in models or patterns to apply to\nan existing or supplemented set(s) of documents.\n The SCAR process may further include as Step 4--Aggregate all evidences on a Company Pair.  Examples of evidences are: 1) **Toyota Corp** is an important Client of **GoodYear Inc**: 0.981; 2) **Goodyear** sold 50M cargo to **Toyota** in 2015:\n0.902; and 3) **Toyota** mentioned that it agreed to buy tyres from **GoodYear Inc**: 0.947.  The aggregate of the evidence is represented as: GoodYear (Supplier)--Toyota (Customer) -&gt;0.99 (aggregated score).\n As used herein Evidence at the Sentence Level refers to the quality of the classification model that classifies a pair of companies at a sentence level.  At a Company Pair Level, for each company pair, all the sentences/evidences above a\nthreshold are chosen and a model calculates an aggregated score for the pair.\n Given a text, the system performs Named Entity Recognition on it using Thomson Reuters OneCalais to identify and extract all company mentions.  It then identifies and/or breaks the text to sentences.  For each sentence that contains a pair of\ncompanies, a \"company-pair,\" (also called evidence text), the system at its core uses a machine learning classifier that predicts the probability of a possible relationship for the given pair of companies in the context of this sentence.  The system then\naggregates all the evidences for each pair of relationship and creates a final probability score of a relationship between the two companies, which in turn is fed to Thomson Reuters knowledge graph to be used for various applications.  The system is able\nto build a graph of all companies with their customers and suppliers extracted from these text data sources.\n FIG. 6 is a schematic diagram representing in more detail an exemplary architecture 600 for use in implementing the invention.\n Named Entity Recognition/Extraction (Companies)--\n The first step by named entity recognition 602 of the system is to identify/extract companies appearing in the text.  This requires running Entity extraction to tag all the companies mentioned in the source text (news or filings document).  The\nsystem, in this exemplary embodiment, uses Thomson Reuters (TR) OneCalais to tag all the companies mentioned.  At the end of this step, all the companies are identified and, in this example, also resolved to a TR PermId (in this context, a unique company\nidentifier).  Using the PermId, we can later use additional metadata about the company, from TR's organization authority and knowledge bases (e.g. industry, public/private).\n Anaphora Resolution for Companies--\n The sentence splitter and anaphora resolver 604 is the next component in the process and system.  In many sentences in the source text a supplier customer relationship information can exist without the text containing the name of the company but\nan anaphora like `We`, `The Company`, `Our`, and so on.  For e.g. in the following snippets: \"In May 2012, we entered into an agreement with Company-A to supply leather products;\" and \"John D, The Chairman of Company-A said that, `Our deal to supply\nleather products to Company-B boosted our growth\\`.\" The system identifies such cases (`we`) and performs an additional layer of company extraction to mark these kinds of anaphoras and resolve them to a company.  Anaphoras contribute to a huge number of\ninstances of evidence sentences having supplier-customer relationships.  Anaphoras are included only if they can be bound to a company, e.g., in cases of filing documents, such unmapped anaphoric instances are resolved to the `Filing Company`.\n Positive and Negative Patterns List Creation and Matching--\n At this stage by pattern matcher 608, the source document text is broken down into a set of sentences and the system now processes each sentence to identify relations.  As a part of the first step at this stage, any sentence that has only one\ncompany marked (resolved anaphora included), gets filtered out and is not processed.  For example: Company-A announced its 3rd quarter results yesterday --Excluded (less than two companies in sentence); Company-A is an important Client of\nCompany-B--Included (at least two companies in sentence).\n To reduce the noise that is being tagged by the classifier, we generated a list of `interesting` patterns (using manual and semi-automatic methods) that have some potential for identifying supplier-customer relations.  For example patterns like\n\"sold\", \"supplied\", \"customers included\", \"client\", \"implemented\", \"use\", etc. were created that helps filter out vast number of noisy sentences but at the same time Includes any sentence that have the potential to be interesting and thus creating an\nhigh recall-low precision bucket of sentences.  The basic idea is to only include sentences that have: a) At least two company ies mentioned in the sentence, and b) Some pattern or text that can be of interest.  If there is no such pattern of text, then\nthese sentences are noisy and can be filtered out, for example: prior to **Company-A**, he served as Manager, Client Experience at **Company-B**--Included (pattern--\"client\"); **Company-A** is an important Client of **Company-B**--Included\n(pattern--\"client\"); and **Company-A** share in the electronic market is significantly less than **Company-B**--Excluded (no pattern).\n The patterns may be created by analyzing examples of supplier-customer pairs, and analyzing all sentences that contained known related company pairs.  These patterns may be generated and extended to suit many different industries.  For example,\nautomobile industry relied heavily on the pattern \"supply\" while technology sector uses different patterns like \"used\", \"implemented\" to suggest relations.  Accordingly, there may be industry-specific patterns used in calculating evidence scores for\ncompany pairs known to be involved in a certain industry.  A set of negative patterns was also curated, whose presence filtered out the sentences.  Some such patterns included \"stock purchase agreement\", \"acquired\", \"merged\", etc. The presence of these\npatterns generally led to sentences that did not have supplier-customer relations.\n Sentence Pre-Processing--\n Each sentence is pre-processed and transformed at the sentence splitter 604 and at sentence/evidence classifier 610.  As a part of pre-processing, the system also checks for multiple companies in a given sentence acting like a list of companies\nand creates instances with each pair.  As a part of pre-processing, the companies in a list are purged and masked to one.  More transformations are also applied on the sentence like shortening a sentence, which removes un-necessary parts of a sentence\nwhile keeping the parts with the most information.\n Sentence/Evidence Level Classifier--\n Also at sentence/evidence classifier 610, given a sentence (that contains at least two companies and a potential pattern), a machine learning classifier is trained which classifies whether the two companies in that sentence context have a\nsupplier-customer relation (including identifying which company is supplying and which company is customer).  For example: \"**Company-A** is an important Client of **Company-B**.\"--A supplies B; \"**Company-A** was supplied 50 barrels of oil by\n**Company-B**.\"--B supplies A; \"**Company-A** supplied to **Company-B** stock options worth $10M.\"--neither.\n Model: The classifier used was a Logistic Regression classifier.  A model is trained per source.  So, news documents are run by the news model classifier and filing documents are classified by a filings model classifier.  This is because the\nstructure and type of sentences vary a lot from source to source.  The sentences in news documents are simpler and have a different vocabulary as compared to SEC filings documents, which can have much longer complex sentences and a different use of\nvocabulary.\n Features:\n Features include context-based positional words, specific pattern-based features, sentence level features including the presence of indicator terms, the original extraction patterns that led to the inclusion of the sentence, distance between the\ntwo companies in the sentence, presence of other companies in the sentence and so on.  Broadly each feature could be divided into a) Direction based feature b) Non-Direction based featured.\n Direction Based Features--\n In order to classify a sentence and also identify the direction, each sentence is duplicated and one is marked as AtoB and the other is marked as B2A.  The features extracted for that sentence are then marked with the respective AtoB or BtoA\ndirections.  The model is now able to learn a set of disjoint features for \"A supplies B\" and \"B supplies A\" cases.  For example if fi is a positional word feature occurring say 1 word before company-B in the sentence, then there would be two features\nfiAtoB or fiBtoA.  Let us take example of a sentence: \"**Company-A** was supplied 50 barrels of oil by **Company-B**.\" For this example, we have a feature which is the word \"by\" appearing one word before Company-B, and let us represent it as fby_B-1. \nWith this approach of feature engineering the fby_B-1 will have a bigger influence on B supplies A sentences and will not be available for A supplies B sentences.\n Non-Direction Based Features:\n Some such features include token length feature, distance between the two companies feature, and so on.  These features contribute more towards whether there is a relation between the two companies or not.\n Word Based Features:\n The feature set include unigrams, bigrams and trigrams before and after Company-A tokens in the sentence, before and after Company-B token in the sentence and words around the pattern that was matched in the sentence.  All these feature are\ndirection based features.\n Sentence Based Features:\n These feature includes features to check if either of the company is in a list of companies, if there are any company to the left or right of the company, if any of the company is an anaphora resolved company, and so on.  These are also\ndirection based features.\n Pattern Indication Features:\n These feature check for specific patterns in the sentence based on the position of the company tokens in the sentence.  For example the presence of a pattern \"provided to Company-B\" and then followed by a list of blacklisted words like \"letter\",\n\"stock\", etc. indicate a negative feature for the sentence.\n Results:\n Both the filing and news model have shown a precision of around 56% and a recall of around 45% at the sentence level on the validation test data.\n Company Pair Level Aggregation--\n The system at pairwise aggregator 614 stores the sentence/evidence level classification result to a knowledge graph 612 where all the evidences/sentences for each pair are aggregated to get an aggregated score for a given pair.  The following\nexamples: \"**Company-A** is an important Client of **Company-B**.\": 0.981 (classifier score); \"**Company-A** sold 50M cargo to **Company-B** in 2015.\": 0.902; \"**Company-B** mentioned that it agreed to buy tyres from **Company-A**\": 0.947; yield an\naggregated score for the company pair A-B as follows: Company-A (as supplier)--Company-B (as customer) of 0.99 (aggregated score).\n The aggregator is a function of the individual evidence scores given by the classifier.  This estimation is based on the evidence collected from the entire corpus, taking into account the source (news/filings) and confidence score of each\ndetection as well as other signals, which either increase or decrease the probability of the relation.\n Results: At the aggregation level, the exemplary system performs with a precision of above 70% for both filings and news documents.\n In one manner of implementation the present invention provides a SCAR and involves building and querying an Enterprise Knowledge Graph.\n With available data concerning a variety of subjects 1) presenting an unprecedented amount that continues to grow at increasing rates, 2) coming from diverse sources, and 3) covering a variety of domains in heterogeneous formats, information\nproviders are faced with the critical challenge to process, retrieve and present such broad array of information to their users to satisfy complex information needs.  The present invention may be implemented, in one exemplary manner, in connection with a\nfamily of services for building and querying an enterprise knowledge graph.  For example, first data is acquired from various sources via different approaches.  Furthermore, useful information is mined from the data by adopting a variety of techniques,\nincluding Named Entity Recognition (NER) and Relation Extraction (RE); such mined information is further integrated with existing structured data (e.g., via Entity Linking (EL) techniques) to obtain relatively comprehensive descriptions of the entities. \nModeling the data as a Resource Description Framework (RDF) graph model enables easy data management and embedding of rich semantics in collected and pre-processed data.\n In one exemplary, but not limiting, implementation, the supply-chain relationship processes herein described may be used in a system to facilitate the querying of mined and integrated data, i.e., the knowledge graph.  For example, a natural\nlanguage interface (e.g., Thomson Reuters Discover interface or other suitable search engine-based interface) allows users to ask questions of a knowledge graph in the user's own words.  Such natural language questions are translated into executable\nqueries for answer retrieval.  To validate performance, the involved services were evaluated, i.e., named entity recognition, relation extraction, entity linking and natural language interface, on real-world datasets.\n Knowledge workers, such as scientists, lawyers, traders or accountants, deal with a greater than ever (and growing) amount of data with an increasing level of variety.  Many solutions of the past have been document-centric, or focused at the\ndocument level, and this has resulted in often less than effective presentation of results for users.  Users information needs are often focused on entities and their relations, rather than on documents.  To satisfy these needs, information providers\nmust pull information from wherever it happens to be stored and bring it together in a summary result.  As a concrete example, suppose a user is interested in companies with the highest operating profit in 2015 currently involved in Intellectual Property\n(IP) lawsuits.  To answer this query, one needs to extract company entities from free text documents, such as financial reports and court documents, and then integrate the information extracted from different documents about the same company together.\n Three key challenges for providing information to knowledge workers so that they can receive the answers they need are: 1) How to process and mine useful information from large amount of unstructured and structured data; 2) How to integrate such\nmined information for the same entity across disconnected data sources and store them in a manner for easy and efficient access; 3) How to quickly find the entities that satisfy the information needs of today's knowledge workers.\n A knowledge graph as used herein refers to a general concept of representing entities and their relationships and there have been various efforts underway to create knowledge graphs that connect entities with each other.  For instance, the\nGoogle Knowledge Graph consists of around 570 million entities as of 2014.  Here, for the purpose of describing how to implement the inventive concepts, and not by limitation, we describe in connection with Thomson Reuters' approach to addressing the\nthree challenges introduced above.  Within Thomson Reuters, data may be produced manually, e.g., by journalists, financial analysts and attorneys, or automatically, e.g., from financial markets and cell phones.  Furthermore, the data we have covers a\nvariety of domains, such as media, geography, finance, legal, academia and entertainment.  In terms of the format, data may be structured (e.g., database records) or unstructured (e.g., news articles, court dockets and financial reports).\n Given this large amount of data available, from diverse sources and about various domains, one key challenge is how to structure this data in order to best support users' information needs.  First, we ingest and consume the data in a scalable\nmanner.  This data ingestion process is preferably robust enough to be capable of processing all types of data (e.g., relation databases, tabular files, free text documents and PDF files) that may be acquired from various data sources.  Although much\ndata may be in structured formats (e.g., database records and statements represented using Resource Description Framework1 (RDF)), significant amounts of desirable data is unstructured free text.\n Unstructured data may include patent filings, financial reports, academic publications, etc. To best satisfy users' information needs, structure may be added to free text documents.  Additionally, rather than having data in separate \"silos\",\ndata may be integrated to facilitate downstream applications, such as search and data analytics.\n Data modeling and storage is another important part of an improved knowledge graph pipeline, with a data modeling mechanism flexible enough to allow scalable data storage, easy data update and schema flexibility.  The Entity-Relationship (ER)\nmodeling approach, for example, is a mature technique; however, we find that it is difficult to rapidly accommodate new facts in this model.  Inverted indices allow efficient retrieval of the data; however, one key drawback is it only supports keyword\nqueries that may not be sufficient to satisfy complex information needs.  RDF is a flexible model for representing data in the format of tuples with three elements and no fixed schema requirement.  An RDF model also allows for a more expressive semantics\nof the modeled data that can be used for knowledge inference.\n In one exemplary implementation of the ingested, transformed, integrated and stored data, a system delivers efficiently retrieval of answers to users in an intuitive manner.  Currently, the mainstream approaches to searching for information are\nkeyword queries and specialized query languages (e.g., SQL and SPARQL (https://www.w3.org/TR.sparql11-overview/)).  The former are not able to represent the exact query intent of the user, in particular for questions involving relations or other\nrestrictions such as temporal constraints (e.g., IBM lawsuits since 2014); while the latter require users to become experts in specialized, complicated, and hard-to-write query languages.  Thus, both mainstream techniques create severe barriers between\ndata and users, and do not serve well the goal of helping users to effectively find the information they are seeking in today's hypercompetitive, complex, and Big Data world.\n The SCAR of the present invention represents improvements achieved in building and querying an enterprise knowledge graph, including the following major contributions.  We first present our data acquisition process from various sources.  The\nacquired data is stored in a raw data store, which may include relational databases, Comma Separated Value (CSV) files, and so on.  We apply our Named Entity Recognition (NER), relation extraction and entity linking techniques to mine valuable\ninformation from the acquired data.  Such mined and integrated data then constitute our knowledge graph.  Further, and in one manner of operation, a natural language interface (e.g., TR Discover) is also used that enables users to intuitively search for\ninformation from the knowledge graph using their own words.  We evaluate our NER, relation extraction and entity linking techniques on a real-world news corpus and validate the effectiveness and improved performance in our techniques.  We also evaluate\nTR Discover on a graph of 2.2 billion triples by using 10K randomly generated questions of different levels of complexity.\n As presented and described below, first presented is an overview of the SCAR service framework.  Next, presented is data acquisition, transformation and interlinking (i.e., NER-named entity recognition, RE-relation extraction and EL-entity\nlinking) processes.  Next is described an exemplary manner of modeling and storing of processed data.  Further, and in one manner of operation, an exemplary natural language interface for querying the KG-knowledge graph.  Next is described an evaluation\nof the components of the system and related work.\n FIG. 7 demonstrates the overall architecture of an exemplary embodiment of the SCAR system 700.  In this diagram, the solid lines represent our batch data processing, whose result will be used to update our knowledge graph; the dotted lines\nrepresent the interactions between users and various services.  For services that are publicly available, a published user guide and code examples in different programming languages is available (e.g., https://permid.org/).\n First of all, during our data acquisition and ingestion processes described in detail below, we consume data from various sources 702, including live data feeds, web pages and other non-textual data (e.g., PDF files).  For example, for PDF\nfiles, we apply commercial Optical Character Recognition (OCR) software to obtain the text from them.  We also analyze web pages and extract their textual information.\n Next, given a document in the raw data 704, a single POST request is issued to our core service for entity recognition and relation extraction.  Furthermore, our service performs disambiguation within the recognized entities at the named entity\nrecognition, extraction and entity linking module or core service 706.  For example, if two recognized entities \"Tim Cook\" and \"Timothy Cook\" have been determined by our system to both refer to the CEO of Apple Inc., they will be grouped together as one\nrecognized entity in the output 714.  Finally, our system will try to link each of the recognized entities to our existing knowledge graph 712.  If a mapping between a recognized entity and one in the knowledge graph 712 is found, in the output 714 of\nthe core service 706, the recognized entity will be assigned the existing entity ID in our knowledge graph 712.\n The entity linking service can also be called separately.  It takes a CSV file as input where each line is a single entity that will be linked to our knowledge graph 712.  In the exemplary deployment, each CSV file can contain up to 5,000\nentities.\n While performing the above-discussed services, with our RDF model, we store our knowledge graph 712, i.e., the recognized entities and their relations, in an inverted index for efficient retrieval with keyword queries (i.e., the Keyword Search\nService 716 in FIG. 7) and also in a triple store in order to support complex query needs.\n Finally, to support the natural language interface 710, e.g., TR Discover, internal processes retrieve entities and relations from the knowledge graph 712 and build the necessary resources for the relevant sub-modules such as the entity matching\nservice 718 (e.g., a lexicon for question understanding).  Users can then enter and submit a natural language query through a Web-based interface.\n Data Acquisition, Transformation and Interlinking--\n The following describes one exemplary manner of implementing the SCAR system.  SCAR accesses a plurality of data sources and obtains/collects electronic data representing documents including textual content as source data, this is referred to as\nthe acquisition and curation process.  Such collected and curated data is then used to build the knowledge graph.  Data Source and Acquisition--In this exemplary implementation, the data used covers a variety of industries, including Financial & Risk\n(F&R), Tax & Accounting, Legal, and News.  Each of these four major data categories can be further divided into various sub-categories.  For instance, our F&R data ranges from Company Fundamentals to Deals and Mergers & Acquisitions.  Professional\ncustomers rely on rich datasets to find trusted and reliable answers upon which to make decisions and advisements.  Below, Table 1 provides a high-level summary of the exemplary data space.\n TABLE-US-00001 TABLE 1 An Overview of Thomson Reuters Data Space Industry Description Financial & Risk F&R data primarily consists of structured data (F&R) such as intra and end-of-day time series, Credit Ratings, Fundamentals, alongside less\nstructured sources, e.g., Broker Research and News.  Tax & Accounting Here, the two biggest datasets are highly structured tax returns and tax regulations.  Legal Our legal content has a US bias and is mostly unstructured or semi-structured.  It ranges\nfrom regulations to dockets, verdicts to case decisions from Supreme Court, alongside numerous analytical works.  Reuters News Reuters delivers more than 2 million news articles and 0.5 million pictures every year.  The news articles are unstructured but\naugmented with certain types of metadata.\n To acquire the necessary data in the above-mentioned domains, we adopted a mixture of different approaches, including manual data entry, web scraping, feed consumption, bulk upload and OCR.  The acquired data is further curated at different\nlevels according to the product requirements and the desired quality level.  Data curation may be done manually or automatically.  Although our acquired data contains a certain amount of structured data (e.g., database records, RDF triples, CSV files,\netc.), the majority of our data is unstructured (e.g., Reuters news articles).  Such unstructured data contains rich information that could be used to supplement existing structured data.  Because our data comes from diverse sources and covers various\ndomains, including Finance, Legal, Intellectual Property, Tax & Accounting, etc., it is very likely that the same entity (e.g., organization, location, judge, attorney and law firm) could occur in multiple sources with complementary information.  For\nexample, \"Company A\" may exist in our legal data and is related to all its legal cases; while at the same time, this company may also appear in our financial data with all its Merger & Acquisition activities.  Being able to interlink the different\noccurrences of the same entity across a variety of data sources is key to providing users a comprehensive view of entities of interest.  An additional operational goal is to update and maintain the graph to keep up with the fast changing nature of source\ncontent.\n To mine information from unstructured data and to interlink entities across diverse data sources, we have devoted a significant amount of effort to developing tools and capabilities for automatic information extraction and data interlinking. \nFor structured data, we link each entity in the data to the relevant nodes in our graph and update the information of the nodes being linked to.  For unstructured data, we first perform information extraction to extract the entities and their\nrelationships with other entities; such extracted structured data is then integrated into our knowledge graph.\n Named Entity Recognition--\n Given a free text document, we first perform named entity recognition (NER) on the document to extract various types of entities, including companies, people, locations, events, etc. We accomplish this NER process by adopting a set of in-house\nnatural language processing techniques that include both rule-based and machine learning algorithms.  The rule-based solution uses well-crafted patterns and lexicons to identify both familiar and unfamiliar entity names.\n Our machine learning-based NER consists of two parts, both of which are based on binary classification and evolved from the Closed Set Extraction (CSE) system.  CSE originally solved a simpler version of the NER problem: extracting only known\nentities, without discovering unfamiliar ones.  This simplification allows it to take a different algorithmic approach, instead of looking at the sequence of words.  First, it searches the text for known entity aliases, which become entity candidates. \nThen it uses a binary classification task to decide whether each candidate actually refers to an entity or not, based on its context and on the candidate alias.  The second component tries to look for unfamiliar entity names, by creating candidates from\npatterns, instead from lexicons.\n Both components use logistic regression for the classification problem, using LIBLINEAR implementation (a known library for large linear classification).  We employ commonly adopted features for our machine learning-based NER algorithm: e.g.,\nparts of speech, surrounding words, various lexicons and gazetteers (company names, people names, geographies & locations, company suffixes, etc.).  We also designed special features to deal with specific sources of interest; such special features are\naimed at detecting source specific patterns.\n Relationship Extraction--\n The core of this approach is a machine learning classifier that predicts the probability of a possible relationship for a given pair of identified entities, e.g., known or recognized companies (which may be tagged in the NER process), in a given\nsentence.  This classifier uses a set of patterns to exclude noisy sentences, and then extracts a set of features from each sentence.  We employ context-based features, such as token-level n-grams and patterns.  Other features are based on various\ntransformations and normalizations that are applied to each sentence (such as replacing identified entities by their type, omitting irrelevant sentence parts, etc.).  In addition, the classifier also relies on information available from our existing\nknowledge graph.  For instance, when trying to identify the relationship between two identified companies, the industry information (i.e., healthcare, finance, automobile, etc.) of each company is retrieved from the knowledge graph and used as a feature. We also use past data to automatically detect labeling errors in our training set, which improves our classifier over time.\n The algorithm is precision-oriented to avoid introducing too many false positives into the knowledge graph.  In one manner of operation, relation extraction is only applied to the recognized entity pairs in each document, i.e., we do not try to\nrelate two entities from two different free text documents.  The relation extraction process runs as a daily routine on live document feeds.  For each pair of entities, the SCAR system may extract multiple relationships; only those relationships with a\nconfidence score above a pre-defined threshold are then added to the knowledge graph.  Named entity recognition and relation extraction APIs, also known as Intelligent Tagging, are publicly available (http://www.opencalais.com/opencalais-api/).\n Entity Linking--\n While the capability to mine information from unstructured data is important, an equally important function of the SCAR system is to be able to integrate such mined information with existing structured data to provide users with comprehensive\ninformation about the entities.  The SCAR system may employ several tools to link entities to nodes in the knowledge graph.  One approach is based on matching the attribute values of the nodes in the graph and that of a new entity.  These tools adopt a\ngeneric but customizable algorithm that is adjustable for different specific use cases.  In general, given an entity, we first adopt a blocking technique to find candidate nodes that the given entity could possibly be linked to.  Blocking can be treated\nas a filtering process and is used to identify nodes that are promising candidates for linking in a lightweight manner.  The actual and expensive entity matching algorithms are then only applied between the given entity and the resulting candidate nodes.\n Next, the SCAR system computes a similarity score between each of the candidate nodes and the given entity using an Support Vector Machine (SVM) classifier that is trained using a surrogate learning technique.  Surrogate learning allows the\nautomatic generation of training data from the datasets being matched.  In surrogate learning, we find a feature that is class-conditionally independent of the other features and whose high values correlate with true positives and low values correlate\nwith true negatives.  Then, this surrogate feature is used to automatically label training examples to avoid manually labeling a large number of training data.\n An example of a surrogate feature is the use of the reciprocal of the block size: 1/block_size.  In this case, for a block containing just one candidate that is most likely a match (true positive), the value for this surrogate feature will be\n1.0; while for a big block containing a matching entity and many non-matching entities (true negatives), the value of the surrogate feature will be small.  Therefore, on average, a high value of this surrogate feature (close to 1.0) will correlate to\ntrue positives and a low value (&lt;&lt;1.0) will correlate to true negatives.\n The features needed for the SVM model are extracted from all pairs of comparable attributes between the given entity and a candidate node.  For example, the attributes \"first name\" and \"given name\" are comparable.  Based upon such calculated\nsimilarity scores, the given entity is linked to the candidate node that it has the highest similarity score with, this may be conditioned on if their similarity score is also above a pre-defined threshold.  The blocking phase is tuned towards high\nrecall, i.e., we want to make sure that the blocking step will be able to cover the node in the graph that a given entity should be linked to, if such a node exists.  Then, the actual entity linking step ensures that we only generate a link when there is\nsufficient evidence to achieve an acceptable level of precision, i.e., the similarity between the given entity and a candidate node is above a threshold.  The entity linking module or component may vary in the way it implements each of the two steps. \nFor example, it may be configured to use different attributes and their combinations for blocking; it also provides different similarity algorithms that can be used to compute feature values.  Exemplary entity linking APIs are publicly available (e.g.,\npermid.org/match).\n FIG. 8 is a flow diagram 800 demonstrating an example of NER 804, entity linking 806, and relation extraction 808 processes.  First, with the NER 804 technique identifies two companies, \"Denso Corp\" and \"Honda\"; each of identified company is\nassigned a temporary identifier ID.  Next in entity linking 806, both recognized companies are linked to nodes in the knowledge graph and each is associated with the corresponding Knowledge Graph ID (KGID).  Furthermore, a relationship, in this case the\nrelationship \"supplier\", (i.e., \"Denso Corp\" and \"Honda\" have a supply chain relationship between them) is extracted at relation extraction 808.  At knowledge graph update 810, the newly extracted relationship is added to the knowledge graph 802, since\nthe score of this relationship (0.95) is above the pre-defined threshold.\n Data Modeling and Physical Storage--\n There are a variety of mechanisms for representing the data, including the Entity-Relation (ER) model (i.e., for relational databases), plain text files (e.g., in tabular formats, such as CSV), or inverted indices (to facilitate efficient\nretrieval by using keyword queries), etc. Plain text files may be easiest to store the data.  However, placing data into files would not allow the users to conveniently obtain the information they are looking for from a massive number of files.  Although\nrelational database is a mature technique and users can retrieve information by using expressive SQL queries, a schema (i.e., the ER model) has to be defined ahead-of-time in order to represent, store and query the data.  This modeling process can be\nrelatively complicated and time-consuming, particularly for companies that have diverse types of datasets from various data sources.  Also, as new data comes in, it may be necessary to keep revising the model and even remodeling the data, which could be\nexpensive in terms of both time and human effort.  Data can also be used to build inverted indices for efficient retrieval.  However, the biggest drawback of inverted indices is that users can only search for information with simple keyword queries;\nwhile in real-world scenarios, many user search needs would be better captured by adopting more expressive query languages.\n Modeling Data as RDF--\n One emerging data representation technique is the Resource Description Framework (RDF).  RDF is a graph based data model for describing entities and their relationships on the Web.  Although RDF is commonly described as a directed and labeled\ngraph, many researchers prefer to think of it as a set of triples, each consisting of a subject, predicate and object in the form of &lt;subject, predicate, object&gt;.\n Triples are stored in a triple store and queried with the SPARQL query language.  Compared to inverted indices and plain text files, triple stores and the SPARQL query language enable users to search for information with expressive queries in\norder to satisfy complex user needs.  Although a model is required for representing data in triples (similar to relational databases), RDF enables the expression of rich semantics and supports knowledge inference.\n Another big advantage of adopting an RDF model is that it enables easier data deletion and update.  Traditional data storage systems are \"schema on write\", i.e., the structure of the data (the data model) is decided at design time and any data\nthat does not fit this structure is lost when ingesting the data.  In contrast, \"schema on read\" systems attempt to capture everything and then apply computation horsepower to enforce a schema when the data is retrieved.  An example would be the\nElastic/Logstash/Kibana stack (www.elastic.co/products) that does not enforce any schema when indexing the data but then tries to interpret one from the built indices.  The tradeoff is future-proofing and nimbleness at the expense of (rapidly\ndiminishing) computing and storage.  RDF sits at a unique intersection of the two types of systems.  First of all, it is \"schema on write\" in the sense that there is a valid format for data to be expressed as triples.  On the other hand, the boundless\nnature of triples means that statements can be easily added/deleted/updated by the system and such operations are hidden to users.  Therefore, adopting an RDF model for data representation fits our needs well.\n FIG. 9 represents an exemplary ontology snippet of an exemplary Knowledge Graph 900 in connection with an operation of the present invention.  While building the knowledge graph 900, we have designed an RDF model for our data.  Our model\ncontains classes (e.g., organizations and people) and predicates (the relationships between classes, e.g., \"works for\" and \"is a board member of\").  For brevity, we only show a snippet of our entire model in FIG. 9.  Here, the major classes include\nOrganization 902, Legal Case 904, Patent 908 and Country 906.  Various relationships also exist between these classes: \"involved in\" connects a legal case and an organization, \"presided over by\" exists between a judge and a legal case, patents can be\n\"granted to\" organizations, an organization can \"develop\" a drug which \"is treatment for\" one or more diseases.  This model is exemplary and may accommodate new domains or add other domains over time.\n Data Storage--\n In this exemplary implementation, we store the triples in two ways.  We index the triples on their subject, predicate and object respectively with the Elastic search engine.  We also build a full-text search index on objects that are literal\nvalues, where such literal values are tokenized and treated as terms in the index.  This enables fast retrieval of the data with simple keyword queries.  Additionally, we store all the triples in a triple store in order to support search with complex\nSPARQL queries.  The exemplary TR knowledge graph manages about five billion triples; however, this only represents a small percentage of related data and the number of triples is expected to grow rapidly over time.\n In addition to the three basic elements in a triple (i.e., subject, predicate and object), a fourth element can also be added, turning a triple to a quad (www.w3.org/TR/n-quads/).  This fourth element is generally used to provide provenance\ninformation of the triple, such as its source and trustworthiness.  Such provenance information can be used to evaluate the quality of a triple.  For example, if a triple comes from a reputable source, then it may generally have a higher quality level. \nIn our current system, we use the fourth element to track the source and usage information of the triples.  The following examples show the usage of this fourth element: &lt;Microsoft, has_address, Address1, Wikipedia&gt;, indicating that this triple\ncomes from Wikipedia; and &lt;Jim Hendler, works_for, RPI, 2007 to present&gt;, showing the time period that Jim Hendler works for RPI.\n Querying the Knowledge Graph with Natural Language--\n Above we have presented a Big Data framework and infrastructure for building an enterprise knowledge graph.  However, given the built graph, one important question is how to enable end users to retrieve the data from this graph in an intuitive\nand convenient manner.  Technical professionals, such as database experts and data scientists, may simply employ SPARQL queries to access this information.  But non-technical information professionals, such as journalists, financial analysts and patent\nlawyers, who can-not be expected to learn such specialized query languages, still need a fast and effective means for accessing the data that is relevant to the task at hand.\n Keyword-based queries have been frequently adopted to allow non-technical users to access large-scale RDF data, and can be applied in a uniform fashion to information sources that may have wildly divergent logical and physical structure.  But\nthey do not always allow precise specification of the user's intent, so the returned result sets may be unmanageably large and of limited relevance.  However, it would be difficult for non-technical users to learn specialized query languages (e.g.,\nSPARQL) and to keep up with the pace of the development of new query languages.\n To enable non-technical users to intuitively find the exact information they are seeking, TR Discover, a natural language interface, bridges the gap between keyword-based search and structured query.  In the TR Discover natural language\ninterface, the user creates natural language questions, which are mapped into a logic-based intermediate language.  A grammar defines the options available to the user and implements the mapping from English into logic.  An auto-suggest mechanism guides\nthe user towards questions that are both logically well-formed and likely to elicit useful answers from a knowledge base.  A second translation step then maps from the logic-based representation into a standard query language (e.g., SPARQL), allowing the\ntranslated query to rely on robust existing technology.  Since all professionals can use natural language, we retain the accessibility advantages of keyword search, and since the mapping from the logical formalism to the query language is\ninformation-preserving, we retain the precision of query-based information access.  The detailed use of TR Discover follows.\n Question Understanding--We use a Feature-based Context-Free Grammar (FCFG) for parsing natural language questions.  Our FCFG consists of phrase structure rules (i.e., grammar rules) on non-terminal nodes and lexical entries (i.e., lexicon) for\nleaf nodes.  The large majority of the phrase structure rules are domain independent allowing the grammar to be portable to new domains.  The following shows a few examples of our grammar rules: G1-G3.  Specifically, Rule G3 indicates that a verb phrase\n(VP) contains a verb (V), noun (N), and a noun phrase (NP).  NP.fwdarw.N G1: NP.fwdarw.NP VP G2: VP.fwdarw.VNP G3:\n Furthermore, as for the lexicon, each entry in the FCFG lexicon contains a variety of domain-specific features that are used to constrain the number of parses computed by the parser preferably to a single, unambiguous parse.  L1-L3 are examples\nof lexical entries.  N[TYPE=drug, NUM=pl, SEM=&lt;.lamda.x.drug(x)&gt;].fwdarw.`drugs` L1: V[TYPE=[drug,org,dev], SEM=&lt;.lamda.X x.X(.lamda.y.dev_org_drug(y,x))&gt;, TNS=past, NUM=?n].fwdarw.`developed by` L2: V[TYPE=[org,country,hq],\nNUM=?n].fwdarw.`headquartered in` L3:\n Here, L1 is the lexical entry for the word, drugs, indicating that it is of TYPE drug, is plural (\"NUM=pl\"), and has the semantic representation .lamda.x.drug(x).  Verbs (V) have an additional feature tense (TNS), as shown in L2.  The TYPE of\nverbs specify both the potential subject-TYPE and object-TYPE.  With such type constraints, we can then license the question drugs developed by Merck while rejecting nonsensical questions like drugs headquartered in the U.S.  on the basis of the mismatch\nin semantic type.  A general form for specifying the subject and object types for verbs is as following: TYPE=[subject_constraint, object_constraint, predicate_name].\n Disambiguation relies on the unification of features on non-terminal syntactic nodes.  We mark prepositional phrases (PPs) with features that determine their attachment preference.  For example, we specify that the prepositional phrase for pain\nmust attach to an NP rather than a VP; thus, in the question Which companies develop drugs for pain?, \"for pain\" cannot attach to \"develop\" but must attach to \"drugs\".  Additional features constrain the TYPE of the nominal head of the PP and the semantic\nrelationship that the PP must have with the phrase to which it attaches.  This approach filters out many of the syntactically possible but undesirable PP-attachments in long queries with multiple modifiers, such as companies headquartered in Germany\ndeveloping drugs for pain or cancer.  When a natural language question has multiple parses, we always choose the first parse.  Future work may include developing ranking mechanisms in order to rank the parses of a question.\n The outcome of our question understanding process is a logical representation of the given natural language question.  Such logical representation is then further translated into an executable query (SPARQL) for retrieving the query results. \nAdopting such intermediate logical representation enables us to have the flexibility to further translate the logical representation into different types of executable queries in order to support different types of data stores (e.g., relational database,\ntriple store, inverted index, etc.).\n Enabling Question Completion with Auto-Suggest--\n Traditional question answering systems often require users to enter a complete question.  However, it may be difficult for novice users to do so, e.g., due to the lack of familiarity and an incomplete understanding of the underlying data.  One\nfeature of the exemplary natural language interface TR Discover is that it provides suggestions in order to help users to complete their questions.  The intuition here is that the auto-suggest module guides users in exploring the underlying data and\ncompleting a question that can be potentially answered with the data.  Unlike Google's query auto-completion that is based on query logs, the present auto-suggestions are computed based upon the relationships and entities in the built knowledge graph and\nby utilizing the linguistic constraints encoded in the grammar feature.\n The present auto-suggest module is based on the idea of left-corner parsing.  Given a query segment-qs (e.g., drugs, developed by, etc.), we find all grammar rules whose left corner-fe on the right side matches the left side of the lexical entry\nof qs.  We then find all leaf nodes in the grammar that can be reached by using the adjacent element of fe.  For all reachable leaf nodes (i.e., lexical entries in our grammar), if a lexical entry also satisfies all the linguistic constraints, we then\ntreat it as a valid suggestion.\n The following describes two exemplary ways of using the auto-suggest facility.  On one hand, users may be interested in broad, exploratory questions; however, due to lack of familiarity with the data, guidance from our auto-suggest module will\nbe needed to help this user build a valid question in order to explore the underlying data.  In this situation, users can work in steps: they could type in an initial question segment and wait for the system to provide suggestions.  Then, users can\nselect one of the suggestions to move forward.  By repeating this process, users can build well-formed natural language questions (i.e., questions that are likely to be understood by our system) in a series of small steps guided by our auto-suggest.\n FIGS. 10(a)-10(c) demonstrate this question building process.  Assuming that User A starts by typing in \"dr\" as shown in FIG. 10(a), drugs will then appear as one or several possible completions.  User A can either continue typing drugs or\nselect it from the drop-down list.  Upon selection, suggested continuations to the current question segment, such as \"using\" and \"developed by,\" are then provided to User A as shown in FIG. 10(b).  Suppose our user is interested in exploring drug\nmanufacturers and thus selects \"developed by.\" In this case, both the generic type, companies, along with specific company instances like \"Pfizer Inc\" and \"Merck & Co Inc\" are offered as suggestions as shown in FIG. 10(c).  User A can then select \"Pfizer\nInc\" to build the valid question, \"drugs developed by Pfizer Inc\" 1052 thereby retrieving answers 1054 from our knowledge graph as shown in the user interface 1050 of FIG. 10(d).\n Alternatively, users can type in a longer string, without pausing, and our system will chunk the question and try to provide suggestions for users to further complete their question.  For instance, given the following partial question cases\nfiled by Microsoft tried in .  . . , our system first tokenizes this question; then starting from the first token, it finds the shortest phrase (a series of continuous tokens) that matches a suggestion and treats this phrase as a question segment.  In\nthis example, cases (i.e., legal cases) will be the first segment.  As the question generation proceeds, our system finds suggestions based on the discovered question segments, and produces the following sequence of segments: cases, filed by, Microsoft,\nand tried in. At the end, the system knows that the phrase segment or text string \"tried in\" is likely to be followed by a phrase describing a jurisdiction, and is able to offer corresponding suggestions to the user.  In general, an experienced user\nmight simply type in cases filed by Microsoft tried in; while first-time users who are less familiar with the data can begin with the stepwise approach, progressing to a more fluent user experience as they gain a deeper understanding of the underlying\ndata.\n We rank the suggestions based upon statistics extracted from our knowledge graph.  Each node in our knowledge graph corresponds to a lexical entry (i.e., a potential suggestion) in our grammar (i.e., FCFG), including entities (e.g., specific\ndrugs, drug targets, diseases, companies, and patents), predicates (e.g., developed by and filed by), and generic types (e.g., Drug, Company, Technology, etc.).  Using our knowledge graph, the ranking score of a suggestion is defined as the number of\nrelationships it is involved in. For example, if a company filed 10 patents and is also involved in 20 lawsuits, then its ranking score will be 30.  Although this ranking is computed only based upon the data, alternative approaches may be implemented or\nthe system's behavior may be tuned to a particular individual user, e.g., by mining query logs for similar queries previously made by that user.\n Question Translation and Execution--\n FIG. 11 depicts a Parse Tree 1100 for the First Order Logic (FOL) of the Question \"Drugs developed by Merck.\" In contrast to other natural language interfaces, our question understanding module first maps a natural language question to its\nlogical representation; and, in this exemplary embodiment, we adopt First Order Logic (FOL).  The FOL representation of a natural language question is further translated to an executable query.  This intermediate logical representation provides us the\nflexibility to develop different query translators for various types of data stores.\n There are two steps in translating an FOL representation to an executable query.  In the first step, we parse the FOL representation into a parse tree by using an FOL parser.  This FOL parser is implemented with ANTLR (a known parser development\ntool).  The FOL parser takes a grammar and an FOL representation as input, and generates a parse tree for the FOL representation.  FIG. 11 shows the parse tree of the FOL for the question \"Drugs developed by Merck\".  We then perform an in-order traversal\n(with ANTLR's APIs) of the FOL parse tree and translate it to an executable query.  While traversing the tree, we put all the atomic query constraints (e.g., \"type(entity0, company)\", indicating that \"entity0\" represents a company entity, and\n\"pid(entity0, 4295904886)\", showing the internal ID of the entity represented by \"entity0\") and the logical connectors (i.e., \"and\" and \"or\") into a stack.  When we finish traversing the entire tree, we pop the conditions out of the stack to build the\ncorrect query constraints; predicates (e.g., \"develop_org_drug\" and \"pid\") in the FOL are also mapped to their corresponding predicates in our RDF model to formulate the final SPARQL query.  We run the translated SPARQL queries against an instance of the\nfree version of GraphDB, a state-of-the-art triple store for storing triple data and for executing SPARQL queries.\n As a concrete example, the following summarizes the translation from a natural language question to a SPARQL query via a FOL representation:\n Natural Language Question:\n Drugs developed by Merck\n FOL: all x.(drug(x).fwdarw.(develop_org_drug(entity0,x) & type(entity0,Company) & pid(entity0,4295904886)))\n SPARQL Query:\n TABLE-US-00002 PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; PREFIX example: http://www.example.com# select ?x where { ?x rdf:type example:Drug .  example:4295904886 example:develops ?x .  }\n Evaluation of Data Transformation and Interlinking--\n Here, we evaluate named entity recognition, relation ex-traction, and entity linking services, i.e., Intelligent Tagging.\n Dataset.\n Named entity recognition is evaluated separately for Company, Person, City and Country; entity linking is evaluated on Company and Person entities.  Table 2 shows the statistics of our evaluation datasets for NER and entity linking.  All\ndocuments were randomly sampled from a large news corpus.  For NER, each selected document was annotated by manually.  It should be noted that these entity mention counts are at the document level, and not the instance level.  For example, if a company\nappeared in three different documents and five times in each, we count it as three company mentions (instance level count would have been 15, unique companies count would have been one).  For entity linking, the randomly selected entities are manually\nresolved to entities in our knowledge graph.\n TABLE-US-00003 TABLE 2 Statistics of NER and Entity Linking Evaluation Datasets Task Entity Type |Document| |Mention| Entity Company 1,496 4,450 Recognition Person 600 787 City 100 101 Country 2,000 1,835 Entity Linking Company 1,000 673 Person\n100 156\n We also evaluate our machine learning-based relation extraction algorithm.  We present the results on two different types of relations: \"Supply Chain\" and \"Merger & Acquisition\".  To evaluate the supply chain relation, we first identified 20,000\npossible supply chain relationships (from 19,334 documents).  We then sent these 20,000 possible relations to Amazon Mechanical Turk (www.mturk.com) for manual annotation.  Each task was sent to two different workers; in case of disagreement between the\nfirst two workers, a possible relation is then sent to a third worker in order to get a majority decision.  The agreement rate between workers was 84%.  Through this crowdsourcing process, we obtained 7,602 \"supply-chain\" relations as reported by the\nworkers.  We then checked the quality of a random sample of these relations and found the reported relations of high quality, so we used all the 7,602 relations as ground truth for our evaluation.\n To evaluate the Merger & Acquisition (M&A) relation, we first identified 2,590 possible M&A relations (from 2,500 documents).  These possible relations were then manually tagged and annotated.  The quality of the tagged set was further assessed\nby another worker by examining randomly sampled annotations, and was found to be 92% accurate.  The overall annotation process resulted in 603 true Merger & Acquisition relations, which were used as ground-truth for our evaluation.\n TABLE-US-00004 TABLE 3 Named Entity Recognition, Relation Extraction and Entity Linking Results Task Entity/Relation Type Precision Recall F1 Entity Company 0.94 0.75 0.83 Recognition Person 0.91 0.87 0.89 City 0.93 0.80 0.86 Country 0.95 0.89\n0.92 Relation Supply Chain 0.76 0.46 0.57 Extraction Merger & Acquisition 0.71 0.51 0.59 Entity Linking Company 0.92 0.89 0.90 Person 0.92 0.73 0.81\n Metrics--\n We use the standard evaluation metrics: Precision, Recall and F1-score, as defined in Equation 1:\n .times..times..times..times..times..times..times..times..times..times..ti- mes..times..times..times..times..times..times..times..times..times.  ##EQU00001##\n The three metrics for relation extraction and entity linking are defined in a similar manner by replacing \"entities\" with \"relations\" or \"entity pairs\" in the above three equations.\n Results--\n Table 3 and FIG. 16 demonstrate the results of our NER component on four different types of entities, the results of our relation extraction algorithm on two different relations, and our entity linking results on two different types of entities. In addition, we report the runtime of our NER shown in graph 2900 and entity linking components shown in graph 2902 on two types of documents: Average and Large.  \"Average\" refers to a set of 5,000 documents whose size is smaller than 15 KB with an\naverage size of 2.99 KB.  \"Large\" refers to a collection of 1,500 documents whose size is bigger than 15 KB but smaller than 500 KB (the maximum document size in our data) with an average size of 63.64 KB.\n Evaluation of Natural Language Querying\n Dataset--\n We evaluate the runtime of the different components of the natural language interface, TR Discover, on a subset of our knowledge graph.  Our evaluation dataset contains about 329 million entities and 2.2 billion triples.  This dataset primarily\ncovers the following domains: Intellectual Property, Life Science, Finance and Legal.  The major entity types include Drug, Company, Technology, Patent, Country, Legal Case, Attorney, Law Firm, Judge, etc. Various types of relationships exist between the\nentities, including Develop (Company develops Drug), Headquartered in (Company headquartered in Country), Involved In (Company involved in Legal Case), Presiding Over (Legal Case presided over by Judge), etc.\n Infrastructure.\n We used two machines for evaluating performance: Server-GraphDB: We host a free version of GraphDB, a triple store, on an Oracle Linux machine with two 2.8 GHz CPUs (40 cores) and 256 GB of RAM; and Server-TRDiscover: We perform question\nunderstanding, auto-suggest, and FOL translation on a RedHat machine with a 16-core 2.90 GHz CPU and 264 GB of RAM.  We use a dedicated server for hosting the GraphDB store, so that the execution of the SPARQL queries is not interfered by other\nprocesses.  A natural language question is first sent from an ordinary laptop to Server-TRDiscover for parsing and translation.  If both processes finish successfully, the translated SPARQL query is then sent to Server-GraphDB for execution.  The results\nare then sent back to the laptop.\n Random Question Generation--\n To evaluate the runtime of TR Discover, we randomly generated 10,000 natural language questions using our auto-suggest component.  We give the auto-suggest module a starting point, e.g., drugs or cases, and then perform a depth-first search to\nuncover all possible questions.  At each depth, for each question segment, we select b most highly ranked suggestions.  Choosing the most highly ranked suggestions helps increase the chance of generating questions that will result in non-empty result\nsets to better measure the execution time of SPARQL queries.  We then continue this search process with each of the b suggestions.  By setting different depth limits, we generate questions with different levels of complexity (i.e., different number of\nverbs).  Using this process, we generated 2,000 natural language questions for each number of verbs from 1 to 5, thus 10,000 questions in total.\n Among these 10,000 questions, we present the evaluation results on the valid questions.  A question is considered valid if it successfully parses and its corresponding SPARQL query returns a non-empty result set.  Our parser relies on a grammar\n(i.e., a set of rules) for question understanding; as the number of rules increases, it is possible that the parser may not be able to apply the right set of rules to understand a question, especially a complex one (e.g., with five verbs).  Also, as we\nincrease the number of verbs in a question (i.e., adding more query constraints in the final SPARQL query), it is more likely for a query to return an empty result set.  In both cases, the runtime is faster than when successfully finishing the entire\nprocess with a non-empty result set.  Thus, we only report the results on valid questions.\n Runtime Results--\n FIG. 14 includes three graphs (a) 1402, (b) 1404, and (c) 1406 that show the runtime of natural language parsing, FOL translation and SPARQL execution respectively.  According to FIG. 14 graph (a) 1402, unless a question becomes truly\ncomplicated (with 4 or 5 verbs), the parsing time is generally around or below three seconds.  One example question with 5 verbs could be Patents granted to companies headquartered in Australia developing drugs targeting Lectin mannose binding protein\nmodulator using Absorption enhancer transdermal.  We believe that questions with more than five verbs are rare, thus we did not evaluate questions beyond this level of complexity.  In our current implementation, we adopt NLTK (http://www.nltk.org/) for\nquestion parsing; however, we supply NLTK with our own FCFG grammar and lexicon.\n From FIG. 14 graph (b) 1404, we can see that only a few milliseconds are needed for translating the FOL of a natural language question to a SPARQL query.  In general, the translator only needs to traverse the FOL parse tree (FIG. 11) and\nappropriately combines the different query constraints.\n Finally, we demonstrate the execution time and the result set size of the translated SPARQL queries in FIG. 14 graph (c) 1406.  For questions of all complexity levels, the average execution time is below 500 milliseconds, showing the potential\nof applying a triple store to real-world scenarios with a similar size of data.  As we increase the number of verbs in a question, the runtime actually goes down, since GraphDB is able to utilize the relevant indices on the triples to quickly find\npotential matches.  In addition, all of our 5-verb testing questions generate an empty result set, thus here a question is valid as long as it successfully parses.\n Time Complexity Analysis--\n For our Natural Language Processing (NLP) modules, the complexity of entity extraction is O(n+k*log k), where n is the length of the input document and k is the number of entity candidates in it (k&lt;&lt;n with some edge cases with a large\nnumber of candidates).  The worst-case complexity of our relation extraction component is O(n+l2), where n is the length of the input document, and 1 is the number of extracted entities, as we consider all pairs of entities in the candidate sentences. \nThe complexity of linking a single entity is O(b*r2), where b is the block size (i.e., the number of linking candidates) and r is the number of attributes for a given entity.\n For natural language interface, the time complexity of parsing a natural language question to its First Order Logic representation (FOL) is O(n3), where n is the number of words in a question.  We then parse the FOL to an FOL parse tree with\ntime complexity O(n4).  Next, the FOL parse tree is translated to a SPARQL query with in-order traversal with O(n) complexity.  Finally, the SPARQL query is executed against the triple store.  The complexity here is largely dependent on the nature of the\nquery itself (e.g., the number of joins) and the implementation of the SPARQL query engine.\n Never-Ending Language Learning (NELL) and Open Information Extraction (OpenIE) are two efforts in extracting knowledge facts from a broad range of domains for building knowledge graphs.  In the Semantic Web community, DBpedia and Wikidata are\ntwo of the notable efforts in this area.  The latest version of DBpedia has 4.58 million entities, including 1.5 million persons, 735K places and 241K organizations, among others.  Wikidata covers a broad range of domains and currently has more than 17\nmillion \"data items\" that include specific entities and concepts.  Various efforts have also been devoted to creating knowledge graphs in multiple languages.\n Named Entity Recognition--\n Early attempts for entity recognition relied on linguistic rules and grammar-based techniques.  Recent research focuses on the use of statistical models.  A common approach is to use Sequence Labeling techniques, such as hidden Markov Models,\nconditional random fields and maximum entropy.  These methods rely on language specific features, which aim to capture linguistic subtleties and to incorporate external knowledge bases.  With the advancement of deep learning techniques, there have been\nseveral successful attempts to design neural network architectures to solve the NER problem without the need to design and implement specific features.  These approaches are suitable for use in the SCAR system.\n Relation Extraction--\n Similar to NER, this problem was initially approached with rule-based methods.  Later attempts include the combination of statistical machine learning and various NLP techniques for relation extraction, such as syntactic parsing, and chunking. \nRecently, several neural network-based algorithms have been proposed for relation extraction.  In addition, research has shown that the joint modeling of entity recognition and relation extraction can achieve better results that the traditional pipeline\napproach.\n Entity Linking--\n Linking extracted entities to a reference set of named entities is another important task to building a knowledge graph.  The foundation of statistical entity linking lies in the work of the U.S.  Census Bureau on record linkage.  These\ntechniques were generalized for performing entity linking tasks in various domains.  In recent years, special attention was given to linking entities to Wikipedia by employing word disambiguation techniques and relying on Wikipedia's specific attributes. Such approaches are then generalized for linking entities to other knowledge bases as well.\n Natural Language Interface (NLI)--\n Keyword search has been frequently adopted for retrieving information from knowledge bases.  Although researchers have investigated how to best interpret the semantics of keyword queries, oftentimes, users may still have to figure out the most\neffective queries themselves to retrieve relevant information.  In contrast, TR Discover accepts natural language questions, enabling users to express their search requests in a more intuitive fashion.  By understanding and translating a natural language\nquestion to a structured query, our system then retrieves the exact answer to the question.\n NLIs have been applied to various domains.  Much of the prior work parses a natural language question with various NLP techniques, utilizes the identified entities, concepts and relationships to build a SPARQL or a SQL query, and retrieves\nanswers from the corresponding data stores, e.g., a triple store, or a relational database.  In addition to adopting fully automatic question understanding, CrowdQ also utilizes crowd sourcing techniques for understanding natural language questions. \nInstead of only using structured data, HAWK utilizes both structured and unstructured data for question answering.\n Compared to the state-of-the-art, we maintain flexibility by first parsing a question into First Order Logic, which is further translated into SPARQL.  Using FOL allows us to be agnostic to which query language will be used later.  We do not\nincorporate any query language statements directly into the grammar, keeping our grammar leaner and more flexible for adapting to other query languages.  Another distinct feature of our system is that it helps users to build a complete question by\nproviding suggestions according to a partial question and a grammar.  Although ORAKEL also maps a natural language question to a logical representation, no auto-suggest is provided to the users.\n Knowledge Graph in Practice--\n The Google Knowledge Graph has about 570 million entities as of 2014 and has been adopted to power Google's online search.  Yahoo and Bing (http://blogs.bing.com/search/2013/03/21/understand-your-world-with-bing/- ) are also building their own\nknowledge graphs to facilitate search.  Facebook's Open Graph Protocol (http://ogp.me/) allows users to embed rich metadata into webpages, which essentially turns the entire web into a big graph of objects rather than documents.  In terms of data, the\nNew York Times has published data in RDF format (data.nytimes.com) (5,000 people, 1,500 organizations and 2,000 locations).  The British Broadcasting Corporation has also published in RDF, covering a much more diverse collection of entities\n(www.bbc.co.uk/things/), e.g., persons, places, events, etc. Thomson Reuters now also provides free access to part of its knowledge graph (permid.org) (3.5 million companies, 1.2 million equity quotes and others).\n Towards Generic Data Transformation and Integration--\n State-of-the-art NER and relation extraction techniques have been mainly focused on common entity types, such as locations, people and organizations; however, our data covers a much more diverse set of types of entities, including drugs, medical\ndevices, regulations, legal topics, etc., thus requiring a more generic capability.  Being able to integrate such mined information from unstructured data with existing structured data and to ultimately generate insights for users based upon such\nintegrated data is a key advantage.\n Although these techniques are used to build and query the graph in the first place, these services can also benefit from information in the knowledge graph.  First of all, our knowledge graph is used to create gazetteers and entity fingerprints,\nwhich help to improve the performance of our NER engine.  For example, company information, such as industry, geographical location and products, from the knowledge graph is used to create a company fingerprint.  For entity linking, when a new entity is\nrecognized from a free text document, the information from the knowledge graph is used to identify candidate nodes that this new entity might be linked to.  Finally, our natural language interface relies on a grammar for question parsing, which is built\nbased upon information from the knowledge graph, such as the entity types (e.g., company and person) and their relationships (e.g., \"works_for\").\n Data Modeling--\n Providers, such as Thomson Reuters, are concerned with a wide range of content covering diverse domains, e.g., that range from finance to intellectual property & science and to legal and tax.  It would be difficult and time-consuming task for\nengineers to precisely model such a complex space of domains and convert the ingested and integrated data into RDF triples.  Rather than have engineers understand and perform modeling, we collaborate closely with editorial colleagues to model the data,\napply the model to new contents, and embed the semantics into our data alongside its generation.\n Distributed and Efficient RDF Data Processing--\n The relative scarcity of distributed tools for storing and querying RDF triples is another challenge.  This reflects the inherent complexities of dealing with graph-based data at scale.  Storing all triples in a single node would allow efficient\ngraph operations while this approach may not scale well when we have an extremely large number of triples.  Although existing approaches for distributed RDF data processing and querying often require a large and expensive infrastructure, one solution is\nto use a highly scalable data warehouse (e.g., Apache Cassandra (http://cassandra.apache.org/) and Elasticsearch) for storing the RDF triples; in the meanwhile, slices of this graph can then be retrieved from the entire graph, put in specialized stores,\nand optimized to meet particular user needs.\n Converging Triples from Multiple Sources--\n Another challenge is the lack of inherent capability within RDF for update and delete operations, particularly when multiple sources converge predicates under a single subject.  In this scenario, one cannot simply delete all predicates and apply\nthe new ones: triples from another source will be lost.  While a simplistic solution might be to delete by predicate, this approach does not account for the same predicate coming from multiple sources.  For example, if two sources state a \"director-of\"\npredicate for a given subject, an update from one source cannot delete the triple from the other source.  One solution is to use quads with the fourth element as a named graph allowing us to track the source of the triple and act upon subsets of the\npredicates under a subject.\n Natural Language Interface--\n The first challenge is the tension between the desire to keep the grammar lean and the need for broad coverage.  Our current grammar is highly lexicalized, i.e., all entities (lawyers, drugs, persons, etc.) are maintained as entries to the\ngrammar.  As the size of grammar expands, the complexity of troubleshooting issues that arise increases as well.  For example, a grammar with 1.2 million entries takes about 12 minutes to load on our server, meaning that troubleshooting even minor issues\non the full grammar can take several hours.  As a solution, we are currently exploring options to delexicalize portions of the grammar, namely collapsing entities of the same type, thus dramatically reducing the size of the grammar.\n The second issue is increasing the coverage of the grammar without the benefit of in-domain query logs both in terms of paraphrases (synonymous words and phrases that map back to the same entity type and semantics) and syntactic coverage for\nvarious constructions that can be used to pose the same question.  Crowdsourced question paraphrases may be used to expand the coverage of both the lexical and syntactic variants.  For example, although we cover questions like which companies are\ndeveloping cancer drugs, users also supplied paraphrases like which companies are working on cancer medications thus allowing us to add entries such as working on as a synonym for develop and medication as a synonym for drug.\n FIG. 12 is a flowchart illustrating a supply chain process 1200 for use in obtaining, preprocessing and aggregating evidences of supply chain relationships as discussed in detail above.  The process 1200 may be used for extracting and updating\nexisting supply chain relationships and incorporating the new data with existing Knowledge Graphs, e.g., both a supplier Knowledge Graph related to a supplier-Company A and a customer Knowledge Graph related to a customer-Company B. The periodic data\nprocess 1202 starts and first consumes/acquires data from the cm-well at step 1204.  This may represent generally the initial process of creating a text corpus ab initio or in updating and maintaining an existing corpus associated with a Knowledge Graph\ndelivery service or platform.  This data from 1204 is sent out and in step 1206 the data is pre-processed, e.g., named entity recognition by OneCalais tagging.  The OneCalais tagging 1206 sends responses and a determination 1208 identifies whether or not\nnew relations, e.g., supplier-customer relationship, were found in the periodic data process 1202.  If new relations are not found the process proceeds to end step 1222.  If new relations were found the process proceeds to loop over extracted supply\nchain relations in step 1210.  An identified and determined list of relations is then processed at 1212 to get existing snippets.  A deduplication \"dedup\" process is performed at step 1214.  An aggregate score is calculated, e.g., in the manner as\ndescribed hereinabove, at 1216 on the output of the dedup process 1214.  The cm-well (corpus) is updated in step 1218.  A determination 1220 identifies if additional relations need to be processed and if so returns to step 1212, if not the process ends\nat step 1222.\n FIG. 13 is a sequence diagram illustrating an exemplary Eikon view access sequence 1300 according to one implementation of the present invention operating in connection with TR Eikon platform.  A user 1302 submits a query for customers of\n\"Google\" at step 1351 to TR Eikon View 1310.  Eikon View 1310 resolves the company name \"Google\" and sends the resolved company name \"Google\" at step 1352 to the Eikon Data Cloud 1320 which returns an ID of \"4295899948.\" Eikon View 1310 requests\ncustomers for entity ID \"4295899948\" at step 1353.  The request is passed by Eikon Data Cloud 1310 to Supply Chain Cm-Well 1330 which returns the company customers to Eikon Data Cloud 1320 at step 1354.  Eikon Data Cloud 1320 identifies and adds\nadditional data such as industry, headquarters, and country to the data returned by Supply Chain Cm-Well 1330 to enrich the data at step 1355 and returns the data as an enriched customer list with the list of customer and enriched data to Eikon View 1310\nat step 1356.  The Eikon View 1310 provides the enriched customer list to the user 1302 at step 1357.  The user 1302 may request to sort this information by name at step 1358 and Eikon View 1310 may sort the information at step 1359 and provide the\nsorted information to the user 1302 as a sorted list at step 1360.\n FIG. 15 is a flowchart of a method 1500 for identifying supply chain relationships.  The first step 1502 provides for accessing a Knowledge Graph data store comprising a plurality of Knowledge Graphs, each Knowledge Graph related to an\nassociated entity and including a first Knowledge Graph associated with a first company and comprising supplier-customer data.  In the second step 1504 electronic documents are received by an input from a plurality of data sources via a communications\nnetwork, the received documents comprise unstructured text.  The third step 1506 performs, by a preprocessing interface, one or more of named entity recognition, relation extraction, and entity linking on the received electronic documents.  In the fourth\nstep 1508 the preprocessing interface generates a set of tagged data.  The fifth step 1510 provides for the parsing of the electronic documents by the preprocessing interface into sentences and identification of a set of sentences with each identified\nsentence having at least two identified companies as an entity-pair.  In step 1512 a pattern-matching module performs a pattern-matching set of rules to extract sentences from the set of sentences as supply chain evidence candidate sentences.  Next in\nstep 1514, a classifier adapted to utilize natural language processing on the supply chain candidate sentences calculates a probability of a supply-chain relationship between an entity-pair associated with the supply chain evidence candidate sentences. \nFinally, in step 1516 an aggregator aggregates at least some of the supply chain evidence candidates based on the calculated probability to arrive at an aggregate evidence score for a given entity-pair, wherein a Knowledge Graph associated with at least\none company from the entity-pair is updated based on the aggregate evidence score.\n Various features of the system may be implemented in hardware, software, or a combination of hardware and software.  For example, some features of the system may be implemented in one or more computer programs executing on programmable\ncomputers.  Each program may be implemented in a high level procedural or object-oriented programming language to communicate with a computer system or other machine.  Furthermore, each such computer program may be stored on a storage medium such as\nread-only-memory (ROM) readable by a general or special purpose programmable computer or processor, for configuring and operating the computer to perform the functions described above.\n<CENTER><b>* * * * *</b></CENTER>\n<HR>\n   <CENTER>\n   <a href=http://pdfpiw.uspto.gov/.piw?Docid=10303999&homeurl=http%3A%2F%2Fpatft.uspto.gov%2Fnetacgi%2Fnph-Parser%3FSect1%3DPTO2%2526Sect2%3DHITOFF%2526p%3D1%2526u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-bool.html%2526r%3D1%2526f%3DG%2526l%3D50%2526co1%3DAND%2526d%3DPTXT%2526s1%3D20180082183%2526OS%3D%2526RS%3D&PageNum=&Rtype=&SectionNum=&idkey=NONE&Input=View+first+page><img src=\"/netaicon/PTO/image.gif\" alt=\"[Image]\" border=\"0\" valign=\"middle\"></A>\n   <TABLE>\n   <TR><TD align=\"center\"><A href=\"https://certifiedcopycenter.uspto.gov/other/patft/view.html?backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26p%3D1%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-bool.html%26r%3D1%26f%3DG%26l%3D50%26co1%3DAND%26d%3DPTXT%26s1%3D20180082183%26OS%3D&backLabel1=Back%20to%20Document%3A%2010303999\"><IMG border=\"0\" src=\"/netaicon/PTO/cart.gif\" border=\"0\" valign=\"m\niddle\" alt=\"[View Shopping Cart]\"></A>\n   <A href=\"https://certifiedcopycenter.uspto.gov/other/patft/order.html?docNumber=10303999&backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26p%3D1%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-bool.html%26r%3D1%26f%3DG%26l%3D50%26co1%3DAND%26d%3DPTXT%26s1%3D20180082183%26OS%3D&backLabel1=Back%20to%20Document%3A%2010303999\">\n   <IMG border=\"0\" src=\"/netaicon/PTO/order.gif\" valign=\"middle\" alt=\"[Add to Shopping Cart]\"></A>\n   </TD></TR>\n   <TR><TD align=\"center\">\n   <A href=\"#top\"><IMG valign=\"middle\" src=\"/netaicon/PTO/top.gif\" border=\"0\" alt=\"[Top]\"></A>\n   </TD></TR>\n   </TABLE>\n   <A name=\"bottom\"></A>\n   <A href=\"/netahtml/PTO/index.html\"><IMG src=\"/netaicon/PTO/home.gif\" alt=\"[Home]\" border=\"0\" valign=\"middle\"></A>\n   <A href=\"/netahtml/PTO/search-bool.html\"><IMG src=\"/netaicon/PTO/boolean.gif\" alt=\"[Boolean Search]\" border=\"0\" valign=\"middle\"></A>\n   <A href=\"/netahtml/PTO/search-adv.htm\"><IMG border=\"0\" src=\"/netaicon/PTO/manual.gif\" alt=\"[Manual Search]\" valign=\"middle\"></A>\n   <A href=\"/netahtml/PTO/srchnum.htm\"><IMG src=\"/netaicon/PTO/number.gif\" alt=\"[Number Search]\" border=\"0\" valign=\"middle\"></A>\n   <A href=\"/netahtml/PTO/help/help.htm\"><IMG border=\"0\" src=\"/netaicon/PTO/help.gif\" alt=\"[Help]\" valign=\"middle\"></A>\n   </CENTER>\n</BODY>", "application_number": "15609800", "abstract": " Systems and techniques for determining relationships and association\n     significance between entities are disclosed. The systems and techniques\n     automatically identify supply chain relationships between companies based\n     on unstructured text corpora. The system combines Machine Learning models\n     to identify sentences mentioning supply chain between two companies\n     (evidence), and an aggregation layer to take into account the evidence\n     found and assign a confidence score to the relationship between\n     companies.\n", "citations": ["20060117252", "20110282878"], "related": ["15351256", "13107665", "61445236"]}, {"id": "20180082197", "patent_code": "10360507", "patent_name": "Systems, methods, and computer readable media for visualization of\n     semantic information and inference of temporal signals indicating salient\n     associations between life science entities", "year": "2019", "inventor_and_country_data": " Inventors: \nAravamudan; Murali (Andover, MA), Soundararajan; Venkataramanan (Windham, NH), Rajasekharan; Ajit (West Windsor, NJ), Gibson; William (Boston, MA)  ", "description": "BACKGROUND\nOF THE INVENTION\nTechnical Field\n Embodiments of the present disclosure relate to systems, methods, and computer readable media for analyzing underlying relationships in data.\nDescription of the Related Art\n The sophistication in visualization of data--particularly exploiting two dimensional and three dimensional layouts in contrast to linear--has rapidly advanced, facilitating the comprehension of data and underlying relationships, regardless of\nthe data being multi-dimensional or real time.  However, these visualization methods are less effective for rendering data sources, where information is unstructured and where semantic reasoning is required to extract structured information.\n Certain data sources are rich in unstructured and semi-structured information, and generally accessed in distinct siloes across different constituents of the pharmaceutical industry.  For instance, the clinicaltrials.gov website is typically\naccessed by translational medicine and clinical development teams; whereas the Federal adverse event reporting system (FAERS) is usually accessed by pharmacovigilance Research and Development (R&D) scientists and commercial data scientists conducting\nmarket research, etc.\n Currently, many challenges exist in generating synopsis/summary responses to user queries, particularly when the responses require semantic synthesis using structured and unstructured information from disparate sources.  For example, in current\nsystems, \"synopsis-style responses.\" that attempt to graduate beyond just spewing matching results to user query, are simple in nature (e.g., single source, trivial summaries lacking semantic depth in generated responses) allowing automation (e.g., real\ntime scores or election status tables for queries such as \"nfl playoffs\" or \"2012 elections\").\n There is hence a need for a superior visualization system for presenting semantic information.\n Some methods in the prior art rely on identifying seminal associations between entity pairs by studying the growth over time of documents citing both entities (i.e., documents with co-occurrences of the entity pairs).  Such methods often harbor\nminimal predictive power, especially when the number of documents with co-occurrences is still very small (i.e., the knowledge of the association is in the incipient stages).  By definition, these methods can only capture seminal associations at their\nmoment of disclosure and cannot do so before they are published.  Methods such as Citation Index also suffer from the need for long monitoring time-periods before any significant inference can be made.  By looking at the citations for a given\npublication, a positive signal will emerge significantly after the seminal association was disclosed, but such a signal cannot be used for predictions.\n Other methods in the prior art (Google's word2vec) do not provide insights on temporal analytics of entity associations.  Furthermore, generic methods in the prior art for Natural Language Processing (NLP) suffer from multiple drawbacks in their\napplication to any specific industry (such as Life Sciences) due to the inherent difficulties in entity recognition (e.g., drugs, genes, diseases) from unstructured sources caused by industry specific usage of language.\n There is hence a need for a superior system that flags nascent and potentially seminal associations and tracks their salience over time.\nSUMMARY\n In accordance with the disclosed subject matter, systems, methods, and computer readable media, are provided for the visualization of semantic information and inference of temporal signals indicating salient associations between life science\nentities.\n Before explaining example embodiments consistent with the present disclosure in detail, it is to be understood that the disclosure is not limited in its application to the details of constructions and to the arrangements set forth in the\nfollowing description or illustrated in the drawings.  The disclosure is capable of embodiments in addition to those described and is capable of being practiced and carried out in various ways.  Also, it is to be understood that the phraseology and\nterminology employed herein, as well as in the abstract, are for the purpose of description and should not be regarded as limiting.  Furthermore, while the discussion in this disclosure focuses on the field of life science, applications of disclosed\nsystems and methods are not limited to this field.\n A method of detecting an association between semantic entities according to one embodiment of the present disclosure can include identifying semantic entities and associated semantic collections present in one or more knowledge bases, wherein\nthe semantic entities include one or more of single words or multi-word phrases, and the semantic entities of a semantic collection share an entity type; determining a tune period for analysis; dividing the time period into one or more time slices;\ngenerating, for each time slice, a set of word embeddings for the identified semantic entities based on one or more corpora; determining, for each time slice, a first semantic association strength between a first semantic entity input and a second\nsemantic entity input; determining, for each time slice, a second semantic association strength between the first semantic entity input and a plurality of semantic entities in a semantic collection that is associated with the second semantic entity; and\nproviding an output based on the first and second semantic association strengths for the one or more tune slices.\n According to some embodiments, the one or more corpora can include structured data and unstructured data.\n According to some embodiments, the identifying semantic entities can include one or more of: (1) automatic methods of identifying one or more single words or multi-word phrases as semantic entities belonging to semantic collections and (2)\nselecting one or more single words or multi-word phrases forcibly from the one or more knowledge bases.\n According to some embodiments, the one or more single words or multi-word phrases can be selected forcibly from information compiled from a structured database.\n According to some embodiments, the identifying semantic entities can be performed on all text in the one or mom knowledge bases for the time period.\n According to some embodiments, the word embeddings can be generated using one or more of Word2vec, AdaGram, fastText, and Doc2vec.\n According to some embodiments, the word embeddings can be generated for each time slice independently of word embeddings generated for other time slices.\n According to some embodiments, the word embeddings for a time slice can be generated by leveraging word embeddings from a previous time slice.\n According to some embodiments, the plurality of semantic entities associated with the semantic collection that is associated with the second semantic entity may not include the second semantic entity.\n According to some embodiments, the second semantic association strength can be a mean, a median, or a percentile of a set of semantic association strengths between the first semantic entity input and the plurality of semantic entities associated\nwith a semantic collection that is associated with the second semantic entity.\n According to some embodiments, the method can further include detecting an increase in the first semantic association strength of a first time slice relative to the first semantic association strength of a second, subsequent time slice, and\ndetermining whether the increase in the first semantic association strength is statistically significant relative to the corresponding second semantic association.\n According to some embodiments, the statistical significance of the increase can be determined based on a p-value as a measure of statistical significance of the first semantic association strength relative to the corresponding second semantic\nassociation.\n According to some embodiments, the method can further include selecting the first entity input and the second entity input based on a level of co-occurrence between the first entity and the second entity in the one or more knowledge bases.\n According to some embodiments, the level of co-occurrence between the first entity and the second entity is zero.\n According to some embodiments, the method can further include receiving the first entity input and tire second entity input from a user.\n According to some embodiments, the method can further include determining, for each time slice, a count of documents present in the one or more corpora containing the first entity and the second entity; and determining a time difference between\n(1) a first date associated with an increase in the first semantic association strength for a first time slice relative to the first semantic association strength for a second, subsequent time slice and (2) a second date associated with an increase in a\ncount of documents containing the first entity and the second entity for a third time slice relative to a count of documents containing the first entity and the second entity for a fourth time slice.\n According to some embodiments, the method, can further include detecting the increase in the count of documents containing the first entity and the second entity based on a slope of a curve in a fixed axis, wherein the curve is based on the time\nperiod on an x-axis of the curve and the count of documents on a y-axis of the curve.\n According to some embodiments, the method can further include detecting the second increase in the count of documents containing the first entity and the second entity based on a document count threshold.\n According to some embodiments, each of the first entity and the second entity can be at least one of the following entity types: bio-molecules, bio-entities, diseases, adverse events, phenotypes, companies, institutions, universities, hospitals,\npeople, drugs, medical instruments, and medical procedures.\n According to some embodiments, the output, can enable a user device to display a graph line that is created by plotting each of the first semantic association strengths for each of the time slices over the time period.\n According to some embodiments, the output can enable a user device to display a graph line that is created by plotting each of mean second semantic association strengths for each of the tune slices over the time period.\n According to some embodiments, the output can enable a user device to display a graph line that is created by plotting a count of documents present in the one or more corpora containing the first entity and the second entity for each of the time\nslices over the time period.\n A method of generating semantic information between entities according to one embodiment of the present disclosure can include identifying a plurality of semantic entities in one or more corpora, wherein the semantic entities include one or more\nof single words or multi-word phrases; identifying a plurality of semantic entity types in the one or more corpora; associating one or more semantic entity types with the semantic entities of the plurality of semantic entities; generating word embeddings\nfor the plurality of semantic entities; determining one or more semantic association scores between semantic entities from the plurality of semantic entities based on the word embeddings; receiving a query term; generating a first list of resulting\nsemantic entities associated with the query term based on the one or more semantic association scores; generating a second list of semantic entity collections based on the semantic entity types associated with the semantic entities of the first list of\nresulting semantic entities, wherein each semantic entity collection from the second list is associated with a semantic entity type, and providing an output based on the second list of semantic entity collections.\n According to some embodiments, the one or more corpora, can include structured data and unstructured data.\n According to some embodiments, the plurality of semantic entity types can be identified based on one or more of a structured database, a custom list of entity types, an output from a neural network, an output from supervised machine learning, or\nan output from unsupervised machine learning.\n According to some embodiments, the neural network architecture can be one or more of a recurrent neural network (RNN) or a Long Short Term Memory (LSTM).\n According to some embodiments/the word embeddings can be generated using one or more of Word2vec, AdaGram, fastText, and Doc2vec.\n According to some embodiments, the generating the second list of semantic entity collections based on the semantic entity types associated with the semantic entities of the first list of resulting semantic entities can include basing the\ngeneration on only those resulting semantic entities that satisfy one or more of the following conditions: a maximum number of resulting semantic entities being associated with a potential semantic entity type; a minimum semantic association score for a\nresulting semantic entity; a minimum number of occurrences of the resulting semantic entity in the one or more corpora; the resulting semantic entity occurring in a minimum number of documents of the one or more corpora, a minimum number of\nco-occurrences of the query term and the resulting semantic entity; a maximum number of co-occurrences of the query term and the resulting semantic entity; a minimum number of documents of the one or more corpora where the query term and the resulting\nsemantic entity co-occur; and a maximum number of documents of the one or more corpora where the query term and the resulting semantic entity co-occur.\n According to some embodiments, the generating the second list of semantic entity collections can include limiting a number of semantic entity collections in the second list to a maximum number.\n According to some embodiments, the generating the second list of semantic entity collections can include requiring a minimum number of semantic entities in each of the semantic entity collections of the second list.\n According to some embodiments, the generating the second list of semantic entity collections can include requiring a minimum semantic association score for each of the semantic entities in each of the semantic entity collections of the second\nlist.\n According to some embodiments, the generating the second list of semantic entity collections can be further based on the one or more resulting semantic entities being associated with selected semantic entity collections.\n According to some embodiments, the output can enable a user device to list one or more of the resulting semantic entities from the first list, and one or more of the semantic entity collections from the second list.\n A method of generating semantic information between entities according to one embodiment of the present disclosure can include identifying a plurality of semantic entities in one or more corpora, wherein the semantic entities include one or more\nof single words or multi-word phrases; generating worn embeddings for the plurality of semantic entities, wherein at least one of the semantic entities is a multi-meaning semantic entity having a plurality of meanings and a corresponding plurality of\nword embeddings, wherein each meaning is associated with a corresponding word embeddings; determining one or more semantic association scores between semantic entities from the plurality of semantic entities based on the word embeddings; receiving a\nquery term; determining if the query term corresponds to a semantic entity that is associated with a plurality of embeddings, when the query term corresponds to a semantic entity that is associated with a plurality of embeddings, generating a set of\nlists, each list containing semantic entities for each embedding of the plurality of embeddings, wherein a semantic entity is included in one or more of the lists based on a comparison of the one or more semantic association scores; and when the query\nterm corresponds to a semantic entity that is associated with a plurality of embeddings, providing an output based on the set of lists.\n According to some embodiments, the one or mom corpora can include structured data and unstructured data.\n According to some embodiments, the method can further include determining a set of measures that measure the occurrences of each meaning of the plurality of meanings of the multi-meaning semantic entity in the one or more corpora.\n According to some embodiments, the measure can be a count of the number of co-occurrences, in one or more documents of the one or more corpora, of the particular meaning of the multi-meaning semantic entity with one or mom of the semantic\nentities of the plurality of semantic entities.\n According to some embodiments, the measure can be a count of documents of the one or more corpora in which the particular meaning of the multi-meaning semantic entity co-occurs with one or mom of the semantic entities of the plurality of\nsemantic entities.\n According to some embodiments, the word embeddings can be generated using Adaptive Skip-gram (AdaGram).\n According to some embodiments, the method further includes generating a percentage for each list in the set of lists, wherein the percentage for each list is calculated by dividing a number of semantic entities in the corresponding list by a\ntotal number of entities in ail of the lists in the set.\n According to some embodiments, the method can further include associating a semantic entity type with one or more lists in the set of lists by analyzing one or more semantic entity types associated with, the semantic entities in the\ncorresponding list.\n According to some embodiments, the analyzing the one or more semantic entity types associated with the semantic entities m the corresponding list can include determining a semantic entity type that is most often associated with semantic entities\nin the corresponding list.\n According to some embodiments, the output can enable a user device to display the set of lists and the resulting semantic entities in each list in the set of lists.\n A method of generating semantic information between entities according to one embodiment of the present disclosure can include identifying a plurality of semantic entities in one or more corpora, wherein the semantic entities include one or more\nof single words or multi-word phrases; identifying a plurality of semantic entity types in the one or more corpora; associating at least one semantic entity type with the semantic entities of the plurality of semantic entities, generating word embeddings\nfor the plurality of semantic entities; determining one or more semantic association scores between semantic entities from the plurality of semantic entities; receiving a query term and an entity type input; determining a query term entity type\nassociated with the query term; generating a first list of resulting semantic entities associated with the query term based on the one or more semantic association scores, wherein the resulting semantic entities from the first list are associated with\nthe same semantic entity type as the query term entity type; generating a second list of resulting semantic entities associated with the query terms based on the one or more semantic association scores, wherein the resulting semantic entities from the\nsecond list are associated with the entity type input; generating a third list of semantic association scores, wherein the third list includes semantic association scores between each of the resulting semantic entities from the first list and each of the\nresulting semantic entities from the second list; and providing an output based on the first list, the second list, and the third list.\n According to some embodiments, the one or more corpora, can include structured data and unstructured data.\n According to some embodiments, the plurality of semantic entity types can be identified based on one or more of a structured database, a custom list of entity types, an output from a neural network, an output from supervised machine learning, or\nan output from unsupervised machine learning.\n According to some embodiments, the neural network architecture can be one or more of: a recurrent neural network (RNN) or a Long Short Term Memory (LSTM).\n According to some embodiments, the word embeddings can be generated using one or more of Word2vec, AdaGram, fastText and Doc2vec.\n According to some embodiments, the generating the first list can include limiting a number of the resulting semantic entities to a maximum count.\n According to some embodiments, the generating the first list can be further based on requiring a semantic association score of each of the resulting semantic entities to be greater than a minimum semantic association score.\n According to some embodiments, the generating the second list can include limiting a number of the resulting semantic entities to a maximum count.\n According to some embodiments, the generating the second list can be further based on requiring a semantic association score of each of the resulting semantic entities to be greater than a minimum semantic association score.\n According to some embodiments, the output can enable a user device to generate a heatmap with the resulting semantic entities from the first list on a y-axis, the resulting semantic entities from the second list on an x-axis, and each of the\nsemantic association scores from the third list being represented as a color or a shade of a color, wherein the color or the shade of a color maps to a semantic association score.\n A method according to one embodiment of the present disclosure can include identifying semantic entities and associated semantic collections present in one or more knowledge bases, wherein the semantic entities include one or more of single\nwords or multi-word phrases, and the semantic entities of a semantic collection share an entity type; determining a time period for analysis; dividing the time period into one or more time slices; generating, for each time slice, a set of word embeddings\nfor the identified semantic entities based on one or more corpora; characterizing a temporal semantic association between a first semantic entity input and a second semantic entity input by performing the steps of determining, for each time slice, a\nfirst semantic association strength between the first semantic entity input and the second semantic entity input; determining, for each time slice, a second semantic association strength between the first semantic entity input and a plurality of semantic\nentities associated with a semantic collection that is associated with the second semantic entity; determining, for each time slice, a probability measure relating the first semantic association strength with the second semantic association strength;\nassigning a time value to each time slice, determining a sequence of two-dimensional points by associating, for each time slice, the assigned time value for the time slice with the probability measure for the time slice, wherein the sequence is ordered\nby increasing time values; fitting a curve to the ordered sequence, extracting characteristics of the curve fit, wherein the characteristics include one or more of a time of increase value representing the time value at which a statistically significant\nmagnitude change of the probability measure occurs, a probability saturation value representing the maximum value of the probability measure, or an area under the curve value; providing the characteristics of the curve fit from the characterizing the\ntemporal semantic association between the first semantic entity input and the second semantic entity input.\n According to some embodiments, the method can further include: receiving the first semantic entity input from a user; receiving the second semantic entity input from a user as a set of at least two second semantic entities; performing the steps\nof the characterizing the temporal semantic association between the first semantic entity input and the second semantic entity input for each of the second semantic entities of the set, and displaying the characteristics of the curve fits for the first\nsemantic entity input and at least two of the second semantic entity inputs of the set.\n According to some embodiments, the displaying the characteristics of the curve fits can include displaying the time of increase value, the probability saturation value, and the area under the curve value for each of the first semantic entity\ninput and the second semantic entity input temporal semantic associations as a bubble plot, wherein a bubble placement along an x-axis of the bubble plot corresponds to the time of increase value, a bubble placement along a y-axis of the bubble plot\ncorresponding to the probability of saturation value, and a size of a bubble corresponds to the area under the curve value.\n According to some embodiments, the method can further include: displaying an identity of the first semantic entity input and an identity of the second semantic entity input associated with a bubble in proximity to the bubble.\n According to some embodiments, the displaying the characteristics of the curve fits can include displaying only the characteristics of the curve fits for which the probability saturation value satisfies a probability threshold value.\n According to some embodiments, the displaying the characteristics of the curve fits can include displaying only the characteristics of the curve fits for temporal semantic associations between first semantic entity inputs and second semantic\nentity inputs having a count of co-occurrence in documents of the one or mow corpora that satisfy a co-occurrence threshold value.\n According to some embodiments, the method cars further include: determining a count of co-occurrence in documents of the one or more corpora of the first semantic entity input and at least two of the second semantic entity inputs of the set.\n According to some embodiments, the displaying the characteristics of the curve fits can include displaying the time of increase value, the probability saturation value, and the count of co-occurrence for each of the first semantic entity input\nand the second semantic entity input temporal semantic associations as a bubble plot, wherein a bubble placement along an x-axis of the bubble plot corresponds to the time of increase value, a bubble placement along a y-axis of the bubble plot\ncorresponding to the probability of saturation value, and a size of a bubble corresponds to the count of co-occurrence.\n According to some embodiments, the probability measure relating the first semantic association strength with the second semantic association strength can be a negative logarithm of a p-value, wherein a relatively higher probability measure\nindicates the first semantic association strength is more statistically significant versus the second semantic association strength as compared to a relatively lower probability measure that indicates the first semantic association strength is not more\nstatistically significant versus the second semantic association strength.\n According to some embodiments, the fitting the curve to the oak red sequence can include fitting a sigmoid curve to the ordered sequence according to the equation:\n .function.  ##EQU00001## where:\n y values are the probability measures of the sequence; and\n x values are the tune values of the sequence.\n According to some embodiments, any of the steps or actions disclosed herein can be performed by a server.  In some embodiments, the server can include a memory that stores a module.  In some embodiments, the server includes a processor\nconfigured to run the module stored in the memory that is configured to cause the processor to perform any of the steps or actions disclosed herein.  According to some embodiments, a non-transitory computer readable medium can have executable\ninstructions operable to cause a server to perform any of the steps or actions disclosed herein.\n Any of the above embodiments or aspects can be combined with other embodiments and/or aspects set forth herein and remain within the scope of the invention. BRIEF DESCRIPTION OF THE DRAWINGS\n Various objects, features, and advantages of the disclosed subject matter can be more folly appreciated with reference to the following detailed description of the disclosed subject matter when considered m connection with the following\ndrawings, in which like reference numerals identify like elements.\n While multiple embodiments are disclosed, still other embodiments of the present disclosure will become apparent to those skilled in the art from the following detailed description, which shows and describes illustrative embodiments of the\ndisclosure.  Accordingly, the drawings and detailed description are to be regarded as illustrative in nature and not restrictive.\n FIG. 1 illustrates a system architecture in accordance with some embodiments of the present disclosure.\n FIG. 2A illustrates one method of providing semantic responses to queries.\n FIG. 2B illustrates a rendition of an interface enabled by synthesizing data from multiple pathways in accordance with some embodiments of the present disclosure.\n FIG. 3A illustrates one method of providing semantic responses to queries.\n FIG. 3B illustrates a rendition of an interface enabled by synthesizing data from multiple pathways in accordance with some embodiments of the present disclosure.\n FIG. 4 illustrates a two dimensional matrix of data generated by the response synthesizer 112 (FIG. 1) in accordance with some embodiments of the present disclosure.\n FIGS. 5A-B illustrate knowledge graphs that relate to diverse entities, as highlighted for the \"neighborhoods\" of the user-supplied exemplary queries in accordance with some embodiments of the present disclosure.\n FIGS. 6A-6D illustrate examples showing how knowledge graphs relate diverse entities, as highlighted for the \"analogies\" in accordance with some embodiments of the present disclosure.\n FIG. 7 illustrates a bull's eye view (an alternate 2D view) that can be a radial representation of the \"symbolic\" temporal phases in accordance with some embodiments of the present disclosure.\n FIG. 8 illustrates a two dimensional matrix of data generated by the response synthesizer 112 (FIG. 1) in accordance with some embodiments of the present disclosure.\n FIG. 9 illustrates output from a Bio-Knowledge graph queried for the exemplary phrase \"Remyelination\" followed by application of Entity Recognition techniques in accordance with some embodiments of the present disclosure.\n FIG. 10 illustrates output from a Bio-Knowledge graph that can enable identifying disease indications that are closely related to any real world phenotype query supplied by the user in accordance with some embodiments of the present disclosure.\n FIG. 11 illustrates entity distribution for a search input where the neighborhood nodes change with time in accordance with some embodiments of the present disclosure.\n FIG. 12 illustrates tin instance of output the temporal progression of concepts across entity classes in accordance with some embodiments of the present disclosure.\n FIG. 13 illustrates the creation of an instance of Knowledge graph subsets at an instant of time in accordance with some embodiments of the present disclosure.\n FIG. 14 illustrates the capturing of temporal progression of entities and consequently entity distribution over time in Knowledge Graph (\"KG\") in accordance with some embodiments of the present disclosure.\n FIG. 15 illustrates exemplary entity collections in accordance with some embodiments of the present disclosure.\n FIG. 16 illustrates a cosine distance probability density function (PDF) graph in accordance with some embodiments of the present disclosure.\n FIG. 17 illustrates a flow chart for tempo ml analysis in accordance with some embodiments of the present disclosure.\n FIG. 18 illustrates a density distribution of semantic association strength for two genes against all disease entities in accordance with some embodiments of the present disclosure.\n FIG. 19 illustrates a process for evaluating statistical background model and presenting summary statistics to user in accordance with some embodiments of the present disclosure.\n FIG. 20 illustrates an example of summary statistics overlayed with temporal analysis in accordance with some embodiments of the present disclosure.\n FIG. 21 illustrates two histograms generated from a random set of vectors in accordance with some embodiments of the present disclosure.\n FIGS. 22A-B illustrates temporal analysis of bona-fide Life Sciences entity pairs in accordance with some embodiments of the present disclosure.\n FIG. 23 illustrates PTEN-KRAS gene-gene temporal analysis in accordance with some embodiments of the present disclosure.\n FIG. 24 illustrates AML1 (RUNX1)-FLT3 gene-gene association temporal analysis in accordance with some embodiments of the present disclosure.\n FIG. 25 illustrates Atypical Hemolytic Uremic Syndrome-CFH (disease-gene) Temporal Analysis in accordance with some embodiments of the present disclosure.\n FIG. 26 illustrates PCSK9-LDLR (Gene-gene) temporal analysis in accordance with some embodiments of the present disclosure.\n FIG. 27 illustrates PCSK9-LDLR (Gene-gene) temporal analysis in accordance with some embodiments of the present disclosure.\n FIG. 28 illustrates a relationship between OT Score and cosine distance (semantic association score) in accordance with some embodiments of the present disclosure.\n FIG. 29 illustrates a negative control graphical representation of temporal statistical inference for a non-significant gene-disease interaction in accordance with some embodiments of the present disclosure.\n FIG. 30 illustrates a positive control graphical representation of temporal statistical inference for a significant gene-disease interaction in accordance with some embodiments of the present disclosure.\n FIG. 31 illustrates an exemplary neighborhood sense interface in accordance with some embodiments of the present disclosure.\n FIG. 32 illustrates an exemplary neighborhood sense interface in accordance with some embodiments of the present disclosure.\n FIG. 33 illustrates an exemplary knowledge diagram interface in accordance with some embodiments of the present disclosure.\n FIG. 34 illustrates an exemplary knowledge diagram interface in accordance with some embodiments of the present disclosure.\n FIG. 35 illustrates an exemplary information box in accordance with some embodiments of the present disclosure.\n FIG. 36 illustrates an exemplary information box in accordance with some embodiments of the present disclosure.\n FIG. 37 illustrates an exemplary knowledge diagram interface in accordance with some embodiments of the present disclosure.\n FIG. 38 illustrates an exemplary knowledge diagram interface in accordance with some embodiments of the present disclosure.\n FIG. 39 illustrates an exemplary knowledge diagram interface in accordance with some embodiments of the present disclosure.\n FIG. 40 illustrates an exemplary knowledge diagram interface in accordance with some embodiments of the present disclosure.\n FIG. 41 illustrates an exemplary knowledge diagram interface in accordance with some embodiments of the present disclosure.\n FIG. 42 illustrates an exemplary knowledge diagram interface in accordance with some embodiments of the present disclosure.\n FIGS. 43-44 illustrate exemplary knowledge diagram interfaces in accordance with some embodiments of the present disclosure.\n FIG. 45 illustrate an exemplary knowledge diagram interface in accordance with some embodiments of the present disclosure.\n FIG. 46 illustrates an exemplary heatmap in accordance with some embodiments of the present disclosure.\n FIG. 47 illustrates an exemplary heatmap in accordance with souse embodiments of the present disclosure.\n FIG. 48 illustrates an exemplary heatmap in accordance with some embodiments of the present disclosure.\n FIG. 49 illustrates an exemplary user interface for a temporal analysis graph in accordance with some embodiments of the present disclosure.\n FIG. 50 illustrates an exemplary knowledge graph interface with a temporal analysis graph in accordance with some embodiments of the present disclosure.\n FIG. 51 illustrates an exemplary knowledge graph interface with a temporal analysis graph in accordance with some embodiments of the present disclosure.\n FIGS. 52-56 illustrate neighborhood, sense diagrams for the entity \"Rho\" that is associated with five different entity types in accordance with some embodiments of the present disclosure.\n FIG. 57 illustrates a data flow in accordance with some embodiments of the present disclosure.\n FIG. 58 illustrates a control/data flow when a user makes a query request to a neighborhood app in accordance with some embodiments of the present disclosure.\n FIG. 59 illustrates an exemplary knowledge graph interface with a temporal analysis graph in accordance with some embodiments of the present disclosure.\nDETAILED DESCRIPTION\n Various other modifications and additions cats be made to the exemplary embodiments discussed without departing from the scope of the present disclosure.  For example, while the embodiments described above refer to particular features or\nparticular steps, the scope of this disclosure also includes embodiments having different combinations of features or steps, and embodiments that do not include all of the above described features or steps.\n The present disclosure describes systems, methods, and computer readable media to overcome many current challenges in generating synopsis/summary responses to user queries, particularly when the responses require semantic synthesis using\nstructured and unstructured information from disparate sources.  In some embodiments, neural networks and/or language models can be used to solve the following task, which at present, is virtually impossible to scale: visualization of semantic\ninformation that is inferred from structured and/or unstructured data, where, optionally, some or all of the data can be aggregated in real time from disparate sources.\n Embodiments of the present disclosure can be applied to various industries and domains.  However, for illustrative purposes, the present disclosure focuses on the healthcare/medical space.  In some embodiments, the following terms can be used\ninterchangeably: \"entity\" and \"token.\" in some embodiments, the following terms can also be used interchangeably: \"entity class\" and \"entity type.\" Moreover, in some embodiments, the following terms can be used interchangeably: \"embeddings\" and\n\"vectors.\" Also, the phrase \"word embeddings\" is used to collectively refer to character, word, paragraph, sentence, and/or document embeddings, unless explicitly specified.  In some embodiments, the following terms can be used interchangeably: \"semantic\nassociation strength,\" \"semantic association score,\" and \"cosine distance.\"\n Disclosed systems and methods can identify semantically related entities using word/document/paragraph/sentence embeddings generated from a corpus when the semantically related entities do not co-occur, where the embeddings can be generated\nusing known techniques, but where terms/phrases are mapped to entities and entity classes.  The similarity (e.g., distance between) in the mappings between each of the non-co-occurring entities and the entities and classes to which they are snapped\nenable the discovery of semantically related entities that are not present as co-occurring concepts in the original corpus.  In other words, the similarity of a first mapping to a second mapping enables the discovery of the related entities in which the\nfirst entity mapping relates the first entity to a first set of entities and classes, and the second mapping relates the second entity to a second set of entities and classes.\n Neural networks can be used to extract semantic information from unstructured data sources towards creating structured aggregates.  In some embodiments, sequence models with state maintenance cars be used within the neural networks for this\npurpose.  In some embodiments, candidate data sets that can power spatial visualizations of data can be created with minimum human validation.\n Disclosed systems and methods of synthesizing multi-dimensional summaries for search queries can include labeled entities and unlabeled terms/phrases.  The search input can be one or more unlabeled entities.  The search query can be\nterms/phrases or a natural language query that can include terms/phrases.  The terms can be harvested from a sequence of queries as in a dialog exchange for disambiguation.\n The labeled entities in the neighborhood of the search input can be used to create the orthogonal dimensions (e.g., rows and columns) of the synthesized summaries.  The synthesizing can be performed by examining the entity distribution in the\nword/document/paragraph/sentence embedding neighborhood of the input terms/phrases and using dial distribution to pick the candidate entities for the summary generation.  The entity distribution, which captures entity/neighborhood relationships, can be a\ngeneral distribution obtained from a universal corpus or a context specific distribution where context is inferred either implicitly or explicitly.\n Language models using word vectors/embeddings (or character vectors composing word embedding) can be used--instead of traditional n-gram models, where words are atomic entities--to establish word embeddings as the de facto representation of\nwords for machine learning models.  Similar to the representation of words as vectors generalizing well beyond capturing mere co-occurrences, the representation of concepts and relationships between concepts as vectors can generalize beyond the explicit\nrelationships encoded in the space from which they are extracted, facilitating the construction of a semantic knowledge graph with concept/relationship embeddings for broader and deeper understanding of data.\n One or more disparate data sources can be aggregated by some embodiments into an exemplary \"Core Corpus.\" For example, one or more data sources from the following table can be used:\n TABLE-US-00001 Resource Drugs@FDA FDA Adverse Event Reporting System (FAERS) Clinicaltrials.gov Wikipedia & DBpedia Pubmed Compounds (NCI, Clinical trails, Drugbank, FDA, Pubchem) Companies (Crunchbase, Linkedin, SBIR, Bloomberg) Structured\nontologies (Hugo, KEGG, MeSH, OMIM)\n According to some embodiments, the aggregation can be performed at various frequencies that can range from real time or substantially real time (e.g., through feeds) to any tune period that is not real time or substantially real time (e.g.,\nseconds, minutes, hours, days, weeks, months, years).  In some embodiments, such a frequency can be based on each resource site's crawl frequency policy, where embodiments of the present disclosure can honor such a policy.  In some embodiments, the\nfrequency can be set differently for one or more of disparate data sources.  In some embodiments, the frequency can be set either statically or dynamically.\n In the healthcare/drag industry, each drug company can have a synopsis of its drugs in various stages of development.  In some embodiments, an aggregated and/or synthesized semantic summary that can automatically cluster information, such as\ndrug classes and disease categories, across different companies requires not only semantic understanding of entities pertaining to this space, but also gleaned and/or synthesized information from disparate structured and unstructured sources (e.g.,\nstructured company sites, quasi structured sites such as clinical trials, unstructured sources like Pubmed).  In some embodiments, an aggregated and/or synthesized semantic summary can be created to improve the scalability and capability to address a\nbroad class of semantic queries that can benefit users to make decisions quickly and eliminate the need to spend a long time (e.g., several hours spanning days if not weeks) to create such a synthesis from disparate structured and unstructured sources.\n Embodiments of the present disclosure can also solve other challenges in extracting semantic information from disparate structured and unstructured sources.  Examples of these challenges are described below.\n First, there can be \"entity class\" inadequacy in human curated ontologies/information repositories to generate semantic responses to the wide range of user searches.  While curated ontologies abound in healthcare industry, entities or entity\nclasses can be missed, causing degenerate response of lexically matched results of user input to documents.  For example, a search term, such as \"remyelination,\" can degenerate to a lexical search response, unless a curated semantic result is constructed\nfor the input.  This is because remyelination is unlikely to be an entity that falls under typical entity types such as indications, diseases, drugs, etc. Even if the response of a word embedding neighborhood is used, this can still be inadequate because\nthe neighborhood of an entity can be a mixed grab-bag of entity types.  Although using the response of a word embedding neighborhood can be marginally better than the result generated from lexical responses, it would still not even be close to a result\nproduced by embodiments of the present disclosure using a semantic synthesis that best matches user intent.\n Second, disambiguation of entities can be required for certain terms.  For example, a lexical search can produce a misleading result for the term \"EGFR,\" which can stand for the gene \"Epidermal Growth Factor Receptor\" or the laboratory test\n\"Estimated Glomerular Filtration Rate.\" This common user query can result in erroneous hits in purely lexical systems.  In some embodiments, this problem can be solved by using a semantic bio-knowledge graph to implicitly disambiguate when context is\npresent, or explicitly disambiguate the entity when no context is present.\n Third, there can be a need to maximize the unambiguous recognition and classification of single word and multiword (phrase) entities in an unstructured source.  The performance of named entity recognition from unstructured data using sequence\nlearning neural net models (e.g., Recurrent Neural Net (RNN) variants in isolation or in combination with Conditional Random Fields (CRF)) can be lacking.  In some embodiments, the performance leveraging off semantic similarities latent in word\nembedding, particularly from semantically related information sources, can be unproved.\n Fourth, there can be a need to extract specific semantic information of interest latent in a structured source.  The organization of data in a structured repository may not lend itself to extracting semantic information across fields and keys in\nthe structured repository.  For example, FAERS (FDA Adverse Event Reporting System) includes structured information on adverse events and medication error reports.  Popular measures that are computed from this repository do not capture some of the\ninsightful latent information due to the organization of data.\n Examples of data sources that are commonly used by various siloes of the pharmaceutical ecosystem, and that can be used by embodiments of the present disclosure are described below.\n Drugs@FDA (www.accessdata.fda.gov/scripts/cder/drugsatfda/).  Drugs@FDA includes over 100,000 current FDA approved labels, older labels, approval letters, reviews (scientific analyses), and information for patients (1998-present).  This largely\nunstructured knowledgebase includes all prescription and over-the-counter human drugs and therapeutic biologicals currently approved for sale in the United States, in addition to all discontinued drugs and Chemical Type 6 approvals.  The following\ntherapeutic biological products are included: monoclonal antibodies, cytokines, growth factors, enzymes, immunomodulators, thrombolytics, proteins intended for therapeutic use that are extracted from animals or microorganisms including recombinant\nversions of these products (except clotting factors), and non-vaccine therapeutic immunotherapies.  The information in Drugs@FDA comes from both the FDA Orange Book (Approved Drug Products with Therapeutic Equivalence Evaluations) and the Center-wide\nOracle-based Management Information System (COMIS) that maintains investigational new drug applications (INDs), new drug applications (NDAs), and abbreviated NDAs (ANDAs).  Some embodiments of the present disclosure can also utilize the FDA's Structured\nProduct Labeling (SPL) resource that includes information on approved products (www.fda.gov/ForIndustry/DataStandards/StructuredProductLabeling), such a dosage forms and drug classes (https:/dailymed.nlm.nib.gov/dailymed).\n Federal Adverse Event Reporting System (https.//open.fda.gov/data/faers/): FDA Adverse Event Reporting System (FAERS) is a database of over 6.1 million reports that includes information on adverse events and medication errors submitted to the\nFDA.  The database is designed to support the FDA's post-marketing safety surveillance program for drug and therapeutic biologic products.  The largely semi-structured and structured FAERS database adheres to the international safety reporting guidance\nissued by the International Conference on Harmonisation (ICH E2B), with Adverse events and medication errors coded to terms in the Medical Dictionary for Regulatory Activities (MedDRA) terminology.  The FAERS includes adverse event reports from\nhealthcare professionals (such as physicians, pharmacists, nurses and others), consumers (such as patients, family members, lawyers and others), and product manufacturers as specified by FDA regulations.\n Clinicaltrials.gov (https://clinicaltrials.gov/): ClinicalTrials.gov is a web-based resource that provides landing pages for 220,000+ clinical trials being conducted, completed, or terminated across all 50 states of the United States and 192\ncountries.  These largely unstructured and semi-structured resource includes information on publicly and privately supported clinical studies on a wide range of diseases and conditions.  The resource is maintained by the National Library of Medicine\n(NLM) at the National Institutes of Health (NIH).  The Information is provided and updated by the sponsor of the clinical trial, or the principal investigator (PI) of the clinical study.  Studies are generally submitted when they begin (register), and\nthe information on the site is updated throughout the study.  In some cases, results of the study are also included after the study ends, also in the form of unstructured text and semi-structured tables.  Each ClinicalTrials.gov record presents summary\ninformation about a study protocol and includes the following: Disease or condition: Intervention (for example, the medical product, behavior, or procedure being studied); Title, description, and design of the study.  Requirements for participation\n(eligibility criteria); Locations where the study is being conducted; Contact information for the study locations; Links to relevant information on other health Web sites, such as NLM's Medline Plus for patient health information and PubMed for citations\nand abstracts of scholarly articles in the field of medicine.  Some records also include information on the results of the study, such as the following: description of study participants (the number of participants starting and completing the study and\ntheir demographic data); outcomes of the study, and summary of adverse events experienced by study participants.  The foil history of the changes made to any clinical trial record are available via the ClinicalTrials.gov archive.\n EDGAR--SEC Filings (www.sec.gov/edgar/searchedgar/companysearch.html): The Electronic Data Gathering, Analysis, and Retrieval (EDGAR) system provides 21 million filings required by the U.S.  Securities and Exchange Commission (SEC).  The EDGAR\nperforms automated collection, validation, indexing, acceptance, and forwarding of all submissions by companies and other entities.  Actual annual reports to shareholders (except in the case of mutual fund companies) need not be submitted on EDGAR,\nalthough some companies do so voluntarily.  However, the annual report on Form 10-K or form 10-KSB, which contains much of the same information, is required to be filed on EDGAR.  There are over 3000 filings every day on the EDGAR database available in a\nlargely unstructured and semi-structured form.\n Wikipedia & DBpedia (www.wikipedia.org/ and http://wiki.dbpedia.org/): Wikipedia is a web-based, free-content, openly-editable Encyclopedia with about 5M+ English articles contributed by about 70,000+ active contributors.  DBpedia is a\ncrowd-sourced community effort to extract structured information from Wikipedia.  The DBpedia knowledgebase describes about 4.58 million things, out of which about 4.22 million are classified in a consistent ontology, including about 1,445,000 persons,\nabout 735,000 places, about 411,000 creative works, about 241,000 organizations (including about 58,000 companies and about 49,000 educational institutions), about 251,000 species, and about 6,000 diseases.\n Pubmed abstracts (www.ncbl.nlm.nthgov/pubmed/): PubMed includes more than 26 million citations for biomedical literature from MEDLINE, life science journals, and online books.  Citations may include links to lull-text content from PubMed Central\nand, publisher web sites.  This includes structured abstracts (www.nlm.nih.gov/bsd/policy/structured_abstracts.html) that use the IMRAD format (introduction.  Methods, Results, and Discussion) for scientific studies and the CONSORT (Consolidated\nStandards of Reporting Trials) format for randomized controlled trials (RCTs).\n Compounds and drug entities: NCI--http://www.cancer.gov/; Clinicaltrials.gov-https://clinicaltrials.gov/; Drugbank--http://wwwdrugbankca/; FDA--http://www.fdagov/; Pubchem--https://pubchem.ncbi.nlm.nih.gov/.\n Companies: Crunchbase--https://www.cruncbhase.com/; Linkedin--https://www.linkedin.com/; SBIR--https://www.sbir.gov/; Bloomberg--http://www.bloomberg.com/.\n Human-curated ontologies: Hugo--http://www genenames.org/; KEGG--http://www.genome jp/kegg/kegg1.html; MeSH--http;//www.ncbi.nlm.nih.gov/mesh: OMIM--http://www.omim.org/.\n In some embodiments, a semantic search system can provide \"summary answers\" to a range of queries about the \"temporal status\" of drug or therapeutic entities.  The temporal status can indicate the stage of development (e.g., preclinical, phase\n1, phase 2, phase 3, marketed) of the drug.  In some embodiments, the temporal status can be automatically mapped to ail \"entity\" and/or \"intersection of one or more entities\" in a semantic bio-knowledge graph (e.g., as shown in FIG. 8).  Non-limiting\nexamples of entities can include: drug (e.g., Gleevec), company/organization (e.g., Roche, Dana Farber), indication (e.g., Non Small Cell Lung Cancer), phenotype (e.g., Remyelination, Angiogenesis), bio-molecular features including gene mutation (e.g.,\nEGER T790M in Lung cancer tumor), RNA or protein expression (e.g., PD-L1 overexpression in cancerous tumors; PLP1 in the demyelinating rare neurological disease PMD), signaling pathway (e.g., JAK/STAT pathway in blood cancer and autoimmune diseases such\nas Rheumatoid.  Arthritis), gene fusion (e.g., BCR/ABL fusion or Philadelphia Chromosome in Leukemia), Copy Number Alterations (e.g., BRCA in breast cancer, APC in colorectal cancer), and \"therapeutic modality\" (e.g., small molecule, antibody,\nimmunotherapy, gene therapy, radiation therapy).\n FIG. 1 illustrates a system architecture in accordance with some embodiments of the present disclosure.  Embodiments of the present disclosure can advantageously extract key data prerequisites (e.g., entities, them attributes, entity types,\nlogical and temporal sequence relationships) from different pathways (e.g., 101a and 102a) and consolidate the key data prerequisites in the system store 114 with the pathways taking advantage of the other.  By using these extraction and consolidation\nmethods, embodiments of the present disclosure can automate synthetic responses.\n Information can generally be scattered across both structured and unstructured data.  For example, temporal sequence of drugs administered as first line and second line therapies are embedded in sentences in clinical trials.  Embodiments of the\npresent disclosure can obviate the need to manually read through the sentences in clinical trials to construct temporal sequences.\n Input data to the system can be structured data 101, semi-structured data 117, and/or unstructured data 102.  In some embodiments, structured data 101 can be in the form of entity tuples.  For example, structured data can include a key-value\ntriple, where the key is \"disease\" and the value is \"cancer.\" In some embodiments, unstructured data 102 can include information in the form of phrases or sentences.  For example, unstructured data can include the phrase \"I have Parkinson's disease and I\ntook drug X.\" In some embodiments, semi-structured data 117 can include both structured data and unstructured data.  For example, semi-structured data can be hierarchical/flat structure of key/value tuples, where some of the values are unstructured.\n In some embodiments, structured data 101 can pass through 101a to a structured data extraction classifier 103 that can identify entity types and their attributes (entities) unambiguously with available context.  For example, if the structured\ndata is \"disease=cancer,\" the structured data extraction classifier 103 can identify that the entity type is \"disease\" and that the entity is \"cancer.\" In some embodiments, the structured data classifier 103 can use a supervised learning model, such as a\nSupport Vector Machine (SVM).  The structured data extraction classifier 103 can store (103a) the extracted data in a system store 114.  In some embodiments, the output of the structured data extraction classifier 103 can be entity types, entities, and\nthe entity types' relationships to other entity types.\n All entity type of an entity can be identified in various other ways.  In some embodiments, an entity type of an entity can be identified based on a sequence model.  For example, LSTM can be used.  The sequence model can be trained on a\nparticular corpus to learn the context in which words arise.  Thus, the sequence model can uncover the context in which entities that presently are unassociated with an entity type arise.  This enables an entity type to be associated with, an entity,\nwhen an entity type is sought for the entity.  Other suitable models for machine learning can also be used to uncover the context in which entities arise.\n In some embodiments, neighbors of an entity can be used to identify the entity's entity type based on the neighbors' entity types.  In some embodiments, the neighbors can be defined as other entities that are closely related--in terms of their\ncosine distances--to the entity.  In some embodiments, a specific number of neighbors can be selected, and a weight can be assigned to each of the selected neighbors.  For example, to associate an entity type with an entity, the entity's neighbors can be\nranked based on their cosine distance from the entity, and the top 500 neighbors can be considered.  Each of the 500 neighbors can be assigned a weight, such as a percentage weight, which can vary based on their rank.  For instance, the first-ranked\nneighbor can be assigned a weight of 1%, the second-ranked neighbor can be assigned a weight of 0.9%, the third-ranked neighbor can be assigned a weight of 0.87%, and so on.  In some embodiments, the decreasing rate of the weight can be based on an\nexponential decay function.  In some embodiments, ail the neighbors can be considered without any limit.  In some embodiments, the amount of the weight can be directly proportional to the value of the cosine distance.  In some embodiments, the amount of\nthe weight can be directly proportional to the value of the rank.  In some cases, such assignment of weights can be referred to as \"continuous decay,\" because the weight continuously decreases as the rank moves towards the bottom.  After the weights have\nbeen assigned, the neighbors' entity types can be examined and grouped by the same entity types.  For each entity type, a sum of the percentage weights of the neighbors for that entity type can be calculated and assigned to that entity type.  For\nexample, if the entity type X is associated with three neighbors with three percentage weights (0.5%, 0.3%, and 0.1%), then X is assigned a percentage of 0.9%.  In some embodiments, this percentage can indicate the probability of the entity being that\nentity type.  In some embodiments, the system can associate an entity with an entity type if such a percentage exceeds a certain threshold number.  In some embodiments, the system can associate an entity with an entity type with the highest percentage.\n In some embodiments, instead of assigning varying weights to the neighbors, each neighbor can be assigned the same weight.  For example, if the top 100 neighbors can be picked, and each neighbor can be assigned 1% as its weight, in this case,\neven if neighbors have different cosine distances, they are treated the same when weights are assigned.\n In some embodiments, unstructured data 102 can pass through 102a to an unstructured data extraction classifier 105.  The output of the unstructured data extraction classifier 105 can store (105a) the extracted data in the system store 114.  In\nsome embodiments, the unstructured data extraction classifier 105 can use a class of artificial neural network (ANN) (e.g., a recurrent neural network (RNN)) and/or a word embedding generator.\n In some embodiments, when them is latent information that can be extracted from structured data 101, a specialized encoder 104 can be used to generate unstructured data from the structured data 101.  The specialized encoder 104 can send (104a)\nthe generated unstructured data to the unstructured data extraction classifier 105, which can in turn send the output through the unstructured data extraction pathway 105a.  In some embodiments, the generated unstructured data is m the form of\nunstructured text.  For example, if the structured data is \"disease=cancer; indication=weight loss; drug=methotrexate; side_effect=dizziness,\" the specialized encoder 104 can generate unstructured data in the form of \"disease cancer indication weight\nloss drug methotrexate side_effect dizziness.\" In this example, latent information in the structured data can be that cancer can be associated with weight loss and methotrexate and that the patient suffers dizziness.  Thus, such latent information cars\nbe extracted and leveraged by using the unstructured data extraction classifier 105 on the structured data 101 that has been processed by specialized encoder 104.  In some embodiments, apart of structured data 101 can be processed using the specialized\nencoder 104.  In other embodiments, the entire structured data 101 can be processed using the specialized encoder 104.  In another example, the specialized encoder 104 can generate unstructured data by using the entities labels to position the entities\nin a given proximity.  For example, given the same set of structure data, the specialized encoder 104 can apply a mapping of \"drug disease drug indication drug side effect\" to create the unstructured text of \"methotrexate cancer methotrexate weight loss\nmethotrexate dizziness\".\n In some embodiments, the structured data portion of the semi-structured data 117 can be passed to the structured data extraction classifier 103.  In some embodiments, the unstructured data portion of the semi-structured data 117 can be passed to\nthe unstructured data extraction classifier 105.  In some embodiments, a part or the entire structured data portion of the semi-structured data 117 can be passed to the specialized encoder 104, which can send the output to the unstructured data\nextraction classifier 105.\n In some embodiments, the output of the unstructured data extraction classifier 105 can include an entity type, entity, document/paragraph/sentence embeddings, entity relationships including temporal/logical sequence relationships, and sequence\nrepresentations.  In some embodiments, entities can be either labeled or unlabeled.  A label can be used to describe an entity.  For example, the entity \"EGFR\" can refer to \"Epidermal Growth Factor Receptor,\" in which case the entity \"EGFR\" can be\nlabelled as a gene.  Without a label, there may be ambiguity as to what an entity may refer to.  For example, if the entity \"EGFR\" is not labelled, \"EGFR\" can be ambiguous because \"EGFR\" can refer to the gene \"Epidermal Growth Factor Receptor\" or the\nlaboratory test \"Estimated Glomerular Filtration Rate.\" Entities can be labeled using various techniques.  For example, a search (e.g., using Google) can produce a label for an entity.  As another example, a corpus can provide labels.  Wikipedia, for\nexample, can provide labels for certain entities on many of its pages.  For unstructured data, context surrounding entities can be analyzed to determine their labels.  In some embodiments, a class of ANN (e.g., an RNN) can be used to perform such\nanalysis.  In some embodiments, the analysis performed using the ANN can be improved by leveraging systems and methods described below in connection with FIGS. 8 and 9.\n In some embodiments, the system store 114 can capture information extracted from two or more source paths (e.g., 103a and 105a) in different forms to facilitate the synthesis of information and/or enable subsequent information extraction through\ndifferent pathways (e.g., pathways 103a and 105a).  The system store 114 can include information stored in a structured semantic database 106 (which can be a traditional database); a knowledge graph(s) 107 (which can be directed graphs of labeled\n(extracted from both paths 101a and 102a) and/or unlabeled entities (extracted from the 102a path)); word embeddings 108 (which can include word(s) and/or sentence(s)), document/paragraph/sentence embeddings 109; and sequence representations of\nunstructured data 110.  In some embodiments, an example of word embedding can be word2vec.  In some embodiments, an example of document/paragraph/sentence embedding can be doc2vec.  In some embodiments, an example of sequence representations 110 can be\nMemory Neural Network (MemNN).  In some embodiments, MemNN can be used for \"Question and Answer\" style discovery, where MemNN can be trained on questions to generate responses/follow-up questions.  In some embodiments, these responses and/or follow-up\nquestions can be used in case of ambiguity.  For example, there may be ambiguity as to what an entity may refer to.\n In some embodiments, the word embeddings 108 and/or document/paragraph/sentence embeddings 109 can be repositories of embeddings generated for a broad, class of domain specific corpus.  In some embodiments, these embeddings can capture one or\nmore relationships of labeled and unlabeled entities in that domain.  In some embodiments, these embeddings can be used to indicate and/or rank the strength of such relationships.\n The embeddings can be used to construct one or more of knowledge graphs 107.  The knowledge graph 107 can be representative of a universal graph, domain, and/or context specific graphs with labeled, and/or unlabeled nodes having\nuniversal/domain/context specific weights.  The corpus can determine the embeddings and in mm the neighborhood nodes in the graph.  Sequence representations 110 can be a repository of universal, domain, and/or context specific sequences, and can be used\nto comprehend and respond to questions spanning multiple sentences/questions.\n The system store 114 can serve to synthesize responses and facilitate subsequent information, extraction through both pathways 107a and 108a.  For example, the word embeddings 108 can be used to eliminate spurious information that can present in\nstructured data.  In some embodiments, if an entity is found in a structured record, and the entity is not semantically related to the other entities in the record, which can be revealed through word embedding neighborhood metrics, then that information\ncan be passed (108a) to the structured data extraction classifier 103, and that entity can be isolated for manual verification.  The structured database pathway 107a can be used to improve the named entity labeling scores.  In some embodiments, the named\nentity labeling scores can be associated with the confidence score of labeling a term/phrase.  In some embodiments, the structured, semantic database 106 can be used to validate and/or confirm the entity type of a term/phrase; this can help improve the\nnamed entity labeling scores and can increase the confidence score of labeling a term/phrase.\n The system store 114 can power a discrimination engine 116 that can include a dialog/query analyser 111 (which can rely largely on sequence representations 110), a response synthesizer 112, and a response templates generator/chooser 115.  The\nresponse template generator/chooser 115 can power user interfaces 113 through 116a.  In some embodiments, the dialog/query analyzer 111 can analyze user input, such as a search term and filter criterion.  For example, if a user searches the term \"AML\" on\nan interface (e.g., the interface in FIG. 8), the dialog/query analyzer 111 can receive and analyze this search term, and pass the search term to the response synthesizer 112 for further processing.  In some embodiments, the dialog/query analyzer 111 can\nreceive data from the system store 114 through 114a for the analysis function.  The response synthesizer 112 can also receive data from the system store 114 through 114b, and use this data to synthesize responses that are relevant for producing results\nfor the user's search action.\n The response template generator/chooser 115 can generate/choose an appropriate template to be used for presenting search results to the user through an interlace.  Different types of templates can be used to generate different types of\nbio-knowledge graphs, such as the bulbs eye bio-knowledge graph in FIG. 7 and, the pipeline bio-knowledge graph in FIG. 8.  In some embodiments, the response template generator/chooser 115 can generate a template based on the labels for the entities that\nare being presented on an interface.  These entities can be selected based, on their entity distribution.  In some embodiments, the response template generator/chooser 115 can choose a template from a set of hard-coded templates.  In some embodiments, a\nhard-coded template can be generated through training (e.g., a system can generate a template by learning certain types of entities and their labels from a corpus).  In other embodiments, a hard-coded template can be manually generated.  In some\nembodiments, a user can override a portion or all of the view in an automatically chosen/generated template.  For example, a user can replace the drug information with the indication information by using filters.\n Various components that are part of the system 100 can be implemented as hardware, software, or combinations of both.  Various components and blocks described herein can be arranged differently (for example, arranged in a different order, or\npartitioned/combined in a different way) all without departing from the scope of the subject technology.\n According to some embodiments, one or more computations by the system in FIG. 1 can be performed by one or more processors in a cloud system.  In some embodiments, any rendering of output (e.g., rendering of user interface) can be performed by a\nuser device (e.g., a personal computer, a mobile device, etc.).  In some embodiments, any input to the system in FIG. 1 can be made by a inputting system that can involve hardware and/or software (e.g., a keypad, a keyboard, a microphone, speech\nrecognition software, etc.).  In some embodiments, a database (e.g., the structured semantic database 106) used in the system in FIG. 1 can be from any source, such as a relational database, NoSQL DB, flat files, and/or any other suitable database.  In\nsome embodiments, the database can be a local database and/or a remote database.\n FIG. 2A illustrates one method of providing semantic responses to queries.  A search engine, such as Google, can be used to find information on the search term \"aml.\"\n FIG. 2B illustrates a rendition of an interface enabled by synthesizing data from multiple pathways in accordance with some embodiments of the present disclosure.  In some embodiments, these pathways can be 101a and 102a from FIG. 1.\n FIG. 3A illustrates one method of providing semantic responses to queries.  FIG. 3B illustrates a rendition of an interlace enabled by synthesizing data from multiple pathways in accordance with some embodiments of the present disclosure.  In\nFIG. 3A, the user interface is powered by a search index of documents 301 with an information box 302 of attributes for the search input \"AML.\" Additionally, a set of questions semantically related to search terms is present in the backend driving the\ninterface.  In contrast, in FIG. 3B, the user interface can have rich semantic information.  For example, the user interface may not have the search term \"AML\" present anywhere in the result.  Even from a cursory glance, one can view that there are no\nresults matching the search term \"AML.\" In some embodiments, the synthesized data powering the interface can be a matrix of rows and columns, where the first column 303 and second column 304 are entities (in this example, the first column includes\ninformation related to drugs and then associated companies; and the second column includes information related to pathways) that can relate to the user input \"AML.\" The subsequent columns can form a temporal sequence, where each column relates to a\ndifferent phase of the drug development.\n FIG. 4 illustrates an example two-dimensional matrix of data generated by the response synthesizer 112 (FIG. 1) in accordance with some embodiments of the present disclosure.  This example two-dimensional matrix of data shows a response to the\nuser input \"AML,\" where the following items are shown: entities 401 (drugs+companies, pathways), temporal sequence of entities 403 (drugs m various stages of development), summary 402, and a matrix transform control 404 that can include semantic filters\nand transforms of data.  The intent determination can be performed by query analyzer 111 (FIG. 1) that can result in the synthesis of the response making use of the response template chooser 115 (FIG. 1).  In some embodiments, the template\nchooser/generator 115 can be trained by a neural network (e.g., a convolutional/RNN combination) to generate the appropriate response template involving entities and entity sequences.\n In some embodiments, components that are supervised learning systems can have user sampling and validation including manual overrides.  Even if the amount of labeled data to train a template chooser/generator 115 is initially low (e.g., for a\nspecific domain and the system may memorize), such a system can scale better than a hand-engineered rules driven template system, where the availability of more labeled data (use cases) can cause the system to become more brittle.  In neural nets, when\nthe number of parameters in the network is large and the training data is small, the system can perform a close fit on the training data, given the large number of parameters.  This is known as overfitting.  Overfitting can be like memorizing the trained\ndata.  This does not necessarily imply that it can generalize well beyond the training data.  The response in FIG. 4 is shown in a two dimensional space for illustrative purposes.  In some embodiments, embodiments of the present disclosure can power an\ninterface with any number of dimensions, including a single dimension and more than two dimensions.\n In the current state of art, a user inter face is often powered from behind by a database view, where the data for the view is curated upfront with select labeled entities.  In contrast, embodiments of the present invention, as illustrated in\nFIG. 4, enable a rich semantic response to be automatically synthesized dynamically, even when the input query includes unlabeled entities (e.g., remyelination, \"elaborate on remyelination\").  In some embodiments, the rich semantic response can be\nsynthesized by (1) examining the entity distribution around the input terms based on the context of the query, where entity distributions can vary around a term within a corpus and/or between different corpora (e.g., FIG. 9 shows entity distribution for\nthe term \"remyelination\"); (2) identifying candidate entities and entity sequences to construct the response; (3) creating the summary based on the identified candidate entities and entity sequences; and (4) choosing transforms appropriate to the chosen\ncandidate entity and entity sequences.\n Furthermore, the system components described in the present disclosure can learn either supervised, unsupervised, or semi supervised, from structured and unstructured data.\n Systems and methods in accordance with embodiments of the present, disclosure can address various challenges--examples of which are discussed directly below--in synthesizing semantic response to user queries.\n An example challenge can be when a user makes a query that is not a labeled entity or entity class (e.g., remyelination).  In this case, the knowledge graph (which includes both labeled and unlabeled entities) can facilitate finding labeled\nentities related to the user input by scanning its neighborhood.  In addition, the classification of the labeled entities (e.g., genes, drugs, indications, companies etc.) can enable computation of an entity distribution (e.g., FIG. 9) to facilitate the\nchoice of most relevant semantic rendition of this entity.  This approach addresses the entity class inadequacy problem described above, where the current state of art systems would output just lexically matching results or just the entities in the\nneighborhood of word embedding for the unlabeled entity.\n Another example challenge is that although word, document/paragraph/sentence embeddings have the inherent, property of bringing semantically related entities together even if they do not co-occur in the corpus, these entities are difficult to\ndiscover from the embeddings.\n Embodiments of the present disclosure can construct a knowledge graph by combining semantic relationships with labeled entities and/or entity classes.  This can enable innovative insights to be unearthed, where those insights are not described\nas a co-occurrence in the primary literature.  For examine, the word \"riluzole\" (an ALS drug) and the word \"vemurafenib\" (a melanoma drag) are proximal to each other, as indicated by their cosine distance which can be about 0.48.  The cosine distance is\nfound to be significant as these two words are found in the context of treating a cancer, and thus, there is an overlap in the context, where these words are found.  However, the proximate co-occurrence of the words \"riluzole\" and \"vemurafenib\" does not\noccur in any primary literature consumed.  For example, a Google search shows no document containing these two words within a five word window proximity.  In some embodiments, novel insights can emerge born a system that can show a relationship between\nwords such as \"riluzole\" and \"vemurafenib.\" These insights can lend themselves to effective generation of BAD hypothesis, clinical trial design, and commercial decisions.  In some embodiments, visualization of these related entities can involve a\nmechanism that distinguishes these neighborhood entities from others that appear only because of physical proximity to the original term in document.  An example can be seen in FIG. 11.  In other words, and as described in snore detail below, the\nrelationship between the entities \"riluzole\" and \"vemurafenib\" is not discovered due to their proximity in the corpus.  Rather, it is the similarity of the characterizations of those entities (e.g., by comparing the cosine distance of their vectors)\nbased on an analysis of the entire corpus that uncovers the otherwise hidden relationship.\n In some embodiments, the knowledge graph created from unstructured and structured sources can be used to create entity specific centroids (e.g., use all or some of the entities pertaining to a context) and use the synthesized centroids to\nvalidate the labeling of entities generated by a sequence learning model, such as bidirectional LSTM (Long short-term memory) RNN.  The semantic bio-knowledge graph can be used to constrain the context in which a specific document has to be interpreted;\nthis can help filter out a lot of \"noise\" from generic databases (e.g., considering a database of all HUGO genes).\n In some embodiments, the key/value fields from a structured source are processed through a specialized encoder that may create a unstructured stream that can be fed to a learning model to generate word embeddings that reveal latent semantic\nrelationships in structured data.\n In some embodiments, the knowledge graph with labeled entities can be directly used to identify entities and generate responses.  The knowledge graph can relate diverse entities, as highlighted for exemplary \"neighborhood\" (FIGS. 5A-5B) and\n\"analogy\" case studies (FIGS. 6A-6D).  In some embodiments, the knowledge graph can include a collection of (1) an aggregate computer system housing all labeled and unlabeled entities, (2) entity specific computer systems, and (3) an unlabeled computer\nsystem houses all terms or phrases that are not labeled as entities.\n FIGS. 5A-B illustrate knowledge graphs that relate to diverse entities, as highlighted for the \"neighborhoods\" of the user-supplied exemplary queries in accordance with some embodiments of the proem disclosure.  The user-supplied exemplary\nqueries are \"Lacosamide\" and \"Certolizumab\" respectively for FIG. 5A and FIG. 5B.  These knowledge graphs can show how a system can function on a corpus (e.g., the \"Core Corpus\").\n FIGS. 6A-6D illustrate examples showing how knowledge graphs relate diverse entities, as highlighted for the \"analogies\" in accordance with some embodiments of the present disclosure.  In some embodiments, the knowledge graph includes labeled\n(word/document/paragraph/sentence embeddings with entities/entity classes assigned) and unlabeled entities.\n FIG. 6A illustrates example #1, where \"anti-EGFR:Erlotinib::infliximab:?\" (using the well-known format of A:B::C:?, meaning A is to B as C is to ?) exists and the top hits include anti-TNF which is the target of infliximab FIG. 6B illustrates\nexample #2, where \"T315I:Ponatinib::CO-1686:?\" exists and the top hits include T790M which is the genotype of CO-1686/Rociletinib.  FIG. 6C illustrates example #3, where \"Arthritis:Certolizumab::Rotigotine:?\" exists and the top hits include parkinsonism\nwhich is the disease indication that Rotigotine is FDA-approved to treat.  FIG. 6D illustrates example #4, where \"Zyrtec:Allergy::Hypercholesterolemia:?\" exists and the top bits include Rosuvastatin which is FDA-approved to treat Hypercholesterolemia.\n In some embodiments, the revealed semantic relationships can help automate workflows for critical commercial, clinical, and R&D functionalities in pharmaceutics.  An example is the generation of strategic insights into the landscape of complex\ndiseases via an Orphan+Rare disease lens towards helping users identify high-value investment white-spaces.  A related use case can be the generation of on-demand, powerful visual snapshots of the competitive clinical investment landscape towards\nsupporting data-driven commercial and clinical strategy.\n In some embodiments, one exemplary use case can involve workflow automation that can use the Bio-Knowledge graph for identifying Orphan/Rare disease innovation whitespaces.  In some embodiments, different steps can be taken to achieve this\nworkflow automation as follows.  First, from the Semantic Bio-Knowledge Graph, the disease indication \"entities\" that are proximal in the neighborhood of each search term can be identified (e.g., for \"real world phenotype\" entities, such as\n\"neurodegeneration,\" \"remyelination,\" \"angiogenesis\" etc.).  Second, for each identified disease indication, a system can determine the market(s) (e.g., commercial market, investment market) in one or more geographical areas, such as the United States,\nWestern Europe, the United Kingdom, Japan, and emerging markets.  The Bio-Knowledge Graph can be further used to power user interface (UI) to visualize the clinical competitive landscape for each disease and/or for each geographic region.\n FIG. 7 illustrates an example bull's eye view (an alternate 2D view) that can be a radial representation of the \"symbolic\" temporal phases, where the closer to the center, the more advanced/marketed drugs; and where the closer to the periphery,\nthe less developed/preclinical assets).  The system can find and place drugs associated with each disease based on the cosine distance between the drug and the disease, and other factors (e.g., the search term, other related entity types, such as\nclinical trials).  The exemplary bulls-eye visual shown for search term \"Neurodegeneration\" can show that rare diseases, such as PKAN, LHON, and CLN2, can offer innovation whitespaces.  Third, the Bio-Knowledge Graph can also support subsequent user\nqueries.  For example, a use case can involve identifying additional risk factors associated with strategic bets for each disease-drug mechanism pair based on various types of information, including failed/terminated trials or signals from the Federal\nAdverse Event Reporting System (FAERS).  Another common follow-up query can involve identifying specific assets (e.g., drugs, devices) that lend themselves to a Merger & Acquisition (M&A) or licensing strategy.  This workflow can also be enabled by the\nvisuals generated from the Bio-Knowledge Graph, such as one of the 2-D renderings (Bulls-eye, Competitive Pipeline, etc.).  In some embodiments, the risk factors can be precompiled from one or more of various sources (e.g., FAERS, the number of\nfailed/terminated clinical trials involving a specific drug and disease indication pain etc.)\n The buffs eye view of FIG. 7 also illustrates how a Bio-Knowledge graph can automatically capture real-time semantic relationships between any user-supplied query (e.g., \"neurodegeneration\") and different entities in its neighborhood (e.g.,\nhighlighted here for eight exemplary disease indications) in accordance with some embodiments of the present disclosure.  For example, in FIG. 7, the user has searched \"neurodegeneration.\" The system uses the Bio-Knowledge graph to identify the top\nlabeled, entities associated with the search term (e.g., \"close\" as measured by distance).  The system then uses the entity type (taken from the label) associated with the top labeled entities to determine the nature of the data to display.  In this\nexample, the entity type/label \"disease\" is associated with the entities closest to the search term.  Thus, the system associates each sector of the bull's eye with an entity that is a specific disease that is close to the search term \"neurodegeneration\"\n(e.g., ALS, U.S.  Parkinson's, Alzheimer's, etc.).  The system, then discovers that the entity type/label \"drug\" is close to the search term.  Thus, the system now fills in each respective sector of the graph with specific drugs that are close to the\nspecific disease corresponding to that sector.\n In some embodiments, the number of entities displayed can be fixed (e.g., the top eight disease indications).  In some embodiments, the number of entities displayed can be overridden by using the \"filters\" function (e.g., the number can be\nincreased or decreased; a specific entity can be added or removed).  In some embodiments, the system can determine the initial number of entities to display based on the availability and significance of different entities in the results.  The exemplary\n\"Bulls-eye visual\" shown here can pull together information from various clinical trials, drug names, and/or other associated information (e.g., company name, mechanism of action, etc.).  This can ensure that corporate strategy and competitive\nintelligence functions are not misinformed of the market.\n FIG. 8 illustrates a Bio-Knowledge graph that enables rapid organization of information on drugs and their competitors--across companies and mechanisms of action (or signaling pathways)--which can be at the heart of competitive market\nintelligence workflows in accordance with some embodiments of the present disclosure.  In tins example, for the search query \"AML,\" the system can retrieve several different signaling pathways central to the disease, and represents drugs across the\nmarket in terms of their stage of development (e.g., here, clinical trial phases 1, 2, 3 and marketed are shown).  Further, attributes of every trial that can impact commercial decisions and market sizing can be captured in the card views (e.g., genetic\nmarkers like FLT3-positive.  First-line treatment information, and combination therapies like cytarabine+daunorubicin).\n The example shown in FIG. 8 illustrates another exemplary use case involving competitive intelligence workflow automation that can integrate information across the diverse silos of R&D, clinical trials, bio-medical knowledgebase, company press\nreleases/investor reports, and/or regulatory bodies to highlight commercially salient factors (e.g., first line treatment, bio-molecular constraints, combination therapy, etc.).  In some embodiments, different steps can be taken to achieve this workflow\nautomation.  First, from the Semantic Bio-Knowledge Graph, for a disease indication entity searched by the user, the \"pathway\" entities and drugs that specifically act via these pathways from a mechanism of action (MOA) standpoint can be identified. \nSimilar to the approach described for FIG. 7, the system displays entities that are pathways and drugs because these entity types/labels correspond to the labels associated with the closes entities associated with the search input \"AML\".  Second, related\nentities from the drugs (such as the \"company\" that markets the drug, the stage of development (e.g., Preclinical, IND, Phase 1, Phase 1/2, Phase 2, Phase 3, Phase 4.  NDA/Expected Launch, Marketed, etc.)) can be identified.  \"Trial group\" entities,\nwhich can be series of clinical trials most closely related to each other (e.g., similar clinical trial parameters, such as first line treatment, combo/mono-therapy, biomarker/target constraints) can be computed.  Third, all the above information in a\nuser-friendly perspective as a \"competitive pipeline\" visual can be integrated.  For example, a pipeline view, as in FIG. 8, can be used.  This pipeline view is a two dimensional spatial organization with each row identifying a specific drug (and the\ndrug company) with the corresponding \"stage of development\" (the x-axis is the \"symbolic\" temporal phase, y-axis is individual drugs).  One innovation in visualization can be the \"aggregation\" of drugs from different companies all sharing some common\ntheme (such as the same molecular target mechanism).  The system can identify such a common theme by examining the entity distribution.  Each trial group can be shown as a distinct row and the individual clinical trial cards that constitute the group\nhighlight just the commercially salient information that the Bio-Knowledge Graph has linked together.  The entity distribution can identify candidate columns which can include entities and entity sequences.  The ordering of terms can be based on\ntemplates or by training a model to generate the proper matrix of entity types to be rendered.  Fourth, the competitive pipeline visual can enable quick summarization of the total number of drugs that are in various stages of development (e.g.,\nPreclinical, IND, Phase 1, Phase 1/2, Phase 2, Phase 3, Phase 4, NDA/Expected Launch, Marketed, etc.).  In some embodiments, the system can determine various stages of drug development based on distance relationships between the specific drugs/pathways\nof each row and clinical trial information in the Bio-Knowledge Graph.  In some embodiments, this visual can provide rapid identification of the most clinically-advanced pathways (systems biology) and/or mechanism of action (of salient interest to R&D\ncorporate strategy and market intelligence).  The competitive pipeline visual can present an overview that enables rapid identification of assets of companies of specific interest, which can then be studied in a detailed fashion.  Fifth, the visual shown\nhere can also be readily expanded to (1) display the date the system last auto-updated information contained in the visual; (2) display subject matter experts (SMEs) involved in specifically validating the data displayed in the visual (including what\ndate each SME last validated the data, manually; and potentially even links to the SME's professional website/Linkedin), and (3) enable users to edit the visual displayed, save workflows/sessions, and download the visual directly as an image/PowerPoint\nslide, thus enhancing user productivity.  The expansions can be based on associations between words that identify the knowledge being sought and the answers.  For example, a user can click on the top row, enter \"experts,\" and get the subject matter\nexperts because of the distance between the drug's name and the expert's name.  As another example, if the user enters \"experts in CAR T-cell therapy,\" the system can retrieve all the current \"key opinion leaders\" in the field of chimeric antigen\nreceptor T-cell (CART) therapy.\n FIG. 9 illustrates a Bio-Knowledge graph queried for the exemplary phrase \"Remyelination\" followed by the Entity Recognition methods enable distinct entity classes to be visualized) in accordance with some embodiments of the present disclosure. \nIn this illustrative example, drugs in panel 902, disease indications in panel 904, and biomolecular signals in panel 906 can show different entity types recognized in the neighborhood of the phrase \"Remyelination.\" The entities can be ranked according\nto the cosine distance to the original query vector \"Remyelination,\" where the cosine distance of 1 being the highest possible rank; and indicating the self-vector \"Remyelination\".\n In some embodiments, another exemplary use case can involve enabling users to query \"Real World Phenotypes\" to visualize related, entities ranked in decreasing relevance, to identify \"Orphan/Rare disease\" investment avenues.  The real world\nphenotype \"Remyelination\" can be salient for pharmaceutical companies studying central nervous system (CMS) disorders.  Yet the \"word\" Remyelination does not exist in human-curated disease/indication corpuses (such as ICD10) which only catalog disease\nindications.  Given that individuals across each Pharmaceutical company possess their own unique set of \"Real World Phenotypes,\" users of competitive intelligence and corporate strategy functionalities are very likely to query any system with real world\nphenotypes like \"Remyelination.\" The use of existing databases for this purpose can lead to misinformed commercial, clinical and R&D investment decisions.  In some embodiments, the Bio-Knowledge Graph includes several millions of entities, including\nthousands of Real-World Phenotypes like \"Remyelination.\" The Bio-Knowledge Graph can further accurately capture the neighborhood of \"Remyelination\" for diverse entities such as drugs/compounds, diseases/indications, genes, etc. as shown in FIG. 9.\n Examples above can show how visuals like the competitive pipeline and bulls-eye can be equipped with the distinctive ability to link diverse important entities (such as genes, drugs, diseases) for high-value queries like \"Remyelination.\" For\nexample, the results for \"Remyelination\" can identify the disease indication Pelizaeus-Merzbacher Disease (PMD) and the causally-linked PLP1 gene (refer to FIG. 10), reflecting these insights on the user interface (UI) makes the visuals \"semantically\naccurate.\" This can overcome a fetal flaw of the existing predominantly-lexical search-and-retrieve powered user interfaces for competitive intelligence and corporate strategy functionalities.  The results for \"Remyelination\" can also identify other\ndisease indications (e.g., any of the other disease indications in panel 904) and find causally-linked genes and/or drugs using a bio-knowledge graph similar to the one shown in FIG. 10.  In some embodiments, the system or user can choose any entity, and\nthe system, can determine other causally-linked entities based on the chosen entity using a bio-knowledge graph similar to the one shown in FIG. 10.  In some embodiments, the system can populate the pathway inhibitor(s) by using a combination of\nstructured knowledge bases to retrieve all known inhibitors that target a pathway that is identified by the system.  In some embodiments, \"neighborhoods\" of an inhibitor can be used to identify all other \"inhibitor's\" entities in that neighborhood, and\nthen in each of those \"inhibitors\" neighborhoods, the system can determine how \"close\" are the \"genes\" or \"pathway\" case-by-case.\n FIG. 10 illustrates a Bio-Knowledge graph that can enable identifying disease indications that are closely related to any real world phenotype query supplied by the user in accordance with some embodiments of the present disclosure.  For the\nillustrative example shown here, the query \"Remyelination\" can result in identification of the orphan/rare disease PMD (Pelizhus-Mazbacher Disease).  Analysis of the neighborhood of PMD in turn can reveal several other real-world phenotypes in decreasing\norder of relation to PMD-specifically, neuronal loss, axonal transport, and neuroinflammation.  The numbers in FIG. 10 represent cosine distances between two entities.  Thus, the higher the cosine distance, the closer the two entities are.  In this\nexample, neuronal loss, axonal transport, and neuroinflammation are less coupled to PMD than Remyelination is.  This can be followed by Protein aggregation, and finally the phenotypes with lowest connection to PMD are synaptic transmission, aging, tissue\nmicroarchitecture, diabetes, and phagocytosis, respectively.  In this example, the genes PLP1 (including the splicing variant DM20), PMP22, MPZ, and GJB1 are all in the neighborhood of the PMD disease vector.  Utilization of additional biopanel screening\nexperimental data sets can further suggest that BRAF and MEK pathway inhibitors are specifically sensitive to samples that over-express the PLP1, PMP22, MPZ, and GJB1 genes.  In some embodiments, these additional biopanel screening experimental data sets\ncan come from one or more structured databases, such as the Cancer Therapeutics Response Portal (CTRP) v2 published by the Broad Institute, the Cancer Cell Line Encyclopedia (CCLE) published by the Broad institute, Sanger Institute's Catalogue Of Somatic\nMutations In Cancer (COSMIC), and Genomics of Drug Sensitivity in Cancer (GDSC) databases.  In some embodiments, any number of external structured databases or knowledgebases can be used to glean additional insights.  Hence, the disclosed invention\nmotivates testing BRAF-MEK inhibitors in remyelination assays.  In this example, as above, the type of entities to show as related to the search term \"PMD\" can be determined by the types of entities associated with the actual entities that are closest to\nthe search term (e.g., real-world phenotype).  Each successive type of entity to be shown (e.g., genes) can in turn be determined by the type of entity associated with the search term \"PMD\" and the top entity value \"remyelination\".  At each level, a new\nentity type is discovered (e.g., drug) from the top entities values associated with preceding entities values (e.g., the specific genes).  In this way, multiple levels of relationships can be uncovered by the system.\n In some cases, the same entity can refer to more than one entity type.  For example, the entity \"ICOS\" can refer to a gene type (Inducible T-Cell Co-Stimulator), a company name (the trademark of Icos Corporation, which was a company that was\nacquired by Eli Lilly and Company in 2007), or some other entity type.  Such an entity can create an ambiguity to traditional systems.  For example, if a user enters the query term \"ICOS\" into a traditional search engine, the search engine produces\nresults that do not account for different meanings of \"ICOS.\" According to some embodiments, disclosed systems and methods can recognize different entity types for a given entity.  These different entity types can be presented in different neighborhood\nsenses.  For each neighborhood, sense, relevant entities associated, with the given entity can be presented.\n FIG. 31 illustrates an exemplary neighborhood sense interface 3100 in accordance with some embodiments of the present disclosure.  When an entity is entered as a query term 3101 and the entity is associated with more than one entity type, then\nthe neighborhood sense interface 3100 presents neighborhood senses, each of which corresponds to one of the different entity types.  Each neighborhood sense is associated with entities that are associated with the query term 3101 and also with the entity\ntype that corresponds to the neighborhood sense.  For example, for the query term \"icos\" 3101, the neighborhood sense interface 3100 can present four different neighborhood senses 3102 (icos.sup.1, icos.sup.2, icos.sup.3, and icos.sup.4).  In some\nembodiments, each neighborhood sense can be associated with a percentage that represents the probability of the query term 3101 being in the respective neighborhood sense.  For example, the value of 61.8% shown beside icos.sup.1 can indicate that the\nquery term \"icos\" is associated with the neighborhood sense icos.sup.1 61.8% of the time m the corpus.  In the neighborhood sense \"icos.sup.1\" words or entities 3103 that are associated with the query term \"icos\" can include \"icos1,\" \"CO_STIMULATOR,\"\n\"pded1,\" \"il2ra,\" \"ICOS,\" CD28,\" and \"forkhead\"--which can be listed in a decreasing order of their semantic association strength with respect to the query term \"icos.\" By analyzing these entities in the neighborhood sense icos.sup.1, the system or the\nuser can recognize that the entity type for \"icos\" in this neighborhood sense is likely to be \"gene type.\" In this example, the row 3104 shows that the word \"pdcd1\" has the third highest semantic association strength of 0.76 and occurs 1,109 times in 188\nof the documents in the corpus.  For a given query term, its neighborhoods senses can be determined using various methods.  For example, Adaptive Skip-gram (Adagram) model can be implemented to capture word vectors that are generated from an unsupervised\nlearning model.  Other methods and models that can be used include Multisense Skip-gram (e.g., Neelakantan et al. (2014)) and/or any other suitable model or method that can infer different senses of a word in a context (e.g., biological context).\n FIG. 32 illustrates an exemplary neighborhood sense interface 3200 in accordance with some embodiments of the present, disclosure.  FIG. 32 shows the remits for the query term \"icos,\" when the neighborhood sense is icos.sup.2.  In FIG. 32,\nentities that have the highest semantic association strengths include \"henneys,\" \"pharmaceuticals,\" \"xoma,\" \"henney,\" \"Genentech,\" \"companies,\" and \"therapeutics.\" From these entities, the system or the user can recognize that the entity type for \"icos\"\nin this neighborhood sense is likely to be \"company name.\"\n FIG. 33 illustrates an exemplary knowledge diagram interface 3300 in accordance with some embodiments of the present, disclosure.  The knowledge diagram interface 3300 can display different neighborhood senses 3302 associated with a query term\n3301.  Each neighborhood sense can be linked to a detail box 3303 that shows a probability of the query term 3301 being in the respective neighborhood sense and the list of entities associated with the neighborhood sense.  For example, for the query\nterra \"icos\" 3301, the knowledge diagram interface 3300 shows that there are four neighborhood senses 3302 (represented as four divisions of a rectangle).  The probability of \"icos\" being in the first neighborhood sense is 61.8%.  The first neighborhood\nsense includes the following entities: \"icos1,\" \"co-simulator,\" \"pdcd1,\" \"il2ra,\" \"icos,\" \"cd28,\" \"forkhead,\" \"icosig,\" \"foxp3,\" and \"klrg1.\" In some embodiments, the detail box 3303 can be accessed by clicking on the division in the rectangle 3302 that\ncorresponds to the desired neighborhood sense.\n Thus, in some embodiments, the system or the user can determine the neighborhood sense corresponding to the desired entity type by analyzing the resulting entities for each neighborhood sense.\n FIG. 34 illustrates an exemplary knowledge diagram interface 3400 in accordance with some embodiments of the present disclosure.  The knowledge diagram interface 3400 is similar to the knowledge diagram 3300 (FIG. 33), except that the detail box\n3403 now shows information for the second neighborhood sense.  The probability of \"icos\" being in the second neighborhood is 28.1%.  The second neighborhood sense includes the following tokens: \"henneys,\" \"pharmaceuticals,\" \"xoma,\" \"henney,\" \"genentech,\"\n\"companies,\" \"therapeutics,\" \"lilly,\" \"boards,\" and \"vaxgen.\"\n FIG. 35 illustrates an exemplary information box 3500 in accordance with some embodiments of the present disclosure.  The information box 3500 can provide aggregated information for an entity.  Tins can be useful when there is ambiguity to which\nentity type the entity belongs.  For example, when the information box 3500 for the entity \"icos\" is launched for the first neighborhood sense, the information box 3500 states that \"icos\" refers to \"Inducible T-Cell Co-Stimulator (ICOS)\" 3501 and\nprovides detailed information 3502 about the gene \"icos.\" The information box 3500 can further provide one or more resources 3503 from which such detailed information is retrieved.  For example, the information box 3500 lists the following resources\n3503: HGNC, GeneCards, GTEx, NIH National Cancer Institute GDC Data Portal, cBioPortal FOR CANCER GENOMICS, UniProt, and Google.  In some embodiments, each of these resources can be linked to its respective resource website or database.\n FIG. 36 illustrates an exemplary information box 3600 in accordance with some embodiments of the present disclosure.  The information box 3600 can provide information about the entity \"icos\" when the entity \"icos\" refers to the entity type\n\"company name.\" In this context, the information box 3600 provides information about the company \"icos\" rather than about the gene type \"icos.\" In some embodiments, only the resource links that provide information about the company \"icos\" can be\ndisplayed.  For example, only the link to Google can be provided in the information box 3600 when all the other resources do not provide information about the company \"icos.\"\n Thus, in some embodiments, the system or the user can determine and choose the neighborhood sense corresponding to the desired entity type by analyzing the information in the information box.\n FIGS. 52-56 illustrate neighborhood sense diagrams for the entity \"Rho\" that is associated, with five different entity types in accordance with some embodiments of the present disclosure.  In some embodiments, \"Rho\" can be represented as a\ndifferent vector for each of its association with the five different entity types.\n In FIG. 52, the first neighborhood sense (Rho.sup.1) is associated with words related to mathematical correlations, such as \"pearson\" and \"spearman\".  Thus, it can be concluded that this neighborhood sense captures the use of \"Rho\" (the Greek\nsymbol) as the symbol for Spearman's Rank Correlation, which is also known as Spearman's Rho.\n In FIG. 53, the second neighborhood sense (Rho.sup.2) is associated with words related to other Greek symbols, including sigma, mu and pi.  Thus, it can be concluded that this neighborhood sense captures the use of \"Rho\" as the 17th letter of\nthe Greek alphabet.\n In FIG. 54, the third neighborhood sense (Rho.sup.3) is associated with words related to other GTPases, including \"guanosine\" and \"RHOA.\" Thus, it can be concluded that this neighborhood sense captures the use of \"Rho\" as the family of small\nGTPases that act as molecular switches in signal transduction cascades.\n In FIG. 55, the fourth neighborhood sense (Rho.sup.4) is associated with words related to Rho Ventures (an investment fond), including the last name of its managing director \"leschly,\" the last name of the managing partner \"kairouz,\" and the\nword \"ventures.\" Thus, it can be concluded that this neighborhood sense captures the use of \"Rho\" as the investment fund.\n In FIG. 56, the fifth neighborhood sense (Rho.sup.5) is associated with the words \"GTPASES\" and \"GTPASE\" as the top associations, in addition to \"RHOA\" (a gene symbol of the GTPase).  Thus, it can be concluded that this neighborhood sense\ncaptures the use of \"Rho\" as the GTPase protein family.\n FIG. 37 illustrates an exemplary knowledge diagram interface 3700 in accordance with some embodiments of the present disclosure.  In some embodiments, the knowledge diagram interface 3700 can provide a query box 3701, where the user can input a\nquery term (a word or a phrase) which can be used to query for entities that have scream, association with the query term.  The knowledge diagram interface 3700 can provide one or more filters for the query.  In some embodiments, the minimum number of\noccurrences 3702 for the resulting entities can be set.  For example, if the user sets the minimum number of occurrences 3702 to \"20,\" the query results can only include entities that occur at least 20 limes in the corpus.\n In some embodiments, the number of results 3703 for the query can be set.  For example, if the user sets the number of results 3702 to \"20,\" only the 20 results are displayed in the output box 3708 that displays the results of cosine analysis of\nword embeddings and related data.  If the number of results is less than 20, the number of results displayed in the output box 3708 can be less than 20.  If the number of results is more than 20, only the top 20 results (e.g., the 20 entities with the\nhighest semantic association strengths) can be displayed.  The other results can be ignored.  Alternatively, a function can provide the user to navigate to another page(s) for the other results.\n In some embodiments, the minimum semantic association strength 3704 can be set.  For example, if the minimum semantic association strength is set to \"0.0,\" all the entities are considered for the query regardless of their semantic association\nstrength.  However, if the minimum semantic association strength 3704 is set to \"0.3,\" only the entities that have their semantic association strengths of 0.3 or more are considered.\n In some embodiments, the corpus selection function 3705 can set the corpus to be considered for the query.  The core corpus can represent a superset of all the available data sets in the system.  For example, if the corpus selection function\n3705 sets \"Core Corpus\" (which includes 98.14k documents in this example), all the available data sets (including Pubmed, Clinical Trials, FDA, SEC, Wikipedia, and Media Corpus) are considered for the query.  One or more of the individual data sets can\nbe selected by selecting options other than the core corpus in the corpus 3705.\n In some embodiments, the knowledge diagram interface 3700 can suggest one or more entities 3706 that may be of interest to the user.  Such suggestions can be based on the user's query term, and/or the user's setting.  Such suggestions can also\nbe based on the user's previous interaction with the system and/or other user's interactions with the system.  In some embodiments, the suggestions can be entities that have high semantic associations with the query term.  In some embodiments, the\nsuggestions can come from synonyms that are stored in a synonym database, in some embodiments.  FASText can be used to determine synonyms.\n In some embodiments, a knowledge synthesis box 3707 can list one or more token collections in the selected neighborhood sense for the query term 3701.  For example, the token collections associated with the neighborhood of the query term \"pcsk9\"\ncan include \"Gene modifiers,\" \"Genes,\" \"All Genes,\" \"Live Hepatocel .  . . ,\" \"Rat Liver (SEQC),\" \"Pathogenic Alle .  . . ,\" \"Liver (GTEx),\" and other neighborhoods (shown as \"More .  . . +15\").  In some embodiments, a token collection can refer to a\ncollection of entities of the same entity type.  In some embodiments, a token collection can be machine-generated and/or human-curated.\n In some embodiments, a set of token collections can be determined based on one or more entities that are associated with the query term 3701.  For example (hereinafter refer to as \"Example A\"), let's assume that the query term \"E1\" is associated\nwith the following entities: \"A1,\" \"A2,\" \"A3,\" \"A4,\" \"A5,\" and \"A6.\" Let's also assume that \"A1,\" \"A2,\" and \"A3\" belong to the token collection \"EC1,\" \"A4\" and \"A5\" belong to the token collection \"EC2\"; and \"A6\" belongs to the token collection \"EC3.\" In\nthis example, the set of token collections can be determined to include EC1, EC2, and EC3.  These token collections can be displayed in the knowledge synthesis box 3707 for the query term \"E1.\" In some embodiments, the knowledge synthesis box 3707 can\ndisplay only a subset of these token collections.  For example, the system can select to display only the top two entity collections with the highest number of entities.  In Example A, it can select to display only EC1 and EC2 because they each include\nmore entities than EC3.  In some embodiments, the system can select to display token collections based on other criteria--for example, selecting to display two token collections with the highest mean or median semantic association strengths of the\nentities included in each token collection.\n In some embodiments, a set of token collections can be determined based on one or more entities that are associated with the query term 3701 and satisfy a certain condition(s).  For example, to be included in the set of token collections, a\ntoken collection must have greater than, less than, or equal to a certain number of entities in the token collection.  As another example, to be included in the set of token collections, a token collection must haw a mean or median semantic association\nstrength that is greater than, less than, or equal to a certain number of semantic association strength, in Example A above, if a token collection requires to have at least two entities to be included in the set of token collections, then only EC1 and\nEC2 would be included in the set of token collections.  Yet in another example, not all entities that are associated with the query term 3701 are considered.  In other words, the system can filter out those entities that do not satisfy a certain\ncondition(s) before determining the set of token collections.  In Example A above, let's further assume that the system requires all entities being considered to have tit least a semantic association strength of 0.7; that A3, A4, A5, and A6 each have a\nsemantic association strength that is greater than 0.7, but A1 and A2 do not; and that the knowledge synthesis box 3707 displays only one token collection with the most number of tokens.  In this case, the knowledge synthesis box 3707 would display EC2\nbecause the system would now determine that EC1 has only A3, EC2 still has A4 and A5, and EC3 still has A6.\n In some embodiments, each token collection can be associated with a percentage, where the percentage can represent the number of tokens in the respective token collection divided by the total number of tokens in all of the token collections.  In\nsome cases, the sum of the percentages of the token collections can add up to more than 100% because one or more tokens can belong to more than one token collection.  In some embodiments, the knowledge synthesis box 3707 can select to display token\ncollections, whose percentages are greater than a certain threshold.\n In some embodiments, one or more filters can be applied, before, during, and/or after generating a list of token collections.  In some embodiments, the query term's neighbors can be filtered out from the results or the token collections when the\nco-occurrence level between the neighbors and the query term is above or below a certain threshold.  For example, only neighbors that have high co-occurrence levels can be selected.  In another example, only neighbors that have zero co-occurrence levels\ncan be selected.  Using filters, it can be controlled to have one or more of the following types of results: (1) neighbors that have high cosine distances and high co-occurrence levels; (2) neighbors that have low or zero co-occurrence levels, but are\nrelated via other entities; (3) neighbors that have high cosine distances but low or zero co-occurrence levels; (4) neighbors that have a high occurrence within the overall corpora of interest and high co-occurrence levels; and (5) neighbors that have a\nlow overall occurrence within the corpora of interest but have high co-occurrence levels.  The later filter can be of particular interest, as it can indicate an association between entities/tokens that is starting to emerge but is not yet well-known or\nrecognized.  These types of results are non-limiting and are not necessarily mutually-exclusive.\n In some embodiments, the output box 3708 can produce results associated with the query term 3701.  For example, the output box 3708 can provide the results in a decreasing order of the semantic association strengths of the resulting entities. \nThe output box 3708 can also display a magnitude, a number of occurrences, and a number of documents associated with each entity in the results.  In some embodiments, the magnitude can refer to the magnitude of a vector associated with an entity, where\nthe magnitude is the L2-norm (i.e., the square root of the sum of the squares of the individual dimensions of the vector).  For example, the entity \"circulating_pcsk9\"'s semantic association strength is 0.81.  Its magnitude is 5.4.  It occurs 494 times\nin 237 of the documents in the selected corpus.  Moreover, entity collections that are associated with each entity can be displayed.\n FIG. 38 illustrates an exemplary knowledge diagram interface 3800 in accordance with some embodiments of the present disclosure.  The knowledge diagram interface 3800 is similar to the knowledge diagram interface 3700 (FIG. 37), except that the\nselected corpus 3805 is \"Pubmed.\" Thus, in this example, the query is limited to the documents that exists in the Pubmed database, causing different results to be produced in the knowledge synthesis box 3807 and the output box 3808.\n FIG. 39 illustrates an exemplary knowledge diagram interface 3900 in accordance with some embodiments of the present disclosure.  The knowledge diagram interface 3900 is similar to the knowledge diagram interface 3700 (FIG. 37), except that the\nselected corpus 3905 is \"SEC.\" Thus, in this example, the query is limited to the documents that exists in the SEC database, causing different results to be produced in the knowledge synthesis box 3907 and the output box 3908.\n FIG. 40 illustrates an exemplary knowledge diagram interface 4000 in accordance with some embodiments of the present disclosure.  The knowledge diagram interface 4000 is similar to the knowledge diagram interface 3700 (FIG. 37), except that the\nselected corpus 4005 is \"Media Corpus.\" Thus, in this example, the query is limited to the documents that exists in the Media Corpus database, causing different results to be produced m the knowledge synthesis box 4007 and the output box 4008.\n FIG. 41 illustrates an exemplary knowledge diagram interface 4100 in accordance with some embodiments of the present, disclosure.  An entity can be associated with one or mom synonyms.  For example, the entity \"pesk9\" can have the following\nsynonyms: \"pcsk9s,\" \"pesk9_pesk9,\" \"pcsk9_ldlr,\" \"ldlr_pcsk9,\" and \"pcsk9_mediated.\" In some embodiments, synonyms can be generated in the same way as how suggestions can be generated, as described above.  Disclosed systems and methods can memo synonyms\nfor an entity such that results of a query do not list synonyms as separate words.  In some embodiments, the knowledge diagram interlace 4100 can allow the synonym merging function 4101 to be enabled or disabled.  When the synonym merging functionality\nis disabled 4101, the system treats an entity and its synonyms as different entities.  For example, when the synonym merging functionality is disabled 4101, the entity \"pcsk9\" and any of its synonyms above are treated as different entities.\n FIG. 42 illustrates an exemplary knowledge diagram interface 4200 in accordance with some embodiments of the present disclosure.  In FIG. 42, the synonym merging functionality is enabled 4201, causing an entity and its synonyms to be treated as\na single entity.  For example, the synonyms in the synonym list 4202 shows ail the synonyms of the entity \"pcsk9\" that are treated as the same entity as the entity \"pcsk9.\"\n FIGS. 43-44 illustrate exemplary knowledge diagram interfaces 4300, 4400, respectively, in accordance with some embodiments of the present disclosure.  The knowledge diagram interfaces 4300, 4400 illustrate token lists 4301, 4401, each of which\nlists the tokens in a given token collection.  For example, the token list 4301 lists all the tokens in the Genes token collection.  As another example, the token list 4401 lists all the tokens in the Liver (GTEx) token collection.\n FIG. 45 illustrate an exemplary knowledge diagram, interface 4500 in accordance with some embodiments of the present disclosure.  The knowledge diagram interface 4500 can provide an autocomplete function 4501.  When a user stalls typing an\nentity in the query box, the autocomplete function 4501 can predict the entity and provide one or more suggestions.  In some embodiments, each of the suggested entities can also include additional information, such as the number of occurrences each\nsuggested entity occurs in the selected corpus.\n According to some embodiments, a heatmap can provide a two-dimensional view of associations between multiple entities and identify relationships between them.  FIG. 46 illustrates an exemplary heatmap 4600 in accordance with some embodiments of\nthe present disclosure.  The heatmap 4600 can show associations between genes (as listed on the y-axis) and drugs (as listed on the x-axis).  In this example, the top row is for the gene \"pd_1\" and the other rows are other genes that are related to the\ngene \"pd_1.\" These genes can be compared to a collection of entities that an, FDA-approved drugs (which can include about 6,500 drugs).  A subset of these drugs that have the highest remade association strength with the gene \"pd_1\" can be selected and\ndisplayed as columns.  Each cell in the heatmap can represent the semantic association strength between the corresponding row and column entities.  In some embodiments, different color's and/or different gradients of colors can be used to represent\nvarious semantic association strengths.  A color legend 4603 can map a color (or a gradient of color) to a semantic association strength.  The heatmap 4600 can uncover various relationships between entities.  For example, in the first row, one can\nobserve that nivolumab and pembrolizumab are drugs that bind pd_1, and \"pd_1\" is most strongly associated with these drugs, as compared to other drugs.  In some embodiments, the heatmap 4600 can include an average 4601 and/or a standard deviation 4602 of\nthe semantic association strength for each row.\n FIG. 47 illustrates an exemplary heatmap 4700 in accordance with some embodiments of the present disclosure.  In this example, the top row represents the drug \"rituximab,\" and the other rows represent other drugs that are associated with the\ndrug \"rituximab.\" These drugs are compared to a collection of disease entities (which can include about 9,500 diseases).  A subset of these diseases that have the highest semantic association with the drug \"rituximab\" can be selected and displayed as\ncolumns.  Similar to the heatmap 4600, every cell value can represent the semantic association strength between the pair of entities (i.e., between a pair of the drug and disease that are represented at that cell).  The heatmap 4700 can reveal that not\nonly the indications where rituximab is currently used (i.e., various subtypes of Lymphoma), but also indications dial are seemingly \"off-label\" including Lupus Nephritis (highlighted).\n FIG. 48 illustrates an exemplary heatmap 4800 in accordance with some embodiments of the present disclosure.  The heatmap 4800 can incorporate molecular analytics.  In this example, the top row represents the gene \"TRIM32,\" and the other rows\nrepresent the other genes that are associated with the gene \"TRIM32.\" These genes are compared to a collection of entities that represent genes that are specifically overexpressed in human brain tissues obtained from the GTEx database.  This can\nrepresent a unique comparison that combines knowledge synthesis with molecular analytics related to expression of genes in the human brain.  In this example, TRIM32 has a very high association to genes that are expressed specifically in the brain.  This\ncan be seen by TRIM32 having high mean semantic association strengths across the columns.  The gene \"TRIM2\" also connects very strongly to TRIM32, and TRIM2 itself is highly expressed in the brain.\n FIG. 11 illustrates entity distribution for a search input where the neighborhood nodes change wan time.  In FIG. 11, each time slot (T1, T2, T3) illustrates the new nodes that emerged at that timestep (for the purpose of illustration).  The\ntransform 1102 can be used to vary the time ranges, etc. The matrix can capture entity neighborhood change with time and can also indicate bow a node 1101 relates to other nodes in the subsequent time step (1105 and 1106).  For example, for a term like\n\"dropout,\" a key method to avoid overfilling in machine learning models, can be used in the context of many neural net models, subsequent to the success of this technique, resulting in a large fanout as illustrated with node 1106.  In addition to terms\nthat relate to each other by actual co-occurrence in the input corpus used to construct the Knowledge graph, entities that are in the neighborhood that are not co-occurring, but are semantically related (1109 and 1110 in black) can also be identified by\nthis process.  While these semantically related entities may have false positives, these entities generate a candidate class for potential insights that would have otherwise been difficult to find out by ocular perusal of neighborhood sets across time.\n FIG. 12 illustrates an instance of output the temporal progression of concepts across entity classes.  In the illustration, the entities in the neighborhood of entity class \"statistics\" can be compared with entity class \"neural networks,\" where\nthe filter/transform 1102 can be used to compare entity class statistics at a time that precedes the entity class for neural networks, and where an entity class is a label to a set of entities.  The filter/transform 1102 can be used to alter the\ndate/time ranges to compare the neighborhood change of entity with time.  FIG. 12 also illustrates the evolution of entities representing the sank, concept from the space of \"statistics\" to \"neural networks\" space.  For an equivalent case from the\nbiological space, the entity distribution in Knowledge graph neighborhood of drug cenicriviroc changes before and after 2014.  Before 2014, the entity distribution is dominated largely by \"anti-viral\" drugs belonging to the drug class \"ccr5 antagonists.\"\nHowever, post 2014, the entity distribution in Knowledge Graph neighbor hood for the same drug shows the emergence of \"liver related diseases\" such, as NASH (non-alcoholic steato hepatitis).\n FIG. 13 illustrates the creation of an instance of Knowledge graph subsets at an instant of time.  A structured/unstructured data snapshot 1301 at an instant of time can be used to generate word, sentence, and document embeddings 1302, which in\nturn are used to create label entities 1303, and collection of labels 1304.  Embeddings can be generated by unsupervised methods like word2vec, doc2vec and also using sequence learning models like RNNs.  These terms/phrases from this process can then be\nlabeled as entities and entity classes, where structured semantic database 1305 can also used.  Furthermore, this process can also yield candidate entities and entity classes that have not been labeled with a priori knowledge.  This can be done making\nuse of entity distribution of unlabeled entities and the asymmetry of neighborhood between pairwise entities.  For example, even though term1 and term2 have one cosine distance measure, the ordering of neighborhood terms of term1 and term2 are\nasymmetric.  This asymmetric nature can also yield different entity distributions and different entity rankings.  Thus, term1 can be a medicine that comes as a neighbor of term2, but not vice versa.  Also term2 can have as its neighbors more entity\nclasses, which term1 may not.  These types of asymmetries can be used to identify candidate new entities and entity classes.  The output, of this process can be a knowledge graph of labeled entities and entity classes and unlabeled entities.  The\nknowledge graph store 1307 can be a store of a universal as wells as domain and sub-domain knowledge graphs, where a single entity (either labeled or unlabeled) can have different entity distributions.\n FIG. 14 illustrates the capturing of temporal progression of entities and consequently entity distribution over time in Knowledge Graph (\"KG\") as knowledge graph snapshots for subset spaces (KGS1 .  . . Sn) [1403] accrued over time, from\nstructured and unstructured data 1401 by the knowledge graph generation process 1402.\n FIG. 57 illustrates a data flow m accordance with some embodiments of the present disclosure.  In some embodiments, structured and/or unstructured data can be fetched and processed, by the system described in FIG. 1 (5701).  The processed data,\ncan be used to generate word/sentence embeddings and/or knowledge graphs (5702).  The processed data can also be used to populate a structured database (5701).  Different assets from 5701 and 5702 can be served through different backend servers/computer\nsystems catering to different applications (5703).  For example, word embeddings can be hosted by word vector servers (5703) and cater to downstream applications, such as a neighborhood app, which can generate neighborhood sense diagram (5704).  Sense\nembeddings can be hosted by an Adagram server/computer system and cater to sense embedding app (5704).  Word embeddings can also be used to generate heatmap data for a Heatmap app (5704).  Moreover, word embeddings can cater to temporal analysis app\n(5704).  Concurrent to tins data flow, collections can be created and hosted in backend servers/computer systems.  The curation of collections can be performed locally by leveraging off structured, and/or unstructured, data (5702).  The curation of\ncollections can also be performed remotely by anyone who wishes to enrich a particular domain of interest by curated/automatically created collections (5705).  These collections can then be hosted on backend servers (5703).  Alternatively, the backend\nservers can be a proxy to remotely hosted collections.\n FIG. 58 illustrates a control/data flow when a user makes a query request to a neighborhood app (5801) in accordance with some embodiments of the present disclosure.  The neighborhood app can provide a user interface, where the user can enter a\nquery term and the neighborhood app can generate different neighborhood senses associated with the query term.  The user interface can look similar to FIGS. 31-32.  In some embodiments, if the user query is a gene, the neighborhood (5802) of that page\nwould most likely contain gene entities and perhaps other related entities like diseases, drugs etc. In some situations, when the query term has multiple meanings, the neighborhood page would have entities semantically related to those different\nmeanings.  The sense embeddings serves to separate the senses to some degree.  (See FIGS. 52-56.)\n The semantic match with collections can generate an enriched representation through the knowledge synthesis (5803) and provide a broader/enriched view beyond just semantic neighborhood from word embeddings.  For example, even if the neighborhood\ndid not have any diseases associated with the gene, disclosed systems and methods can semantically match genes with disease collections associated with the query gene and show an enriched view that shows beyond just the semantic neighborhood.  The\nmatching of collections with the neighborhood can span from simple lexical matches to semantic matches with varying degrees of abstraction/separation.  (See FIGS. 7-10.) For example, while a query about a gene can be enriched with gene collections, it\ncan also be enriched by a related disease or even a collection involving people doing research on those genes.  In some embodiments, the triangulation that picks candidate collections is not fixed, and is quite broad and varied (lexical to semantic\nmatch) offering a truly enriched experience beyond just neighborhood entities to a query.  In some embodiments, the triangulation process can refer to mapping neighborhood results to entity collections that may be manually curated and/or machine\ngenerated.  In some embodiments, a lexical match can refer to matching a search term with the name of a token collection.  In some embodiments, a semantic snatch can refer to analyzing neighbors of the search term, and entity types that are associated\nwith the neighbors.  In some embodiments, disclosed enriched synthesis boxes are distinct from existing search systems whose information augmenting results--even if semantic--are just clusters of the semantic results, or synopsis results.  Disclosed\nsystems and methods provide true enrichment by not only semantically matching neighborhood with collections but also enable the user in one interface to get a panoramic view of the semantic match information of the collections and the current page.\n Disclosed systems, methods, and computer readable media can identify significant associations between life science entities at their incipient stages of knowledge creation, including prescient associations that predate seminal publications\nestablishing those precise causal associations.  In some embodiments, the system can also provide seamless incorporation of the growing repertoire of human canned entity collections, including custom entity collections that are subsets, supersets, or\nentirely novel sets of entities from across life science corpora.  In some embodiments, the system can rely on pre-created and/or regularly updated corpora, that are temporally sliced to various resolutions, enabling retrospective and near-real-time\ntracking of the temporal evolution in semantic association strength between life science entity pairs.  In some embodiments, the system can readily make statistical inference of the specificity that may be attributed to each association based on the\naffiliated entity collections.\n Disclosed systems and methods establish that the discovery of novel biological associations can be achieved through temporal analysts of the semantic neighborhood (e.g., in all documents found in PubMed) of a given pair of entities (words or\nphrases).  These pairs can be of any entity type used in the Life Science literature (e.g., gene-gene or gene-disease) leading to hypothesis generation that can have a profound impact in strategic decision making.  The complex set of phrases that\nconstitute life science entities (e.g., diseases, genes) are often constituted of multiple words, and preserving such phrases is central to maximizing the value of Natural Language Processing (NLP) in the Life Sciences.\n According to embodiments, temporal analysis of semantic association strengths or scores can enable identification of novel associations that predate or coincide with a seminal biological discovery published in the scientific literature.  The\nstrong semantic association score signal can occur on the year of the seminal publication, or several years prior to such a seminal publication.  Consequently, the semantic association scores (cosine distances) described herein can be used today to\npredict novel biological associations that have yet to be disclosed in the biomedical literature.\n Disclosed systems and methods can identify and visualize, at the incipient stages, significant associations between hie science entities (e.g., the gene EGFR is a life science entity).  Sets of entities can be grouped into entity collections,\nwhich include but are not limited to the following: Biomolecules (e.g., genes, DNA or RNA polymers, proteins, lipids, metabolites, coding and non-coding RNA, peptides, antigens, mutations, etc.), Bio-entities (e.g., cells, organs, etc.), Diseases (e.g.,\nNon small cell lung cancer, Rheumatoid Arthritis, Hypercholesterolemia, Multiple Sclerosis, Parkinson's disease, NASH, NAFLD, AIDS, Sepsis, etc.), Adverse Events, Microorganisms (e.g., H. pylori, influenza H1N1 virus.  Hepatitis C Virus, Candida\nalbicans, etc.), Assays (e.g., High throughput cell screening, Kinome profiling.  Growth inhibition, mass spectrometry, etc.), Companies/Institutions (e.g., pharmaceutical, biotechnology, CROs, diagnostics/device manufacturers, hospitals, clinics,\nuniversities, etc.), People (e.g., researchers/scientists, doctors/physicians, physician names, NPI IDs of physicians, executives, etc.), Phenotypes (e.g., in-vitro, in-vivo observable/measurable/subjective, etc.), Drugs (e.g., compounds/small molecules,\nantibodies, cells, etc.), Medical instruments, Medical Procedures (e.g., surgery, transplantation, radiation etc.), and other entity collections that can be compiled by users of diverse Biomedical corpora (see FIG. 15).  In some embodiments, the terms\n\"knowledgebase\" and \"entity collection\" are interchangeable.\n FIG. 15 illustrates exemplary entity collections in accordance with some embodiments of the present disclosure.  FIG. 15 highlights super-collections that include several smaller sub-collections, as well as collections that overlap across\nmultiple other entity collections in accordance with some embodiments of the present disclosure.  The superset of all collections in the Life Science corpus itself may be construed as a \"Master Entity Collection\" (the collection of all collections and\nentities in the corpus).  In some embodiments, custom collections that will be created by users of the system may also be labeled as Entity Collections.  In the entity collection schematic visualized herein, diverse entity collections can be deposited,\nwhere entities can belong to multiple entity collections, and entity collections can be nested within one another or extend across other entity collections.\n According to some embodiments, a set of industry specific entity collections can be created to provide a basis for the comparison of the evolution history of the \"aggregated collection\" against a singleton entity so that statistically robust\ninference can be made, for example, on the salience of the singleton entity's association with another entity over time.\n Vector Space Models represent words in a continuous vector space where \"semantic-ally\" similar words are mapped to neighboring points (i.e., such words are embedded nearby each other in a synthetic high-dimensional space).  Such techniques have\na long, rich history in the field of Natural Language Processing (NLP), but all methods depend in some way or another on the Distributional Hypothesis, which states that words that appear in the same contexts share semantic meaning.  The different\napproaches that leverage this principle can be divided into two categories: count-based methods (e.g., Latent Semantic Analysis), and Predictive methods (e.g., neural probabilistic language models).  Count-based methods compute the statistics of how\noften some word co-occurs with its neighbor words in a large text corpus, and then map these count-statistics down to a small, dense vector for each word.  Predictive models directly try to predict a word from its neighbors in terms of learned small,\ndense embedding vectors (considered parameters of the model).  Word2vec is a particularly computationally-efficient predictive model for learning word embeddings from raw text.  It comes in two flavors: the Continuous Bag-of-Words model (CBOW) and the\nSkip-Grain model.  (See Section 3.1 and 3.2 in Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean, Efficient Estimation of Word Representations in Vector Space, ICLR Workshop, 2013 (\"Mikolov et al.\")).  Algorithmically, these models am similar,\nexcept that CBOW predicts target words (e.g., \"mat\") from source context words (e.g., \"the eat sits on the\"), while the skip-gram does the inverse and predicts source context-words from the target words.  This inversion might seem like an arbitrary\nchoice, but statistically it has the effect that CBOW smoothens over a lot of the distributional information (by treating an entire context as one observation).  For the most part, this turns out to be useful for smaller datasets.  However, skip-grain\ntreats each context-target pair as a new observation, and this tends to do better for larger datasets, such as the gargantuan Life Sciences corpus summarized in Table 1 below.\n TABLE-US-00002 TABLE 1 Data Source Data Type Drugs@FDA (www.accessdata.fda.gov/scripts/cder/drugsatfda/) Largely Drug marketing labels (full prescribing information) and associated FDA filings unstructured such as medical reviews, pharmacology\nreviews, and labeling revisions text Clinical Trials (https://clinicaltrials.gov/) Semi- Phase 1, Phase 1/2, Phase 2, Phase 2/3, Phase 3, and Phase 4 clinical trial structured records: inclusion/exclusion criteria, trial purpose, trial arms, outcome data\nsets measures, title, etc. FDA Averse Event Reporting System (FAERS) Structured (https://open.fda.gov/data/faers/) database Real World Evidence (RWE) of adverse event reports submitted to FDA by pharmaceutical/biotechnology companies, health care\npractitioners, and patients PubMed structured abstracts from the National Library of Medicine (NLM) Largely (www.nlm.nih.gov/bsd/policy/structured_abstracts.html) unstructured Abstracts from the scientific literature published across journals cited in\ntext PubMed including GWAS studies, precision medicine efforts, clinical trial outcomes, etc PubMed Central (PMC) Open Access Full-text Papers (https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/) Complete scientific journal articles from the PMC Open\nAccess (OA) papers Security and Exchange Commission (SEC) filings (www.sec.gov) Largely SEC filings from pharmaceutical, biotechnology, and healthcare companies unstructured text Wikipedia (www.wikipedia.org) Largely Wikipedia articles unstructured text\nPress Releases and Media Articles Largely BusinessWire, STAT news, MedPage today, Xconomy, FierceBiotech, unstructured FiercePharma, and primary webpages of biotechnology, pharmaceutical, and text medical device companies, CROs and regulatory agencies\n According to some embodiments, the Biomedical corpora can include, but not limited to, data from one or more of the following sources: scientific literature (e.g., articles found in PubMed.  PubMed Central--PMC Open Access, NCBI Bookshelf etc.),\nclinical literature (e.g., records in clinicaltrials.gov), regulatory literature (e.g., FDA documentation), and commercial literature (e.g., SEC filings, drug marketing information, drug prescription & transaction datasets, etc.).  The exemplary sources\nof the Biomedical corpora are enumerated in Table 1.  This corpus can be expanded and enhanced with various customer-supplied proprietary documents and/or public documents from across the Life Sciences ecosystem with the methods introduced herein. \nExamples of proprietary databases are Electronic Health Records (EHRs), Physician notes.  Adverse event reports, etc. The formats of documents can include, but are not limited to, slide decks prepared or reviewed by scientists (e.g., presentations made\nin PowerPoint or Keynote), text files or spreadsheets with analyzed data (e.g., in TXT, CSV, XLS, or XLSX formats), or documents capturing scientific, medical, clinical, commercial or regulatory insights (e.g., in DOC, DOCX, PDF, or any other suitable\nformats).\n As illustrated in FIG. 1 in accordance with some embodiments of the present disclosure, the system store 114 can capture information extracted from two or more source paths (e.g., 103a and 105a) in different forms to facilitate the synthesis of\ninformation and/or enable subsequent information extraction through different pathways (e.g., pathways 103a and 105a).  In some embodiments.  FIG. 1 includes the system store 114 that can be used to convert words into vectors and analysis of the\nresulting semantic BioKnowledge graph in accordance with some embodiments of the present disclosure.  The system store 114 can include information stored in a structured semantic database 106 (which can be a traditional database); a knowledge graph(s)\n107 (which can be directed graphs of labeled (extracted from both paths 101a and 102a) and/or unlabeled entities (extracted from the 102a path)); word embeddings 108 (which can include word(s) and/or sentence(s)), document/paragraph/sentence embeddings\n109; and sequence representations of unstructured data 110.  In some embodiments, tin example of word embedding can be word2vec.  In some embodiments, an example of document/paragraph/sentence embedding can be doc2vec.  In some embodiments, an example of\nsequence representations 110 can be Memory Neural Network (MemNN).\n According to some embodiments, the system, in FIG. 1 can take as input structured data 101 (e.g., curated by humans), unstructured data 102 (e.g., raw text), and/or semi-structured data 117 (e.g., any combination of the structured and\nunstructured data).  Examples of structured data is a table of name-value pairs, or a curated ontology of terms.  Unstructured data can be just text (e.g., tins very description).  Structured and semi-structured data can go to 101a through a\nnormalization and classification processes (103, 104) that merge them into the existing structured semantic database 106.  In some embodiments, the normalization process can involve generating Resource Description Framework (RDF) triples (node A, node B,\nwith an attribute edge connecting them).  The normalization/classification can leverage 107a, 108a off the existing structured data 106 and embeddings 108 for merging.  Unstructured data can go to 102a through a tokenization/normalization, which can\ninvolve, for example, cleaning up tokens.  In some embodiments, tokens can be words and/or phrases that constitute input to a machine teaming model.  For example, the word \"the\" is a token.  As another example, the words \"new york\" is a two-word phrase\nthat can become one token by adding a special character (e.g., \"_\") as follows: \"new_york.\" In some embodiments, text input can go through input processing, which converts the text into one or more tokens.  In some embodiments, phrases can be created as\ntokens independently of prior knowledge outside given input.  For example, when the words \"New York\" occur frequently in the input, \"New York\" can be converted into a phrase, which can then become a token as \"New_York.\" In some embodiments, a plurality\nof words can become a phrase and/or a token even if they do not occur frequently in the input.  For example, if the words \"lung cancer\" do not occur frequently in input, they can be forced to become a phrase and/or a token, in some embodiments, a known\ntechnique (e.g., Word2Phrase) can be used in the tokenization process.  Specialized encoders may be used for handling the parsing of specific data sources 104a.  Further still, phrases can be recognized/generated by analyzing a structured database in\nwhich, such phrases exist as identified entities.\n All the processed data, can flow (103a, 105a) into a repository, such as the system store 114.  This data along with models generated from this data 107, 108, 109, 110 can serve as from system store 114.  Word and document embeddings 108 can\ninclude embeddings generated from both structured (converted to a text stream through specialized transformation) and unstructured data using models/tools, such as Word2vec, Adagram, Fasttext, doc2vec, and/or any other suitable model/tool.  Incremental\nand temporal versions of embeddings can also be generated and stored in the system store 114.  Sequence embeddings can be generated using Recurrent Neural Network (RNN) family of neural net models (e.g., bidirectional Long Short Term Memory (LSTM)\nnetworks).  In addition to embeddings, other models can also be stored in this repository--such as the knowledge graph 107 and neural net models facilitating interactions (e.g., recurrent entity networks).\n A sub-system 116 represents one embodiment of modules 111, 112, 115 facilitating interaction with the sub-system 113.  Data can flow front the system store 114 to the sub-system 116 via paths 114a and 114b Data can flow between the sub-system\n116 and the sub-system 113 via a path 116a.  While the system in FIG. 1 has been illustrated from an information flow perspective, some of the models can be trained end-to-end using the data in the system store as input and as labeled data (structured\ndata, used as labeled data).  The word, end-to-end can have the specific meaning that the parameters of the computational flow graph is trained end to end with one loss function.  For example, a bi-directional LSTM encoder/decoder can be used, with word\nembeddings 108, representing a language, and generate output in another language for an interactive application.  The models in 114 and 116 can be generated by unsupervised, supervised, and/or reinforcement learning methods for a wide variety of\ndiscovery methods.  Generative models (GANs) can also be used to create labeled data for tasks, where labeled data is sparse.\n The system in FIG. 1 can also leverage traditional graph based algorithms taking as input word embeddings to find patterns that can compensate for absence of labeled data (e.g. entity distribution).  The system can leverage off state of the art\nmodels adapting them to specific tasks and/or combining/augmenting them with traditional algorithms, one example of which is to compensate for absence of labeled data.  The models can afford live or offline interaction with system through any of the\ninterfaces 113.\n The system depicted in FIG. 1 can include a processors) that is configured to implement the functionality described herein using computer executable instructions stored in temporary and/or permanent non-transitory memory.  The processor can be a\ngeneral purpose processor and/or can also be implemented using an application specific integrated circuit (ASIC), programmable logic array (PLA), field programmable gate array (FPGA), and/or any other integrated circuit.\n The processor(s) can execute an operating system that can be any suitable operating system (OS), including atypical operating system such as any version or type of Windows, Mac OS, Unix, Linux, VXWorks, Android, Blackberry OS, iOS, Symbian, or\nother OS.  The processor(s) can also execute any instructions from web-server related hardware and/or software.\n FIG. 16 illustrates a cosine distance probability density function (PDF) graph in accordance with some embodiments of the present disclosure.  Use graph visually describes the implementation of a word2vec like Vector Space Model based on the\nsystem store 114.  The system store 114 can result in a Semantic Bio-Knowledge Graph of nodes representing the words/phrases chosen to be represented as vectors and edge weights determined by measures of Semantic Association Strength (e.g., the Cosine\nDistance between a pair of word embeddings represented as vectors in a large dimensional space).  The cosine distance ranges from 0 (representing no semantic association) to 1 (representing strongest association).  This metric of association can reflect\nthe contextual similarity of the entities in the Biomedical Corpora.\n FIG. 17 illustrates a flow chart for temporal analysis in accordance with some embodiments of the present disclosure.  Word/Phrase embeddings of temporal slices of documents can be created (Step 1701) as described m accordance with some\nembodiments of the present disclosure.  Word embeddings can be generated by means that are not limited by the ways described herein.\n In some embodiments, a time slice can represent a specific period of time (e.g., a month, a year, five years, a decade, a century, or any other period of time).  Word embeddings can be generated for each time slice.  For example, all journal\narticles published in a year can be taken as one time slice--e.g., science articles from 1996 belong to one time slice, science articles from 1997 belong to another time slice, and so on.  In some embodiments, the terms \"word embeddings\" and \"word\nvectors\" are interchangeable.\n In some embodiments, word vectors can be generated for each time slice separately or independently.  In this case, word vectors for entities are initialized randomly and independently for each time slice during the training process in machine\nlearning.  For example, when creating word vectors for entities in science articles from 1996 and 1997, the tune slice for the science articles from 1996 can be initialized independently of the time slice for the science articles from 1997.  Thus,\nsemantic associations that exist in 1996 do not affect semantic associations for 1997 because no word vector from 1996 was used in generation of the word vectors for 1997.  This approach can be useful for analyzing semantic associations for each time\nslice independently.\n In some embodiments, word vectors can be generated for each time slice by leveraging off word vectors from one or more of other time slice(s).  In this case, when generating word vectors for entities for a time slice, word vectors from another\ntime sheets) are used to start off the training process in machine learning.  For example, when creating word vectors for entities in 1997, the word vectors that were created for 1996 can be used to start off the training process.  Thus, semantic\nassociations from 1996 can affect semantic associations for 1997 because the word vectors from 1996 were used m generation of the word vectors for 1997.  This approach can be useful for analyzing sematic associations for a time slice in view of semantic\nassociations from another time slice(s).\n In some embodiments, these other time slice(s) can be from a previous time slice(s).  In some embodiments, word vectors can be generated from the entire corpus, where these embeddings can become the universe.  In some embodiments, alter word\nvectors are generated, disclosed systems and methods can analyze how the semantic neighborhood of a term changes over time.\n Once embeddings of time slices are generated, term pairs can be chosen either automatically or by user (Step 1702).  In the automatic case, for instance, candidate pairs can be chosen from a combination of entity types, such as gene x gene, gene\nx disease, or any other suitable combination of entity types.  In some embodiments, the candidate set can be culled by picking those that occur either infrequently or do not occur at all in a time slice.  These candidates, in contrast to highly\nco-occurring pairs, can be potential pairs for prediction.  These pairs can then be analyzed (Step 1703) across time slices.  In some embodiments, one method includes a novelty measure that varies across time and the cosine distance between the two\nterms.  This analysis can yield as its output (Step 1704) items for which the system can predict that these terms will likely be associated, stronger in future.  The novelty measure can bang to the fore term pairs that occur infrequently or do not occur\ntogether at all, thereby enabling the discovery of potential links between term pairs that could strengthen over time slices, and are candidates for time gain prediction.  For term pairs that have a high co-occurrence in the time slices examined,\ndisclosed systems and methods can perform a retrospective causal path study between two terms.\n In order to study the evolution of concepts quantitatively, it is important to understand the behavior of the semantic association strength metric.  The metric and the statistical properties of the metric are described under the Null hypothesis\nin order to make stronger statements on the semantic association strength's salience that arises from disclosed systems and methods.  In some embodiments, the word embedding (d-dimensional vector representation of a word or phrase in the corpus under\nconsideration) generated by the Unsupervised Neural Network ran be compared to another by using the dot product (or inner product).  The dot product between two vectors a and b is defined as: ab=.parallel.a.parallel..parallel.b.parallel.  cos .theta.  ,\nwhere .parallel.a.parallel.  and .parallel.b.parallel.  are the respective magnitudes (also termed L2 norm) of the vectors, and cos .theta.  is the cosine distance with a value ranging from -1 to 1.  The objective function used by the Neural Network is\nformulated in such a fashion as to bring together the words that co-occur in a local sliding window.  That is, the angle between such pair of words will be closer together, and the cosine distance will be higher.  One of the behaviors exhibited by the\nNeural Network is to cluster words that are semantically close to each other, in a corpus containing a diverse set of entities (\"classes\" of words such as Genes, Diseases, Companies, Drugs, People, etc.), words of the same entity type tend to have high\ncosine distances compared to a randomly chosen pair of words.  Hence, one question that repeatedly shows up while investigating word associations is the statistical significance of a particular cosine distance observed between a pair of word vectors. \nTowards assessing the statistical significance, we first formally analyze the nature of cosine distances that can be observed in a d-dimensional space consisting of word vectors that are distributed uniformly.  To declare a certain word pair association\nas significant would entail that the cosine distance of that pair should be highly unlikely to have been generated by the above random distribution.\n Let us choose a vector v on the d-dimensional unit sphere (d-sphere that lives in R.sup.d).  We want to compute the probability that another vector w randomly chosen from the unit sphere has cosine distance x from v. All the vectors can be\nassumed to be uniformly distributed in the d-space.  We have a random variable, the angle between the vectors, and a random variable X=cos .THETA., the cosine of the angle .THETA.  between the randomly chosen w and the fixed vector v. For example, the\n3-dimensional space (d=3) can be analyzed.  In a unit sphere, an arbitrary unit vector v can be fixed.  The vectors which are at angle .THETA.  from v all live on a circle of radius sin .THETA.  (the plane of this circle is at a distance cos .THETA. \nfrom the center of the sphere--see FIG. 15).  To compute the probability that the vector w has an angle .THETA.  with respect to vector v, we need to know the fractional area of the sphere where w will live.  In the 3-dimensional space, such a fractional\narea, is nothing but the product of the circumference of the specific circle (which has a radius sin .THETA.) and a small differential\n .DELTA..theta..function..DELTA..theta..fwdarw..times..DELTA..theta..times- ..times..theta.  ##EQU00002## The probability is then\n .times..pi..times..times..times..times..THETA..times..times..times..times- ..theta..times..pi.  ##EQU00003## For the general case of a d-dimensional space, these vectors will live in a (d-1)-sphere of radius sin .THETA..  Let A.sub.d(r) denote\nthe surface area of a d-sphere.  Examples A.sub.2(r)=2.pi.r, A.sub.3(r)=4.pi.r.sup.2.  The fractional area of the is A.sub.d-1(sin .THETA.)d.theta.  and the probability that the angle is .THETA.:\n .THETA..function..THETA..times..times..times..theta..function..times..tim- es..THETA..times..times..times..theta..function..varies..times..times..THE- TA..times..times..times..theta..times.  ##EQU00004##\n In the above equation, the proportionality holds, since a d-sphere of radius r has a surface area proportional to r.sup.d-2.  Changing variables from .THETA.  to x: Let x=cos .THETA.  and hence sin .THETA.= {square root over (1-x.sup.2)}\n .times..times..THETA..times..times..times..times..theta..times..times..th- eta..times..times..THETA.  ##EQU00005## .times..times..times..function..times..varies..times..times..function..ti- mes..varies..times.  ##EQU00005.2##\n.function..ltoreq..ltoreq..times..times.  ##EQU00005.3## .times..times..intg..times..times.  ##EQU00005.4## Eq.  2 gives the probability density function of the cosine distance distribution.\n FIG. 16 illustrates the probability density function (pdf) for the various N-dimensional space in accordance with some embodiments of the present disclosure.  In some embodiments, the typical dimensionality used by a neural network is 300.  As\ncan be seen in the graph, the distribution is highly peaked with most of the mass centered around 0--that is, a randomly chosen pair of vectors typically are orthogonal or close to orthogonal (angle is close to\n .pi.  ##EQU00006## The following Table 2 shows the probability of observing vector pairs having a certain cosine distance and the expected number of random vectors for various cosine distances.\n TABLE-US-00003 TABLE 2 Expected number of random Cosine distance p-value vectors above the cosine distance 0.90 5.538751e-111 1 0.85 2.302365e-86 1 0.80 1.939344e-69 1 0.75 9.426968e-57 1 0.70 9.109259e-47 1 0.65 1.152092e-38 1 0.60 5.855381e-32\n1 0.55 2.457876e-26 1 0.50 1.389285e-21 1 0.45 1.493984e-17 1 0.40 3.933236e-14 1 0.35 3.061398e-11 1 0.30 8.135050e-09 1 0.25 8.253500e-07 40 0.20 3.493872e-05 1678 0.15 6.638752e-04 31867 0.10 6.040020e-03 289921\n For example, at a cosine distance of 0.9 (approx. angle 26.degree.), the probability is exceedingly tiny at 5.5.times.10.sup.-111; and even at a larger cosine distance of 0.3 (approx. angle 73.degree.), the probability is small at\n8.1.times.10.sup.-9.  In some embodiments, a typical corpus that is encountered in a disclosed system tends to have several million words/phrases.  Consequently, the conventional belief of using the random distribution (e.g., cosine distribution) will\ngive very good p-values, resulting in flagging too many associations as statistically significant.  One way to make the interpretation of associations tighter is to compare the expected number of random vectors above the observed cosine distance with the\nactual number of vectors above that cosine distance.  The third column of Table 2 shows the expected number of random vectors for various cosine distances.  As an example, on a core corpus with 48 million vectors, for several Life Science entities such\nas Genes, Diseases, Drugs etc., we typically have 50K+ vectors above a cosine distance of 0.3.  In such cases, it can be a logical basis to use the ratio of expected random vectors to observed actual vectors as a measure of statistical significance. \nWhen assessing statistical significance of closely related entities (such as Gene/Gene or Gene/Disease associations), a higher bar may be needed for credibility.\n In some embodiments, statistical interpretation of the significance of association strength between a pair of entities involves multiple covariates, including but not limited to the number of documents, the source of documents, and the entity\ncollections that contain the pair of word embeddings.  The resulting association metric can be tracked over time, allowing for a temporal inference of the relationship between two Life Sciences entities and establishing the statistical significance of\nsuch a relationship.  A number of examples listed below illustrate that a high semantic association strength pre-dated the eventual seminal publications that firmly established the relationship between the two entities.  This notion can be captured as\n\"Time Gain.\" which can represent the amount of time (e.g., years) between the emergence of the \"semantic signal\" (i.e., an increase in Semantic Association Strength) and the actual occurrence of the association in the primary literature (i.e., an\nincrease in documents reporting the association).\n This methodology can be used to identify specific pairs and networks of statistically significant entity associations.  Analyzing semantic association strength over time (i.e., performing Temporal Analysis) can discover and track nascent seminal\nassociations from corpora, such as the Life Science corpora.  Temporal Analysis can compare two entities, such as Life Sciences entities, by tracking their semantic association strength over time.  In some embodiments, more man two entities can be\ncompared.  For example, if there are entities A, B, C, and D, then these entities can be compared pair-by-pair in an order--such as A-B, then B-C, then A-C, then A-D, then B-D, then C-D, etc. Life Sciences entities can be classified de-novo or defined\nusing pre-existent classification schemes found in the scientific literature.  In the latter case, structured databases can be leveraged to determine entity classes.  For example, genes can be derived from NCBI's Gene Records\n(https://www.ncbi.nlm.nih.gov/gene) and/or the HUGO Gene Nomenclature Committee data set (https://www.genenames.org).  Similarly, disease names and ontologies can be obtained from the Medical Subject Headings (MeSH) collection\n(https://meshb.nlm.nih.gov).\n Once entity types are determined, their association strength in the Biomedical Corpora can be tracked over time.  Entities can be first generated by a phrase generation process, where the candidate entities are selected not only by the standard\nmeasure of their occurrence/co-occurrence of the n-gram words composing them, but also by forcibly choosing candidates obtained from a dictionary compiled from structured collections.  The standard measure can be a Pointwise Mutual Information (PMI)\nmeasure.  This can ensure a phrase is generated even if the occurrence counts do not satisfy the thresholds for becoming a phrase.  This can be of particular value in temporal slices, where the vocabulary may not be lame enough and\noccurrence/co-occurrence of terms constituting an important phrase may not be sufficient.\n In some embodiments, the phrase generation process can use the vocabulary generated for any time slice as past of the preserved word list of the subsequent time slice.  This can ensure terms that occur in a time slice can be tracked from that\npoint onwards regardless of the counts of the individual terms constituting a phrase being low.  The first step of the cascading can use a corpus vocabulary that is combination of all the corpora text, increasing the likelihood of important phrases being\npreserved in time slice training despite the low occurrence of individual icons constituting the phrase.  The phrase generation process can also ensure that the count of the constituent terms of a phrase are at least as many as the phrase count itself. \nThis can ensure that during the training, all the phrases and their constituent terms can participate in the training process, individually and as composites.\n The training process can generate word vector embeddings by iterating through the corpus generated above.  Word embeddings can be generated for each time slice that is then used, for temporal analysis and comparison of entities over time.\n The training process can perform two forms of generation of vectors.  In one form, the word vectors of every slice is initialized randomly, and the vector embeddings are learnt during training.  This form of training is useful to see how a\nparticular term evolved relative to another term independent of its past.  In the second form of training, every time slice is instantiated prior to training by the embeddings of the previous instance.  This can be particularly useful to see how a term\nevolved over time.\n The entity type of each term in a time slice can be evaluated for that time slice using an entity type distribution computation, constrained to that corpus.  This can enable the tracking of a term's semantic meaning over time.  For instance, the\nneighborhood of a term may change with time, causing its entity type distribution to change with time.  This method can enable tracking of entity type distribution change over time.\n For each time slice, causal pathways that brought two terms together can be analyzed by examining a training graph generated by keeping track of words that occur within the training window.  This training graph overlaid on the trained cosine\ndistance graph can provide the causal explanation for words coming together at different levels of indirection.\n Within the Temporal Analysis plots, certain markers associated with nascent associations can be identified.  A sharp increase in Semantic Association Strength that precedes an increase in documents containing both entities can be a clear signal\nthat the system can predict seminal associations before they are reported in the literature.  The increase in Semantic Association Strength can be captured, as a maximum of the second-derivative of the curve, whereas the increase in the document count\ncan be captured by looking at the slope of the curve in a fixed axis or through crossing of a pre-specified document count threshold.  Repeated Time Gain cases for known seminal associations validate disclosed systems and methods as having predictive\ncapabilities.  Life Sciences entity pairs that have a high Semantic Association Strength today with no documents with both of them occurring can be flagged as potentially novel and seminal associations.\n Nascent Life Sciences entity associations that are detected can be further characterized by their features that can be found in various proprietary and/or public datasets.  For example, for gene entities, their expression in normal human tissues\ncan be determined by using a dataset, such as the GTEx dataset from the Broad Institute (https://gtexportal.org/home/), and correlate that to their Semantic Association Score.  Similarly, gene and disease associations can be stress-tested for novelty by\ndetermining their association score in database, such as the OpenTargets database (https://www.targetvalidation.org/), which should be low for our predicted nascent Life Sciences entity pairs.\n Statistical Interpretation\n Multiple factors can affect the association between two entities.  When comparing two entities, the entity collection to which each entity belongs can affect the Semantic Association Strength.  Furthermore, each entity has a different\ndistribution of Semantic Association Strength when being compared to a given entity collection.\n FIG. 18 illustrates a density distribution of semantic association strength for two genes against all disease entities in accordance with some embodiments of the present disclosure.  The semantic association strength (cosine distance on the\nx-axis and, probability density function on the y-axis) can be measured for genes, such as ADAL and ADGRG1, against some or all disease entities.  The two distributions can be different.  For example, FIG. 18 illustrates that the distributions for ADAL\nand ADGRG1 are different.  When comparing the gene ADGRG1 to all disease terms in an entity collection of diseases, the distribution of Semantic Association Strengths has a mean of 0.34 and a standard deviation of 0.13.  On the other hand, when comparing\nthe gene ADAL to the same collection of disease terms, the distribution of Semantic Association Strengths has a mean of 0.19 and a standard deviation of 0.067.  On average, one would expect that majority of gene-disease associations to be noise rather\nthan representing causal links.  In some embodiments, because the distribution of Semantic Association Strengths for a given entity class vanes depending on the entity queried, this effect should be accounted for in attempting to draw statistical\ninferences.\n Disclosed systems and methods can aid users in interpreting their queries by providing a measure of the background model of the semantic association strength for a query of the type they are entering.  FIG. 19 illustrates a process for\nevaluating statistical background model and presenting summary statistics to user in accordance with some embodiments of the present disclosure.  A user can input two entities E1 and E2 for comparison (Steps 1905, 1910, 1915) A suitable entity collection\n(e.g., the most relevant entity collection) to which E1 and E2 each belong can be computed by default, or the user can specify the entity collection to be used for each entity in the query (Steps 1920, 1925).  The two selected entity collections can be\ndefined as EC1 (length n.sub.1) and EC2 (length n.sub.2) for E1 and E2, respectively (Steps 1930, 1935).  E1 can be subtracted from EC1 to generate a new entity collection called EC1' (Step 1940).  The same operation can be performed on EC2 to generate\nEC2' (Step 1945).  The Semantic Association Strength between E1 and all members of EC2' (containing n.sub.2-1 members) can be computed to generate a vector D.sub.2 with length n.sub.2-1.  The Semantic Association Strength can also be computed between E2\nand all members of EC1' (containing n.sub.1-1 members) to generate vector D1 (Steps 1950, 1955).\n The vectors D1 and D2 represent the distribution of Semantic Association Strengths for all queries of type E2.times.EC1 and E1.times.EC2, respectively.  These distributions can be useful for the purpose of using as a background (null) model for\nstatistical inference of significant associations.  To aid in this statistical inference, summary statistics such as the mean of D1 and/or D2 can be computed and presented to the user (Steps 1960, 1965).  These summary statistics include, but are not\nlimited to the mean, median, percentiles, and p-values.  More complex functions can also be presented.  One such function can be the area to the right of the probability distribution function of random draws from D1 and D2.  In this equation, one views\npoints from D1 and D2 as random variables (d1 and d2).  The sum of these random variables is defined as a new random variable h (Equation 1).  The probability distribution function of h can be computed as the convolution (*) of D1 and D2.  The observed\nSemantic Association Strength of E1.times.E2 (SAS.sub.E1.times.E2) is drawn from D1 and D2, thus the statistic of merit (p.sub.conv) is the proportion of random draws from D1 and D2 that exceed twice SAS.sub.E1.times.E2.\n .times..times..times..times.  ##EQU00007## .function..times..times..times..times.  ##EQU00007.2## .intg..times..times..times..times..times..infin..times..function..times.  ##EQU00007.3##\n FIG. 20 illustrates an example of the display of these summary statistics overlayed with temporal analysis.  In this example, the gene \"EPX\" is queried against the disease entity \"eosinophil_count.\" Summary statistics--including the mean (line\n2001), 25.sup.th percentile and 75.sup.th percentile (box 2002) of the semantic association strength for all queries of type EPX vs disease_entity--are presented for each year presented in the temporal analysis.  Summary statistics include, but are not\nlimited to, the mean, median, percentiles, and p-values.  The number of documents containing both entities is also shown for each year.\n Novelty Measure\n One of the interesting properties of the high dimensional vector space produced by the Neural Network is the clustering of certain vectors, whose corresponding phrases have not co-occurred in the corpus even once.  This is counter intuitive at\nfirst sight: the optimization criteria used to train the Neural Network relies on maximizing the probability of words occurring with a small local sliding window.  The vectors corresponding to the words in a sliding window are typically brought together\nas part of the back propagation process--that is, more frequently two words occur together in a sliding window, it would be natural to expect a high cosine distance between that pair of vectors.  However, one principle used in the training process (also\ncalled as Negative Sampling or Noise Contrastive Estimation), explicitly minimizes the cosine distance between frequently occurring words (e.g., common English words, such as \"the,\" \"of,\" \"to,\" etc.) and other words in the sliding window.  The net result\nof the above can enable two words that have not co-occurred even once (or have co-occurred very few times) to still have a high cosine distance.  Such an association can be termed as a \"Novel Association,\" as the association between such pair of words is\nstrong (i.e., high cosine distance) despite the lack of evidence in the primary literature.\n FIG. 21 illustrates two histograms generated from a random set of vectors (in vector space generated by the Neural Network) where one distribution (denoted as \"DISTANCE&lt;0.32\") represents all vector pairs whose cosine distance is less than\n0.32 (deemed \"not strong associations\") and the other distribution (denoted as \"DISTANCE&gt;=0.32\") represents all vector pairs whose cosine distance is greater than 0.32 (deemed \"strong associations\"), in accordance with some embodiments of the present\ndisclosure.  This can show how common a phenomenon it is to find word vector pairs that have very good cosine distances but yet not co-occur even once in the corpus.  The \"DISTANCE&gt;=0.32\" bar at zero value suggests that roughly 11% of vector pairs\nwhose cosine distances where greater than 0.32 (\"strong associations\") never occurred together even once in a document.  It is also clear from the figure that albeit more of the mass of the \"DISTANCE&gt;=0.32\" distribution is skewed to the right as\nexpected (more co-occurrences and hence unsurprisingly larger cosine distances), there is a long tail of the \"DISTANCE&lt;0.32\" distribution (very high co-occurrences but small cosine distances).  The long tail is a direct consequence of negative\nsampling--where vectors corresponding to common words that co-occur quite often with significant words in a sliding window are moved away from vectors of the other words.\n According to some embodiments, a quantitative metric can be provided to measure Novelty based on Pointwise Mutual Information (PMI).  The PMI measures the strength of association between two random variables X and Y as follows:\n .function..times..function..function..times..function.  ##EQU00008## where p(x) and p(y) are the probabilities of random variables X and, Y, and p(x,y) is the joint probability of X and Y.\n Let w.sub.1 and w.sub.2 be the number of occurrences of words 1 and 2 respectively.  Let w.sub.c be the number of co-occurrences of words 1 and 2 in a corpus of size T.\n .times..times.  ##EQU00009## .times..times..times.  ##EQU00009.2## We constrain the pmi values between 0 and 1 by using the logistic function (sigmoid) and additionally use exponential damping to disfavor very large co-occurrence counts.  The\nrationale is that the larger the co-occurrence count w.sub.c, we expect the Neural Network to bring the vectors together and the damping helps to account for that artifact.\n Taken together, we define the novelty measure as follows:\n .alpha..times..times..times.  ##EQU00010## where .alpha.  is the damping coefficient (e.g., typically set to 0.01).  A novelty of 1 (or 100%) indicates that w.sub.c=0.  The novelty measure should be carefully interpreted in conjunction with the\ncosine distance, as it is possible to have a good novelty score and yet have a weak cosine distance.  In some embodiments, a typical practice is where we tend to interpret the novelty as 0 below a certain threshold cosine distance (e.g., approx. 0.3\nwould snake a good choice in 300 dimensional space of a 50M word corpus, as it represents the distance above which there is utmost one random vector and that too with a very small probability).\n In some embodiments, temporal analysis can reveal seminal associations in the Life Sciences before the major publication describing them was released.  FIGS. 22A-B illustrates temporal analysis of bona-fide Life Sciences entity pairs in\naccordance with some embodiments of the present disclosure.  These figures illustrate how the semantic association strength (Cosine Distance) between a pair of Life Sciences entities is plotted over time (in years) for the PubMed corpus (denoted by\n\"Semantic Association Strength\" in the legend) along with the document co-citations count (denoted by \"Documents with both Words\" in the legend).  As noted earlier, the document co-citations curve (denoted by \"Documents with both Words\" in the legend)\ndoes not provide any \"predictive\" edge and purely reflects the accumulating number of articles that discuss both words.  However, bona-fide biological associations (pd-1: pd-11.parallel.pd-1: tumor-infiltrating lymphoyctes) have strong semantic\nassociations even when the knowledge around these genes was nascent.  Specifically, the Semantic Association Scores for these pairs were suddenly and significantly increased around the 1997-2001 time-frame, when papers co-citing the words (pd-1:\npd-11.parallel.pd-1: tumor-infiltrating lymphocytes) had not appealed yet.  This result showcases why the Semantic Association Strength (Cosine Distance) between a pan of Life Sciences entities captures the temporal evolution of concept associations in a\nhighly sensitive fashion (when the knowledge was nascent, and only reported by a handful of articles).\n According to some embodiments, the Time Gain between when the method here described, gives a strong Semantic Association Score for two Life Sciences entity pairs and when enough number of documents co-cite the pair of words or phrases is a\nsalient application of the system.  FIG. 23 provides an exemplary PTEN-KRAS temporal analysis in accordance with some embodiments of the present disclosure.  FIG. 23 shows a graph that is similar to the graphs shown in FIGS. 20, 22A-22B, except the graph\nin FIG. 23 is for the entities PTEN and KRAS.  The oncogenes PTEN and KRAS were not documented to be associated, to each other until the year 2000 (Ikeda, T.; Yoshmaga, K.; Suzuki, A.; Sakurada, A.; Ohmori, H.; Horii A. Anticorresponding Mutations of the\nKRAS and PTEN Genes in Human Endometrial Cancer.  Oncol.  Rep.  2000, 7, 567-570), a foil 3 years after the semantic association score between PTEN and KRAS increased to significant level, providing an ample window of opportunity.  Once a significant\nincrease in the association score is detected, this can be marked as the year of interest (shown as \"Time--Emergence of Association Signal\" in FIG. 23).  In cases where the signal oscillates, it can be advantageous to wait for stabilization of the\nassociation (i.e., Time--Stabilization of Association Score), which in this PTEN-KRAS example is simultaneous with its emergence.  The Time Gain is defined as the time-period between the Emergence of Association Signal and the time when the co-occurrence\ndocuments count increases significantly (shown as \"Time--Emergence of High co-occurrence Document Count\" in FIG. 23), which is indicative of scientific community awareness of the association.\n In some embodiments, if one could predict the association of a pair of currently-unrelated disease biomarkers, this could be used to shed insight into the molecular mechanism, of the disease.  Such insights can dramatically accelerate the pace\nof pharmaceutical and clinical R&D efforts.  As a proof of concept, we have documented several additional retrospective case studies where the Semantic Association Score did predate a subsequent significant biological discovery and publication.  These\nfindings validate the system described herein that utilizes Semantic Association Score (Cosine Distance between word or phrase pairs) dynamics, and provides concrete examples where knowing said information at the time would have added tremendous valise\nto ventures in that space.  These additional illustrative exampled are outlined in FIGS. 24-27, which show graphs that are similar to the graphs shown in FIGS. 20, 22A-22B, and 23, except with different input values, such as entities.\n FIG. 24 illustrates AML1 (RUNX1)-FLT3 gene-gene association temporal analysis in accordance with some embodiments of the present disclosure.  AML1 (RUNX1) and FLT3 are two genes tightly associated with acute myeloid leukemia, and their clear\nconnection was not fully explored until after 2002 (de Guzman, C. G.; Warren, A. J., Zhang, Z.; Gartland, L.; Erickson, P.; Drabkin, H.; Hiebert, S. W.; King, C. A. Hematopoietic Stem Cell Expansion and Distinct Myeloid Developmental Abnormalities in a\nMurine Model of the AML1-ETO Translocation.  Mol. Cell.  Biol.  2002, 22, 5506-5517).  This is almost 10 years after a disclosed system detected a strong semantic association score between these two genes.\n FIG. 25 illustrates Atypical Hemolytic Uremic Syndrome-CFH (disease-gene) temporal analysis in accordance with some embodiments of the present disclosure.  In this case a single document published in 2005, describing a novel association between\nComplement Regulatory Gene Factor H (CFH) and atypical hemolytic uremic syndrome (Hageman, G. S.; Anderson, D. H.; Johnson, L. V., Hancox, L. S.; Taiber, A. J.; Hardisty, L. I.; Hageman, J. L.; Stockman, H. A.; Borchardt, J. D.; Gehrs, K, M.; et al. A\nCommon Haplotype in the Complement Regulatory Gene Factor H (HF1/CFH) Predisposes Individuals to Age-Related Macular Degeneration.  PNAS 2005, 102, 7227-7232), managed to maximize the semantic association score between the two terms.  Conventional\nassumptions would have warranted caution in exploring this association, whereas the score from a disclosed system suggests it would be prudent to pursue it.\n FIG. 26 illustrates PCSK9-LDLR (Gene-gene) temporal analysis in accordance with some embodiments of the present disclosure.  In this case, a 2004 study that first observed a correlation in expression of PCSK9 to knockdown of LDLR (Maxwell, K.\nN.; Breslow, J. L. Adenoviral-Mediated Expression of Pcsk9 in Mice Results in a Low-Density Lipoprotein Receptor Knockout Phenotype.  PNAS 2004, 101, 7100-7105) dramatically increased the semantic association score between the two genes, well before a\nnumber of subsequently published studies validated this relationship.\n FIG. 27 illustrates PCSK9-LDLR (Gene-gene) temporal analysis m accordance with some embodiments of the present disclosure.  Another example where semantic association score gives validity to published studies is the discovery of an association\nbetween oncogenes BRAF and KRAS in 2002.  (Rajagopalan, H.; Bardelli, A.; Lengauer, C. Kinzler, K. W., Vogelstein, B.; Velculescu, V. E. Tumorigenesis: RAF/RAS Oncogenes and Mismatch-Repair Status.  Nature 2002, 418, 934-934).  That single study\nincreased the association score significantly, and predates increases in document counts as well.\n FIG. 49 illustrates an exemplary user interface 4900 for a temporal analysis graph in accordance with some embodiments of the present disclosure.  The user interface 4900 can be used to perform a temporal analysis for two entities.  In some\nembodiments, the two entities can be entered into a first entity box 4901 and a second entity box 4902.  The user can click on the compare button 4903 to perform a temporal analysis between the two entities.  For example, the user can enter \"egfr\" in the\nfirst entity box 4901 and \"nsclc\" in the second entity box 4902.  The user can then click on the \"Compare\" button 4903 to produce a temporal analysis graph 4907.  In this example, the entity \"egfr\" and the entity \"nsclc\" have been analyzed over a time\nperiod between 1990 and 2016, where there are 27 times slices (1 time slice per year).\n In some embodiments, the temporal analysis graph 4907 can include one or more lines to provide information regarding the two entities.  A semantic strength association line 4912 can represent the semantic association strength between the entity\n\"egfr\" and the entity \"nsclc\" over the time period.  A \"Documents with both tokens\" line 4913 can show the numbers of documents that contain both \"egfr\" and \"nsclc\" over the time period.  The mean line 4910 can show the mean of the semantic association\nstrength for all queries of \"egfr\" vs.  the entities in the Disease entity type over the time period.  The 95th percentile line 4911 can show the 95th percentile of the semantic association strength, for all queries of \"egfr\" vs.  the entities in the\nDisease entity type over the time period.  In this example, the lines 4910, 4911, 4912, and 4913 have been drawn based on 27 plotted points.  The time period, the number of plotted points (which are based on the number of time slices), the percentile,\nand any other setting in this graph can be customized.\n In some embodiments, the mean line 4910 and the 95th percentile line 4911 can be compared to the semantic strength association line 4912 to see whether the semantic strength between \"egfr\" and \"nsclc\" is particularly strong.  For example, the\nhigher the semantic strength association line 4912 relative to the 95.sup.th percentile line 4911, the more likely that the semantic strength between \"egfr\" and \"nsclc\" is particularly strong.\n In some embodiments, the universe of corpus 4904 that is used for the analysis can be selected.  In this example, the Pubmed database has been selected, causing the temporal analysis graph 4907 to be produced based on this database.  In some\nembodiments, the control collection 4909 can be customized.  In this example, the control collection is \"Disease,\" which indicates that the temporal analysis graph 4907 is generated based on this entity collection.  For example, the mean line 4910 is\nbased on comparing \"egfr\" against the entities in the control collection \"Disease\" (i.e., the Disease entity type).\n In some embodiments, the percentile 4908 can be customized for the temporal analysis graph 4907.  For example, when the percentile 4908 is set to \"95,\" the 95th percentile line 4911 is drawn to show the 95th percentile of the semantic\nassociation strength for ail queries of a given entity vs.  the entities in a given entity type.  As another example, if the percentile 4908 is set to \"30,\" a 30th percentile line can be drawn to represent the 30th percentile of the semantic association\nstrength for all queries of a given entity vs.  the entities in a given entity type.\n In some embodiments, the current semantic association strength 4905 and the current novel association 4906 can be displayed.  In this example, the current semantic association strength between \"egfr\" and \"nsclc\" is shown as \"0.58.\" The novel\nassociation between them is \"0%,\" which can indicate that the probability of the association between the two entities being novel is zero.  In some embodiments, the novelty score ran be inversely proportional to the total number of documents with both\ntokens.\n FIG. 50 illustrates tin exemplary knowledge graph interface 5000 with a temporal analysts graph in accordance with some embodiments of the present disclosure.  The knowledge graph interface 5000 can show that ail or a subset of an entity type\n(e.g., \"All Diseases\") can be selected (5001) as the control collection.\n FIG. 51 illustrates an exemplary knowledge graph interface 5100 with a temporal analysis graph in accordance with some embodiments of the present disclosure.  The knowledge graph interface 5100 can show information relating to the entity\n\"parkinsons_disease\" as it relates to the entity \"tremors.\"\n We also compared all Life Sciences associations enclosed in the OpenTargets database and their relevant association score (referred to hereafter as \"OT Score\") to the given pairs semantic association strength.  The OpenTargets Platform seeks to\nannotate gene-disease pairs with evidence of an association between them agglomerated from various sources, including an alternative NLP method for text-mining.  Overall, we found a poor correlation between the association scores, as shown in FIG. 28.\n FIG. 28 illustrates a relationship between OT Score and cosine distance (semantic association score) in accordance with some embodiments of the present disclosure.  This is a plot of OpenTargets association score (OT Score) with respect to their\nCosine Distance (Semantic Association Score) for all Life Science entity pairs found, in OpenTargets.  The distribution graph 2801 (on the right side of the y-axis) is based on the OT Association Score (where closer to 1 represents higher association),\nand the distribution graph 2802 (on top of the x-axis) is based on the Cosine Distance (which in turn is based on analysis of thousands of gene/disease associations).  The rectangle area (enclosed by the x-axis, y-axis, and the distributions graphs 2801,\n2802) represents the mapping between the OT Association Score and the cosine distance.  FIG. 28 shows that this mapping is not one-to-one.  Disclosed systems and methods have, thus, discovered that there are differences between what the OT Association\nScore reveals and what the Cosine Distance reveals.  These differences can be due to errors and/or deficiency in the OT Association Score.\n Further inspection revealed that OT Scores are bimodal, with a small subset having a very high score and the rest having a low score.  These high scores are attributed to well-known gene-disease associations (e.g., BRAF-neoplasms), which have a\ncorresponding high Semantic Association Score.  This exemplifies why current approaches to biological association discovery simply recapitulate what is already known in the literature and have little to no predictive capability.\n FIG. 29 illustrates a graphical representation of temporal statistical inference for a non-significant gene-disease interaction (i.e., the negative control).  The \"c9orf72 vs kuru\" line represent the cosine distance between the gene and disease\nterms (c9orf72 and kuru, respectively).  The \"c9orf72 vs all diseases (25-75% tile) bars represent the 25-75th percentiles for the cosine distances between c9orf72 and all diseases.  The \"-log(p-value)\" line represents the negative log of the p-value for\nthe gene-disease relationship queried being different from the true disease-gene relationship mean.  In this case, the gene c9orf72 is not associated with Kuru.\n FIG. 30 illustrates a graphical representation of temporal statistical inference for a significant gene-disease interaction (i.e., the positive control).  The \"c9orf72 vs als_ftd\" line represents the cosine distance between the gene and disease\nterms (c9orf72 and Amyotrophic Lateral Sclerosis/Frontotemporal Dementia (als_ftd), respectively).  The \"c9orf72 vs all diseases (25-75% tile)\" bars represent the 25-75th percentiles for the cosine distances between c9orf72 and all diseases.  The\n\"-log(p-value)\" line represents the negative log of the p-value for the gene-disease relationship queried being different from the true disease-gene relationship mean.  In this case, repeats in the gene c9orf72 cause the disease Amyotrophic Lateral\nSclerosis/Frontotemporal dementia.  This is clearly shown when the negative log of the p-value significantly jumps up in the 2010-12 timeframe.  This timeframe to the year when the number of co-occurring documents between c9orf72 and Amyotrophic Lateral\nSclerosis increases substantially represents another instance of the \"Time Gain.\"\n Disclosed systems and methods can capture evolution of semantic associations between two entities over a period of time.  In some cases, as semantic associations evolve over time for a pair of entities, the user or the system can detect an\nincrease in semantic associations that may or may not be statistically significant.  In some embodiments, disclosed systems and methods can detect a time at which a statistically significant increase occurs for a pair of entities by using various\nmethods, including a method that uses the Sigmoid Curve.  In some embodiments, semantic association scores can be generated between a first entity (which can be associated with a first entity collection) and a second, entity (which can be associated with\na second entity collection) for a time period.  Semantic association scores can also be generated between the first entity and the entities in the second entity collection.  In some embodiments, when determining these semantic association scores, the\nsecond entity itself can be excluded from the calculation.  Details of systems and methods that calculate these first and second semantic association scores have been described in other parts of this disclosure.  (See e.g., FIGS. 19, 20, 22-27, 49-51 and\nthe descriptions for these figures.)\n In some embodiments, the p-values can be generated by the p-value approach to hypothesis testing when evaluating whether the semantic association score of the first entity vs.  the second entity is statistically significant when compared with\nthe semantic association score of the first entity vs.  all entities of the second entity collection.  In other words, the p-value can be employed as a measure of statistical significance of the first entity vs.  the second entity, as opposed to the\nfirst entity vs.  all entities of the second entity collection.  In some embodiments, a null hypothesis can state that the semantic association of the first entity vs.  the second entity is not statistically significant when compared with the semantic\nassociation score of the first entity vs all entities of the second entity collection.  A low p-value indicates that the null hypothesis should be rejected.  Because a low p-value results in a high-log(p-value), a high-log(p-value) should cause us to\nreject the null hypothesis.  Thus, if the semantic association of the first entity vs the second entity is significant, a relatively high-log(p-value) will result, and we can reject the null hypothesis.\n In some embodiments, a negative log p-value carve can be drawn with the time period on the x-axis and the negative log p-value on the y-axis (see FIGS. 29 and 30).  In some embodiments, when there is tin increase in the semantic association\nstrength between the first entity and the second entity over time, the negative log p-value increases over the time period in such a fashion that the Sigmoid curve can fit over the negative log p-value curve.  In some embodiments, before the Sigmoid\ncurve is fitted, the negative log p-value can be smoothened using a filter, such as the Savitzky-Golay filter.  In some embodiments, one or more fitting parameters associated with the Sigmoid curve can be optimized for speed and accuracy.\n In some embodiments, after the negative log p-value curve has been fitted with the Sigmoid curve, the following formulas associated with the Sigmoid curve can be used to determine (1) the time of increase (which is X.sub.0), (2) the saturation\nvalue (which is K+c, at X=.infin.), and (3) the area under the curve (AUC):\n The Sigmoid fit formula is:\n .function.  ##EQU00011## The area under the curve (AUC) formula is:\n .times..function..function..function..function.  ##EQU00012## In some embodiments, the saturation value can be used to approximate the final (e.g., maximum) negative log p-value.\n In some embodiments, a set of negative log p-values can be calculated for multiple pairs of entities, where, for each pain one entity in the pair is from a first entity collection and the other entity in the pair is from a second entity\ncollection.  In some embodiments, all the possible pairs of entities between two entity collections can be used to calculate a set of negative log p-values.  In some embodiments, the negative log p-value curve, as described above, can be created, to\ndetermine the time of increase, the saturation value, and/or the AUC.  In some embodiments, these multiple pairs can be compared manually and/or automatically.  In some embodiments, these multiple pairs can be displayed in a user interface.\n FIG. 59 illustrates an exemplary knowledge graph interface 5900 with a temporal analysis graph in accordance with some embodiments of the present disclosure.  The knowledge graph interface 5900 can provide semantic association strength\ninformation for multiple pairs of entities.  The knowledge graph interlace 5900 includes a query term box 5901, a submit button 5902, a minimum co-occur filter 5903, a max co-occur filter 5904, a minimum negative log P-value filter 5905, a graph type\nselection 5906, a comparison semantic entity collection tab 5907, a graph rendering section 5908, an entity pair representation 5909, and an entity pair description box 5910.\n An example use case starts when a user enters the gene \"aqp4\" as a query term into the query term box 5901 and clicks the submit button 5902.  Because the comparison semantic entity collection tab 5907 is selected as Diseases (this can be\nmanually or automatically selected), the system calculates a series of negative log p-values over time between \"aqp4\" and one or more of the entities (e.g., 1, 2, 5, all entities) in the Diseases collection.  Each \"bubble\" (e.g., bubble 5909) can\nrepresent an entity pair, for which the negative log p-values over time have been calculated.  In some embodiments, one or more entity pairs can be filtered out before, during, and/or after the negative log p-value time series calculations have been\nperformed based on one or more conditions.  For example, the user can filter out (1) those entity pairs whose number of co-occurrences of the entities m the pair are less than the minimum co-occurrence value (as specified in the minimum co-occur filter\n5903), (2) those entity pairs whose number of co-occurrences of the entities in the pair are greater than the maximum co-occurrence value (as specified in the maximum co-occur filter 5904), and/or (3) those entity pairs whose negative log p-values are\nless than the minimum negative log p-value (as specified in the minimum negative log P-value filter 5905).  In some embodiments, the bubbles corresponding to the values associated with the entity pairs are plotted in the graph rendering section 5908. \nThe bubble for a particular entity pair is placed along the x-axis and y-axis according to the time of increase value and final -log(p-value), respectively, as determined from the curve fits.  In some embodiments, the size of a bubble ran be directly\nproportional to the AUC value calculated, for the entity pair represented by the bubble, again, as determined by the curve fits.  Although not shown, the size of the bubble can be made proportional to the number of co-occurrences between the entity pair\nin the corpora being analyzed when the user makes the appropriate selection in the graph type selection 5906 control.  In some embodiments, detailed information can be provided, for each bubble.  For example, by placing the mouse cursor on the bubble\n5909 (or by using any other suitable triggering mechanism), the entity pair description box 5910 can be displayed.  The entity pair description box 5910 can display information about the entity pair (e.g., aqp4 & neuromyelitis_optica), the date of the\nincrease of the semantic association strength for the entity pair (e.g., 86.sup.th day of 2006), the negative log p-value (e.g., 3.01), the AUC value (e.g., 35.76), and/or the number of co-occurrences (e.g., 1169).  In some embodiments, by using the\nknowledge graph interface 5900, the use can generate and/or display only entity pairs that have statistical significant associations between entities.  In some embodiments, the knowledge graph interface 5900 can uncover entities with statistically strong\nsemantic association strengths even when those entities' co-occurrence is low or non-existent.\n In some embodiments, the following formula can be used to describe a relationship between a negative log p-value and a percentile of the first entity vs.  second entity, where the percentile of the first entity vs.  second entity is the\npercentile of the semantic association strength between the first entity and the second entity, as compared to the semantic association strengths between the first entity and all entities of the second semantic entity collection: Negative log\nP-value=-log.sub.10(1-Percentile/100).  For example, a 95th percentile gives a negative log p-value of about 1.3.  In some embodiments, other formulas can be used to describe relationships between negative log p-values and percentiles.\n In some embodiments, one or more semantic entities from the second semantic entity collection can be omitted when calculating the semantic association strengths between the first entity and entities of the second entity collection, although the\nabove steps describe that \"all entities of the second entity collection\" to be used.  For example, such semantic association strengths can be calculated between the first entity and all the entities of the second semantic entity collection except for the\nsecond semantic entity itself.\n Disclosed systems and methods can be used in, and/or expanded to industries other than life science.  Other industries may nave their own applicable corpus.  For example, for the entertainment industry, disclosed systems and methods can use\nmovie reviews as its corpus.\n Those of skill in the art would appreciate that the various illustrations in the specification and drawings described herein can be implemented as electronic hardware, computer software, or combinations of both.  To illustrate this\ninterchangeability of hardware and software, various illustrative blocks, modules, elements, components, methods, and algorithms have been described above generally in terms of their functionality.  Whether such functionality is implemented as hardware,\nsoftware, or a combination depends upon the particular application and design constraints imposed on the overall system.  Skilled artisans can implement the described functionality in varying ways for each particular application.  Various components and\nblocks can be arranged differently (for example, arranged in a different order, or partitioned in a different way) all without departing from, the scope of the subject technology.\n Furthermore, an implementation of the communication protocol can be realized in a centralized fashion in one computer system, or in a distributed fashion where different elements are spread across several interconnected computer systems.  Any\nkind of computer system, or other apparatus adapted for carrying out the methods described herein, is suited to perform foe functions described herein.\n A typical combination of hardware and software could be a general purpose computer system with a computer program that, when being loaded and executed, controls the computer system such that it carries out the methods described herein.  The\nmethods for the communications protocol can also be embedded in a non-transitory computer-readable medium or computer program product, which comprises ail the features enabling the implementation of the methods described herein, and which, when loaded in\na computer system is able to carry out these methods.  Input to any part of the disclosed systems and methods is not limited to a text input interface.  For example, they can work with any form of user input including text and speech.\n Computer program or application in the present context means any expression, in any language, code or notation, of a set of instructions intended to cause a system, having an information processing capability to perform a particular function\neither directly or after either or both of the following a) conversion to another language, code or notation; b) reproduction in a different material form.  Significantly, this communications protocol can be embodied in other specific forms without\ndeparting from the spirit or essential attributes thereof, and accordingly, reference should be had to the following claims, rather than to the foregoing specification, as indicating the scope of the invention.\n The communications protocol has been described in derail with specific reference to these illustrated embodiments.  It will be apparent, however, that various modifications and changes can be made within the spirit and scope of the disclosure as\ndescribed in the foregoing specification, and such modifications and changes are to be considered equivalents and part of this disclosure.\n It is to be understood dial the disclosed subject matter is not limited in its application to the details of construction and to the arrangements of the components set fords in the following description or illustrated in the drawings.  The\ndisclosed subject matter is capable of other embodiments and of being practiced and earned out in various ways.  Also, it is to be understood, that the phraseology and terminology employed herein are for the purpose of description and should not be\nregarded as limiting.\n As such, those skilled in the art will appreciate dial the conception, upon which this disclosure is based, may readily be utilized as a basis for the designing of other structures, systems, methods and media for carrying out the several\npurposes of the disclosed subject matter.  It is important, therefore, that the claims be regarded as including such equivalent constructions insofar as they do not depart from the spirit and scope of the disclosed, subject matter.\n Although the disclosed subject matter has been described and illustrated in the foregoing exemplary embodiments, it is understood that the present disclosure has been made only by way of example, and that numerous changes in the details of\nimplementation of the disclosed subject matter may be made without departing from the spirit and scope of the disclosed subject matter.", "application_number": "15713426", "abstract": " Disclosed systems, methods, and computer readable media can detect an\n     association between semantic entities and generate semantic information\n     between entities. For example, semantic entities and associated semantic\n     collections present in knowledge bases can be identified. A time period\n     can be determined and divided into time slices. For each time slice, word\n     embeddings for the identified semantic entities can be generated; a first\n     semantic association strength between a first semantic entity input and a\n     second semantic entity input can be determined; and a second semantic\n     association strength between the first semantic entity input and semantic\n     entities associated with a semantic collection that is associated with\n     the second semantic entity can be determined. An output can be provided\n     based on the first and second semantic association strengths.\n", "citations": ["9514405", "20080243825"], "related": ["62514697", "62398386"]}, {"id": "20180101613", "patent_code": "10360643", "patent_name": "Distance-based social message pruning", "year": "2019", "inventor_and_country_data": " Inventors: \nBastide; Paul R. (Boxford, MA), Broomhall; Matthew E. (Goffstown, NH), Loredo; Robert E. (North Miami Beach, FL)  ", "description": "BACKGROUND\n The present invention relates generally to the field of data processing, and more particularly to data processing based on a user profile or attribute.\n Social media are computer-mediated tools that allow people, companies, and other organizations to create, share, or exchange information, career interests, ideas, and pictures/videos in virtual communities and networks.  Social media depend on\nmobile and web-based technologies to create highly interactive platforms through which individuals, communities, and organizations can share, co-create, discuss, and modify user-generated content.  They introduce substantial and pervasive changes to\ncommunication between businesses, organizations, communities, and individuals.\nSUMMARY\n Embodiments of the present invention disclose a method, computer program product, and system for dynamically processing information in an activity stream based on uniqueness and a relationship criteria.  Correspondences in a social networking\nsystem are analyzed to determine at least one topic.  An activity stream with the at least one topic is analyzed.  A target audience for the activity steam is identified.  The activity stream is analyzed according to a uniqueness and a relationship\ncriteria to form an assessment.  The assessment is analyzed to a predetermined action criteria.  Performing an action responsive to determining the assessment satisfies the predetermined action criteria. BRIEF DESCRIPTION OF THE DRAWINGS\n These and other objects, features and advantages of the present invention will become apparent from the following detailed description of illustrative embodiments thereof, which is to be read in connection with the accompanying drawings.  The\nvarious features of the drawings are not to scale as the illustrations are for clarity in facilitating one skilled in the art in understanding the invention in conjunction with the detailed description.  In the drawings:\n FIG. 1 illustrates a networked computer environment according to at least one embodiment;\n FIG. 2 illustrates an operational flowchart illustrating an example pruning process by a dynamic pruning program according to at least one embodiment;\n FIG. 3 illustrates a close-up view of a portion of an exemplary knowledge graph for the at least one determined topic and the remaining correspondence within a social networking system according to at least one embodiment;\n FIG. 4 is a block diagram of internal and external components of computers and servers depicted in FIG. 1 according to at least one embodiment;\n FIG. 5 is a block diagram of an illustrative cloud computing environment including the computer system depicted in FIG. 1, in accordance with an embodiment of the present disclosure; and\n FIG. 6 is a block diagram of functional layers of the illustrative cloud computing environment of FIG. 5, in accordance with an embodiment of the present disclosure.\n FIG. 7 illustrates a block diagram of an example natural language processing system configured to analyze content within a social networking system, in accordance with embodiments of the present disclosure.\nDETAILED DESCRIPTION\n Detailed embodiments of the claimed structures and methods are disclosed herein; however, it can be understood that the disclosed embodiments are merely illustrative of the claimed structures and methods that may be embodied in various forms. \nThis invention may, however, be embodied in many different forms and should not be construed as limited to the exemplary embodiments set forth herein.  Rather, these exemplary embodiments are provided so that this disclosure will be thorough and complete\nand will fully convey the scope of this invention to those skilled in the art.  In the description, details of well-known features and techniques may be omitted to avoid unnecessarily obscuring the presented embodiments.\n The present invention may be a system, a method, and/or a computer program product at any possible technical detail level of integration.  The computer program product may include a computer readable storage medium (or media) having computer\nreadable program instructions thereon for causing a processor to carry out aspects of the present invention.\n The computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device.  The computer readable storage medium may be, for example, but is not limited to, an electronic\nstorage device, a magnetic storage device, an optical storage device, an electromagnetic storage device, a semiconductor storage device, or any suitable combination of the foregoing.  A non-exhaustive list of more specific examples of the computer\nreadable storage medium includes the following: a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a static random access memory (SRAM), a\nportable compact disc read-only memory (CD-ROM), a digital versatile disk (DVD), a memory stick, a floppy disk, a mechanically encoded device such as punch-cards or raised structures in a groove having instructions recorded thereon, and any suitable\ncombination of the foregoing.  A computer readable storage medium, as used herein, is not to be construed as being transitory signals per se, such as radio waves or other freely propagating electromagnetic waves, electromagnetic waves propagating through\na waveguide or other transmission media (e.g., light pulses passing through a fiber-optic cable), or electrical signals transmitted through a wire.\n Computer readable program instructions described herein can be downloaded to respective computing/processing devices from a computer readable storage medium or to an external computer or external storage device via a network, for example, the\nInternet, a local area network, a wide area network and/or a wireless network.  The network may comprise copper transmission cables, optical transmission fibers, wireless transmission, routers, firewalls, switches, gateway computers and/or edge servers. \nA network adapter card or network interface in each computing/processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium\nwithin the respective computing/processing device.\n Computer readable program instructions for carrying out operations of the present invention may be assembler instructions, instruction-set-architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, firmware\ninstructions, state-setting data, configuration data for integrated circuitry, or either source code or object code written in any combination of one or more programming languages, including an object oriented programming language such as Smalltalk, C++,\nor the like, and procedural programming languages, such as the \"C\" programming language or similar programming languages.  The computer readable program instructions may execute entirely on the user's computer, partly on the user's computer, as a\nstand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server.  In the latter scenario, the remote computer may be connected to the user's computer through any type of network,\nincluding a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).  In some embodiments, electronic circuitry including, for\nexample, programmable logic circuitry, field-programmable gate arrays (FPGA), or programmable logic arrays (PLA) may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to\npersonalize the electronic circuitry, in order to perform aspects of the present invention.\n Aspects of the present invention are described herein with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems), and computer program products according to embodiments of the invention.  It will be\nunderstood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer readable program instructions.\n These computer readable program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute\nvia the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.  These computer readable program instructions may also be\nstored in a computer readable storage medium that can direct a computer, a programmable data processing apparatus, and/or other devices to function in a particular manner, such that the computer readable storage medium having instructions stored therein\ncomprises an article of manufacture including instructions which implement aspects of the function/act specified in the flowchart and/or block diagram block or blocks.\n The computer readable program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other device to cause a series of operational steps to be performed on the computer, other programmable apparatus or\nother device to produce a computer implemented process, such that the instructions which execute on the computer, other programmable apparatus, or other device implement the functions/acts specified in the flowchart and/or block diagram block or blocks.\n The flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present invention. \nIn this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of instructions, which comprises one or more executable instructions for implementing the specified logical function(s).  In some alternative\nimplementations, the functions noted in the blocks may occur out of the order noted in the Figures.  For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse\norder, depending upon the functionality involved.  It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special\npurpose hardware-based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.\n Social networking systems may be a universal mechanism to connect end users and information in logical and organized manners that may enable sharing and processing of information between the end users.  Currently, common mechanisms of sharing\nand processing information utilize an inbox, wall, activity stream, timeline, or profile.  Hereinafter, an activity stream may include, but is not limited to, an instant messaging (IM), short message services (SMS), blog, website, community, news feed,\nemail, Voice over Internet Protocol (VoIP), inbox, wall, timeline, and profile.  These mechanisms enable an end user to rapidly share information with other end users, as well as also gather information from the end users in social networking systems,\nand have also resulted in a rapid increase in the number of correspondence that must be stored.  The number of messages sent per day may be on the order of four to five billion emails, tens of millions of photos an hour, a billion messages per day on\nsocial media websites, and five hundred million short messages per day.  The rapid increase in correspondence results in incredibly large mail files and a personal message data repository where each message may be given substantially similar treatment. \nThe likelihood two coworkers who are working on the same project from ten years ago are still corresponding regarding the past project may be very unlikely.  Accordingly, there is a clear value to selectively pruning correspondence within a social\nnetworking system.  Hereafter, pruning refers to deleting, removing, separating, merging, and/or archiving correspondence(s) within a social networking system.\n Therefore, it may be advantageous to, among other things, provide a way to dynamically parse correspondence(s) in a social networking environment, and then determine which portion of the parsed correspondence(s) to prune.  The correspondence(s)\nmay be pruned by selecting a primary population of messages within the social network, determining a unique set of attributes for the correspondence(s) of the primary population, analyzing the correspondence(s) of the primary population, establishing\ngeneration links between the correspondence(s), and then suggest conversations to be pruned based, at least in part, on the number of established generation links.\n A pruning program may be implemented to assist an end user with pruning correspondence(s) within a social network (e.g., a computing environment) by automatically establishing links between correspondence(s) (e.g., messages or text on a wall)\nwithin the social networking system.  The end user may be a user of the social networking system.  The end user may have every message since she joined the social networking system in 2013 (e.g., 120,000 messages in three years collected at a hundred\nmessages a day that amounts to five gigabytes worth of messages).  A message administrator of the social networking system may have set a limit of 100,000 messages for the end user.  As the end user has 120,000 messages, a threshold relating to the\nmessage limit may have been satisfied (e.g., the number of messages saved for the user has exceeded the limit), and the pruning program may be activated in response to the threshold being satisfied.  The pruning program may display a prompt for the end\nuser to select her most important messages until she selects a predetermined amount of messages (e.g., 100 messages).  The pruning program may group each user selected message into a conversation data element, and then may analyze attributes for the\nconversation data elements (e.g., users, subject, and natural language).  The pruning program may then identify an activity stream (e.g., remaining messages within an inbox), and then analyze the remaining messages for attributes (e.g., users, subject,\nand natural language) analyzed within and related to the user selected messages.  The pruning program may then establish a generation link with the user selected messages to the remaining (e.g., unselected) messages within the activity stream for each\noverlap in the attributes (e.g., users, subject, and natural language) that may be in the form of a link between two nodes, and a weight of one may be added for each link between a user selected message and an unselected message.  The pruning program may\nsuggest a portion of the messages within the social messaging system to prune based on the unlinked correspondence(s) elements.  The end user may end up reducing the total messages in her activity stream based on the pruning.  The end user may have\nselected these based on a tree or a graph representing the relationships between conversations that may have been generated and then displayed by the pruning program.\n The following described exemplary embodiments provide a system, method, and program product for dynamically pruning social networking systems based on keywords and analyzed data elements within user selected electronic document (e.g., messages)\nwithin the social networking system.  As such, embodiments of the present disclosure may improve the technical field of data processing by organizing/pruning correspondence within a social networking system according to a linkage between correspondences. More specifically, embodiments may reduce irrelevant correspondences within a social networking system by dynamically pruning correspondences according to content and/or keywords within user selected messages so that a user may reduce wasting resources.\n It is to be understood that the aforementioned advantages are example advantages and should not be construed as limiting.  Embodiments of the present disclosure can contain all, some, or none of the aforementioned advantages while remaining\nwithin the spirit and scope of the present disclosure.\n Referring to FIG. 1, an exemplary networked computer environment 100 in accordance with at least one embodiment is depicted.  The networked computer environment 100 may include a computer 102 with a processor 104 and a data storage device 106\nthat is enabled to run a software program 108 and a dynamic pruning program 110a.  The networked computer environment 100 may also include a server 112 that is enabled to run a dynamic pruning program 110b that may interact with a database 114 and a\ncommunication network 116.  The networked computer environment 100 may include one or more computers 102 and servers 112, only one of which is shown.  The communication network 116 may include various types of communication networks, such as a wide area\nnetwork (WAN), local area network (LAN), a telecommunication network, a wireless network, a public switched network and/or a satellite network.  It should be appreciated that FIG. 1 provides only an illustration of one implementation and does not imply\nany limitations with regard to the environments in which different embodiments may be implemented.  Many modifications to the depicted environments may be made based on design and implementation requirements.\n The client computer 102 may communicate with the server computer 112 via the communications network 116.  The communications network 116 may include connections, such as wire, wireless communication links, or fiber optic cables.  As will be\ndiscussed with reference to FIG. 4, server computer 112 may include internal components 902a and external components 904a, respectively, and client computer 102 may include internal components 902b and external components 904b, respectively.  Server\ncomputer 112 may also operate in a cloud computing service model, such as Software as a Service (SaaS), Platform as a Service (PaaS), or Infrastructure as a Service (IaaS).  Server 112 may also be located in a cloud computing deployment model, such as a\nprivate cloud, community cloud, public cloud, or hybrid cloud.  Client computer 102 may be, for example, a mobile device, a telephone, a personal digital assistant, a netbook, a laptop computer, a tablet computer, a desktop computer, or any type of\ncomputing devices capable of running a program, accessing a network, and accessing a database 114.  According to various implementations of the present embodiment, the dynamic pruning program 110a, 110b may interact with a database 114 that may be\nembedded in various storage devices, such as, but not limited to a computer/mobile device 102, a networked server 112, or a cloud storage service.  The database 114 can include a repository of any transactions associated or initiated with the dynamic\npruning program 110a and 110b.  The dynamic pruning program 110a and 110b may be updated in any system associated with the dynamic pruning program 110a and 110b (e.g., database 114).\n According to the present embodiment, a user using a client computer 102 or a server computer 112 may use the dynamic pruning program 110a, 110b (respectively) to parse correspondence within a social network system, analyze any activity stream\nassociated with the social networking system according to a uniqueness and/or a relationship criteria to form an assessment, then prune, among other actions, a portion of correspondence(s) associated with the social networking system.  The dynamic\npruning method is explained in more detail below with respect to FIGS. 2 and 3.\n Referring now to FIG. 2, an operational flowchart illustrating the exemplary pruning process 200 (e.g., executed by the dynamic pruning program 110a and 110b shown in FIG. 1) according to at least one embodiment is depicted.  At 202, the dynamic\npruning program 110a and 110b (FIG. 1) transmits a request to analyze correspondences in a social networking system to determine at least one topic.  The request to analyze correspondences in a social networking system to determine at least one topic may\nactivate the dynamic pruning program 110a and 110b (FIG. 1).  The request may have been an automatic setting made by the user as an option within the dynamic pruning program 110a and 110b (FIG. 1).  For example, the dynamic pruning program 110a and 110b\n(FIG. 1) may have an option to automatically transmit the request when correspondence(s) associated with the social networking system satisfies a threshold (e.g., an inbox associated with a user of the social networking system contains more than five\nthousand messages).  The dynamic pruning program 110a and 110b (FIG. 1) may display the request within a user interface (UI).\n At 204, the dynamic pruning program 110a and 110b (FIG. 1) determines the at least one topic.  The at least one topic may be one or messages or electronic documents that each may include attribute within text.  The dynamic pruning program 110a\nand 110b (FIG. 1) may determine the at least one topic by requesting a user to select the at least one topic using a generated list compiled by the dynamic pruning program 110a and 110b (FIG. 1) that may be displayed within the UI.  The at least one\ntopic may be a set of messages selected by the user as anchor points for analysis.  The displayed list may be all correspondence within the social networking system, and the user may select a predetermined amount of messages (e.g., one hundred messages)\nfrom the social networking system that the user may define as important or include content (e.g., events, projects, subject material, confidential information, etc.) that he thinks is important.  The at least one determined topic may be at least a single\nmessage selected by the user, up to a predefined threshold (e.g., 5 messages or 5,000 messages), or the user may select as many messages as he wishes.  The dynamic pruning program 110a and 110b (FIG. 1) may also automatically determine the at least one\ntopic based on historical data that includes previous selections by the user.  The dynamic pruning program 110a and 110b (FIG. 1) may utilize analytical software techniques commonly known in the art to generate the list of messages, or to determine the\nat least one topic.  In some embodiments, the dynamic pruning program 110a and 110b (FIG. 1) may automatically select the at least one topic from a last hour, week, month, and/or year of activity within the social networking system, or may be selected\nbased on a user's frequently used search terms or topics.\n Next, at 206, the dynamic pruning program 110a and 110b (FIG. 1) associates an activity stream with the at least one determined topic.  For example, if the at least one determined topic is a message within an inbox of a social networking system,\nthen the activity stream may be an aggregate of the remaining messages within the inbox.  The activity stream may be an aggregate of all correspondence within the social networking system, or any text written or displayed within a user profile of the\nsocial networking system.  For example, the activity stream may be an aggregate of all messages (inbox, sent, spam, drafting, trash, etc.) associated with the social networking system, and may also be any text or messages associated with a second social\nnetworking system that is also associated with the social networking system.\n Then, at 208, the dynamic pruning program 110a and 110b (FIG. 1) identifies a target audience for the activity stream.  The target audience may be correspondence within the social networking system with substantially similar or related content\nas the at least one determined topic.  For example, the target audience may have the same sender or receiver of a selected message, or include events or projects that are included in the at least one determined topic.  The dynamic pruning program 110a\nand 110b (FIG. 1) may identify the target audience by parsing the at least one determined topic (e.g., the user selected messages), then generate corresponding data structures for one or more portions of the user selected messages.  For example, the\ndynamic pruning program 110a and 110b (FIG. 1) may output parsed text elements from the user selected messages as data structures.  Additionally, a parsed text element may be represented in the form of a parse tree or other graph structure.  The dynamic\npruning program 110a and 110b (FIG. 1) may also parse audio and video recordings within the user selected messages.  The target audience may be any messages within an inbox of a social networking system that includes the parsed text elements or keywords\nthat are associated with keywords or parsed text elements of the at least one determined topic.\n Additionally, the dynamic pruning program 110a and 110b (FIG. 1) may generate then extract a correspondence identification from the at least one determined topic, then query a data repository (e.g., database 114) for all the text elements\nrelated to the conversation identification.  The dynamic pruning program 110a and 110b (FIG. 1) may link correspondence text elements based on substantially similar attributes.  The dynamic pruning program 110a and 110b (FIG. 1) may create a synthetic\nmessage representing one or more users, addresses, key terms, n-grams, natural language, and metadata.  The synthetic message may be a message compiled by the dynamic pruning program 110a and 110b (FIG. 1) that includes keywords and attributes of the at\nleast one determined topic.  The dynamic pruning program 110a and 110b (FIG. 1) may extract data from within the synthetic message using natural language processing and field access via application program interface (API) to the synthetic message, and\nmay ignore signatures or greetings within the activity stream when processing the natural language.  The dynamic pruning program 110a and 110b (FIG. 1) may keep a synthetic element of the synthetic message, such as {\"conversation\": {subject: Fred's\nRetirement, users: {fred, bob, alice, charlie}}}.  Additionally, the dynamic pruning program 110a and 110b (FIG. 1) may ignore key terms or attributes (e.g., tea, finance, etc.); and, may also decay the importance of specific terms (e.g. tea is never\nimportant, or finance is only important for a finite time); and, may ignore repeated messages or words, such as the words \"thanks\" and/or \"hello,\" or duplicate messages.  The dynamic pruning program 110a and 110b (FIG. 1) may ignore terms, or messages,\nand then store those matching messages in a separate data repository.\n According to at least one embodiment, the dynamic pruning program 110a and 110b (FIG. 1) may be or include a natural language processing system capable of executing entity resolution techniques that may be helpful in identifying important\nentities within the user selected messages.  Entity resolution techniques may identify concepts and keywords within a user selected message.  Once entities have been identified, correlations and linguistic links between entities may be detected and used\nto establish relevance of the entities and, ultimately, the context of the user selected messages.  An example technique that may be useful in determining the relative importance of a given entity to the context of the passage is inverse document\nfrequency, which utilizes the relative commonality of the entity as an indicator of its importance to evaluating context.  Many other techniques may also be used.  These same techniques may be useful for determining the main idea or critical words of the\nuser selected messages and then identifying a target audience (e.g., remaining messages within the activity stream that include a substantially similar main idea or keyword).\n The text elements may be any words in the form of text, audio, or video that appears more than once or have a relative importance to the user selected messages (e.g., the message date, title, sender, receiver, and/or frequently used words).  The\ntext elements may be a concept within the text of the user selected messages or within an audio and video recording within the user selected messages.  The parsed text elements or keywords of the user selected messages may be included more than once and\nmay be a different font (e.g., larger than other words within the user selected messages) or presented in a different manner than other words within the user selected messages (e.g., bolded or in italics).  Additionally, the text element and/or keywords\nmay be listed in a table for visual view to the end user.  The table may be ordered based on user pre-configuration (e.g., most important to least important).\n According to some embodiments, the dynamic pruning program 110a and 110b (FIG. 1) may generate a knowledge graph for the user selected messages.  The knowledge graph may have the same or similar characteristics and/or appearance as the knowledge\ngraph that will be discussed in reference to FIG. 3.  For example, the knowledge graph may include a plurality of nodes and edges.  The nodes may relate to concepts found in the user selected messages, such as keywords, message titles, and/or who is\nreceiving/transmitting the correspondence.  The nodes may be linked together with edges to represent a connection between the nodes.  The knowledge graph will be discussed in further detail with reference to FIG. 3.\n Then, at 210, the dynamic pruning program 110a and 110b (FIG. 1) analyzes the activity stream according to a uniqueness (e.g., a generated numerical value) and a relationship criteria to form an assessment.  The uniqueness may be associated with\nlinks of the knowledge graph that connect two nodes.  The links may have a metric \"distance\" that may define the uniqueness (e.g., quantity of connections) and the relationship (e.g., the type of connection) between two nodes that are connected by the\nlink.  The types of connections may be based on generation (e.g., a measure of relatedness), conversation subject, natural language, and/or addressees.  The relationship criteria between any two nodes may be based on an elapsed time between the nodes\n(e.g., frequency of correspondence or date since last correspondence), a community distinction (e.g., a member of a rock climbing community communicating with a member of a skydiving community), and/or a social relationship between the target audience\nand the at least one determined topic (e.g., correspondence between a manager and an assistant).  The dynamic pruning program 110a and 110b (FIG. 1) may analyze the activity stream in multiple iterations, each iteration analyzing the activity stream\naccording to a disparate synthetic message that includes one or more key terms related to the at least one determined topic.  The assessment may be in the form of a grouped activity stream that includes the selected messages, target audience, and the\nremaining messages that are organized into four groups: a first generation (i.e., the at least one determined topic); a second generation (i.e., the target audience); a third generation (i.e., a target audience of the target audience) that may be\nmessages that may include substantially similar content as the target audience; and, an orphan generation that may be remaining messages of the activity stream (i.e., unlinked messages within the knowledge graph) which are neither the at least one\ndetermined topic (e.g., user selected messages) nor the target audience.  The first generation, second generation, and third generation may all be stored in a same data repository (e.g., database 114) or in disparate data repositories, or some\ncombination thereof.  In some embodiments, the dynamic pruning program 110a and 110b (FIG. 1) may establish a data management policy that may comply with the following conditions: the third generation is stored on a tape, the second generation is stored\non a disk, and the first generation may be stored in a solid-state drive, or some combination thereof.\n The dynamic pruning program 110a and 110b (FIG. 1) may, in part, form the assessment by weighting links (e.g., each link has a weight of one) of the knowledge graph, as well as parsed text elements and/or keywords within the at least one\ndetermined topic and/or target audience.  Weighting may occur in instances when there are multiple identified text elements and/or keywords that are not relevant but appear multiple times within the target audience.  For example, if the unrelated text\nelements and/or keywords are \"surf\" and \"skyscraper,\" and the unrelated determined topics appear multiple times throughout the user selected messages, each unrelated determined topic may be weighted to determine which topic more accurately describes the\ncontent of the correspondence and/or target audience within the social networking system for pruning.  The dynamic pruning program 110a and 110b (FIG. 1) may weight each text element and/or keyword according to the number of appearances within the user\nselected messages or according to the location within the user selected messages (e.g., in the title).  The value of the weights given to the text elements and/or keywords may be adjusted by the user or automatically by the dynamic pruning program 110a\nand 110b (FIG. 1).\n Then, at 212, the dynamic pruning program 110a and 110b (FIG. 1) compares the assessment to a predetermined action criteria.  The predetermined action criteria may be a user defined policy or an automated policy implemented by dynamic pruning\nprogram 110a and 110b (FIG. 1) that prunes a portion of the activity stream according to a particular generation.  For example, the predetermined action criteria may prune any correspondence that is more than two generations (i.e., prune the third\ngeneration, and orphan generation); prune all generations besides the first generation; or prune the orphan generation, and store the third generation to a solid-state drive or tape; or, prune and then store all generations besides the first generation\nto separate data repositories.  In some embodiments, the dynamic pruning program 110a and 110b (FIG. 1) may determine the action criteria after no selection has occurred for a predefined amount of time (e.g., five minutes).\n Then, at 214, the dynamic pruning program 110a and 110b (FIG. 1) determines whether the assessment satisfies the predetermined action criteria based on the comparison.  If the dynamic pruning program 110a and 110b (FIG. 1) determines the\nassessment satisfies the predetermined action criteria based on the comparison (214, \"YES\" branch), the pruning process 200 may perform an action at 216.  If the dynamic pruning program 110a and 110b (FIG. 1) determines the assessment does not satisfy\nthe predetermined action criteria based on the comparison (214, \"NO\" branch), the pruning process 200 may continue to receive a user selection at 218.\n If the dynamic pruning program 110a and 110b (FIG. 1) determines the assessment satisfies the predetermined action criteria based on the comparison, then, at 216, the dynamic pruning program 110a and 110b (FIG. 1) performs an action.  The\ndynamic pruning program 110a and 110b (FIG. 1) may prune each message that does not satisfy the assessment.  For example, the dynamic pruning program 110a and 110b (FIG. 1) may prune the messages with zero linkage with other messages (i.e., orphan\ngeneration messages), or weak links (i.e., third generation messages).  The user may preconfigure the dynamic pruning program 110a and 110b (FIG. 1) to display the messages to be pruned so that the user may determine if any messages should not be\ndeleted.  The dynamic pruning program 110a and 110b (FIG. 1) may also store backup copies of any pruned messages according to the management policy.  Alternatively, the performed action may include presenting an organizational update of the activity\nstream (e.g., a merging, a separation, or an archive).  The merging may include grouping substantially similar correspondence into one or more folders within the social networking system based on the generation of the correspondence.  The separation may\ninclude separating correspondences that includes disparate content into one or more folders based on the content or the generation of the correspondence.  The archiving may include storing correspondence into one or more databases (e.g., database 114)\nbased on the content or the generation of the correspondence.  Once the dynamic pruning program 110a and 110b (FIG. 1) performs an action, the pruning process 200 may terminate.\n However, if the dynamic pruning program 110a and 110b (FIG. 1) determines the assessment does not satisfy the predetermined action criteria based on the comparison, then, at 218, the dynamic pruning program 110a and 110b (FIG. 1) displays a\nselection for the user to return to 202.  If the dynamic pruning program 110a and 110b (FIG. 1) receives a selection from the user to return to 202 (218, \"YES\" branch), the pruning process 200 may return to 202.  If the dynamic pruning program 110a and\n110b (FIG. 1) receives a selection from the user to not return to 202, (218, \"NO\" branch), the pruning process 200 may terminate.\n According to at least one embodiment, the dynamic pruning program 110a and 110b (FIG. 1) may selectively ignore messages (e.g., all messages from management, all messages with the word tea or bank, and/or all payroll stubs) based on historical\ndata, and may also ignore messages which need to follow a message retention policy (e.g., legal messages).  The dynamic pruning program 110a and 110b (FIG. 1) may establish a minimum retention policy for a message that is identified as an orphan\ngeneration, and then may present a report of the orphaned messages, and the date at which the orphaned generations are to be removed.  The dynamic pruning program 110a and 110b (FIG. 1) may predict that a message is going to be an orphan generation due\nto the reduction in the links/strengths of the links between nodes of the knowledge graph, discussed more with reference to FIG. 3.  The dynamic pruning program 110a and 110b (FIG. 1) may monitor search terms to record matches between the terms and\npossible orphaned data.  The dynamic pruning program 110a and 110b (FIG. 1) may include linked data such as files, wikis, blogs, and/or activity streams.  As previously discussed, activity streams may include files, wikis, blogs, email correspondence,\nand any other text and/or correspondence associated with a social networking system.  The dynamic pruning program 110a and 110b (FIG. 1) may store results from prior analysis to reduce repeated calculation.\n According to at least one embodiment, a user may select one or more emails within a social networking system to store.  A processor may identify key terms/concepts within the one or more user selected message using NLP.  The processor may weigh\nthe key terms/concepts according to importance (e.g., number of times the email was used, where the email was used, similar subject of email, and/or sender of the email).  The processor may score each unselected email according to the overlap of key\nconcepts (i.e., the number of times the key concepts are in each unselected email).  The processor may rank the unselected emails.  The processor may allow user (or automatically) to prune the set of unselected emails according to their score and the\nmaximum number of messages allowed.\n FIG. 3 illustrates a close-up view of a portion 300A of an exemplary knowledge graph 300 for the at least one determined topic and the remaining correspondence within a social networking system, in accordance with embodiments of the present\ndisclosure.  The close-up view of the portion 300A includes eleven nodes 301-311, with each node representing a different concept.  For example, a node may represent a title, addressee, event, key word, and/or main idea of a parsed correspondence\naccording to a uniqueness and a relationship criteria.  For example, a node may represent an at least one determined topic, or a correspondence within the activity stream.  The nodes 301-311 are connected by edges that represent connections between the\ncorrespondences.  For example, if two connected correspondences correspond to an event or an ongoing project, an edge connecting them may represent a link for the event or the project.  There may be two links connecting them, a first link representing\nthe event, and a second link representing the project.  The dynamic pruning program 110a and 110b (FIG. 1) may generate the knowledge graph 300 using known natural language processing techniques.  The illustrated portion 300A of the knowledge graph 300\nis an undirected part of the knowledge graph, meaning that the edges shown represent symmetric relations between the concepts.  If, however, an edge presented a different relationship, the edge may be a directed edge.\n The number of edges connecting two concepts may correspond to a level of relatedness between the concepts.  For example, concept A 301, which may correspond to an at least one determined topic (e.g., a first message), and concept B 302, which\nmay correspond to a correspondence within the activity stream (e.g., a second message), are connected with three edges, whereas concept A 301 is connected to concept E 305, which may correspond to a similar sender of correspondence, by a single edge. \nThis may indicate that concept A 301 and concept B 302 are more closely related than concept A 301 and concept E 305.  As an additional example, concept C 303 may correspond to correspondence within an inbox folder (e.g., the activity stream) and concept\nG 307 may correspond to correspondence within a sent folder (e.g., the activity stream), and are connected with two edges.  The two edges between concept C 303 and concept G 307 may represent a title of a message and a key term.  The dynamic pruning\nprogram 110a and 110b (FIG. 1) may assign a numerical value for two concepts based on the number of edges connecting the two concepts together.\n The numerical value may also consider the relatedness of concepts that, while not directly connected to each other in the knowledge graph 300, are each connected to the same concept.  The dynamic pruning program 110a and 110b (FIG. 1) may look\nat whether an event or key term linking two edges can be taken through other concepts to connect the two concepts.  For example, an event can be drawn to connect concept A 301 and concept F 306 by going through concept E 305, which may correspond to a\nproject that is included in both concept A 310 and concept F 306.  The length of the path may be considered when determining a numerical value (i.e., uniqueness and a relationship criteria) between two concepts.  For example, the numerical value may be\nbased on the degrees of separation between concepts.  Two concepts that are linked together (e.g., concept A 301 and concept B 302) may have 1 degree of separation, whereas two concepts that are not linked together but are both linked to a third concept\n(e.g., concept A 301 and concept F 306) may have 2 degrees of separation, for example.  Additionally, the numerical value can be inversely related to the number of degrees of separation.\n The dynamic pruning program 110a and 110b (FIG. 1) may also consider the number of other concepts that the two concepts are connected to in determining a numerical value.  For example, concept G 307 is not connected by an edge to concept A 301. \nHowever, concept G 307 and concept A 301 are both connected to concepts C 303 and B 302.  The dynamic pruning program 110a and 110b (FIG. 1) may determine that, despite not being directly connected, concepts G 307 and A 301 are somewhat related. \nAccordingly, the numerical value between concepts G 307 and A 301 may be higher than the numerical value between concept A 301 and concept I 309, which are distantly connected to each other, or than concept A 301 and concept K 311, which cannot be\nconnected.\n The illustrated portion 300A of the knowledge graph 300 has two connected components.  A connected component of an undirected graph includes a subgraph in which any two nodes in the subgraph are connected to each other by paths (including paths\nthrough other nodes), but cannot be connected to at least one other node in the graph.  For example, concept K 311 and concept J 310 are connected to each other, but no path exists in the illustrated portion 300A of the knowledge graph 300 that can\nconnect either concept K 311 or concept J 310 to concept I 309.  Likewise, any two nodes that represent concepts A through I 301-309 can be connected to each other by at least one path, but none of the nodes representing concepts A through I 301-309 can\nbe connected to either concept J 310 or concept K 311.  Because there are two subgraphs that satisfy this criteria, the illustrated portion 300A of the knowledge graph 300 includes two connected components.\n The knowledge graph 300 (or a portion thereof) may have an isolated node (i.e., an orphan generation).  An isolated node includes a node relating to a concept that does not connect to any other nodes through an edge.  Isolated nodes (i.e., an\norphan generation) may be particularly likely to exist in knowledge graphs generated for correspondences (e.g., at least one determined topic, message of the activity stream, and/or synthetic message) mentioned only briefly (e.g., in a single message). \nAn isolated node is a type of connected component.\n The nodes 301-311 may be generated using \"fuzzy logic\" and/or concept matching, which may be done to ensure that different words or phrases relating to the same concept are included in a single node (e.g., if an event's title changes throughout\nan activity stream).  Fuzzy logic is a technique that may represent different representations of an event or concept as a same entity.  For example, the at least one determined topic may refer to an event's \"title,\" \"Ceremony,\" and \"Banquet\" at different\npoints.  The dynamic pruning program 110a and 110b (FIG. 1) using natural language processing techniques and fuzzy logic may determine that all three words refer to the same concept.  Accordingly, all three terms may be represented in the knowledge graph\nusing a single node and any edges between any of the three terms and other concepts may connect to that node.\n The nodes 301-311 can be weighted according to their importance.  This may be represented in the knowledge graph 300 by making the nodes 301-311 larger or smaller.  The nodes 301-311 may be weighted according to the number of edges that connect\nto the nodes.  The nodes 301-311 may be weighted according to the importance of the associated concept.  For example, correspondences within the activity stream that include an important project may be weighted more than concepts relating to events\n(e.g., lunch and/or party).  Also, at least one topic previously used by the user may be weighted more heavily.\n One or more of the nodes 301-311 may be considered potentially important nodes.  This may be represented in the knowledge graph by making the potentially important nodes larger, smaller, or boldface type.  A node may be a potentially important\nnode if it has a high number of edges connecting to it.  For example, the dynamic pruning program 110a and 110b (FIG. 1) may determine that a node is a potentially important node by comparing the number of edges connected to the node to an important node\nthreshold.  The important node threshold may be configured by a user.  The important node threshold may be determined by the dynamic pruning program 110a and 110b (FIG. 1) based on the number of edges connected to each node.  For example, the dynamic\npruning program 110a and 110b (FIG. 1) may determine that 10% of nodes in the knowledge graph have more than 20 edges connected to them.  Accordingly, the dynamic pruning program 110a and 110b (FIG. 1) may set the important node threshold at 20 edges. \nTherefore, any node with more than 20 connected edges may be considered a potentially important node.\n It may be appreciated that FIGS. 2 and 3 provide only an illustration of one embodiment and do not imply any limitations with regard to how different embodiments may be implemented.  Many modifications to the depicted embodiment(s) may be made\nbased on design and implementation requirements.\n FIG. 4 is a block diagram 900 of internal and external components of computers depicted in FIG. 1 in accordance with an illustrative embodiment of the present invention.  It should be appreciated that FIG. 4 provides only an illustration of one\nimplementation and does not imply any limitations with regard to the environments in which different embodiments may be implemented.  Many modifications to the depicted environments may be made based on design and implementation requirements.\n Data processing system 902a, b, and 904, b is representative of any electronic device capable of executing machine-readable program instructions.  Data processing system 902a, b, and 904, b may be representative of a smart phone, a computer\nsystem, PDA, or other electronic devices.  Examples of computing systems, environments, and/or configurations that may represented by data processing system 902a, b, and, 904 a, b include, but are not limited to, personal computer systems, server\ncomputer systems, thin clients, thick clients, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, network PCs, minicomputer systems, and distributed cloud computing environments that include any of the above systems or\ndevices.\n User client computer 102 (FIG. 1), and network server 112 (FIG. 1) may include respective sets of internal components 902 a, b and external components 904 a, b illustrated in FIG. 4.  Each of the sets of internal components 902 a, b includes one\nor more processors 906, one or more computer-readable RAMs 908 and one or more computer-readable ROMs 910 on one or more buses 912, and one or more operating systems 914 and one or more computer-readable tangible storage devices 916.  The one or more\noperating systems 914 and the software program 108 (FIG. 1) and the dynamic pruning program 110a (FIG. 1) in client computer 102 (FIG. 1) and the dynamic pruning program 110b (FIG. 1) in network server 112 (FIG. 1), may be stored on one or more\ncomputer-readable tangible storage devices 916 for execution by one or more processors 906 via one or more RAMs 908 (which typically include cache memory).  In the embodiment illustrated in FIG. 4, each of the computer-readable tangible storage devices\n916 is a magnetic disk storage device of an internal hard drive.  Alternatively, each of the computer-readable tangible storage devices 916 is a semiconductor storage device such as ROM 910, EPROM, flash memory or any other computer-readable tangible\nstorage device that can store a computer program and digital information.\n Each set of internal components 902 a, b also includes a R/W drive or interface 918 to read from and write to one or more portable computer-readable tangible storage devices 920 such as a CD-ROM, DVD, memory stick, magnetic tape, magnetic disk,\noptical disk or semiconductor storage device.  A software program, such as the software program 108 (FIG. 1) and the dynamic pruning program 110a and 110b (FIG. 1) can be stored on one or more of the respective portable computer-readable tangible storage\ndevices 920, read via the respective R/W drive or interface 918 and loaded into the respective hard drive 916.\n Each set of internal components 902 a, b may also include network adapters (or switch port cards) or interfaces 922 such as a TCP/IP adapter cards, wireless wi-fi interface cards, or 3G or 4G wireless interface cards or other wired or wireless\ncommunication links.  The software program 108 (FIG. 1) and the dynamic pruning program 110a (FIG. 1) in client computer 102 (FIG. 1) and the dynamic pruning program 110b (FIG. 1) in network server computer 112 (FIG. 1) can be downloaded from an external\ncomputer (e.g., server) via a network (for example, the Internet, a local area network or other, wide area network) and respective network adapters or interfaces 922.  From the network adapters (or switch port adaptors) or interfaces 922, the software\nprogram 108 (FIG. 1) and the dynamic pruning program 110a (FIG. 1) in client computer 102 (FIG. 1) and the dynamic pruning program 110b (FIG. 1) in network server computer 112 (FIG. 1) are loaded into the respective hard drive 916.  The network may\ncomprise copper wires, optical fibers, wireless transmission, routers, firewalls, switches, gateway computers and/or edge servers.\n Each of the sets of external components 904 a, b can include a computer display monitor 924, a keyboard 926, and a computer mouse 928.  External components 904 a, b can also include touch screens, virtual keyboards, touch pads, pointing devices,\nand other human interface devices.  Each of the sets of internal components 902 a, b also includes device drivers 930 to interface to computer display monitor 924, keyboard 926 and computer mouse 928.  The device drivers 930, R/W drive or interface 918\nand network adapter or interface 922 comprise hardware and software (stored in tangible storage device 916 and/or ROM 910).\n It is understood in advance that although this disclosure includes a detailed description on cloud computing, implementation of the teachings recited herein are not limited to a cloud computing environment.  Rather, embodiments of the present\ninvention are capable of being implemented in conjunction with any other type of computing environment now known or later developed.\n Cloud computing is a model of service delivery for enabling convenient, on-demand network access to a shared pool of configurable computing resources (e.g. networks, network bandwidth, servers, processing, memory, storage, applications, virtual\nmachines, and services) that can be rapidly provisioned and released with minimal management effort or interaction with a provider of the service.  This cloud model may include at least five characteristics, at least three service models, and at least\nfour deployment models.\n Characteristics are as follows:\n On-demand self-service: a cloud consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with the service's provider.\n Broad network access: capabilities are available over a network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, laptops, and PDAs).\n Resource pooling: the provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to demand.  There is a sense of\nlocation independence in that the consumer generally has no control or knowledge over the exact location of the provided resources but may be able to specify location at a higher level of abstraction (e.g., country, state, or datacenter).\n Rapid elasticity: capabilities can be rapidly and elastically provisioned, in some cases automatically, to quickly scale out and rapidly released to quickly scale in. To the consumer, the capabilities available for provisioning often appear to\nbe unlimited and can be purchased in any quantity at any time.\n Measured service: cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported providing transparency for both the provider and consumer of the utilized service.\n Service Models are as follows:\n Software as a Service (SaaS): the capability provided to the consumer is to use the provider's applications running on a cloud infrastructure.  The applications are accessible from various client devices through a thin client interface such as a\nweb browser (e.g., web-based e-mail).  The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage, or even individual application capabilities, with the possible exception of limited\nuser-specific application configuration settings.\n Platform as a Service (PaaS): the capability provided to the consumer is to deploy onto the cloud infrastructure consumer-created or acquired applications created using programming languages and tools supported by the provider.  The consumer\ndoes not manage or control the underlying cloud infrastructure including networks, servers, operating systems, or storage, but has control over the deployed applications and possibly application hosting environment configurations.\n Infrastructure as a Service (IaaS): the capability provided to the consumer is to provision processing, storage, networks, and other fundamental computing resources where the consumer is able to deploy and run arbitrary software, which can\ninclude operating systems and applications.  The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, deployed applications, and possibly limited control of select networking components\n(e.g., host firewalls).\n Deployment Models are as follows:\n Private cloud: the cloud infrastructure is operated solely for an organization.  It may be managed by the organization or a third party and may exist on-premises or off-premises.\n Community cloud: the cloud infrastructure is shared by several organizations and supports a specific community that has shared concerns (e.g., mission, security requirements, policy, and compliance considerations).  It may be managed by the\norganizations or a third party and may exist on-premises or off-premises.\n Public cloud: the cloud infrastructure is made available to the general public or a large industry group and is owned by an organization selling cloud services.\n Hybrid cloud: the cloud infrastructure is a composition of two or more clouds (private, community, or public) that remain unique entities but are bound together by standardized or proprietary technology that enables data and application\nportability (e.g., cloud bursting for load-balancing between clouds).\n A cloud computing environment is service oriented with a focus on statelessness, low coupling, modularity, and semantic interoperability.  At the heart of cloud computing is an infrastructure comprising a network of interconnected nodes.\n Referring now to FIG. 5, illustrative cloud computing environment 1000 is depicted.  As shown, cloud computing environment 1000 comprises one or more cloud computing nodes 100 with which local computing devices used by cloud consumers, such as,\nfor example, personal digital assistant (PDA) or cellular telephone 1000A, desktop computer 1000B, laptop computer 1000C, and/or automobile computer system 1000N may communicate.  Nodes 100 may communicate with one another.  They may be grouped (not\nshown) physically or virtually, in one or more networks, such as Private, Community, Public, or Hybrid clouds as described hereinabove, or a combination thereof.  This allows cloud computing environment 1000 to offer infrastructure, platforms and/or\nsoftware as services for which a cloud consumer does not need to maintain resources on a local computing device.  It is understood that the types of computing devices 1000A-N shown in FIG. 5 are intended to be illustrative only and that computing nodes\n100 and cloud computing environment 1000 can communicate with any type of computerized device over any type of network and/or network addressable connection (e.g., using a web browser).\n Referring now to FIG. 6, a set of functional abstraction layers 1100 provided by cloud computing environment 1000 (FIG. 5) is shown.  It should be understood in advance that the components, layers, and functions shown in FIG. 6 are intended to\nbe illustrative only and embodiments of the invention are not limited thereto.  As depicted, the following layers and corresponding functions are provided:\n Hardware and software layer 60 includes hardware and software components.  Examples of hardware components include: mainframes 61; RISC (Reduced Instruction Set Computer) architecture based servers 62; servers 63; blade servers 64; storage\ndevices 65; and networks and networking components 66.  In some embodiments, software components include network application server software 67 and database software 68.\n Virtualization layer 70 provides an abstraction layer from which the following examples of virtual entities may be provided: virtual servers 71; virtual storage 72; virtual networks 73, including virtual private networks; virtual applications\nand operating systems 74; and virtual clients 75.\n In one example, management layer 80 may provide the functions described below.  Resource provisioning 81 provides dynamic procurement of computing resources and other resources that are utilized to perform tasks within the cloud computing\nenvironment.  Metering and Pricing 82 provide cost tracking as resources are utilized within the cloud computing environment, and billing or invoicing for consumption of these resources.  In one example, these resources may comprise application software\nlicenses.  Security provides identity verification for cloud consumers and tasks, as well as protection for data and other resources.  User portal 83 provides access to the cloud computing environment for consumers and system administrators.  Service\nlevel management 84 provides cloud computing resource allocation and management such that required service levels are met.  Service Level Agreement (SLA) planning and fulfillment 85 provide pre-arrangement for, and procurement of, cloud computing\nresources for which a future requirement is anticipated in accordance with an SLA.\n Workloads layer 90 provides examples of functionality for which the cloud computing environment may be utilized.  Examples of workloads and functions which may be provided from this layer include: mapping and navigation 91; software development\nand lifecycle management 92; virtual classroom education delivery 93; data analytics processing 94; transaction processing 95; and dynamic pruning 96.\n Referring now to FIG. 7, shown is a block diagram of an exemplary system architecture 700, including a dynamic pruning program 110b and a natural language processing system 712, configured to analyze the at least one determined topic or other\nelectronic communication to identify content and to generate related information based on attributes of the at least one determined topic, in accordance with embodiments of the present disclosure.  In some embodiments, a remote device may submit\nelectronic documents (such as electronic messages of the social networking system) to be analyzed to the natural language processing system 712 which may be housed on a host device.  In some embodiments, a second remote device may submit other electronic\ncontent (such as content displayed on a social networking system) to be analyzed to the natural language processing system 712.  Such remote devices may each include a client application 708, which may itself involve one or more entities operable to\ngenerate or modify content from a social networking system or other electronic communication that is then dispatched to a natural language processing system 712 via a network 116.\n Consistent with various embodiments, the natural language processing system 712 may respond to content submissions sent by a client application 708.  Specifically, the natural language processing system 712 may analyze a received content from a\nsocial networking system or other received electronic communication content to identify characteristics about the received content (e.g., keywords, events, projects, and/or titles of messages).  In some embodiments, the natural language processing system\n712 may include a natural language processor 714, and data sources 724.  The natural language processor 714 may be a computer module that analyzes the received content.  The natural language processor 714 may perform various methods and techniques for\nanalyzing the received content (e.g., syntactic analysis, semantic analysis, etc.).  The natural language processor 714 may be configured to recognize and analyze any number of natural languages.  In some embodiments, the natural language processor 714\nmay parse passages of the received content.  Further, the natural language processor 714 may include various modules to perform analyses of electronic documents.  These modules may include, but are not limited to, a tokenizer 716, a part-of-speech (POS)\ntagger 718, a semantic relationship identifier 720, and a syntactic relationship identifier 722.\n In some embodiments, the tokenizer 716 may be a computer module that performs lexical analysis.  The tokenizer 716 may convert a sequence of characters into a sequence of tokens.  A token may be a string of characters included in written passage\nand categorized as a meaningful symbol.  Further, in some embodiments, the tokenizer 716 may identify word boundaries in content and break any text passages within the content into their component text elements, such as words, multiword tokens, numbers,\nand punctuation marks.  In some embodiments, the tokenizer 716 may receive a string of characters, identify the lexemes in the string, and categorize them into tokens.\n Consistent with various embodiments, the POS tagger 718 may be a computer module that marks up a word in passages to correspond to a particular part of speech.  The POS tagger 718 may read a passage or other text in natural language and assign a\npart of speech to each word or other token.  The POS tagger 718 may determine the part of speech to which a word (or other text element) corresponds based on the definition of the word and the context of the word.  The context of a word may be based on\nits relationship with adjacent and related words in a phrase, sentence, or paragraph.  In some embodiments, the context of a word may be dependent on one or more previously analyzed content (e.g., the content of message may shed light on the meaning of\ntext elements in related message, or content of a first message by a user on an social networking system may shed light on meaning of text elements of a second message by that user on the same or different social networking system).  Examples of parts of\nspeech that may be assigned to words include, but are not limited to, nouns, verbs, adjectives, adverbs, and the like.  Examples of other part of speech categories that POS tagger 718 may assign include, but are not limited to, comparative or superlative\nadverbs, wh-adverbs, conjunctions, determiners, negative particles, possessive markers, prepositions, wh-pronouns, and the like.  In some embodiments, the POS tagger 718 may tag or otherwise annotate tokens of a passage with part of speech categories. \nIn some embodiments, the POS tagger 718 may tag tokens or words of a passage to be parsed by the natural language processing system 712.\n In some embodiments, the semantic relationship identifier 720 may be a computer module that may be configured to identify semantic relationships of recognized text elements (e.g., words, phrases) in received content.  In some embodiments, the\nsemantic relationship identifier 720 may determine functional dependencies between entities and other semantic relationships.\n Consistent with various embodiments, the syntactic relationship identifier 722 may be a computer module that may be configured to identify syntactic relationships in a passage composed of tokens.  The syntactic relationship identifier 722 may\ndetermine the grammatical structure of sentences such as, for example, which groups of words are associated as phrases and which word is the subject or object of a verb.  The syntactic relationship identifier 722 may conform to formal grammar.\n In some embodiments, the natural language processor 714 may be a computer module that may parse received content and generate corresponding data structures for one or more portions of the received content.  For example, in response to receiving\ncorrespondence from a social networking system at the natural language processing system 712, the natural language processor 714 may output parsed text elements from the correspondence as data structures.  In some embodiments, a parsed text element may\nbe represented in the form of a parse tree or other graph structure.  To generate the parsed text element, the natural language processor 714 may trigger computer modules 716-722.\n In some embodiments, the output of natural language processor 714 (e.g., ingested content) may be stored within data sources 724, such as corpus 726.  As used herein, a corpus may refer to one or more data sources, such as the data sources 724\nof FIG. 7.  In some embodiments, the data sources 724 may include data warehouses, corpora, data models, and document repositories.  In some embodiments, the corpus 726 may be a relational database.\n The descriptions of the various embodiments of the present invention have been presented for purposes of illustration, but are not intended to be exhaustive or limited to the embodiments disclosed.  Many modifications and variations will be\napparent to those of ordinary skill in the art without departing from the scope of the described embodiments.  The terminology used herein was chosen to best explain the principles of the embodiments, the practical application or technical improvement\nover technologies found in the marketplace, or to enable others of ordinary skill in the art to understand the embodiments disclosed herein.", "application_number": "15288264", "abstract": " Correspondences in a social networking system are analyzed to determine\n     at least one topic. An activity stream with the at least one topic is\n     analyzed. A target audience for the activity steam is identified. The\n     activity stream is analyzed according to a uniqueness and a relationship\n     criteria to form an assessment. The assessment is analyzed to a\n     predetermined action criteria. Performing an action responsive to\n     determining the assessment satisfies the predetermined action criteria.\n", "citations": ["8141127", "8442073", "8595234", "8898698", "9143468", "9449050", "9928484", "10002127", "20120131139", "20130110638", "20140006400", "20150032707", "20150256634", "20160034571", "20160070808"], "related": []}, {"id": "20180109840", "patent_code": "10306309", "patent_name": "Systems and methods for identifying a source of a user interface from a\n     fingerprint of the user interface", "year": "2019", "inventor_and_country_data": " Inventors: \nStathacopoulos; Paul T. (San Carlos, CA), Maughan; Benjamin H. (Pleasanton, CA)  ", "description": "BACKGROUND OF THE INVENTION\n Over-the-top (OTT) applications for consuming video and other media are quickly becoming prevalent, and are rapidly replacing other video consumption means, such as set-top boxes and traditional cable television, as sources of media. \nTraditional video consumption means, such as set-top boxes, offer established manners of tracking what media users are consuming, and how those users are viewing that media.  It is unknown how to track devices and services that are providing OTT media.\nSUMMARY\n Systems and methods are provided herein for determining aspects of a user interface, and determining therefrom a service or device that corresponds to the user interface.  In doing so, a user's activity can be profiled, and aggregate usage data\nover a population of particular devices and OTT applications can be tracked.\n In some aspects, systems and methods are provided for resolving an unknown user interface of a known service.  To this end, control circuitry may capture a fingerprint of a user interface.  For example, control circuitry may fingerprint an image\nor a series of images comprising a user interface of a media consumption application, such as Netflix, and/or comprising a user interface of a source output device, such as an XBOX from which a media consumption application is provided.\n In some embodiments, control circuitry may access a database comprising entries of characteristics of fingerprints of a plurality of user interfaces.  For example, a database may be maintained that stores characteristics of known fingerprints of\nknown user interfaces, such as various user interfaces that correspond to an OTT application like Netflix.  Many known fingerprints may be reflected in the database entries because a single OTT application may have dozens or hundreds of associated user\ninterfaces that correspond to different source devices or versions of the OTT applications.  Control circuitry may access such a database to compare characteristics of the fingerprint taken against the characteristics of the entries in order to\ndetermine, based on the comparing, whether the characteristics of the fingerprint do not fully match the characteristics of any of the entries.\n If control circuitry determines that there is a full match, control circuitry may determine that the user interface that a fingerprint was captured from corresponds to the user interface corresponding to the matching entry.  However, if control\ncircuitry determines that the characteristics of the fingerprint do not fully match the characteristics of any of the entries, control circuitry may use other means to determine a service and/or source device associated with the user interface.\n In some embodiments, in an effort to determine the service and/or source device associated with the user interface, control circuitry may compare portions of the characteristics of the fingerprint against the characteristics of the entries.  For\nexample, control circuitry may determine whether a portion of the captured fingerprint (as opposed to the entire captured fingerprint) matches any characteristics of the entries.  Control circuitry may then determine a set of the fingerprints of the\nplurality of user interfaces where the portions of the characteristics of the fingerprint match the characteristics of the entries.  Control circuitry may use these partial matches to determine a service and/or source device that corresponds to the\npartial match.\n In some embodiments, control circuitry may access a knowledge graph that describes relationships between the entries.  For example, the knowledge graph may draw connections between characteristics of various entries, such as similarities. \nControl circuitry may determine, based on the relationships, a relationship between each fingerprint of the set of fingerprints.  For example, control circuitry may determine that some, a majority of, or all of the fingerprints of the set of fingerprints\nbelong to the same OTT provider, and may thereby determine determining that the user interface is a particular user interface based on the relationship.\n In some embodiments, when control circuitry captures the fingerprint of the user interface, control circuitry identifies elements of an image that comprise the user interface.  For example, control circuitry may determine that a first portion of\nthe screen, such as a header, corresponds to an OTT provider, such as Netflix, and a second portion of the screen, such as a video, does not comprise a portion of a user interface and instead could be universal to any user interface of any OTT provider. \nAccordingly, control circuitry may in this manner identify elements of the image that do not comprise the user interface and may isolate the elements of the image that comprise the user interface from the elements of the image that do not comprise the\nuser interface.  Control circuitry may then, when capturing the fingerprint, only fingerprint those elements of the image that comprise the user interface.\n In some embodiments, when control circuitry determines, based on the comparing, that the characteristics of the fingerprint do not fully match the characteristics of any of the entries, control circuitry may determine that the user interface is\nunknown to the database.  Upon forming this determination, control circuitry may, as described above and below, resolve a service and/or device corresponding to the user interface, and update the database such that it now knows the user interface.\n In some embodiments, when control circuitry is comparing portions of the characteristics of the fingerprint against the characteristics of the entries, control circuitry may identify a portion of the characteristics of the fingerprint comprising\na discrete component of the user interface.  For example, control circuitry may determine that a logo (e.g., a Netflix logo for the OTT video provider Netflix) is a discrete component of the user interface.  Control circuitry may then determine whether\nthe characteristics of an entry of the entries comprise the discrete component (e.g., by determining whether the same or a similar logo appears in other user interface entries).\n In some embodiments, control circuitry may determine the set of fingerprints based on the determining of whether the characteristics of the entry of the entries comprise the discrete component.  Following the logo example above, the set of\nfingerprints may include those fingerprints which share a given discrete component with the fingerprint of the user interface, such as a logo or any other discrete component.\n In some embodiments, the relationships between the entries in the knowledge graph comprise a service common to the set of entries.  For example, control circuitry may determine, based on an indication from the knowledge graph, that any user\ninterface with a given symbol corresponds to a given OTT application or source device.  In this manner, control circuitry may determine that the knowledge graph describes relationships by indicating discrete components of user interfaces that\nsubstantially match discrete components of other user interfaces.\n In some embodiments, control circuitry may update the database to know a previously unknown user interface by generating an entry for the database that corresponds to the particular user interface, or by instructing the database to generate such\nan entry.  For example, when an unknown user interface is determined to correspond to a given service and/or source device, control circuitry may update the database with this information.\n In some embodiments, control circuitry may use the updated database to resolve that a fingerprint of the same user interface now matches an entry of the database.  Control circuitry may do so by capturing a second fingerprint of a second user\ninterface, comparing characteristics of the second fingerprint against the characteristics of the entries of the database, and determining that the second user interface is the particular user interface based on the comparing.\n In some embodiments, capturing the fingerprint may comprise object recognition, image recognition, or characteristic recognition.  For example, control circuitry may determine that a given object, such as a logo, is within an image, or control\ncircuitry may determine other images or characteristics are in an image, such as a prevalent color, and the like.\n In some aspects, system and methods are provided herein for determining when a source device used by a user for consuming media has been replaced with another source device.  To this end, systems and methods are provided where control circuitry\ncaptures, at a first time, a first plurality of fingerprints of a first image.  Any of the given fingerprints may be captured in any manner described above and below.  Control circuitry may determine, based on a first fingerprint of the first plurality\nof fingerprints, an identity of a first output device that is providing the image.  For example, if an image contains multiple user interfaces, such as a first user interface corresponding to an image provided by a first output device, and a second user\ninterface comprising an indication of an input port to a display device, control circuitry may fingerprint each user interface individually.\n In some embodiments, control circuitry may determine, based on a second fingerprint of the first plurality of fingerprints, an input port that the first output device is providing the first image through.  For example, if a first output device,\nsuch as an XBOX device, is provided through an input port, such as HDMI1, on a television device, then an image indicating a source of HDMI1 may be fingerprinted, and control circuitry may determine therefrom the input port.\n In some embodiments, control circuitry may, at a second time later than the first time, capture a second plurality of fingerprints of a second image, and may determine, based on a third fingerprint of the second plurality of fingerprints, that\nthe second image is provided through the same input port (e.g., HDMI1).  Control circuitry may additionally determine, based on a fourth fingerprint of the second plurality of fingerprints, that a second output device different from the first output\ndevice is providing the second image through the input port, and may responsively determine that a user has discontinued use of the first output device.  Control circuitry may make this determination of discontinued use by determining, based on the\nfingerprints described above, that a different source device's user interface is being provided through a same interface.\n In some embodiments, control circuitry may transmit a notification that the user has discontinued use of the first output device to a server that is remote from the first output device.  For example, control circuitry may transmit this\ninformation to a server that tracks device usage for various purposes (e.g., user profiling, device profiling, etc.).\n In some embodiments, the server may aggregate data of the notification with data of other notifications to track a loss of popularity of the first output device.  For example, if many people replace an output device corresponding to an input\nport with a different output device, this may signal a loss of popularity to the server.  Similarly, control circuitry may transmit a notification that the user has begun use of the second output device to a server that is remote from the second output\ndevice, and the server may aggregate data of the notification with data of other notifications to track a gain of popularity of the second output device.  Thus, using these systems and methods, a loss of popularity may be tracked with respect to one\ndevice while a gain of popularity of a different device may be tracked.\n In some embodiments, control circuitry may determine that the first output device corresponds to the input port based on a user interface corresponding to the first output device being displayed simultaneously with an indicator of the input\nport.  For example, when a television is first switched to a particular input port, an image indicating the selected input port is generated for display, as is the image corresponding to an output device feeding the input port.  This simultaneous display\nmay be fingerprinted, as described above and below, to determine that the feed is fed by a particular output device, and that the output device is feeding into a particular input port.\n In some embodiments, when an identity of the second output device cannot be determined based on the fourth fingerprint, control circuitry may prompt the user to identify the identity of the second output device.  For example, control circuitry\nmay prompt the user, and may receive from a user an indication that a given user interface corresponds to a particular source device, such as an XBOX.  Control circuitry may then update the database and knowledge graph with this information.\n In some embodiments, control circuitry may determine, based on a fifth fingerprint of the second plurality of fingerprints, that a particular application is being run by the second output device.  For example, while the second output device is\nrunning, control circuitry may capture a fingerprint corresponding to a particular application, such as an OTT application like Netflix, and using the systems and methods described above and below, control circuitry may resolve a provider of the OTT\napplication based on the fingerprint.\n In some aspects, systems and methods are provided for tracking device usage.  For example, control circuitry may monitor what device is plugged into a particular source (e.g., HDMI1) over a period of time, and may learn when a device is swapped\nout for a different device.  Control circuitry may accomplish this by fingerprinting a user interface of a device and a user interface of a user equipment (e.g., a source indicator) to track which device is plugged in at a given time, and may monitor\nwhen that device is changed based on a user interface of a device changing.\n In some embodiments, control circuitry may capture, at a first time, a first plurality of fingerprints of a first image.  For example, an image may include multiple user interfaces, such as a user interface provided by a device (e.g., an XBOX),\na user interface provided by a user equipment (e.g., a source indicator, such as \"HDMI1\"), and/or a user interface provided by an application (e.g., a video application, like Netflix).  Control circuitry may capture a plurality of fingerprints, each\nfingerprint of the plurality corresponding to a different user interface.\n In some embodiments, control circuitry may determine, based on a first fingerprint of the first plurality of fingerprints, an identity of a first output device that is providing the image.  For example, control circuitry may compare\ncharacteristics of a fingerprint of a source identifier, as described above and below, against a database of fingerprint characteristics, to determine that the source identifier corresponds to a particular output device, such as an XBOX console.\n In some embodiments, control circuitry may determine, based on a second fingerprint of the first plurality of fingerprints, an input port that the first output device is providing the first image through.  For example, control circuitry may,\nbased on a fingerprint of a user interface of a user equipment (e.g., an \"HDMI1\" indicator on a user interface), control circuitry may determine that an HDMI1 input port is being used.  Further, control circuitry may determine that because the HDMI1\nindicator simultaneously appears with a user interface of, e.g., an XBOX console, that the XBOX console is providing input to the user equipment through the HDMI1 input port.\n In some embodiments, control circuitry may capture, at a second time later than the first time, a second plurality of fingerprints of a second image.  For example, the second image may include a user interface showing an application's user\ninterface (e.g., Netflix), an input port's user interface (e.g., \"HDMI1\"), and an output device's user interface (e.g., XBOX ONE, which is an upgraded version of an XBOX console).  A fingerprint for each of these user interfaces may be captured.\n In some embodiments, control circuitry may determine, based on a third fingerprint of the second plurality of fingerprints, that the second image is provided through the input port.  For example, control circuitry may determine that the second\nimage is provided through the same HDMI1 input port based on a fingerprint of the user equipment's input port indicator in the image generated for display by the user equipment.\n In some embodiments, control circuitry may determine, based on a fourth fingerprint of the second plurality of fingerprints, that a second output device different from the first output device is providing the second image through the input port. For example, control circuitry may determine that an XBOX ONE, which is different from the XBOX (and is an upgraded version of the XBOX), is providing the second image through the same HDMI1 input port.  In response to determining that the second output\ndevice is providing the second image, control circuitry that a user has discontinued use of the first output device.  For example, control circuitry may determine that the user has replaced his XBOX with an XBOX ONE, which may indicate that XBOX has\nbecome obsolete.\n In some embodiments, control circuitry may transmit a notification that the user has discontinued use of the first output device to a server that is remote from the first output device.  For example, control circuitry may indicate to a remote\nserver that an XBOX device is no longer feeding into an HDMI1 port, and thus that use of the XBOX device is discontinued.  Similarly, control circuitry may indicate to the remote server that an XBOX ONE device is now feeding into the HDMI1 port, and thus\nthat use of the XBOX ONE device has begun.  Accordingly, the server may aggregate this data to track a loss of popularity of the XBOX device, as well as a gain of popularity of the XBOX ONE device.\n In some embodiments, if control circuitry cannot determine an identity of the second output device based on the fourth fingerprint, control circuitry may prompt the user to identify the identity of the second output device.  For example, if\ncontrol circuitry is not able to determine that the second output device is an XBOX ONE, control circuitry may query the user to identify that the second output device is an XBOX ONE.  Control circuitry may thereafter transmit an update to a database of\nuser interface fingerprint characteristics to generate an entry corresponding to the fourth fingerprint and to the identity of the second output device, such that going forward, control circuitry is able to identify that a particular user interface\ncorresponds to an XBOX ONE. BRIEF DESCRIPTION OF THE DRAWINGS\n The above and other objects and advantages of the disclosure will be apparent upon consideration of the following detailed description, taken in conjunction with the accompanying drawings, in which like reference characters refer to like parts\nthroughout, and in which:\n FIG. 1 shows an illustrative embodiment of a display screen that may be used to provide media guidance application listings and other media guidance information, in accordance with some embodiments of the disclosure;\n FIG. 2 shows another illustrative embodiment of a display screen that may be used to provide media guidance application listings, in accordance with some embodiments of the disclosure;\n FIG. 3 is a block diagram of an illustrative user equipment (UE) device in accordance with some embodiments of the disclosure;\n FIG. 4 is a block diagram of an illustrative media system in accordance with some embodiments of the disclosure;\n FIG. 5 depicts a user equipment configured to display various user interfaces, in accordance with some embodiments of this disclosure;\n FIG. 6 is a flowchart of illustrative steps involved in resolving an unknown user interface, in accordance with some embodiments of the disclosure;\n FIG. 7 is a flowchart of illustrative steps involved in using fingerprinting to determine when a device has been replaced by another device at a user input, in accordance with some embodiments of the disclosure;\n FIG. 8 is a flowchart of illustrative steps involved in executing an algorithm for control circuitry (e.g., control circuitry 304) to search a database and to identify a source of a user interface by way of fingerprinting, in accordance with\nsome embodiments of the disclosure; and\n FIG. 9 depicts pseudocode that describes an algorithm to identify a source of a user interface by way of fingerprinting, in accordance with some embodiments of the disclosure.\nDESCRIPTION\n Systems and methods are provided herein for determining aspects of a user interface, and determining therefrom a service or device that corresponds to the user interface.  In doing so, a user's activity can be profiled, and aggregate usage data\nover a population of particular devices and OTT applications can be tracked.\n In some aspects, systems and methods are provided for resolving an unknown user interface of a known service.  To this end, control circuitry may capture a fingerprint of a user interface.  For example, control circuitry may fingerprint an image\nor a series of images comprising a user interface of a media consumption application, such as Netflix, and/or comprising a user interface of a source output device, such as an XBOX from which a media consumption application is provided.\n In some embodiments, control circuitry may access a database comprising entries of characteristics of fingerprints of a plurality of user interfaces.  For example, a database may be maintained that stores characteristics of known fingerprints of\nknown user interfaces, such as various user interfaces that correspond to an OTT application like Netflix.  Many known fingerprints may be reflected in the database entries because a single OTT application may have dozens or hundreds of associated user\ninterfaces that correspond to different source devices or versions of the OTT applications.  Control circuitry may access such a database to compare characteristics of the fingerprint taken against the characteristics of the entries in order to\ndetermine, based on the comparing, whether the characteristics of the fingerprint do not fully match the characteristics of any of the entries.\n If control circuitry determines that there is a full match, control circuitry may determine that the user interface that a fingerprint was captured from corresponds to the user interface corresponding to the matching entry.  However, if control\ncircuitry determines that the characteristics of the fingerprint do not fully match the characteristics of any of the entries, control circuitry may use other means to determine a service and/or source device associated with the user interface.\n In some embodiments, in an effort to determine the service and/or source device associated with the user interface, control circuitry may compare portions of the characteristics of the fingerprint against the characteristics of the entries.  For\nexample, control circuitry may determine whether a portion of the captured fingerprint (as opposed to the entire captured fingerprint) matches any characteristics of the entries.  Control circuitry may then determine a set of the fingerprints of the\nplurality of user interfaces where the portions of the characteristics of the fingerprint match the characteristics of the entries.  Control circuitry may use these partial matches to determine a service and/or source device that corresponds to the\npartial match.\n In some embodiments, control circuitry may access a knowledge graph that describes relationships between the entries.  For example, the knowledge graph may draw connections between characteristics of various entries, such as similarities. \nControl circuitry may determine, based on the relationships, a relationship between each fingerprint of the set of fingerprints.  For example, control circuitry may determine that some, a majority of, or all of the fingerprints of the set of fingerprints\nbelong to the same OTT provider, and may thereby determine determining that the user interface is a particular user interface based on the relationship.\n In some aspects, system and methods are provided herein for determining when a source device used by a user for consuming media has been replaced with another source device.  To this end, systems and methods are provided where control circuitry\ncaptures, at a first time, a first plurality of fingerprints of a first image.  Any of the given fingerprints may be captured in any manner described above and below.  Control circuitry may determine, based on a first fingerprint of the first plurality\nof fingerprints, an identity of a first output device that is providing the image.  For example, if an image contains multiple user interfaces, such as a first user interface corresponding to an image provided by a first output device, and a second user\ninterface comprising an indication of an input port to a display device, control circuitry may fingerprint each user interface individually.\n In some embodiments, control circuitry may determine, based on a second fingerprint of the first plurality of fingerprints, an input port that the first output device is providing the first image through.  For example, if a first output device,\nsuch as an XBOX device, is provided through an input port, such as HDMI1, on a television device, then an image indicating a source of HDMI1 may be fingerprinted, and control circuitry may determine therefrom the input port.\n In some embodiments, control circuitry may, at a second time later than the first time, capture a second plurality of fingerprints of a second image, and may determine, based on a third fingerprint of the second plurality of fingerprints, that\nthe second image is provided through the same input port (e.g., HDMI1).  Control circuitry may additionally determine, based on a fourth fingerprint of the second plurality of fingerprints, that a second output device different from the first output\ndevice is providing the second image through the input port, and may responsively determine that a user has discontinued use of the first output device.  Control circuitry may make this determination of discontinued use by determining, based on the\nfingerprints described above, that a different source device's user interface is being provided through a same interface.\n Over-the-top (OTT) applications for consuming video and other media are quickly becoming prevalent, and are rapidly replacing other video consumption means, such as set-top boxes and traditional cable television, as sources of media. \nTraditional video consumption means, such as set-top boxes, offered established manners of tracking what media people were consuming, and how they were viewing that media.  It is unknown how to track devices and services that are providing OTT media.\n The amount of content available to users in any given content delivery system can be substantial.  Consequently, many users desire a form of media guidance through an interface that allows users to efficiently navigate content selections and\neasily identify content that they may desire.  An application that provides such guidance is referred to herein as an interactive media guidance application or, sometimes, a media guidance application or a guidance application.\n Interactive media guidance applications may take various forms depending on the content for which they provide guidance.  One typical type of media guidance application is an interactive television program guide.  Interactive television program\nguides (sometimes referred to as electronic program guides) are well-known guidance applications that, among other things, allow users to navigate among and locate many types of content or media assets.  Interactive media guidance applications may\ngenerate graphical user interface screens that enable a user to navigate among, locate and select content.  As referred to herein, the terms \"media asset\" and \"content\" should be understood to mean an electronically consumable user asset, such as\ntelevision programming, as well as pay-per-view programs, on-demand programs (as in video-on-demand (VOD) systems), Internet content (e.g., streaming content, downloadable content, Webcasts, etc.), video clips, audio, content information, pictures,\nrotating images, documents, playlists, websites, articles, books, electronic books, blogs, advertisements, chat sessions, social media, applications, games, and/or any other media or multimedia and/or combination of the same.  Guidance applications also\nallow users to navigate among and locate content.  As referred to herein, the term \"multimedia\" should be understood to mean content that utilizes at least two different content forms described above, for example, text, audio, images, video, or\ninteractivity content forms.  Content may be recorded, played, displayed or accessed by user equipment devices, but can also be part of a live performance.\n The media guidance application and/or any instructions for performing any of the embodiments discussed herein may be encoded on computer readable media.  Computer readable media includes any media capable of storing data.  The computer readable\nmedia may be transitory, including, but not limited to, propagating electrical or electromagnetic signals, or may be non-transitory including, but not limited to, volatile and non-volatile computer memory or storage devices such as a hard disk, floppy\ndisk, USB drive, DVD, CD, media cards, register memory, processor caches, Random Access Memory (\"RAM\"), etc.\n With the advent of the Internet, mobile computing, and high-speed wireless networks, users are accessing media on user equipment devices on which they traditionally did not.  As referred to herein, the phrase \"user equipment device,\" \"user\nequipment,\" \"user device,\" \"electronic device,\" \"electronic equipment,\" \"media equipment device,\" or \"media device\" should be understood to mean any device for accessing the content described above, such as a television, a Smart TV, a set-top box, an\nintegrated receiver decoder (IRD) for handling satellite television, a digital storage device, a digital media receiver (DMR), a digital media adapter (DMA), a streaming media device, a DVD player, a DVD recorder, a connected DVD, a local media server, a\nBLU-RAY player, a BLU-RAY recorder, a personal computer (PC), a laptop computer, a tablet computer, a WebTV box, a personal computer television (PC/TV), a PC media server, a PC media center, a hand-held computer, a stationary telephone, a personal\ndigital assistant (PDA), a mobile telephone, a portable video player, a portable music player, a portable gaming machine, a smart phone, or any other television equipment, computing equipment, or wireless device, and/or combination of the same.  In some\nembodiments, the user equipment device may have a front facing screen and a rear facing screen, multiple front screens, or multiple angled screens.  In some embodiments, the user equipment device may have a front facing camera and/or a rear facing\ncamera.  On these user equipment devices, users may be able to navigate among and locate the same content available through a television.  Consequently, media guidance may be available on these devices, as well.  The guidance provided may be for content\navailable only through a television, for content available only through one or more of other types of user equipment devices, or for content available both through a television and one or more of the other types of user equipment devices.  The media\nguidance applications may be provided as on-line applications (i.e., provided on a web-site), or as stand-alone applications or clients on user equipment devices.  Various devices and platforms that may implement media guidance applications are described\nin more detail below.\n One of the functions of the media guidance application is to provide media guidance data to users.  As referred to herein, the phrase \"media guidance data\" or \"guidance data\" should be understood to mean any data related to content or data used\nin operating the guidance application.  For example, the guidance data may include program information, guidance application settings, user preferences, user profile information, media listings, media-related information (e.g., broadcast times, broadcast\nchannels, titles, descriptions, ratings information (e.g., parental control ratings, critic's ratings, etc.), genre or category information, actor information, logo data for broadcasters' or providers' logos, etc.), media format (e.g., standard\ndefinition, high definition, 3D, etc.), advertisement information (e.g., text, images, media clips, etc.), on-demand information, blogs, websites, and any other type of guidance data that is helpful for a user to navigate among and locate desired content\nselections.\n FIGS. 1-2 show illustrative display screens that may be used to provide media guidance data.  The display screens shown in FIGS. 1-2 may be implemented on any suitable user equipment device or platform.  While the displays of FIGS. 1-2 are\nillustrated as full screen displays, they may also be fully or partially overlaid over content being displayed.  A user may indicate a desire to access content information by selecting a selectable option provided in a display screen (e.g., a menu\noption, a listings option, an icon, a hyperlink, etc.) or pressing a dedicated button (e.g., a GUIDE button) on a remote control or other user input interface or device.  In response to the user's indication, the media guidance application may provide a\ndisplay screen with media guidance data organized in one of several ways, such as by time and channel in a grid, by time, by channel, by source, by content type, by category (e.g., movies, sports, news, children, or other categories of programming), or\nother predefined, user-defined, or other organization criteria.\n FIG. 1 shows illustrative grid of a program listings display 100 arranged by time and channel that also enables access to different types of content in a single display.  Display 100 may include grid 102 with: (1) a column of channel/content\ntype identifiers 104, where each channel/content type identifier (which is a cell in the column) identifies a different channel or content type available; and (2) a row of time identifiers 106, where each time identifier (which is a cell in the row)\nidentifies a time block of programming.  Grid 102 also includes cells of program listings, such as program listing 108, where each listing provides the title of the program provided on the listing's associated channel and time.  With a user input device,\na user can select program listings by moving highlight region 110.  Information relating to the program listing selected by highlight region 110 may be provided in program information region 112.  Region 112 may include, for example, the program title,\nthe program description, the time the program is provided (if applicable), the channel the program is on (if applicable), the program's rating, and other desired information.\n In addition to providing access to linear programming (e.g., content that is scheduled to be transmitted to a plurality of user equipment devices at a predetermined time and is provided according to a schedule), the media guidance application\nalso provides access to non-linear programming (e.g., content accessible to a user equipment device at any time and is not provided according to a schedule).  Non-linear programming may include content from different content sources including on-demand\ncontent (e.g., VOD), Internet content (e.g., streaming media, downloadable media, etc.), locally stored content (e.g., content stored on any user equipment device described above or other storage device), or other time-independent content.  On-demand\ncontent may include movies or any other content provided by a particular content provider (e.g., HBO On Demand providing \"The Sopranos\" and \"Curb Your Enthusiasm\").  HBO ON DEMAND is a service mark owned by Time Warner Company L. P. et al. and THE\nSOPRANOS and CURB YOUR ENTHUSIASM are trademarks owned by the Home Box Office, Inc.  Internet content may include web events, such as a chat session or Webcast, or content available on-demand as streaming content or downloadable content through an\nInternet web site or other Internet access (e.g. FTP).\n Grid 102 may provide media guidance data for non-linear programming including on-demand listing 114, recorded content listing 116, and Internet content listing 118.  A display combining media guidance data for content from different types of\ncontent sources is sometimes referred to as a \"mixed-media\" display.  Various permutations of the types of media guidance data that may be displayed that are different than display 100 may be based on user selection or guidance application definition\n(e.g., a display of only recorded and broadcast listings, only on-demand and broadcast listings, etc.).  As illustrated, listings 114, 116, and 118 are shown as spanning the entire time block displayed in grid 102 to indicate that selection of these\nlistings may provide access to a display dedicated to on-demand listings, recorded listings, or Internet listings, respectively.  In some embodiments, listings for these content types may be included directly in grid 102.  Additional media guidance data\nmay be displayed in response to the user selecting one of the navigational icons 120.  (Pressing an arrow key on a user input device may affect the display in a similar manner as selecting navigational icons 120.)\n Display 100 may also include video region 122, advertisement 124, and options region 126.  Video region 122 may allow the user to view and/or preview programs that are currently available, will be available, or were available to the user.  The\ncontent of video region 122 may correspond to, or be independent from, one of the listings displayed in grid 102.  Grid displays including a video region are sometimes referred to as picture-in-guide (PIG) displays.  PIG displays and their\nfunctionalities are described in greater detail in Satterfield et al. U.S.  Pat.  No. 6,564,378, issued May 13, 2003 and Yuen et al. U.S.  Pat.  No. 6,239,794, issued May 29, 2001, which are hereby incorporated by reference herein in their entireties. \nPIG displays may be included in other media guidance application display screens of the embodiments described herein.\n Advertisement 124 may provide an advertisement for content that, depending on a viewer's access rights (e.g., for subscription programming), is currently available for viewing, will be available for viewing in the future, or may never become\navailable for viewing, and may correspond to or be unrelated to one or more of the content listings in grid 102.  Advertisement 124 may also be for products or services related or unrelated to the content displayed in grid 102.  Advertisement 124 may be\nselectable and provide further information about content, provide information about a product or a service, enable purchasing of content, a product, or a service, provide content relating to the advertisement, etc. Advertisement 124 may be targeted based\non a user's profile/preferences, monitored user activity, the type of display provided, or on other suitable targeted advertisement bases.\n While advertisement 124 is shown as rectangular or banner shaped, advertisements may be provided in any suitable size, shape, and location in a guidance application display.  For example, advertisement 124 may be provided as a rectangular shape\nthat is horizontally adjacent to grid 102.  This is sometimes referred to as a panel advertisement.  In addition, advertisements may be overlaid over content or a guidance application display or embedded within a display.  Advertisements may also include\ntext, images, rotating images, video clips, or other types of content described above.  Advertisements may be stored in a user equipment device having a guidance application, in a database connected to the user equipment, in a remote location (including\nstreaming media servers), or on other storage means, or a combination of these locations.  Providing advertisements in a media guidance application is discussed in greater detail in, for example, Knudson et al., U.S.  Patent Application Publication No.\n2003/0110499, filed Jan.  17, 2003; Ward, III et al. U.S.  Pat.  No. 6,756,997, issued Jun.  29, 2004; and Schein et al. U.S.  Pat.  No. 6,388,714, issued May 14, 2002, which are hereby incorporated by reference herein in their entireties.  It will be\nappreciated that advertisements may be included in other media guidance application display screens of the embodiments described herein.\n Options region 126 may allow the user to access different types of content, media guidance application displays, and/or media guidance application features.  Options region 126 may be part of display 100 (and other display screens described\nherein), or may be invoked by a user by selecting an on-screen option or pressing a dedicated or assignable button on a user input device.  The selectable options within options region 126 may concern features related to program listings in grid 102 or\nmay include options available from a main menu display.  Features related to program listings may include searching for other air times or ways of receiving a program, recording a program, enabling series recording of a program, setting program and/or\nchannel as a favorite, purchasing a program, or other features.  Options available from a main menu display may include search options, VOD options, parental control options, Internet options, cloud-based options, device synchronization options, second\nscreen device options, options to access various types of media guidance data displays, options to subscribe to a premium service, options to edit a user's profile, options to access a browse overlay, or other options.\n The media guidance application may be personalized based on a user's preferences.  A personalized media guidance application allows a user to customize displays and features to create a personalized \"experience\" with the media guidance\napplication.  This personalized experience may be created by allowing a user to input these customizations and/or by the media guidance application monitoring user activity to determine various user preferences.  Users may access their personalized\nguidance application by logging in or otherwise identifying themselves to the guidance application.  Customization of the media guidance application may be made in accordance with a user profile.  The customizations may include varying presentation\nschemes (e.g., color scheme of displays, font size of text, etc.), aspects of content listings displayed (e.g., only HDTV or only 3D programming, user-specified broadcast channels based on favorite channel selections, re-ordering the display of channels,\nrecommended content, etc.), desired recording features (e.g., recording or series recordings for particular users, recording quality, etc.), parental control settings, customized presentation of Internet content (e.g., presentation of social media\ncontent, e-mail, electronically delivered articles, etc.) and other desired customizations.\n The media guidance application may allow a user to provide user profile information or may automatically compile user profile information.  The media guidance application may, for example, monitor the content the user accesses and/or other\ninteractions the user may have with the guidance application.  Additionally, the media guidance application may obtain all or part of other user profiles that are related to a particular user (e.g., from other web sites on the Internet the user accesses,\nsuch as www.allrovi.com, from other media guidance applications the user accesses, from other interactive applications the user accesses, from another user equipment device of the user, etc.), and/or obtain information about the user from other sources\nthat the media guidance application may access.  As a result, a user can be provided with a unified guidance application experience across the user's different user equipment devices.  This type of user experience is described in greater detail below in\nconnection with FIG. 4.  Additional personalized media guidance application features are described in greater detail in Ellis et al., U.S.  Patent Application Publication No. 2005/0251827, filed Jul.  11, 2005, Boyer et al., U.S.  Pat.  No. 7,165,098,\nissued Jan.  16, 2007, and Ellis et al., U.S.  Patent Application Publication No. 2002/0174430, filed Feb.  21, 2002, which are hereby incorporated by reference herein in their entireties.\n Another display arrangement for providing media guidance is shown in FIG. 2.  Video mosaic display 200 includes selectable options 202 for content information organized based on content type, genre, and/or other organization criteria.  In\ndisplay 200, television listings option 204 is selected, thus providing listings 206, 208, 210, and 212 as broadcast program listings.  In display 200 the listings may provide graphical images including cover art, still images from the content, video\nclip previews, live video from the content, or other types of content that indicate to a user the content being described by the media guidance data in the listing.  Each of the graphical listings may also be accompanied by text to provide further\ninformation about the content associated with the listing.  For example, listing 208 may include more than one portion, including media portion 214 and text portion 216.  Media portion 214 and/or text portion 216 may be selectable to view content in\nfull-screen or to view information related to the content displayed in media portion 214 (e.g., to view listings for the channel that the video is displayed on).\n The listings in display 200 are of different sizes (i.e., listing 206 is larger than listings 208, 210, and 212), but if desired, all the listings may be the same size.  Listings may be of different sizes or graphically accentuated to indicate\ndegrees of interest to the user or to emphasize certain content, as desired by the content provider or based on user preferences.\n Various systems and methods for graphically accentuating content listings are discussed in, for example, Yates, U.S.  Patent Application Publication No. 2010/0153885, filed Nov.  12, 2009, which is hereby incorporated by reference herein in its\nentirety.\n Users may access content and the media guidance application (and its display screens described above and below) from one or more of their user equipment devices.  FIG. 3 shows a generalized embodiment of illustrative user equipment device 300. \nMore specific implementations of user equipment devices are discussed below in connection with FIG. 4.  User equipment device 300 may receive content and data via input/output (hereinafter \"I/O\") path 302.  I/O path 302 may provide content (e.g.,\nbroadcast programming, on-demand programming, Internet content, content available over a local area network (LAN) or wide area network (WAN), and/or other content) and data to control circuitry 304, which includes processing circuitry 306 and storage\n308.  Control circuitry 304 may be used to send and receive commands, requests, and other suitable data using I/O path 302.  I/O path 302 may connect control circuitry 304 (and specifically processing circuitry 306) to one or more communications paths\n(described below).  I/O functions may be provided by one or more of these communications paths, but are shown as a single path in FIG. 3 to avoid overcomplicating the drawing.\n Control circuitry 304 may be based on any suitable processing circuitry such as processing circuitry 306.  As referred to herein, processing circuitry should be understood to mean circuitry based on one or more microprocessors, microcontrollers,\ndigital signal processors, programmable logic devices, field-programmable gate arrays (FPGAs), application-specific integrated circuits (ASICs), etc., and may include a multi-core processor (e.g., dual-core, quad-core, hexa-core, or any suitable number\nof cores) or supercomputer.  In some embodiments, processing circuitry may be distributed across multiple separate processors or processing units, for example, multiple of the same type of processing units (e.g., two Intel Core i7 processors) or multiple\ndifferent processors (e.g., an Intel Core i5 processor and an Intel Core i7 processor).  In some embodiments, control circuitry 304 executes instructions for a media guidance application stored in memory (i.e., storage 308).  Specifically, control\ncircuitry 304 may be instructed by the media guidance application to perform the functions discussed above and below.  For example, the media guidance application may provide instructions to control circuitry 304 to generate the media guidance displays. \nIn some implementations, any action performed by control circuitry 304 may be based on instructions received from the media guidance application.\n In client-server based embodiments, control circuitry 304 may include communications circuitry suitable for communicating with a guidance application server or other networks or servers.  The instructions for carrying out the above mentioned\nfunctionality may be stored on the guidance application server.  Communications circuitry may include a cable modem, an integrated services digital network (ISDN) modem, a digital subscriber line (DSL) modem, a telephone modem, Ethernet card, or a\nwireless modem for communications with other equipment, or any other suitable communications circuitry.  Such communications may involve the Internet or any other suitable communications networks or paths (which is described in more detail in connection\nwith FIG. 4).  In addition, communications circuitry may include circuitry that enables peer-to-peer communication of user equipment devices, or communication of user equipment devices in locations remote from each other (described in more detail below).\n Memory may be an electronic storage device provided as storage 308 that is part of control circuitry 304.  As referred to herein, the phrase \"electronic storage device\" or \"storage device\" should be understood to mean any device for storing\nelectronic data, computer software, or firmware, such as random-access memory, read-only memory, hard drives, optical drives, digital video disc (DVD) recorders, compact disc (CD) recorders, BLU-RAY disc (BD) recorders, BLU-RAY 3D disc recorders, digital\nvideo recorders (DVR, sometimes called a personal video recorder, or PVR), solid state devices, quantum storage devices, gaming consoles, gaming media, or any other suitable fixed or removable storage devices, and/or any combination of the same.  Storage\n308 may be used to store various types of content described herein as well as media guidance data described above.  Nonvolatile memory may also be used (e.g., to launch a boot-up routine and other instructions).  Cloud-based storage, described in\nrelation to FIG. 4, may be used to supplement storage 308 or instead of storage 308.\n Control circuitry 304 may include video generating circuitry and tuning circuitry, such as one or more analog tuners, one or more MPEG-2 decoders or other digital decoding circuitry, high-definition tuners, or any other suitable tuning or video\ncircuits or combinations of such circuits.  Encoding circuitry (e.g., for converting over-the-air, analog, or digital signals to MPEG signals for storage) may also be provided.  Control circuitry 304 may also include scaler circuitry for upconverting and\ndownconverting content into the preferred output format of the user equipment 300.  Circuitry 304 may also include digital-to-analog converter circuitry and analog-to-digital converter circuitry for converting between digital and analog signals.  The\ntuning and encoding circuitry may be used by the user equipment device to receive and to display, to play, or to record content.  The tuning and encoding circuitry may also be used to receive guidance data.  The circuitry described herein, including for\nexample, the tuning, video generating, encoding, decoding, encrypting, decrypting, scaler, and analog/digital circuitry, may be implemented using software running on one or more general purpose or specialized processors.  Multiple tuners may be provided\nto handle simultaneous tuning functions (e.g., watch and record functions, picture-in-picture (PIP) functions, multiple-tuner recording, etc.).  If storage 308 is provided as a separate device from user equipment 300, the tuning and encoding circuitry\n(including multiple tuners) may be associated with storage 308.\n A user may send instructions to control circuitry 304 using user input interface 310.  User input interface 310 may be any suitable user interface, such as a remote control, mouse, trackball, keypad, keyboard, touch screen, touchpad, stylus\ninput, joystick, voice recognition interface, or other user input interfaces.  Display 312 may be provided as a stand-alone device or integrated with other elements of user equipment device 300.  For example, display 312 may be a touchscreen or\ntouch-sensitive display.  In such circumstances, user input interface 310 may be integrated with or combined with display 312.  Display 312 may be one or more of a monitor, a television, a liquid crystal display (LCD) for a mobile device, amorphous\nsilicon display, low temperature poly silicon display, electronic ink display, electrophoretic display, active matrix display, electro-wetting display, electrofluidic display, cathode ray tube display, light-emitting diode display, electroluminescent\ndisplay, plasma display panel, high-performance addressing display, thin-film transistor display, organic light-emitting diode display, surface-conduction electron-emitter display (SED), laser television, carbon nanotubes, quantum dot display,\ninterferometric modulator display, or any other suitable equipment for displaying visual images.  In some embodiments, display 312 may be HDTV-capable.  In some embodiments, display 312 may be a 3D display, and the interactive media guidance application\nand any suitable content may be displayed in 3D.  A video card or graphics card may generate the output to the display 312.  The video card may offer various functions such as accelerated rendering of 3D scenes and 2D graphics, MPEG-2/MPEG-4 decoding, TV\noutput, or the ability to connect multiple monitors.  The video card may be any processing circuitry described above in relation to control circuitry 304.  The video card may be integrated with the control circuitry 304.  Speakers 314 may be provided as\nintegrated with other elements of user equipment device 300 or may be stand-alone units.  The audio component of videos and other content displayed on display 312 may be played through speakers 314.  In some embodiments, the audio may be distributed to a\nreceiver (not shown), which processes and outputs the audio via speakers 314.\n The guidance application may be implemented using any suitable architecture.  For example, it may be a stand-alone application wholly-implemented on user equipment device 300.  In such an approach, instructions of the application are stored\nlocally (e.g., in storage 308), and data for use by the application is downloaded on a periodic basis (e.g., from an out-of-band feed, from an Internet resource, or using another suitable approach).  Control circuitry 304 may retrieve instructions of the\napplication from storage 308 and process the instructions to generate any of the displays discussed herein.  Based on the processed instructions, control circuitry 304 may determine what action to perform when input is received from input interface 310. \nFor example, movement of a cursor on a display up/down may be indicated by the processed instructions when input interface 310 indicates that an up/down button was selected.\n In some embodiments, the media guidance application is a client-server based application.  Data for use by a thick or thin client implemented on user equipment device 300 is retrieved on-demand by issuing requests to a server remote to the user\nequipment device 300.  In one example of a client-server based guidance application, control circuitry 304 runs a web browser that interprets web pages provided by a remote server.  For example, the remote server may store the instructions for the\napplication in a storage device.  The remote server may process the stored instructions using circuitry (e.g., control circuitry 304) and generate the displays discussed above and below.  The client device may receive the displays generated by the remote\nserver and may display the content of the displays locally on equipment device 300.  This way, the processing of the instructions is performed remotely by the server while the resulting displays are provided locally on equipment device 300.  Equipment\ndevice 300 may receive inputs from the user via input interface 310 and transmit those inputs to the remote server for processing and generating the corresponding displays.  For example, equipment device 300 may transmit a communication to the remote\nserver indicating that an up/down button was selected via input interface 310.  The remote server may process instructions in accordance with that input and generate a display of the application corresponding to the input (e.g., a display that moves a\ncursor up/down).  The generated display is then transmitted to equipment device 300 for presentation to the user.\n In some embodiments, the media guidance application is downloaded and interpreted or otherwise run by an interpreter or virtual machine (run by control circuitry 304).  In some embodiments, the guidance application may be encoded in the ETV\nBinary Interchange Format (EBIF), received by control circuitry 304 as part of a suitable feed, and interpreted by a user agent running on control circuitry 304.  For example, the guidance application may be an EBIF application.  In some embodiments, the\nguidance application may be defined by a series of JAVA-based files that are received and run by a local virtual machine or other suitable middleware executed by control circuitry 304.  In some of such embodiments (e.g., those employing MPEG-2 or other\ndigital media encoding schemes), the guidance application may be, for example, encoded and transmitted in an MPEG-2 object carousel with the MPEG audio and video packets of a program.\n User equipment device 300 of FIG. 3 can be implemented in system 400 of FIG. 4 as user television equipment 402, user computer equipment 404, wireless user communications device 406, or any other type of user equipment suitable for accessing\ncontent, such as a non-portable gaming machine.  For simplicity, these devices may be referred to herein collectively as user equipment or user equipment devices, and may be substantially similar to user equipment devices described above.  User equipment\ndevices, on which a media guidance application may be implemented, may function as a standalone device or may be part of a network of devices.  Various network configurations of devices may be implemented and are discussed in more detail below.\n A user equipment device utilizing at least some of the system features described above in connection with FIG. 3 may not be classified solely as user television equipment 402, user computer equipment 404, or a wireless user communications device\n406.  For example, user television equipment 402 may, like some user computer equipment 404, be Internet-enabled allowing for access to Internet content, while user computer equipment 404 may, like some television equipment 402, include a tuner allowing\nfor access to television programming.  The media guidance application may have the same layout on various different types of user equipment or may be tailored to the display capabilities of the user equipment.  For example, on user computer equipment\n404, the guidance application may be provided as a web site accessed by a web browser.  In another example, the guidance application may be scaled down for wireless user communications devices 406.\n In system 400, there is typically more than one of each type of user equipment device but only one of each is shown in FIG. 4 to avoid overcomplicating the drawing.  In addition, each user may utilize more than one type of user equipment device\nand also more than one of each type of user equipment device.\n In some embodiments, a user equipment device (e.g., user television equipment 402, user computer equipment 404, wireless user communications device 406) may be referred to as a \"second screen device.\" For example, a second screen device may\nsupplement content presented on a first user equipment device.  The content presented on the second screen device may be any suitable content that supplements the content presented on the first device.  In some embodiments, the second screen device\nprovides an interface for adjusting settings and display preferences of the first device.  In some embodiments, the second screen device is configured for interacting with other second screen devices or for interacting with a social network.  The second\nscreen device can be located in the same room as the first device, a different room from the first device but in the same house or building, or in a different building from the first device.\n The user may also set various settings to maintain consistent media guidance application settings across in-home devices and remote devices.  Settings include those described herein, as well as channel and program favorites, programming\npreferences that the guidance application utilizes to make programming recommendations, display preferences, and other desirable guidance settings.  For example, if a user sets a channel as a favorite on, for example, the web site www.allrovi.com on\ntheir personal computer at their office, the same channel would appear as a favorite on the user's in-home devices (e.g., user television equipment and user computer equipment) as well as the user's mobile devices, if desired.  Therefore, changes made on\none user equipment device can change the guidance experience on another user equipment device, regardless of whether they are the same or a different type of user equipment device.  In addition, the changes made may be based on settings input by a user,\nas well as user activity monitored by the guidance application.\n The user equipment devices may be coupled to communications network 414.  Namely, user television equipment 402, user computer equipment 404, and wireless user communications device 406 are coupled to communications network 414 via\ncommunications paths 408, 410, and 412, respectively.  Communications network 414 may be one or more networks including the Internet, a mobile phone network, mobile voice or data network (e.g., a 4G or LTE network), cable network, public switched\ntelephone network, or other types of communications network or combinations of communications networks.  Paths 408, 410, and 412 may separately or together include one or more communications paths, such as, a satellite path, a fiber-optic path, a cable\npath, a path that supports Internet communications (e.g., IPTV), free-space connections (e.g., for broadcast or other wireless signals), or any other suitable wired or wireless communications path or combination of such paths.  Path 412 is drawn with\ndotted lines to indicate that in the exemplary embodiment shown in FIG. 4 it is a wireless path and paths 408 and 410 are drawn as solid lines to indicate they are wired paths (although these paths may be wireless paths, if desired).  Communications with\nthe user equipment devices may be provided by one or more of these communications paths, but are shown as a single path in FIG. 4 to avoid overcomplicating the drawing.\n Although communications paths are not drawn between user equipment devices, these devices may communicate directly with each other via communication paths, such as those described above in connection with paths 408, 410, and 412, as well as\nother short-range point-to-point communication paths, such as USB cables, IEEE 1394 cables, wireless paths (e.g., Bluetooth, infrared, IEEE 802-11x, etc.), or other short-range communication via wired or wireless paths.  BLUETOOTH is a certification mark\nowned by Bluetooth SIG, INC.  The user equipment devices may also communicate with each other directly through an indirect path via communications network 414.\n System 400 includes content source 416 and media guidance data source 418 coupled to communications network 414 via communication paths 420 and 422, respectively.  Paths 420 and 422 may include any of the communication paths described above in\nconnection with paths 408, 410, and 412.  Communications with the content source 416 and media guidance data source 418 may be exchanged over one or more communications paths, but are shown as a single path in FIG. 4 to avoid overcomplicating the\ndrawing.  In addition, there may be more than one of each of content source 416 and media guidance data source 418, but only one of each is shown in FIG. 4 to avoid overcomplicating the drawing.  (The different types of each of these sources are\ndiscussed below.) If desired, content source 416 and media guidance data source 418 may be integrated as one source device.  Although communications between sources 416 and 418 with user equipment devices 402, 404, and 406 are shown as through\ncommunications network 414, in some embodiments, sources 416 and 418 may communicate directly with user equipment devices 402, 404, and 406 via communication paths (not shown) such as those described above in connection with paths 408, 410, and 412.\n Content source 416 may include one or more types of content distribution equipment including a television distribution facility, cable system headend, satellite distribution facility, programming sources (e.g., television broadcasters, such as\nNBC, ABC, HBO, etc.), intermediate distribution facilities and/or servers, Internet providers, on-demand media servers, and other content providers.  NBC is a trademark owned by the National Broadcasting Company, Inc., ABC is a trademark owned by the\nAmerican Broadcasting Company, Inc., and HBO is a trademark owned by the Home Box Office, Inc.  Content source 416 may be the originator of content (e.g., a television broadcaster, a Webcast provider, etc.) or may not be the originator of content (e.g.,\nan on-demand content provider, an Internet provider of content of broadcast programs for downloading, etc.).  Content source 416 may include cable sources, satellite providers, on-demand providers, Internet providers, over-the-top content providers, or\nother providers of content.  Content source 416 may also include a remote media server used to store different types of content (including video content selected by a user), in a location remote from any of the user equipment devices.  Systems and\nmethods for remote storage of content, and providing remotely stored content to user equipment are discussed in greater detail in connection with Ellis et al., U.S.  Pat.  No. 7,761,892, issued Jul.  20, 2010, which is hereby incorporated by reference\nherein in its entirety.\n Media guidance data source 418 may provide media guidance data, such as the media guidance data described above.  Media guidance data may be provided to the user equipment devices using any suitable approach.  In some embodiments, the guidance\napplication may be a stand-alone interactive television program guide that receives program guide data via a data feed (e.g., a continuous feed or trickle feed).  Program schedule data and other guidance data may be provided to the user equipment on a\ntelevision channel sideband, using an in-band digital signal, using an out-of-band digital signal, or by any other suitable data transmission technique.  Program schedule data and other media guidance data may be provided to user equipment on multiple\nanalog or digital television channels.\n In some embodiments, guidance data from media guidance data source 418 may be provided to users' equipment using a client-server approach.  For example, a user equipment device may pull media guidance data from a server, or a server may push\nmedia guidance data to a user equipment device.  In some embodiments, a guidance application client residing on the user's equipment may initiate sessions with source 418 to obtain guidance data when needed, e.g., when the guidance data is out of date or\nwhen the user equipment device receives a request from the user to receive data.  Media guidance may be provided to the user equipment with any suitable frequency (e.g., continuously, daily, a user-specified period of time, a system-specified period of\ntime, in response to a request from user equipment, etc.).  Media guidance data source 418 may provide user equipment devices 402, 404, and 406 the media guidance application itself or software updates for the media guidance application.\n In some embodiments, the media guidance data may include viewer data.  For example, the viewer data may include current and/or historical user activity information (e.g., what content the user typically watches, what times of day the user\nwatches content, whether the user interacts with a social network, at what times the user interacts with a social network to post information, what types of content the user typically watches (e.g., pay TV or free TV), mood, brain activity information,\netc.).  The media guidance data may also include subscription data.  For example, the subscription data may identify to which sources or services a given user subscribes and/or to which sources or services the given user has previously subscribed but\nlater terminated access (e.g., whether the user subscribes to premium channels, whether the user has added a premium level of services, whether the user has increased Internet speed).  In some embodiments, the viewer data and/or the subscription data may\nidentify patterns of a given user for a period of more than one year.  The media guidance data may include a model (e.g., a survivor model) used for generating a score that indicates a likelihood a given user will terminate access to a service/source. \nFor example, the media guidance application may process the viewer data with the subscription data using the model to generate a value or score that indicates a likelihood of whether the given user will terminate access to a particular service or source. In particular, a higher score may indicate a higher level of confidence that the user will terminate access to a particular service or source.  Based on the score, the media guidance application may generate promotions and advertisements that entice the\nuser to keep the particular service or source indicated by the score as one to which the user will likely terminate access.\n Media guidance applications may be, for example, stand-alone applications implemented on user equipment devices.  For example, the media guidance application may be implemented as software or a set of executable instructions which may be stored\nin storage 308, and executed by control circuitry 304 of a user equipment device 300.  In some embodiments, media guidance applications may be client-server applications where only a client application resides on the user equipment device, and server\napplication resides on a remote server.  For example, media guidance applications may be implemented partially as a client application on control circuitry 304 of user equipment device 300 and partially on a remote server as a server application (e.g.,\nmedia guidance data source 418) running on control circuitry of the remote server.  When executed by control circuitry of the remote server (such as media guidance data source 418), the media guidance application may instruct the control circuitry to\ngenerate the guidance application displays and transmit the generated displays to the user equipment devices.  The server application may instruct the control circuitry of the media guidance data source 418 to transmit data for storage on the user\nequipment.  The client application may instruct control circuitry of the receiving user equipment to generate the guidance application displays.\n Content and/or media guidance data delivered to user equipment devices 402, 404, and 406 may be over-the-top (OTT) content.  OTT content delivery allows Internet-enabled user devices, including any user equipment device described above, to\nreceive content that is transferred over the Internet, including any content described above, in addition to content received over cable or satellite connections.  OTT content is delivered via an Internet connection provided by an Internet service\nprovider (ISP), but a third party distributes the content.  The ISP may not be responsible for the viewing abilities, copyrights, or redistribution of the content, and may only transfer IP packets provided by the OTT content provider.  Examples of OTT\ncontent providers include YOUTUBE, NETFLIX, and HULU, which provide audio and video via IP packets.  Youtube is a trademark owned by Google Inc., Netflix is a trademark owned by Netflix Inc., and Hulu is a trademark owned by Hulu, LLC.  OTT content\nproviders may additionally or alternatively provide media guidance data described above.  In addition to content and/or media guidance data, providers of OTT content can distribute media guidance applications (e.g., web-based applications or cloud-based\napplications), or the content can be displayed by media guidance applications stored on the user equipment device.\n Media guidance system 400 is intended to illustrate a number of approaches, or network configurations, by which user equipment devices and sources of content and guidance data may communicate with each other for the purpose of accessing content\nand providing media guidance.  The embodiments described herein may be applied in any one or a subset of these approaches, or in a system employing other approaches for delivering content and providing media guidance.  The following four approaches\nprovide specific illustrations of the generalized example of FIG. 4.\n In one approach, user equipment devices may communicate with each other within a home network.  User equipment devices can communicate with each other directly via short-range point-to-point communication schemes described above, via indirect\npaths through a hub or other similar device provided on a home network, or via communications network 414.  Each of the multiple individuals in a single home may operate different user equipment devices on the home network.  As a result, it may be\ndesirable for various media guidance information or settings to be communicated between the different user equipment devices.  For example, it may be desirable for users to maintain consistent media guidance application settings on different user\nequipment devices within a home network, as described in greater detail in Ellis et al., U.S.  Patent Publication No. 2005/0251827, filed Jul.  11, 2005.  Different types of user equipment devices in a home network may also communicate with each other to\ntransmit content.  For example, a user may transmit content from user computer equipment to a portable video player or portable music player.\n In a second approach, users may have multiple types of user equipment by which they access content and obtain media guidance.  For example, some users may have home networks that are accessed by in-home and mobile devices.  Users may control\nin-home devices via a media guidance application implemented on a remote device.  For example, users may access an online media guidance application on a website via a personal computer at their office, or a mobile device such as a PDA or web-enabled\nmobile telephone.  The user may set various settings (e.g., recordings, reminders, or other settings) on the online guidance application to control the user's in-home equipment.  The online guide may control the user's equipment directly, or by\ncommunicating with a media guidance application on the user's in-home equipment.  Various systems and methods for user equipment devices communicating, where the user equipment devices are in locations remote from each other, is discussed in, for\nexample, Ellis et al., U.S.  Pat.  No. 8,046,801, issued Oct.  25, 2011, which is hereby incorporated by reference herein in its entirety.\n In a third approach, users of user equipment devices inside and outside a home can use their media guidance application to communicate directly with content source 416 to access content.  Specifically, within a home, users of user television\nequipment 402 and user computer equipment 404 may access the media guidance application to navigate among and locate desirable content.  Users may also access the media guidance application outside of the home using wireless user communications devices\n406 to navigate among and locate desirable content.\n In a fourth approach, user equipment devices may operate in a cloud computing environment to access cloud services.  In a cloud computing environment, various types of computing services for content sharing, storage or distribution (e.g., video\nsharing sites or social networking sites) are provided by a collection of network-accessible computing and storage resources, referred to as \"the cloud.\" For example, the cloud can include a collection of server computing devices, which may be located\ncentrally or at distributed locations, that provide cloud-based services to various types of users and devices connected via a network such as the Internet via communications network 414.  These cloud resources may include one or more content sources 416\nand one or more media guidance data sources 418.  In addition or in the alternative, the remote computing sites may include other user equipment devices, such as user television equipment 402, user computer equipment 404, and wireless user communications\ndevice 406.  For example, the other user equipment devices may provide access to a stored copy of a video or a streamed video.  In such embodiments, user equipment devices may operate in a peer-to-peer manner without communicating with a central server.\n The cloud provides access to services, such as content storage, content sharing, or social networking services, among other examples, as well as access to any content described above, for user equipment devices.  Services can be provided in the\ncloud through cloud computing service providers, or through other providers of online services.  For example, the cloud-based services can include a content storage service, a content sharing site, a social networking site, or other services via which\nuser-sourced content is distributed for viewing by others on connected devices.  These cloud-based services may allow a user equipment device to store content to the cloud and to receive content from the cloud rather than storing content locally and\naccessing locally-stored content.\n A user may use various content capture devices, such as camcorders, digital cameras with video mode, audio recorders, mobile phones, and handheld computing devices, to record content.  The user can upload content to a content storage service on\nthe cloud either directly, for example, from user computer equipment 404 or wireless user communications device 406 having content capture feature.  Alternatively, the user can first transfer the content to a user equipment device, such as user computer\nequipment 404.  The user equipment device storing the content uploads the content to the cloud using a data transmission service on communications network 414.  In some embodiments, the user equipment device itself is a cloud resource, and other user\nequipment devices can access the content directly from the user equipment device on which the user stored the content.\n Cloud resources may be accessed by a user equipment device using, for example, a web browser, a media guidance application, a desktop application, a mobile application, and/or any combination of access applications of the same.  The user\nequipment device may be a cloud client that relies on cloud computing for application delivery, or the user equipment device may have some functionality without access to cloud resources.  For example, some applications running on the user equipment\ndevice may be cloud applications, i.e., applications delivered as a service over the Internet, while other applications may be stored and run on the user equipment device.  In some embodiments, a user device may receive content from multiple cloud\nresources simultaneously.  For example, a user device can stream audio from one cloud resource while downloading content from a second cloud resource.  Or a user device can download content from multiple cloud resources for more efficient downloading. \nIn some embodiments, user equipment devices can use cloud resources for processing operations such as the processing operations performed by processing circuitry described in relation to FIG. 3.\n As referred herein, the term \"in response to\" refers to initiated as a result of.  For example, a first action being performed in response to another action may include interstitial steps between the first action and the second action.  As\nreferred herein, the term \"directly in response to\" refers to caused by.  For example, a first action being performed directly in response to another action may not include interstitial steps between the first action and the second action.\n FIG. 5 depicts a user equipment configured to display various user interfaces, in accordance with some embodiments of this disclosure.  User equipment 500 includes a display comprising source indicator 502, application indicator 504, device\nindicator 506, and media asset 508.  While each of source indicator 502, application indicator 504, device indicator 506, and media asset 508 are simultaneously displayed in FIG. 5, this is for illustrative purposes only, and need not all be displayed\nsimultaneously.  User equipment 500 includes all functionality described with respect to user television equipment 402, user computer equipment 404, and wireless user communications device 406.\n User equipment 500 may include control circuitry 304.  In some embodiments, control circuitry 304 may capture a fingerprint of a user interface.  As used in this disclosure, the term \"fingerprint\" means any indicia of content of an image, a\nportion of an image, or a sample of an image taken using any known sampling technique.  For example, a decimated version of an image or portion of an image may comprise a fingerprint, as may a representative portion of a fingerprint.  A capture of any\ndata of an image that characterizes the image sufficiently such that the image may be identified from that data alone may comprise a fingerprint.\n As an example, control circuitry 304 may capture a fingerprint of any user interface displayed on user equipment 500, such as source identifier 502, which is an interface identifying that the source of media presently displayed at user equipment\n500 is being fed through an HDMI1 port.  A fingerprint may additionally or alternatively be captured of application indicator 504 and/or device indicator 506, each of which are user interfaces which respectively identify an application that is presently\nbeing accessed (e.g., Netflix), and a device from which the application is being accessed (e.g., XBOX).  Any given user interface described herein may be displayed by way of display 312.  A fingerprint may be captured by way of object recognition.\n In some embodiments, control circuitry 304 may access a database comprising entries of characteristics of fingerprints of a plurality of user interfaces.  For example, control circuitry 304 may access a database, such as media guidance data\nsource 418, or media content source, by way of communications network 414.  The database may comprise entries (interchangeably referred to herein as listings) in the database, where each entry indicates a characteristic of a fingerprint, and where each\nfingerprint corresponds to a user interface.  For example, some of the characteristics may correspond to a user interface for a first media consumption application, such as Netflix, which is an application for streaming videos.  Other characteristics may\ncorrespond to a user interface for another media consumption application, such as Hulu, which is also an application for streaming videos.  Yet other characteristics may correspond to a user interface for a media device that transmits video, such as an\nXBOX or Playstation.  Collectively, such devices and applications are referred to as \"services\" throughout this disclosure.  As will be described below, control circuitry 304 may retrieve these entries to determine whether a particular fingerprint\ncorresponds to a particular user interface.\n In some embodiments, control circuitry 304 may compare characteristics of the fingerprint against the characteristics of the entries.  For example, control circuitry 304 may have captured a fingerprint of device indicator 506, which is an image\nof an XBOX controller.  Control circuitry 304 may determine that the fingerprint indicates various characteristics of device indicator 506, such as an indication that it is a video game console controller, that it has two joysticks, that it has XBOX\ninsignia, and the like.  Control circuitry 304 may compare each of these characteristics against characteristics of the entries to identify entries with similar characteristics to device indicator 506.\n In some embodiments, control circuitry 304 may determine, based on the comparing, that the characteristics of the fingerprint (e.g., the fingerprint of device indicator 506, which is an image of an XBOX controller) do not fully match the\ncharacteristics of any of the entries.  For example, control circuitry 304 may not find an exact match for a user interface like device indicator 506 in the database.  If control circuitry 304 does find an exact match, control circuitry 304 may determine\nthat device indicator 506 is a known user interface corresponding to a known entity.  However, if control circuitry 304 does not find an exact match for device indicator 504 in the entries, control circuitry 304 may resolve a user interface that device\nindicator 506 corresponds to.\n In response to determining that there is no exact match, control circuitry 304 may compare portions of the characteristics of the fingerprint against characteristics of the entries.  For example, a fingerprint of device indicator 506, which is\nan image of an XBOX controller, may have individual portions reflecting different characteristics of the XBOX controller, such as its color, size, button layout, textual features, and any other potential characteristic.  Control circuitry 304 may compare\neach of these individual portions of a fingerprint of device indicator 506 against characteristics of the entries to find partially matching fingerprints, such as fingerprints that share some but not all characteristics with the fingerprint of device\nindicator 506.\n In some embodiments, control circuitry 304 may determine a set of fingerprints of the plurality of user interfaces where the portions of the characteristics of, e.g., device indicator 506, match the characteristics of the entries.  For example,\ncontrol circuitry 304 may identify a set of partially matching fingerprints, such as fingerprints that also indicate XBOX controllers, the same color, the same button configuration, etc.\n In some embodiments, control circuitry 304 may access a knowledge graph that describes relationships between the entries.  Control circuitry 304 may access the knowledge graph by way of communications network 414.  The knowledge graph may be\nstored at storage 308, media content source 416, or media guidance data source 418.  The knowledge graph may indicate various relationships between the entries, such as that 60% of the entries correspond to user interfaces that are controllers, that 80%\nof the entries correspond to user interfaces that are provided by XBOX systems, and the like.\n Control circuitry 304 may determine, based on the relationships, a relationship between each fingerprint of the set of fingerprints.  For example, control circuitry 304 may determine that each fingerprint corresponds to a user interface that is\nprovided by an XBOX system.  Control circuitry 304 may thereby determine that the user interface (e.g. device indicator 506 itself), is a particular user interface based on the relationship.  In other words, control circuitry 304 may determine that\ndevice indicator 506 is a user interface of an XBOX.\n In some embodiments, when control circuitry 304 captures the fingerprint of the user interface of device indicator 506, control circuitry 304 may first capture an image that includes multiple user interfaces.  For example, if a screenshot of\nuser equipment 500 were taken as it is illustrated in FIG. 5, source indicator 502, application indicator 504, and device indicator 506 would all be included in the screenshot.  If control circuitry 304 is trying to resolve what user interface device\nindicator 506 is, for example, control circuitry 304 may identify elements that do not comprise that user interface (i.e., source indicator 502 and application indicator 504).  Control circuitry 304 may then isolate the elements of the image that\ncomprise device indicator 506 from the other elements of the image, and capture a fingerprint of only the elements of device indicator 506.  In this manner, control circuitry 304 avoids fingerprinting irrelevant user interfaces, which would add noise to\nthe comparison and make it less likely for control circuitry 304 to identify the user interface using the methods and systems described above and below.\n In some embodiments, when control circuitry 304 is comparing portions of the characteristics of the fingerprint against the characteristics of the entries of the database, control circuitry 304 may first identify a discrete component of the user\ninterface.  For example, control circuitry 304 may identify a single button or insignia of device indicator 506.  Control circuitry 304 may then determine whether the characteristics of an entry of the database matches the discrete component.  For\nexample, the same insignia of device indicator 506 may be on various other XBOX user interfaces, and would provide a clue to control circuitry 304 that device indicator 506 is an XBOX user interface.\n In some embodiments, the knowledge graph may describe relationships between characteristics of entries by indicating discrete components of user interfaces that substantially match discrete components of other user interfaces.  For example, the\nknowledge graph may indicate that a particular button configuration of a controller substantially matches a button configuration of virtually all XBOX controllers.  Control circuitry 304 may glean from this indication that device indicator 506 is likely\nan XBOX user interface.\n In some embodiments, control circuitry 304 may generate an entry for the database that corresponds to device indicator 506.  For example, after resolving that device indicator 506 is an XBOX user interface, control circuitry may inform the\ndatabase such that in the future, control circuitry 304 may quickly identify that device indicator 506 is an XBOX user interface through a simple comparison.\n In some aspects, control circuitry 304 may track device usage.  To this end, control circuitry 304 may capture, at a first time, a first plurality of fingerprints of a first image.  Control circuitry 304 may capture the first plurality of\nfingerprints of a first image in any manner described above and below.  As depicted in FIG. 5, control circuitry may capture a screenshot of a display of user equipment 500, which may be the first image.  Control circuitry 304 may capture a plurality of\nfingerprints of the screenshot, such as a fingerprint of source indicator 502, a fingerprint of source indicator 504, and a fingerprint of device indicator 506.\n In some embodiments, control circuitry 304 may determine, based on a first fingerprint of the first plurality of fingerprints, an identity of a first output device that is providing the image.  For example, control circuitry 304 may use the\nsystems and methods described above and below to determine an identity of an output device corresponding to device indicator 506.  In one example, control circuitry 304 may determine that device indicator 506 corresponds to an XBOX.\n In some embodiments, control circuitry 304 may determine, based on a second fingerprint of the first plurality of fingerprints, an input port that the first output device is providing the first image through.  For example, using the systems and\nmethods described above and below, control circuitry 304 may determine that source identifier 502 corresponds to input port HDMI1 of user equipment 500.\n In some embodiments, control circuitry 304 may capture, at a second time later than the first time, a second plurality of fingerprints of a second image.  For example, control circuitry 304 may capture fingerprints of source indicator 502,\napplication indicator 504, and device indicator 506 at a second time.  Control circuitry 304 may capture this second plurality of fingerprints to determine whether any of source indicator 502, application indicator 504, and device indicator 506 has\nchanged.\n In some embodiments, control circuitry 304 may determine, based on a third fingerprint of the second plurality of fingerprints, that the second image is provided through the input port.  For example, control circuitry 304 may determine, based on\na fourth fingerprint of the second plurality of fingerprints, that a second output device different from the first output device is providing the second image through the input port.  Control circuitry 304 may perform this determination by identifying a\ndevice that corresponds to device indicator 506 using the systems and methods described above and below.  In response to determining that the second output device is providing the second image, control circuitry 304 may determine that a user has\ndiscontinued use of the first output device.  For example, control circuitry 304 may determine that an XBOX ONE is now providing input by way of input port HDMI1, and therefore that an XBOX ONE has replaced the outdated XBOX.  Control circuitry 304 may\nresponsively determine that a user has discontinued use of the XBOX.\n In some embodiments, control circuitry 304 may transmit a notification (e.g., by way of communications network 414) that the user has discontinued use of the first output device (e.g., XBOX), to a server (e.g., media guidance data source 418)\nthat is remote from the first output device.  The server may aggregate data of the notification received from control circuitry 304 with data of other notifications to track a loss of popularity of the first output device.  For example, various user\nequipment 500 may transmit notifications that an XBOX has been disconnected from a respective user equipment 500, and the server may thereby track a loss of popularity of the XBOX.\n Similarly, in some embodiments, control circuitry 304 may transmit a notification that a user has begun use of the second output device (e.g., XBOX ONE) to the server that is remote from the second output device, and may, in a manner similar to\ntracking a loss of popularity of the XBOX, track a gain of popularity of the XBOX ONE.\n In some embodiments, when control circuitry 304 cannot determine the identity of the second output device based on the fourth fingerprint (e.g., because the knowledge graph does not indicate a sufficient match), control circuitry 304 may prompt\nthe user (e.g., by way of display 312) to identify the identity of the second output device.  Control circuitry 304 may receive an indication of the identity by way of user input interface 310.  Control circuitry 304 may transmit an update to the\ndatabase (e.g., media content source 416 or media guidance data source 418) to generate an entry corresponding to the fourth fingerprint and to the identity of the second output device.  For example, if the user interface of the XBOX ONE was previously\nunknown, the database will now reflect it, such that in the future, control circuitry 304 will be able to identify an output device as an XBOX ONE based on a fingerprint of the user interface.\n FIG. 6 is a flowchart of illustrative steps involved in resolving an unknown user interface, in accordance with some embodiments of the disclosure.  It should be noted that process 600 or any step thereof could be performed on, or provided by,\nany of the devices shown in FIGS. 3-5.  For example, process 600 may be executed by control circuitry 304 (FIG. 3) as instructed by control circuitry implemented on user equipment 402, 404, and/or 406 (FIG. 4) in order to determine a country of origin of\na user.  In addition, one or more steps of process 600 may be incorporated into or combined with one or more steps of any other process or embodiment.\n Process 600 begins at 602, where control circuitry 304 may capture a fingerprint of a user interface, such as such as source identifier 502, application indicator 504 and/or device indicator 506.  Any given user interface described herein may be\ndisplayed by way of display 312.  As described above and below, a fingerprint may be captured by any known means of object recognition, and/or by any known form sampling of a user interface.\n Process 600 may then continue to 604, where control circuitry 304 may access a database comprising entries of characteristics of fingerprints of a plurality of user interfaces.  For example, control circuitry 304 may access a database, such as\nmedia guidance data source 418, or media content source, by way of communications network 414.  The database may comprise entries (interchangeably referred to herein as listings) in the database, where each entry indicates a characteristic of a\nfingerprint, and where each fingerprint corresponds to a user interface.  For example, some of the characteristics may correspond to a user interface for a first media consumption application, such as Netflix, which is an application for streaming\nvideos.  Other characteristics may correspond to a user interface for another media consumption application, such as Hulu, which is also an application for streaming videos.  Yet other characteristics may correspond to a user interface for a media device\nthat transmits video, such as an XBOX or Playstation.\n Process 600 may then continue to 606, where control circuitry 304 may compare characteristics of the fingerprint against the characteristics of the entries.  For example, control circuitry 304 may have captured a fingerprint of device indicator\n506, which is an image of an XBOX controller.  Control circuitry 304 may determine that the fingerprint indicates various characteristics of device indicator 506, such as an indication that it is a video game console controller, that it has two\njoysticks, that it has XBOX insignia, and the like.  Control circuitry 304 may compare each of these characteristics against characteristics of the entries to identify entries with similar characteristics to device indicator 506.\n Process 600 may then continue to 608, where control circuitry 304 may determine, based on the comparing, whether the characteristics of the fingerprint (e.g., the fingerprint of device indicator 506, which is an image of an XBOX controller)\nfully match the characteristics of any of the entries.  For example, control circuitry 304 may not find an exact match for a user interface like device indicator 504 in the database.  If control circuitry 304 does find an exact match, process 600 may\ncontinue to 610, where control circuitry 304 may determine that device indicator 506 is a known user interface corresponding to a known entity and may identify that known user interface.  However, if control circuitry 304 does not find an exact match for\ndevice indicator 506 in the entries, processor 600 may continue to 612, where control circuitry 304 may determine that the user interface is unknown, and may use the remaining elements of process 600 to resolve a user interface that device indicator 506\ncorresponds to.\n Process 600 may continue to 614 from 612, where control circuitry 304 may compare portions of the characteristics of the fingerprint against characteristics of the entries.  For example, a fingerprint of device indicator 506, which is an image\nof an XBOX controller, may have individual portions reflecting different characteristics of the XBOX controller, such as its color, size, button layout, textual features, and any other potential characteristic.  Control circuitry 304 may compare each of\nthese individual portions of a fingerprint of device indicator 506 against characteristics of the entries to find partially matching fingerprints, such as fingerprints that share some but not all characteristics with the fingerprint of device indicator\n506.\n Process 600 may continue to 616, where control circuitry 304 may determine a set of fingerprints of the plurality of user interfaces where the portions of the characteristics of a fingerprint of a user interface, such as device indicator 506,\nmatch the characteristics of the entries.  For example, control circuitry 304 may identify a set of partially matching fingerprints, such as fingerprints that also indicate XBOX controllers, the same color, the same button configuration, etc.\n Process 600 may then continue to 618, where control circuitry 304 may access a knowledge graph that describes relationships between the entries.  Control circuitry 304 may access the knowledge graph by way of communications network 414.  The\nknowledge graph may be stored at storage 308, media content source 416, or media guidance data source 418.  The knowledge graph may indicate various relationships between the entries, such as that 60% of the entries correspond to user interfaces that are\ncontrollers, that 80% of the entries correspond to user interfaces that are provided by XBOX systems, and the like.\n Process 600 may then continue to 620, where control circuitry 304 may determine, based on the relationships, a relationship between each fingerprint of the set of fingerprints.  For example, control circuitry 304 may determine that each\nfingerprint corresponds to a user interface that is provided by an XBOX system.  Process 600 may then proceed to 622, where control circuitry 304 may thereby determine that the user interface (e.g., device indicator 506 itself) is a particular user\ninterface based on the relationship.  In other words, by example, control circuitry 304 may determine that device indicator 506 is a user interface of an XBOX.\n It is contemplated that the steps or descriptions of FIG. 6 may be used with any other embodiment of this disclosure.  In addition, the steps and descriptions described in relation to FIG. 6 may be done in alternative orders or in parallel to\nfurther the purposes of this disclosure.  For example, each of these steps may be performed in any order or in parallel or substantially simultaneously to reduce lag or increase the speed of the system or method.  Furthermore, it should be noted that any\nof the devices or equipment discussed in relation to FIGS. 3-5 could be used to perform one or more of the steps in FIG. 6.\n FIG. 7 is a flowchart of illustrative steps involved in using fingerprinting to determine when a device has been replaced by another device at a user input, in accordance with some embodiments of the disclosure.  It should be noted that process\n700 or any step thereof could be performed on, or provided by, any of the devices shown in FIGS. 3-5.  For example, process 700 may be executed by control circuitry 304 (FIG. 3) as instructed by control circuitry implemented on user equipment 402, 404,\nand/or 406 (FIG. 4) in order to determine a country of origin of a user.  In addition, one or more steps of process 700 may be incorporated into or combined with one or more steps of any other process or embodiment.\n Process 700 begins at 702, where control circuitry 304 may capture, at a first time, a first plurality of fingerprints of a first image.  Control circuitry 304 may capture the first plurality of fingerprints of a first image in any manner\ndescribed above and below.  As depicted in FIG. 5, control circuitry may capture a screenshot of a display of user equipment 500, which may be the first image.  Control circuitry 304 may capture a plurality of fingerprints of the screenshot, such as a\nfingerprint of source indicator 502, a fingerprint of source indicator 504, and a fingerprint of device indicator 506.\n Process 700 may continue to 704, where control circuitry 304 may determine, based on a first fingerprint of the first plurality of fingerprints, an identity of a first output device that is providing the image.  For example, control circuitry\n304 may use the systems and methods described above and below to determine an identity of an output device corresponding to device indicator 506.  In one example, control circuitry 304 may determine that device indicator 506 corresponds to an XBOX.\n Process 700 may continue to 706, where control circuitry 304 may determine, based on a second fingerprint of the first plurality of fingerprints, an input port that the first output device is providing the first image through.  For example,\nusing the systems and methods described above and below, control circuitry 304 may determine that source identifier 502 corresponds to input port HDMI1 of user equipment 500.\n Process 700 may continue to 708, where control circuitry 304 may capture, at a second time later than the first time, a second plurality of fingerprints of a second image.  For example, control circuitry 304 may capture fingerprints of source\nindicator 502, application indicator 504, and device indicator 506 at a second time.  Control circuitry 304 may capture this second plurality of fingerprints to determine whether any of source indicator 502, application indicator 504, and device\nindicator 506 has changed.\n Process 700 may continue to 710, where control circuitry 304 may determine, based on a third fingerprint of the second plurality of fingerprints, that the second image is provided through the input port.  Process 700 may continue to 712, where\ncontrol circuitry 304 may determine, based on a fourth fingerprint of the second plurality of fingerprints, that a second output device is providing the second image through the input port.\n Process 700 may continue to 714, where control circuitry 304 may determine whether the second output device is different from the first output device.  Control circuitry 304 may perform this determination by identifying a device that corresponds\nto device indicator 506 at the second time using the systems and methods described above and below, and determining that this device is different than the device that corresponds to device indicator 506 at the first time.  If the second output device is\nnot different from the first output device, process 700 ends.  However, if the second output device is different from the first output device, process 700 continues to 718, where control circuitry 304 may determine that a user has discontinued use of the\nfirst output device.  For example, control circuitry 304 may determine that an XBOX ONE is now providing input by way of input port HDMI1, and therefore that an XBOX ONE has replaced the outdated XBOX.  Control circuitry 304 may responsively determine\nthat a user has discontinued use of the XBOX.\n It is contemplated that the steps or descriptions of FIG. 7 may be used with any other embodiment of this disclosure.  In addition, the elements and descriptions described in relation to FIG. 7 may be done in alternative orders or in parallel to\nfurther the purposes of this disclosure.  For example, each of these steps may be performed in any order or in parallel or substantially simultaneously to reduce lag or increase the speed of the system or method.  Furthermore, it should be noted that any\nof the devices or equipment discussed in relation to FIGS. 3-5 could be used to perform one or more of the steps in FIG. 6.\n FIGS. 8 and 9 present an algorithm for control circuitry (e.g., control circuitry 304) to identify a source of a user interface by way of fingerprinting using a database containing known fingerprints of known user interfaces in accordance with\nsome embodiments of the disclosure.  In some embodiments this algorithm may be encoded on to non-transitory storage medium (e.g., storage device 308) as a set of instructions to be decoded and executed by processing circuitry (e.g., processing circuitry\n306).  Processing circuitry may in turn provide instructions to other sub-circuits contained within control circuitry 304, such as the tuning, video generating, encoding, decoding, encrypting, decrypting, scaling, analog/digital conversion circuitry, and\nthe like.\n The flowchart in FIG. 8 describes an algorithm for control circuitry (e.g., control circuitry 304) to search a database and to identify a source of a user interface by way of fingerprinting in accordance with some embodiments of the disclosure.\n At 802, the algorithm to search a database and to identify a source of a user interface by way of fingerprinting will begin based on a trigger event, such as a user equipment or an output device being powered on, an application being accessed,\nand/or an input port of a user equipment being accessed.  In some embodiments, this may be done either directly or indirectly in response to a user action or input (e.g., from signals received by control circuitry 304 or user input interface 310).\n At 804, control circuitry 304 proceeds to retrieve the next captured fingerprint of a user interface (e.g., user interfaces 502, 504, and/or 506).  In some embodiments control circuitry 304 may retrieve a single primitive data structure that\nrepresents the value of one or more characteristics of the fingerprint.  In some embodiments control circuitry 304 may retrieve the value from a larger class or data structure.\n At 806, control circuitry 304 accesses a database containing entries of the fingerprints of known user interfaces.  In some embodiments, this database may be stored locally (e.g., on storage device 308) prior to beginning the algorithm.  In some\nembodiments the database may also be accessed by using communications circuitry to transmit information across a communications network (e.g., communications network 414) to a database implemented on a remote storage device (e.g., media guidance data\nsource 418).\n At 808, control circuitry 304 searches database tables for entries matching the fingerprint of the user interface.  In some embodiments this may be done by comparing an identifier, for example a string or integer representing the characteristics\nof the fingerprint of the user interface, that matches the types of identifiers used inside the database.  In some embodiments control circuitry 304 may submit a general query to the database for table entries matching the characteristics of the\nfingerprint of the user interface, and control circuitry 304 may receive a list of indices or a data structure containing a portion of the database contents.  In some embodiments the database may implement a junction table that in turn cross-references\nentries from other databases.  In this case, control circuitry 304 may retrieve indices from a first database that in turn can be used to retrieve information from a second database.  Although we may describe control circuitry 304 interacting with a\nsingle database for purposes of clarity, it is understood that the algorithm of FIG. 7 may be implemented using multiple independent or cross-referenced databases.\n At 810, control circuitry 304 may determine if there are database entries matching all of the characteristics of the fingerprint of the user interface.  In some embodiments control circuitry 304 may receive a signal from the database indicating\nthat there are no matching entries.  In some embodiments control circuitry 304 may instead receive a list of indices or data structures with a NULL or dummy value.  If control circuitry 304 identifies that there are database entries matching all of the\ncharacteristics of the fingerprint of the user interface the algorithm proceeds to 812, otherwise the algorithm proceeds to 814.\n At 812, control circuitry 304 will execute a subroutine to identify the source of the user interface based on the matching entry.  Afterwards, the algorithm may proceed to 820.\n At 814, control circuitry 304 may determine if there are database entries similar to the fingerprint of the user interface.  For example, in some embodiments, if the fingerprint of the user interface is encoded as a string with multiple\ncharacters, control circuitry 304 may perform additional database queries for similar strings with individual characters replaced, removed or added.  In some embodiments control circuitry 304 may also determine if the original query was a commonly\nmisspelled word, and will submit a query with the correct spelling instead.  In another example, the characteristics of the fingerprint of the user interface may be encoded as an integer; control circuitry 304 may perform additional queries for other\nintegers within a certain range.  In some embodiments control circuitry 304 may retrieve database entries similar to the individual characteristics of the fingerprint of the user interface without requiring further queries.  If control circuitry 304\nidentifies that there are database entries similar to the individual characteristics of the fingerprint of the user interface the algorithm proceeds to step 816; otherwise the algorithm proceeds to step 818.\n At 816, control circuitry 304 will execute a subroutine to identify a source of the user interface based on the similar entries.  Afterwards, the algorithm may proceed to step 820.\n At 818, control circuitry 304 will execute a subroutine to prompt a user to identify the source of the user interface.  Afterwards, the algorithm may proceed to 820.\n At 820, control circuitry 304 will determine if all instances of the fingerprint of the user interface are accounted for and if further iterations are needed.  If further iterations are needed the algorithm will loop back to 804 where control\ncircuitry 304 will retrieve the next instance of the characteristics of the fingerprint of the user interface.  If no further iterations are needed the algorithm will proceed to 822.\n At 822, control circuitry 304 will execute a subroutine to apply the user interface source to a given function (e.g., tracking popularity of a source device or of an application in the manner described above and below).\n It is contemplated that the descriptions of FIG. 8 may be used with any other embodiment of this disclosure.  In addition, the descriptions described in relation to the algorithm of FIG. 8 may be done in alternative orders or in parallel to\nfurther the purposes of this disclosure.  For example, control circuitry 304 may submit multiple queries to the database in parallel, or it may submit multiple queries to a plurality of similar databases in order to reduce lag and speed the execution of\nthe algorithm.  As a further example, although 812 and 816 are described as being mutually exclusive, both exact entries and similar entries may be processed for a single instance of an individual characteristic of the fingerprint of the user interface. \nTo further this purpose, in some embodiments 810 and 814 may be performed in parallel by control circuitry 304.  Furthermore, it should be noted that the algorithm of FIG. 8 may be implemented on a combination of appropriately configured software and\nhardware, and that any of the devices or equipment discussed in relation to FIGS. 3-4 could be used to implement one or more portions of the algorithm.\n The pseudocode in FIG. 9 describes an algorithm to identify a source of a user interface by way of fingerprinting in accordance with some embodiments of the disclosure.  It will be evident to one skilled in the art that the algorithm described\nby the pseudocode in FIG. 9 may be implemented in any number of programming languages and a variety of different hardware, and that the style and format should not be construed as limiting, but rather a general template of the steps and procedures that\nwould be consistent with code used to implement some embodiments of this disclosure.\n At line 901, the algorithm may run a subroutine to initialize variables and prepare to identify a source of a user interface by way of fingerprinting, which begins on line 905.  For example, in some embodiments control circuitry 304 may copy\ninstructions from non-transitory storage medium (e.g., storage device 308) into RAM or into the cache for processing circuitry 306 during the initialization stage.\n At line 905, control circuitry 304 may receive instances of fingerprints of a user interface.\n At line 906, control circuitry 304 may iterate through the various instances of fingerprints of a user interface; if only a single instance is available, the loop will only execute once.  This loop may be implemented in multiple fashions\ndepending on the choice of hardware and software language used to implement the algorithm of FIG. 8; for example, this may be implemented as part of a \"for\" or \"while\" loop, in some programming languages.  In some embodiments it may be convenient to\nstore the instances of fingerprints of a user interface in a single class or encapsulated data structure that will perform the loop as part of an internal method.\n At line 907, control circuitry 304 may query a database for entries matching a fingerprint of a user interface.  Depending on how the database is implemented and how a fingerprint of a user interface is stored, an intermittent step may be\nrequired to convert the fingerprint into a form consistent with the database.  For example, the fingerprint may be encoded into a string or an integer using an appropriate hashing algorithm prior to being transmitted to the database by control circuitry\n304 as part of a query.  In some embodiments the fingerprint may be encoded as a primitive data structure, and control circuitry 304 may submit the fingerprint as a query to the database directly.  After querying the database, control circuitry 304 may\nreceive a set of database entries matching the fingerprint.  In some embodiments control circuitry 304 may receive these entries in the form of a data-structure, a set of indices of the database, or a set of indices of another cross-referenced database.\n At line 908, control circuitry 304 will determine if there are any database entries matching the fingerprint.  In some embodiments control circuitry 304 may determine this by checking if the database returned an empty data structure or a NULL\nvalue in response to the query in line 907.  If there are matching database entries the algorithm may proceed to line 909.  If there were no matching database entries the algorithm may instead proceed to line 812.\n At line 909, control circuitry 304 may retrieve one or more user interface identifiers from the database entries matching the fingerprint of the user interface.  For example, if control circuitry 304 retrieves a list of indices after querying\nthe database in line 907, in some embodiments control circuitry 304 may retrieve the database known user interface identifiers located at the received indices.  In some embodiments the indices may point to a larger data structure contained within the\ndatabase, and control circuitry 304 may retrieve the values of user interface identifiers from within the data structure using appropriate accessor methods.  In some embodiments control circuitry 304 may retrieve the values of the user interface\nidentifiers and store them in a separate data structure locally (e.g., in storage 308) prior to proceeding further.  After retrieving the values of characteristics of fingerprints of known user interfaces the algorithm will proceed to line 910.\n At line 910, control circuitry 304 will execute a subroutine to use the user interface identifier to determine the source of the user interface using control circuitry 304.  Afterwards, the algorithm may proceed to line 915.\n At line 911, control circuitry 304 may determine if there are any database entries similar to the fingerprint of the user interface.  For example, the fingerprint may be represented by an object of a class.  Control circuitry 304 may call a\nfunction to perform a fuzzy comparison (e.g., a comparison to identify similar objects of the class) by comparing specific fields of the class or by performing approximate string matching on data related to the fingerprint.  If database entries similar\nto the fingerprint are found by control circuitry 304 then the algorithm proceeds to line 912.  If control circuitry 304 does not find matching entries (e.g., a query to the database returns a NULL value), the algorithm proceeds to line 912.\n At line 912, control circuitry 304 will execute a subroutine to use the values of the set of similar entries to identify a common user interface source.  Afterwards, the algorithm may proceed to line 915.\n At line 911, control circuitry 304 will have determined that there were no database entries matching the fingerprint of the user interface.  In this case, the algorithm will proceed to line 912.\n At line 912, control circuitry 304 will execute a subroutine to determine the source of the user interface based of the identifying of the common user interface source.  Afterwards, the algorithm may proceed to line 914.\n At line 914, control circuitry 304 will execute a subroutine to prompt a user for an input of an identification of the source of the user interface if neither of the conditions at lines 909 or 911 are satisfied.\n At line 915, control circuitry 304 will execute a subroutine to use the user interface source for an intended function (e.g., to track popularity of a given device or application).  Afterwards, the algorithm may proceed to the termination\nsubroutine at line 917.\n At line 917, control circuitry 304 may execute a termination subroutine after the algorithm has performed its function and all instances of a user interface fingerprint have been processed and checked against the database.  For example, in some\nembodiments control circuitry 304 may destruct variables, perform garbage collection, free memory or clear the cache of processing circuitry 306.\n It will be evident to one skilled in the art that the algorithm described by the pseudocode in FIG. 9 may be implemented in any number of programming languages and a variety of different hardware, and the particular choice and location of\nprimitive functions, logical evaluations, and function evaluations are not intended to be limiting.  It will also be evident that the code may be refactored or rewritten to manipulate the order of the various logical evaluations, perform several\niterations in parallel rather than in a single iterative loop, or to otherwise manipulate and optimize run-time and performance metrics without fundamentally changing the inputs or final outputs.  For example, in some embodiments the code may be\nre-written so control circuitry 304 is instructed to evaluate multiple instances of a fingerprint of a user interface and submit multiple database queries simultaneously using a plurality of processors or processor threads.  It is also understood that\nalthough we may describe control circuitry 304 interacting with a single database, this is only a single embodiment described for illustrative purposes, and the algorithm of FIG. 9.  may be implement using multiple independent or cross-referenced\ndatabases.  For example, a database stored locally (e.g., on storage 308) may index or cross-reference a database stored remotely (e.g., media guidance data source 418), which may be accessible through any number of communication channels (e.g.,\ncommunications network 414).  In some embodiments, this may allow control circuitry 304 to utilize a look-up table or database front-end efficiently stored on a small local drive to access a larger database stored on a remote server on demand.\n It will be apparent to those of ordinary skill in the art that methods involved in the present invention may be embodied in a computer program product that includes a computer-usable and/or readable medium.  For example, such a computer-usable\nmedium may consist of a read-only memory device, such as a CD-ROM disk or conventional ROM devices, or a random access memory, such as a hard drive device or a computer diskette, having a computer-readable program code stored thereon.  It should also be\nunderstood that methods, techniques, and processes involved in the present invention may be executed using processing circuitry.  For instance, fingerprinting of a user interface may be performed by processing circuitry, e.g., by processing circuitry 306\nof FIG. 3.  The processing circuitry, for instance, may be a general purpose processor, a customized integrated circuit (e.g., an ASIC), or a field-programmable gate array (FPGA) within user equipment 300, media content source 416, or media guidance data\nsource 418.  For example, an entry of a fingerprint as described herein may be stored in, and retrieved from, storage 308 of FIG. 3, or media guidance data source 418 of FIG. 4.  Furthermore, processing circuitry, or a computer program, may update\nsettings associated with a user, such as a country of origin, updating the information stored within storage 308 of FIG. 3 or media guidance data source 418 of FIG. 4.\n The processes discussed above are intended to be illustrative and not limiting.  One skilled in the art would appreciate that the steps of the processes discussed herein may be omitted, modified, combined, and/or rearranged, and any additional\nsteps may be performed without departing from the scope of the invention.  More generally, the above disclosure is meant to be exemplary and not limiting.  Only the claims that follow are meant to set bounds as to what the present invention includes. \nFurthermore, it should be noted that the features and limitations described in any one embodiment may be applied to any other embodiment herein, and flowcharts or examples relating to one embodiment may be combined with any other embodiment in a suitable\nmanner, done in different orders, or done in parallel.  In addition, the systems and methods described herein may be performed in real time.  It should also be noted, the systems and/or methods described above may be applied to, or used in accordance\nwith, other systems and/or methods.", "application_number": "15836466", "abstract": " Systems and methods are provided herein for determining aspects of a user\n     interface, and determining therefrom a service or device that corresponds\n     to the user interface. In doing so, a user's activity can be profiled,\n     and aggregate usage data over a population of particular devices and OTT\n     applications can be tracked. Moreover, losses and gains of popularity of\n     services or devices may be monitored.\n", "citations": ["6725061", "7053811", "7889926", "9569520", "20090077606", "20100091135", "20120106366", "20140181853", "20140195666", "20150040180", "20160142629", "20160142647", "20160234550", "20180103290"], "related": ["14919425"]}, {"id": "20180196796", "patent_code": "10366168", "patent_name": "Systems and methods for a multiple topic chat bot", "year": "2019", "inventor_and_country_data": " Inventors: \nWu; Xianchao (Tokyo, JP)  ", "description": "BACKGROUND\n Bots are becoming more and more prevalent and are being utilized for more and more different tasks.  As understood by those skilled in the art, bots are software applications that may run automated tasks over a network, such as the Internet. \nChat bots are designed to conduct a conversation with a user via text, auditory, and/or visual methods to simulate human conversation.  A chat bot may utilize sophisticated natural language processing systems or scan for keywords from a user input and\nthen pull a reply with the most matching keywords or the most similar wording pattern from a database.  However, chat bots are often limited to simple task driven conversations.\n It is with respect to these and other general considerations that aspects disclosed herein have been made.  Also, although relatively specific problems may be discussed, it should be understood that the aspects should not be limited to solving\nthe specific problems identified in the background or elsewhere in this disclosure.\nSUMMARY\n In summary, the disclosure generally relates to systems and methods for multiple topic automated chatting.  The systems and methods as described herein provide artificial intelligence chatting with multiple topics by analyzing user inputs in a\nconversation to determine a plurality topics, to determine and score features related to the determined topics and different users, and to create a knowledge graph of the determined topics.  Based on these determinations, the systems and methods may\ndetermine if a reply should be provided and then predict a reply to provide.\n As such, the systems and methods as described herein perform multiple topic intelligent automated chatting that is more effective, more engaging, easier to use, and more lifelike than previously utilized chat bots that were not able to track and\nrespond to multiple topics in a conversation between one or more users.\n One aspect of the disclosure is directed to a system for a multiple topic chat bot.  The system includes at least one processor and a memory.  The memory encodes computer executable instruction that, when executed by the at least one processor,\nare operative to: collect user inputs in a conversation to form a collection; analyze the collection to determine topics in the conversation; assign an emotion label to each topic; identify a relationship between different users; score a closeness of the\nrelationship based on social connection, agreement, and sentiment analysis to form a scored first feature; score each user's interest in each topic based on user sentiment toward each topic and engagement frequency in each topic to form a scored second\nfeature; score an engagement rate for each topic of the topics based on a number of users engaged in a topic, frequency of the topic in the conversation, timing of the topic, and the user sentiment toward the topic to form a scored third feature; create\na knowledge graph of the topics that graphs relationships between the topics utilizing topic keywords based on the collection and world knowledge; determine that a first topic meets a relevancy threshold based on scored features for the first topic;\npredict one or more first responses based on the knowledge graph and the user inputs associated with the first topic; and provide the one or more first responses to the conversation.  The scored features include the scored first feature, the scored\nsecond feature, and the scored third feature for the first topic.\n In another aspect, a method for automated multi-topic chatting is disclosed.  The method includes: collecting inputs in a conversation to form a collection; analyzing the collection to determine topics in the conversation; assign a sentiment to\neach topic; scoring an engagement rate for each topic to form an engagement score for each topic; scoring a user interest in each topic to form an interest score for each topic; creating a knowledge graph between the topics that graphs relationships\nbetween the topics; determining that a first topic of the topics meets a relevancy threshold based on the engagement score and the interest score of the first topic; predicting a first response utilizing the knowledge graph and inputs associated with the\nfirst topic; and providing the first response to the conversation.\n In yet another aspect of the invention, the disclosure is directed to a system for a multiple topic intelligent chat bot.  The system includes at least one processor and a memory.  The memory encodes computer executable instruction that, when\nexecuted by the at least one processor, are operative to: collect user inputs from a group chat of a first user and a second user to form a collection; analyze the collection to determine a first topic and a second topic; assign a sentiment to each of\nthe first topic, the second topic, and the third topic; create a knowledge graph of the first topic and the second topic; identify a first relationship between the first user and the second user; score a closeness of the first relationship to form a\nscored first relationship; score an interest of each of the first user and the second user in the first topic to form a scored first user-first topic interest and a scored second user-first topic interest; score the interest of each of the first user and\nthe second user in the second topic to form a scored first user-second topic interest and a scored second user-second topic interest; score an engagement rate for each of the first topic and the second topic to form a scored first topic engagement rate\nand a scored second topic engagement rate; determine a first relevancy score of the first topic based on a first evaluation of: the scored first relationship if both the first user and the second user discussed the first topic with each other; the scored\nfirst user-first topic interest, the scored second user-first topic interest, and the scored first topic engagement rate; determine a second relevancy score of the second topic based on a second evaluation of: the scored first relationship if both the\nfirst user and the second user discussed the second topic with each other; the scored first user-second topic interest, the scored second user-second topic interest, and the scored second topic engagement rate; determine that a second topic meets a\nrelevancy threshold based on the second relevancy score of the second topic; predict a response utilizing the knowledge graph and the user inputs associated with the second topic; and provide the response to the group chat.\n This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description.  This summary is not intended to identify key features or essential features of the claimed subject\nmatter, nor is it intended to be used to limit the scope of the claimed subject matter. BRIEF DESCRIPTION OF THE DRAWINGS\n Non-limiting and non-exhaustive embodiments are described with reference to the following Figures.\n FIG. 1A is a schematic diagram illustrating a multiple topic artificial intelligence (AI) chat bot on a client computing device being utilized by a user, in accordance with aspects of the disclosure.\n FIG. 1B is a schematic diagram illustrating a multiple topic AI chat bot on a server computing device being utilized by a user via a client computing device, in accordance with aspects of the disclosure.\n FIG. 2A is a schematic diagram illustrating a work flow for a multiple topic AI chat bot, in accordance with aspects of the disclosure.\n FIG. 2B is a schematic diagram illustrating operations 126, 133, and 116 for the work flow for the emotionally intelligent AI chat bot shown in FIG. 2A, in accordance with aspects of the disclosure.\n FIG. 3A is a schematic diagram illustrating a screen shot of a user interface of the user's client computing device during a conversation with a multiple topic AI chat bot system with multiple users, in accordance with aspects of the disclosure.\n FIG. 3B is a schematic diagram illustrating a screen shot of a user interface of the user's client computing device shown in FIG. 3A during a conversation with the multiple topic AI chat bot during a conversation with a single user, in\naccordance with aspects of the disclosure.\n FIG. 4 is a block flow diagram illustrating a method for multiple topic intelligent automated chatting, in accordance with aspects of the disclosure.\n FIG. 5 is a block diagram illustrating example physical components of a computing device with which various aspects of the disclosure may be practiced.\n FIG. 6A is a simplified block diagram of a mobile computing device with which various aspects of the disclosure may be practiced.\n FIG. 6B is a simplified block diagram of the mobile computing device shown in FIG. 6A with which various aspects of the disclosure may be practiced.\n FIG. 7 is a simplified block diagram of a distributed computing system in which various aspects of the disclosure may be practiced.\n FIG. 8 illustrates a tablet computing device with which various aspects of the disclosure may be practiced.\n FIG. 9 is a schematic diagram illustrating an example of the syntactic dependency tree created by a topic detection system for an example sentence of \"Anybody coming to happy hour tonight?\", in accordance with aspects of the disclosure.\n FIG. 10 is a schematic diagram illustrating an example of the syntactic dependency tree created by a topic jumping system for an example sentence of \"Today's happy hour is at the restaurant named My Yakitori in Ooimachi\", in accordance with\naspects of the disclosure.\n FIG. 11 is a schematic diagram illustrating an example of a topic knowledge graph created by the topic jumping system based on the conversation shown in FIG. 3A, in accordance with aspects of the disclosure.\nDETAILED DESCRIPTION\n In the following detailed description, references are made to the accompanying drawings that form a part hereof, and in which are shown by way of illustrations specific aspects or examples.  These aspects may be combined, other aspects may be\nutilized, and structural changes may be made without departing from the spirit or scope of the present disclosure.  The following detailed description is therefore not to be taken in a limiting sense, and the scope of the present disclosure is defined by\nthe claims and their equivalents.\n Bots are becoming more and more prevalent and are being utilized for more and more different tasks.  As understood by those skilled in the art, bots are software applications that may run automated tasks over a network, such as the Internet. \nChat bots are designed to conduct a conversation with a user via auditory or visual methods to simulate human conversation.  A chat bot may utilize sophisticated natural language processing systems or scan for keywords from a user input and then pull a\nreply with the most matching keywords or the most similar wording pattern from a database.  Chat bots are often utilized for customer service or information acquisition.  However, chat bots are often limited to simple task driven conversations.\n For example, e-commerce online shopping customizes the general chat bots to fit individual shops (for clothes, shoes, cameras, cosmetics and so on) and supply online and in-time conversation-style consumer services.  Through this multiple round\nconversation, the consumers' questions are answered and the consumers' orders will be consequently received.  In addition, consumers' detailed requests are clarified step-by-step during the session of a conversation.  However, these types of consumer\nservice chat bots are typically designed to be single-round question-answering service.  Further, the user can often tell that they are conversing with a chat bot due to the lack of emotion and limited task oriented questions.  As such, the currently\nutilized chat bots are unable to follow and respond to multiple topics in a conversation with one or multiple users.\n As such, the systems and method as disclosed herein are directed to a multiple topic artificial intelligence (AI) chat bot that can respond to user queries and spontaneously respond (without a response request) in a conversation with multiple\ntopics with one or more users.  The multiple topic AI chat bot utilizes deep learning and sentiment analysis to identify topics, determine and score relationships between users, determine and score topic engagement, and to determine and score user-topic\ninterest.  Further, the multiple topic AI chat bot creates a knowledge graph of the relationships between the different determined topics.  The multiple topic AI chat bot determines if a response should be provided by the chat bot by evaluating the\ndetermined the determined and scored relationships between users, the determined and scored topic engagement, and the determined and scored user-topic interest for each identified topic.  If the multiple topic chat bot 100 determines to provide a\nresponse, the multiple topic AI chat bot predicts a response based at least on the created knowledge graph.  In some aspects, the multiple topic AI chat bot utilizes user feedback and/or world feedback to train and update the learning algorithms and/or\nmodels of the AI chat bot to improve the multiple topic AI chat bot's responses over time.\n The ability of the systems and methods to perform multiple topic intelligent automated chatting as described herein provides a chat bot or application that is capable of tracking multiple topics in a conversation with one or more users and\nproviding contextually and emotionally appropriate spontaneous responses or requested responses.  Further, the ability of the systems and methods described herein to select contextually and emotionally appropriate responses for multiple topic\nconversations improves the user's trust and engagement with chat bot.  As such, the systems and methods that perform multiple topic automated chatting as described herein provide a chat bot that is more effective, more engaging, easier to use, and more\nlifelike than previously utilized chat bots that were not able to track and respond to multiple topic conversations between one or more users.\n FIGS. 1A and 1B illustrate different examples of a multiple topic chat bot 100 or multiple topic AI chat bot 100 being utilized by a user, in accordance with aspects of the disclosure.  The chat bot 100 is capable of determining, tracking, and\nspontaneously or upon request responding in a conversation with multiple topics with one or more users.\n The chat bot 100 includes a language understanding (LU) system 110, a topic detection system 112, a sentiment system 114, a response prediction system 116, a feedback system 119, a core worker 111, a user relationship system 140, a topic\nengagement system 142, a user-topic system 144, and a topic jumping system 146.  The chat bot 100 may also communicate with other databases 109 and servers 105 via network 113, such as database that tracks and stores world feedback 122.  In some aspects,\nthe network 113 is a distributed computing network, such as the internet.  In some aspects, the chat bot 100 communicates with and/or retrieves or accesses data from world knowledge 118 via a network 113.\n In some aspects, the chat bot 100 is implemented on the client computing device 104 as illustrated by FIG. 1A.  In a basic configuration, the client computing device 104 is a computer having both input elements and output elements.  The client\ncomputing device 104 may be any suitable computing device for implementing the chat bot 100.  For example, the client computing device 104 may be a mobile telephone, a smart phone, a tablet, a phablet, a smart watch, a wearable computer, a personal\ncomputer, a gaming system, a desktop computer, a laptop computer, and/or etc. This list is exemplary only and should not be considered as limiting.  Any suitable client computing device 104 for implementing the chat bot 100 and/or for communicating with\nthe chat bot 100 may be utilized.\n In other aspects, the chat bot 100 is implemented on a server computing device 105, as illustrated in FIG. 1B.  The server computing device 105 may provide data to and/or receive data from the client computing device 104 through the network 113. In further aspects, that chat bot 100 is implemented on more than one server computing device 105, such as a plurality or network of server computing devices 105.  For example, the LU system 110 may be located on server separate from a server containing\nthe core worker 111.  In some aspects, the chat bot 100 is a hybrid system with portions of the chat bot 100 on the client computing device 104 and with portions of the chat bot 100 on one or more server computing devices 105.\n FIG. 2A illustrates a work flow diagram 200 for a multiple topic AI chat bot 100, in accordance with aspects of the disclosure.  FIG. 2B illustrates systems/operations 126, 133, and 116 for the work flow diagram 200 for the multiple topic AI\nchat bot 100 shown in FIG. 2A, in accordance with aspects of the disclosure.  The chat bot 100 collects user inputs 130 for a conversation.  The conversation may between the chat bot 100 and one user as illustrated in FIG. 3B or between the chat bot 100\nand a plurality of users as illustrated in FIG. 3B.  As such, the user inputs 130 may be from one or more users 102 of the chat bot 100 engaged in a conversation with each other and the chat bot 100.  The user inputs 130 may include one or more queries\nfor the chat bot 100.  A conversation as utilized herein refers to electronic communication between the one or more users and the AI chat bot, which may be hosted by a site, a location, an application, the chat bot 100, and/or computing device.  Further,\nthe topics of the conversation may be open domain oriented.  That is, any domain in ordinary lives may be involved in the conversation.\n In some aspects, the multiple topic AI chat bot 100 utilizes learning algorithms and/or models and sentiment analysis 114 to identify topics 128 utilizing a topic detection system 112, determine and score relationships 131 between users 102\nutilizing a user relationship detection system 140, determine and score topic engagement utilizing a topic engagement system 142, and to determine and score user-topic interest utilizing a user-topic system 144.  Further, the multiple topic AI chat bot\n100 creates a topic knowledge graph 135 of the relationships between the different determined topics 128 utilizing a topic jumping system 146.  The multiple topic AI chat bot 100 determines if a spontaneous or requested response should be provided by the\nAI chat bot 100 based on the scored relationships, scored topic engagement, and/or the scored user-topic interest utilizing a response prediction system 116.  When the AI chat bot 100 decides to provide a response, the response prediction system 116\npredicts a response utilizing collected inputs, labeled sentences, labeled topics, scored features, and/or the created knowledge graph 135.\n The one or more users 102 enter input 130 into the user interfaces' of their client computing devices to participate in the conversation.  The input is collected by the AI chat bot 100 and saved and provided (such as displayed) to the forum\nhosting the conversation.  The user interface of the client computing devices 104 of each user 102 in the conversation may provide (such as display) all of the inputs for the conversation to their user.  A user input 130 as utilized herein refers to a\nuser question, a user query 127, a user comment, a user answer, or any other user information input and intended for the conversation with the chat bot 100 and/or other users 102 in the conversation.  A user query 127 as utilized herein refers to any\nquestion or request for the chat bot 100 from one or more users that requires or is intended to illicit a response or action by the chat bot 100.  Each user 102 may provide his or her input 130 as text, video, audio, and/or any other known method for\ngathering user input.  In the user's input area, a user 102 can type text, select emoji symbols, and make a short-cut of current screen.  Additionally, the user 102 can make a voice call or a video conversation with the chat bot 100 and/or other users\n102.  For example, the user interface of the client computing device 104 may receive the user's input 130 as voice input.\n The chat bot 100 collects the user input 130 from the client computing device 104.  The term \"collect\" as utilized herein refers to the passive receiving or receipt of data and/or to the active gathering or retrieval of data.  The core worker\n111 of the chat bot 100 collects the user input 130.\n For example, in the group conversation between User 1, User 2, User 3, User 4, and the chat bot 100 as shown in the user interface (UI) illustrated in FIG. 3A, the chat bot 100 collects the user input 130 from User 1: \"Anybody coming to happy\nhour tonight?\"; \"At Ooimachi @ 6 pm\"; \"@User3: thank you! @User2: next time.  Rinna did you record?\".  In FIGS. 3A-3B, Rinna stands for the name of the AI chat bot 100.  The chat bot 100 also collects the user input 130 from User 2: \"Bad for tomorrow\nwork attendance\" and \"sorry for not being able to come\"; from User 3: really? I love drinking with User1, so, where and when?\" and \"Done!\"; and from User 4: \"Excuse me, I just finished working, what's going on here?\" and \"Rinna, you are so cute.  Thank\nyou! I am currently at Shinagawa.\" Each of the above sentences are transferred to the \"request queue\", which stores users' requests in multimedia format including texts, sounds, images, and even videos by the core worker 111 of the chat bot 100. \nHowever, the chat bot 100 deals with different kinds of multimedia inputs differently.  For example, for real-time sounds and videos, the AI chat both 100 needs a sufficient amount of core workers 111 to ensure that the queue is not too long so a user\nutilizing the chat bot 100 does not receive too long of a delay between his or her input 130 and the AI chat bot 100 reply 132.  For texts and images, the chat bot 100 may utilize less core workers 111 for processing.\n The core worker 111 collects the request queue as input.  Requests in the queue are served and/or responded to in first-in-first-out manner by the core worker 111.  As such, the core worker 111 will one-by-one determine a type of input (voice,\nvideo, text, etc.) of each input 130 for proper processing by the chat bot 100.  For example, the core worker 111 will send the user inputs 130 to the topic detection system 112, the sentiment system 114, the response prediction system 116, user\nrelationship system 140, topic engagement system 142, user-topic system 144, topic jumping system 146 and/or the feedback system 119.\n The core worker 111 utilizes or sends the user's input 130 to a language understanding (LU) system 110 for processing.  The LU system 110 converts the user's inputs 130 into text and/or annotated text.  The LU system 110 includes application\nprogramming interfaces (APIs) for text understanding, speech recognition, and/or image/video recognition for processing user inputs 130 into text and/or annotated text form.\n Sounds need to be recognized and decoded as texts.  A speech recognition API may be necessary for the speech-to-text conversion task and is part of the LU system 110.  Furthermore, the LU system 110 may need to convert a generated response 132\nfrom text to voice to provide a voice response to the user 102.  Further, the LU system 110 may also include an image recognition API to \"read\" and \"understand\" received images from the user 102.  The image recognition API of the LU system 110 translates\nor decodes received images into text.  Further, a response 132 by the chat bot 100 may be translated into images by the LU system 110 to provide an image response to the user 102.  For example, if the selected response is good job, the LU system 110\ncould convert this text into a thumbs-up, which is displayed to the user as an image or emoticon.  The core worker framework allows APIs to be easily added or removed.  As such, the core worker framework is extensible.\n For example, the core worker 111 utilizing the LU system 110 provides Rinna's (or chat bot's 100) responses of: \"Rinna also want to join! Should I like the drinking party?\"; \"[show map of the restaurant]; cost estimate: $40/person; at 18:00\";\n\"Drinking party! Currently we have two attendees.\"; \"[show map of the restaurant]; cost estimate: $40/person; at 18:00.\"; and \"[], here are the directions from Shinagawa to Ooimachi\" that include text, emoji's and maps as illustrated by FIG. 3A.\n The responses selected by the response prediction system 116 of the chat bot 100 are provided to the core worker 111.  The core worker 111 transfers the response to the response queue or into a cache.  The cache is necessary to make sure that a\nsequence of AI chat bot responses 132 or replies 132 can be shown to the user in a pre-defined time stream.  That is, for one user's input, if there are no less than two responses generated by the core worker 111, then a time-delay setting for the\nresponses may be necessary.\n For example, if the user says, \"Rinna, did you eat your breakfast?\", the multiple topic AI chat bot 100 may generate two responses, such as \"yes, I ate bread,\" and \"How about you? Still felling hungry?\".  In this scenario the core worker 111\nensures that the first response is provided to the user immediately.  Also, the core worker 111 of the chat bot 100 may ensure that the second response is provided in a time delay, such as 1 or 2 seconds, so that the second message will be provided to\nthe user two seconds after the first message.  As such, the cache of the core worker 111 manages these to-be-sent response messages together with user identities and appropriate timing for each chat bot generated question or comment.\n The text or annotated text generated by the LU system 110 for each collected input in the conversation is collected by the topic detection system 112 of the chat bot 100.  The one or more inputs in a conversation between the chat bot 100 and/or\none or more users may be referred to herein as a collection.  The topic detection model 112 analyzes the user inputs 130 or collection to determine one or more topics 128 in the conversation between the chat bot 100 and one or more users 102.  The topic\ndetection system 112 may utilize a syntactic dependency parser to parse sentences in session and pick noun words/phrases that are topic candidates.  An existing topic database can be obtained in a similar way by dependency parsing the web-sites and/or\nchatbot's large-scale chat logs in chat sessions.  For example, for the sentence \"?/Anybody coming to happy hour tonight?\" as illustrated in FIG. 3A, the different topics that may be identified by a semantic dependency tree of the topic detection system\n112 are listed below: -D/Tonight -D/drinking party/happy hour -D/coming /anyone Further, FIG. 9 illustrates an example of the syntactic dependency tree for the example sentence discussed above.  Part-of-speech (POS) tags are also labeled to each word in\nthe tree.  Accordingly, the candidate nouns for the example sentence above include \"happy hour\" and \"anyone\".  Since \"anyone\" is closer to a pronoun, \"happy hour\" is selected as the topic keyword by the topic detection system 112 for the example sentence\nprovided above.  Further, the topic detection system 112 may identify that an input is a query and that the query input is associated with one or more topics.  In response to identifying the query 127, the response prediction system 116 may collect the\nquery 127 and associated one or more topics from the topic detection system 112.  As such, topic detection system 112 may associate every sentence with one or more topic or topic keywords.\n The topic detection system 112 stores each determined topic.  In some aspects, the topic detection system 112 compares the timing of each determined topic to a timing threshold.  In these aspects, if the timing of the determined topic does not\nmeet a timing threshold, the topic detection system 112 deletes the topic from topic detection system 112 and the chat bot 100.  If the timing of the determined topic meets the timing threshold, the determined topic is continued to be stored by the topic\ndetection system 112 and the chat bot 100.  The timing of a topic refers to how recently the topic was last discussed in a conversation.  If a determined topic has not been discussed by any user in the conversation for 23 hour, the timing of this topic\nis 23 hours.  In some aspects, the timing threshold is 1 week, 3 days, 1 day, 12 hours, 6 hours, 5 hours, 1 hour, 30 minutes, 20 minutes, 10 minutes, or 5 minutes.  However, these timing thresholds are exemplary and are not meant to be limiting.  Any\nsuitable timing threshold may be utilized by the topic detection system 112 as would be understood by a person of skill in the art.\n The sentiment system 114, also referred to herein as the sentiment analysis system or sentiment analysis classifier, collects user input 130 and the topics 128 from topic detection system 112.  In some aspects, the sentiment system 114 of the\nchat bot 100 collects the user input 130 from the client computing devices 104.  In other aspects, the sentiment system 114 collects the user input 130 from the topic detection system 112.\n The sentiment system 114 analyzes the input sentences associated with each topic 128 to determine an emotion for each topic 128.  In some aspects, the sentiment system 114 determines if the emotion of a topic 128 is positive or negative.  In\nother aspects, the sentiment system 114 determines if the emotion for a topic 128 is positive, negative, or neutral.  The sentiment system 114 may outputs an emotion label 115 for each topic 128 that is representative of the emotions of the users 102 for\nthat topic based on the sentiment assigned to the sentences of the users that related to that topic.  Accordingly, the sentiment system 114 also assigns an emotion label that represents the emotion of a user for each input sentence of a user in a\nconversation.  The emotion label 115 may be assigned utilizing a simple heuristic rule so that a positive emotion for topic 128 receives a score or emotion label of 2, a neutral emotion for a topic 128 receives a score or label or 1, and a negative\nemotional label for a topic 128 receives an emotion label or score of -1.  As such, the emotion labels for each input sentence associated with a topic may be averaged to determine an appropriate emotion label for a given topic.  A topic 128 with an\nassigned emotion label 115 may be referred to herein as a labeled context sentence.  An input sentence with an assigned emotion label is referred to herein as a labeled sentence.  The sentiment system 114 identifies an emotion label 115 by utilizing one\nor more the following features: Word ngrams: unigrams and bigrams for words in the text input; Character ngrams: for each word in the text, character ngrams are extracted, for example, 4-grams and 5-grams may be utilized; Word skip-grams: for all the\ntrigrams and 4-grams in the text, one of the words is replaced by * to indicate the presence of non-contiguous words; Brown cluster ngrams: brown clusters are utilized to represent words (in text), and extract unigrams and bigrams as features;\nPart-of-speech (POS) tags: the presence or absence of part-of-speech tags are used as binary features; Lexicons: the English wordnet Sentiment Lexicon may be utilized; Social network related words: number (in text) of hashtags, emoticons, elongated\nwords, and punctuations are may also be utilized; and Word2vec cluster ngrams: Word2vec tool may be utilized to learn 100-dimensional word embedding from a social network dataset, next a K-means algorithm and L2 distance of word vectors is employed to\ncluster the million-level vocabulary into 200 classes that represent generalized words in the text.  A multiple class support vector machine (SVM) model is trained utilizing these features to determine the sentiment of each topic 128.  In some aspects,\nthe sentiment system 114 may also utilize sound-based sentiment analysis for any received recorded voice of the applicant to judge how positive the applicant is during a topic 128.\n The analysis systems 126 and the topic jumping system 146 collect the labeled topics 128 and/or the labeled sentences associated with one or more topics from the sentiment analysis system 114 as illustrated in FIG. 2A.  In some aspects, the\nresponse prediction system 116 also collects the labeled topics 128 and/or the labeled sentences associated with one or more topics from the sentiment analysis system 114 as illustrated in FIG. 2A.\n The analysis systems 126 include the user relationship system 140, the topic engagement system 142, and/or the user-topic system 144 as illustrated in FIG. 2B.  The analysis systems 126 each determine and score a specific feature 133 based on\nthe collected topics 128, collect labeled sentences associated with one or more topics, and/or the topic emotion labels 115.  In some aspects, each of the analysis systems 126 (the user relationship system 140, the topic engagement system 142, and/or the\nuser-topic system 144) output a score that utilize the same scale, such as 1-3, 1-5, 1-10, or etc. The use of the same scale for each of the analysis systems 126, allow for easier and/or more efficient comparisons between the different determinations\nfrom the different models of systems 140, 142, and 144.\n The user relationship system 140 determines the relationship between each set of two different users.  However, if only one user and the chat bot 100 are engaged in the conversation this relationship system 140 may be skipped or not performed. \nIn some aspects, the user relationship system 140 utilizes a learning algorithm or model.  As illustrated by FIG. 2B, the output of the user relationship system 140 may be an identified relationship 131 between a set of two different users 102 in\nconversation with the chat bot 100 and a score 141 of the closeness of this relationship 131.  For example, as illustrated by FIG. 2B, the user relationship detection model 140 may determine the relationship 131A between User1 129A and User3 129B and a\ncloseness score 141A for the relationship 131A.  In some aspects, possible relationships 131 include: [Family]: Parents, Sisters, brothers, Lover(s), children, other family relations; [Company]: leaders/managers, colleagues, co-workers; [Education]:\nSchool/college/university: professors, teachers, class-mates; [Habits]: Interesting groups: friends; and [Services]: serviceman/servicewoman for shopping, traveling, and so on.  It may be relatively difficult to build a machine learning model for the\nabove relationships since the training data is quite sparse and difficult to collect.  Accordingly, the relationship detection system 140 builds a heuristic rule based model to determine user relationships 131 by utilizing hint words.  For example, if\none user tells another use that \"our leader gave us a new task\", then the relationship between them is more like \"colleagues\" or \"co-workers\".  If the sentence is \"can you help repair that machine? It did not work for a long time\", then the relationship\nis more similar to \"consumer--serviceman\".  Accordingly, the relationship detection system 140 utilizes predetermined hand-made heuristic rules.\n Next, the relationship detection system 140 calculates the \"closeness\" score 141 between a set of two users based on social connection, agreement, and/or sentiment analysis of inputs provided between the two different users.  The social\nconnection refers to how close a determined relationship usually is between two people.  For example, immediate family may be ranked closer than extended family and friends may be ranked closer than work colleagues.  Agreement refers to how often the two\ndifferent users agree on opinions or topics.  For example, the more topics or opinions the two different users agree on, the higher their agreement score.  Lastly, the sentiment of the sentences between the two users is analyzed to determine if the user\nusually exchange more positive or more negative inputs with each other.  The more positive the inputs between the users in the conversation, the higher the sentiment score between the two users.  The higher the social connection, agreement, and/or\nsentiment scores between the two users, the higher the closeness score of the relationship between the two different users.  The lower the social connection, agreement, and/or sentiment scores between the two users, the lower the closeness score of the\nrelationship between the two different users.\n In some aspects, the closeness score 141 of the relationship 131 between the two user is determined utilizing the pre-defined equations of: S(user1,user2)=a1*accumulated topic engagement rates of shared topics/(accumulated engagement rates of\ntopics of user1+accumulated engagement rates of topics of user2)+a2*number of @ between them/number of @ in the group EQ#1; where, a1 and a2 are pre-defined weights for these two parts, the first part is a shared topic score and the second is the\ninteraction score by the hint of @ (where the @ symbol is utilized to cite/refer to another user).  When we sum up over different user2, we obtain the popular score of user1, that is: P(user1)=Sum over user2 of S(user1,user2)=.SIGMA..sub.user in the\ngroupS(user1,user) EQ#2.  For example, if User1 129A and User3 129B are based on the conversation displayed in FIG. 2B, the determined relationship 131A for User1 129A and User 3 129B may be identified as friends with a high closeness score 141 based on\nfriends being a close relationship, the positive sentiment between the User #1 and User#3 (\"I love drinking with User1\"), the frequency with which User 1 and User 3 converse (3 interactions), and that both User 1 and User 3 agree that happy hour should\nbe attended.\n The topic engagement system 142 determines a score for each topic that represents how often and how recently a topic is discussed or engaged in by the one or more user in the conversation with the chat bot 100.  In some aspects, the topic\nengagement system 142 utilizes a learning algorithm or model.  As illustrated by FIG. 2B, the output of the topic engagement system 142 may be an engagement rate score 142 for each identified topic 128.  The engagement rate of one topic ranges over all\nthe users in a group chat.  The topic engagement system 142 links each determined topic to any user engaged in (or provides input that relates to) that determined topic and scores the determined topic based on the number of engaged users or users linked\nto the topic, frequency of the topic (how many times the topic is discussed in the conversation), timing of the topic (or how recently the topic was discussed by one or more users), and/or the emotion label 115 of the topic.  The higher the number of\nengaged users, the higher the frequency, the more recent the timing, and the more positive the emotion label 115 for the topic, the higher the topic engagement score is for the topic.  The lower the number of engaged users, the lower the frequency, the\nless recent the timing, and the less positive the emotion label 115 for the topic, the lower the topic engagement score is for the topic.  For example, the topic engagement system 142 may score an identified topic of Happy Hour from the conversation\nshown in FIG. 2B high because each user in the conversation (User1, User2, User3, and User4) is engaged in the conversation, because User1, User3, and User4 had a positive sentiment to the happy hour topic, and because the Happy Hour topic was discussed\nwithin the last five minutes.\n In some aspects, the topic engagement system 142 utilizes an adapted time-delay model that expresses the engagement rate of one topic.  Accordingly, one appearance of the topic (related sentence from some user) will contribute (improve) the\nengagement rate of the topic, yet this engagement rate attenuates or decreases as time passes or gets farther from the current time.  For example, for one topic, if it is introduced 5 minutes before, and after that no one else responds or comments on\nthis topic, then this topic's engagement rate continues to reduce as time passes.  Alternatively in this example, if in the first minute, two users followed respond or comment on this topic, then the engagement rate is improved, yet this improvement rate\ntends to be weaker as time passes.  Further, in this example, the more users that send comments relating to this topic that continue appear during 5 minutes, then the engagement rate of the topic continues increases.  In some aspects, the topic\nengagement system 142 utilizes the following equations to compute the engagement rate for one topic:\n .times..times..function..function..lamda..function..times..times..gtoreq.- .times..times.&lt;.times..times.  ##EQU00001## where function k computes the importance of one appearance of one topic (as the importance will be reduced as time goes);\nwhere .lamda.  is the delay constant; when .lamda.  is 0, we have k(t, t.sub.o)=1, which means that the topic was commented on within 5 minutes ago.  At current time point t, and suppose the history H is H={(w.sub.1, t.sub.1), .  . . , (w.sub.n,\nt.sub.n)} where w.sub.1 to w.sub.n are the topics (allow to be duplicated) of time point from t.sub.1 to t.sub.n.  That is, at time point t.sub.1, there is one sentence in the individual/group chat that includes topic w.sub.1, .  . . , at time point\nt.sub.n, there is one sentence in the individual/group chat that includes topic w.sub.n.\n .times..times..function..times..delta..function..times..function..times..- times.  ##EQU00002## Then, we compute the accumulated \"engagement rate\" of a topic w with equation #4 above.  Here, .delta.(w,wi) is the delta function: if w=wi then\n.delta.(w,wi)=1; otherwise .delta.(w,wi)=0.  3.  Thus, given a history dialog session, topic w's engagement rate at \"current\" time point t is:\n .function..function..function..function..mu.'.times..function.'.mu..times- ..times.  ##EQU00003## where, 0.ltoreq..mu..ltoreq.1 is Lidston smoothing parameter; when .mu.  is 0, this equation is similar to Maximum Likelihood Estimation (MLE).\n The user-topic model 144 determines each user's interest in any identified topic.  The user-topic model 144 may determines each user's interest based on introduction or following of a topic, engagement frequency in a topic, and/or sentiment\nanalysis of the user to the topic.  In some aspects, the user-topic model 144 utilizes a learning algorithm or model.  As illustrated by FIG. 2B, the output of the user-topic model 144 may be the score 145 of each user's 102 interest in each identified\ntopic 128.  As such, the user-topic model 144 links each user to each identified topic and then may scores each user's interest in the topic utilizing engagement frequency in the topic and each user's sentiment for that topic.  For example, the\nuser-topic model 144 determines that User1's 129A interest in topic1 128A is represented by score 145A.  Each topic has only one \"introducer\" who mentions the topic the first time in the group chat.  Also, each topic has no less than one \"follower\" who\nexpresses positive/negative/neutral opinions (also referred to herein as sentiment) to this topic.  For \"introducer\", the user-topic detection system 112 may score the introducer as alpha and score the \"follower\" as beta with a sentiment analysis weight\nof w. For example, in some aspects, the following equation may be utilized by the user-topic model 144: f(user, topic)=alpha if user is the \"introducer\"; and =w*beta if user is the \"follower\".  EQ#6 In some aspects, the user-topic model 144 scores the\nalpha as 2, the beta as 1 and the w as 1 when the sentiment analysis model returns a positive label, the w as 0.1 when the sentiment analysis model returns a negative label, and the w as 0.5 when the sentiment model 114 returns a neutral label.\n For example, based on the conversation shown in FIG. 3A, if topic 1 128A is \"happy hour\", User1 129A may have a high interest score in Topic 1 128A of happy hour because User 1 introduced the topic, repeatedly engaged in the topic, and expressed\na positive sentiment relating to the topic.  However, in this same example, based on the conversation shown in FIG. 3A, the user-topic model 144 may determine that User2 has a lower interest score in topic 1 128A of \"happy hour\" since User 2 is a\nfollower that only discussed the happy hour topic once and expressed a negative sentiment with regards to the happy hour topic (\"Bad for work attendance\").\n The topic jumping system 146 determines a relationship between each determined topic.  As discussed above, the topic jumping system 146 collects the topics, input sentences, labeled input sentences, and/or the labeled topics.  The topic jumping\nsystem 146 construct a knowledge graph 135 of topic keywords and the relationship between every different topic based on the inputs or labeled inputs in the conversation (or the collection, the topic keyword or labeled topic keyword, and/or world\nknowledge 118.  The world knowledge 118 as utilized herein refers to any information, attributes, relationships, and/or already formed relationship knowledge graphs that are accessible to the topic jumping system 146 via a network 113.  The topic jumping\nsystem 146 may utilized the world knowledge to enrich, to assign attributes, and/or find relationships between different topics or topic keywords.  Further, the topic jumping model 146 may utilize random walk to help track jumping from one existing topic\nto some new topic (which is not included in the dialog session yet).  The topic jumping system 146 supplies users' requirements of \"freshness\" or relevancy for any given topic.  For example, given the following sentence, \"/today /'s /drinking party\n/Ooimachi /'s /My Yakitori /named /restaurant /takes place at,\" the syntactic dependency tree determined by the topic jumping system 146 is as follows:\n TABLE-US-00001 -D today's ---------D happy hour ---D | Ooimachi (a subway station) `s -D | My Yakitori -D | named -D restaurant takes place at\n FIG. 10 illustrates an example of a graph of the dependency tree listed above for the example sentence, \"/today /'s /happy hour /Ooimachi /'s /My Yakitori /named /restaurant /takes place at.\"\n This dependency tree informs the topic jumping system 146 that the place and restaurant are two attributes for the topic word of \"happy hour\".  Furthermore, topic jumping system 146 may determine that these attributes are further topics.  Thus,\nthrough this dependency tree, the topic jumping system 146 extracts or constructs a knowledge graph 135 as illustrated in FIG. 11.  FIG. 11 illustrates an example knowledge graph 135 created by the topic jumping system 146 for the conversation between\nmultiple user and the chat bot 100 shown in FIG. 3A.  The transfer probability between any two topics in the knowledge graph 135 is determined or calculated by the topic jumping system 146 based on how many sentences that include both two topics and\ntheir dependency directions.  The more sentences that support two different topics connections, the higher probability that users will or can jump between the two different topics following the dependency direction.  In some aspects, the topic jumping\nsystem 146 may utilizes these probabilities or scores to determine the weight for a random walk from one topic to some novel topic in the group chat (which is supported by the pre-constructed knowledge graph 135).\n The response prediction system 116 collects the determined topics 128, associated emotion labels 115, scored features 133, labeled user input sentences associated with one or more topics, and/or the created topic knowledge graph 135.  In some\naspects, the user input 130 includes a query 127.  In some aspects, the response prediction system 116 collects input 130 from the client computing devices.  In other aspects, response prediction system 116 collects the labeled user input sentences,\ntopics 128, topic associated emotion labels 115, user labels 129, and scored features 133 and/or the topic knowledge graph 135 from the sentiment system 114, the topic detection system 112, the analysis systems 126, and/or the topic jumping system 146.\n The response prediction system 116 determines if one or more responses 132 should be predicted based on the feature scores 133.  The response prediction system 116 compares each topic and the topics associated features scores 133 to a relevancy\nthreshold.  In some aspects, the response prediction system 116 utilizes the features scores to determine or calculate a relevancy score for a given topic.  If the feature scores 133 or relevancy score for a given topic meet the relevancy threshold, the\nresponse prediction system 116 determines that response should be provided to conversation.  In response to the determination to provide a response, the response prediction system 116 predicts a response based on the labeled inputs associated with the\ntopic, such as a query, the labeled topic, the topic knowledge graph 135, scored features 133, and/or world knowledge 118.  If the feature scores 133 or relevancy do not meet the relevancy threshold, the response prediction system 116 determines that a\nresponse should not be predicted for the conversation by the chat bot 100.  In some aspects, the relevancy threshold includes a minimum combined or average feature score 133 between the three different analysis models 126.  In some aspects, the response\nprediction system 116 automatically determines that a topic meets a relevancy threshold, when the topic is associated with a user input that is a query 127.  As such, the chat bot 100 may provide a response to the conversation with the one or more users\nin response to a user query 127 and/or spontaneously during the conversation when no response was requested by any of the users in the conversation.\n In some aspects, the response prediction system 116 ranks each determined topic based on each topic's scored features, any associated query, and/or a topic's emotion label.  In these aspects, the ranking of the topics is comparted to the\nrelevant threshold.  If the topic's ranking meets the relevancy threshold, then a response is predicted by the response prediction system 116.  In these aspects, if the topic's ranking does not meet the relevancy threshold, then a response is not\npredicted by the response prediction system 116.\n As discussed above, the response prediction system 116 predicts one or more responses 132 in response to a topic meeting the relevancy threshold.  The response 132 may be a comment, a summary, a question, and/or any other suitable output for\nconversing with one or more users 102.  The response may be predicted by the response prediction system 116 based on a query if present, relevant topic, the emotion label associated with the relevant topic, labeled input sentences associated with the\nrelevant topic, scored features, and/or the topic knowledge graph 135.\n In some aspects, the response prediction system 116 utilizes a response ranking model to predict the response.  In some aspects, the ranking model is a learning-to-rank (LTR) architecture of pairwise learning for constructing the relevance-based\nresponse ranking model (for specific topic and/or specific user(s)).  The ranking model may rank available response texts to some specific user(s) under a specific query and/or some specific topic.  The response prediction system 116 trains a gradient\nboosting decision tree (GBDT) for the ranking task.  The ranking model utilizes the scored features 133 and the topic knowledge graph 135 along the following items, to predict a response: Word ngrams: unigrams and bigrams for words in the text input;\nCharacter ngrams: for each word in the text, character ngrams are extracted, for example, 4-grams and 5-grams may be utilized; Word skip-grams: for all the trigrams and 4-grams in the text, one of the words is replaced by * to indicate the presence of\nnon-contiguous words; Brown cluster ngrams: brown clusters are utilized to represent words (in text), and extract unigrams and bigrams as features; Part-of-speech (POS) tags: the presence or absence of part-of-speech tags are used as binary features;\nLexicons: the English wordnet Sentiment Lexicon may be utilized; Social network related words: number (in text) of hashtags, emoticons, elongated words, and punctuations are may also be utilized; and Word2vec cluster ngrams: Word2vec tool may be utilized\nto learn 100-dimensional word embedding from a social network dataset, next a K-means algorithm and L2 distance of word vectors is employed to cluster the million-level vocabulary into 200 classes that represent generalized words in the text.  As\ndiscussed above, the core worker 111 may collect the one or more responses from the response prediction system 116 and reconfigure the response if necessary before providing the response to the conversation.\n In some aspects, the response prediction system 116 may select a response that summarizes the topic that meets the relevancy threshold.  For example, FIG. 3B illustrates a chat bot summary of the conversation shown in FIG. 3A in a conversation\nbetween User5 and the chat bot 100 (or Rinna).  In further aspects, the response prediction system 116 may perform a requested action and/or provide a requested response from a user query, such as the retrieval of information or the making of a dinner\nreservation.  In other aspects, the response selected by the response prediction system 116 may be directed to an entirely new topic that was not previously discussed.  For example, as illustrated in FIG. 3A Rinna (or chat bot 100) provides the response\n\"Rinna also wants to join! Should I like the happy hour?\" In this example, the chat bot 100 did not receive a user query, but responded spontaneously to the conversation since the topic of happy hour met a relevancy threshold.  Similarly, as illustrated\nin FIG. 3A, Rinna (or chat bot 100) provides the response, \"[smiley face], here are the directions from Shinagawa to Ooimachi.\", even though, none of the user requested or asked for directions from Shinagawa to Ooimachi.  Further, Rinna spontaneously\nintroduced a new topic of \"liking\" on social media the proposed happy hour topic as illustrated in FIG. 3A, and spontaneously introduced a new topic of favorite fried chicken restaurants as illustrated in FIG. 3B, even though, social media comments and\nfried chicken restaurants were not previously discussed by any of the users in the conversations.  Also, as illustrated in FIG. 3A, Rinna (or chat bot 100) provides the response \"show map of the restaurant; cost estimate: $40/person; at 18:00\" in\nresponse to a user query of \"Rinna did you record?\" Additionally, as illustrated in FIG. 3A Rinna (or chat bot 100) provides the response \"Drinking party! Currently we have two attendees\" in response to a user question to the entire group of the\nconversation and not necessarily directed to the chat bot 100.\n The chat bot 100 also includes a feedback system 119.  The feedback system 119 utilizes user feedback and/or world feedback 122 to train or update one or more learning algorithms and/or models utilized by the topic detection system 112, the\nsentiment system 114, and/or the response prediction system 116.  In some aspects, the feedback system 119 utilizes user feedback and/or world feedback 122 to train or updated the one or more learning algorithms and/or leaning models utilized by the user\nrelationship system 140, the topic engagement system 142, the user-topic system 144, and/or the topic jumping system 146.\n In some aspects, the feedback system 119 collects world feedback 122 via a network 113.  The world feedback 122 may include queries and corresponding responses from other users of the chat bot 100 that can be utilized as positive or negative\ntraining data.\n In other aspects, the feedback system 119 collects user answers from one or more users 102 in reply to a previously provided response by the chat bot 100 in a conversation.  The feedback system 119 analyzes the answer to determine user feedback\nfor the response.  The feedback system 119 utilizes the determined user feedback as positive or negative training data.  In some aspects, the user feedback determined by the feedback system 119 for the result response is based at least on the emotion\nlabel of the answer.\n FIG. 4 illustrates a flow diagram conceptually illustrating an example of a method 400 for multiple topic intelligent automated chatting.  In some aspects, method 400 is performed by an application, such as the chat bot 100 described above. \nMethod 400 provides multiple topic intelligent automated chatting by analyzing the inputs in a conversation to determine topics in the conversation, to assign emotion labels to topics and inputs in the conversation, create a knowledge graph of the\ntopics, and to assign feature scores.  Method 400 determines and scores the relationship between each set of two different user, determines and scores topic engagement, and determines and scores user-topic interest.  Based on these feature scores, method\n400 determines whether or not to provide a spontaneous or a requested response to the conversation.  If a response is provided, method 400 predicts one or more responses utilizing knowledge graph created by method 400, labeled input sentences associated\nwith a topic, scored features, world knowledge, and/or a labeled topic.  As such, method 400 performs multiple topic automated chatting that is more effective, more engaging, easier to use, and more lifelike than previously utilized automated chat\nmethods that were not able to track and respond to different topics in a multiple topic conversation between one or more uses.\n Method 400 starts at operation 402.  At operation 402, one or more user inputs are collected to form a collection.  In some aspects, any user inputs in the conversation are collected.  In other aspects, only user inputs for a session of a\nconversation are collected to form a collection at operation 402.  The user inputs may be provided in one or more different modalities, such as video, voice, images, and/or texts.  In some aspects, at operation 402 an input is processed or converted into\ntext.  In some aspects, a LU system with one or more different APIs is utilized to convert the received user inputs into text and/or annotated text.  In some aspects, the user input includes a user query.\n In some aspects, method 400 includes operations 404 and 406.  At operation 404, a determination is made whether or not any world feedback and/or user feedback has been collected.  At operation 404, user feedback may be collected from user inputs\nand world feedback may be collected via a network.  If no feedback has been collected at operation 404, then operation 408 is performed.  If feedback has been collected at operation 404, then operation 406 is performed.  At operation 406, the feedback is\nsent to one or more learning models or algorithms utilized by method 400 to update or train those models or algorithms based on the feedback.\n At operation 408, the user inputs are analyzed to determine one or more topics in the conversation.  The conversation is between one or more users and the automated chatting method 400.  In some aspects, a syntactic dependency parser is utilized\nto parse sentences in the conversation and pick noun words/phrases that are topic candidates at operation 408.\n After the performance of operation 408, operation 410 is performed.  At operation 410, an emotion label is assigned to every sentence in the one or more input sentences.  At operation 410 each sentence in the input collection is semantically\nevaluated or analyzed.  In some aspects, voice data and/or text data from each user's input sentences are evaluated to determine the emotion of the user during the input sentence.  In further aspects, each sentence is analyzed utilizing a multiple class\nvector support machine at operation 410.  Next, at operation 410, an emotion label for each sentence is identified and assigned to form labeled sentences.  Each input sentence is associated with one or more topic based on operation 408.  As such each\nlabeled sentence is associated with one or more topics.  In some aspects, the emotion label is positive or negative.  In other aspects, the emotion label is positive, negative, or neutral.  In further aspects, a user's text input is evaluated utilizing a\nmultiple class vector support machine trained utilizing word ngrams, character ngrams, word skip-grams, brown cluster ngrams, part-of-speech tags, lexicons, social network related words, and/or word2vec cluster ngrams to identify the emotion label for\neach sentence at operation 410.\n Further, at operation 410 a sentiment or an emotion label is assigned to each topic.  At operation 410, the labeled input sentences for each topic are collected and analyzed.  Based on the emotion labels for the input sentences associated with\neach topic, each topic may be assigned an emotion label or sentiment at operation 410.  For example, the emotion labels for each input sentence associated with each topic may be averaged to determine the appropriate emotion label or sentiment for each\ntopic.\n At operation 412, a knowledge graph between topics identified at operation 412 is created that graphs relationships between the identified topics and/or labeled topic.  In some aspects, the knowledge graph of the topics is created based on the\ninputs and/or labeled inputs in the conversation (or the collection) and/or world knowledge 118 at operation 412.  In further aspects at operation 412, a dependency tree may be created from the one or more inputs that is enriched utilizing world\nknowledge.  As such, the dependency tree assigns attributes to the one or more topics.  In these aspects, the knowledge graph may be extracted and/or constructed from the dependency tree at operation 412.\n After the performance of operation 410, operation 414, 416, and/or 418 are performed.  At operation 414 a closeness of the relationship each set of two different user is scored.  However, if the there is only one user in the conversation,\noperation 410 may be skipped or not performed.  First, at operation 410, a relationship between each set of two different users is identified.  The closeness of each determined relationship may be determined at operation 410 based on social connection,\nagreement, and sentiment analysis to form a scored first feature or a scored relationship closeness.\n At operation 416 a user's interest in each topic is sored to form a scored second feature or a scored user-topic interest.  The user's interest in each topic is determined or scored based on the user's sentiment toward each topic and engagement\nfrequency in each topic at operation 416.  In further aspects, the user interest in each topic is further determined or scored based on whether the user introduced the topic or commented on the topic after introduction by another user.\n At operation 418 an engagement rate for each topic is scored to form a scored third feature or scored topic engagement rate.  In some aspects, the score for the engagement rate for a topic is determined based on the number of users engaged in a\ntopic, frequency of the topic in the conversation, timing of the topic, and the sentiment of the users toward the topic who engaged in the topic;\n In some aspects, method 400 includes operation 420.  At operation 420, the scored features are collected and evaluated to determine a relevancy score of each topic.  The higher the relevancy score of a topic, the more relevant the topic is to\nthe conversation.  In some aspects, each of the different feature scores utilizes the same scale to provide an easier comparison between the different features.  In other aspects, each feature may be weighed.  In these aspect, the score of the feature\nwith higher weights will be given more consideration in the relevancy score calculation than the lower weighted features.  In some aspects at operation 420, each relationship for a set of two different users is linked with any topic the set of two\ndifferent users discussed between each other.  In some aspects at operation 420, the closeness score of the relationships, any user interest scores for that topic, and the engagement rate of the topic are utilized to calculate the relevancy score for a\ngiven topic.  In some aspects, the feature scores are averaged or totaled to determine a relevancy score for a given topic at operation 420.  In other aspects, the relevancy scores of each topic is utilized to rank each identified topic.  Topics with\nhigher relevancy scores are ranked above topics with lower relevancy scores.  In further aspects, topics with an associated user query may have an increased relevancy score.\n Method 400 includes operation 422.  At operation 422 a determination of whether any topic meets a relevancy threshold is made.  At operation 422, if operation 420 is performed, a relevancy score for each feature is compared to the relevancy\nthreshold.  At operation 422, if method 400 does not include operation 420, the feature scores associated with each topic may each be compared to the relevancy threshold individually or in combination.  If operation 422 determines that the relevancy\nthreshold has been met, then operation 424 is performed.  If operation 422 determines that the relevancy threshold has not been met, then method 400 may end until new input is collected at operation 402 to restart the performance of method 400.\n At operation 424, one or more responses are predicted or determined based on the labeled input sentence for the relevant topic, the knowledge graph, and/or the labeled relevant topic.  In some aspects, method 424 is performed by the response\nprediction system 116 as disclosed above.  In further aspects, method 424 is performed utilizing a LTR architecture of pairwise learning for constructing a relevance-based response ranking model (for specific topic and/or specific user(s)).  In some\naspects, the response is directed to a new topic.  In other aspects, the response replies to a user query.  In further aspects, the response may be a summary of a topic in the conversation.  In yet other aspects, the response may be or include an action\nperformance, such as the booking of a reservation and the retrieval of information.\n In response to the one or more responses being predicted at operation 424, operation 426 is performed.  At operation 426 the one or more response are provided to the conversation.  In some aspects, the one or more responses are provided by a\nclient computing device and/or an application to the conversation at operation 426.  In other aspects, instructions are sent to the client computing device to provide the one or more responses to the conversation at operation 426.  The client computing\ndevice provides the one or more responses utilizing any known electronic communication modality and may be in the form of a visual, audio, tactile, and/or other sensory mechanisms at operation 426.  For example, the client computing device may provide\nthe one or more responses to an application hosting the conversation.  After operation 426, method 400 ends until method 400 collects new input at operation 402 to restart the performance of method 400.\n FIGS. 5-8 and the associated descriptions provide a discussion of a variety of operating environments in which aspects of the disclosure may be practiced.  However, the devices and systems illustrated and discussed with respect to FIGS. 5-8 are\nfor purposes of example and illustration and are not limiting of a vast number of computing device configurations that may be utilized for practicing aspects of the disclosure, described herein.\n FIG. 5 is a block diagram illustrating physical components (e.g., hardware) of a computing device 500 with which aspects of the disclosure may be practiced.  For example, the multiple topic AI chat bot 100 could be implemented by the computing\ndevice 500.  In some aspects, the computing device 500 is a mobile telephone, a smart phone, a tablet, a phablet, a smart watch, a wearable computer, a personal computer, a desktop computer, a gaming system, a laptop computer, and/or etc. The computing\ndevice components described below may include computer executable instructions for the chat bot 100 that can be executed to employ method 400.  In a basic configuration, the computing device 500 may include at least one processing unit 502 and a system\nmemory 504.  Depending on the configuration and type of computing device, the system memory 504 may comprise, but is not limited to, volatile storage (e.g., random access memory), non-volatile storage (e.g., read-only memory), flash memory, or any\ncombined of such memories.  The system memory 504 may include an operating system 505 and one or more program modules 506 suitable for running software applications 520.  The operating system 505, for example, may be suitable for controlling the\noperation of the computing device 500.  Furthermore, aspects of the disclosure may be practiced in conjunction with a graphics library, other operating systems, or any other application program and is not limited to any particular application or system. \nThis basic configuration is illustrated in FIG. 5 by those components within a dashed line 508.  The computing device 500 may have additional features or functionality.  For example, the computing device 500 may also include additional data storage\ndevices (removable and/or non-removable) such as, for example, magnetic disks, optical disks, or tape.  Such additional storage is illustrated in FIG. 5 by a removable storage device 509 and a non-removable storage device 510.\n As stated above, a number of program modules and data files may be stored in the system memory 504.  While executing on the processing unit 502, the program modules 506 (e.g., LU system 110, topic detection system 112, sentiment system 114, core\nworker 111, feedback system 119, and/or the response prediction system 116) may perform processes including, but not limited to, performing method 400 as described herein.  For example, the processing unit 502 may implement the chat bot 100, including\nthe LU system 110, topic detection system 112, sentiment system 114, response prediction system 116, core worker 111, and/or the feedback system 119.  Other program modules that may be used in accordance with aspects of the present disclosure, and in\nparticular to generate screen content, may include a digital assistant application, a voice recognition application, an email application, a social networking application, a collaboration application, an enterprise management application, a messaging\napplication, a word processing application, a spreadsheet application, a database application, a presentation application, a contacts application, a gaming application, an e-commerce application, an e-business application, a transactional application,\nexchange application, a device control application, a web interface application, a calendaring application, etc. In some aspect, the chat bot 100 allows a user to interact with in one or more of the above referenced applications in more effective, more\nefficient, and improved manner.\n Furthermore, aspects of the disclosure may be practiced in an electrical circuit comprising discrete electronic elements, packaged or integrated electronic chips containing logic gates, a circuit utilizing a microprocessor, or on a single chip\ncontaining electronic elements or microprocessors.  For example, aspects of the disclosure may be practiced via a system-on-a-chip (SOC) where each or many of the components illustrated in FIG. 5 may be integrated onto a single integrated circuit.  Such\nan SOC device may include one or more processing units, graphics units, communications units, system virtualization units and various application functionality all of which are integrated (or \"burned\") onto the chip substrate as a single integrated\ncircuit.  When operating via an SOC, the functionality, described herein, with respect to the capability of client to switch protocols may be operated via application-specific logic integrated with other components of the computing device 500 on the\nsingle integrated circuit (chip).\n Aspects of the disclosure may also be practiced using other technologies capable of performing logical operations such as, for example, AND, OR, and NOT, including but not limited to mechanical, optical, fluidic, and quantum technologies.  In\naddition, aspects of the disclosure may be practiced within a general purpose computer or in any other circuits or systems.\n The computing device 500 may also have one or more input device(s) 512 such as a keyboard, a mouse, a pen, a microphone or other sound or voice input device, a touch or swipe input device, etc. The output device(s) 514 such as a display,\nspeakers, a printer, etc. may also be included.  The aforementioned devices are examples and others may be used.  The computing device 500 may include one or more communication connections 516 allowing communications with other computing devices 550. \nExamples of suitable communication connections 516 include, but are not limited to, RF transmitter, receiver, and/or transceiver circuitry, universal serial bus (USB), parallel, and/or serial ports.\n The term computer readable media or storage media as used herein may include computer storage media.  Computer storage media may include volatile and nonvolatile, removable and non-removable media implemented in any method or technology for\nstorage of information, such as computer readable instructions, data structures, or program modules.  The system memory 504, the removable storage device 509, and the non-removable storage device 510 are all computer storage media examples (e.g., memory\nstorage).  Computer storage media may include RAM, ROM, electrically erasable read-only memory (EEPROM), flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic\ndisk storage or other magnetic storage devices, or any other article of manufacture which can be used to store information and which can be accessed by the computing device 500.  Any such computer storage media may be part of the computing device 500. \nComputer storage media does not include a carrier wave or other propagated or modulated data signal.\n Communication media may be embodied by computer readable instructions, data structures, program modules, or other data in a modulated data signal, such as a carrier wave or other transport mechanism, and includes any information delivery media. \nThe term \"modulated data signal\" may describe a signal that has one or more characteristics set or changed in such a manner as to encode information in the signal.  By way of example, and not limitation, communication media may include wired media such\nas a wired network or direct-wired connection, and wireless media such as acoustic, radio frequency (RF), infrared, and other wireless media.\n FIGS. 6A and 6B illustrate a mobile computing device 600, for example, a mobile telephone, a smart phone, a tablet, a phablet, a smart watch, a wearable computer, a personal computer, a desktop computer, a gaming system, a laptop computer, or\nthe like, with which aspects of the disclosure may be practiced.  With reference to FIG. 6A, one aspect of a mobile computing device 600 suitable for implementing the aspects is illustrated.  In a basic configuration, the mobile computing device 600 is a\nhandheld computer having both input elements and output elements.  The mobile computing device 600 typically includes a display 605 and one or more input buttons 610 that allow the user to enter information into the mobile computing device 600.  The\ndisplay 605 of the mobile computing device 600 may also function as an input device (e.g., a touch screen display).\n If included, an optional side input element 615 allows further user input.  The side input element 615 may be a rotary switch, a button, or any other type of manual input element.  In alternative aspects, mobile computing device 600 may\nincorporate more or less input elements.  For example, the display 605 may not be a touch screen in some aspects.  In yet another alternative aspect, the mobile computing device 600 is a portable phone system, such as a cellular phone.  The mobile\ncomputing device 600 may also include an optional keypad 635.  Optional keypad 635 may be a physical keypad or a \"soft\" keypad generated on the touch screen display.\n In addition to, or in place of a touch screen input device associated with the display 605 and/or the keypad 635, a Natural User Interface (NUI) may be incorporated in the mobile computing device 600.  As used herein, a NUI includes as any\ninterface technology that enables a user to interact with a device in a \"natural\" manner, free from artificial constraints imposed by input devices such as mice, keyboards, remote controls, and the like.  Examples of NUI methods include those relying on\nspeech recognition, touch and stylus recognition, gesture recognition both on screen and adjacent to the screen, air gestures, head and eye tracking, voice and speech, vision, touch, gestures, and machine intelligence.\n In various aspects, the output elements include the display 605 for showing a graphical user interface (GUI).  In aspects disclosed herein, the various user information collections could be displayed on the display 605.  Further output elements\nmay include a visual indicator 620 (e.g., a light emitting diode), and/or an audio transducer 625 (e.g., a speaker).  In some aspects, the mobile computing device 600 incorporates a vibration transducer for providing the user with tactile feedback.  In\nyet another aspect, the mobile computing device 600 incorporates input and/or output ports, such as an audio input (e.g., a microphone jack), an audio output (e.g., a headphone jack), and a video output (e.g., a HDMI port) for sending signals to or\nreceiving signals from an external device.\n FIG. 6B is a block diagram illustrating the architecture of one aspect of a mobile computing device.  That is, the mobile computing device 600 can incorporate a system (e.g., an architecture) 602 to implement some aspects.  In one aspect, the\nsystem 602 is implemented as a \"smart phone\" capable of running one or more applications (e.g., browser, e-mail, calendaring, contact managers, messaging clients, games, and media clients/players).  In some aspects, the system 602 is integrated as a\ncomputing device, such as an integrated personal digital assistant (PDA) and wireless phone.\n One or more application programs 666 and/or the chat bot 100 run on or in association with the operating system 664.  Examples of the application programs include phone dialer programs, e-mail programs, personal information management (PIM)\nprograms, word processing programs, spreadsheet programs, Internet browser programs, messaging programs, and so forth.  The system 602 also includes a non-volatile storage area 668 within the memory 662.  The non-volatile storage area 668 may be used to\nstore persistent information that should not be lost if the system 602 is powered down.  The application programs 666 may use and store information in the non-volatile storage area 668, such as e-mail or other messages used by an e-mail application, and\nthe like.  A synchronization application (not shown) also resides on the system 602 and is programmed to interact with a corresponding synchronization application resident on a host computer to keep the information stored in the non-volatile storage area\n668 synchronized with corresponding information stored at the host computer.  As should be appreciated, other applications may be loaded into the memory 662 and run on the mobile computing device 600.\n The system 602 has a power supply 670, which may be implemented as one or more batteries.  The power supply 670 might further include an external power source, such as an AC adapter or a powered docking cradle that supplements or recharges the\nbatteries.\n The system 602 may also include a radio 672 that performs the function of transmitting and receiving radio frequency communications.  The radio 672 facilitates wireless connectivity between the system 602 and the \"outside world,\" via a\ncommunications carrier or service provider.  Transmissions to and from the radio 672 are conducted under control of the operating system 664.  In other words, communications received by the radio 672 may be disseminated to the application programs 666\nvia the operating system 664, and vice versa.\n The visual indicator 620 may be used to provide visual notifications, and/or an audio interface 674 may be used for producing audible notifications via the audio transducer 625.  In the illustrated aspect, the visual indicator 620 is a light\nemitting diode (LED) and the audio transducer 625 is a speaker.  These devices may be directly coupled to the power supply 670 so that when activated, they remain on for a duration dictated by the notification mechanism even though the processor 660 and\nother components might shut down for conserving battery power.  The LED may be programmed to remain on indefinitely until the user takes action to indicate the powered-on status of the device.  The audio interface 674 is used to provide audible signals\nto and receive audible signals from the user.  For example, in addition to being coupled to the audio transducer 625, the audio interface 674 may also be coupled to a microphone to receive audible input.  The system 602 may further include a video\ninterface 676 that enables an operation of an on-board camera 630 to record still images, video stream, and the like.\n A mobile computing device 600 implementing the system 602 may have additional features or functionality.  For example, the mobile computing device 600 may also include additional data storage devices (removable and/or non-removable) such as,\nmagnetic disks, optical disks, or tape.  Such additional storage is illustrated in FIG. 6B by the non-volatile storage area 668.\n Data/information generated or captured by the mobile computing device 600 and stored via the system 602 may be stored locally on the mobile computing device 600, as described above, or the data may be stored on any number of storage media that\nmay be accessed by the device via the radio 672 or via a wired connection between the mobile computing device 600 and a separate computing device associated with the mobile computing device 600, for example, a server computer in a distributed computing\nnetwork, such as the Internet.  As should be appreciated such data/information may be accessed via the mobile computing device 600 via the radio 672 or via a distributed computing network.  Similarly, such data/information may be readily transferred\nbetween computing devices for storage and use according to well-known data/information transfer and storage means, including electronic mail and collaborative data/information sharing systems.\n FIG. 7 illustrates one aspect of the architecture of a system for processing data received at a computing system from a remote source, such as a general computing device 704, tablet 706, or mobile device 708, as described above.  Content\ndisplayed and/or utilized at server device 702 may be stored in different communication channels or other storage types.  For example, various documents may be stored using a directory service 722, a web portal 724, a mailbox service 726, an instant\nmessaging store 728, and/or a social networking site 730.  By way of example, the chat bot may be implemented in a general computing device 704, a tablet computing device 706 and/or a mobile computing device 708 (e.g., a smart phone).  In some aspects,\nthe server 702 is configured to implement a chat bot 100, via the network 715 as illustrated in FIG. 7.\n FIG. 8 illustrates an exemplary tablet computing device 800 that may execute one or more aspects disclosed herein.  In addition, the aspects and functionalities described herein may operate over distributed systems (e.g., cloud-based computing\nsystems), where application functionality, memory, data storage and retrieval and various processing functions may be operated remotely from each other over a distributed computing network, such as the Internet or an intranet.  User interfaces and\ninformation of various types may be displayed via on-board computing device displays or via remote display units associated with one or more computing devices.  For example user interfaces and information of various types may be displayed and interacted\nwith on a wall surface onto which user interfaces and information of various types are projected.  Interaction with the multitude of computing systems with which aspects of the invention may be practiced include, keystroke entry, touch screen entry,\nvoice or other audio entry, gesture entry where an associated computing device is equipped with detection (e.g., camera) functionality for capturing and interpreting user gestures for controlling the functionality of the computing device, and the like.\n Embodiments of the present disclosure, for example, are described above with reference to block diagrams and/or operational illustrations of methods, systems, and computer program products according to aspects of the disclosure.  The\nfunctions/acts noted in the blocks may occur out of the order as shown in any flowchart.  For example, two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order,\ndepending upon the functionality/acts involved.\n This disclosure described some embodiments of the present technology with reference to the accompanying drawings, in which only some of the possible aspects were described.  Other aspects can, however, be embodied in many different forms and the\nspecific embodiments disclosed herein should not be construed as limited to the various aspects of the disclosure set forth herein.  Rather, these exemplary aspects were provided so that this disclosure was thorough and complete and fully conveyed the\nscope of the other possible aspects to those skilled in the art.  For example, aspects of the various embodiments disclosed herein may be modified and/or combined without departing from the scope of this disclosure.\n Although specific aspects were described herein, the scope of the technology is not limited to those specific aspects.  One skilled in the art will recognize other aspects or improvements that are within the scope and spirit of the present\ntechnology.  Therefore, the specific structure, acts, or media are disclosed only as illustrative aspects.  The scope of the technology is defined by the following claims and any equivalents therein.", "application_number": "15404932", "abstract": " Systems and methods for multiple topic automated chatting are provided.\n     The systems and method provide multiple topic automated (or artificial\n     intelligence) chatting by analyzing user inputs in a conversation to\n     determine a plurality topics, to determine and score features related to\n     the determined topics and different users, and to create a knowledge\n     graph of the determined topics. Based on these determinations, the\n     systems and methods may determine if a reply should be provided and then\n     predict a reply.\n", "citations": ["9369410", "20050192959", "20070294229", "20080189367", "20140122083", "20150127526", "20150142704", "20150286709", "20160080485", "20160094492"], "related": []}, {"id": "20180205727", "patent_code": "10362016", "patent_name": "Dynamic knowledge-based authentication", "year": "2019", "inventor_and_country_data": " Inventors: \nHwang; Inseok (Austin, TX), Kistler; Michael D. (Austin, TX), Mukundan; Janani (Austin, TX), Taban; Gelareh (Austin, TX)  ", "description": "BACKGROUND\n Embodiments of the present invention relate to user authentication and, more specifically, to dynamic knowledge-based authentication.\n Many systems require user authentication in order to provide security for data and activities inside the system.  Generally, user authentication requires entry of a username and password.  A user is likely to have secure accounts on multiple\nsystems, including an enterprise network, social media, financial websites, and various other systems.  As a result, the user is required to remember multiple passwords.\n Some security systems use questions and answers for authentication, instead of or in addition to passwords.  With such systems, a user is asked in advance to select one or more questions and then write answers for each selected question.  Later,\nto authenticate himself, the user is presented with the questions previously selected, and must then supply the answers exactly as originally written.\nSUMMARY\n According to an embodiment of this disclosure, a computer-implemented method includes receiving user data from a plurality of data sources, where the user data describes a user.  A plurality of events are determined within the user data by\napplying anomaly detection to the user data.  A request to authenticate the user is received.  A key event is selected from among the plurality of events, responsive to the request to authenticate the user.  For the user, a challenge is generated, by a\ncomputer processor, based on the key event and responsive to the request to authenticate the user.  A response to the challenge is received.  It is decided whether to authenticate the user based on the response to the challenge.\n In another embodiment, a system includes a memory having computer-readable instructions and one or more processors for executing the computer-readable instructions.  The computer-readable instructions include receiving user data from a plurality\nof data sources, where the user data describes a user.  Further according to the computer-readable instructions, a plurality of events are determined within the user data by applying anomaly detection to the user data.  A request to authenticate the user\nis received.  A key event is selected from among the plurality of events, responsive to the request to authenticate the user.  For the user, a challenge is generated, based on the key event and responsive to the request to authenticate the user.  A\nresponse to the challenge is received.  It is decided whether to authenticate the user based on the response to the challenge.\n In yet another embodiment, a computer-program product for attempting to authenticate a user includes a computer-readable storage medium having program instructions embodied therewith.  The program instructions are executable by a processor to\ncause the processor to perform a method.  The method includes receiving user data from a plurality of data sources, where the user data describes a user.  Further according to the method, a plurality of events are determined within the user data by\napplying anomaly detection to the user data.  A request to authenticate the user is received.  A key event is selected from among the plurality of events, responsive to the request to authenticate the user.  For the user, a challenge is generated, based\non the key event and responsive to the request to authenticate the user.  A response to the challenge is received.  It is decided whether to authenticate the user based on the response to the challenge.\n Additional features and advantages are realized through the techniques of the present invention.  Other embodiments and aspects of the invention are described in detail herein and are considered a part of the claimed invention.  For a better\nunderstanding of the invention with the advantages and the features, refer to the description and to the drawings. BRIEF DESCRIPTION OF THE DRAWINGS\n The subject matter regarded as the invention is particularly pointed out and distinctly claimed in the claims at the conclusion of the specification.  The foregoing and other features and advantages of the invention are apparent from the\nfollowing detailed description taken in conjunction with the accompanying drawings in which:\n FIG. 1 is a block diagram of a security system, according to some embodiments of this invention;\n FIG. 2 is a table of example user data of the security system, according to some embodiments of this invention;\n FIG. 3 is an example knowledge graph of the security system, according to some embodiments of this invention;\n FIG. 4 is a flow diagram of a method for authenticating a user, according to some embodiments of this invention; and\n FIG. 5 is a block diagram of a computer system for implementing some or all aspects of the security system, according to some embodiments of this invention.\nDETAILED DESCRIPTION\n User authentication with passwords presents several difficulties.  For instance, secure, randomized passwords tend not to be memorable.  To enable a password to be secure and thus more difficult for a malicious party to guess, security systems\noften set requirements for passwords.  For instance, passwords are often required to include numerals, a mix of capital and lowercase letters, and a special character.  As a result, users have difficulty remembering their passwords.\n An additional problem is that, in order for a user to remember a password, that password is likely to remain static for an extended period.  For instance, a user is likely to use the same password for months or years, thus increasing the\npossibility that the password will be discovered by a malicious party.  Further, for convenience, users often use the same or similar passwords across multiple security systems.  Thus, once a malicious party has access to one secured account of the user,\nit becomes easier to gain access to other secured accounts.\n Similar issues exist when questions and answers are used for authentication.  When presented with previously selected questions for authentication, the user must remember the exact format used when he originally provided answers to those\nquestions and must enter the answers in that exact format.  Further, the user is likely to use the same or similar questions and answers across multiple security systems, and thus, a malicious party would find it easy to gain access to all the user's\naccounts after having gained access to a first one.\n Turning now to an overview of aspects of the present invention, some embodiments of a security system according to this disclosure authenticate users by way of dynamic challenges that are drawn from a knowledge base.  The knowledge base may be\ncompiled from public data, private data, or both.  The user need not select questions and provide exact answers in advance.  Rather, the security system may gather data about the user, and may dynamically devise challenges based on that data.  Further,\nfuzzy matching may be used to verify the user's responses to the challenges, such that there is little to no reliance on the user providing his answer in a specific format or with precise wording.\n FIG. 1 is a block diagram of the security system 100, according to some embodiments of this invention.  As shown in FIG. 1, the security system 100 may include an authentication broker 110, which may receive user data from one or more data\nsources 120.  The authentication broker 110 may be in communication with one or more secured services 170, each of which may use the security system 100, by way of the authentication broker 110, to authenticate a user requesting access to the secured\nservice 170.\n In some embodiments, the authentication broker may include or be in communication with a data collector 130, an event extractor 140, a challenge generator 150, and a verifier 160.  Generally, the data collector 130 may collect user data from the\ndata sources 120; the event extractor 140 may graph knowledge known about a user and analyze patterns in the user data to extract events upon which challenges can be based; the challenge generator 150 may generate challenges for user authentication; and\nthe verifier 160 may verify the user's responses to the challenges.  It will be understood that although the data collector 130, the event extractor 140, the challenge generator 150, and the verifier 160 are illustrated as distinct components in FIG. 1,\nthese components may be combined or further divided based on design decisions.\n Generally, a user may have an account established with the security system 100, and thus with the authentication broker 110.  The authentication broker 110 may receive user data from the one or more data sources 120.  A secured service 170 may\nbe a service that requires user authentication for access.  For example, and not by way of limitation, the secured service 170 may be a computer network, a website, a web application, an email server, a financial account, or other service.  The user may\nhave or desire an account with, or some other form of access to, the secured service 170.  To gain the desired access, the user may be authenticated through the authentication broker 110.  The authentication broker 110 may confirm the user's identity to\nthe secured service 170 based on the user's responses to dynamic challenges, which may be devised by the authentication broker 110 based on the user data.\n In some embodiments, a secured service 170 may be integrated with the authentication broker 110.  In other words, the authentication broker 110 may be part of the secured service 170 such that no third party need be contacted to verify the\nuser's identity.\n In some embodiments, the data collector 130 of the authentication broker 110 may collect user data from the data sources 120.  Each data source 120 may be a source of user data describing one or more users.  The user data associated with a user\nmay describe the user and may include public data, private data, or both.  In some embodiments, some or all user data may be associated with timestamps, such that the security system 100 may match items of the user data with times at which such items\noccurred.  Further, in some embodiments, the security system 100 may take advantage of the internet of things (IoT), such that internet-connected devices are used as data sources 120.  For example, and not by way of limitation, data sources 120 may\ninclude fitness trackers, smartphones, vehicles, and social media systems (e.g., Facebook.RTM., Twitter.RTM., LinkedIn.RTM.).  For instance, a smartphone may be a data source 120 providing an associated user's location (e.g., by way of global positioning\nsystem (GPS), or cellular tower), wireless fidelity (WiFi) connections, Bluetooth connections, calendar, task list, emails, text messaging, contacts list, and other data.  A vehicle may be a data source 120 providing an associated user's location,\nbraking history, acceleration and velocity history, the number of passengers, and other data.  A social media system may be a data source 120 providing information disclosed in a user's posts, or information about the user's friends and groups and their\nposts.  In some embodiments, a data source 120 may provide user data that is biological or cyber-physical.  For instance, a user's fitness tracker may be a data source 120 providing an associated user's heart rate, as well as location, workout times, and\nactivities.  It will be understood that various other data sources 120 may be used instead of or in addition to these.\n In some embodiments, a data source 120 may be enabled to provide information to the authentication broker 110 about a user only after the user provides permission for that provision of information.  However, in some cases, the security system\n100 may be able to provide a broader range of questions or greater accuracy when provided a large range of data sources 120 for the user.  Further, user data may be validated by comparing information received from a single data source 120 or across\nmultiple data sources 120, and as a result, a larger number of data sources 120 may lead to both a larger volume of user data as well as more accurate user data.\n The security system 100 may identify anomalies in the user data for each user, and these anomalies may be used to identify events on which authentication challenges may be devised.  In some embodiments, these operations may be performed by the\nevent extractor 140 of the authentication broker 110.  To this end, the security system 100 may perform temporal clustering on the user data for a user.  This clustering may be performed using techniques known in the art, such as k-means clustering.  The\nsecurity system 100 may then perform anomaly detection on the clustered user data.  For example, and not by way of limitation, this anomaly detection may be performed by unsupervised learning to extract patterns that represent high-level activities. \nThis can be performed by techniques known in the art.\n FIG. 2 is an example table of user data describing a user, according to some embodiments of this invention.  In this example, the table includes user data received from data sources 120 including a fitness tracker, a smartphone, and a vehicle. \nThe user data includes timestamped information describing the user's heart rate, braking, the number of passengers in the vehicle, and speed.  In this example, heart rate is given in beats per minute, braking is given in units such that a higher number\nreflects greater pressure applied to the brakes and a lower number reflects lesser pressure applied to the brakes, and speed is given in miles per hour.  As shown in the table of FIG. 2, the user's heart rate jumped dramatically at 4:30 pm, braking\npressure increased, and speed dropped.  At this time, there was one passenger in the vehicle, and that passenger was the user.  Given the abrupt change in heart rate, speed, and braking, the security system 100 could classify this as an anomaly.\n The security system 100 may generate a knowledge graph corresponding to each user, and that knowledge graph may be a graph of events, where each event is an anomaly identified in the user data for the user.  FIG. 3 is an example knowledge graph\n300 for one user, according to some embodiments of this invention.  As shown in FIG. 3, various events 310 may be represented as nodes in the knowledge graph 300, and edges 320 may connect related pairs of such events 310.  An edge 320 may be drawn\nbetween two events 310 in the knowledge graph 300 if those two events 310 are deemed connected.  Connections may be of various types, such as temporal, geographical, relationship-related, aspect of life, or other types of connections.  For example, a\nconnection may be deemed to exist between two events 310 that are related to the user's job, occurred within the same hour, involve the same friend, or occurred in the same location.  Thus, an edge 320 would be generated in the knowledge graph between\ntwo such events 310.\n In some embodiments, Bayesian modeling may be used to generate probability distributions for events 310 that have occurred.  As discussed above, anomaly detection may be used to identify events, but in some embodiments, Bayesian modeling may be\nused to generate weighted edges 320 in the knowledge graph 300.  The weight of a weighted edge 320 may reflect the probability that the events 310 adjacent to the weighted edge 320 are related (i.e., connected).  Further, in some embodiments, a\nprobability threshold may be used to determine whether an edge should exist at all between two events 310 in the knowledge graph 300.\n In some embodiments, a knowledge graph 300 may incorporate user data from other users who have relationships with the user associated with the knowledge graph 300.  For example, and not by way of limitation, the security system 100 may be aware\nof user relationships by way of user data received from social media systems, or may be aware of information (e.g., workplace) that is similar across users and thus forms a connection.  In the case of such connections, events 310 may be added to a first\nuser's knowledge graph 300 based on user data describing a second user.  For example, if an event 310 is determined from user data of a second user, where that event 310 is related to a workplace of the second user, and where the first user and the\nsecond user share a workplace, then that event 310 may also be added to the knowledge graph 300 of the first user.\n The security system 100 may generate challenges for a user based on events 310 detected and, in some embodiments, further based on the knowledge graph 300 of the user.  In some embodiments, each challenge may be generated dynamically, when the\nuser attempts to access a secured service 170.  However, in some embodiments, challenges may be generated offline, and then presented to the user when the user attempts access to a secured service 170.  In some embodiments, the generation of challenges\nmay be performed by the challenge generator 150 of the authentication broker 110.\n In generating a challenge, the security system 100 may select at least one event 310 on which to base the challenge.  Generally, the security system 100 may seek an event 310 that is both memorable and safe.  In this disclosure, memorability\nrefers to a likelihood that a user will recall an event 310.  The security system 100 may seek an event 310 with high memorability because, if a user remembers the event 310, that user is more likely to be able to correctly respond to a challenge (e.g.,\nanswer a question) based on that event 310.  In this disclosure, safety refers to a likelihood that an event 310 is not known or not memorable to people other than the user.  The security system 100 may seek a safe event 310 to ensure that a person other\nthan the user cannot gain access to the secured service 170 by knowing how to correctly respond to the challenge based on the event 310.\n In some embodiments, both memorability and safety may be reflected by a respective number, which may be determined by a respective function.  Specifically, memorability may be calculated by a memorability function, and safety may be calculated\nby a safety function.  These functions may vary based on implementation.  However, in some embodiments, the memorability function may be based on factors that include one or more of the following: recurrence, rarity, biological changes (e.g., changes in\nheart rate), social importance, personal importance, contextual importance, and user profile.  The user profile may refer to information known about the user by the security system 100, which may include user data and the knowledge graph 300, for\nexample.  Further, if Bayesian modeling is being used, the probability distribution of an event's occurrence may also contribute to the memorability function.  In some embodiments, the safety function may be based on factors including one or more of the\nfollowing: passive versus active involvement by other people, online mining, guessability, and user profile.  With respect to passive versus active involvement of other people, this may be considered by the safety function because another person is less\nlikely to recall an event 310 if she was only passively involved in it, but that other person is more likely to recall the event 310 if she was actively involved in the event 310.  Online mining refers to the ability to discover information about the\nevent 310 online.  The security system 100 may perform online mining related to an event 310 before using the event 310 as a challenge and may incorporate results found from the online mining into a determination of safety.  Guessability refers to the\nlikelihood of someone other than the user being able to guess the event 310.\n When generating a challenge, the security system 100 may select an event 310 that is both memorable and safe.  In some embodiments, this may be achieved by selecting an event 310 in which the calculated memorability falls into a desirable\nmemorability range, and the calculated safety falls into a desirable safety range.  For instance, if a high value for memorability suggests a more memorable event 310, then calculated memorability of an event 310 may be required to be above a\nmemorability threshold for selection, but if a low value for memorability suggests a more memorable event 310, then the calculated memorability of the event 310 may be required to be below the memorability threshold for selection.  Analogously, if a high\nvalue for safety suggests a safer event 310, then calculated safety of an event 310 may be required to be above a safety threshold for selection, but if a low value for safety suggests a safer event 310, then the calculated safety may be required to be\nbelow the safety threshold for selection.\n When authentication to a secured service 170 is requested, in some embodiments, the security system 100 may generate a challenge including one or more questions, and the security system 100 may further generate an expected answer for each of the\none or more questions in the challenge.  Alternatively, however, challenges may be generated offline, such that they are ready to be presented when authentication to the secured service 170 is requested.  For each challenge, one or more events 310 may be\nused on which to base that challenge.  After at least one event 310 for a challenge is selected, in some embodiments, the security system 100 may generate a challenge using template-based challenge generation, multi-tiered challenge generation using\nepisodic memory model, or both.\n With template-based challenge generation, the security system 100 may have access to a set of challenge templates.  For example, one template may be as follows: \"What were you doing on [date and time]?\" To use this template, the security system\n100 may simply fill in the date and time of the event 310 selected.  In this case, the expected response to the challenge is an activity that the security system 100 knows the user was performing, based on the user data.  Other example templates are as\nfollows: \"At what time on [date] did you perform [activity]?\" and \"What song did you listen to while you performed [activity]?\"\n Multi-tiered challenge generation using episodic memory may involve generating a challenge having two or more questions that are related to one another and build on one another.  A benefit of this technique is that early questions in a series\nmay be used to jog a user's memory, thus improving the chances that the user can respond correctly to the challenge.  Returning to the example of the table shown in FIG. 2, the security system 100 may determine that an accident, near-accident, or other\nsignificant incident occurred in the user's vehicle at 4:30 pm.  Based on this event 310, the security system 100 may devise a multi-tiered challenge, where the first question in the challenge is: \"What were you doing at 4:30 pm on Monday?\" An acceptable\nresponse to this challenge would be \"driving,\" for example.  A second question in the challenge may ask: \"Did anything happen while you were driving?\" Acceptable answers to this question could include \"almost had an accident\" or \"was rear-ended,\" for\nexample.\n In some embodiments, the security system 100 may receive an answer to each question in a challenge.  To verify the user's identity, the security system 100 may determine whether the user's answers are correct.  Specifically, in some embodiments,\nthis determination may be performed by the verifier 160 of the authentication broker 110.  However, because the answers are not passwords with a specific format that must match character-by-character, as is conventionally the case, the security system\n100 need not require answers that precisely match the expected answers to the questions.  In some embodiments, the security system 100 may compare the user's answers to the expected answers using fuzzy matching.  If each answer is deemed to match the\ncorresponding expected answer, such as via fuzzy matching, then the security system 100 may deem the user's answers to be correct.  As such, the security system 100, specifically the authentication broker 110 for example, may verify the user's identity\nand may notify the secured service 170 that the user has been authenticated.  The secured service 170 may then allow the user to access secure data and services.\n Alternatively, however, in some embodiments, the security system 100 may provide the secured service 170 with the complete challenge, including one or more questions and the corresponding one or more answers.  In this case, the secured service\n170 may be responsible for determining whether the user correctly responds to the challenge.\n In some embodiments, if the user fails to correctly respond to a challenge, the security system 100 may offer an additional challenge, which may be generated as described above.  The additional challenge may give the user another chance to\nauthenticate himself, which may be necessary, for example, if the selected event 310 is not as memorable as determined by the security system 100.  Different users remember different categories of events 310.  Thus, the security system 100 may keep track\nof the types of challenges the user tends to answer correctly and the types the user tends to answer incorrectly.  For example, and not by way of limitation, a user may have a good memory for music and may usually correctly respond to challenges about\nthe music he listened to.  When a user responds to a challenge correctly, this may be reflected in association with the knowledge graph 300.  Due to the use of the edges 320 to connect related events 310, events 310 may be clustered based on similarity. \nA correct challenge response may add greater weight to a cluster of the knowledge graph 300 near the event 310 used to devise the challenge, while conversely, an incorrect challenge response may reduce the weight of the cluster of the knowledge graph\n300.\n Specifically, for instance, each event 310 in the knowledge graph 300 may be associated with a corresponding weight.  A correct answer to a challenge may increase the weight of the events 310 upon which the challenge was devised, and further the\ncorrect response may increase, to a lesser degree, the weights of events 310 connected by edges 320 to those used in the challenge.  In the example knowledge graph 300 of FIG. 3, the weighting is illustrated by the size of each node representing the\ncorresponding event 310.  This weighting may be incorporated into the user profile, of which the knowledge graph 300 is a part, and may thus be used when determining the memorability of events 310 being considered for future challenges.  As a result, the\nuser's ability to answer certain types of challenges (i.e., challenges based on certain types of events 310) may affect the generation of later challenges of those types.  Thus, the knowledge graph 300 may be adaptable based on learning performed\nautomatically by the security system 100.\n FIG. 4 is a flow diagram of a method 400 for authenticating a user, according to some embodiments of this invention.  It will be understood that, although this method 400 refers to a single user, the security system 100 may perform these\noperations to provide authentication for numerous users.  As shown in FIG. 4, at block 405, user data may be collected in association with a user.  At block 410, anomaly detection may be performed on the user data to identify a set of events 310.  In\nsome embodiments, the anomaly detection may include temporal clustering and then detection of anomalies from among the clusters.  At block 415, a knowledge graph 300 may be generated based on the identified events 310.\n At block 420, in some embodiments, user authentication may be requested by a secured service 170 for the user.  However, in some embodiments, blocks 425 and 430 may occur before user authentication is requested.  In that case, the security\nsystem 100 may select key events and generate challenges, at blocks 425 and 430, such that challenges are ready to be presented when user authentication is requested.  At block 425, a key event 310 may be selected from those identified.  In some\nembodiments, this selection may be based on calculated memorability, may be based on calculated safety, and may take the knowledge graph 300 into consideration.  At block 430, one or more questions and corresponding expected answers may be generated as a\nchallenge based on the key event 310.  At block 435, the challenge may be presented to the user.  At block 440, the user may provide an answer to each question in the challenge.  At decision block 445, it may be determined whether the user's one or more\nanswers match the corresponding expected one or more answers.  Fuzzy matching may be performed to make this determination.  In some embodiments, however, the secured service 170 may be responsible for determining whether the user's response to the\nchallenge matches the one or more expected answers.\n If the user's answers do not match the expected answers, then at block 450 the knowledge graph 300 may be updated to reflect the incorrect response.  Additionally, at decision block 455, it may be determined whether the user gets to make another\nattempt.  If another attempt is allowed, then the method 400 may return to block 430, where generation of an additional challenge may occur.  Alternatively, in some embodiments, the method 400 may return to block 425, where a new key event may be\nselected, upon which the additional challenge may be based.  If no additional attempt is allowed (e.g., if the user has reached a threshold number of incorrect answers), then at block 460 authentication may fail, and the secured service 170 may be\nnotified of the failure.  However, if the user's answers are deemed to match the expected answers, then at block 465, the knowledge graph 300 may be updated to reflect the correct response.  Additionally, at block 470, the user may be authenticated, and\nthe security system 100 may verify the user's identity to the secured service 170.\n FIG. 5 illustrates a block diagram of a computer system 500 for use in implementing a security system 100 or method according to some embodiments.  The security systems 100 and methods described herein may be implemented in hardware, software\n(e.g., firmware), or a combination thereof.  In some embodiments, the methods described may be implemented, at least in part, in hardware and may be part of the microprocessor of a special or general-purpose computer system 500, such as a personal\ncomputer, workstation, minicomputer, or mainframe computer.  For example, and not by way of limitation, the authentication broker 110 of the security system 100 may be implemented on a computer system 500.\n In some embodiments, as shown in FIG. 5, the computer system 500 includes a processor 505, memory 510 coupled to a memory controller 515, and one or more input devices 545 and/or output devices 540, such as peripherals, that are communicatively\ncoupled via a local I/O controller 535.  These devices 540 and 545 may include, for example, a printer, a scanner, a microphone, and the like.  Input devices such as a conventional keyboard 550 and mouse 555 may be coupled to the I/O controller 535.  The\nI/O controller 535 may be, for example, one or more buses or other wired or wireless connections, as are known in the art.  The I/O controller 535 may have additional elements, which are omitted for simplicity, such as controllers, buffers (caches),\ndrivers, repeaters, and receivers, to enable communications.\n The I/O devices 540, 545 may further include devices that communicate both inputs and outputs, for instance disk and tape storage, a network interface card (NIC) or modulator/demodulator (for accessing other files, devices, systems, or a\nnetwork), a radio frequency (RF) or other transceiver, a telephonic interface, a bridge, a router, and the like.\n The processor 505 is a hardware device for executing hardware instructions or software, particularly those stored in memory 510.  The processor 505 may be a custom made or commercially available processor, a central processing unit (CPU), an\nauxiliary processor among several processors associated with the computer system 500, a semiconductor based microprocessor (in the form of a microchip or chip set), a macroprocessor, or other device for executing instructions.  The processor 505 includes\na cache 570, which may include, but is not limited to, an instruction cache to speed up executable instruction fetch, a data cache to speed up data fetch and store, and a translation lookaside buffer (TLB) used to speed up virtual-to-physical address\ntranslation for both executable instructions and data.  The cache 570 may be organized as a hierarchy of more cache levels (L1, L2, etc.).\n The memory 510 may include one or combinations of volatile memory elements (e.g., random access memory, RAM, such as DRAM, SRAM, SDRAM, etc.) and nonvolatile memory elements (e.g., ROM, erasable programmable read only memory (EPROM),\nelectronically erasable programmable read only memory (EEPROM), programmable read only memory (PROM), tape, compact disc read only memory (CD-ROM), disk, diskette, cartridge, cassette or the like, etc.).  Moreover, the memory 510 may incorporate\nelectronic, magnetic, optical, or other types of storage media.  Note that the memory 510 may have a distributed architecture, where various components are situated remote from one another but may be accessed by the processor 505.\n The instructions in memory 510 may include one or more separate programs, each of which comprises an ordered listing of executable instructions for implementing logical functions.  In the example of FIG. 5, the instructions in the memory 510\ninclude a suitable operating system (OS) 511.  The operating system 511 essentially may control the execution of other computer programs and provides scheduling, input-output control, file and data management, memory management, and communication control\nand related services.\n Additional data, including, for example, instructions for the processor 505 or other retrievable information, may be stored in storage 520, which may be a storage device such as a hard disk drive or solid state drive.  The stored instructions in\nmemory 510 or in storage 520 may include those enabling the processor to execute one or more aspects of the security systems 100 and methods of this disclosure.\n The computer system 500 may further include a display controller 525 coupled to a display 530.  In some embodiments, the computer system 500 may further include a network interface 560 for coupling to a network 565.  The network 565 may be an\nIP-based network for communication between the computer system 500 and an external server, client and the like via a broadband connection.  The network 565 transmits and receives data between the computer system 500 and external systems.  In some\nembodiments, the network 565 may be a managed IP network administered by a service provider.  The network 565 may be implemented in a wireless fashion, e.g., using wireless protocols and technologies, such as WiFi, WiMax, etc. The network 565 may also be\na packet-switched network such as a local area network, wide area network, metropolitan area network, the Internet, or other similar type of network environment.  The network 565 may be a fixed wireless network, a wireless local area network (LAN), a\nwireless wide area network (WAN) a personal area network (PAN), a virtual private network (VPN), intranet or other suitable network system and may include equipment for receiving and transmitting signals.\n Security systems 100 and methods according to this disclosure may be embodied, in whole or in part, in computer program products or in computer systems 500, such as that illustrated in FIG. 5.\n Technical effects and benefits of some embodiments include the ability to authenticate a user without the use of pre-established passwords or questions.  Some embodiments of the security system 100 described herein may dynamically generate\nauthentication challenges based on user data collected, including user data from IoT, biological sensors, and cyber-physical sensors.  More specifically, the security system 100 may apply adaptive learning to a knowledge graph 300 based on this user data\nand may generate dynamic challenges as a result.\n The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention.  As used herein, the singular forms \"a\", \"an\" and \"the\" are intended to include the plural forms as\nwell, unless the context clearly indicates otherwise.  It will be further understood that the terms \"comprises\" and/or \"comprising,\" when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and/or\ncomponents, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof.\n The corresponding structures, materials, acts, and equivalents of all means or step plus function elements in the claims below are intended to include any structure, material, or act for performing the function in combination with other claimed\nelements as specifically claimed.  The description of the present invention has been presented for purposes of illustration and description, but is not intended to be exhaustive or limited to the invention in the form disclosed.  Many modifications and\nvariations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the invention.  The embodiments were chosen and described in order to best explain the principles of the invention and the practical\napplication, and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated.\n The present invention may be a system, a method, and/or a computer program product at any possible technical detail level of integration.  The computer program product may include a computer readable storage medium (or media) having computer\nreadable program instructions thereon for causing a processor to carry out aspects of the present invention.\n The computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device.  The computer readable storage medium may be, for example, but is not limited to, an electronic\nstorage device, a magnetic storage device, an optical storage device, an electromagnetic storage device, a semiconductor storage device, or any suitable combination of the foregoing.  A non-exhaustive list of more specific examples of the computer\nreadable storage medium includes the following: a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a static random access memory (SRAM), a\nportable compact disc read-only memory (CD-ROM), a digital versatile disk (DVD), a memory stick, a floppy disk, a mechanically encoded device such as punch-cards or raised structures in a groove having instructions recorded thereon, and any suitable\ncombination of the foregoing.  A computer readable storage medium, as used herein, is not to be construed as being transitory signals per se, such as radio waves or other freely propagating electromagnetic waves, electromagnetic waves propagating through\na waveguide or other transmission media (e.g., light pulses passing through a fiber-optic cable), or electrical signals transmitted through a wire.\n Computer readable program instructions described herein can be downloaded to respective computing/processing devices from a computer readable storage medium or to an external computer or external storage device via a network, for example, the\nInternet, a local area network, a wide area network and/or a wireless network.  The network may comprise copper transmission cables, optical transmission fibers, wireless transmission, routers, firewalls, switches, gateway computers and/or edge servers. \nA network adapter card or network interface in each computing/processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium\nwithin the respective computing/processing device.\n Computer readable program instructions for carrying out operations of the present invention may be assembler instructions, instruction-set-architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, firmware\ninstructions, state-setting data, configuration data for integrated circuitry, or either source code or object code written in any combination of one or more programming languages, including an object oriented programming language such as Smalltalk, C++,\nor the like, and procedural programming languages, such as the \"C\" programming language or similar programming languages.  The computer readable program instructions may execute entirely on the user's computer, partly on the user's computer, as a\nstand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server.  In the latter scenario, the remote computer may be connected to the user's computer through any type of network,\nincluding a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).  In some embodiments, electronic circuitry including, for\nexample, programmable logic circuitry, field-programmable gate arrays (FPGA), or programmable logic arrays (PLA) may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to\npersonalize the electronic circuitry, in order to perform aspects of the present invention.\n Aspects of the present invention are described herein with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems), and computer program products according to embodiments of the invention.  It will be\nunderstood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer readable program instructions.\n These computer readable program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute\nvia the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.  These computer readable program instructions may also be\nstored in a computer readable storage medium that can direct a computer, a programmable data processing apparatus, and/or other devices to function in a particular manner, such that the computer readable storage medium having instructions stored therein\ncomprises an article of manufacture including instructions which implement aspects of the function/act specified in the flowchart and/or block diagram block or blocks.\n The computer readable program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other device to cause a series of operational steps to be performed on the computer, other programmable apparatus or\nother device to produce a computer implemented process, such that the instructions which execute on the computer, other programmable apparatus, or other device implement the functions/acts specified in the flowchart and/or block diagram block or blocks.\n The flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present invention. \nIn this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of instructions, which comprises one or more executable instructions for implementing the specified logical function(s).  In some alternative\nimplementations, the functions noted in the blocks may occur out of the order noted in the Figures.  For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse\norder, depending upon the functionality involved.  It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special\npurpose hardware-based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.\n The descriptions of the various embodiments of the present invention have been presented for purposes of illustration, but are not intended to be exhaustive or limited to the embodiments disclosed.  Many modifications and variations will be\napparent to those of ordinary skill in the art without departing from the scope and spirit of the described embodiments.  The terminology used herein was chosen to best explain the principles of the embodiments, the practical application or technical\nimprovement over technologies found in the marketplace, or to enable others of ordinary skill in the art to understand the embodiments disclosed herein.", "application_number": "15408512", "abstract": " A computer-implemented method includes receiving user data from a\n     plurality of data sources, where the user data describes a user. A\n     plurality of events are determined within the user data by applying\n     anomaly detection to the user data. A request to authenticate the user is\n     received. A key event is selected from among the plurality of events,\n     responsive to the request to authenticate the user. For the user, a\n     challenge is generated, by a computer processor, based on the key event\n     and responsive to the request to authenticate the user. A response to the\n     challenge is received. It is decided whether to authenticate the user\n     based on the response to the challenge.\n", "citations": ["7404087", "8955066", "20030101104", "20030203756", "20090089876", "20090198587", "20090305670", "20150033305"], "related": []}, {"id": "20180210913", "patent_code": "10311050", "patent_name": "Crowdsourced discovery of paths in a knowledge graph", "year": "2019", "inventor_and_country_data": " Inventors: \nBeller; Charles E. (Baltimore, MD), Byron; Donna K. (Petersham, MA), Johnson; Benjamin L. (Baltimore, MD)  ", "description": "BACKGROUND\n The present disclosure relates generally to the field of semantic queries.  Semantic queries enable the retrieval of both explicitly and implicitly derived information based on syntactic, semantic and structural information contained in data. \nSemantic queries often involve the use of a knowledge base.  A knowledge base may store complex structured and unstructured information used by a computer system.  A knowledge graph depicts the relationship between various entities contained in the\nknowledge base.\nSUMMARY\n The present disclosure includes various embodiments including a system, computer-implemented method, computer program product for improving a knowledge graph.  The disclosed embodiments seek to improve the process of expanding a knowledge graph\nby identifying information that is useful and relevant to the needs of the users based on questions being asked by users.  In one embodiment, the information is collected using a gaming platform.  The disclosed embodiments augment the knowledge graph, by\nadding new entities and/or relationships, based on the gathered information.\n As an example, the disclosed embodiments include a computer-implemented method for modifying a knowledge graph.  The computer-implemented method includes the step of receiving a question and determining a first entity and a second entity from\nthe question.  The computer-implemented method determines whether a knowledge gap exists between the first entity and the second entity in the knowledge graph.  In one embodiment, a knowledge gap exists if there is less than a predetermined number of\nconnection paths between the first entity and the second entity that are within a predetermined number of hops.  The computer-implemented method performs a request to collect information to correct the knowledge gap in the knowledge graph in response to\na determination that the knowledge gap exists between the first entity and the second entity in the knowledge graph.  In one embodiment, the request for information is implemented in a gaming application.  The computer-implemented method receives the\ninformation based on the request and validates the information.  The computer-implemented method updates the knowledge graph based on the information in response to the information being validated.\n The disclosed embodiments also include a system configured to modify a knowledge graph.  In one embodiment, the system includes a processor configured to execute instructions to receive a question and determine a first entity and a second entity\nfrom the question.  The processor further executes instructions to determine whether a knowledge gap exists between the first entity and the second entity in the knowledge graph; perform a request to collect information to correct the knowledge gap in\nthe knowledge graph in response to a determination that the knowledge gap exists between the first entity and the second entity in the knowledge graph; receive the information based on the request; validate the information; and update the knowledge graph\nbased on the information in response to the information being validated.\n Another disclosed embodiment is a computer program product comprising a computer readable storage medium having program instructions embodied therewith, the program instructions executable by a processor.  Executing the program instructions\ncauses the processor to receive a question; determine a first entity and a second entity from the question; determine whether a knowledge gap exists between the first entity and the second entity in the knowledge graph; and perform a corrective action to\ncorrect the knowledge gap in the knowledge graph in response to a determination that the knowledge gap exists between the first entity and the second entity in the knowledge graph. BRIEF DESCRIPTION OF THE DRAWINGS\n For a more complete understanding of this disclosure, reference is now made to the following brief description, taken in connection with the accompanying drawings and detailed description, wherein like reference numerals represent like parts.\n FIG. 1 is a block diagram illustrating a system for improving a knowledge graph according to an embodiment of the present disclosure;\n FIG. 2 is a flowchart of a computer-implemented method for improving a knowledge graph according to an embodiment of the present disclosure;\n FIG. 3 is a flowchart of a computer-implemented method for performing corrective action to fill in a knowledge gap according to an embodiment of the present disclosure;\n FIG. 4 is a user interface for gathering information for improving a knowledge graph according to an embodiment of the present disclosure; and\n FIG. 5 is a data processing system according to an embodiment of the present disclosure.\n The illustrated figures are only exemplary and are not intended to assert or imply any limitation with regard to the environment, architecture, design, or process in which different embodiments may be implemented.\nDETAILED DESCRIPTION\n It should be understood at the outset that, although an illustrative implementation of one or more embodiments are provided below, the disclosed systems, computer program product, and/or methods may be implemented using any number of techniques,\nwhether currently known or in existence.  The disclosure should in no way be limited to the illustrative implementations, drawings, and techniques illustrated below, including the exemplary designs and implementations illustrated and described herein,\nbut may be modified within the scope of the appended claims along with their full scope of equivalents.\n As referenced herein, the term database or knowledge base is defined as collection of structured or unstructured data.  Although referred in the singular form, the database may include one or more databases, and may be locally stored on a system\nor may be operatively coupled to a system via a local or remote network.\n As used within the written disclosure and in the claims, the terms \"including\" and \"comprising\" are used in an open-ended fashion, and thus should be interpreted to mean \"including, but not limited to\".  Unless otherwise indicated, as used\nthroughout this document, \"or\" does not require mutual exclusivity, and the singular forms \"a\", \"an\" and \"the\" are intended to include the plural forms as well, unless the context clearly indicates otherwise.  Also, as used herein, the term \"module\"\nrefers to a component or function that is part of program or system, and may be implemented in software, hardware, or a combination thereof.  The software portion of a module may be implemented using any form of computer readable program instructions as\ndescribed herein.\n As stated above, existing knowledge graphs have been shown to be useful for semantic search.  One downside of existing knowledge graphs, particularly ones representing common sense or world knowledge, is that they are often sparse in terms of\nhow many edges (relations) they contain.  Additionally, these edges are usually representative of subsumption or location relationships (e.g., \"Ben was born in Baltimore,\" or \"Apples are fruit\"), other types of relationships are quite sparse.  However,\nthe space of all possible missing elements of a knowledge graph is vast.  Extending the knowledge graph with random new knowledge may not contribute to improved performance of the systems that draw on the knowledge stored in the knowledge graph, and\nencompasses a lot of time and effort.\n Thus, the disclosed embodiments seek to improve the process of expanding a knowledge graph by identifying information that is useful and relevant to the needs of the users, and automatically gathering that information and augmenting the\nknowledge graph, by adding new entities and/or relationships, based on the gathered information.  The extended knowledge graph can then be used to provide answers to questions that were previously not answerable using the old knowledge graph.  Another\nadvantage in the disclosed embodiments is that it is more responsive to user inputs and can be adaptive with the much lower latency than existing systems.  For example, if a new item of interest shows up with user queries, due to for example an event in\nthe news or a suggested linkage between a TV personality and a favorite childhood vacation spot, the knowledge elicitation method opportunistically augments the knowledge graph to help serve additional user requests.\n With reference now to FIG. 1, a system 100 for improving a knowledge graph according to an embodiment of the present disclosure is presented.  The system 100 includes a knowledge base 110, a discovery module 130, a game module 140, and one or\nmore end user devices 150.  In various embodiments, the knowledge base 110, the discovery module 130, and the game module 140 may be implemented on one system or multiple systems.  For example, in one embodiment, the knowledge base 110 may be implemented\non a first system, while the discovery module 130 and the game module 140 are implemented on a second system.  In another embodiment, the knowledge base 110 may be implemented on a first system, the discovery module 130 may be implemented on a second\nsystem, and the game module 140 may be implemented on a third system.  Further, although the depicted embodiment describes the discovery module 130 and the game module 140 as separate modules, in some embodiments, the discovery module 130 and the game\nmodule 140 may be combined and/or the functions described in each of the modules may be performed by the other modules.\n The knowledge base 110 is a collection of data, which may include structured and unstructured data.  In one embodiment, the knowledge base 110 is a graph database that stores a knowledge graph 120.  The knowledge graph 120 may be generated from\nan existing knowledge graph such as, but not limited to, Yago.TM.  and/or Freebase.TM..  Alternatively, the knowledge graph 120 may be generated from scratch.  The knowledge graph 120 comprises numerous entities 122 that have relationships or properties\n124.  Entities 122 represent real-world objects like people, places, and things.  For example, an entity 122 may be labeled \"car\" that has a property 124 labeled \"is-a\" which connects to another entity 122 labeled \"vehicle.\" In one embodiment, the\nentities 122 and properties 124 of the knowledge graph 120 are defined using Resource Description Framework (RDF).  RDF is a standard model for data interchange on the Web.  RDF extends the linking structure of the Web to use Uniform Resource Identifiers\n(URIs) to name the relationship (i.e., link or property 124) between things as well as the two ends of the link in the form of an RDF triple, which consist of a subject, predicate, and object.  For instance, in the above example, the subject is a car,\nthe predicate (i.e., property) is \"is-a\" which indicates that it is a type of, and the object is vehicle.  The same entity 122 may have multiple properties 124.  For example, the car entity may have another property labeled \"manufacturer\" that\nconnects/links to the car's manufacturer.  The car's manufacturer may have a property labeled \"cars produced\" that links to all the models of cars produced by that car manufacturer.  Thus, using the knowledge graph 120, someone searching for information\nabout a particular car, may also receive information about other cars produced by the same manufacturer.  As easily recognizable, the knowledge graph 120 may expand indefinitely to include all types of information.  As mentioned above, one aspect of the\ndisclosed embodiments is to determine what useful information (i.e., information that is most likely to be asked by a user) is missing from the knowledge graph 120.  Another aspect of the disclosed embodiments is to gather that missing information for\nautomatically expanding the relation inventory by adding new entities or relations in the knowledge graph 120.\n In one embodiment, the discovery module 130 is configured to receive a question from end user devices 150.  Based on the received questions, the discovery module 130 is configured to determine the information that is useful and missing from the\nknowledge graph 120.  In an embodiment, the discovery module 130 includes a parsing module 132 and a gap assessment module 134.\n The parsing module 132 is configured to parse a question to identify entity pairs (e.g., car and manufacturer) in the question.  In an embodiment, the parsing module 132 may be configured to determine the subject, predicate and object of the\nquestion in order to identify entity pairs.  In an embodiment, the parsing module 132 may also be configured to perform tokenization and segmentation, morpholexical analysis, semantic analysis, and syntactic analysis.  The parsing module 132 may also\ninclude a part-of-speech (POS) tagger to tag the parts of speech such as noun, verb, adjective, adverb, qualifier/intensifier, determiner, preposition, subordinating conjunction, and coordinating conjunction in order to identify entity pairs.\n The gap assessment module 134 is configured to receive the entity pairs and for each entity pair, determine if there is a knowledge gap between the entity pairs in the knowledge graph 120.  In one embodiment, a knowledge gap exists if there is\nless than a predetermined number of connection paths (N) between the two entities of the entity pair that are within a predetermined number (M) of hops (e.g., there may be only 2 connection paths between the two entities that are less than five hops). \nIn one embodiment, the values for N and M may be predetermined based on a statistical analysis of the knowledge graph 120 to identify the number of connection paths and hops between entities that provide the most useful knowledge.  In an embodiment, this\nstatistical analysis may be performed in real-time as the knowledge graph 120 is updated.  One purpose of the disclosed embodiments is to increase the number of connection paths between various entities and lower the number of hops between entities.  For\nentity pairs that have less than the predetermined number of connection paths between the two entities that are within the predetermined number of hops, the gap assessment module 134 passes the entity pair on to the game module 140.  In one embodiment,\nthis selection process produces two types of ideal entity pairs that may require additional information.  The first are the entity pairs that are distant/far away in the knowledge graph 120 as they are poorly related to each other.  The second are the\nentity pairs that have few or no relations as there is a lack of relationship information.  Other criteria for determining a knowledge gap in the knowledge graph 120 may also be suitable for use in the disclosed embodiments.\n The game module 140 receives the entity pair from the gap assessment module 134 and is configured to gather the missing information related to the entity pair for populating the knowledge graph 120.  The game module 140 may be implemented across\nany type of platform including personal computers, mobile devices using a mobile web interface or as a mobile application, and video gaming platforms.  In an embodiment, the game module 140 includes a user interface module 142, an answer validation\nmodule 144, an incentive module 146, and a knowledge graph update module 148.\n The user interface module 142 is configured to generate a user interface that enables a user of the end user devices 150 to provide information linking the entities in the entity pair.  In one embodiment, the user interface may provide a set\nnumber of predefined relations that a user may select to link the two entities.  For example, in an embodiment, the set number of predefined relations may include an indication that one entity is a theme of the other (e.g., heat is a theme of fire), that\nan entity is an agent of the other (e.g., a seamstress is the agent who performs a `sew` action), or that an entity is a subgroup of the other (e.g., apple is a fruit).  Additionally, the user interface may include a preset list of other existing\nentities in the knowledge graph 120 that a user may select as intermediate entities between the entity pairs.  The other existing entities in the preset list may be selected based on their relationships with one or both of the entities in the entity\npairs.  In an embodiment, the game module 140 may include a restriction list that includes one or more sets of entities and/or relations that may not be used to construct paths within the knowledge graph 120.  In other words, the user may be restricted\nfrom selecting a particular relationship between two entities and/or may be restricted from linking two entities together.  One purpose for the restriction may be because a particular relationship between two entities is well-known or obvious.  For\nexample, linking a \"person\" entity to a \"human being\" entity does not provide any useful information that would add to the information already existing in the knowledge graph 120.  Another reason for the restriction is that certain relations between two\nentities may be nonsensical.  Thus, by restricting certain relationships between certain entities and/or by restricting the pairing of certain entities, the game module 140 generates useful information that is both missing from the knowledge graph 120\nand likely to be requested from a user.  Additionally, the knowledge graph 120 does not become bloated with useless information.\n The answer validation module 144 is configured to validate a user's response.  In one embodiment, the user's response may be validated manually by another user (e.g., an administrator or another participant).  The user's response may also be\nvalidated automatically by comparing the user's answer to answers provided by other users to the same entity pair.  For example, in an embodiment, if the same answer is given by a predetermined number of users to an answer pair, the answer validation\nmodule 144 validates that answer as accurate.  Still, in an embodiment, a reliability rating may be assigned to a user based on responses provided by the user.  For instance, if a user has an A reliability rating, then in one embodiment, the user's\nresponse may automatically be accepted as a correct response or may require less user validation than a response provided by a user with a lower reliability rating.\n The incentive module 146 is configured to track the user's reliability rating or other point system for providing users with an incentive to participate in providing correct responses to the entity pair questions.  In an embodiment, users may\nacquire more points and/or higher ratings for factual knowledge rather than common sense knowledge.  For example, providing a particular location for place of birth would be a fact, whereas the knowledge that a person is born would be common sense\nknowledge.  Users may also acquire more points and/or higher ratings for better or more complex answers than simple ones.\n The point system or reliability rating may be used as a tracking mechanism simply for fun (e.g., a competition among friends) or may be used to provide the users with either monetary awards or other prizes.  For instance, if a user acquires\nenough points, a user may be awarded a particular monetary amount and/or may be awarded a particular prize.  If the disclosed embodiments are implemented internally within a corporation, the point system or reliability rating may also be used as part of\nan employee's review.  For example, the disclosed embodiments may be used internally by a company to enable employees of the company to crowdsource their knowledge for expanding the knowledge graph 120.  Employees may participate in the providing\nresponses during their off time or during breaks.  In some embodiments, the incentive module 146 may be configured to enable a group score to enable different departments of the company to compete.  Similarly, a group of friends or a random group may be\ncreated from the general public to enable crowdsourcing of knowledge to populate the knowledge graph 120.  Thus, the task of expanding the knowledge graph 120 becomes more like a game than actual work.\n The knowledge graph update module 148 is configured to update the knowledge graph 120 with the new entities and/or relations after they are validated.  In one embodiment, the knowledge graph update module 148 is configured to automatically\ngenerate an RDF triple or other knowledge graph insertion code containing the new entities and/or relations for modifying the knowledge graph 120.\n As depicted in FIG. 1, the knowledge base 110, the discovery module 130, the game module 140, and the one or more end user devices 150 communicate with each other through a communication link 102.  Communication link 102 may include both wired\nand wireless links.  Communication link 102 may be a direct link or may comprise of multiple links passing through one or more communication network devices such as, but not limited to, routers, firewalls, servers, and switches.  In some embodiments,\ncommunication link 102 may encompass various types of networks such as local-area networks (LANs), wide-area networks (WANs), and metropolitan-area networks (MANs).  These networks may include private networks and/or public networks such as the Internet. For instance, as described above, the game module 140 may be implemented on a Web server and access by any Internet enabled user device 150.  In an alternative embodiment, all the above described functions including those of the end user device 150 may\nbe incorporated into one system, in which case, the communication link 102 represents the internal communication bus of the system.\n As described herein, the end user device 150 is any type of electronic device that may be used by a user of the system 100 to ask a question and/or to provide missing information as described above.  Non-limiting examples of end user devices 150\ninclude a personal computer (desktop or laptop), mobile devices (e.g., personal digital assistant (PDA), smart phone, tablet), and dummy terminals that simply provide the interface for asking a question and/or for providing the missing information.  In\none embodiment, the same end user device 150 may be used for both tasks.  In some embodiments, only designated end user devices 150 may be capable of interacting with the discovery module 130 and/or the game module 140.\n FIG. 2 is a flowchart of a computer-implemented method 200 for improving a knowledge graph according to an embodiment of the present disclosure.  In an embodiment, the computer-implemented method 200 may be implemented by a processor as shown in\nFIG. 5 by executing instructions associated with the discovery module 130 and the game module 140 as described in FIG. 1.  In the depicted embodiment, the computer-implemented method 200 begins at step 202 by receiving a question from a user.  The\nquestion may be received directly from the user or from another tool or system such as IBM Watson Discovery Advisor.RTM..  At step 204, the computer-implemented method 200 parses the question to identify entity pairs in the question.  For example, in an\nembodiment, the computer-implemented method 200 identifies at least the subject/focus of the question and the object of the question as an entity pair.  A question may include more than one subject and/or object.  In an embodiment, the\ncomputer-implemented method 200 may generate entity pairs using synonyms for the subject or object of the question.  As an example, the question may be \"Which doorknob manufacturers are located in Croatia?\" In this example, the computer-implemented\nmethod 200 may identify doorknob and Croatia as an entity pair.  Using this example, at step 206, the computer-implemented method 200 determines if there is a knowledge gap between the entity pairs by querying the knowledge graph 120 for paths between\ndoorknob and Croatia.  In one embodiment, the computer-implemented method 200 determines that a knowledge gap exists if there are less than a predetermined number of connection paths between doorknob and Croatia that are within a predetermined number of\nhops.  For example, in one embodiment, if the computer-implemented method 200 determines that there is not one connection path having less than five hops between doorknob and Croatia, then the computer-implemented method 200 determines that a knowledge\ngap exists.  For each entity pair that the computer-implemented method 200 determines has a knowledge gap, the computer-implemented method 200 at step 208 is configured to perform corrective action to fill in the knowledge gap in the knowledge graph 120.\n FIG. 3 is a flowchart of a computer-implemented method 300 for performing corrective action to fill in a knowledge gap according to an embodiment of the present disclosure.  In an embodiment, the computer-implemented method 300 may be\nimplemented by a processor as shown in FIG. 5 by executing instructions associated with the game module 140 as described in FIG. 1.  The computer-implemented method 300 begins at step 302 by receiving an entity pair that has been determined to have a\nknowledge gap.  At step 304, the computer-implemented method 300 generates a user interface for enabling a user to link the entities in the entity pair.  In one embodiment, the computer-implemented method 300 retrieves a set of predefined relations\nand/or other existing entities in the knowledge graph 120 that a user may select to link the two entities.  In an embodiment, the user interface may limit the user input to a certain number of hops/connections.  Additionally, in some embodiments, the\nuser interface may restrict the user input from selecting particular entities and/or relationships from being used to force the user to provide information that is both missing from the knowledge graph 120 and would be useful to include in the knowledge\ngraph 120.\n At step 306, the computer-implemented method 300 receives the user's response from the entity pair user interface.  The computer-implemented method 300 validates the user's response at step 308.  In one embodiment, the computer-implemented\nmethod 300 validates the user's response by comparing the user's answer to the answers provided by other users to same entity pair.  In an embodiment, if the same or similar response is provided by a predetermined number of users, the response is\nconsidered validated.  Alternatively, the computer-implemented method 300 may provide the user's response to another user or users and request that the other user(s) verify the user's response.  In one embodiment, in order for a user to play, the user\nmust first validate another user's response.\n At step 310, the computer-implemented method 300 updates the user's score or points based on the user's input.  In one embodiment, the score may be based on the usefulness and/or accuracy of the user's response.  Other non-limiting criteria may\ninclude length and/or use of esoteric graph entities.  In one embodiment, points may be deducted from a user's score for undesirable answers and/or if it is determined that a user is not properly validating another user's response.  In an embodiment, the\ncomputer-implemented method 300 may also update a leaderboard based on the updated scores.  If the user's response has been validated, the computer-implemented method 300 updates the knowledge graph 120 at step 312.  In an embodiment, the\ncomputer-implemented method 300 automatically generates an RDF triple or other knowledge graph insertion code containing the new entities and/or relations for modifying the knowledge graph 120.\n FIG. 4 is an example of a user interface 400 for gathering information for improving a knowledge graph according to an embodiment of the present disclosure.  In an embodiment, the user interface 400 may be implemented by a processor as shown in\nFIG. 5 by executing instructions associated with the game module 140 as described in FIG. 1.  In the depicted embodiment, the user interface 400 request user information between entity pair \"sapling\" 402 and \"space shuttle\" 408.  The user interface 400\nmay also include one or more sets of relationships 404 and entities 406 that a user may select from to build a path between entity pair \"sapling\" 402 and \"space shuttle\" 408.  In one embodiment, the user interface 400 may include a button, not shown,\nthat enables a user to add an additional entity/relationship to the user interface for generating longer paths.  In an embodiment, the user may also have the option to add new entities and/or relationships that are not part of the predefined\nrelationships 404 and entities 406 sets.  Using the given example, the user may first select from relationships 404 \"is-a .fwdarw.\" and \"tree\" from entity list 406 to connect sapling 402 to tree.  Continuing on, the user may select \"theme-of .fwdarw.\" to\nconnect \"tree\" to the entity \"burn\".  The user again may then select \"theme-of .fwdarw.\" to connect \"burn\" to the entity \"rocket fuel.\" Finally, the user may select the relationship \"part of\" to connect \"rocket fuel\" to \"space shuttle\" to complete the\npath from \"sapling\" 402 to \"space shuttle\" 408 (i.e., Sapling-is-a.fwdarw.Tree-theme-of.fwdarw.burn-theme-of.fwdarw.rocket fuel-part-of.fwdarw.space shuttle).  Other variations of the user interface 400 may be employed in accordance with the disclosed\nembodiments.  For example, in one embodiment, the user interface 400 may employ a graph or tree like structure for enabling a user build a path between two entities.\n FIG. 5 is a block diagram of an example data processing system in which aspects of the illustrative embodiments may be implemented.  Data processing system 500 is a simplistic example of a computer that can be applied to implement the processes\nof the disclosed embodiments.  However, the disclosed embodiments may also be implemented in very advance systems such as an IBM.RTM.  Power 750 servers or the IBM Watson.RTM.  supercomputer, which employs a cluster of ninety IBM Power 750 servers, each\nof which uses a 3.5 GHz POWER7 eight-core processor, with four threads per core.\n In the depicted example, data processing system 500 employs a hub architecture including north bridge and memory controller hub (NB/MCH) 506 and south bridge and input/output (I/O) controller hub (SB/ICH) 510.  Processor(s) 502, main memory 504,\nand graphics processor 508 are connected to NB/MCH 506.  Graphics processor 508 may be connected to NB/MCH 506 through an accelerated graphics port (AGP).  A computer bus, such as bus 532 or bus 534, may be implemented using any type of communication\nfabric or architecture that provides for a transfer of data between different components or devices attached to the fabric or architecture.\n In the depicted example, LAN adapter 516 connects to SB/ICH 510.  Audio adapter 530, keyboard and mouse adapter 522, modem 524, read-only memory (ROM) 526, hard disk drive (HDD) 512, compact disk read-only memory (CD-ROM) drive 514, universal\nserial bus (USB) ports and other communication ports 518, and peripheral component interconnect/peripheral component interconnect express (PCI/PCIe) devices 520 connect to SB/ICH 510 through bus 532 and bus 534.  PCI/PCIe devices may include, for\nexample, Ethernet adapters, add-in cards, and PC cards for notebook computers.  PCI uses a card bus controller, while PCIe does not.  ROM 526 may be, for example, a flash basic input/output system (BIOS).  Modem 524 or network adapter 516 may be used to\ntransmit and receive data over a network.\n HDD 512 and CD-ROM drive 514 connect to SB/ICH 510 through bus 534.  HDD 512 and CD-ROM drive 514 may use, for example, an integrated drive electronics (IDE) or serial advanced technology attachment (SATA) interface.  Super I/O (SIO) device 528\nmay be connected to SB/ICH 510.  In some embodiments, HDD 512 may be replaced by other forms of data storage devices including, but not limited to, solid-state drives (SSDs).\n An operating system runs on processor(s) 502.  The operating system coordinates and provides control of various components within the data processing system 500 in FIG. 5.  Non-limiting examples of operating systems include the Advanced\nInteractive Executive (AIX.RTM.) operating system or the Linux.RTM.  operating system.  Various applications and services may run in conjunction with the operating system.  For example, in one embodiment, International Business Machines (IBM).RTM. \nDeepQA software, which is designed for information retrieval that incorporates natural language processing and machine learning, is executed on data processing system 500.\n Data processing system 500 may include a single processor 502 or may include a plurality of processors 502.  Additionally, processor(s) 502 may have multiple cores.  For example, in one embodiment, data processing system 500 may employ a large\nnumber of processors 502 that include hundreds or thousands of processor cores.  In some embodiments, the processors 502 may be configured to perform a set of coordinated computations in parallel.\n Instructions for the operating system, applications, and other data are located on storage devices, such as one or more HDD 512, and may be loaded into main memory 504 for execution by processor(s) 502.  In certain embodiments, HDD 512 may\ninclude a knowledge graph in the form of a triplestore or RDF store for the storage and retrieval of RDF triples through semantic queries.  In some embodiments, additional instructions or data may be stored on one or more external devices.  The processes\nfor illustrative embodiments of the present invention may be performed by processor(s) 502 using computer usable program code, which may be located in a memory such as, for example, main memory 504, ROM 526, or in one or more peripheral devices 512 and\n514.\n The present invention may be a system, a method, and/or a computer program product at any possible technical detail level of integration.  The computer program product may include a computer readable storage medium (or media) having computer\nreadable program instructions thereon for causing a processor to carry out aspects of the present invention.\n The computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device.  The computer readable storage medium may be, for example, but is not limited to, an electronic\nstorage device, a magnetic storage device, an optical storage device, an electromagnetic storage device, a semiconductor storage device, or any suitable combination of the foregoing.  A non-exhaustive list of more specific examples of the computer\nreadable storage medium includes the following: a portable computer diskette, a hard disk, a random-access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a static random access memory (SRAM), a\nportable compact disc read-only memory (CD-ROM), a digital versatile disk (DVD), a memory stick, a floppy disk, a mechanically encoded device such as punch-cards or raised structures in a groove having instructions recorded thereon, and any suitable\ncombination of the foregoing.  A computer readable storage medium, as used herein, is not to be construed as being transitory signals per se, such as radio waves or other freely propagating electromagnetic waves, electromagnetic waves propagating through\na waveguide or other transmission media (e.g., light pulses passing through a fiber-optic cable), or electrical signals transmitted through a wire.\n Computer readable program instructions described herein can be downloaded to respective computing/processing devices from a computer readable storage medium or to an external computer or external storage device via a network, for example, the\nInternet, a local area network, a wide area network and/or a wireless network.  The network may comprise copper transmission cables, optical transmission fibers, wireless transmission, routers, firewalls, switches, gateway computers and/or edge servers. \nA network adapter card or network interface in each computing/processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium\nwithin the respective computing/processing device.\n Computer readable program instructions for carrying out operations of the present invention may be assembler instructions, instruction-set-architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, firmware\ninstructions, state-setting data, configuration data for integrated circuitry, or either source code or object code written in any combination of one or more programming languages, including an object oriented programming language such as Smalltalk, C++,\nor the like, and procedural programming languages, such as the \"C\" programming language or similar programming languages.  The computer readable program instructions may execute entirely on the user's computer, partly on the user's computer, as a\nstand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server.  In the latter scenario, the remote computer may be connected to the user's computer through any type of network,\nincluding a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).  In some embodiments, electronic circuitry including, for\nexample, programmable logic circuitry, field-programmable gate arrays (FPGA), or programmable logic arrays (PLA) may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to\npersonalize the electronic circuitry, in order to perform aspects of the present invention.\n Aspects of the present invention are described herein with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems), and computer program products according to embodiments of the invention.  It will be\nunderstood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer readable program instructions.\n These computer readable program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute\nvia the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.  These computer readable program instructions may also be\nstored in a computer readable storage medium that can direct a computer, a programmable data processing apparatus, and/or other devices to function in a particular manner, such that the computer readable storage medium having instructions stored therein\ncomprises an article of manufacture including instructions which implement aspects of the function/act specified in the flowchart and/or block diagram block or blocks.\n The computer readable program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other device to cause a series of operational steps to be performed on the computer, other programmable apparatus or\nother device to produce a computer implemented method, such that the instructions which execute on the computer, other programmable apparatus, or other device implement the functions/acts specified in the flowchart and/or block diagram block or blocks.\n The flowchart and block diagrams in the figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present invention. \nIn this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of instructions, which comprises one or more executable instructions for implementing the specified logical function(s).  In some alternative\nimplementations, the functions noted in the blocks may occur out of the order noted in the Figures.  For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse\norder, depending upon the functionality involved.  It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special\npurpose hardware-based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.\n It should be apparent from the foregoing that the disclosed embodiments have significant advantages over current art.  As an example, the disclosed embodiments utilize actual questions to determine the useful and missing information from a\nknowledge graph.  Thus, the knowledge graph is not filled simply with irrelevant information.  This improves the efficiency of the system because the system is able to perform a query faster on a smaller knowledge graph than a large one, and at the same\ntime, the knowledge graph should include the desired information.  Another advantage is that the disclosed embodiments utilize a game implementation that benefits from various users contributing information to the knowledge graph in a fun way.  Compared\nto other knowledge-extension techniques, the human labor expense is greatly reduced with the disclosed embodiments.  Additionally, knowledge can be gathered cheaply from people with more widely available technical skills.  Moreover, the symbiotic\nrelationship between the discovery tool and the game platform creates a virtuous cycle where the use of the discovery tool improves the game platform by providing puzzle content, and the use of the game platform in turn improves the discovery tool by\nincreasing the coverage of the knowledge graph.\n The descriptions of the various embodiments of the present invention have been presented for purposes of illustration, but are not intended to be exhaustive or limited to the embodiments disclosed.  Many modifications and variations will be\napparent to those of ordinary skill in the art without departing from the scope and spirit of the described embodiments.  Further, the steps of the methods described herein may be carried out in any suitable order, or simultaneously where appropriate. \nThe terminology used herein was chosen to best explain the principles of the embodiments, the practical application or technical improvement over technologies found in the marketplace, or to enable others of ordinary skill in the art to understand the\nembodiments disclosed herein.", "application_number": "15413103", "abstract": " The disclosed embodiments include a system, computer program product, and\n     computer-implemented method configured to modify a knowledge graph. The\n     disclosed embodiments seek to improve the process of expanding a\n     knowledge graph by identifying information that is useful and relevant to\n     the needs of the users based on questions being asked by users. In one\n     embodiment, the information is collected using a gaming platform. The\n     disclosed embodiments augment the knowledge graph, by adding new entities\n     and/or relationships, based on the gathered information.\n", "citations": ["7555472", "8407253", "9832159", "20140280307", "20140297644", "20140379755", "20150074034", "20150363461", "20160098645", "20160224637", "20160314121", "20170262551"], "related": []}, {"id": "20180276261", "patent_code": "10331644", "patent_name": "Process and framework for facilitating information sharing using a\n     distributed hypergraph", "year": "2019", "inventor_and_country_data": " Inventors: \nSmart; J. Cory (Clarksville, MD)  ", "description": "BACKGROUND OF THE EMBODIMENTS\nField of the Embodiments\n Embodiments of the invention relate to the field of large-scale, multi-source data sharing.  More particularly, embodiments of the invention relate to an infrastructure for facilitating data sharing and analysis across disparate data sources in\na secure, distributed process.\nDescription of Existing Art and Identification of Technical Problem\n Our world is a complex place.  It is faced with many difficult, interdependent problems; some caused by nature, others caused by man.  As individuals, families, communities, and nations, we face an ever changing and compounding series of\nperplexing challenges spanning numerous domains: defense, health, climate, food, cyber, energy, transportation, education, weather, the economy.  Compounding pressures in each of these areas threaten our health, our safety, our security, our livelihood,\nand our sustainability.  We seek improved capabilities to detect, understand, mitigate, and prevent our brave new world of threats.  To address these challenges, we invariably resort to science, our systematic enterprise for building and organizing\nknowledge that helps us understand, explain, and predict our world around us.  At the core of science is inquiry.  We formulate questions.  We generate hypotheses.  We predict consequences.  We experiment.  We analyze.  We evaluate.  We repeat.  Our\nproblems are complex; the process is slow.\n Fueling the scientific process are the observations we make and the data we collect.  With the advent of the 21st century telecommunications explosion, data is now flowing and evolving all around us in massive volumes, with countless new\nstreams, mixing and shifting each minute.  This data space is enormous and continuously changing.  And by many accounts, its expansion and movement has only just begun.  Analyzing and understanding this vast new ocean of data is now of paramount\nimportance to addressing many of the complex challenges facing our world.\n Today's data analytic industry is vibrant with a continuous supply of new and innovative products, services, and techniques that thrive and prosper based on their relative merits in the respective marketplaces.  Unfortunately, these components\nare rarely interoperable at any appreciable scale.  Moreover, the rapid proliferation of analytic tools has further compounded the problem.  With only loose coordination, these partial solutions are ineffective at combating the broad spectrum of\nproblems.  Attempting to impose a \"one-size-fits-all\" analytic solution, however, across today's tremendous data expanse poses significant scientific, technical, social, political, and economic concerns.  Consequently, an enormous amount of resources\nmust regularly be expended to address isolated issues and mitigate specific threats.  Thus, the analytic community faces considerable challenges dealing with major classes of problems-particularly those at national and international levels.\n Specifically, data and analytics collaborators often adopt unique trust relationships with data source owners, evolve unique analytic approaches, use a variety of visualization systems, and leverage a diversity of analytic platforms and tools. \nManaging a shared knowledge space that is centrally located requires all transactions between these items to flow into a single site and then flow back out, creating a bottleneck and a single-point of failure.  The loss or deterioration of the central\npoint's resources implies the loss or deterioration of the entire knowledge space.  Replicating the knowledge space with multiple sites serving as mirrors and/or backups leads to unnecessary duplication, complex interfaces, large data movement, and\ncomplicated synchronization, privacy, and security policies.  Institutionally, such alternatives invariably require organizations to commit to a structure over which some may have little control, whilst placing greater operational burden, responsibility,\nand control on others.  Balancing all these factors invariably leads to difficult negotiations involving data ownership, knowledge curation, organizational autonomy, and research independence.  Accommodating the continuous flood of new and ever-changing\ndata, theories, and interpretations also requires a dynamic knowledge space, further challenging a centralized design.\n Accordingly, there is a need for a solution that addresses numerous issues standing in the way of sharing data for analytics on a global scale.  These issues include, for example: the massive logistics problem with attempting to integrate\nthousands of government/non-government data systems at scale when the systems have different standards, models, security, infrastructure, procedures, policies, networks, access, compartments, applications, tools, protocols, and the like; the increased\nsecurity risk that follows large-scale integration of data resources; the lack of analytic algorithm techniques to automatically detect data patterns and provide alerts, i.e., the means to transition from \"analytic dumpster diving\" to early-warning\nindication and real-time notification; and the privacy tensions between security and liberty.\nSUMMARY OF THE EMBODIMENTS\n The present embodiments describe an approach for organizing and analyzing our new data-centric world, rapidly sharing information derived from that data, unifying this information into a common global knowledge framework, and enabling scientific\nanalysis at a scale never before possible.  The approach is a transformative, multi-disciplinary approach to collaborative global-scale integrative research.  This approach offers a new method for dramatically reducing complexity and accelerating the\nscientific discovery process.  This approach was specifically formulated to address extremely complex problems related to global security, world health, and planet sustainability.  The approach advocates the construction of a unified global systems\nmodel.  At the core of its design is an extensible knowledge representation architecture specifically crafted for large-scale, high-performance, real-time scientific analysis of heterogeneous, dynamic and widely distributed data sources without\ncompromise of data security, safety, or privacy.  By way of example, the approach may be applied for SIMPLEX Knowledge Representation as discussed herein.\n The approach employs a powerful integration of advanced concepts and emerging capabilities selected from across the government, commercial, laboratory, and academic sectors.  The resulting framework offers participants a pragmatic means to think\nglobally, leveraging the aggregate of combined knowledge to further science, and better inform local, regional and international decision making.  This approach exploits the uniqueness of heterogeneous data sources and diverse scientific theories,\nweaving all such points together into a powerful collaborative fabric.  Great care, however, must be taken with this stitching process.  The movement and replication of data is costly and risks inappropriate information disclosure and/or compromise.  The\nfarther data flows from its source, and the more information that is aggregated by an increasing number of parties, the greater the security and privacy concerns and accompanying loss of autonomy.  As described herein, the approach addresses these\nconcerns in its foundational design tenets.\n The primary goal of this approach is to achieve a computational science analog to self-sustained nuclear fission.  That is, the approach advocates a method for reaching a critical knowledge (mass) density that is capable of sustaining an\nunprecedented analytic (energy) yield.  To achieve such high levels, the approach grapples with scale right from the onset, encouraging knowledge enrichment \"downward\" in addition to traditional data pipeline scaling \"upward\".  To this end, the approach\nincludes the construction of a knowledge representation structure that spans the planet and numerous of its complex, interdependent subsystems.  The construction of this structure is accomplished via carefully formulated transformations of the world's\nexponentially increasing data into an asymptotically limited information space.  This technique enables global-scale computational tractability, promising a reduction in integration time and cost from quadratic down to linear in the number of source data\nsystems.  Thus, the analytic framework offered provides a practical, achievable means for accomplishing multi-disciplinary research across many diverse, complex and often heavily interdependent domains.  The results of this work offer a conceptually\nsimple yet elegant method for the scientific community to manage complexity across many heterogeneous domains at a scale never before possible.\n The solution described herein prescribes an innovative method for achieving the above-identified objectives through the construction of a knowledge model \"overlay\" for organizing and analyzing large, dynamic data volumes (e.g. the World-Wide\nWeb).  This overlay is implemented as a hypergraph that manifests as a result of a distributed theory-driven data source transformation process.  This process maps exponentially growing data into an asymptotically limited information space.  Within this\nspace, real-world entities (e.g. people, computers, cities, Earth) and their complex interdependencies (e.g. social networks, connectivity, causal relationships) are represented as attributed hypergraph elements (i.e. both hypervertices and hyperedges). \nAttributes are represented as state vectors affixed to any hypergraph element.  Transformation of source system data into this overlay structure is accomplished with minimal data movement and replication.  Rather, the overlay structure is populated using\na universal \"pointer\" like mechanism that is managed in a decentralized fashion by the respective transformation components.  Access to the knowledge overlay is performed via a hypergraph communication protocol.  This protocol was specifically designed\nwith formal mathematical verification in mind to enable extremely robust security and \"black box\" privacy isolation techniques.  This protocol is encapsulated within a very simple, common hypergraph API and accompanying hypergraph toolkit that shield the\nscientific research community from much of the implementation engineering details.\n The solution supports both qualitative and quantitative knowledge, but makes a distinction between knowledge that characterizes the temporal state of a system versus that which defines the dynamics of its state changes.  The former is\nrepresented within the knowledge hypergraph structure, while the latter is embodied within the surrounding analytic components that produce and consume this structure.  The solution was specifically formulated to accommodate an enormous diversity of\ndata.  The scheme discourages data movement beyond the boundaries of its system of origin.  To achieve this objective, source data is fundamentally treated as scientific observations or measurements of some set of interdependent subsystems (e.g. an\nindividual, cyberspace, Earth).  The transformation components are used to map source system data into the knowledge overlay structure, thus minimizing data replication and movement.  Except when cached for performance reasons, data largely remains at\nrest in its origin system and in its original structure and form.  This approach discourages notions of a universal data representation or universal data federation as an effective means to complexity reduction.  Rather, it advocates that data be acted\nupon locally with transformation components that map sources into a shared knowledge model.\n Modularity and scalability are at the core of the preferred solution.  The development of this design was driven by pragmatic engineering constraints to ensure practical implementation and performance, given current and emerging computational\nresources and technology.  Although domain-agnostic, the approach is directed at hypergraph representation of macroscopic entities and their relationships (e.g. people, cities, social networks, cyberspace) in contrast to the comparatively unbounded data\nworld of microscopic structures.  Microscopic-level information can, however, be affixed to hypergraph elements as attribute state vectors.  In addition, the solution does allow for recursive application of itself.  The solution can be applied to\ndiscrete micro-level sub-systems that in turn can be integrated into a single larger macro-level system via hierarchical application of its principles.\n While domain-agnostic, the utility of this approach is highly dependent upon the existence of a unified (but extensible) ontological framework that formally defines a system's knowledge model structure.  Historically, without taking careful\nprecautions, ontology development can evolve into a horribly nightmarish resolution process.  However, by constraining the knowledge model scope to real-world entities that obey an asymptotically limited discovery/growth profile (e.g. people,\norganizations, cellphones), this process can be dramatically simplified.  The solution thus prescribes a flexible ontological process that balances entity/relationship type specification with attribute type extension.  In this setting, the incorporation\nof multiple domains/use cases is encouraged very early on as the distillation of this balance can be accelerated considerably.\n In a first implementation, a process for organizing data from multiple heterogeneous data sources in a knowledge graph, the knowledge hypergraph being accessible by multiple unrelated analytics, is described.  The process includes: receiving a\nfirst data set from a first data source at a first of multiple hypergraph servers; graphing by the first hypergraph server first data from the first data set in a knowledge hypergraph in accordance with a knowledge model (M), the knowledge hypergraph\nbeing defined by hypergraph elements including hypervertices and hyperedges, wherein attributes of the first data are represented in the hypergraph by first state vectors affixed to one or more hypergraph elements; receiving a second data set from a\nsecond data source at a second of multiple hypergraph servers; graphing by the second hypergraph server second data from the second data set in the hypergraph, wherein attributes of the second data are represented in the hypergraph by second state\nvectors affixed to one or more hypergraph elements; further wherein at least one of the first state vectors and at least one of the second state vectors is affixed to the same hypergraph element; receiving at the first hypergraph server from a first\nanalytic a search request directed to the hypergraph; and querying by the first hypergraph server the hypergraph, wherein query results include attributes from both the first data and the second data.\n In a second implementation, a process for requesting information from a distributed knowledge graph, is described.  The process includes: formulating, at a first adapter, a query directed to a first hypergraph element of the distributed\nknowledge graph using a common hypergraph application programming interface for processing by a first hypergraph server associated with the first adapter, the first hypergraph server being programmed to receive queries in a format of the common\nhypergraph application programming interface and further being programmed to communicate with other hypergraph servers using a defined hypergraph transfer protocol (HGTP); packaging, by the first hypergraph server, the query into a HGTP query message and\nforwarding the query message to a second hypergraph server in accordance with first location instructions for the first hypergraph element at a second adapter; receiving, by the second hypergraph server, the packaged query message, un-packaging the\npackaged query message and relaying the query to the second adapter; retrieving results to the query from the associated first hypergraph element by the second adapter and returning the results to the second hypergraph server using the common hypergraph\napplication programming interface; and packaging, by the second hypergraph server, the results into a HGTP results message and forwarding the results message to the first hypergraph server in accordance with second location instructions for the first\nadapter associated therewith.\n In a third implementation, a framework to provide information sharing and analysis over a knowledge hypergraph is described.  The framework includes: a first layer including multiple data sources, wherein the multiple data sources include at\nleast one structured data source and at least one unstructured data source; a second layer, including multiple servers and multiple associated adapters, for transforming structured and unstructured data from the multiple data sources for use by a third\nlayer and further wherein the multiple data sources, multiple servers and multiple associated adapters are accessed using a common hypergraph application programming interface; a third layer including the knowledge hypergraph comprised of hypergraph\nelements formed from the transformed structured and unstructured data, the hypergraph elements being distributed over the multiple servers and multiple associate adapters and being accessible from server to server using a defined hypergraph transfer\nprotocol (HGTP); and a fourth layer including accessing components for initiating queries to and receiving results from the knowledge hypergraph using the common hypergraph application programming interface. SUMMARY OF THE FIGURES\n The Summary of the Embodiments, as well as the following Detailed Description, are best understood when read in conjunction with the following exemplary drawings:\n FIG. 1 is a first schematic of the framework described herein to implement preferred embodiments;\n FIG. 2 graphs the nature of knowledge aggregation;\n FIG. 3 is a second schematic of the framework described herein to implement preferred embodiments;\n FIG. 4 is a schematic illustrating analytic components of the framework described herein;\n FIG. 5 illustrates hypergraph elements in accordance with the embodiments described herein;\n FIG. 6 is a schematic of an architectural framework in accordance with the embodiments described herein;\n FIG. 7 is a hypergraph virtual representation in accordance with the embodiments described herein;\n FIG. 8 a hypergraph physical instantiation schematic in accordance with the embodiments described herein;\n FIG. 9 is a sample hypergraph query in accordance with the embodiments described herein;\n FIG. 10 is a sample probabilistic hypergraph query in accordance with the embodiments described herein;\n FIG. 11 is a schematic illustrating a privacy assurance process usable within the framework described herein;\n FIG. 12 is a schematic illustrating a Black Box usable within the framework described herein;\n FIG. 13 is a schematic showing components of Layer A of the framework described herein;\n FIG. 14 is a schematic showing components of Layer B of the framework described herein;\n FIGS. 15a-15c illustrate a first exemplary privacy assurance block box use case described herein;\n FIG. 16 is an exemplary logic statement (pattern) specified as a graph for input to the black box in accordance with first use case of FIG. 15a-15c;\n FIGS. 17a-17c illustrate a second exemplary privacy assurance block box use case described herein;\n FIG. 18a illustrates an exemplary logic statement (pattern) specified as a graph for input to the black box in accordance with second use case of FIG. 17a-17c; and\n FIG. 18b illustrates an exemplary output from the black box in accordance with second use case of FIG. 17a-17c.\nDETAILED DESCRIPTION\n The knowledge-share infrastructure framework identifies four fundamental layers of technology, depicted in FIG. 1.  These four layers each serve a critical core function.  In unison, these functions work together to provide an end-to-end\nsolution for extreme-scale information sharing and analysis.  These four specific layers were very carefully formulated from fundamental computational science principles to ensure rapid and open innovation and leverage capabilities proven within other\ndomains.\n Layer A \"Source Data Systems\"--This layer comprises a complex system's (e.g. our world's) vast, complex, heterogeneous collection of raw, processed, correlated, and uncorrelated data spanning public, private, commercial, and government\nsystems--including databases, repositories, files, the web, archives, live feeds, real-time control systems, etc.--highly distributed throughout an enterprise, region, nation, or the planet.\n Layer B \"Transformation & Isolation\"--This layer provides a critical transformation of Layer A source data into the Layer C knowledge model.  This transformation is guided by domain theory and the Layer C ontology.  In addition, Layer B enables\na robust security and privacy isolation barrier that is extremely difficult to physically or electronically defeat.  This boundary enables distributed Layer A systems to interoperate at global scale without requiring centralized management.\n Layer C \"Information Sharing, Knowledge Representation and Analysis\"--This layer provides a unification of the many Layer A systems beneath, offering a distributive, high security/privacy-assured fabric for sharing information, representing\nknowledge, and performing collaborative global-scale analysis.  This layer is structured as an extremely large, decentralized formal knowledge hypergraph.\n Layer D \"The User Experience\"--This layer interacts directly with researchers, scientists, users, and operators, providing visualization applications, decision support functions, situational awareness services, prediction and mitigation tools,\nlarge-scale simulation, alerting and notification, and command and control capabilities, each tunable to the particular domain application.\n The instantiation of this framework described herein employs a parallel collection of data space to information space transformations.  While data growth is essentially unbounded, the rate at which physical world entities characterized by this\ndata are revealed (discovered) typically asymptotically approaches a finite upper bound.  That is, while the data flood may never subside, its flow describes a relatively finite number of objects in our physically constrained world (e.g. people,\norganizations, computers, phones, cars, cities, roads, power lines, etc.) statistically discovered and characterized at an asymptotic rate.  The quantity of such entities may seem large, however, the upper bound on the number of such items of analytic\ninterest is now computationally manageable in the new \"Big Data\" era (e.g. 7 billion people, 2 billion smart-phones, 40,000 cities, 11 million miles of paved roads).  One of the key concepts embodied in the approach described herein is the transformation\nof data into an information space that is naturally limited by these finite aspects of the planet.  The approach is fundamentally designed as a global system representation technique.\n While data growth has been proceeding on an exponential path, the preserved volume of data over time d(t) must ultimately be linearly limited by the maximum physical data collection rate at the sources multiplied by the filtering rate at which\ndata must be discarded due to finite storage requirements, or d(t).fwdarw.c*(t-t.sub.0)*f where c=the collection rate and f=the filtering rate.  To keep up, current data processing practice must either increase the storage limit or increase the filtering\nrate.  Unfortunately, neither option is attractive.  A continuous increase in storage implies a continue increase in cost (and associated power, space, and cooling); discarding data implies an irretrievable loss of an important asset.  The asymptotic\nnature of information discovery holds the key.  That is, for each data source, the discovery of \"information\" over time can be calculated via i(t)=(1-e.sup.-t/T) where n.sub.0 is represents the entity population limit and T is the \"time constant\" of the\nsource with T=n.sub.0/r where r=the information entity collection rate.  Interestingly, each of these quantities can be measured and/or calculated for every data source, providing key architectural metrics.\n A large segment of analysis is dedicated to understanding real-world (or virtual world) entities and their complex interrelationships and dependencies.  In computer science jargon, a representation of this understanding is frequently referred to\nas computational knowledge.  The framework was specifically formulated to aid the representation and capture of such knowledge.  This is most invaluable, although perhaps not immediately obvious.  Recall the asymptotic behavior of Information discovery. \nAlthough the time `constants` may actually vary over extended periods, observations of specific types of entity relationships exhibit this same asymptotic behavior.  Of keen analytic interest is when the entities and their relationships are aggregated\ninto a single construct.  The power of this knowledge aggregation process is now a fairly widely known and frequently applied, leveraging basic graph theory (i.e., loosely speaking, the mathematical study of node/link structures).  The framework\ndescribed herein applies this process at global scale to aid the creation of a knowledge base with world coverage.  The estimated upper bound of such a knowledge base equates roughly to a graph of approximately one trillion nodes and one quadrillion\nlinks.\n The utility of a knowledge representation of our world would be extraordinary.  Knowledge aggregation (in contrast to data aggregation) is an extremely powerful method for enabling a significant boost to analytic yield.  In simple terms,\nanalytic yield can be loosely described as the amount of analytic \"work\" that can be accomplished during a fixed duration of time.  While analytic work performed by a person is generally considerably richer than that of a machine, a computer is\nparticularly adept at examining exceptionally large volumes of entities and relationships for specific patterns.  When knowledge is aggregated in this fashion, analytic yield increases significantly once a \"critical mass\" is reached, as shown in FIG. 2. \nAchieving this high analytic yield is the primary goal of the framework described herein.\n These notions are calculable, providing two important quantitative framework metrics.  The knowledge density (.delta.) of an information space characterizes its connectedness and can be determined by the formula:\n .delta..times..lamda.  ##EQU00001## where |V| is the number nodes (entities), |E| is the number of links (relationships).  The exponent .lamda.  in the denominator is used for normalization purposes depending upon the domain since |E| is\npotentially O(n.sup.2) larger than |V|.  An information space with .delta.=1 implies a fully connected knowledge base; .delta.=0 is fully disconnected.\n The notion of analytic yield characterizes the number of computational inferences that an analytic engine could potentially perform given a specific knowledge base.  Analytic yield is thus likened to potential energy (or work) in physics and can\nbe computed via the formula:\n .apprxeq..function..times..times..times..times.  ##EQU00002## where H.sub.i is the i'th connected component of the m-connected graph G=(V,E) that represents the knowledge base.\n The key milestone of a framework instantiation is to achieve a \"critical mass\" density for a given set of data sources such that the analytic yield is at least, say, 10.times.  over conventional data-centric processing approaches.  Used in\ncombination, these metrics can be incorporated into an overall integration analytic power rating (p) to compare performance and effectiveness of framework instantiations and competing architectural approaches:\n ##EQU00003##\n The solution described herein provides a transformative, unifying approach for managing system complexity to enable collaborative, global-scale integrative research for accelerating multi-disciplinary scientific discovery.  It accomplishes this\nby abandoning the traditional pipeline/waterfall approach to data-information-knowledge processing.  Rather, it prescribes a distributed analytic ecosystem for examining data, communicating information, and discovering, representing, and enriching shared\nknowledge.  The approach begins with the establishment of a conceptual boundary to whatever it is applied.  This target system may be a microscopic organism, the human body, a community, a continent, or the planet Earth.\n Within a system boundary, data is regarded as a collection of observations about this system.  That is, given a complex system S, the (generally unbounded) set of data D={d.sub.1, d.sub.2, d.sub.3, .  . . } contains data about S where each\nd.sub.i.di-elect cons.D is viewed as a \"scientific measurement\" of some aspect of S. For example, an element d may be spectral decomposition of a chemical, a gene sequence of an organism, an e-mail message of an individual, the text of community's legal\nstatute, or the seismic motion of a tectonic plate.  When viewed in this manner, associated with every datum d is a measurement precision, measurement accuracy, and a frame of reference (e.g. spatial-temporal perspective) in which the measurement was\nmade (i.e. data was collected).  These items may be either explicitly stated or implicitly assumed, and may not always be imminently calculable, or perhaps even practically knowable.  This treatment, however, is important so that subsequent data\ncollection and analysis about whatever system is being examined rests on a scientific foundation versus simply conjecture or subjectivity.\n For a system S, the solution next prescribes the notion of a model M for S. The model M is a knowledge representation of system S, which may likely be time varying and dynamic depending upon the complexity of S. Associated with every model M is\nan ontology O that formally defines the semantics of all element types of M. To aid complexity reduction, and often as a practical and computational necessity, a model M and its accompanying ontology often represent a simplification of S which may\npurposefully ignore various characteristics of S that may be deemed irrelevant or of little consequence to the particular line of scientific inquiry.\n The solution provides a collaborative approach for building shared representation models for systems of high complexity and large scale.  These models are derived by leveraging large, distributed, and heterogeneous data spaces.  Specifically,\nthe approach is formulated to enable modeling at global scale, where the complexity of our world with its large number of inhabitants can be captured for a very diverse range of application domains.  The solution enables these same techniques to be\nrecursively applied in a hierarchical fashion and then dovetailed, yielding an even broader ecosystem of integrated model components to span multiple levels of abstraction.\n Within a particular system S domain, the model M for system S is constructed through an ongoing sequence of observations (i.e. data) and the application of scientific theories (i.e. analytics) about such observations.  These theories define a\ncollection of transformation operations .GAMMA.={.gamma..sub.1, .gamma..sub.2, .gamma..sub.3, .  . . } upon M. In essence, theory operations transform a system model from one state to another.  For example, a specific theory .gamma..sub..alpha.  might\ndefine an initial state based on a particular data subset as in .gamma..sub..alpha.(O,d.sub.i,d.sub.j,d.sub.k, .  . . )M whereas another theory .gamma..sub..beta.  might transform a model's current state into a new state given new data as in\n.gamma..sub..beta.(M,d.sub.i,d.sub.j,d.sub.k, .  . . )M'.\n Other theories may transform a model simply based on the value of its current state without involving any data observations, as in .gamma..sub..chi.(M)M'.\n At an even greater level complexity, a theory .gamma..sub..beta.  might transform a model based on models of other systems as in .gamma..sub..beta.(M,M.sub.X,M.sub.Y, .  . . )M'. The solution embraces all of these forms of transformations and\nmany variants.\n Given a model M for system S, the solution recognizes that there may exist multiple interpretations of M depending upon the specific domain.  The solution thus incorporates the notion of views W={w.sub.1, w.sub.2, w.sub.3, .  . . } where each\nw.sub.i.di-elect cons.W is created via a function from the set .PHI.={.PHI..sub.1, .PHI..sub.2, .PHI..sub.3, .  . . } that contains the various interpretations of M. Each interpretation function .PHI..sub.1.di-elect cons..PHI.  defines a mapping\n.PHI..sub.i: M.fwdarw.W that translates some subset of model M into its respective view of M. The solution imposes no particular mathematical structure on views or their respective interpretation functions.  An interpretation of M may produce a view as\nsimple as a range of scalar values or as complex as a multi-dimensional vector space.  Alternatively, there may be a need for only one view with M=W and .PHI.  containing only the identity mapping.  The view formalism is provided so that the same system\nknowledge representation may provide varying perspectives to best support a wide range of end-user applications.\n The blending of these various concepts is depicted in FIG. 3.  The bottom layer of FIG. 3 is a representation of the vast data space D for system S. The middle layer of FIG. 3 contains the system model M created as a result of the theory set\n.GAMMA..  At the top of FIG. 3 is the view space V created as a result of the interpretation set .PHI.  that results when applied to model M.\n In summary, the solution mathematically portrays a complex system S as the 6-tuple S=(M,O,D,W,.GAMMA.,.PHI.).  That is, the process solution strives to create a unified model M with ontology O of a complex system S that explains phenomena D via\ntheory .GAMMA., presenting results W via interpretation .PHI..  The scientific method provides a mechanism for testing the validity of theory .GAMMA.  and the utility of model M through experimentation, offering an evolutionary (or revolutionary) path to\nnew models, new theories, new interpretations, and thus ultimately new discoveries.\n The solution knowledge framework provides a method for constructing and analyzing multi-disciplinary models of large complex systems (e.g. global communication, energy, and transportation infrastructure; world population systems).  Given such a\nsystem S, the solution prescribes a distributed, collaborative approach for developing, analyzing, maintaining, and applying the resulting model M. To accomplish this, the solution regards the model M as a critical resource sharable amongst all system S\nresearch participants.  If S is of global scale, then the size of M and/or the number of participants involved may be quite large.  In order to represent global-scale systems, the solution enables model sizes on the order of one trillion nodes and one\nquadrillion links.\n Within this framework, the solution characterizes all entities that access the knowledge space as knowledge producers and/or knowledge consumers.  The solution uses the term analytic to describe either of these roles.  Producers are analytics\nthat enrich the knowledge space through the transformation of data sources.  Each of the transformations in the theory set .GAMMA.  above have a knowledge producer role.  In contrast, many end-user applications' needs only consume this knowledge, such as\nvisualization tools, trend analyzers, and alerting functions.  Each of the functions in the interpretation set .PHI.  above have a knowledge consumer role.  Some analytics may exhibit dual roles, both producing and consuming knowledge.  Contextual\nanalytics that perform a model transformation based on input data and existing model state are an example of a dual knowledge consumer/producer.  Similarly, `pure` analytics (i.e. those that interact only with the knowledge space) that produce new\nknowledge based solely on consumption and theorization of the existing model are another dual example.\n FIG. 4 illustrates all the possible combinations of analytic processes and their interactions with external data sources and/or application users.  Information flow is depicted via arrow directionality.  The solution supports all such types. \nNote that all analytics that interact with the external environment (e.g. source data systems or visualization components) are labeled as \"adapters\", a special case of analytic.  Analytics are entirely modular, supported in \"plug-in\" fashion, added and\ndeleted depending upon the knowledge enrichment (production) or knowledge application (consumption) services needed.\n At the core of FIG. 4 is the knowledge space for capturing M. To achieve its scalability and application generality, the solution utilizes a hypergraph knowledge representation structure for M. Instances of knowledge concepts (model nodes) and\nconceptual relationships (model links) are represented as hypergraph vertices and hypergraph edges, respectively.  The solution enables information attributes to be attached to any hypergraph element.  Using this attribute mechanism, hypergraph elements\ncan be richly annotated with weights, color, or more general name-value attribute pairs.  The representation of entity/relationship state vectors may be readily embedded in this structure.  This attributed hypergraph structure was specifically chosen as\nit enables dense representation of the broadest range of complex semantic relationships.  Whereas a sizable portion of the current knowledge representation industry is restricted to subject-predicate-object expressions (e.g. RDF triples), the solution's\nrelationship structure enables any number of entities (represented as hypergraph vertices) to be contained in a single relationship (represented as a hypergraph edge).\n As a further generalization, the solution employs hypergraph element polymorphism enabling hypergraph edges to behave as hypergraph vertices, and hypergraph vertices to behave as hypergraph edges.  A depiction of all of the solution's hypergraph\nelements is shown below in FIG. 5.  Using the hypergraph formulation, for example, the family relationship of five people can be represented with five graph vertices (one for each family member) and a single hypergraph edge versus six traditional graph\nvertices (one for each family member and one special vertex for the family) and five graph member edges.  In the solution hypergraph domain, the hypergraph edge representing the family relationship can then in turn be captured as a single hypervertex\nrepresentation of the aggregate family unit for use in other model relationships.  This yields a very compact, simple structure for representing a wide range of complex relationships.  This technique can be used recursively to dramatically simplify the\nconceptual and visual representation of systems with extremely complex interdependencies.\n In addition to attributes, the solution also incorporates the notion of events that may be associated with any hypergraph element.  Events are used to signal occurrences of various model state transitions and conditions.  Analytics can\nasynchronously publish events on hypergraph elements and similarly asynchronously subscribe to such events.  In this manner the production of knowledge by one analytic can trigger the consumption of knowledge by another, yielding the aforementioned\necosystem of analytics, all interacting via the common knowledge model M.\n The main components of the solution framework are shown in FIG. 6.  The framework is a four-layer system design.  At the heart of this architecture is Layer C containing the knowledge model (MA), a very large distributed set of objects that\ncollectively represents the world's physical (and virtual) entities and their relationships.  The structure of this space is that of a large attributed hypergraph with the envisioned scale in excess of .about.1 trillion vertices, .about.1 quadrillion\nedges.  At the bottom of FIG. 6 are the data sources D.sub.1, D.sub.2, D.sub.3 .  . . (i.e. Layer A).  The union of all these data sources collectively defines the data space D. It is assumed that the number of such sources is substantial (i.e.\nthousands) and that sources are typically heterogeneous, encompassing diverse data structures and format ranging from fixed flat files to real-time streaming flows, both structured and unstructured.\n Upon Layer A is a set of data source transformations T=T.sub.1, T.sub.2, T.sub.3, .  . . that comprise the Layer B. The role of each Layer B transformation is to continuously extract entity/relationship state information from each data source\nand update the hypergraph knowledge model M managed by Layer C above.  Each transformation implements one or more theories from the theory set .GAMMA..  Access to the knowledge hypergraph is provided via an open, standardized Application Programming\nInterface (API).  Surrounding the knowledge hypergraph are a set of analytic platforms P.sub.1, P.sub.2, P.sub.3, .  . . . An analytic platform is a compute engine specifically designed to accelerate certain classes of analytic algorithms (e.g.\ngraph-based, relational-based, table-based, statistical-based, etc.).  Each analytic platform P.sub.i hosts a set of analytics A.sub.i1, A.sub.i2, A.sub.i3, .  . . that interact with the knowledge hypergraph via the common API.  The Layer B\ntransformations are considered a special case of analytic (i.e. adapters) that interface directly to one or more data sources.  Each transformation T.sub.i is hosted on some analytic engine P, sharable with other analytics.\n At the top of FIG. 6 is a collection of views W.sub.1, W.sub.2, W.sub.3, .  . . (Layer D).  The union of all such views collectively defines the view space W. Each view is implemented via one or more mapping functions from the interpretation set\n.PHI..  Views are again considered a special case of analytic that interfaces to one or more users or user applications.  Views are similarly hosted on an analytic engine, sharable with other analytics.  Views enable researchers to monitor model changes\nin real time from several different perspectives.  This is of critical importance when working to understand the dynamics of many real-world systems (e.g. critical infrastructure, cyberspace).  All views access the Layer C knowledge hypergraph using the\nsame API as all other analytics.  Hence, the power of the solution hinges heavily upon this API.  Envisioned are API bindings specified in all of the leading programming languages (e.g. Java, C++, Python, Ruby, Perl) and analytic tools (e.g. R, SAS,\nMATLAB).  This API defines a simple set of general, core primitives, engineered specifically for global scalability and proof-of-correctness.  These primitives allow a client (i.e. an analytic) to connect to the knowledge model, create and delete\nhypergraph elements, invoke their methods, and asynchronously publish and subscribe to events.  In addition, these primitives enable a very robust form of knowledge space isolation in support of privacy enforcement and cyber security.\n The solution recognizes that no single platform (i.e. analytic engine) is ideal for all classes of analytics and that performance can vary as much as six orders of magnitude (e.g. Semantic Web versus in-memory graph traversal).  The preferred\nsolution imposes only one formal API specification that is identical across all components to ensure a high degree of analytic interoperability and portability.  The specific implementation of this API internal (i.e. its body), however, can vary\nconsiderably across platforms.  By enabling implementation internals to be tuned from platform to platform, the solution is able to better exploit the specific computational/performance features of each.  This tuning involves the degree of hypergraph\ncaching and pre-caching that must be performed.  For example, on a very large shared memory machine, large segments of the hypergraph can be pre-loaded into main memory to yield near constant time graph traversals.  For global search applications,\nhowever, a parallel map-reduce infrastructure may be more effective.  Similarly, for transaction oriented analytic operations, a columnar or conventional database platform may be more ideal.  The solution enables these platforms to be mixed and matched\nto better balance costs and performance while preserving analytic design investment.\n By way of specific and representative example, an infrastructure implemented in accordance with one or more embodiments described herein protects infrastructure API, Server and Adaptors using SAML 2 Service Providers (SP) using Enhanced Client\nor Proxy (ECP).  The SAML 2 specification defines a profile of usage known as ECP, which extends the capabilities of the standard web Single Sign On (SSO) profile to HTTP user agents which are more advanced than simple web browsers.  An ECP user agent is\naware of and actively participates in the SAML flow between the Service Provider (SP) protecting a resource and the Identity Provider (IdP) providing authentication and attribute assertions about the subject invoking the user agent.\n Infrastructure API clients accessing the knowledge-share infrastructure use the ECP profile to authenticate to the knowledge-share infrastructure Server and Adaptors.  The advantages of this approach are twofold: 1) Users will be able to access\n(potentially widely distributed) knowledge-share infrastructure API services by authenticating with their existing home institution credentials, avoiding the need to build identity and credential management into the system, and 2) API endpoints will be\nable to protect their services by leveraging the (potentially rich) set of user attribute information made available by the user's IdP and exposed by the service's SP.  This enables data owners to impose whatever fine-grained access control they desire\nor their data collection restrictions mandate.  For example, a first institution can allow a particular researcher from a second institution to access the first institution's data, without having to give access to all of the first institution's data to\nall researchers at the second institution.\n Many United States higher education and research institutions already support SAML federated authentication via existing SAML infrastructure (e.g. via Shibboleth) as well as membership in the InCommon Federation.  All currently supported\nversions of both the Shibboleth IdP and SP support the ECP profile out-of-the-box.  Only additional, minimal authentication configuration is required to support the use of the ECP profile with the Shibboleth IdP.  The InCommon Federation will serve to\nestablish the trust fabric between the SP's protecting knowledge-share infrastructure services and the home institution IdP's providing user identity assertions.  The use of Shibboleth, SAML and InCommon will allow the knowledge-share infrastructure to\nleverage existing trust relationships and technical infrastructure to provide proven authentication services.\n The solution embraces two quantitative metrics for comparing progress and effectiveness.  As defined above, first is the notion of knowledge density that characterizes the connectedness of the hypergraph.  The second is the notion of analytic\nyield that characterizes the number of computational inferences that are possible over a given hypergraph implementation.\n The knowledge hypergraph is implemented using a distributed overlay approach.  That is, the hypergraph need not ever be instantiated in any one physical location or on any one computational platform.  From an analytics' perspective, the\nhypergraph appears as a large virtual cloud of interconnected hypervertices and hyperedges, each representing some aspect of an accompanying model M for a system S. This is depicted in FIG. 7.\n The hypergraph physically exists, however, as a result of interconnected servers communicating in peer-to-peer fashion using a common hypergraph transfer protocol (HGTP).  Every analytic, regardless of its role as a data adapter, knowledge\ntransformer, or view application, interacts as a client with one or more of these servers, as shown in FIG. 8.  Just as the World-Wide Web manifests as a result of globally distributed web servers communicating via HTTP, the hypergraph manifests as a\nresult of globally distributed hypergraph servers communicating via HGTP.  Analogous to the web presenting users with an interlinked view of content, the solution presents researchers with a hyperlinked model of a complex system's entities and\nrelationships with affixed attributes and methods.  Unlike the web, however, the hypergraph model is not intended for human-readable access.  Rather, it is a knowledge representation intended for machine-readable access by computational analytics.\n To create the knowledge hypergraph, adapters attached as clients to their respective servers make requests of the server to create an entity in the knowledge space, and specify how the entity behaviors are to be fulfilled.  This fulfillment\nprocess involves making a logical connection between the entity in the knowledge space, and the adapter that will respond to any requests upon it.  That is, when any client attached to the knowledge space makes a request of the entity, the request is\nforwarded to the adapter that created it.  The adapter interprets the request and generates any necessary response.  In the knowledge hypergraph, adapters behave as plug-ins connected via the standardized API, akin to the Common Gateway Interface (CGI)\nassociated with web servers.\n The standardized API enables entities to be attached as attributes to other entities in a generalized way, with the adapters performing the majority of work.  When a client requests an entity attribute, or \"invokes\" an entity's method, the\nrequest is routed to the proper adapter to fulfill.  Fulfillment might involve a simple value lookup, a database transaction, or execution of a command and control sequence depending entirely on the adapter implementation.  This structure is general and\nexpressive.  For it to work, all entities in the knowledge space have a universally unique identifier (UUID) that differentiates it from all others.  In an exemplary embodiment, hypergraph elements are identified by a 128-bit UUID, akin to a\nmachine-encoded version of a World Wide Web Universal Resource Identifier (URI).  To eliminate an extra table mapping, embedded within this UUID is sufficient routing information to locate the original system where the element was created and persisted. \nFor example, the top 64-bits may be reserved for the HGTP server address, and the lower 64-bit reserved for the specific hypergraph element maintained by that server.\n In addition to the basic request fulfillment primitives, the API also specifies operations for event publication and subscription.  This capability provides analytics with asynchronous notifications of changes in the knowledge space.  In all,\nthe API defines 11 basic primitives, including:\n CREATE--Creates entity in the knowledge space: (uuid, parameters, authorization)status\n DELETE--Deletes entity from the knowledge space: (uuid, parameters, authorization)status\n CONNECT--Connects entity to an adapter: (uuid, adapter, authorization)status\n DISCONNECT--Disconnects entity from an adapter: (uuid, adapter, authorization)status\n INVOKE--Invokes entity's method: (uuid, method, parameters, authorization)status PUBLISH--Publishes an entity event: (uuid, event, parameters, authorization)status\n SUBSCRIBE--Subscribes to entity event: (uuid, event, parameters, authorization)status\n OPEN--Opens a knowledge space notification outlet: (uuid, authorization)status\n CLOSE--Closes a knowledge space notification outlet: (uuid, authorization)status\n ATTACH--Attaches analytic to notification outlet: (uuid, event, parameters, authorization)status\n DETACH--Detaches analytic from notification outlet: (uuid, event, parameters, authorization)status\n Each of the primitives can be accessed by an analytic via the same standardized system-wide API.  Depending upon their role, analytics request these primitives in order to, e.g., create and delete elements in the hypergraph, to invoke operations\non these elements, and to request notification of events.  Construction of a hypergraph element begins with execution of a CREATE primitive by an analytic.  The HGTP server processing this request allocates a universally unique identifier (UUID) for this\nnew element in the knowledge space.  In addition, the server records which of its analytics is attached to this element.  The DELETE primitive permanently detaches an element from analytic and permanently removes it from the hypergraph space.  As\ncurrently envisioned, hypergraph elements are identified by a 128-bit UUID, akin to a machine-encoded version of a World Wide Web Universal Resource Identifier (URI).  To eliminate an extra table mapping, embedded within this UUID is sufficient routing\ninformation to locate the original system where the element was created and persisted.  For example, the top 64-bits may be reserved for the HGTP server address, and the lower 64-bit reserved for the specific hypergraph element maintained by that server.\n Leveraging existing Internet web tradecraft, HGTP servers may be single processes that manage only a small number of hypergraph elements, or a collection of mirrored clusters that manage many hypergraph elements.  The API specification for the\nCREATE and DELETE primitives is summarized as follows: CREATE(uuid,{authorization})=&gt;status; DELETE(uuid,{authorization})=&gt;status; When created, the HGTP server records the hypergraph element UUID in an internally persisted table.  Included in this\ntable is a local identifier for the analytic that created the element.  This is needed so that the server knows how to forward HGTP requests to the correct analytic.  Analytics that create (produce) hypergraph elements are distinguished as adapters. \nThat is, they perform the mathematical process of transforming (adapting) the knowledge hypergraph model M based on their input data dD from an external source.  Adapter analytics are what define and give hypergraph elements their behavior.  These\nbehaviors are accessible by other analytics in the ecosystem via the INVOKE primitive.  This primitive accepts a hypergraph element UUID as its primary argument, followed by the \"method\" identifier to be invoked for that object.  This method invocation\nmechanism is how the solution implements its hypergraph operations.  Via this primitive, adapters can be instructed to add and delete hypergraph entities and relationships, add and delete attributes, examine attributes and adjacencies, and traverse\nedges.  The execution of these instructions is defined and controlled entirely by the analytic with no centralized management thus required.  The API specification for the INVOKE primitive is summarized as follows:\nINVOKE(uuid,method,{parameters,authorization})=&gt;status; By way of particular example, Georgetown University's Sapien database maintains a statistical representation of the Earth's entire human population.  Via an adapter, each person in Sapien appears\nas a unique entity in the knowledge space.  To obtain information about a specific Sapien individual, a client performs an INVOKE operation on that entity, specifying the necessary set of parameters.  For instance, to obtain the statistical age of a\nperson x, the following operations would have to occur: INVOKE(person_x,age)=&gt;(value) Through the API, the client sends an INVOKE message to the server with the UUID person_x and an encoding that specifies the \"age\" attribute.  A typical response\ncontains a value for the requested information.  Other parameters might include name, location, gender, demographics, etc. Other response types can include single values (e.g. temperature), sets of values (e.g. flow rate profile), or sets of other\nentities (e.g. a social network).  The ontology provides the common understanding of the entity classes and the meanings and permissible ranges and units for the attributes.\n Assuming a simplified security/privacy scheme initially, any analytic in the solution ecosystem can access any hypergraph element's methods, provided it knows the element's UUID, the method's identifier, and the method's parameters.  Methods and\nassociated parameters, however, may vary considerably from one hypergraph element type to another.  The solution provides a common hypergraph \"tool kit\" for use by all analytics.  This tool kit provides support for common operations such as inquiry of an\nelement's ontologic type (e.g. person, place, thing), and a reflection of all its supported methods.  When an analytic wishes to invoke a particular hypergraph element's method, it forwards the UUID of the element, the method identifier, and the method\nparameters to its local HGTP server via the common API.  The HGTP server packages these items into an HGTP message and forwards the message to the appropriate HGTP server responsible for managing that hypergraph element based on its UUID.  At the\nreceiving end, the remote HGTP sever un-packages the HGTP message and relays its contents to the appropriate analytic that is attached to the hypergraph element.  If the remote analytic is able and willing to execute the method, it performs the\noperation, returning the result via its respective server to be relayed back to the originating analytic.  In this manner, all analytics can access hypergraph elements and all their operations using the same identical API from anywhere within a solution\nenterprise.\n The proposed toolkit creates a software stack upon the API for greater simplicity, expedited development and implementation ease.  Similar to the API, this toolkit will be bound to leading programming languages and analytic tools, prioritized by\nrequirements.  Included in this toolkit are resources for creating and managing common hypergraph design patterns, and persisting these structures in hypergraph entity registries for rapid indexing and search.  The intent of the toolkit is to alleviate\nanalytic developers of tedium that might otherwise be associated with direct API/HGTP access.\n The solution hypergraph is a dynamic structure with continuous real-time enrichment anticipated.  As a consequence, scalable and efficient operators are needed to perform hypergraph modifications.  The toolkit is to include four types of basic\nupdate operators: Insertion of a new (hyper)vertex into the hypergraph; Deletion of an existing (hyper)vertex from the hypergraph; Insertion of a new (hyper)edge into the hypergraph; Deletion of an existing (hyper)edge from the hypergraph; Modification\nof an existing (hyper)edge in the hypergraph.\n These tasks are quite involved.  For instance, in the case of deletion of an existing hypergraph vertex, all hyperedges that are associated with the vertex must be found and deleted as well.  Likewise, as all of the operations on the hypergraph\nare built on top of various index structures, the algorithms to implement these operations traverse and update the index structures.  For example, modification of an edge may be viewed as a combination of insertion and deletion--but it is often much more\nefficient to automatically implement as a single modification operator.\n The PUBLISH and SUBSCRIBE primitives are used for asynchronous notification of events that can occur on hypergraph elements.  Events may be triggered (published) automatically by a hypergraph element's analytic (adapter) when it detects certain\nelement state changes (e.g. an attribute value update).  Or, events may be triggered manually by an analytic that requests a PUBLISH operation.  Both operations accept a hypergraph element UUID and an event identifier as parameters.  The SUBSCRIBE\nprimitive is used by an analytic when it needs to be asynchronously notified of specific hypergraph element state changes.  Each HGTP server is responsible for maintaining event queues for each of the hypergraph elements that it supports.  Notification\nof an event publication is envisioned via a call-back construct particular to whatever analytic programming environment that employed (e.g. Java, C++, Python, Ruby, Pearl, R, SAS), etc. The API specification for the PUBLISH and SUBSCRIBE primitives is\nsummarized as follows: PUBLISH(uuid,event{authorization})=&gt;status; SUBSCRIBE(uuid,event,{authorization})=&gt;status; Two additional primitives (CONNECT and DISCONNECT) may also be incorporated in a solution embodiment to allow multiple analytics to be\nattached to a single hypergraph element.  This is particularly helpful for creating inheritance structures and collaborative enrichment processes.\n And, finally, the OPEN primitive opens a knowledge space notification outlet: (uuid, authorization)status, while the CLOSE primitive closes a knowledge space notification outlet: (uuid, authorization)status.  And the ATTACH primitive attaches an\nanalytic to a notification outlet: (uuid, event, parameters, authorization)status, while the DETACH primitive detaches an analytic from a notification outlet: (uuid, event, parameters, authorization)status.\n The resulting framework is generic and highly scalable.  To apply this framework, an ontology must first be selected in unison with the definition of model M. The solution is ontologically neutral, imposing no particular semantic relationship\nhierarchy.  The solution does, however, prescribe that the ontology's concept types and conceptual relationship types are selected so that instance discovery will conform to an asymptotic information curve versus an exponential data curve.  This is to\nprevent the hypergraph size from exploding beyond calculable limits.  As mentioned, the solution model size design goal is on the order of trillion-node/quadrillion-link scale.  This is adequate to represent a large array of complex globally\ninterconnected systems (e.g. population, energy, food, transportation, communications).  In contrast, representing specific system data measurements (observations) such as individual's e-mail messages, temperature/pressure readings, object displacements,\nchemical reactions, etc. as nodes within the model would be problematic as there exists no natural constraint to their upper bound.  As these items would likely follow a data exponential growth profile, they are best handled as computed state vectors\nattached as attributes to hypergraph elements (via the theory set .GAMMA.) versus direct vertex/edge representation.\n In simple terms, the solution advocates that hypergraph vertices and edges represent observable objects in the physical-world versus measurements of phenomena.  Measurements are used to attribute hypergraph model elements via the application of\ntheories regarding the observed phenomena.  Thus, the solution is well suited for analysis of complex systems that contain large numbers of highly interdependent components that can each be characterized and differentiated through a generalized state\nvector.\n The solution is applicable to very diverse scientific use cases.  For example, it is equally applicable to societal resiliency and humanitarian informatics.  The resiliency use case focuses on the resources and economics required to sustain\ncommunities through a plethora of threats, pressures, and constraints including natural disaster, emergency crisis, terrorist attack, climate change, energy food supply disruption, and communication failures.  The model components (hypergraph elements)\nof such systems involve people, roads, communication infrastructure, facilities, organizations, communities, cities, etc. The second use case focuses on individuals, helping to predict and mitigate their displacement due to natural disaster, political\nunrest, humanitarian crisis, climate change, and economic collapse.  The model components of such systems involve people, families, neighborhoods, relief organizations, regional climate, energy and food supply, political and economic structures.  While\nthese two use cases have dramatically different purposes, the models involved have significant overlap, particularly at increasing global scale.  The solution described herein recognizes that multi-disciplinary domains greatly benefit from sharing and\ncollaborative development (i.e. increased knowledge density/analytic yield).  The global scalability of the knowledge hypergraph is critical for this purpose.\n The expressive power of hypergraph structures is enormous; requiring, at base, powerful, general-purpose, reusable hypergraph analytics.  In a preferred solution, the hypergraph analytics have a \"plug-in\" type structure that enables further\ncontributions from the open community.  This structure is intended to welcome both open source and proprietary analytics into the solution environment in order to leverage the best analytic capabilities available.\n Among the core analytic services provided by the solution hypergraph is the ability to search and discover patterns in large hypergraph structures.  Using this capability, researchers will be able to define patterns of interest to them, search\nfor these patterns in any hypergraph maintained by the solution, and find all possible answers.  For example, a researcher working with a hypergraph of the entire population of Somalia may require several labeled hyperedge types.  One hyperedge may\ninclude all members of the Hiraab tribe--if this set of individuals is S1, then the labeled hyperedge would be (S1,Hiraab).  Likewise, another hyperedge might describe the set S2 of all residents of the town of Kismayo.  In this case, the labeled\nhyperedge would be (S2,Kismayo-residents).  Likewise, if there are direct connections (e.g. a \"linked_with\" edge between two individuals x and y), then the hypergraph would have the hyperedge ({x,y},linked_with) between them.\n FIG. 9 provides a simple example of a query that a researcher may want to ask.  In this case, the query is to find all people ?x who are members of the Hiraab sub-clan and who also live in Kismayo and who are a friend of a person ?y who works\nfor a bank ?b that is linked to Isse Yuluh (a well-known pirate commander in the area).\n In this sample query, hyperedges (HE) include, e.g. Kismayo-residents, Hiraab and ?b. The example also includes traditional edges TE (a special case of a hyperedge) such as the \"friend\" edges and \"type\" edges shown above.\n The toolkit includes a query structure over an hypergraph G=(V,H) where V is a set of vertices and H is a finite set of hyperedges (HE) (subsets of V).  Hypergraph queries are thus themselves graphs in which hyperedges (HE) in H can be nodes. \nAlternatively, nodes can be members of V or even variables such as ?x, ?y and ?b. The toolkit includes a defined labeled hypergraph query language (LHQL) which can express many of the types of queries that researchers are interested in asking. \nHypergraph partitioning methods are used to answer LHQL queries efficiently; even against large labeled hypergraphs.  The LHQL analytic module is integrated into the framework with hypergraph API access.  The toolkit also includes an index structure to\nsplit H into \"bite-sized\" manageable pieces.\n The toolkit includes a hypergraph probabilistic pattern detection analytic for detecting probabilistic patterns in huge hypergraph structures.  In many real world applications, researchers are not entirely certain of what it is they need.  For\nexample, consider FIG. 10.  In FIG. 10, a researcher is ideally looking for a person ?x who satisfies numerous constraints, but is also willing to consider people satisfying only a subset of these constraints.\n FIG. 10 illustrates a scenario where the researcher is looking for a person ?x who, in addition to the constraints expressed in the query of FIG. 9, also wants ?x to be a friend of an additional person ?z who is also connected to bank ?b which,\nadditionally, is now required to be in the UAE (United Arab Emirates).  However, the researcher is aware that the hypergraph may not only contain people who satisfy all the conditions that ?x is required to satisfy, but also many people who satisfy most\nbut not all of the conditions expressed in FIG. 10.  The researcher may wish to take a look at some of these people as well as they too might be suspicious.\n The researcher can accomplish this by specifying a penalty in the query.  These penalties are shown along the edges of FIG. 10.  For instance, a potential candidate (say Joe) for ?x might lose 0.3 (from a perfect score of 1) if he does not\nsatisfy the constraint of being a friend of ?z--in contrast, not being a friend of ?y would cost him a higher penalty of 0.3 from the perfect score of 1.  Simply put, what the researcher is specifying is the importance of each query constraint.\n Another complication is the penalty of small mismatches.  For instance, in the query of FIG. 10, ?b is required to be connected to Isse Yuluh, but in the hypergraph H, there may be an individual called Issa Yuluh but not Isse Yuluh.  An exact\nmatch between words would disregard this node but in fact, these may be alternative spellings of each other.  This approach assumes the existence of a function sim that measures the similarity between two node label attributes.  The preference then is to\ndefine the quality of a match between a query graph and a subgraph of a hypergraph G. Such a quality function must be: Monotonic with respect to a function sim that measures similarity of node labels and anti-monotonic with respect to the sum of the\npenalties of unsatisfied edge constraints in the query.\n Once these criteria are defined, it is then possible to develop better index structures to answer uncertain queries to hypergraph databases and query processing algorithms to effectively answer such queries.  Thus, the toolkit also includes a\ndefined uncertain labeled hypergraph query language (u-LHQL) which will extend LHQL to account for uncertainty in an analyst's query.  Given a similarity function sim measuring the similarities between node labels, one in the hypergraph database, and one\nin the query, functions may be developed to measure the quality of a match between a query graph and a subgraph of the hypergraph database.  And methods may be developed to answer u-LHQL queries by finding the top-K (in terms of quality) matches.  The\nu-LHQL analytic module is integrated into the framework with hypergraph API access.\n The toolkit further includes a hypergraph sentiment pattern detection analytic.  In many diverse surveillance activities (e.g. examination of email of foreign nationals done by NSA), analysts are often seeking to understand the sentiments\nexpressed by a person in those emails.  For instance, in the example from Somalia, understanding the attitudes of different individuals towards Isse Yuluh may be quite critical in measuring his base of support and identifying his vulnerabilities.\n Given a set T of topics of interest and a hypergraph G=(V,H), a sentiment-hypergraph is a partial mapping .sigma.: T X(V U H).fwdarw.[-1, +1].  Intuitively, given a topic t and either a vertex or a hyperedge in the graph, the function .sigma.(t,\ne) returns a number between -1 and +1 denoting the intensity of the sentiment expressed by that vertex (or the set of vertices if e is a hyperedge) on the topic t. A score of .sigma.(t, e)=-1 describes the case when the sentiment of the set e of entities\nis maximally negative, while a score of +1 denotes the most positive case.  A score of 0 denotes complete neutrality.\n For example, a person Ali in the hypergraph may have the score .sigma.(Isse Yuluh, Ali)=-0.6 suggesting that from Ali's emails, it can be deduced that he has a fairly strong negative sentiment towards Isse Yuluh.  On the other hand, the\nsentiment of a group of individuals (i.e. a hyperedge) can also be measured.  For instance, in examples in FIGS. 9 and 10, Hiraab is a hypergraph consisting of the set of all individuals belonging to the Hiraab sub-clan.  It may turn out that the score\n.sigma.(Isse Yuluh, Hiraab)=0.3, suggesting that as a whole, the Hiraab sub-clan is quite positively disposed towards Isse Yuluh.  Thus, each node in the hypergraph, as well as each edge in the hypergraph, is regarded as labeled with an assortment of\nsentiment tags, one for each topic of interest to the researcher or the application built atop of the solution.\n Using sentiment hypergraphs, researchers will be able to pose hypergraph queries that involve assessing the sentiment of individuals and organizations.  For instance, returning to the query in FIG. 9, an analyst might wish to extend that query\nto find all people ?x who are members of the Hiraab sub-clan and who also live in Kismayo and who is a friend of a person ?y who works for a bank ?b that is linked to Isse Yuluh--but additionally look for ?x to be positively viewed by the Hiraab (with\nsentiment score over 0.6) and who views Isse Yuluh negatively (with sentiment score below -0.4).  These latter two requirements can be viewed as the additional constraints .sigma.(?x, Hiraab)&gt;0.6 & .sigma.(Isse Yuluh, ?x)&lt;-0.4.  Thus, the toolkit\nalso includes a defined sentiment hypergraph query language (SHQL) which will extend LHQL to account for, e.g., score, sentiment.  The SHQL may leverage Sentimetrix's SentiGrade technology.  The SHQL analytic module is integrated into the framework with\nhypergraph API access.\n The toolkit further includes a hypergraph anomaly detection analytic, including algorithms and processing to identify semantic anomalies in massive hypergraphs.  For example, consider the sample Somali scenario above and suppose an agency such\nas NSA is monitoring communications of foreign nationals based in Somalia.  They notice that the Hiraab sub-clan is generally positive toward Isse Yuluh.  But they also note that there are various small subsets C.sub.1, C.sub.2, .  . . , C.sub.m of\nindividuals in the Hiraab clan who are opposed to Isse Yuluh--and in fact, each of the C.sub.i's is also tightly knit as well in the sense that individuals within each of these clusters largely like each other.  US interests may suggest a means to\nconsolidate these disparate groups into one group that forms a more potent opposition to Isse Yuluh.  In order to achieve this, the ability to identify such anomalous groups is needed.\n A semantic anomaly with respect to a hypergraph database G=(V,H) is a finite set C of vertices such that some semantic property with respect to C is anomalous.  For instance, in our example above, the semantic property is a variation of\nsentiment toward Isse Yuluh when compared with the sentiment of the Hiraab sub-clan.  This variation may, for instance, be defined as being 3 standard deviations or more below the mean sentiment towards Isse Yuluh amongst the Hiraab.  However, in this\ncase, it is also noted that some constraints are imposed, namely that C must have negative sentiment toward Isse Yuluh, and that C's sentiment is compared with the Hiraab.  Such constraints come from a research analyst.\n The algorithms can identify anomalous sets C of vertices in a hypergraph database that satisfy various analyst specified constraints.  Each such anomalous set will have a defined anomaly score.  For instance, an anomaly score in the above\nexample might be the number of standard deviations below the mean that the sentiment of a given candidate C is relative to the sentiment of the Hiraab as a whole.  The algorithms find the top K most anomalous subsets of vertices.  The anomaly detection\nanalytic includes a language and interface through which a researcher can specify the kinds of anomalies being sought; a scoring model for characterizing anomalous node structures; and an algorithm to find the k most anomalous subsets of nodes that\nsatisfy the researcher's constraints.\n Hypergraph utility and ease of use is improved with an algebra for expressing hypergraph queries.  The algebra may be an extension of the relational algebra common for today's data systems, but applied to the present solution's\ndistributed/attributed hypergraph structure.  Included is a set of algebraic operators that allow an analyst to search, query, and otherwise manipulate the knowledge hypergraph.  This task will provide a query capability for executing these algebraic\nexpressions.  Specifically, the hypergraph algebra is developed upon consideration of the following aspects.\n Query Equivalences:\n Given any two operations described in the preceding sections, do these operations commute, i.e. can the order in which these two operations are executed be interchanged? If not, are there specific conditions under which such an interchange of\norder is possible?\n Query Costing:\n Given an algebraic query in SHQL, how can the expected run time of the query be estimated? What query and data statistics need to be maintained to make this possible? This effort will leverage the past work by the authors on query costing in\nheterogeneous environments for this purpose.\n Query Rewriting:\n Given a query (algebraic expression) in SHQL, is there a way to automatically rewrite the query, taking into account the equivalences established above, so as to reduce the expected cost substantially?\n Equipped with the hypergraph API, toolkit, algebra, query language, and the analytic tool suite, researchers will be presented with a rich and powerful set of resources with which to design, implement, and integrate analytic components into the\nresulting hypergraph environment.  The potential complexity of this process is recognized, requiring general computational science programming knowledge and expertise to accomplish.  To significantly ease this development burden and further accelerate\ndeployment, a special hypergraph Analytic Builder application may be included with the preferred solution.\n The hypergraph analytic builder includes an intuitive user interface for specifying and connecting heterogeneous data sources, binding them to transformation/adapters and other existing analytics, and then automatically generating an operational\nanalytic.  The builder enables users to specify data transforms (theories), hypergraph patterns (models), and view generators (interpretations) using the hypergraph algebra.  These items can then be selected in drag-and-drop style to automatically create\na hypergraph adapter, analytic, or view.  This application is akin to the \"Ruby on Rails\" style of application development, but specifically designed for hypergraph analytic design and implementation.\n The solution described herein has broad applicability across myriad scenarios and is critically dependent on the sharing of knowledge across myriad of organizations and data sources.  The recent disclosures of the government's global\nsurveillance activities have reignited the complex debate regarding access to personal information.  This debate places our desire for national security in tension with our Constitutional protection of individual liberties.  Public opinion has framed\nthis as a tradeoff, balancing sacrifices of civil liberties for gains in national security.  In a specific implementation of the solution, sharing and analysis of private information is accomplished through the realization of a \"Black Box\", an\ninformation container whose contents by design can never be inspected.  This container is a special computational device that allows information to be input, but provides no means for any individual to ever examine its contents.  Rather, the box can only\nreport the detection of specific patterns that have previously been authorized as lawful, without ever revealing any of the private information itself.  The basis for this approach is the standard of law derived from the U.S.  Constitution that enables\nlaw enforcement to investigate items that present reasonable suspicion without resorting to unreasonable search or seizure.\n Important, private information is often NOT shared today due to overriding privacy concerns.  The recent case of the missing Malaysia Airlines Flight #370 involving travelers with stolen passports is a real, sobering example.  Information today\nis frequently not shared among various government organizations despite their authorized charters for national security, securing the general welfare, and the benefiting the greater public good.  Privacy concerns, particularly in the wake of the Snowden\ndisclosures, has arguably created an information sharing paralysis that is widespread across the government.  Current techniques for limited sharing often employ anonymization techniques shown to be ineffective at increasing global scale.  Other\ntechniques resort to obfuscation of data that often results in lack of analytic fidelity and poor analytic yield.  Alternative approaches have resorted to liberal interpretations of the law in attempts to balance civil liberties with national security\nneeds, arguing that in this modern age the expectation of privacy has been significantly diminished.  And numerous hybrids have been offered involving limited access controls, compartmentalization, \"blindfolded\" search, and \"trusted\" 3.sup.rd party\nagreements, each with a resulting blend of the above limitations.  The Black Box approach to privacy proposed here is successfully being deployed for sharing of very sensitive private information pertaining to HIV/AIDS (described in.sup.[2]), but in a\ncustom, special-purpose device fashion.  This proposed effort generalizes that approach, offering a generic solution for a broad spectrum of private information analysis and sharing applications.\n Within the framework's Layer C (FIG. 1), information must be analyzed in accordance with policy and law.  Thus, it is within Layer C that automated implementation of privacy is maintained.  This is shown notionally in FIG. 11.  In its most\nrobust protected form, Layer C is viewed as a strict \"black box\" that makes absolutely no allowance for any access to the information contained inside.  This security concept allows information to be aggregated but prevents it from ever being released\noutside of its confines.  Rather, policy/law compliant patterns approved by an appropriate policy body are provided to the black box through a very strictly controlled interface.  The black box operates by outputting only the identifiers of patterns that\nare detected, and any associated information that the policy body unanimously pre-approved for reporting.  Participating organizations use these automated pattern detection reports to initiate legally appropriate actions to further investigate within\ntheir currently existing policy/legal authorities.  As privacy concerns and authorities vary widely from organization to organization, the framework recognizes that a spectrum of black box containers, each with a varying degree of data/privacy\nprotections is needed.  Thus the framework enables black boxes of varying shades (i.e. grey boxes) each with differing levels of privacy restrictions, such as \"Public/Open\", \"Sensitive\", \"Restricted\", \"Classified\", \"Compartmented\", \"Strictly Private\",\netc.\n More specifically, the black box of FIG. 11 is a computational device (i.e. a computer with self-contained memory and storage) specifically designed to be \"non-queryable\" by any means.  That is, the device offers no mechanism of any kind for any\nindividual, regardless of authority level or security access, to examine its contents.  The box does, however, have exactly one additional input and exactly one output.  At the left interface, \"patterns\" of specific interest are input to the box.  These\npatterns are template-like encodings of information relationships that a duly authorized policy/oversight body has reviewed and approved for submission into the box.  Put another way, the patterns are a set of analytical rules that define the Black Box's\nreasonable search behavior.  The only patterns that are admissible to the Black Box are those that the policy/oversight body has reviewed and has unanimously confirmed as meeting the necessary threshold (e.g. the legal standards for reasonable suspicion\nand ethical standards of non-discrimination).\n An algorithm that continuously observes for conditions that match any of the submitted patterns lies within the Black Box, in addition to the information that it receives from each contributing organization, and the patterns it receives from the\npolicy/oversight body.  Upon detecting such a pattern, the Black Box outputs an identifier for the pattern and any authorized auxiliary parameters.  The specification for such auxiliary information is incorporated within the original pattern definition,\nenabling the policy/oversight body to review and approve, and ensure privacy compliance.  The internal Black Box operation is a continuous process.  It is executed in real-time and without human intervention.\n Upon a pattern detection event, the information output by the box would flow only to those organizations authorized for that pattern, as specified in the original pattern input.  All information output by the Black Box would be available to the\npolicy/oversight body or a duly constituted alternative body to continuously verify compliance.  An exemplary general description of an applicable black box concept is found in United States Publication Number 2012/0131189 entitled Apparatus and Method\nfor Information Sharing and Privacy Assurance the contents of which is incorporated herein by reference in its entirety.\n The framework Black Box construct recognizes that the level of privacy obtainable is directly related to the level of system \"impenetrability\" that can be achieved, involving a risk-cost-benefit tradeoff.  Depending upon the nature of the\ninformation to be protected, not all information sharing and analysis application will require the same degree of rigor to ensure adequate protections.  Consequently, a multi-level privacy certification rating is envisioned.  Analogous with cryptographic\nsystems, the following four levels of privacy certification are proposed:\n Type 1 Privacy: a device or system that is certified for government use to securely share and analyze private information consistent with the highest level of protections awarded by the U. S. Constitution.  Achievement of this rating implies\nthat all components of the end-to-end system have been subjected to strict verification procedures and protected against tampering via strict supply chain controls.  Type 2 Privacy: a device or system that is certified for commercial use to securely\nshare and analyze public information consistent with U.S.  commercial law.  Achievement of this rating implies that all interface components of the system have been subjected to strict verification and supply chain controls and that all other components\nhave been subjected to reasonable best industry practices for operation verification and supply chain control.  Type 3 Privacy: a device or system that is certified for public use to securely share and analyze sensitive information.  Achievement of this\nrating implies that all components of the system have been subjected to reasonable best industry practices for operation verification and supply chain control.  Type 4 Privacy: a device or system that is registered for information sharing and analysis,\nbut not certified for privacy protection.  No assumptions regarding component verification or supply chain controls are made about systems at this privacy protection level.\n For the highest grades of privacy protection, it is important that Black Box components be procured and protected via supply channels that prevent malicious (or accidental) tampering.  While not typically prevalent throughout the commercial\nsector, the notion of trusted supply chain is commonplace in the defense/weapons industry where the consequences of compromise could be enormous.  While the detailed specifics of these controls might often be compartmented for security purposes,\nestablished techniques include the use of trusted chip foundries, anti-tamper packaging, security seals, and two-person rule handling procedures.\n Using this approach, infiltrating malicious code into the Black Box, or exfiltrating private information from the Black Box will be extremely difficult.  To accomplish either would require an extremely sophisticated insider with the ability to\nbypass the internal Black Box controls in order to introduce a covert channel.  While not impossible, this would substantially raise the risk and cost an adversary must expend, particularly to subvert the verification process and any accompanying set of\nsupply chain controls.  For the highest levels of privacy protection, these verification and supply controls would need to be applied end-to-end to the system including power and cooling supplies for the most robust \"Type 1\" privacy applications. \nThorough vulnerability analysis processes of this type are now common for very high-graph physical security systems involving the protection of extremely valuable assets (e.g. nuclear material and associated weapons components) and could be readily\nleveraged in future efforts.\n Referring to FIG. 12, the five key components of the Black Box design include: Black Box Boundary; Black Box Adapters; Black Box Converters; Black Box Isolators and Black Box Engine\n The Black Box Boundary\n At the center of FIG. 12 is a depiction of the Black Box container with a rigid \"impenetrable\" boundary.  The design and implementation of this boundary is of critical importance.  The container itself may range in size from an actual physical\nbox to an entire data center.  Should the contents of the container of whatever size be viewable, either through direct or indirect means, the premise of its operation would be compromised.  Perfect impenetrability, however, is difficult or impossible to\nprove, and the costs may be prohibitively high depending upon the level of protection necessary for the information being protected.  \"Impenetrability\" here is defined as the condition in which a team of qualified, expert witnesses under sworn testimony\nprovide a preponderance of evidence that the box exhibits no known exploitable vulnerabilities that would provide access under a reasonable set of assumptions.  It is envisioned that these assumptions will vary depending upon specific application, the\nsensitivities of the data, and the damage of compromise.  Thus this effort proposes an accompanying set of privacy certifications levels (above) to be better match cost with specific privacy needs depending upon the information sharing application.  For\nefforts requiring the most robust civil liberties protections, the Black Box boundary is assumed to be no less than a physical and electronic hardware barrier that has been subject to formal vulnerability analysis and proof-of-correctness (i.e. \"Type I\"\nor \"Type II\" privacy).\n The creation of \"impenetrable\" physical and electrical barriers is not new.  The defense industry has a very extensive, and often highly compartmented, anti-tamper tradecraft for assuring such impenetrability particularly for weapons systems\n(both nuclear and conventional) where the consequence of compromise could be quite severe.  What is new is the application of these techniques to information sharing where the protection of private information is given analogous security treatment.\n The Black Box Adapters\n Perhaps the most significant challenge in constructing a Black Box is in interfacing it to an external environment.  This is obviously a necessity if the box is to be of any value.  However, a system's interfaces are generally where its\nvulnerabilities are most often found and exploited.  As mentioned, software systems of appreciable size (e.g. &gt;100,000 lines of code) are notoriously difficult to prove correctness.  In order to render this verification process tractable, a very\nsimple yet very powerful specification is used for all of the Black Box's interface channels.  To ensure adequate analytic expressive power within the Black Box, a graph theoretic information representation is taken.  That is, the information content of\nthe Black Box is organized as a large (hyper) graph consisting of attributed vertices and attributed (hyper) edges.  As this is now a commonplace representation framework in modern analytic tradecraft, the graph construct is chosen as it provides\nsufficiently rich expressive power for most if not all envisioned analytic/information sharing processing applications.  These techniques have been shown to yield high analytic performance at very large scale (e.g. &gt;billion vertices; &gt;trillion\nedges).  In addition, the graph construct is known to be computationally complete, enabling any pattern-matching algorithm of arbitrary complexity to be encoded in this framework.\n External to the Black Box in FIG. 12 are three types of adapters: input, output, and pattern.  The roll of an input adapter is to convert information from an input source into a subgraph structure, such as the \"dot\" networks depicted at the\nbottom of FIG. 11.  It is these subgraph structures that are input into the Black Box, rather than raw data from the input source itself.  This is particularly important for both scaling and information security (discussed below).  The graph\nrepresentation choice has two additional benefits.  First, patterns can also be specified as attributed subgraphs, where the attributes affixed to specific graph vertex/edge structures specify pattern-matching conditions.  In essence, graph patterns\n(queries) can themselves be readily encoded as subgraph structures.  Secondly, the result of a graph pattern match is itself also a subgraph that can subsequently be output from the box.  External to the box, this subgraph can be converted via the output\nadapter into an appropriate format for dissemination.  Using graph structures in this manner as a common canonical form, the design and engineering of the box and its interfaces is greatly simplified, leveraging a large wealth of existing graph analytic\ntradecraft.  This also greatly aids the formal verification process, simplifying the system interface specification considerably.\n The application of the graph paradigm to analytic processing is now commonplace.  The ability, however, to adapt numerous disparate, distributed data sources into a single common graph framework is only just recently become possible via\nRaytheon's information overlay technology.  This technology performs all adapter communication using a simple yet powerful graph-based data exchange protocol.  This protocol enables the adapters to specify the creation and deletion of graph elements\n(vertices and edges) and to affix attributes to each.  The input and output adapters communicate with their respective Black Box converters using precisely this protocol.  The same is true for the communication between the Black box output converter and\nits respective adapter.  The utility of this exchange protocol is discussed next.\n The Black Box Converters\n Despite substantial progress in software and hardware development practices, contemporary systems often contain flaws in their specification, design, and/or implementation.  An adversary attempts to exploit these flaws in order to gain\nunauthorized access to a system or to put the system into an undefined or unintended state.  Examples include buffer overflows, stack overflows, and exceeding array bounds and variable constraints.  In these circumstances, an adversary may find a way to\npenetrate a system by disguising malicious code within data that may later be inadvertently executed to gain unauthorized access.  The methods used by adversaries in recent years have become increasingly clever and insidious.  These are serious concerns\nthat must be very carefully addressed in order to best assure the integrity of the Black Box.  The Black Box converters are incorporated to address these issues.\n The roll of the Black Box converters are to transform the graph data exchange protocol communicated by the Black Box Adapters into a physical electrical or optical protocol to traverse the Black Box boundary.  This lowest level protocol is\nrigidly defined and enforced using combinatorial logic that can be unambiguously specified and verified in advance.  As envisioned, the Black Box converters are implemented using Application-Specific Integrated Circuit (ASIC) or Field Programmable Gate\nArray (FPGA) hardware devices to best aid high-assurance operation.  The ability to convert and serialize a graph data exchange protocol into an electrical or optical bit stream has been recently demonstrated at Raytheon using a FPGA implementation. \nThis bit stream can be generated at the Black Box boundary and transmitted directly across or encapsulated within an encrypted point-to-point long-distance data channel.\n The Black Box Isolators\n Properly contained within the Black Box boundary are the Black Box isolators.  The isolators are the internal counterparts to the converters, implemented again via ASIC/FPGA.  While the input and pattern converters transform the graph exchange\nprotocol into an electrical/optical digital signal to traverse the Black Box boundary, the input and pattern isolators perform the reverse function transforming this digital signal back into graph primitives for exchange with the internal logic engine. \nFor Black Box output channel, the process is essentially reversed with logic engine output flowing first through the output isolator that generates the signal encoding of the graph exchange protocol before traversing the boundary in route to the output\nconverter/adapter.  As envisioned, the graph exchange prototype is symmetric, thus the same design can be used for both the converters and the isolators, greatly simplifying the implementation burden.  The graph exchange protocol communicated by the\nconverters and isolators involves a strict set of fixed primitives.  These primitives are fully specified meaning that all protocol fields, parameters, and states must be completely defined in advance.\n The converter-isolator pair construct is critical to the Black Box approach.  These devices enable the Black Box to communicate with the external world, but only through very strict, rigidly defined messages.  Using the graph exchange protocol,\nall the fields in these messages are fully specified.  The only parameters specified by the prototype are constrained integer values, fully enumerated type fields, or fixed length attribute strings.  With the input, output, and pattern adapters, their\nrespective converters, and the indeed the input sources themselves all sitting outside of the Black Box boundary, it is possible that any of these components could potentially be compromised.  With the input, output, and pattern isolators all residing\nwithin the boundary, however, the correctness of their operation can be verified and then sustained.  This is accomplished via the application of formal methods technique and strict supply chain controls of the devices.\n Researchers at Southern Methodist University (SMU) in cooperation with the University of Texas-Austin have demonstrated the ability to formally verify the security properties of isolator-like circuitry.  This was previously performed using the\ninitial prototype FPGA isolator developed at Raytheon.  The process begins by generating a formal specification of the circuitry given the Hardware Description Language (e.g. Verilog or VHDL), followed by the formulation of a threat model.  Threats can\noccur as a result of exploitation of design errors or the exploitation of unspecified design functionality.  Given the threat model and the device specification, the security properties may then be formally proven to hold using either automated theorem\nproves or model checkers, with the latter being recommended by SMU for this effort.  The model checking approach enables the isolator FPGA circuit model expressed in the HDL to be used directly without the need for intermediate theorem proved synthesis\nsteps.\n For the highest grades of privacy protection, it is important that Black Box components be procured and protected via supply channels that prevent malicious (or accidental) tampering.  While not typically prevalent throughout the commercial\nsector, the notion of trusted supply chain is commonplace in the defense/weapons industry where the consequences of compromise could be enormous.  While the detailed specifics of these controls might often be compartmented for security purposes,\nestablished techniques include the use of trusted chip foundries, anti-tamper packaging, security seals, and two-person rule handling procedures.\n Using this approach, infiltrating malicious code into the Black Box, or exfiltrating private information from the Black Box will be extremely difficult.  To accomplish either would require an extremely sophisticated insider with the ability to\nbypass the internal Black Box controls in order to introduce a covert channel.  While not impossible, this would substantially raise the risk and cost an adversary must expend, particularly to subvert the verification process and any accompanying set of\nsupply chain controls.  For the highest levels of privacy protection, these verification and supply controls would need to be applied end-to-end to the system including power and cooling supplies for the most robust \"Type 1\" privacy applications. \nThorough vulnerability analysis processes of this type are now common for very high-graph physical security systems involving the protection of extremely valuable assets (e.g. nuclear material and associated weapons components) and could be readily\nleveraged in future efforts.\n The Black Box Engine\n At the core of the Black Box is the logic engine.  The logic engine is a computational device programmed to maintain a representation of the source inputs in volatile memory and continuously execute a pattern-matching algorithm that examines\nthis representation and reasons about its contents.  In essence, the logic engine is an inference machine that uses the input sources to construct a graph-structured fact base and the pattern statements to define search rules over that base.\n In anticipation of the formal verification process, this effort proposes the use of the eXtensible Knowledge Server (XKS) from HIGHFLEET, Inc.  as the prototype logic engine.  As a commercial product, XKS is unique in that it allows problems to\nbe expressed using a first-order logic compatible semantics.  This will be a particularly important aid during the verification phase.\n The XKS system implements a deductive database server.  It uses a technology that is based on Resolution Theorem proving, with ideas from XSB and its SLG (tabling) resolution, and parallel inference that handles recursion through tabling of\nintermediate results.  The model (or ontology) for an XKS database is defined by a General Logic Program written in KFL (Knowledge Frame Language) which is largely a syntactic variant of ECLIF (Extended Common Logic Interchange Format).  The deployment\nof an XKS from a given KFL source is similar to compiling a logic program.  The XKS stores facts persistently providing standard ACID properties of a database system (Atomicity, Consistency, Isolation, Durability).\n The KFL language supports general logic programming statements where each relation defined in the KFL source may have any number of rules associated with it where each such rule has the form of a (&lt;=HEAD GENERAL_BODY).  In such a rule the\nHEAD is a simple literal for that relation with arguments that are any well-formed terms in KFL and the GENERAL_BODY is any well-formed formula in first-order logic expressed in the syntax of ECLIF.  ECLIF provides syntactic expression for basic terms\n(strings, numbers, symbols, variables) and compound `function` terms (a special term with a symbol term as its function and any number of arguments that are terms).  Certain function terms are given special interpretation by the XKS--these include dates,\ntime spans, and measurements (e.g. `(kilometer 37)`).\n There are several extensions beyond classical First-Order Logic supported by KFL/ECLIF and the XKS: integrity constraints, temporal logic, aggregation operations (e.g. 39etoff, count, average, min), extensional equality, and measurement terms. \nThe syntax of First-Order Logic supported in ECLIF include terms as discussed above, atomic literals that name a relation and provide terms as arguments, and standard operators that combine literals such as `and`, `or`, `=&gt;` (implication), `forall`,\n`exists`, and `not`.  For example, a relation that relates a person to things that person makes, `makes`, might have facts such as (makes anne brick) to say the Anne makes bricks.  (makes anne brick) is an atomic literal.  (not (makes anne pie)) asserts\nthat it is not true that Anne makes pies.  To say that everything Anne makes is a BuildingMaterial we can use the well-formed formula (forall (?x) (=&gt; (makes Anne ?x) (BuildingMaterial ?x)).\n KFL includes special relations: `inst` for `instance of`, `sup` Rel for \"is a super relation of\", and the unary relation Property for \"is a Property\".  There is a complex \"upper level ontology\" (`ULO`, written in KFL) that is used when creating\na model in KFL.  The ULO source is added to the new model KFL source to make one (large) program.  The XKS presumes the presence of certain aspects of the ULO for any KFL source that it deploys.  The XKS system has special implementations for `inst`,\n`supRel`, Property, and many other standard parts of the ULO.\n The KFL deployment/compilation process mostly processes the KFL source as though it was a collection of queries, applying the same compilation process to the KFL source statements as the XKS applies to queries of a deployed database.  This\ncompilation process produces a collection of `clauses` where a clause has a HEAD (similar to the GLP HEAD above) and a NORMAL_BODY.  The NORMAL_BODY is a simple conjunction of literals, either atomic literals or negated atomic literals.  This collection\nof clauses is the `store` that represents the model for the XKS.  It is unchanging for the duration of a running XKS (it can be updated, but this requires stopping the XKS).\n A query of an XKS is compiled `against` the store for that XKS creating an optimized specialized version of the store that is developed just for evaluating that query.  This is a kind of partial evaluation where the results of `partially\nevaluating` an ECLIF formula is a collection of clauses.  There is a unique top-level literal that when evaluated produces exactly the answers for the original query.\n In the Black Box setting, information received from the source data systems via the input adapter-converter-isolator channels will be treated as logic assertions that XKS uses to build and populate the graph/logic model.  Similarly, patterns\nreceived via the pattern adapter-converter-isolator channel are forwarded to XKS as queries.  XKS will be configured to handle these in a continuous, non-stop persisted query fashion.  When XKS detects a new pattern, the response will be transmitted via\nthe output isolator-converter-adapter channel.\n For this effort, the correctness of the logic engine operation will be assumed.  To address the inevitable possibility of both design and implementation flaws, the pattern-matching algorithm is to be subjected to formal verification.  The\nselection of XKS with its first-order logic semantics is expected to aid this process.  Verification of the entirety of the underlying logic engine is important for applications requiring the most robust privacy protections (i.e. \"Type I\").  In such\nsettings, the logic engine in turn will need to be hosted on trusted, formally verified tamper-protected, supply chain-controlled processors.  While this level of verification is impractical for today's commodity processors equipped with both very rich\nmodern and legacy instructions sets, it is considered tractable for future generations, given the discrete combinatorial logic structure of these units.  By restricting the logic engine specifications to first-order logic, the architecture and design\nrequirements for a new generation of high-assurance trusted logic engine devices/processors can be readily envisioned for future privacy assurance endeavors.\n \"Black Box\" Operation\n Using all the components identified, the operation of the proposed Black Box works as follows.  For each input source, an input adapter is assigned that interfaces to that source.  The adapter extracts the primary information entities and\nrelationships that the source organization desires to be shared privately via the Black Box.  The adapters converts these items into an attributed subgraph, then using the graph exchange protocol communicates the elements of this subgraph one at a time\nvia an encrypted channel to its respective input converter.  The input adapter would typically be located in close proximity to the source system, and managed under its security controls.  In contrast, its corresponding input converter would be located\nat the Black Box boundary to prevent input tampering and disclosure.  Input adapters continuously monitor their sources for changes in their information subgraphs, forwarding these changes to their respective converters to update the Black Box.\n The input converters at the Black Box receive encrypted graph exchange messages from their respective adapters.  The messages are \"down converted\" into digital electrical/optical signals via the FPGA/ASIC converters and then transmitted across\nthe Black Box boundary to the receiving isolators within.  These isolators then re-convert the signals back into their corresponding graph operations and feed these into the logic engine.  Depending up the operation, the logic engine updates the\naggregate graph knowledge base (logic model) by adding or deleting the appropriate vertices, edges, and/or affixed attributes.  This process is repeated indefinitely with information updates flowing from each input source continuously into the Black Box\nas they occur.  Should power ever be interrupted to the Black Box, or the logic engine is restarted, the process is purposefully repeated over from the start across all sources.\n Operating in parallel with the Black Box input process is the pattern process.  As patterns are reviewed and approved by the policy/oversight body, the patterns are encoded in a formal high-level language.  These language statements are then\npassed through a compiler that creates a set of first-order logic statements.  For this initial effort, English is envisioned as the high-level language with the compilation being done by hand.  In subsequent efforts, however, a more appropriate\nrestricted, structured language for encoding patterns will be selected for automated compilation.  The first-order logic results of this compilation (whether generated manually or automatically) will be forwarded to the pattern adapter.  The pattern\nadapter transforms the first-order logic statements into a subgraph using first-order logic attributed semantics.  This subgraph is then sent to the pattern converter to be down converted into a set of Black Box digital signals to traverse the boundary\nin via the pattern channel.  Within the Black Box, each of these signals are up-converted back to a corresponding set of graph exchange messages forwarded to the logic engine.  The engine uses these messages to create the corresponding pattern subgraph\nwithin its aggregate knowledge (graph) base.  The logic engine algorithm then adds this pattern to its continuous matching process.\n When the logic engine detects a pattern match, the engine generates a results subgraph based upon the output specification affixed as attributes to the pattern subgraph.  This results in a subgraph that is then be passed via the output isolator\nto the output converter and finally the output adapter for dissemination at the output user interface.  For the proposed activity, a simple printout is proposed initially.  For subsequent implementations, however, a much richer automated distribution\nprocess is envisioned that automatically forward results directly to the appropriate organization to pursue (e.g. one or more of the authorized input).  The result might also be automatically forwarded to the policy/oversight body for oversight and\ncompliance audit.\n In the context of the solution framework described above (FIG. 1), the primary requirement imposed on Layer A source systems is that each provide some sort of interfacing mechanism or API to enable access to the allowable functions they choose\nto offer and support within a framework environment.  It is via this local API that a Layer A source system is connected into the larger framework enterprise.  As shown schematically in FIG. 13, this connection is accomplished via a composite adapter.  A\ncomposite adapter is a software (typically, but not necessarily) subsystem that interfaces a Layer A source system via its API to the Layer B above.\n The composite adapter is important for several reasons.  It is assumed that Layer A source systems are enormously diverse in their capabilities, functions, and implementations.  The design of this adapter structure is formulated to dramatically\ndecrease the cost and time associated with integrating such diverse systems at such large scale.  The role of the composite adapter is to isolate all unique, custom interfacing work for interacting with Layer A source system data into exactly one\nlocation in Layer A so that the other components across the framework may be standardized and generic.  This is an extremely powerful and discriminating offering.  This structure promotes an open \"culture\" for large-scale, highly secure distributed\nsystems integration.  That is, all data systems desiring framework integration can contribute regardless of their capabilities, but without fear of compromise.  Systems with a very rich API would be able to leverage all features and benefits of the\nframework while systems with limited APIs would only participate up to their limits of capability.  The composite adapter handles this often complex, troublesome interfacing burden, transforming the Layer A source system's custom API into a framework\ncompliant component.  This transformation is aided via a standardized, non-proprietary protocol to the Layer B above.\n Layer B's primary purpose is to provide a common, robust, scalable access mechanism to Layer A information, but without sacrificing local autonomy, jeopardizing system security and integrity, or violating privacy.  Essentially, Layer B performs\nan integration service across the entire spectrum of Layer A products, services and systems, providing a fabric that weaves together each into a single cohesive, unified framework.  Layer B accomplishes this \"plumbing\" challenge through a series of\npowerful information transformations specifically formulated for large-scale distributed systems operation.  These transformations effectively map Layer A source system data exhibiting exponential growth into Layer C information entities and\nrelationships with limited asymptotic behavior.\n As a result of these transformations, Layer B enables the creation of an extremely strong security isolation barrier to prevent unauthorized data breaches, vulnerability-inducing data or cyber contamination (e.g. malware transmission), or\nusurpation of control (e.g. hacking).  This barrier establishes a trust boundary above which Layer C analytic processing can be performed, but without compromise to any constituent Layer A source components.  Similarly, the barrier prevents individual\nLayer A source components from compromising the integrity of Layer C operations or another Layer A source component in an aggregate framework enterprise.  To achieve this high degree of integrity, the framework Layer B boundary is designed specifically\nso that the interface implementation can be rigorously defined and mathematically proven.  The robustness of this trust/isolation boundary is particularly important for applications requiring the highest levels of privacy protection and preservation\n(i.e. \"Black Box\" applications).\n The key components of Layer B are shown in FIG. 14.  Information distributed across Layer A source systems is pushed and pulled on demand to and from the Black Layer above through a series of Layer A-Layer B adapter/converter channels.  The\nLayer A adapters (in FIG. 14) connect Layer A source systems to these channels via each system's respective Layer A composite adapters (from FIG. 13).  Layer A adapters speak to upper Layer B components through a Layer B API that defines a special\nprotocol.  That is, all communication to the upper layer components references framework entities, their relationships, attributes, and events.  The Layer A adapters are responsible for transforming Layer A source system data, as presented by the Layer A\ncomposite adapters, into these graph elements in conformance with the Layer B API.  Stated differently, the Layer A adapters are the framework components that perform the transformation from the exponential-grown data streams into the asymptotically\nlimited information entity/relationship sets.  By doing so, the Layer C analytics above will need not be concerned with the specific implementation and data details of these elements.\n Upon the Layer A adapter's transformation of Layer A source system data into graph elements, communication of these elements can now be serialized for security and privacy isolation purposes.  The role of the Layer A-Layer B converters is to do\njust that.  That is, the Layer A converter takes incoming graph elements from the Layer A adapter and converts these elements into a set of serialized graph transactions (e.g. create node, create link, add attribute, remove attribute, etc.).  These\ntransactions are formatted using a simple protocol consisting of well-defined operation codes and message parameters that fully encode the graph transaction.  This encoded sequence is then delivered to the Layer A isolator for transmission across the\nLayer A-Layer B isolation boundary to the corresponding Layer B isolator.  To ensure that only the properly structured transactions successfully cross the Layer A-Layer B boundary, isolators are typically implemented as hardware devices such as Field\nProgrammable Gate Arrays (FPGAs) or Application Specific Integrated Circuits (ASICs).  Using formal methods based on model checking techniques for information processing circuitry, the operation of these devices can be verified mathematically to\nestablish the correctness of their implementation.  As these devices are built from combinatorial logic circuitry, verifying their correctness is a manageable process conducive to automation, whereas verifying an alternative conventional software\nimplementation (e.g. code development in Java or C++) could quickly become unwieldy.  The choice of hardware isolation makes this verification process tractable using contemporary proof techniques.\n On the Layer B side of the boundary, the Layer B isolator and Layer B converter perform the reverse operation of their Layer A counterparts.  The Layer B isolator device receives encoded graph transactions from the Layer A isolator, and then\nforwards these transactions to its corresponding Layer B converter.  The Layer B converter converts the serialized graph transactions back into the equivalent entity/relationship representation.  The Layer B adapter then contributes these elements to\nLayer C knowledge graph in conformance with the Layer C API.\n Accordingly, the role of the Layer B converters is to transform the graph data exchange protocol communicated into a physical electrical or optical bit stream to traverse the Layer C boundary.  This bit stream can be generated at the Layer C\nboundary and transmitted directly across or encapsulated within an encrypted point-to-point long-distance data channel.\n As mentioned, all Layer B adapters interact with Layer C via the exact same API.  That is, the specification that defines the complete set of operations and accompanying parameters that a Layer B adapter can perform upon the Layer C knowledge\ngraph are identical for all Layer B adapters, regardless of their location in a global framework enterprise.  However, the specific implementation of this API on the local platform where this adapter is running can vary considerably.  This discriminating\nframework feature is very important as it allows implementations to exploit the unique performance features of the local platform, yet maintain platform independence of developed software.\n To further greatly reduce the framework implementation burden, the Layer C API specification that is visible to all Layer B adapters is actually also identical to the Layer B API specification visible to all Layer A adapters.  The implementation\nof these APIs, however, is again quite different.  The Layer A Converter, the Layer A-Layer B isolator pair, and the Layer B adapter essentially recreate the Layer B API at Layer C but with the critical interim hardware-assisted filtering steps necessary\nfor security and privacy enforcement.  Whereas a Layer C API implementation exists within (above) the Layer A-Layer B trust boundary, a Layer B API implementation is actually responsible for creating and enforcing that boundary.  The Layer B enforcement\nmechanism employs the convert-isolator pairs for high-assurance operation founded on mathematical proof-of-correctness.  It is envisioned that Layer B API implementations will be additionally augmented with anti-tamper capabilities to further deter cyber\nadversaries and attempts at privacy violation.  The resulting Layer B component design makes the transmission and/or exploitation of malware, viruses, backdoors, Trojan horses, etc. extremely difficult, significantly raising the risk, level of\nsophistication, and amount of investment needed by an adversary.\n Specific and non-limiting black box use cases are described herein to illustrate functionality and application.  Initially, suppose a group of people wish to conduct a legitimate test of the \"Birthday Paradox\", but do not wish to reveal any of\ntheir personal information.  The Birthday Paradox concerns the probability that, in a set of n randomly chosen people, some pair of them will have the same birthday.  By necessity, the probability reaches 100% when the number of people in the group\nreaches 367 (since there are only 366 possible birthdays, including February 29).  However, with just 23 people, the probability is 50%, and with 70 people, the probability is 99.9%.  To test the Birthday Paradox without revealing personal information, a\nBlack Box is used.  To illustrate how this is done, let the set P={p.sub.1, p.sub.2, .  . . p.sub.n} represent the group of n people.  Each participant in P considers their name and birthday to be private.  The process begins with each person (privately)\nexpressing this information via an input adapter in a formal manner.  This is accomplished with statements of the following form: P(p.sub.1) name_of(p.sub.1)=\"Alice\" birthdate_of(p.sub.1)=\"01/23\" P(p.sub.2) name_of(p.sub.2)=\"Bob\"\nbirthdate_of(p.sub.2)=\"02/04\" P(p.sub.3) name_of(p.sub.1)=\"Carol\" birthdate_of(p.sub.3)=\"12/25\" : : where the predicate P(p.sub.i) is interpreted as \"p.sub.i.di-elect cons.P is a person\", and the functions name_of and birthdate_of return the name and\nbirthdate, respectively, of that person.  That is, the person named \"Alice\" was born on January 23, the person named \"Bob\" was born on February 4, etc. The role of the Black Box in this example is to determine the truth value of the logic statement L\n(i.e. a Black Box pattern): L=.E-backward.p,q|P(p) P(q) birthdate_of(p)=birthdate_of(q) p.noteq.q To accomplish this in a general-purpose fashion, a graph-based representation of the problem is used by the Black Box.  That is, the box internally\nconstructs a graph G=(V, E) where the vertex set V={.nu..sub.1, .nu..sub.2, .  . . .nu..sub.n} with n=|V| and the edge set E={e.sub.1, e.sub.2, .  . . e.sub.m} with m=|E| and e.sub.j.di-elect cons.V.times.V.  Thus each element e.sub.j.di-elect cons.E is\nan ordered pair of the form (.nu..sub.k, .nu..sub.l) with .nu..sub.k, .nu..sub.l.di-elect cons.V.  The graph is attributed so that every vertex .nu..sub.i.di-elect cons.V may be assigned a vertex type attribute .nu..sub.i'TYPE.di-elect\ncons.T.sub.V={.mu..sub.1, .mu..sub.2, .  . . } and every edge e.sub.j.di-elect cons.E may be assigned an edge type attribute e.sub.j'TYPE.di-elect cons.T.sub.E={.epsilon..sub.1, .epsilon..sub.2, .  . . } where T.sub.V and T.sub.E are finite sets.  For\nthis example, T.sub.V={person, name, birthday} and T.sub.E={name_of, birthdate_of}.  In addition, every vertex .nu..sub.i.di-elect cons.V may also be assigned a vertex value which is dependent upon its vertex type.  For this example, the following vertex\nvalues may be assigned:\n TABLE-US-00001 Vertex type Vertex Value person integer name fixed length alpha string (e.g. max. of 50 characters) birthday four numeric characters (e.g. MM/DD format)\n This graph structure enables the box to represent any number of individuals with their respective names and birthdays.  For example, the logic statement about Alice P(p.sub.1) name_of(p.sub.1)=\"Alice\" birthdate_of(p.sub.1)=\"11/23\" is represented\nvia a three vertex, two edge attributed subgraph as shown in FIG. 15a.  In a Black Box system, this subgraph is generated by Alice's input adapter and passed to her input converter for transition into the Black Box.\n To test the Birthday Paradox, the name and birthday of every person p.di-elect cons.P, must first be loaded into the box.  This is accomplished by converting the graph representation of every individual's logic statement into a graph protocol\nmessage to transition the Black Box boundary.  This protocol is very rigid so that no other form of access can occur over the input channel, and so that the implementation (say, via FPGA) can be subsequently verified.  This message protocol is defined\nvia the following grammar:\n Non-Terminals:\n TABLE-US-00002 &lt;graph_message&gt; ::= &lt;SOG&gt; &lt;graph&gt; &lt;EOG&gt; &lt;checksum&gt; &lt;graph&gt; ::= &lt;vertex_message&gt; | &lt;vertex_message&gt; &lt;graph&gt; | &lt;edge_message&gt; | &lt;edge_message&gt; &lt;graph&gt;\n&lt;vertex_message&gt; ::= &lt;SOV&gt; &lt;vertex&gt; &lt;EOV&gt; &lt;vertex&gt; ::= &lt;person_type&gt; &lt;identifier&gt; | &lt;name_type&gt; &lt;name&gt; | &lt;birthdate_type&gt; &lt;birthdate&gt; &lt;edge_message&gt; ::= &lt;SOE&gt; &lt;edge&gt;\n&lt;EOE&gt; &lt;edge&gt; ::= &lt;edge_type&gt;&lt;vertex&gt; &lt;vertex&gt; &lt;edge_type&gt; ::= &lt;birthdate_of_type&gt; | &lt;name_of_type&gt;\n Terminals:\n TABLE-US-00003 &lt;SOG&gt; ::= start_of_graph_operation_code {e.g. \"0\"} &lt;EOG&gt; ::= end_of_graph_operation_code {e.g. \"1\"} &lt;SOV&gt; ::= start_of_vertex_operation_code {e.g. \"2\"} &lt;EOV&gt; ::= end_of_vertex_operation_code {e.g. \"3\"}\n&lt;SOE&gt; ::= start_of_edge_operation_code {e.g. \"4\"} &lt;EOE&gt; ::= end_of_edge_operation_code {e.g. \"5\"} &lt;person_type&gt; ::= person_vertex_type_code {e.g. \"0\"} &lt;name_type&gt; ::= name_vertex_type_code {e.g. \"1\"} &lt;birthdate_type&gt;::=\nbirthdate_vertex_type_code {e.g. \"2\"} &lt;name_of_type&gt; ::= name_of_edge_type_code {e.g. \"3\"} &lt;birthdate_of_type&gt; ::= birthdate_of_vertex_type_code {e.g. \"4\"} &lt;identifier&gt; ::= unique_identifier {e.g. 64-bit integer} &lt;name&gt; ::=\nfixed_length_alpha_string {e.g. 50 characters} &lt;birthdate&gt; ::= fixed_length_numeric_string {e.g. 4 characters in \"MMDD\" format} &lt;checksum&gt; ::= computed_checksum\n For Alice to share her birthdate (i.e. November 23) via the Black Box, her input converter would transform her subgraph representation into the following graph message for transmission into the Black Box.\n [SOG]\n [SOE]\n [name_of_type=3]\n [SOV]\n [person_type=0]\n [identifier=p.sub.1]\n [EOV]\n [SOV]\n [name_type=1]\n [name=\"Alice\"]\n [EOV]\n [EOE]\n [SOE]\n [birthdate_of_type=4]\n [SOV]\n [person_type=0]\n [identifier=p.sub.1]\n [EOV]\n [SOV]\n [birthdate_type=2]\n [birthdate=01/23]\n [EOV]\n [EOE]\n [EOG]\n [checksum]\n This message would be received by Alice's Black Box input isolator and converted back to the subgraph form for subsequent merging into the Black Box's internal graph data structure.  Assuming Bob and Carol performed the same process with their\nprivate information, the graph contents of the Black Box, although it would not be visible to anyone, would be as shown in FIG. 15b.  Now suppose Sam decides that he too wants to participate in the group, and his birthday, unbeknownst to the others, is\nalso on February 4, expressed as P(p.sub.4) name_of(p.sub.4)=\"Sam\" birthdate_of(p.sub.4)=\"02/04\" Following the same process, the contents of the Black Box graph would then be as shown in FIG. 15c.\n When the group is finally content that enough people have shared their information with the Black Box, the box could then be asked the question if any two of them share the same birthdate.  That is, does L=.E-backward.p,q|P(p) P(q)\nbirthdate_of(p)=birthdate_of(q) p.noteq.q=TRUE? As with the information about each individual, so too can this logic statement (pattern) be specified as a graph for input to the Black Box (via the pattern channel) as shown in FIG. 16.\n This graph can subsequently be transmitted to the Black Box (via the pattern converter) using the following message:\n [SOG]\n [SOE]\n [birthdate_of_type=4]\n [SOV]\n [person_type=0]\n [identifier=p]\n [EOV]\n [SOV]\n [birthdate_type=2]\n [birthdate=?]\n [EOV]\n [EOE]\n [SOE]\n [birthdate_of_type=4]\n [SOV]\n [person_type=0]\n [identifier=q]\n [EOV]\n [SOV]\n [birthdate_type=2]\n [birthdate=?]\n [EOV]\n [EOE]\n [EOG]\n [checksum]\n This message is received by the Black Box via the pattern converter, and transformed back into a subgraph structure.  This subgraph is then passed to the logic engine where it is specifically interpreted as a graph query (i.e. a pattern), not as\nnew graph information as per that received via the source input channels.  The vertex value \"?\" is a special value interpreted by the Black Box as a variable for pattern matching whenever received via its pattern channel.\n Within the Black Box, this pattern is then used by the logic engine for conventional inference.  For this example, the box output channel need only be a single electric light that turns on whenever L=TRUE, remaining off otherwise.  The internal\ngraph structure, however, enables a significant performance opportunity.  Determining if two individuals have the same birthday requires simply examining only those vertices of type birthday and determining if any have an in-degree greater than 1. \nDepending upon how the internal graph data structure is organized, this determination can be done very fast for this and problems of significantly higher scale.\n In a second exemplary black box use case, consider the fact pattern of Umar Farouk Abdulmutallab.  On Christmas Day in 2009, 23-year-old Nigerian Umar Farouk Abdulmutallab traveled from Ghana to Amsterdam where he successfully boarded Northwest\nAirlines Flight 253.  Concealed upon him were plastic explosives sewn in his underwear that he attempted but failed to detonate properly.  Reports at that time indicated that the U.S.  had received intelligence regarding a planned attack by a Yemen-based\nNigerian man.  Well in advance of the flight, the Watchlisting Office at the National Counterterrorism Center (NCTC) had created a basic terrorist record for Abdulmutallab in the Terrorist Identities Datamart Environment (TIDE), the U.S.  Intelligence\nCommunity's central repository of international terrorist identities.\n In the airline and travel industry, the computer reservation systems maintain a passenger name record (PNR) for each individual or groups of passengers that are traveling.  Each PNR contains itinerary information along with personal identifying\ninformation such as name, gender, passport details, date and place of birth, etc. The European Union (EU) places strict constraints on how this PNR information is shared.  Through a series of complex international negotiations, a collection of PNR\nsharing agreements have resulted between the EU and the United States, and many other countries.  For the U.S., these treaties forbid the sharing of this information for U.S.  Intelligence purposes.  Rather, they designate the Department of Homeland\nSecurity as the executor of this information, restricting the use of PNR for domestic security purposes only with strict handling guidelines.  On Dec.  25, 2009, a PNR for Abdulmutallab existed in the airline reservation system.  As a result of a complex\nseries of now well-published information sharing failures, Abdulmutallab nevertheless retained a valid U.S.  tourist visa to enter the United State that had not been revoked by the U.S.  State Department.  Similarly, Abdulmutallab was not placed in the\n\"Terror Screening Database\", on the \"Selectee List\", nor on the \"No-Fly List\" that existed at that time.  In simple terms, the EU reservation systems held a PNR that indicated Abdulmutallab was traveling on Flight 253 and the U.S.  Intelligence Community\nheld a TIDE record indicating Abdulmutallab as a terrorist identity, but the two \"dots\" could not be connected via the existing privacy rules and regulations.  The Black Box provides a simple solution to this problem.\n To illustrate how this is accomplished, let the set P={p.sub.1, p.sub.2, .  . . p.sub.n} represent the population of n passengers for which there exists a PNR in the EU reservation system.  Let the set F={f.sub.1, f.sub.2, .  . . f.sub.x}\nrepresent the set of all x flights departing or arriving in the EU.  Let the set T={t.sub.1, t.sub.2, .  . . t.sub.m} represent the set of m terrorists maintained by the U.S.  Intelligence Community in their database.  From the EU airline reservation\nsystems, statements describing individuals traveling on flights (i.e. PNR records) are expressed to the Black Box via its designated adapter as follows: P(p.sub.i) F(f.sub.j) name_of(p.sub.i)=passenger_name flight_of(p.sub.i)=f.sub.j where the predicate\nP(p.sub.i) is interpreted as \"p.sub.i.di-elect cons.P is a person\", the predicate F(f.sub.j) is interpreted as \"f.sub.j.di-elect cons.F is a flight\", and the functions name_of and traveling_on return the name and flight, respectively, for that person. \nFor example, P(p.sub.1) F(f.sub.123)name_of(p.sub.1)=\"Alice Angle\" flight_of(p.sub.1)=f.sub.123 P(p.sub.2) F(f.sub.253)name_of(p.sub.2)=\"Karl Kind\" flight_of(p.sub.2)=f.sub.253 P(p.sub.3) F(f.sub.253)name_of(p.sub.3)=\"Umar Farouk Abdulmutallab\"\nflight_of(p.sub.3)=f.sub.253 From the U.S.  Intelligence Community, statements describing terrorist identities (i.e. TIDE records) are expressed to the Black Box as T(t.sub.i) name_of(t.sub.i)=terrorist_name where the predicate T(t.sub.i) is interpreted\nas \"t.sub.i.di-elect cons.T is a suspected terrorist\", and the function name_of returns the name of the terrorist suspect.  For example, T(t.sub.1) name_of(t.sub.1)=\"Boris Badenov\" T(t.sub.2) name_of(t.sub.2)=\"Umar Farouk Abdulmutallab\" The role of the\nBlack Box then is to determine the truth value of the logic statement L (i.e. the Black Box pattern), and output the flight designator f whenever a true pattern occurrence is detected: L(f)=.E-backward.p,t|P(p) T(t) name_of(p)=name_of(t) flight_of(p)=f\nTo accomplish this in a general-purpose fashion, the Black Box again uses the same graph-based structure as before.  For backward compatibility with the Birthday Paradox, additional vertex type codes for flights (type code 5) and terrorists (type code 6)\nare simply added to the graph message protocol and the internal Black Box graph representation.  Similarly, a new edge type code for flight_of (type code 7) is also added, yielding:\n TABLE-US-00004 Vertex type Type Code 0 Vertex Value integer (UUID) person name 1 fixed length alpha string (e.g. max. of 50 characters) flight 5 integer (UUID) terrorist 6 integer (UUID) Edge type Type Code name of 3 flight of 7\n The above PNR statements are represented via the subgraphs shown in FIG. 17a.  These subgraphs are transmitted to the Black Box (via the designated EU PNR input converter) with the following messages:\n TABLE-US-00005 Alice PNR Karl PNR Umar PNR [SOG] [SOG] [SOG] [SOE] [SOE] [SOE] [name_of_type = 3] [name_of_type = 3] [name_of_type = 3] [SOV] [SOV] [SOV] [person_type = 0] [person_type = 0] [person_type = 0] [identifier = p.sub.1] [identifier =\np.sub.2] [identifier = p.sub.3] [EOV] [EOV] [EOV] [SOV] [SOV] [SOV] [name_type = 1] [name_type = 1] [name_type = 1] [name = \"Alice [name = \"Karl Kind\"] [name = \"Umar Farouk Applebee\"] Abdulmutallab\"] [EOV] [EOV] [EOV] [EOE] [EOE] [EOE] [SOE] [SOE] [SOE]\n[flight_of_type = 7] [flight_of_type = 7] [flight_of_type = 7] [SOV] [SOV] [SOV] [person_type = 0] [person_type = 0] [person_type = 0] [identifier = p.sub.1] [identifier = p.sub.2] [identifier = p.sub.3] [EOV] [EOV] [EOV] [SOV] [SOV] [SOV] [flight_type =\n5] [flight_type = 5] [flight_type = 5] [identifier = f.sub.123] [identifier = f.sub.253] [identifier = f.sub.253] [EOV] [EOV] [EOV] [EOE] [EOE] [EOE] [EOG] [EOG] [EOG] [checksum] [checksum] [checksum]\n The above logic statements about terrorists \"Boris\" and \"Umar\" are represented via the subgraphs illustrated in FIG. 17b.  These subgraphs are transmitted to the Black Box (via, e.g., the designated U.S.  Intelligence Community input converter)\nwith the following messages:\n TABLE-US-00006 Boris Umar [SOG] [SOG] [SOE] [SOE] [name_of_type = 3] [name_of_type = 3] [SOV] [SOV] [terrorist_type: 6] [terrorist_type = 6] [identifier = t.sub.1] [identifier = t.sub.2] [EOV] [EOV] [SOV] [SOV] [name_type = 1] [name_type = 1]\n[name = \"Boris Badenov\"] [name = \"Umar Farouk Abdulmutallab\"] [EOV] [EOV] [EOE] [EOE] [EOG] [EOG] [checksum] [checksum]\n The resulting aggregated graph that appears within the Black Box (though not visible to anyone) is shown in FIG. 17c.  And the graph representation for the Black Box search pattern is shown in FIG. 18a.  Upon approval by the privacy body, this\npattern (FIG. 18a) is input to the Black Box via the pattern adapter-converter channel via the following message:\n [SOG]\n [SOE]\n [name_of_type=3]\n [SOV]\n [person_type=0]\n [identifier=p]\n [EOV]\n [SOV]\n [name_type=1]\n [name=?]\n [EOV]\n [EOE]\n [SOE]\n [name_of_type=3]\n [SOV]\n [terrorist_type=6]\n [identifier=t]\n [EOV]\n [SOV]\n [name_type=1]\n [name=?]\n [EOV]\n [EOE]\n [SOE]\n [flight_of_type=7]\n [SOV]\n [person_type=0]\n [identifier=p]\n [EOV]\n [SOV]\n [flight_type=5]\n [flight=[OUTPUT]]\n [EOV]\n [EOE]\n [EOG]\n [checksum]\n Within the logic engine the execution (search) of this pattern requires a straight forward reasoning process readily handled by most modest inference engines.  As specified in the approved input pattern, the Black Box outputs a simple one-vertex\nsubgraph for each pattern detected.  For the above example, this consists of the simple graph shown in FIG. 18b transmitted via the following graph exchange message:\n [SOG]\n [SOV]\n [flight_type=5]\n [EOV]\n [EOG]\n [checksum]\n The above example illustrates how both private and very sensitive (e.g. classified) information about individuals can be examined while maintaining privacy.  In this case, the Black Box outputs only the flight identifier, indicating that there\nis a valid terrorism concern, but without any U.S.  entity ever needing to exam the protected PNR information, nor the EU ever needing to exam the classified TIDE information.  Without requiring any new legal mechanisms, the organizations involved here\ncan use their existing investigate authorities to locate the specific individual, given that a specific terrorist threat has been established for a specific flight.  Via existing treaty, the U.S.  and the EU already have mechanisms in place to rapidly\nexpedite this investigative process under such compelling circumstances.\n The Abdulmutallab incident described above was heralded as a significant intelligence failure that prompted the President to initiate a major review of the intelligence process with an extensive investigation and senate committee hearings. \nThese investigations resulted in significant changes to international travel and Intelligence Community operations.\n Today, more than five years later, the disappearance of Malaysian Flight 370 has revealed the existence of the same types information sharing challenges and trust issues.  Two individual were allowed to board the aircraft with passports known to\nhave been stolen from citizens in European countries.  While Interpol maintains a database of lost and stolen passports, few countries consistently use this database due to privacy concerns.  The Black Box approach promises a robust solution to this\nunresolved, recurring problem.\n The Black Box solution also finds applicability in the \"name resolution\" use case.  In today's information age, organizations frequently provide overlapping services to individuals.  Such overlap can be costly, resulting in unnecessary\nduplication and expenditure of resources.  Resolving this overlap, however, can be extremely complex and time-consuming.  Where individuals live, where they work, and how and where they receive these services, and how and when these might change can all\nvary.  Further complicating this process is the incompleteness, errors, and ambiguity in the information each organization may associate with an individual.  The spelling of names, accuracy of birthdates, absence of a consistent universal identifier\n(e.g. in the U.S., a Social Security Number), etc. all compound this resolution complexity.  Given the sensitive nature of the personal information and the complex policies and law regarding its proper handling, organizations unfortunately are often\nforced to resort to costly, time-consuming manual methods to address duplication discrepancies.  An application of the Black Box approach to privacy to help automate this process in near real-time fashion while maintaining and preserving the privacy of\neach individual's information is described.\n In a particular implementation of the solution to the name resolution use case Black Box system consists of a single, self-contained computer that is physically contained within a steel reinforced enclosure with multiple security locks (one for\neach participating organization).  This unit is intended to be housed in a non-descript, limited access Tier 3 data center facility with 24/7 video motion detected alarm surveillance.  The enclosure is configured such that the computer within cannot be\nremoved without resulting in loss of its electrical power.  The computer itself is delivered sealed from the factory and is installed and configured only in the presence of security representatives from each organization.  The computer is equipped with\nthe most minimal of services, disabling nearly all features including the removal of keyboard and mouse input, video display, and unnecessary operating systems components.  The disk contents are secured with high-grade encryption.  All wireless\ninterfaces (e.g. WiFi and Bluetooth) are disabled, and no external I/O devices may be attached nor ports accessed once secured within the enclosure.\n The operating system, network, and supporting firewall infrastructure are configured to allow only secure file transfer access into and out of specific directories, one directory allocated for each participating organization.  The only\noperations that are permitted by an organization are reading, writing and deleting files in their respective directories.  All file accesses are performed via high-grade commercial public-key end-to-end encryption.  No other external operations are\npossible with the enclosure/computer other than the removal of its network connection and or the unplugging of its power cord.  All administration services, login capabilities, web services, e-mail, etc. have been disabled and/or removed.  Specifically,\nthe computer is configured to execute one program and one program only.  That is, the computer executes only the single program that has been tested, inspected, and unanimously approved by the policy/oversight body, which in this case is to consist of\nthe security representatives from each organization.  In this Type 3 device configuration, it is this single program that implements the privacy device's \"pattern\" detection algorithm referenced above.\n The security configuration process for this device is meticulous and comprehensive requiring the presence of multiple individuals in order to make even the slightest of changes.  As a result, the reliability of this device and its correctness is\nof highest concern.  Failure to accurately compute results or properly protect the information contained within renders the device useless, with significant loss of confidence from each of the participating organizations and their constituency. \nConsistent with its design goals, the Ada programming language is selected for the pattern-matching algorithm implementation, discussed below.\n Operation of this privacy device is intentionally very simple.  In order for organizations to identify potential duplication issues, each must generate a data file that contains a set of records for the individuals covered in their respective\ndatabases.  For the initial prototype system, key fields include: Last name (bounded Latin-1 Unicode string) First name (bounded Latin-1 Unicode string) Date of birth (Latin-1 string; YYYYMMDD format) Gender (\"M\" or \"F\" or \"U\" if unknown) Ethnicity\n(enumerated type) Universal identifier (e.g. U.S.  SSN, if it exists) Local identifier (bounded Latin-1 Unicode string) Using an agreed upon data file format for these records, each organization must securely transfer its file to its respective directory\non the privacy device computer.  Within the computer, a single program is running that continuously examines each of the directories for new data files.  When a new data file is detected, the file is ingested, an in-memory representation of the data is\ncreated, and the filed is immediately security deleted using multiple file re-writes.  The directory scan time and file ingest time are specifically engineered so that the raw data files from contributing organizations resides on the computer disk for a\nminimal amount of time (e.g. &lt;1 second).\n Whenever a new data representation is obtained for an organization, any old representation is immediately discarded (released in memory), and the new representation is compared against the representations held for each of the other\norganizations.  After the comparison is made, an output file is prepared for each organization, identifying only those matches that are made with records for another organization.  Match files remain in a directory until deleted by the respective\norganization (or whenever the privacy device system is restarted via power cycling).  To further prevent PII exposure, the match files contain only the local organization's unique identifiers, and no other source data fields.  Each of the participating\norganizations can then use these identifiers to discuss possible lost-to-care or duplicate-care issues with the other corresponding organizations.\n As the privacy device computer intentionally contains no console or display that enables anyone to ascertain its operating status, the pattern-matching program maintains a simple log file in each organization's directory.  This log contains the\ndates and times of ingested data files, when the matching process is performed, and the generated match files.  Any errors detected in the input data files are reported back to the respective organization through this log mechanism.\n The component of greatest concern is arguably the pattern-matching algorithm (program) contained within the privacy device.  From a reliability perspective, if this program were to fail (e.g. via an uncaught constraint error exception), the end\nusers of this system would have no way of knowing when or the cause.  Although encrypted, such a failure could result in a file containing PII persisting far beyond it expected (very short) lifetime on the device.  Such failures, however, could also\nseverely jeopardize an organization's confidence and trust in the device.  If the device is not reliable, organizations may be justifiably skeptical of its accuracy and its ability to protect such important information.  The resulting loss of trust would\nrender the privacy device of little or no value, with the possibility of introducing harm via improper disclosure or wasted time pursuing inaccurate results.  Thus, the reliability of the algorithm is of utmost importance to the process.\n The Ada programming language was selected for the algorithm implementation.  Its unambiguous semantics, extremely strong type and constraint checking, exception protections, and overall reliability philosophy were key ingredients leading to this\ndecision.\n The main subprogram of the prototype algorithm is the following infinite loop:\n TABLE-US-00007 with Black_Box; use Black_Box; procedure Main is begin Initialize; -- Erase/build directories & logs loop if Update then -- Check for new data files Analyze; -- Search for matches Report; -- Report matches Clear; -- Clear matches\nend if; deley scan_time; end loop; end Main;\n The subprogram Initialize is used to create each organization's directory and corresponding log file should, they not already exist.  It the directory does exist, its contents is erased, ensuring a fresh start.  The package Black_Box contains\nthe data structures that represent each organization's data set representation and resulting cross organizational matches, along with the operations that act upon them (Update, Analyze, Report, and Clear).  Each of these subprograms is coded so that they\nsuccessfully complete, regardless of any internal exceptions that might result.\n Of all the subprograms, Update is perhaps the most worrisome and complex as it involves the ingestion of external data files.  While each organization agrees to a single input format, the algorithm can make no assumptions regarding the input\nfile's compliance with that format as an uncaught constrain failure would render the system painfully inoperative.  Thus when a new data file is detected within Update, the input file must be carefully parsed to ensure proper range values and format.  In\nactual practice, it is not uncommon for the source databases to contain blank fields or legacy field formats containing various wild card characters or special values for missing data elements (e.g. a birth year, but no birth month or day, or\n\"000-00-0000\" when a SSN is unknown).  The Update subprogram's job is to reliably parse through all these various possibilities, reporting format errors back to an organization through its log file, ultimately creating a vector of properly type\nconstrained person records for the corresponding organization.  If the process is successful, Update returns a true value.  However, if an unrecoverable problem is detected, false is returned, precluding the subsequent matching and reporting operations\nfrom executing until a new data file is received for the organization.\n For an initial pilot system, the following person record definition is used, where each record component is a fixed length string, bounded string, or enumeration type:\n TABLE-US-00008 type person is record org_id : identifier; last_name : name_string; first_name : name_string; last_soundex : name_string; first_soundex : name_string ; dob : dob_string; ssn : ssn_string; gender : sex; race : ethnicity;\npartial_ssn : boolean; partial_dob : boolean; end record;\n With a successful (true) completion of Update, the remaining operations Analyze and Report are far less perilous as all data structures are now properly type and range constrained in comfortable Ada fashion.  The primarily role of Analyze is to\ncreate a vector of match records across all of the person representations.  Match records contain enumeration values that identify the organization, their respective person record unique identifiers, and a set of boolean values that characterize which\nand how their fields match including the incorporation of fuzziness based on Soundex values for first and last names and partial matches on date of birth and SSN.  Match records are defined as follows:\n TABLE-US-00009 type match is record o1 : organization; o2 : organization; uid1 : identifier; uid2 : identifier; last_name : Boolean := false; first_name : Boolean := false; last_soundex : Boolean := false; first_soundex : Boolean := false; dob :\nBoolean := false; partial_dob : Boolean := false; sex : Boolean := false; ssn : Boolean := false; partial_ssn : Boolean := false; score : scoring := none; end record;\n Match records also contain a score field that characterizes the degree of match based on criteria established in advanced by the organizations.  The value of this score ranges from an exact match down to a very low match as defined by the\nfollowing enumerated type: type scoring is (exact, very_high, high, medium_high, medium, medium_low, low, very_low); The actual scoring criteria is encoded within the Analyze subprogram via a sequence of if-then-elsif conditional style statements similar\nto the following, where the record m is of type match:\n TABLE-US-00010 if (m.last_name and m.first_name and m.dob and m.ssn and m.sex) then m.score := exact; elsif (m.last_name and m.first_name and m.dob and m.sex) or m.ssn then m.score := very_high; elsif ...  ... elsif ...  ... m.score := very_low;\nend if;\n The subprogram Report has very a predictable role and behavior, predominately using Ada.Text_IO to create output files in each of the appropriate organization directories to report the matching results.  To ensure no memory leaks, the remaining\nClear subprogram is used to properly release the dynamic data structures used in the matching process, before repeating the entire process after a short specified time delay.\n The embodiments described herein provide an approach (system and method) for organizing and analyzing our new data-centric world, sharing information derived from that data across multiple institutions, and unifying this information into a\ncommon global knowledge framework in support of scientific analysis at a scale never before possible.  This approach was specifically formulated to address complex problems related to global security, world health, and planet sustainability.  Through the\ncreation of this single replicable infrastructure component, a powerful method for information sharing and analytic operability results, allowing institutions both large and small with different computational resources to participate.  As a result, the\nemerging knowledge space enables multi-disciplinary integrative research with a fidelity and diversity previously unobtainable.  In addition, the approach can be recursively applied, enabling research communities working at various levels of abstraction\nto computationally interact, from the macroscopic such as climatic, geologic, and sociopolitical systems, to the microscopic such as biological, cellular, and genomic systems.\n The embodied infrastructure component provides an innovative method for multiple institutions to meaningfully collaborate within a single unified, scalable computational fabric.  This interoperability is achieved through the creation of shared\n\"knowledge space\" that overlays the data infrastructures of the participating institutions.  This space is specifically designed to accommodate models of complex interdependent systems at global scale.  Furthermore, participants create this space using\nopt-in/opt-out semantics, requiring minimal data replication and movement.  Instead, the technique employs a data adapter process that transforms source data into sets of entities, relationships, and accompanying attributes, all expressed and shared\nwithin this single space, yet managed by the data owner.  The managed ontology that accompanies this space is readily extensible, enabling broad application across many diverse scientific domains.\n In an exemplary use case, one or more of the embodiments described herein were implemented to facilitate cross-jurisdictional HIV data sharing.  The HIV care continuum depicts the stages of health system engagement for those living with HIV\ndisease.  This model assists the public health community by quantifying care engagement in steps along the continuum.  However, ambiguities limit its utility when applied to cross-jurisdictional regions.  More specifically, its inherent linear assumption\ndoes not account for the churn effect (person in -and-out migration for HIV care) across jurisdictional boundaries.  Addressing this challenge requires a structured and consistent data-sharing mechanism in such regions.  However, traditional data sharing\nacross jurisdictions is legally challenging, technologically time-consuming, and organizationally quite complex.\n The cross-jurisdictional District of Columbia (DC) metropolitan region, including areas in DC, Maryland (MD), and Virginia (VA), experiences some of the highest prevalence rates among key population groups in the country, and is therefore\ncritical to the national response to HIV.  This region has long observed that persons migrate from one jurisdiction to another and cross borders for HIV care, or changes place of residence or places of both residence and care.  The need for a novel\nreal-time and automated approach to data sharing in this cross-jurisdictional area, which could simultaneously account for the highly private and sensitive nature of HIV data was identified.  By implementing the shared knowledge space infrastructure\ndiscussed herein across relevant data sources, it just took 21 minutes and 58 seconds of processing time to identify 21,472 person matches in HIV surveillance (eHARS) data across DC, MD and VA.\n The benefits of implementing the present embodiments in this example are numerous and include the ability to validate new HIV cases in an automated and real-time fashion, replacing a cumbersome manual process with a streamlined process providing\nmore immediate opportunities to intervene in new case diagnoses, or for sharing information about persons who have moved with fellow jurisdictions.  The data-sharing agreements developed here can also benefit future collaborative efforts by serving as\ntemplates.  The HIV community faces a new era of engagement in HIV care, in which HIV-infected individuals live longer with the disease, and where mobility and technology are becoming increasingly more common in everyday life.  Effectively adapting to\nthis new era requires re-examining traditional, and brainstorming new HIV surveillance data sharing models and technology.  Developing the organizational processes that facilitate such activities are essential to improving HIV surveillance informing care\nand prevention services, and ultimately achieving better health outcomes.\n The knowledge-share infrastructure described herein can be implemented to analyze human responses (e.g., forced migration) to numerous environmental and man-made changes, including, but not limited to: major weather and geological events (e.g.,\nfloods, hurricanes, tsunamis, earthquakes, drought); war; famine; disease outbreaks; and the like.  In a second exemplary use case, the infrastructure has been implemented to focus on human responses to floods in terms of people's perceptions,\nexpectations and intentions as they anticipate and experience the onset and aftermath of storm-driven flooding, specifically in Hoboken, N.J.  The use case incorporates the 7 billion person-entity Sapien Database developed and maintained by Georgetown\nUniversity; a high-fidelity simulation of the physical layout and structure of Hoboken, N.J.  developed and maintained by Stevens Institute of Technology; a storm surge model that predicts water levels on each block, over time, depending on the selected\nseverity of the storm (developed by Stevens Institute of Technology); and a model of local perception of threat (developed by Georgetown University).\n Applying the knowledge-share infrastructure described herein to these separate sources enables multi-site collaboration of scientists using dispersed databases, distinct analytics, and existing visualization tools on a significant problem in a\ndensely-populated location in the United States.\n Each element of this second use case currently exists but in isolation from each other.  For example, in collaboration with the city of Hoboken, Stevens has instrumented the entire city of Hoboken with sensors that feed relevant data to archives\non the Stevens campus.  Alan Blumberg has developed the hydrology model that quite accurately predicts the degree and spatial location of flooding in the Hoboken-New York City region.  Stevens hosts Virtual Hoboken, a high fidelity, three-dimensional\nreplica of Hoboken that was built with LIDAR imagery and is interactively visualized in Stevens' Immersion Lab with 8' by 20' 180 degree touch sensitive screens.  Using Virtual Hoboken, a user can virtually walk (or fly) around the city and see the view\nfrom any window of any building.  Further, employing data from LANDSCAN at Oak Ridge National Laboratory and other sources, the Sapien database at Georgetown University maps 7 billion statistical persons to their location on the globe in a\nhypergraph-structured knowledge base.  Georgetown has also developed a model of local perception of threat that helps anticipate the movement of people when faced with local threats in menacing contexts.  Applying the knowledge-share infrastructure as\ndescribed herein to these sources, using an ontology devised in accordance with the modeling, enables scientists from Stevens and Georgetown to collaborate on research about urban resilience using these resources without moving the data, analytics or the\nvisualization center from their current institutional homes.\n More particularly, as described above, the knowledge-share infrastructure uses an ontology to coordinate the parts of the distributed hypergraph stored by different organizations, on different servers, in different locations.  Not all of the\ninstances of the same class will be sustained by the same organization.  For example, today Stevens Institute of Technology has a hydrology model that includes every building in Hoboken, N.J.  We expect Stevens will create knowledge space entities for\neach of these buildings, entities of the class \"Building\".  Similarly, today Georgetown sustains the Sapien database with 7 billion artificial persons, one for each person in the real world, geographically distributed the way that people are distributed\nin the world.  These 7 billion entities in the knowledge space are all of class \"Person\".  Many of these Georgetown-sustained people entities reside in buildings, and we thus expect Georgetown's analytic models to create entities of class \"Occupant\" to\nrepresent the relationships between the people and the buildings in which they reside.\n The following discussion provides a brief, general description of a suitable computing environment to implement embodiments set forth herein.  In accordance with various Figures above, including but not limited to, FIGS. 1, 4, 6, 8, and 12-14\nand the descriptions thereof, embodiments may be implemented using various and different combinations of hardware and software in different physical locations.  Example computing devices include, but are not limited to, personal computers, server\ncomputers or server farms, hand-held or laptop devices, mobile devices (such as mobile phones, Personal Digital Assistants (PDAs), media players, and the like), multiprocessor systems, consumer electronics, mini computers, mainframe computers, and\ndistributed computing environments that include any of the above systems or devices, and the like.\n Although not required, embodiments are described in the general context of \"computer readable instructions\" being executed by one or more computing devices.  Computer readable instructions may be distributed via computer readable media\n(discussed below).  Computer readable instructions may be implemented as program modules, such as functions, objects, Application Programming Interfaces (APIs), data structures, and the like, that perform particular tasks or implement particular abstract\ndata types.  Typically, the functionality of the computer readable instructions may be combined or distributed as desired in various environments.\n Individual computing devices configured to implement one or more embodiments provided herein include at least one processing component and memory.  Depending on the exact configuration and type of computing device, memory may be volatile (such\nas RAM, for example), non-volatile (such as ROM, flash memory, etc., for example) or some combination of the two.  The individual computing devices may include additional features and/or functionality such as additional storage (e.g., removable and/or\nnon-removable) including, but not limited to, magnetic storage, optical storage, and the like.  Examples of computer storage media include, but are not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, Digital Versatile Disks\n(DVDs) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by one or more computing\ndevices.  Any such computer storage media may be part of the one or more computing devices.\n Individual computing devices also include communication connection(s) for facilitating communication with other devices.  Such communication connection(s) include, but are not limited to, a modem, a Network Interface Card (NIC), an integrated\nnetwork interface, a radio frequency transmitter/receiver, an infrared port, a USB connection, or other interfaces for connecting computing devices.  Communication connection(s) may include a wired connection or a wireless connection and may operate in\ntransmit and/or receive modes.\n Computing devices may include or be connected to input device(s) such as keyboard, mouse, pen, voice input device, touch input device, infrared cameras, video input devices, and/or any other input device.  Similarly, computing devices may\ninclude or be connected to output device(s) such as one or more displays, speakers, printers, and/or any other output device.  Input and output device(s) may be connected to computing devices via a wired connection, wireless connection, or any\ncombination thereof.\n Components of computing devices may be connected by various interconnects, such as a bus.  Such interconnects may include a Peripheral Component Interconnect (PCI), such as PCI Express, a Universal Serial Bus (USB), firewire (IEEE 1394), an\noptical bus structure, and the like.  Components of computing devices may be interconnected by a network and need to not be in the same physical locations.\n Those skilled in the art will realize that storage devices utilized to store computer readable instructions may be distributed across a network.  For example, a computing device accessible via network may store computer readable instructions to\nimplement one or more embodiments provided herein.  Computing device may access other computing devices and download a part or all of the computer readable instructions for execution.  Alternatively, computing devices may download pieces of the computer\nreadable instructions, as needed, or some instructions may be executed at different computing devices connected over a network.\n Various operations of embodiments are provided herein.  In one embodiment, one or more of the operations described may constitute computer readable instructions stored on one or more computer readable media, which if executed by a computing\ndevice, will cause the computing device to perform the operations described.  Unless so stated, the order in which some or all of the operations are described should not be construed as to imply that these operations are necessarily order dependent. \nAlternative ordering will be appreciated by one skilled in the art having the benefit of this description.  Further, it will be understood that not all operations are necessarily present in each embodiment provided herein.\n One skilled in the art recognizes that although the disclosure has been shown and described with respect to one or more embodiments, equivalent alterations and modifications are anticipated upon a reading and understanding of this specification\nand the annexed drawings.  The disclosure includes all such modifications and alterations and is limited only by the scope of the following claims.", "application_number": "15983814", "abstract": " A knowledge model \"overlay\" for organizing and analyzing large, dynamic\n     data volumes is implemented as a hypergraph that manifests as a result of\n     a distributed theory-driven data source transformation process. This\n     process maps exponentially growing data into an asymptotically limited\n     information space. Within this space, real-world entities (e.g. people,\n     computers, cities, Earth) and their complex interdependencies (e.g.\n     social networks, connectivity, causal relationships) are represented as\n     attributed hypergraph elements (i.e. both hypervertices and hyperedges).\n     Attributes are represented as state vectors affixed to any hypergraph\n     element. Transformation of source system data into this overlay structure\n     is accomplished with minimal data movement and replication using a\n     universal \"pointer\" like mechanism that is managed in a decentralized\n     fashion by the respective transformation components. Access to the\n     knowledge overlay is performed via a hypergraph communication protocol\n     encapsulated within a common hypergraph API and accompanying hypergraph\n     toolkit.\n", "citations": ["5752017", "5809499", "6108787", "6850252", "6988093", "7603344", "7649452", "8250235", "9507875", "20020112181", "20030174723", "20040111639", "20040133536", "20050108526", "20050203892", "20070105627", "20070182983", "20080072290", "20080263130", "20090055934", "20090254572", "20090300002", "20100049687", "20100085948", "20100287158", "20100290476", "20100318655", "20110167110", "20110258303", "20110295854", "20120131189", "20120131530", "20120330959", "20130111434", "20130158975", "20130212131", "20140122228"], "related": ["14724921", "62005385", "61114883"]}, {"id": "20180324117", "patent_code": "10341272", "patent_name": "Personality reply for digital content", "year": "2019", "inventor_and_country_data": " Inventors: \nBadr; Ibrahim (Zurich, CH), Kumar; Aayush (Zurich, CH), Bakir; Goekhan Hasan (Zurich, CH), Grimsmo; Nils (Adliswil, CH), Buisman; Bianca Madalina (Kilchberg, CH)  ", "description": "FIELD\n The present specification is related to mobile devices.\nBACKGROUND\n User devices such as mobile smartphones may include digital cameras that capture digital images depicting particular content items.  Such user devices may also include a computing system that executes program code for using at least one\nmessaging application to exchange data communications during an electronic conversation.  An electronic conversation can include at least two users that exchange data communications which may include the sharing of digital image and video data.\nSUMMARY\n According to the described technologies, a computing system receives an item of digital content from a user device, such as a digital image that depicts a particular item.  The system generates one or more labels that indicate attributes of the\nitem of digital content.  At least one label can be descriptive of the particular item depicted in the digital image.  The system generates one or more conversational replies to the item of digital content based on the one or more labels that indicate\nattributes of the item of digital content.  The system selects a conversational reply from among the one or more conversational replies and provides the conversational reply for output to the user device.\n One aspect of the subject matter described in this specification can be embodied in a computer-implemented method.  The method includes, receiving, by a computing system and from a user device, an item of digital content; generating, by the\ncomputing system, one or more labels indicating attributes of the item of digital content; based on the one or more labels that indicate attributes of the item of digital content, generating, by the computing system, one or more conversational replies to\nthe item of digital content; selecting, by the computing system and from among the one or more conversational replies, a conversational reply; and providing, for output to the user device, the conversational reply.\n These and other implementations can each optionally include one or more of the following features.  For example, in some implementations, generating the one or more conversational replies includes: determining, by at least one module, a\nsimilarity score that indicates a similarity between the item of digital content and one other item of digital content that is associated with an electronic conversation; and responsive to the similarity score exceeding a threshold similarity score,\ngenerating, by the at least one module and based on the other item of digital content, the one or more conversational replies and a confidence score for each conversational reply of the one or more conversational replies; and wherein selecting the\nconversational reply includes: selecting, from among the one or more conversational replies, a particular conversational reply based on the confidence score for the particular conversational reply.\n In some implementations, generating the one or more conversational replies includes: analyzing, by at least one module, content of a content database comprising multiple items of media content, wherein the content is associated with at least one\nitem of media content; determining, by the at least one module, a similarity score that indicates a similarity between at least one label of the one or more labels and the analyzed content; and responsive to the similarity score exceeding a threshold\nsimilarity score, generating, by the at least one module and based on a particular item of media content, the one or more conversational replies and a confidence score for each conversational reply of the one or more conversational replies; and wherein\nselecting the conversational reply includes: selecting, from among the one or more conversational replies, a particular conversational reply based on the confidence score for the particular conversational reply.\n In some implementations, the at least one item of media content includes content that is reproduced based on video data, audio data, electronic text data, or digital image data, and wherein the content associated with the at least one item of\nmedia content is a quotation of text or speech from media content of the database.\n In some implementations, generating the one or more conversational replies includes: determining, by at least one module, a first similarity score that indicates a similarity between at least one label of the one or more labels and a\npredetermined conversational reply of a reply content database; and responsive to the first similarity score exceeding a threshold similarity score, generating, by the at least one module and based on a particular predetermined conversational reply, the\none or more conversational replies and a confidence score for each conversational reply of the one or more conversational replies; and where selecting the conversational reply includes: selecting, from among the one or more conversational replies, a\nparticular conversational reply based on the confidence score for the particular conversational reply.\n In some implementations, generating the one or more conversational replies further includes: determining, by the at least one module and based on the one or more labels, a related entity that has a threshold relevance to the item of digital\ncontent, wherein the related entity is determined using a knowledge graph; determining, by the at least one module, a second similarity score that indicates a similarity between at least one label of the one or more labels, the determined related entity,\nand the particular predetermined conversational reply; and responsive to the second similarity score exceeding a threshold similarity score, selecting by the at least one module, a predetermined conversational reply for inclusion with the one or more\nconversational replies generated by the computing system.\n In some implementations, generating the one or more conversational replies includes at least one of: generating, by a first module of the computing system, a first conversational reply and generating a first confidence score for the first\nconversational reply; and generating, by a second module of the computing system, a second conversational reply and generating a second confidence score for the second conversational reply.  In some implementations, generating the first confidence score\nfor the first conversational reply by the first module includes: determining a first relevance parameter that indicates a relevance between the first conversational reply and the item of digital content; and generating the first confidence score based on\nthe first relevance parameter.\n In some implementations, generating the second confidence score for the second conversational reply by the second module includes: determining a second relevance parameter that indicates a relevance between the second conversational reply and at\nleast one label of the one or more labels; and generating the second confidence score based on the second relevance parameter.  In some implementations, generating the one or more labels indicating an attribute of the item of digital content includes:\nusing a digital image recognition system to analyze the received item of digital content; determining, based on analysis of the received item of digital content, a particular content item of the item of digital content; and generating, based on the\ndetermined particular content item, the one or more labels indicating an attribute of the item of digital content.\n In some implementations, the item of digital content includes image content received by the computing system from the user device, and the method further includes: selecting a particular number of conversational replies from among the one or\nmore conversational replies based on respective confidence scores of the one or more conversational replies; and providing, for output by the user device, the particular number of conversational replies as a suggested reply to the image content received\nby the computing system.\n In some implementations, providing the conversational reply selected from among the one or more conversational replies includes: the computing system receiving the image content as a communication message of an electronic conversation generated\nby a messaging application, the electronic conversation including at least one user; providing the particular number of the conversational replies as a suggested reply message to the communication message of the electronic conversation; and receiving, by\nthe computing system, an indication that the at least one user selected a conversational reply from among the particular number of conversational replies as a reply message to the communication message of the electronic conversation.\n In some implementations, generating the one or more labels indicating attributes of the item of digital content includes: generating respective confidence scores for each label of the one or more labels, the respective confidence scores each\nindicating a relevance of a label to an attribute of the item of digital content; and selecting at least one label based on a confidence score of the at least one label exceeding a threshold confidence score.\n Other implementations of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.  A system of one or more\ncomputers can be configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes or cause the system to perform the actions.  One or more\ncomputer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.\n The subject matter described in this specification can be implemented in particular implementations and can result in one or more of the following advantages.  A computing system according to the described technologies can be configured to\ngenerate one or more conversational replies that are descriptive of an item of digital content.  For example, the item of digital content can be a digital image that depicts a particular item and a generated conversional reply may either identify the\nparticular item, or may indicate an attribute of the particular item.\n In some instances, conversational replies that indicate an attribute of an item of digital content can include text or image content that may be perceived as delightful, pleasant, or pleasing to a user.  For example, a computing system can\ninclude a database of predetermined conversational replies and at least one generated reply can be selected from among the predetermined conversational replies of the database.  The predetermined replies can be curated such that each predetermined\nconversational reply includes at a portion of text or image content that may be perceived as delightful.\n The details of one or more implementations of the subject matter described in this specification are set forth in the accompanying drawings and the description below.  Other features, aspects, and advantages of the subject matter will become\napparent from the description, the drawings, and the claims. BRIEF DESCRIPTION OF THE DRAWINGS\n FIG. 1 is an example computing system for generating one or more conversational replies.\n FIG. 2 illustrates a diagram including modules associated with an example computing device of the computing system of FIG. 1.\n FIG. 3 is a flow diagram of an example process for generating one or more conversational replies.\n FIG. 4 is a block diagram of a computing system that can be used in connection with computer-implemented methods described in this specification.\n Like reference numbers and designations in the various drawings indicate like elements.\nDETAILED DESCRIPTION\n According to the described technologies, a computing system receives an item of digital content from a user device, such as a digital image that depicts a particular item.  The system generates one or more labels that indicate attributes of the\nitem of digital content.  For example, the labels can include words or text phrases that are descriptive of the particular item depicted in the digital image.  The system generates one or more conversational replies to the item of digital content based\non the one or more labels that indicate attributes of the item of digital content.  The system selects a conversational reply from among the one or more conversational replies and provides the conversational reply for output to the user device.\n FIG. 1 is an example computing system 100 for generating one or more conversational replies.  System 100 generally includes user device 102.  Example user devices 102 can include smartphones, mobile computing devices, laptop/desktop computers,\ntablet devices, smart televisions, gaming consoles, or other related computing device.\n User device 102 can include a digital camera, and a user of device 102 can use the digital camera to capture image content.  In the context of system 100, the captured image content can be an item of digital content, such as digital image,\ndigital photo, or electronic picture that includes or depicts a particular item/content item 104.  As shown in FIG. 1, the user may be located in Paris, France and the captured image content may include a particular content item 104 such as the Eiffel\ntower that is also located in Paris, FR.\n User device 102 can execute program code for enabling a virtual device assistant of the device.  In some implementations, a device assistant of user device 102 can be configured to generate one or more replies 105 based on input 107 received by\nuser device 102 or based on an input 107 corresponding to image content captured by user device 104.  For example, a camera of user device 102 can capture image content and a computing system of user device 102 can cause the device assistant to generate\nreply content 105 based on the captured image content.\n Current device assistants, or other conventional application programs that process inputs, may generate example reply content that can be perceived by a user as lacking \"personality.\" For example, current device assistants are often configured\nto provide (or suggest) machine generated replies that often times are not perceived by a user as being delightful or conversational in a tone, nature or substance.\n In particular, current device assistants, or related application programs that process digital image content, may provide or suggest a reply such as \"I can see images.\" Although this reply is not inaccurate given the received input, such reply\ncontent may be perceived by a user as overly terse and lacking of a certain conversational feel.  Thus, this reply might not attract the interest of a user and, hence, may not elicit a response or additional queries from the user.\n Referring again to FIG. 1, according to technologies described herein, system 100 can be configured to generate one or more conversational replies that are, for example, at least descriptive of an item of digital content (e.g., a digital photo). For example, system 100 can receive an item of digital content that corresponds to input 107.  The item of digital content can be a digital image that depicts a particular item 104, e.g., the Eiffel tower.  In contrast to current device assistants or\nother current programs, a generated conversional reply 105 of system 100 can either identify the particular item 104, or can indicate an attribute of the particular item.\n For example, a conversational reply 105 can be \"I am no architect, but the Eiffel tower seems like quite a construction!\" In particular, conversational reply 105 identifies the particular item 104 as being the Eiffel tower, and includes text\ncontent that is descriptive or indicative of an attribute of the Eiffel tower, e.g., that the Eiffel tower is a \"construction,\" such as a physical structure or building.  Further, in this implementation, reply 105 is not overly terse and includes content\nthat may be perceived by a user as having more of a conversational tone.\n As described in more detail below, in some implementations, in addition to indicating an attribute of an item of digital content, conversational reply 105 can include text or image content that may be perceived as delightful, interesting,\npleasant, or pleasing to a user.  One or more conversational replies generated by components or devices of system 100 can be provided for output to user device 102 and may be generated for presentation to a user via display 103 of user device 102.\n As shown, system 100 includes a computing device/server 106 that receives data signals, e.g., non-transitory propagating signals, from at least one user device 102.  As shown, server 106 can include an image recognition module 108, a previous\nreplies module 110, a media content replies module 112, a predetermined replies module 114, and a reply selection module 116.  In some implementations, server 106 can include additional or fewer modules and system 100 can include one or more additional\nservers or computing devices.\n Module 108 depicted in FIG. 1 is generally representative of image or data analysis, image feature extraction, and label generation functions that can be executed or performed by server 106.  An output of module 108 can include at least one of:\ni) one or more labels that indicate attributes of an item of digital content provided by, or received from, user device 102; or ii) image data or image pixel data associated with digital image content corresponding to the item of digital content.\n Labels and/or image pixel data output by module 108 can be provide to, or received by, one or more of modules 110, 112, and 114.  As used herein, labels generated by module 108 can be individual words or text phrases that indicate one or more\nattributes of an item of digital content or that describe one or more features of an item of digital content.  As described in more detail below, each word or text phrase can be assigned a relevance or confidence score that indicates a relevance of a\nparticular word or text phrase (e.g., a label) with regard to attributes or features of a received item of digital content.\n Each of modules 110, 112, and 114 depicted in FIG. 1 are generally representative of data analysis and data signal processing functions that can be executed by server 106 to generate one or more conversational replies based on label or image\npixel data received from module 108.  For example, each of modules 110, 112, and 114 can include one or more databases having multiple content items and can also include program code or logic configured to access the databases and to use the content\nitems to generate respective sets of conversational replies.  As described in more detail below, each conversational reply of the respective sets of conversational replies can be assigned a confidence score that indicates a relevance of a particular\nconversational reply with regard to labels or pixel data received from module 108.\n Reply selection module 116 includes program code or logic that can analyze scoring and/or ranking data associated with respective sets of conversational replies generated by each of modules 110, 112, and 114.  One or more conversational replies\nselected by module 116 can be provided for output to user device 102 by server 106.  Modules 108, 110, 112, 114, and 116 are each described in more detail below with reference to FIG. 2.\n As used in this specification, the term \"module\" is intended to include, but is not limited to, one or more computers configured to execute one or more software programs that include program code that causes a processing unit(s) of the computer\nto execute one or more functions.  The term \"computer\" is intended to include any data processing device, such as a desktop computer, a laptop computer, a mainframe computer, a personal digital assistant, a server, a handheld device, a tablet device, or\nany other device able to process data.\n FIG. 2 illustrates a diagram including an example module grouping 200 associated with computing server 106 of system 100.  Module grouping 200 can be disposed within server 106 or can include independent computing devices that collectively are\ncoupled to, and in data communication with, server 106.  Module grouping 200 generally includes modules 108, 110, 112, 114, and 116 discussed briefly above with reference to FIG. 1, and an entity relatedness module 234.\n In general, described actions or functions of user device 102, server 106, and modules of module grouping 200 can be enabled by computing logic or instructions that are executable by a processor and memory associated with these electronic\ndevices.  For example, each of user device 102, server 106, and module grouping 200 (collectively \"devices of system 100\") can include one or more processors, memory, and data storage devices that cooperatively form a computing system of each device. \nExecution of the stored instructions can cause one or more of the actions described herein to be performed by devices of system 100.\n In other implementations, multiple processors may be used, as appropriate, along with multiple memories and types of memory.  For example, user device 102 or server 106 may be connected with multiple other computing devices, with each device\n(e.g., a server bank, groups of servers, or a multi-processor system) performing portions of the actions or operations associated with the various processes or logical flows described in this specification.\n Referring again to FIG. 2, image recognition module 108 can generate one or more labels that indicate attributes of an item of digital content or that describe characteristics of a particular item depicted in the of digital content.  For\nexample, image recognition module 108 can execute program code to analyze a digital image of an item of digital content.  In response to analyzing the digital image, module 108 can use feature extraction logic 204 to extract one or more features of the\ndigital image.  Module 108 can then use label generation logic 206 to generate at least one label that indicates attributes of the digital image or that describe characteristics of a particular item depicted in the digital image.\n For example, if a digital image received by module 108 includes a particular content item(s) such as the Eiffel tower, and/or a dog standing in front of the Eiffel tower, then module 108 can use logic 204 to extract image features, or pixel\ndata, that correspond to at least one of: a) the Eiffel tower, or b) the dog.  Module 108 can then use label generation logic 206 to generate one or more labels (e.g., words, or text phrases) based on extracted features for Eiffel tower and dog.\n Example extracted features that correspond to the Eiffel tower may cause logic 206 to generate one or more example labels such as \"Eiffel,\" \"Eiffel tower,\" \"tower,\" \"Paris,\" \"France,\" or \"iron lattice tower.\" Likewise, example extracted features\nthat correspond to the dog may cause logic 206 to generate one or more example labels such as \"dog,\" \"golden retriever,\" \"cocker spaniel,\" \"cute dog,\" \"big cute golden retriever,\" or \"cute cocker spaniel.\"\n Module 108 further includes scoring/ranking logic 208.  Logic 208 is used to analyze multiple labels generated using logic 206 and, based on the analysis, generate respective confidence scores for each label of the multiple labels.  Each label\ncan be assigned confidence score that indicates a relevance of a particular word or text phrase (e.g., a label) with regard to attributes, or extracted image features, of a received item of digital content.\n In some implementations, labels that are more definitive or descriptive of particular attributes or extracted image features of an item of digital content may be assigned a higher confidence score relative to labels that more generic.  For\nexample, referencing the above extracted features for the Eiffel tower and the dog, descriptive labels such as \"Eiffel\" or \"Eiffel tower\" may receive higher confidence scores when compared to more generic labels such as \"tower\" or \"Paris.\" Likewise,\ndescriptive labels such as \"golden retriever\" or \"cute cocker spaniel\" may receive higher confidence scores when compared to more generic labels such as \"dog\" or \"cute dog.\"\n In some implementations, module 108 can execute program code to generate at least one boundary box that bounds at least one feature of a received digital image or item of digital content.  In some instances, at least one label may be generated\nby module 108 prior to module 108 generating a boundary box that bounds at least one feature of the digital image.  In this instance, module 108 can determine if the at least one generated label is descriptive of a feature that is bounded by the boundary\nbox.  Labels that are descriptive of features of a boundary box can receive higher confidence scores relative to labels that are not descriptive of features of a boundary box.\n In other implementations, a digital image can include at least two features and a first feature can be more prominent within the image than a second feature.  A first boundary box can bound the first more prominent feature of the digital image,\ne.g., the Eiffel tower, while a second boundary box can bound the second less prominent feature of the digital image, e.g., the dog.  Labels that are descriptive of the first more prominent feature of the first boundary box can receive higher confidence\nscores relative to labels that are descriptive of the second less prominent feature of the second boundary box.\n Module 108 can generate multiple labels and can use logic 208 to rank each label based on a respective confidence score that is assigned to each label to form a subset of ranked labels.  In some implementations, a subset of ranked labels can\ninclude at least two labels that have the highest confidence scores from among the respective confidence scores assigned to each of the multiple labels.  In other implementations, a subset of ranked labels can include one or more labels having confidence\nscores that exceed a threshold confidence score.\n As noted above, each respective confidence score indicates a relevance of a particular label to an attribute or extracted image feature of the item of digital content.  Module 108 can select at least one label based on a confidence score of the\nat least one label exceeding a threshold confidence score.  Module 108 can provide the selected at least one label to one or more of modules 110, 112, and 114.  Alternatively, module 108 can select at least one label, of the subset of ranked labels, and\nprovide the selected at least one label to one or more of modules 110, 112, and 114.\n Previous replies module 110 generally includes machine learning logic 210, content extraction database 212, and scoring/ranking logic 214.  Module 110 can receive at least one of: i) image data or image pixel data associated with digital image\ncontent for an item of digital content received by server 106 from user device 102; or ii) one or more labels from module 108 that indicate attributes of an item of digital content received by server 106 from user device 102.\n Content extraction database 212 can include multiple other items of digital content (\"chat content\") that have been extracted from a variety of electronic conversations, or electronic \"chats,\" that occur between at least two users.  In some\nimplementations, the electronic conversations can occur via an example messaging or chat application and can include a communication message provided by at least a first user and a reply message provided by at least a second user.\n Extracted chat content can include multiple digital content items such as texts, words, text phrases, or digital image data.  Module 110 can generate one or more conversational replies based on a similarity between image pixel data received from\nmodule 108 and at least one content item of the extracted chat content stored in database 212.  In alternative implementations, module 110 can generate one or more conversational replies based on a similarity between at least one received label received\nfrom module 108 and at least one content item of the extracted chat content.\n For example, referencing the above extracted features for the Eiffel tower and the dog, pixel data can indicate that the Eiffel tower and the dog are particular items included in a digital image received by server 106.  Module 110 can then scan\nor analyze database 212 to identify texts, words, text phrases, or image data having an apparent relation to the Eiffel tower or the dog.\n The words, text phrases, and digital pictures/images can be previous replies and other chat messages mined or extracted over-time by system 100 and stored in database 212.  The words or text phrases stored in database 212 can include content\nitems such as: \"Eiffel tower,\" \"Paris,\" \"France,\" \"golden retriever,\" \"cocker spaniel,\" or \"cute dog.\" Digital pictures or image data stored in database 212 can include images of a variety dogs, images of the Eiffel tower, or images of a variety of\nlocations in Paris, France.\n In some implementations, module 110 uses machine learning logic 210 to compute inferences using an example neural network of system 100.  The computed inferences are used to determine digital content items of database 212 that are similar or\nrelevant to the image pixel data of the item of digital content received from user device 102.\n Module 110 can use scoring/ranking logic 214 to determine at least one similarity score that indicates a similarity between: i) image pixel data of an item of digital content; and ii) at least one content item of chat content extracted from an\nelectronic conversation.  For example, module 110 can determine a similarity score between pixel data for the Eiffel tower and respective images of the Eiffel tower accessed from database 212.  Likewise, module 108 can determine a similarity score\nbetween pixel data for the Eiffel tower and respective words or text phrases accessed from database 212 that are descriptive of the Eiffel tower.\n Module 110 can also determine whether similarity scores exceed a threshold similarity score.  In response to determining that one or more similarity scores exceed a threshold similarity score, module 110 can generate one or more conversational\nreplies and a confidence score for each conversational reply.\n Module 110 generates the conversational replies based on at least one content item of chat content (e.g., another item of digital content) accessed from database 212.  In some implementations, conversational replies generated by module 110\ninclude digital image data from database 212, text data such as words or text phrases from database 212, or a combination of image and text data from database 212.\n Module 110 can analyze one or more determined similarity scores and, based on the analysis, generate respective confidence scores for each conversational reply.  Each conversational reply can be assigned a confidence score that indicates a\nrelevance between the conversational reply and the image pixel data for the received item of digital content.\n In some implementations, determined similarity scores can indicate an extent to which a content item of database 212 is similar or relevant to image pixel data of the received item of digital content.  For example, determining the similarity\nscores can correspond to determining a relevance parameter that indicates a relevance between a conversational reply and an item of digital content received by server 106 from user device 102.  Hence, module 110 can generate a confidence score based on a\ndetermined relevance parameter.\n Similarity scores for content items accessed from database 212 can be ranked based on a numerical value of the score such that scores with larger numerical values (e.g., high similarity scores) are ranked higher than scores with lower numerical\nvalues (e.g., low similarity scores).  Conversational replies generated from content items of database 212 that have high similarity scores may be assigned higher confidence scores relative to conversational replies generated from content items of\ndatabase 212 that have low similarity scores.\n Module 110 can generate a set of conversational replies and each conversational reply in the set can have a corresponding confidence score.  Further, module 110 can use logic 214 to rank each conversational reply in the set based on the\ncorresponding confidence score for the reply.\n For example, module 110 can generate a first set of conversational replies.  This example first set of conversational replies can include: i) a first reply that includes a close-up image of the Eiffel tower, and/or text that states \"wow the\nEiffel tower looks really tall up close, don't you think?\"; ii) a second reply that includes an image taken several miles away from the Eiffel tower and that shows multiple other buildings in the city of Paris, France, and/or text that states \"Paris has\nso many cool places that surround the tower.\"; and iii) a third reply that includes an image taken from within the Eiffel tower showing multiple other buildings in the city of Paris, France but the image does not show the Eiffel tower, and/or text that\nstates \"Look at all the really nice places to visit that are around the Eiffel tower.\"\n Further, regarding this first set of conversational replies, the first reply might receive an example confidence score of 0.8, the second reply might receive an example confidence score of 0.6, and the third reply might receive an example\nconfidence score of 0.3.  The first set of conversational replies can include: a) the first reply being ranked highest, e.g., ranked first out of the three replies, based on the confidence score of 0.8; b) the second reply being ranked between the first\nreply and the third reply, e.g., ranked second out of the three replies, based on the confidence score of 0.6; and c) the third reply being ranked after the first reply and the second reply, e.g., ranked third or last out of the three replies, based on\nthe confidence score of 0.3.\n As described in more detail below, a set of conversational replies generated by module 110, and the corresponding confidence scores for each reply, are provided to module 116 for analysis and selection of a particular conversational reply from\namong multiple conversational replies.  In some implementations, the example first set of conversational replies described above can be provided to module 116 along with ranking data that indicates a ranking of a particular conversational reply relative\nto other replies in the first set.\n Media content replies module 112 generally includes machine learning logic 218, media content database 220, and scoring/ranking logic 222.  Module 112 can receive one or more labels from module 108 that indicate attributes of an item of digital\ncontent received by server 106 from user device 102.\n Media content database 220 can include multiple other items of digital content (\"media content\") that have been extracted or reproduced from a variety of different types of media content such as films or video data, music or audio data,\nbooks/articles/publications text data, or other forms of digital text, image or video data.  Media content items of database 220 includes multiple quotes, e.g., texts, words, or text phrases, relating to content and data extracted or produced from\ndigital text, image or video data stored in database 212.\n Module 112 can generate one or more conversational replies based on a similarity between labels received from module 108 and at least one media content item stored in database 220.  For example, referencing the above extracted features for the\nEiffel tower and the dog, one or more labels can include text phrases or words such as \"Eiffel tower\" and \"cocker spaniel.\" Module 112 can then scan or analyze database 220 to identify quotes (e.g., text content) or other media content relating to texts,\nwords, or text phrases that have an apparent relation to \"Eiffel tower\" or \"cocker spaniel.\"\n Quotes, words, text phrases, or other media content of database 220 can be content items, e.g., from movies, television shows, songs, books, or magazines, that have been mined or extracted over-time by system 100.  The quotes, words, or text\nphrases stored in database 220 can include content items such as: \"Eiffel tower,\" \"Paris,\" \"France,\" \"golden retriever,\" \"cocker spaniel,\" or \"cute dog.\" Thus, at least one media content item can be related to, or descriptive of, particular items\ndepicted in a digital image received from user device 102.  Further, as noted above, the at least one media content item can be another item of digital content that is distinct from the item of digital content received by server 106 from user device 102.\n In some implementations, module 112 uses machine learning logic 218 to compute inferences using an example neural network of system 100.  The computed inferences are used to determine media content items of database 220 that are similar or\nrelevant to the labels generated by module 108, and that indicate an attribute of the item of digital content received from user device 102.\n Module 112 can use scoring/ranking logic 222 to determine at least one similarity score that indicates a similarity between: i) the one or more labels indicating an attribute of an item of digital content; and ii) at least one media content item\nstored in database 220.  For example, module 112 can determine a similarity score between a label including \"Eiffel tower\" and respective quotes, words, text phrases, or other media content relating to Eiffel tower accessed from database 220.\n In some implementations, module 112 can execute program code for data matching and data comparison processes such as entity matching, n-gram similarity, phrase matching, and feature similarity, to indicate a threshold level of similarity between\nlabels provided by module 108 and media content items accessed from database 220.  Module 112 can then generate a similarity score based on an outcome of the data matching process.\n Module 112 can also determine whether similarity scores exceed a threshold similarity score.  In response to determining that one or more similarity scores exceed a threshold similarity score, module 112 can generate one or more conversational\nreplies and a confidence score for each conversational reply.  Module 112 generates the conversational replies based on at least one media content item (e.g., a quote or other item of digital content) accessed from database 220.  In some implementations,\nconversational replies generated by module 112 include quotes or text data accessed from database 220.\n Module 112 can analyze one or more determined similarity scores and, based on this analysis, generate respective confidence scores for each conversational reply.  Each conversational reply can be assigned a confidence score that indicates a\nrelevance between the conversational reply and labels from module 108 that indicate an attribute of the received item of digital content.  In some implementations, determined similarity scores can indicate an extent to which a media content item of\ndatabase 220 is similar or relevant to labels generated by module 108.\n For example, determining the similarity scores can correspond to determining a relevance parameter that indicates a relevance between a conversational reply and an item of digital content received by server 106 from user device 102.  Hence,\nsimilar to module 110, module 112 can also generate a confidence score based on a determined relevance parameter.\n Module 112 can generate conversational replies using quotes, words or text phrases or other media content that are associated with particularly high similarity scores (e.g., as indicated by a corresponding relevance parameter for the similarity\nscore).  Such high similarity scores can indicate that these quotes, words or text phrases have substantial relevance to the labels generated module 108.\n Similarity scores for media content items accessed from database 220 can be ranked based on a numerical value of the score such that scores with larger numerical values (e.g., high similarity scores) are ranked higher than scores with lower\nnumerical values (e.g., low similarity scores).  Conversational replies generated from media content items of database 220 that have high similarity scores may be assigned higher confidence scores relative to conversational replies generated from media\ncontent items of database 220 that have low similarity scores.\n Module 112 can generate a set of conversational replies and each conversational reply in the set can have a corresponding confidence score.  Further, module 112 can use logic 222 to rank each conversational reply in the set based on the\ncorresponding confidence score for the reply.\n For example, module 112 can generate a second set of conversational replies relative to the example first set generated by module 110.  This example second set of conversational replies can include: i) a first reply that includes text stating\n\"wow the Eiffel tower looks really tall up close, don't you think?\", where the text is a quote from a song by a singer and the reply further includes an image of the singer standing in front of the Eiffel tower; ii) a second reply that includes text\nstating \"Paris has so many cool places that surround the tower,\" where the text is a quote from a movie and the reply further includes an image from a scene of the movie that shows the Eiffel tower and multiple buildings that surround the tower; and iii)\na third reply that includes text stating \"Paris has nice places to visit around the Eiffel tower,\" where the text is a quote from a web-based article.\n Further, regarding this second set of conversational replies, the first reply might receive an example confidence score of 0.88, the second reply might receive an example confidence score of 0.7, and the third reply might receive an example\nconfidence score of 0.2.  The second set of conversational replies can include: a) the first reply being ranked highest, e.g., ranked first out of the three replies, based on the confidence score of 0.88; b) the second reply being ranked between the\nfirst reply and the third reply, e.g., ranked second out of the three replies, based on the confidence score of 0.7; and c) the third reply being ranked after the first reply and the second reply, e.g., ranked third or last out of the three replies,\nbased on the confidence score of 0.2.\n As described in more detail below, a set of conversational replies generated by module 112, and the corresponding confidence scores for each reply, are provided to module 116 for analysis and selection of a particular conversational reply from\namong multiple conversational replies.  In some implementations, the example second set of conversational replies described above can be provided to module 116 along with ranking data that indicates a ranking of a particular conversational reply relative\nto other replies in the second set.\n Predetermined replies module 114 generally includes machine learning logic 226, predetermined replies database 228, and scoring/ranking logic 230.  Module 114 can receive one or more labels from module 108 that indicate attributes of an item of\ndigital content received by server 106 from user device 102.\n Predetermined replies database 228 can include multiple predetermined conversational replies and at least one conversational reply generated by module 112 can be selected from among the multiple predetermined conversational replies of database\n228.  The predetermined replies can be curated such that each predetermined conversational reply stored in database 228 includes at least a portion of text/words, text phrases, or image content that may have a likelihood of being perceived as delightful,\npleasing, pleasant, or interesting to a user.\n Module 114 can generate one or more conversational replies based on a similarity between labels received from module 108 and at least one content item stored in database 228.  The content item can include one or more of: i) words/text included\nin predetermined replies stored in database 228; ii) text phrases included in predetermined replies stored in database 228; and iii) predetermined replies stored in database 228.\n For example, referencing the above extracted features for the Eiffel tower and the dog, one or more labels can include text phrases or words such as \"Eiffel tower\" and \"cocker spaniel.\" Module 114 can then scan or analyze database 228 to\nidentify predetermined replies or other content items relating to texts, words, or text phrases that have an apparent relation to \"Eiffel tower\" or \"cocker spaniel.\"\n Predetermined replies, words, or text phrases of database 228 can be content items, e.g., a string of curated text/words forming snippets of descriptive and interesting content, that have been drafted using computer-based or human reply\ndrafters.  The predetermined replies, words, or text phrases stored in database 228 can include content items such as: \"Eiffel tower,\" \"Paris,\" \"wow the Eiffel tower seems really cool, I'd like to visit Paris,\" \"cocker spaniel,\" \"that cocker spaniel\nseems really small compared to the Eiffel,\" or \"I am no architect, but the Eiffel tower seems like quite a construction!\" Thus, at least one content item of database 228 can be related to, substantially related to, or descriptive of, particular items\ndepicted in a digital image received from user device 102.\n In some implementations, module 114 uses machine learning logic 226 to compute inferences using an example neural network of system 100.  The computed inferences are used to determine content items of database 228 that are similar or relevant to\nthe labels generated by module 108, and that indicate an attribute of the item of digital content received from user device 102.\n Module 114 can use scoring/ranking logic 230 to determine at least one similarity score that indicates a similarity between: i) the one or more labels indicating an attribute of an item of digital content; and ii) at least one content item that\nincludes predetermined replies stored in database 228.  For example, module 114 can determine a similarity score between a label including \"Eiffel tower\" and respective predetermined replies, words, or text phrases relating to Eiffel tower accessed from\ndatabase 228.\n In some implementations, module 114 can execute program code for data matching and data comparison processes such as entity matching, n-gram similarity, phrase matching, and feature similarity, to indicate a threshold level of similarity between\nlabels provided by module 108 and predetermined replies or other content items accessed from database 228.  Module 114 can then generate a similarity score based on an outcome of the data matching process.\n Module 114 can also determine whether similarity scores exceed a threshold similarity score.  In response to determining that one or more similarity scores exceed a threshold similarity score, module 114 can generate one or more conversational\nreplies and a confidence score (described below) for each conversational reply.  Module 114 generates the conversational replies based on at least one content item (e.g., a word or text phrases included in a predetermined reply) accessed from database\n228.\n In some implementations, conversational replies generated by module 114 can include various combinations of content items accessed from database 228.  For example, module 114 can generate a conversational reply that is the same as, similar to,\nor substantially similar to, a predetermined reply stored in database 228.\n In some instances, module 114 generates a conversational reply by modifying an existing predetermined reply to include one or more words or text phrases from another predetermined reply of database 228.  In related instances, module 114\ngenerates a conversational reply by using individual words or text phrases from existing predetermined replies to form new replies that are then stored in database 228 as new predetermined replies.\n Referring now to the confidence scores, module 114 can analyze one or more determined similarity scores and, based on this analysis, generate respective confidence scores for each conversational reply.  Each conversational reply can be assigned\na confidence score that indicates a relevance between the conversational reply and labels from module 108 that indicate an attribute of the received item of digital content.  In some implementations, determined similarity scores can indicate an extent to\nwhich a content item of database 228 is similar or relevant to the labels generated by module 108.\n For example, determining the similarity scores can correspond to determining a relevance parameter that indicates a relevance between a conversational reply and an item of digital content received by server 106 from user device 102.  Hence,\nsimilar to modules 110 and 112, module 114 can also generate a confidence score based on a determined relevance parameter.\n Module 114 can generate conversational replies using words or text phrases that are associated with particularly high similarity scores (e.g., as indicated by a corresponding relevance parameter for the similarity score).  Such high similarity\nscores can indicate that these words or text phrases have substantial relevance to the labels generated module 108.  Similarity scores for content items accessed from database 228 can be ranked based on a numerical value of the score such that scores\nwith larger numerical values (e.g., high similarity scores) are ranked higher than scores with lower numerical values (e.g., low similarity scores).\n Conversational replies generated from content items of database 228 that have high similarity scores may be assigned higher confidence scores relative to conversational replies generated from content items of database 228 that have low\nsimilarity scores.\n Module 114 can generate a set of conversational replies and each conversational reply in the set can have a corresponding confidence score.  Further, module 114 can use logic 230 to rank each conversational reply in the set based on the\ncorresponding confidence score for the reply.\n For example, module 114 can generate a third set of conversational replies relative to the example first and second sets generated by modules 110 and 112, respectively.  This example third set of conversational replies can include: i) a first\nreply that includes predetermined reply text stating \"I am no architect, but the Eiffel tower seems like quite a construction!\"; and ii) a second reply that includes predetermined reply text stating \"wow the Eiffel tower seems really cool, I'd like to\nvisit Paris.\"\n Further, regarding this third set of conversational replies, the first reply might receive an example confidence score of 0.92 and the second reply might receive an example confidence score of 0.65.  The third set of conversational replies can\ninclude: a) the first reply being ranked highest, e.g., ranked first out of the two replies, based on the confidence score of 0.92; and b) the second reply being ranked after the first reply, e.g., ranked second out of the two replies, based on the\nconfidence score of 0.65.\n As described in more detail below, a set of conversational replies generated by module 114, and the corresponding confidence scores for each reply, are provided to module 116 for analysis and selection of a particular conversational reply from\namong multiple conversational replies.  In some implementations, the example third set of conversational replies described above can be provided to module 116 along with ranking data that indicates a ranking of a particular conversational reply relative\nto other replies in the third set.\n Reply selection module 116 receives respective sets of conversational replies generated by each of modules 110, 112, and 114.  For each set of conversational replies, module 116 can also receive respective confidence scores for each\nconversational reply in the set as well as any associated ranking data that indicates of ranking of confidence scores.\n Module 116 can include program code or logic to analyze the respective confidence scores, and ranking data, for each conversational reply in the sets of conversational replies generated by each of modules 110, 112, and 114.  In some\nimplementations, analyzing the respective confidence scores includes ranking each conversational reply based on the corresponding confidence score for the reply.  Conversational replies can be ranked based on a numerical value of their associated\nconfidence score such that replies having scores with larger numerical values (e.g., high confidence scores) are ranked higher than replies having scores with lower numerical values (e.g., low confidence scores).\n In other implementations, module 116 can assign a weighting or boosting parameter to at least one of modules 110, 112, 114.  The weighting parameter can be used to boost numerical values of confidence scores for conversational replies generated\nby the module that was assigned the weighting parameter.  Conversational replies generated by a module 110, 112, 114 that was assigned a particular weighting parameter can be ranked higher relative to replies generated by another module 110, 112, 114\nthat was not assigned a particular weighting or boosting parameter.\n Based on analysis of the respective confidence scores, module 116 can select a particular number of conversational replies from among the replies included in the respective sets of replies generated by modules 110, 112, and 114.  Module 116 can\nselect one or more conversational replies that have the highest confidences among the replies included in the respective sets of replies.\n For example, referencing the above described first, second, and third sets of conversational replies, module 116 can select the first reply of the third set of conversational replies generated by module 114 and that has a corresponding\nconfidence score of 0.92.  Likewise, module 116 can also select the first reply of the second set of conversational replies generated by module 112 and that has a corresponding confidence score of 0.88.  Module 116 selects the first reply of the third\nset based on the reply's corresponding confidence score of 0.92 being the highest among scores for all replies of the respective sets.  Further, module 116 selects the first reply of the second set based on the reply's corresponding confidence score of\n0.88 being the second highest among scores for all replies of the respective sets.\n One or more conversational replies selected by module 116 are provided for output to user device 102 by server 106.  For example, server 106 can provide the selected first reply of the third set based on the reply's corresponding confidence\nscore of 0.92 being the highest among scores for all replies of the respective sets.  In some implementations, selected conversational replies can be provided to user device 102 as a suggested reply to an item of digital content provided to server 106\nfrom user device 102.  In other implementations, the selected conversational reply can be provided to user device 102 in response to user device 102 receiving an item of digital content as a communication message of an electronic conversation generated\nby a messaging application.\n For example, user device 102 may include a messaging application used to exchange data communications between at least two users that are associated with an electronic conversation.  The messaging application can receive a communication message\nthat includes an item of digital content, e.g., a digital image.  User device 102 can provide the digital image to server 106 and server 106 can generate a conversational reply based on the digital image and according to the technologies described\nherein.\n Server 106 provides the generated conversational reply for output to user device 102.  User device 102 may suggest or output the conversational reply as a reply message to the communication message of the electronic conversation.  User device\n102 suggests the conversational reply to at least one user as a reply message to the communication message of the electronic conversation.  Further, user device 102 outputs the conversational reply via a graphical display of the device that presents a\ngraphical interface showing the electronic conversation.\n Entity relatedness module 232 receives one or more labels from module 108 that indicate attributes of an item of digital content received by server 106 from user device 102.  In response to receiving a label, module 232 can access knowledge\ngraph 234 and use the label to generate one or more related entities that have a threshold relevance to the item of digital content.\n At least one of modules 110, 112, 114 can receive one or more related entities from module 232 and use the related entities to generate one or more conversational replies.  For example, predetermined replies module 112 can generate one or more\nconversational replies based on a similarity between: i) labels received from module 108 and at least one content item stored in database 228; and ii) one or more related entities received from module 232 and at least one content item stored in database\n228.\n For example, referencing the above extracted features for the Eiffel tower and the dog, one or more labels can include text phrases or words such as \"Eiffel tower\" and \"cocker spaniel.\" Module 232 can then use knowledge graph 234 to identify\nrelated entities such as content items including texts, words, or text phrases that have an apparent relation to \"Eiffel tower\" or \"cocker spaniel.\" Example related entities provided by graph 234 can include content items such as: \"Paris,\" \"Paris,\nFrance,\" \"English spaniel,\" or \"American spaniel.\"\n Thus, at least one content item of knowledge graph 234 can be an entity that is related to, or substantially related to, a label generated by module 108.  Further, at least one of modules 110, 112, 114 can generate one or more conversational\nreplies based on a similarity between two or more of: i) labels received from module 108, ii) image pixel data received from module 108, iii) related entities received from module 232, or iv) content items stored in a respective database of the module.\n Module 114 can use scoring/ranking logic 230 to determine at least one similarity score that indicates a similarity between two or more of: i) labels received from module 108, ii) image pixel data received from module 108, iii) related entities\nreceived from module 232, or iv) content items stored in a respective database of the module.  Module 114 can also determine whether the similarity scores exceed a threshold similarity score.\n In response to determining that the similarity score exceeds a threshold similarity score, module 114 can generate one or more conversational replies and a confidence score for each conversational reply.  Module 114 can generate the\nconversational replies based on the related entity and based on at least one content item (e.g., a word or text phrases included in a predetermined reply) accessed from database 228.  In some implementations, in response to determining that the\nsimilarity score exceeds a threshold similarity score, module 114 can select, from database 228, a content item or a predetermined conversational reply for inclusion with one or more conversational replies generated by modules 110 and 112 of system 100.\n Knowledge graph 234 is a collection of data representing entities and relationships between entities.  The data is logically described as a graph, in which each distinct entity is represented by a respective node and each relationship between a\npair of entities is represented by an edge between the nodes.  Each edge is associated with a relationship and the existence of the edge represents that the associated relationship exists between the nodes connected by the edge.  For example, if a node A\nrepresents a person alpha, a node B represents a person beta, and an edge E is associated with the relationship \"is the father of,\" then having the edge E connect the nodes in the direction from node A to node B in the graph represents the fact that\nalpha is the father of beta.\n A knowledge graph can be represented by any of a variety of convenient physical data structures.  For example, a knowledge graph can be represented by triples that each represent two entities in order and a relationship from the first to the\nsecond entity; for example, [alpha, beta, is the father of], or [alpha, is the father of, beta], are alternative ways of representing the same fact.  Each entity and each relationship can be and generally will be included in multiple triples.\n Alternatively, each entity can be stored as a node once, as a record or an object, for example, and linked through a linked list data structure to all the relationships the entity has and all the other entities to which the entity is related. \nMore specifically, a knowledge graph can be stored as an adjacency list in which the adjacency information includes relationship information.  It is generally advantageous to represent each distinct entity and each distinct relationship with a unique\nidentifier.\n The entities represented by a knowledge graph need not be tangible things or specific people.  The entities can include particular people, places, things, artistic works, concepts, events, or other types of entities.  Thus, a knowledge graph can\ninclude data defining relationships between people, e.g., co-stars in a movie; data defining relationships between people and things, e.g., a particular singer recorded a particular song; data defining relationships between places and things, e.g., a\nparticular type of wine comes from a particular geographic location; data defining relationships between people and places, e.g., a particular person was born in a particular city; and other kinds of relationships between entities.\n In some implementations, each node has a type based on the kind of entity the node represents; and the types can each have a schema specifying the kinds of data that can be maintained about entities represented by nodes of the type and how the\ndata should be stored.  So, for example, a node of a type for representing a person could have a schema defining fields for information such as birth date, birth place, and so on.  Such information can be represented by fields in a type-specific data\nstructure, or by triples that look like node-relationship-node triples, e.g., [person identifier, was born on, date], or in any other convenient predefined way.  Alternatively, some or all of the information specified by a type schema can be represented\nby links to nodes in the knowledge graph; for example, [one person identifier, child of, another person identifier], where the other person identifier is a node in the graph.\n FIG. 3 is a flow diagram of an example process 300 for generating one or more conversational replies.  In some implementations, process 300 may be performed or executed by one or more electronic devices, modules, or components of system 100\ndescribed above.\n At block 302 of process 300, server 106 of system 100 receives an item of digital content from user device 102.  The item of digital content can include a digital image that depicts a particular item.  At block 304, system 100 generates one or\nmore labels that indicate attributes of the item of digital content or that describe characteristics of the particular item.  For example, image recognition module 108 can execute program code to analyze the digital image of the item of digital content. \nIn response to analyzing the digital image, module 108 can extract one or more features of the image and use the extracted features to generate the one or more labels that indicate attributes of the item of digital content.\n At block 306 of process 300, system 100 generates one or more conversational replies to the item of digital content based on the one or more labels that at least indicate attributes of the item of digital content.  For example, server 106 can\nuse one or more of modules 110, 112, or 114 to generate the one or more conversational replies based on at least one label generated by module 108.  In some implementations, rather than generating conversational replies based on the one or more labels,\nsystem 100 can instead use module 110 to generate conversational replies based on image data of the item of digital content.\n At block 308, system 100 selects a conversational reply from among the one or more conversational replies that are generated by the one or more modules of server 106.  System 100 can use reply selection module 116 to select a particular\nconversional reply from among multiple conversational replies that are generated by at least one module of server 106.  At block 310 of process 300, system 100 can cause server 106 provide the selected conversational reply for output to user device 102.\n Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the\nstructures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.  Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or\nmore modules of computer program instructions encoded on a tangible non-transitory program carrier for execution by, or to control the operation of, data processing apparatus.\n Alternatively or in addition, the program instructions can be encoded on an artificially-generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for\ntransmission to suitable receiver apparatus for execution by a data processing apparatus.  The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a\ncombination of one or more of them.\n A computer program, which may also be referred to or described as a program, software, a software application, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted\nlanguages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.  A computer program may, but need\nnot, correspond to a file in a file system.\n A program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g.,\nfiles that store one or more modules, sub-programs, or portions of code.  A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a\ncommunication network.\n The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output.  The processes\nand logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).\n FIG. 4 is a block diagram of computing devices 400, 450 that may be used to implement the systems and methods described in this document, as either a client or as a server or plurality of servers.  Computing device 400 is intended to represent\nvarious forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.  Computing device 450 is intended to represent various forms of mobile devices,\nsuch as personal digital assistants, cellular telephones, smartphones, smartwatches, head-worn devices, and other similar computing devices.  The components shown here, their connections and relationships, and their functions, are meant to be exemplary\nonly, and are not meant to limit implementations described and/or claimed in this document.\n Computing device 400 includes a processor 402, memory 404, a storage device 406, a high-speed interface 408 connecting to memory 404 and high-speed expansion ports 410, and a low speed interface 412 connecting to low speed bus 414 and storage\ndevice 406.  Each of the components 402, 404, 606, 408, 410, and 412, are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.  The processor 402 can process instructions for execution within\nthe computing device 400, including instructions stored in the memory 404 or on the storage device 406 to display graphical information for a GUI on an external input/output device, such as display 416 coupled to high speed interface 408.  In other\nimplementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.  Also, multiple computing devices 600 may be connected, with each device providing portions of the necessary\noperations, e.g., as a server bank, a group of blade servers, or a multi-processor system.\n The memory 404 stores information within the computing device 400.  In one implementation, the memory 404 is a computer-readable medium.  In one implementation, the memory 404 is a volatile memory unit or units.  In another implementation, the\nmemory 404 is a non-volatile memory unit or units.\n The storage device 406 is capable of providing mass storage for the computing device 400.  In one implementation, the storage device 406 is a computer-readable medium.  In various different implementations, the storage device 406 may be a floppy\ndisk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.  In one implementation, a\ncomputer program product is tangibly embodied in an information carrier.  The computer program product contains instructions that, when executed, perform one or more methods, such as those described above.  The information carrier is a computer- or\nmachine-readable medium, such as the memory 404, the storage device 406, or memory on processor 402.\n The high speed controller 408 manages bandwidth-intensive operations for the computing device 400, while the low speed controller 412 manages lower bandwidth-intensive operations.  Such allocation of duties is exemplary only.  In one\nimplementation, the high-speed controller 408 is coupled to memory 404, display 416, e.g., through a graphics processor or accelerator, and to high-speed expansion ports 410, which may accept various expansion cards (not shown).  In the implementation,\nlow-speed controller 412 is coupled to storage device 406 and low-speed expansion port 414.  The low-speed expansion port, which may include various communication ports, e.g., USB, Bluetooth, Ethernet, wireless Ethernet, may be coupled to one or more\ninput/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.\n The computing device 400 may be implemented in a number of different forms, as shown in the figure.  For example, it may be implemented as a standard server 420, or multiple times in a group of such servers.  It may also be implemented as part\nof a rack server system 424.  In addition, it may be implemented in a personal computer such as a laptop computer 422.  Alternatively, components from computing device 400 may be combined with other components in a mobile device (not shown), such as\ndevice 450.  Each of such devices may contain one or more of computing device 400, 450, and an entire system may be made up of multiple computing devices 400, 450 communicating with each other.\n Computing device 450 includes a processor 452, memory 464, an input/output device such as a display 454, a communication interface 466, and a transceiver 468, among other components.  The device 450 may also be provided with a storage device,\nsuch as a microdrive or other device, to provide additional storage.  Each of the components 450, 452, 464, 454, 466, and 468, are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other\nmanners as appropriate.\n The processor 452 can process instructions for execution within the computing device 450, including instructions stored in the memory 464.  The processor may also include separate analog and digital processors.  The processor may provide, for\nexample, for coordination of the other components of the device 450, such as control of user interfaces, applications run by device 450, and wireless communication by device 450.\n Processor 452 may communicate with a user through control interface 458 and display interface 456 coupled to a display 454.  The display 454 may be, for example, a TFT LCD display or an OLED display, or other appropriate display technology.  The\ndisplay interface 456 may include appropriate circuitry for driving the display 454 to present graphical and other information to a user.  The control interface 458 may receive commands from a user and convert them for submission to the processor 452. \nIn addition, an external interface 462 may be provided in communication with processor 452, so as to enable near area communication of device 450 with other devices.  External interface 462 may provide, for example, for wired communication, e.g., via a\ndocking procedure, or for wireless communication, e.g., via Bluetooth or other such technologies.\n The memory 464 stores information within the computing device 450.  In one implementation, the memory 464 is a computer-readable medium.  In one implementation, the memory 464 is a volatile memory unit or units.  In another implementation, the\nmemory 464 is a non-volatile memory unit or units.  Expansion memory 474 may also be provided and connected to device 450 through expansion interface 472, which may include, for example, a SIMM card interface.  Such expansion memory 474 may provide extra\nstorage space for device 450, or may also store applications or other information for device 450.  Specifically, expansion memory 474 may include instructions to carry out or supplement the processes described above, and may include secure information\nalso.  Thus, for example, expansion memory 474 may be provided as a security module for device 450, and may be programmed with instructions that permit secure use of device 450.  In addition, secure applications may be provided via the SIMM cards, along\nwith additional information, such as placing identifying information on the SIMM card in a non-hackable manner.\n The memory may include for example, flash memory and/or MRAM memory, as discussed below.  In one implementation, a computer program product is tangibly embodied in an information carrier.  The computer program product contains instructions that,\nwhen executed, perform one or more methods, such as those described above.  The information carrier is a computer- or machine-readable medium, such as the memory 464, expansion memory 474, or memory on processor 452.\n Device 450 may communicate wirelessly through communication interface 466, which may include digital signal processing circuitry where necessary.  Communication interface 466 may provide for communications under various modes or protocols, such\nas GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others.  Such communication may occur, for example, through radio-frequency transceiver 468.  In addition, short-range communication may occur, such as using\na Bluetooth, WiFi, or other such transceiver (not shown).  In addition, GPS receiver module 470 may provide additional wireless data to device 450, which may be used as appropriate by applications running on device 450.\n Device 450 may also communicate audibly using audio codec 460, which may receive spoken information from a user and convert it to usable digital information.  Audio codec 460 may likewise generate audible sound for a user, such as through a\nspeaker, e.g., in a handset of device 450.  Such sound may include sound from voice telephone calls, may include recorded sound, e.g., voice messages, music files, etc., and may also include sound generated by applications operating on device 450.\n The computing device 450 may be implemented in a number of different forms, as shown in the figure.  For example, it may be implemented as a cellular telephone 480.  It may also be implemented as part of a smartphone 482, personal digital\nassistant, or other similar mobile device.\n Various implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs, computer hardware, firmware, software, and/or combinations thereof.  These\nvarious implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to\nreceive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.\n These computer programs, also known as programs, software, software applications or code, include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language,\nand/or in assembly/machine language.  As used herein, the terms \"machine-readable medium\" \"computer-readable medium\" refers to any computer program product, apparatus and/or device, e.g., magnetic discs, optical disks, memory, Programmable Logic Devices\n(PLDs) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal.  The term \"machine-readable signal\" refers to any signal used to\nprovide machine instructions and/or data to a programmable processor.\n To provide for interaction with a user, the systems and techniques described here can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the\nuser and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.  Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user\ncan be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.\n The systems and techniques described here can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component such as an application server, or that includes a front-end\ncomponent such as a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here, or any combination of such back-end, middleware, or front-end\ncomponents.  The components of the system can be interconnected by any form or medium of digital data communication such as, a communication network.  Examples of communication networks include a local area network (\"LAN\"), a wide area network (\"WAN\"),\nand the Internet.\n The computing system can include clients and servers.  A client and server are generally remote from each other and typically interact through a communication network.  The relationship of client and server arises by virtue of computer programs\nrunning on the respective computers and having a client-server relationship to each other.\n Further to the descriptions above, a user may be provided with controls allowing the user to make an election as to both if and when systems, programs or features described herein may enable collection of user information (e.g., information\nabout a user's social network, social actions or activities, profession, a user's preferences, or a user's current location), and if the user is sent content or communications from a server.  In addition, certain data may be treated in one or more ways\nbefore it is stored or used, so that personally identifiable information is removed.  For example, in some embodiments, a user's identity may be treated so that no personally identifiable information can be determined for the user, or a user's geographic\nlocation may be generalized where location information is obtained (such as to a city, ZIP code, or state level), so that a particular location of a user cannot be determined.  Thus, the user may have control over what information is collected about the\nuser, how that information is used, and what information is provided to the user.\n A number of embodiments have been described.  Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the invention.  For example, various forms of the flows shown above may be\nused, with steps re-ordered, added, or removed.  Also, although several applications of the payment systems and methods have been described, it should be recognized that numerous other applications are contemplated.  Accordingly, other embodiments are\nwithin the scope of the following claims.\n While this specification contains many specific implementation details, these should not be construed as limitations on the scope of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments. \nCertain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment.  Conversely, various features that are described in the context of a single embodiment can\nalso be implemented in multiple embodiments separately or in any suitable subcombination.  Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed\ncombination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.\n Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations\nbe performed, to achieve desirable results.  In certain circumstances, multitasking and parallel processing may be advantageous.  Moreover, the separation of various system modules and components in the embodiments described above should not be\nunderstood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.\n Particular embodiments of the subject matter have been described.  Other embodiments are within the scope of the following claims.  For example, the actions recited in the claims can be performed in a different order and still achieve desirable\nresults.  As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results.  In some cases, multitasking and parallel processing may be\nadvantageous.", "application_number": "15587783", "abstract": " A computer-implemented method is described. The method includes a\n     computing system receiving an item of digital content from a user device.\n     The computing system generates one or more labels that indicate\n     attributes of the item of digital content. The computing system also\n     generates one or more conversational replies to the item of digital\n     content based on the one or more labels that indicate attributes of the\n     item of digital content. The method also includes the computing system\n     selecting a conversational reply from among the one or more\n     conversational replies and providing the conversational reply for output\n     to the user device.\n", "citations": ["5751286", "7647227", "7707218", "8417712", "8515765", "8958661", "9015049", "9235864", "20110255736", "20130212501"], "related": []}, {"id": "20180332347", "patent_code": "10349134", "patent_name": "Analyzing multimedia content using knowledge graph embeddings", "year": "2019", "inventor_and_country_data": " Inventors: \nHamiti; Sofian (Dublin, IE), Limsopatham; Nut (Dublin, IE), Zaman; Md Faisal (Dublin, IE), Lecue; Freddy (Castleknock, IE), Oliveira Antonino; Victor (Dublin, IE), Kaila; Gaurav (Dublin, IE)  ", "description": "BACKGROUND\n Multimedia may include content that uses a combination of different content media forms, such as text, audio, images, animations, video, and interactive content.  Multimedia may be differentiated from other media that use only rudimentary\ncomputer displays, such as text-only or traditional forms of printed or hand-produced material.  Multimedia can be recorded, played, displayed, interacted with, or accessed by information content processing devices, such as computerized and electronic\ndevices.\nSUMMARY\n According to some possible implementations, a device may include one or more processors to receive multimedia data, metadata, and policy data.  The multimedia data may be related to multimedia content.  The policy data may be related to a policy\nthat restricts access to particular multimedia content.  The one or more processors may process the policy data using a first set of techniques to identify a set of terms or phrases associated with the policy data.  The one or more processors may\ndetermine a first set of embeddings for the set of terms or phrases after identifying the set of terms or phrases.  The first set of embeddings may identify the policy data.  The one or more processors may process the multimedia data and/or the metadata\nusing a second set of techniques to determine a second set of embeddings.  The second set of embeddings may identify the multimedia data.  The second set of embeddings may identify the metadata.  The one or more processors may process the first set of\nembeddings and the second set of embeddings using a knowledge graph to determine whether the multimedia content violates the policy.  The one or more processors may perform an action based on a result of processing the first set of embeddings and the\nsecond set of embeddings.\n According to some possible implementations, a method may include receiving, by a device, multimedia data, metadata, or policy data.  The multimedia data, the metadata, or the policy data may be used to determine whether multimedia content or\naccess to the multimedia content by a user violates a policy associated with the multimedia content.  The method may include processing, by the device, the policy data using a first set of techniques to determine a first set of embeddings for the policy\ndata.  The method may include processing, by the device, the multimedia data or the metadata using a second set of techniques to determine a second set of embeddings for the multimedia data or the metadata.  The method may include processing, by the\ndevice, the first set of embeddings and the second set of embeddings using a knowledge graph to determine whether the multimedia content or the access by the user violates the policy.  The method may include performing, by the device, an action based on\na result of processing the first set of embeddings and the second set of embeddings.  The action may relate to the multimedia content or the access to the multimedia content by the user.\n According to some possible implementations, a non-transitory computer-readable medium may store one or more instructions that, when executed by one or more processors, cause the one or more processors to receive multimedia data, metadata, and/or\npolicy data.  The multimedia data may be related to multimedia content.  The metadata may be related to a destination to which the multimedia content is to be provided.  The policy data may be related to a policy that restricts access to particular\nmultimedia content or from a particular destination.  The one or more instructions, when executed by the one or more processors, may cause the one or more processors to process the policy data using a first set of techniques to identify a set of terms or\nphrases associated with the policy data.  The one or more instructions, when executed by the one or more processors, may cause the one or more processors to determine a first set of embeddings for the set of terms or phrases to permit determination of\nwhether the multimedia content or the access from the destination violates the policy.  The first set of embeddings may identify information associated with the policy.\n The one or more instructions, when executed by the one or more processors, may cause the one or more processors to process the multimedia data or the metadata using a second set of techniques to determine a second set of embeddings.  The second\nset of embeddings may identify information associated with the multimedia content or the destination to which the multimedia content is to be provided.  The one or more instructions, when executed by the one or more processors, may cause the one or more\nprocessors to process the first set of embeddings and the second set of embeddings to determine whether the multimedia content or the access from the destination violates the policy.  The one or more instructions, when executed by the one or more\nprocessors, may cause the one or more processors to perform an action related to the multimedia content or the access to the multimedia content from the destination after processing the first set of embeddings and the second set of embeddings.\nBRIEF DESCRIPTION OF THE DRAWINGS\n FIGS. 1A-1G are diagrams of an overview of an example implementation described herein;\n FIG. 2 is a diagram of an example environment in which systems and/or methods, described herein, may be implemented;\n FIG. 3 is a diagram of example components of one or more devices of FIG. 2; and\n FIG. 4 is a flow chart of an example process for analyzing multimedia content using knowledge graph embeddings.\nDETAILED DESCRIPTION\n The following detailed description of example implementations refers to the accompanying drawings.  The same reference numbers in different drawings may identify the same or similar elements.\n Multimedia content moderation may be controlled by a policy.  For example, the policy may restrict access to particular multimedia content (e.g., violent content, content associated with a particular rating, offensive content, etc.), may\nrestrict access based on an age, a location, and/or the like of a user of a client device attempting to access the multimedia content, and/or the like.  In some cases, an organization and/or an individual may lack a computer-based technique for\nautomatically analyzing multimedia content, and determining whether a user of a client device is permitted to access the multimedia content based on a policy related to the multimedia content.\n Some implementations, described herein, provide a multimedia analysis platform that is capable of automatically analyzing multimedia content and determining whether a policy permits a particular user to access the multimedia content (e.g., based\non an age of the user, a location of the user, a type of multimedia content, etc.).  In this way, the multimedia analysis platform may automatically and dynamically enforce a content-related policy.  This improves application of a policy to multimedia\ncontent and/or increases an efficiency of applying a policy to multimedia content.  In addition, this conserves processing resources that would otherwise be consumed due to an inefficient review of content.  Further, this permits review of an amount of\ncontent that cannot be processed manually or objectively by a human actor, or processed in a threshold amount of time by a human actor.\n FIGS. 1A-1G are diagrams of an overview of an example implementation 100 described herein.  Implementation 100 includes a multimedia analysis platform and a client device (shown in FIG. 1G).  Implementations described herein with respect to\nFIGS. 1A-1G may be used to process thousands, millions, billions, etc., of multimedia data elements relative to hundreds, thousands, millions, etc., of content-related policies.  In this way, the multimedia analysis platform may process a data set that\ncannot be processed manually or objectively by a human actor.  In addition, although implementation 100 shows a single client device, in practice, there may be hundreds, thousands, millions, etc. of client devices in communication with the multimedia\nanalysis platform.\n As shown in FIG. 1A, and by reference number 102, the multimedia analysis platform may receive multimedia data related to multimedia content, metadata associated with the multimedia content and/or a user to which the multimedia content is to be\nprovided, and/or policy data related to a policy that restricts access to particular multimedia content and/or by particular users.  For example, the multimedia analysis platform may receive policy data indicating that multimedia data available in Saudi\nArabia cannot contain nudity (e.g., shown as \"Saudi Arabia: No Nudity\").  As another example, the multimedia analysis platform may receive policy data indicating that multimedia data available in France containing a war scene cannot be accessed by anyone\nunder 12 years of age (e.g., shown as \"France: No one under 12 for war scene\").  As still another example, the multimedia analysis platform may receive policy data indicating that multimedia data available in Ireland containing a war scene cannot be\naccessed by anyone under 18 years of age (e.g., shown as \"Ireland: No one under 18 for war scene\").\n The multimedia data, that the multimedia analysis platform may receive, may include audio that includes cheerleaders cheering at a basketball game, video of the basketball game, and/or the like.  Additionally, or alternatively, the metadata that\nthe multimedia analysis platform receives may include metadata identifying a user requesting the multimedia data (e.g., an Internet Protocol (IP) address of a client device (not shown in FIG. 1A) requesting the multimedia data, a username associated with\na user requesting the multimedia data, a device name of the client device requesting the multimedia content, etc.).\n As shown by reference number 104, the multimedia analysis platform may process the policy data using a first set of techniques to identify a set of terms and/or phrases associated with the policy.  For example, the multimedia analysis platform\nmay use a natural language processing technique, a computational linguistics technique, a text analysis technique, and/or the like, to identify a set of terms and/or phrases included in the policy data.\n As shown by reference number 106, the multimedia analysis platform may identify a set of terms and/or phrases that indicate multimedia data, destined for a client device located in Saudi Arabia, cannot contain nudity.  For example, the term\nand/or phrase may include \"Saudi Arabia: No Nudity.\"\n As shown by reference number 108, the multimedia analysis platform may identify a set of terms and/or phrases that indicate that multimedia data, destined for a client device located in France and associated with a user under 12 years of age,\ncannot contain a war scene.  For example, the term and/or phrase may include \"France: No one under 12 for war scene.\"\n As shown in FIG. 1B, and by reference number 110, the multimedia analysis platform may determine a first set of embeddings for the set of terms and/or phrases after identifying the set of terms and/or phrases.  In some implementations, the\nmultimedia analysis platform may determine the first set of embeddings for the set of terms and/or phrases using a technique, such as a Word2Vec technique.\n Prior to determining the first set of embeddings for the set of terms and/or phrases identified in the policy data, the multimedia analysis platform may identify a set of terms and/or phrases that is semantically similar to the set of terms\nand/or phrases identified in the policy data.  For example, the multimedia analysis platform may use WordNet, BabelNet, and/or the like to identify a set of terms and/or phrases that is similar to the set of terms and/or phrases identified in the policy\ndata.  This permits the multimedia analysis platform to more intelligently determine restrictions associated with a policy relative to not identifying semantically similar terms and/or phrases.\n As shown by reference number 112, the set of embeddings may include a numerical multi-dimensional representation of a concept.  For example, the numbers included in an embedding may indicate a likelihood that a concept (e.g., Woman or Gun) is\nassociated with dimensions of other concepts (e.g., nudity, violence, age), where a higher number indicates a higher likelihood relative to a lower number.  For example, the term \"gun\" present in multimedia data may be highly correlated (e.g., a\nlikelihood greater than a threshold, such as 0.5) with various dimensions of violence (e.g., 0.7; 0.8; 0.4; and 0.6) and not highly correlated with nudity (e.g., 0.0; 0.0; and 0.0).  As another example, the term \"women\" present in multimedia data may be\nhighly correlated (e.g., a likelihood greater than a threshold, such as 0.5) with various dimensions of nudity (e.g., 0.8; 0.6; and 0.4) and not highly correlated with age (e.g., 0.2; 0.3; and 0.3).  In this way, the multimedia analysis platform may\ndetermine whether the presence of a concept (e.g., a gun) in multimedia data likely violates a concept in a policy (e.g., a prohibition on violence included in multimedia content).\n As shown in FIG. 1C, and by reference number 114, the multimedia analysis platform may process the multimedia data and/or metadata using a second set of techniques to determine a second set of embeddings that identifies the multimedia data\nand/or the metadata.  The manner in which the multimedia analysis platform processes the multimedia data and/or metadata may differ based on a type of data the multimedia analysis platform is processing.\n As shown by reference number 116, the multimedia analysis platform may process audio data included in the multimedia data.  The multimedia analysis platform may process audio data to extract audio sequences from the audio data, and may process\nthe audio sequences using a recurrent neural network to identify sounds included in the audio data.  For example, the multimedia analysis platform may identify women's voices in the audio data (e.g., cheerleaders), that the audio data includes yelling,\nand/or the like.\n As shown by reference number 118, the multimedia analysis platform may determine a set of embeddings for the sounds identified in the audio data.  In some implementations, the set of embeddings may identify likelihoods that various dimensions\nrelated to female-related sounds (e.g., 0.7; 0.8; and 0.6) and yelling-related sounds (e.g., 0.4; 0.0; and 0.0) indicate the presence of a cheerleader in the multimedia data.\n As shown by reference number 120, the multimedia analysis platform may process the metadata.  In some implementations, the multimedia analysis platform may process the metadata using natural language processing, text analysis, computational\nlinguistics, and/or the like, to identify a set of terms and/or phrases included in the metadata.  For example, the multimedia analysis platform may identify an IP address (e.g., IP address: 124.0.0.1), a device name, and/or the like included in the\nmetadata.\n As shown by reference number 122, the multimedia analysis platform may determine a set of embeddings for the set of terms and/or phrases included in the metadata.  The set of embeddings may identify likelihoods that various dimensions related to\nan Arabic Region (e.g., 0.7; 0.8; and 0.6) and related to a region other than an Arabic Region (e.g., 0.4 and 0.0) indicate that the IP address (e.g., IP address 124.0.0.1) identifies a client device, to which the multimedia data is destined, that is\nlocated in Saudi Arabia.\n As shown in FIG. 1D, and by reference number 124, the multimedia analysis platform may process video data included in the multimedia data.  The multimedia analysis platform may extract video sequences from the video data, and/or may extract\nframes from the video sequences.  For example, the multimedia analysis platform may process the video sequences and/or frames using a convolutional neural network, a motion extraction model, a recurrent neural network, and/or the like, to identify\nobjects in the frames and/or video sequences, actions shown in the frames and/or video sequences, and/or the like.  For example, and as shown, the multimedia analysis platform may identify, in the video data, a basketball bouncing, people running, and/or\nthe like, that are shown in the frames and/or video sequences.\n As shown by reference number 126, the multimedia analysis platform may determine a set of embeddings for an object, a motion, an action, and/or the like, identified in the video data.  The set of embeddings may identify likelihoods that a\nbasketball bouncing (e.g., 0.7; 0.8; 0.6; and 0.4) and people running (e.g., 0.1; 0.0; 0.0; and 0.0) indicate a basketball game shown in the video data.\n As shown in FIG. 1E, and by reference number 128, the multimedia analysis platform may process the first set of embeddings and/or the second set of embeddings using a knowledge graph to determine whether the multimedia content and/or access by\nthe user violates the policy.  For example, the multimedia analysis platform may use the knowledge graph to identify matches between multimedia data, metadata, and policy data (e.g., to identify multimedia content and/or attempted access to the\nmultimedia content that violates or complies with a content-related policy).\n As shown by reference number 130, when processing the first set of embeddings and/or the second set of embeddings, the multimedia analysis platform may identify a set of terms and/or phrases associated with the first set of embeddings and/or the\nsecond set of embeddings.  In some implementations, for the first set of embeddings, the multimedia analysis platform may identify the terms \"nudity\" and \"age.\" For the second set of embeddings, the multimedia analysis platform may identify the terms\n\"basketball bouncing\" and \"people running.\"\n As shown by reference number 132, the multimedia analysis platform may determine a score for the set of terms and/or phrases identified with respect to the first set of embeddings and the second set of embeddings.  When determining the score for\nthe set of terms and/or phrases, the multimedia analysis platform may determine an average score of likelihoods of dimensions associated with each term and/or phrase.  For example, the multimedia analysis platform may determine an average score of 0.6\nfor likelihoods of dimensions associated with the term \"nudity,\" an average score of 0.3 for likelihoods of dimensions associated with the term \"age,\" an average score of 0.7 for likelihoods of dimensions associated with the term \"basketball bouncing,\"\nand an average score of 0.1 for likelihoods of dimensions associated with the term \"people running.\"\n As shown by reference number 134, the multimedia analysis platform may identify terms and/or phrases that have a threshold score.  For example, the multimedia analysis platform may identify terms that have a threshold score of 0.5 (e.g.,\n\"nudity\" and \"basketball bouncing\").  In this way, the multimedia analysis platform may identify a set of terms and/or phrases that have the highest probability of identifying the multimedia content, the metadata, and/or the policy (e.g., relative to\nother terms and/or phrases associated with the first set of embeddings and/or the second set of embeddings).\n As shown in FIG. 1F, and by reference number 136, the multimedia analysis platform may populate a knowledge graph with terms and/or phrases associated with the first set of embeddings and/or the second set of embeddings.  The terms and/or\nphrases associated with the first set of embeddings and/or the second set of embeddings that the multimedia analysis platform uses may be terms with a threshold score.\n As shown in FIG. 1G, and by reference number 138, the multimedia analysis platform may identify edges common to the terms and/or phrases used to populate the knowledge graph.  Identifying edges may permit the multimedia analysis platform to\ndetermine whether terms and/or phrases that identify the multimedia content and/or a destination for the multimedia content match terms and/or phrases associated with a policy (e.g., relative to other terms and/or phrases).  As shown, the multimedia\nanalysis platform may determine that the terms and/or phrases associated with the multimedia content and/or the metadata (e.g., \"basketball,\" \"cheerleader,\" and \"girl\") do not violate a corresponding content-based policy (e.g. based on determining that\nthe multimedia content does not include \"nudity\").\n As shown by reference number 140, the multimedia analysis platform may perform an action based on a result of processing the first set of embeddings and/or the second set of embeddings.  For example, the multimedia analysis platform may generate\na report that identifies a result of an analysis of the multimedia data, the metadata, and/or the policy data.\n As shown by reference number 142, the multimedia analysis platform may provide multimedia content to the client device (e.g., based on determining that the multimedia content does not violate a policy that applies to the multimedia content\nand/or access to the multimedia content).  The multimedia analysis platform may provide the multimedia content to the client device for display via the client device.\n In this way, the multimedia analysis platform may automatically and dynamically enforce a content-related policy.  This improves application of a policy to multimedia content and/or increases an efficiency of applying a policy to multimedia\ncontent.  In addition, this conserves processing resources that would otherwise be consumed due to an inefficient review of content.  Further, this permits review of an amount of content that cannot be processed manually or objectively by a human actor,\nor processed in a threshold amount of time by a human actor.\n As indicated above, FIGS. 1A-1G are provided merely as an example.  Other examples are possible and may differ from what was described with regard to FIGS. 1A-1G.\n FIG. 2 is a diagram of an example environment 200 in which systems and/or methods, described herein, may be implemented.  As shown in FIG. 2, environment 200 may include a client device 210, a server device 220, a multimedia analysis platform\n230 provided within a cloud computing environment 232 that includes a set of computing resources 234, and a network 240.  Devices of environment 200 may interconnect via wired connections, wireless connections, or a combination of wired and wireless\nconnections.\n Client device 210 includes one or more devices capable of receiving, generating, storing, processing, and/or providing information associated with multimedia content, a policy related to the multimedia content, and/or a destination of the\nmultimedia content.  For example, client device 210 may include a desktop computer, a mobile phone (e.g., a smart phone or a radiotelephone), a laptop computer, a tablet computer, a gaming device, a wearable communication device (e.g., a smart wristwatch\nor a pair of smart eyeglasses), or a similar type of device.  In some implementations, client device 210 may provide data related to a user of client device 210 that is attempting to access multimedia content, as described elsewhere herein. \nAdditionally, or alternatively, client device 210 may receive information identifying a result of an analysis performed by multimedia analysis platform 230 (e.g., to be provided for display), as described elsewhere herein.  While a single client device\n210 is shown in FIG. 2, in practice, there can be hundreds, thousands, millions, etc. of client devices 210 in communication with multimedia analysis platform 230.\n Server device 220 includes one or more devices capable of receiving, generating, storing, processing, and/or providing information associated multimedia content, a policy related to the multimedia content, and/or a destination of the multimedia\ncontent.  For example, server device 220 may include a server (e.g., in a data center or a cloud computing environment), a data center (e.g., a multi-server micro data center), a workstation computer, a virtual machine (VM) provided in a cloud computing\nenvironment, or a similar type of device.  In some implementations, server device 220 may provide, to multimedia analysis platform 230, information related to a policy associated with multimedia content, as described elsewhere herein.  Additionally, or\nalternatively, server device 220 may store information related to a result of an analysis performed by multimedia analysis platform 230 (e.g., to facilitate machine learning to improve future analyses of multimedia content and/or analyses by other\nmultimedia analysis platforms 230), as described elsewhere herein.  While a single server device 220 is shown in FIG. 2, in practice, there can be hundreds, thousands, millions, etc. of server devices 220 in communication with multimedia analysis\nplatform 230.\n Multimedia analysis platform 230 includes one or more devices capable of automatically analyzing multimedia data, policy data, and/or metadata to determine whether a content-related policy prevents access to multimedia content by a user of\nclient device 210.  For example, multimedia analysis platform 230 may include a cloud server or a group of cloud servers.  In some implementations, multimedia analysis platform 230 may be designed to be modular such that certain software components can\nbe swapped in or out depending on a particular need.  As such, multimedia analysis platform 230 may be easily and/or quickly reconfigured for different uses.\n In some implementations, as shown in FIG. 2, multimedia analysis platform 230 may be hosted in cloud computing environment 232.  Notably, while implementations described herein describe multimedia analysis platform 230 as being hosted in cloud\ncomputing environment 232, in some implementations, multimedia analysis platform 230 may not be cloud-based (i.e., may be implemented outside of a cloud computing environment) or may be partially cloud-based.\n Cloud computing environment 232 includes an environment that hosts multimedia analysis platform 230.  Cloud computing environment 232 may provide computation, software, data access, storage, and/or other services that do not require end-user\nknowledge of a physical location and configuration of a system and/or a device that hosts multimedia analysis platform 230.  As shown, cloud computing environment 232 may include a group of computing resources 234 (referred to collectively as \"computing\nresources 234\" and individually as \"computing resource 234\").\n Computing resource 234 includes one or more personal computers, workstation computers, server devices, or another type of computation and/or communication device.  In some implementations, computing resource 234 may host multimedia analysis\nplatform 230.  The cloud resources may include compute instances executing in computing resource 234, storage devices provided in computing resource 234, data transfer devices provided by computing resource 234, etc. In some implementations, computing\nresource 234 may communicate with other computing resources 234 via wired connections, wireless connections, or a combination of wired and wireless connections.\n As further shown in FIG. 2, computing resource 234 may include a group of cloud resources, such as one or more applications (\"APPs\") 234-1, one or more virtual machines (\"VMs\") 234-2, one or more virtualized storages (\"VSs\") 234-3, and/or one or\nmore hypervisors (\"HYPs\") 234-4.\n Application 234-1 includes one or more software applications that may be provided to or accessed by one or more devices of environment 200.  Application 234-1 may eliminate a need to install and execute the software applications on devices of\nenvironment 200.  For example, application 234-1 may include software associated with multimedia analysis platform 230 and/or any other software capable of being provided via cloud computing environment 232.  In some implementations, one application\n234-1 may send/receive information to/from one or more other applications 234-1, via virtual machine 234-2.\n Virtual machine 234-2 includes a software implementation of a machine (e.g., a computer) that executes programs like a physical machine.  Virtual machine 234-2 may be either a system virtual machine or a process virtual machine, depending upon\nuse and degree of correspondence to any real machine by virtual machine 234-2.  A system virtual machine may provide a complete system platform that supports execution of a complete operating system (\"OS\").  A process virtual machine may execute a single\nprogram, and may support a single process.  In some implementations, virtual machine 234-2 may execute on behalf of a user (e.g., a user of client device 210), and may manage infrastructure of cloud computing environment 232, such as data management,\nsynchronization, or long-duration data transfers.\n Virtualized storage 234-3 includes one or more storage systems and/or one or more devices that use virtualization techniques within the storage systems or devices of computing resource 234.  In some implementations, within the context of a\nstorage system, types of virtualizations may include block virtualization and file virtualization.  Block virtualization may refer to abstraction (or separation) of logical storage from physical storage so that the storage system may be accessed without\nregard to physical storage or heterogeneous structure.  The separation may permit administrators of the storage system flexibility in how the administrators manage storage for end users.  File virtualization may eliminate dependencies between data\naccessed at a file level and a location where files are physically stored.  This may enable optimization of storage use, server consolidation, and/or performance of non-disruptive file migrations.\n Hypervisor 234-4 provides hardware virtualization techniques that allow multiple operating systems (e.g., \"guest operating systems\") to execute concurrently on a host computer, such as computing resource 234.  Hypervisor 234-4 may present a\nvirtual operating platform to the guest operating systems, and may manage the execution of the guest operating systems.  Multiple instances of a variety of operating systems may share virtualized hardware resources.\n Network 240 includes one or more wired and/or wireless networks.  For example, network 240 may include a cellular network (e.g., a long-term evolution (LTE) network, a code division multiple access (CDMA) network, a 3G network, a 4G network, a\n5G network, or another type of cellular network), a public land mobile network (PLMN), a local area network (LAN), a wide area network (WAN), a metropolitan area network (MAN), a telephone network (e.g., the Public Switched Telephone Network (PSTN)), a\nprivate network, an ad hoc network, an intranet, the Internet, a fiber optic-based network, a cloud computing network, and/or the like, and/or a combination of these or other types of networks.\n The number and arrangement of devices and networks shown in FIG. 2 are provided as an example.  In practice, there may be additional devices and/or networks, fewer devices and/or networks, different devices and/or networks, or differently\narranged devices and/or networks than those shown in FIG. 2.  Furthermore, two or more devices shown in FIG. 2 may be implemented within a single device, or a single device shown in FIG. 2 may be implemented as multiple, distributed devices. \nAdditionally, or alternatively, a set of devices (e.g., one or more devices) of environment 200 may perform one or more functions described as being performed by another set of devices of environment 200.\n FIG. 3 is a diagram of example components of a device 300.  Device 300 may correspond to client device 210, server device 220, multimedia analysis platform 230, and/or computing resource 234.  In some implementations, client device 210, server\ndevice 220, multimedia analysis platform 230, and/or computing resource 234 may include one or more devices 300 and/or one or more components of device 300.  As shown in FIG. 3, device 300 may include a bus 310, a processor 320, a memory 330, a storage\ncomponent 340, an input component 350, an output component 360, and a communication interface 370.\n Bus 310 includes a component that permits communication among the components of device 300.  Processor 320 is implemented in hardware, firmware, or a combination of hardware and software.  Processor 320 includes a central processing unit (CPU),\na graphics processing unit (GPU), an accelerated processing unit (APU), a microprocessor, a microcontroller, a digital signal processor (DSP), a field-programmable gate array (FPGA), an application-specific integrated circuit (ASIC), or another type of\nprocessing component.  In some implementations, processor 320 includes one or more processors capable of being programmed to perform a function.  Memory 330 includes a random access memory (RAM), a read only memory (ROM), and/or another type of dynamic\nor static storage device (e.g., a flash memory, a magnetic memory, and/or an optical memory) that stores information and/or instructions for use by processor 320.\n Storage component 340 stores information and/or software related to the operations and use of device 300.  For example, storage component 340 may include a hard disk (e.g., a magnetic disk, an optical disk, a magneto-optic disk, and/or a solid\nstate disk), a compact disc (CD), a digital versatile disc (DVD), a floppy disk, a cartridge, a magnetic tape, and/or another type of non-transitory computer-readable medium, along with a corresponding drive.\n Input component 350 includes a component that permits device 300 to receive information, such as via user input (e.g., a touch screen display, a keyboard, a keypad, a mouse, a button, a switch, and/or a microphone).  Additionally, or\nalternatively, input component 350 may include a sensor for sensing information (e.g., a global positioning system (GPS) component, an accelerometer, a gyroscope, and/or an actuator).  Output component 360 includes a component that provides output\ninformation from device 300 (e.g., a display, a speaker, and/or one or more light-emitting diodes (LEDs)).\n Communication interface 370 includes a transceiver-like component (e.g., a transceiver and/or a separate receiver and transmitter) that enables device 300 to communicate with other devices, such as via a wired connection, a wireless connection,\nor a combination of wired and wireless connections.  Communication interface 370 may permit device 300 to receive information from another device and/or provide information to another device.  For example, communication interface 370 may include an\nEthernet interface, an optical interface, a coaxial interface, an infrared interface, a radio frequency (RF) interface, a universal serial bus (USB) interface, a Wi-Fi interface, a cellular network interface, or the like.\n Device 300 may perform one or more processes described herein.  Device 300 may perform these processes based on processor 320 executing software instructions stored by a non-transitory computer-readable medium, such as memory 330 and/or storage\ncomponent 340.  A computer-readable medium is defined herein as a non-transitory memory device.  A memory device includes memory space within a single physical storage device or memory space spread across multiple physical storage devices.\n Software instructions may be read into memory 330 and/or storage component 340 from another computer-readable medium or from another device via communication interface 370.  When executed, software instructions stored in memory 330 and/or\nstorage component 340 may cause processor 320 to perform one or more processes described herein.  Additionally, or alternatively, hardwired circuitry may be used in place of or in combination with software instructions to perform one or more processes\ndescribed herein.  Thus, implementations described herein are not limited to any specific combination of hardware circuitry and software.\n The number and arrangement of components shown in FIG. 3 are provided as an example.  In practice, device 300 may include additional components, fewer components, different components, or differently arranged components than those shown in FIG.\n3.  Additionally, or alternatively, a set of components (e.g., one or more components) of device 300 may perform one or more functions described as being performed by another set of components of device 300.\n FIG. 4 is a flow chart of an example process 400 for analyzing multimedia content using knowledge graph embeddings.  In some implementations, one or more process blocks of FIG. 4 may be performed by multimedia analysis platform 230.  In some\nimplementations, one or more process blocks of FIG. 4 may be performed by another device or a group of devices separate from or including multimedia analysis platform 230, such as client device 210, and/or server device 220.\n As shown in FIG. 4, process 400 may include receiving multimedia data related to multimedia content, metadata associated with the multimedia content and/or a user to which the multimedia content is to be provided, and/or policy data related to a\npolicy that restricts access to particular multimedia content and/or by particular users (block 410).  For example, multimedia analysis platform 230 may receive multimedia data related to multimedia content, metadata associated with the multimedia\ncontent, metadata associated with a user to which the multimedia content is to be provided, metadata associated with a characteristic of a user to whom the multimedia content is to be provided (e.g., an age, a location, a job title, etc.), metadata\nassociated with a time when the multimedia content is to be provided, and/or policy data related to a policy that restricts access to particular multimedia content and/or by particular users.\n In some implementations, multimedia analysis platform 230 may receive millions, billions, trillions, etc., of data elements when receiving the multimedia data, the policy data, and/or the metadata.  In this way, multimedia analysis platform 230\nmay receive a data set that cannot be processed manually or objectively by a human actor.\n In some implementations, multimedia content may include video content, audio content, animation content, an image, text content, and/or the like.  In this way, multimedia analysis platform 230 may process data related to multiple types of\nmultimedia content simultaneously, at different times, and/or the like.  This permits multimedia analysis platform 230 to be used in a variety of analysis scenarios, thereby improving a use of multimedia analysis platform 230.  In some implementations,\nand as a specific example, multimedia content may include video data and audio data.  In some implementations, when the multimedia data includes video data and audio data, multimedia analysis platform 230 may align video data and corresponding audio\ndata, such as to permit multimedia analysis platform 230 to identify a visual context of audio and/or an audio context of video.  This may permit multimedia analysis platform 230 to more accurately determine whether multimedia content and/or access to\nthe multimedia content violates a policy relative to not aligning video data and corresponding audio data.\n In some implementations, a policy may include a set of rules related to access to multimedia content.  For example, a policy may restrict access to a particular type of multimedia content and/or by a particular user (e.g., based on a geographic\nlocation of the user, a service level of an account associated with the user, whether the multimedia content includes objectionable content, such as nudity, violence, particular language, etc.), and/or the like.  In some implementations, policy data may\ninclude text data (e.g., a document, a webpage, a text file, etc.).\n In some implementations, metadata may include information associated with a user of client device 210 that is attempting to access the multimedia content.  For example, metadata may include a username, a password, an IP address, a MAC address,\naccount information (e.g., a service level of an account, an account number, etc.), and/or the like associated with a user attempting to access multimedia content and/or a destination of the multimedia content.  Additionally, or alternatively, and as\nanother example, metadata may be associated with a user associated with an organization and may identify a job title of the user, a level of the user within the organization, a role of the user, a clearance level of the user (e.g., a clearance level\nassociated with viewing sensitive or proprietary information), and/or the like to permit multimedia analysis platform 230 to determine whether access to content violates rules related to content that employees of the organization can access.\n In some implementations, multimedia analysis platform 230 may receive the multimedia data as multimedia content is being requested (e.g., in real-time or near real-time).  Additionally, or alternatively, multimedia analysis platform 230 may\nreceive the multimedia data prior to the multimedia content being requested.  For example, multimedia analysis platform 230 may process the multimedia content preemptively, such as to preemptively determine whether multimedia content violates a policy.\n In this way, multimedia analysis platform 230 may receive multimedia data, metadata, and/or policy data to permit multimedia analysis platform 230 to process the multimedia data, the metadata, and/or the policy data.\n As further shown in FIG. 4, process 400 may include processing the policy data using a first set of techniques to identify a set of terms and/or phrases associated with the policy (block 420).  For example, multimedia analysis platform 230 may\nprocess the policy data using a first set of techniques to identify a set of terms and/or phrases associated with the policy.\n In some implementations, multimedia analysis platform 230 may process millions, billions, trillions, etc., of data elements associated with hundreds, thousands, millions, etc., of policies.  In this way, multimedia analysis platform 230 may\nprocess a data set that cannot be processed manually or objectively by a human actor.\n In some implementations, when processing the policy data using the first set of techniques, multimedia analysis platform 230 may process the policy data using natural language processing, text analysis, computational linguistics, and/or the\nlike.  For example, multimedia analysis platform 230 may process text associated with a policy using natural language processing to identify a set of terms and/or phrases included in the text.  In some implementations, as described elsewhere herein,\nmultimedia analysis platform 230 may use identified terms and/or phrases to determine whether multimedia content and/or an attempted access to the multimedia content violates a policy.\n In some implementations, multimedia analysis platform 230 may identify a set of terms and/or phrases that is semantically similar to the set of terms and/or phrases identified in the policy data.  For example, multimedia analysis platform 230\nmay use a lexical database (e.g., WordNet), a multilingual lexicalized semantic network and ontology (e.g., BabelNet), and/or the like, to identify the set of terms and/or phrases that is semantically similar to the set of terms and/or phrases identified\nin the policy data.\n In this way, identifying a set of terms and/or phrases that is semantically similar to the set of terms and/or phrases identified in the policy data may permit multimedia analysis platform 230 to expand a policy beyond the set of terms and/or\nphrases identified in the policy data.  In some implementations, expanding the policy beyond the set of terms and/or phrases identified in the policy data may increase an accuracy of determining whether multimedia content violates a policy via use of a\nset of terms and/or phrases that is semantically similar to the set of terms and/or phrases included in the policy data.  In addition, this permits a more intelligent analysis of whether multimedia content and/or an attempted access to the multimedia\ncontent violates a policy.\n In this way, multimedia analysis platform 230 may process the policy data using a first set of techniques, to permit multimedia analysis platform 230 to determine a first set of embeddings.\n As further shown in FIG. 4, process 400 may include determining a first set of embeddings for the set of terms and/or phrases after identifying the set of terms and/or phrases (block 430).  For example, multimedia analysis platform 230 may\ndetermine a first set of embeddings for the set of terms and/or phrases after identifying the set of terms and/or phrases.  In some implementations, the first set of embeddings may identify the policy data (e.g., terms included in the policy data, terms\nsimilar to terms included in the policy data, etc.).\n In some implementations, multimedia analysis platform 230 may determine the first set of embeddings using a technique.  For example, multimedia analysis platform 230 may determine the first set of embeddings using Word2Vec, global vectors\n(GloVe), and/or the like.  In some implementations, an embedding may include a mapping of a term and/or phrase to an n-dimensional vector of real numbers.  In some implementations, to generate the mapping, multimedia analysis platform 230 may use a\nneural network, dimensionality reduction on a word or term co-occurrence matrix, a probabilistic model, and/or explicit representation in relation to a context in which a word or term appears.  In some implementations, an embedding may quantify and/or\ncategorize semantic similarities between linguistic items (e.g., words, terms, phrases, etc.) based on a manner in which the linguistic items are distributed in samples of language data (e.g., text).  In some implementations, generating an embedding may\ninclude performing a mathematical embedding from a space with one dimension per word or term to a continuous vector space with a lower dimension.\n In some implementations, multimedia analysis platform 230 may determine a score associated with an embedding.  For example, multimedia analysis platform 230 may determine a score for each dimension of an embedding.  In some implementations, a\nscore may be based on a context of an identified term and/or phrase (e.g., terms and/or phrases identified in policy data), a frequency of the term and/or phrase being included in policy data, a result of performing machine learning and/or using\nartificial intelligence (e.g., determining that policy data being analyzed is similar to other policy data on which multimedia analysis platform 230 was trained), a degree to which other terms and/or phrases are semantically similar to terms and/or\nphrases identified in the policy data, and/or the like.\n As a specific example, if multimedia analysis platform 230 identifies the term \"war\" in policy data, multimedia analysis platform 230 may determine a score for other dimensions identified by the terms \"violence,\" \"fighting,\" \"nudity,\" and/or the\nlike that indicates a degree to which those dimensions are associated with \"war.\" In some implementations, a threshold score, or a higher score relative to a lower score, may indicate that a dimension is more closely associated with an identified term\nand/or phrase.  For example, if multimedia analysis platform 230 determines a score of 0.7 for a \"violence\" dimension and a score of 0.1 for a \"nudity\" dimension, multimedia analysis platform 230 determine that the violence dimension is more closely\nassociated with \"war\" relative to \"nudity.\"\n Continuing with the previous example, multimedia analysis platform 230 may determine that multimedia content that includes \"violence\"-related content is more likely to violate a restriction on \"war\"-related content relative to \"nudity\"-related\ncontent.  In this way, multimedia analysis platform 230 may determine a score for various dimensions associated with an embedding that indicates a likelihood that the dimensions are associated with the embedding.  This provides multimedia analysis\nplatform 230 with flexibility to intelligently analyze multimedia content, thereby improving an analysis of multimedia content.\n In this way, multimedia analysis platform 230 may determine the first set of embeddings for the set of terms and/or phrases after identifying the set of terms and/or phrases and prior to processing the multimedia data and/or the metadata to\ndetermine a second set of embeddings.\n As further shown in FIG. 4, process 400 may include processing the multimedia data and/or the metadata using a second set of techniques to determine a second set of embeddings that identifies the multimedia data and/or the metadata (block 440). \nFor example, multimedia analysis platform 230 may process the multimedia data and/or the metadata using a second set of techniques to determine a second set of embeddings that identifies the multimedia data and/or the metadata.  In some implementations,\na technique, of the second set of techniques, may be associated with a type of data.  For example, a first technique may be associated with audio data, a second technique may be associated with video data, a third technique may be associated with\nmetadata, and/or the like.\n In some implementations, multimedia analysis platform 230 may process the multimedia data and/or the metadata using the second set of techniques to identify an object in an image, a particular type of motion in video content, particular types of\nsounds in audio content, and/or the like.  In some implementations, when processing the multimedia data and/or the metadata, multimedia analysis platform 230 may process millions, billions, trillions, etc. of data elements.  In this way, multimedia\nanalysis platform 230 may process a data set that cannot be processed manually or objectively by a human actor.  Additionally, or alternatively, multimedia analysis platform 230 may process metadata and/or multimedia data associated with hundreds,\nthousands, millions, etc. of items of multimedia content, requests for multimedia content, and/or the like.\n In some implementations, when processing audio data, multimedia analysis platform 230 may extract audio sequences from the audio data.  For example, multimedia analysis platform 230 may extract equal length audio sequences from the audio data. \nIn some implementations, multimedia analysis platform 230 may process the audio sequences using a recurrent neural network.  For example, multimedia analysis platform 230 may process the audio sequences to identify particular sounds in the audio, such as\nscreaming, fighting, a male voice, a female voice, and/or the like.  In some implementations, multimedia analysis platform 230 may determine an embedding for the sounds identified in the audio.  For example, multimedia analysis platform 230 may determine\nan embedding in a manner similar to that described elsewhere herein.  Additionally, or alternatively, multimedia analysis platform 230 may determine scores for a set of dimensions associated with an embedding for audio data in a manner similar to that\ndescribed elsewhere herein.\n In some implementations, when processing video data, multimedia analysis platform 230 may extract video sequences from the video data.  For example, multimedia analysis platform 230 may extract equal length video sequences from the video data. \nAdditionally, or alternatively, and as another example, multimedia analysis platform 230 may extract frames from the video sequences (e.g., from the video sequences extracted from video data).\n In some implementations, multimedia analysis platform 230 may process the frames using a convolutional neural network.  For example, multimedia analysis platform 230 may process the frames using a convolutional neural network to identify\nfeatures of the frames, such as objects shown in the frames.  Additionally, or alternatively, multimedia analysis platform 230 may process the frames using a motion extraction model (e.g., an incremental visual tracker to extract motion features from the\nframes to determine a type of motion or action shown in the frames).\n Additionally, or alternatively, multimedia analysis platform 230 may process the frames using a recurrent neural network (e.g., to extract temporal features of the frames, such as a sequence of events shown in video data, an amount of time\nparticular objects or actions are shown in video data, etc.).  In some implementations, multimedia analysis platform 230 may determine an embedding for an object, a motion, an action, and/or the like, identified in the video data in a manner similar to\nthat described elsewhere herein.  Additionally, or alternatively, multimedia analysis platform 230 may determine scores for a set of dimensions associated with an embedding for video data in a manner similar to that described elsewhere herein.\n In some implementations, when processing the metadata, multimedia analysis platform 230 may process the metadata using natural language processing, text analysis, computational linguistics, and/or the like.  For example, multimedia analysis\nplatform 230 may process the metadata to identify a term and/or a phrase included in the metadata (e.g., to determine an age of a user of client device 210 associated with the metadata, to determine a location of a user of client device 210 associated\nwith the metadata, to determine an organization with which client device 210 and/or the user of client device 210 is associated, to determine a context of multimedia content based on a description of the multimedia content, etc.).  In some\nimplementations, multimedia analysis platform 230 may determine an embedding for the term and/or phrase identified in the metadata in a manner similar to that described elsewhere herein.  Additionally, or alternatively, multimedia analysis platform 230\nmay determine scores for a set of dimensions associated with an embedding for metadata in a manner similar to that described elsewhere herein.\n In this way, multimedia analysis platform 230 may process the multimedia data and/or the metadata using a second set of techniques to determine a second set of embeddings prior to processing the first set of embeddings and/or the second set of\nembeddings using a knowledge graph.\n As further shown in FIG. 4, process 400 may include processing the first set of embeddings and/or the second set of embeddings using a knowledge graph to determine whether the multimedia content and/or access by the user violates the policy\n(block 450).  For example, multimedia analysis platform 230 may process the first set of embeddings and/or the second set of embeddings using a knowledge graph to determine whether the multimedia content and/or access by the user violates the policy.  In\nsome implementations, multimedia analysis platform 230 may use a knowledge graph to determine whether a user is permitted access to the multimedia content based on an age of the user, a location of the user, an image included in the multimedia content,\nan object included in the multimedia content, an action shown in the multimedia content, a sound included in the multimedia content, and/or the like (e.g., based on using the knowledge graph to determine whether multimedia content and/or access to the\nmultimedia content satisfies permissions and/or restrictions included in the policy).\n In some implementations, multimedia analysis platform 230 may process thousands, millions, billions, etc. of data elements when processing the first set of embeddings and/or the second set of embeddings.  In addition, multimedia analysis\nplatform 230 may process data related to thousands, millions, etc. of items of multimedia content and/or requests for multimedia content.  Further, multimedia analysis platform 230 may process multimedia content and/or requests for multimedia content\nwith respect to hundreds, thousands, millions, etc. of policies.  In this way, multimedia analysis platform 230 may process a data set that cannot be processed manually or objectively by a human actor.\n In some implementations, prior to processing the first set of embeddings and/or the second set of embeddings using a knowledge graph, multimedia analysis platform 230 may identify a set of terms associated with an embedding.  For example, an\nembedding may include a set of probabilities for a set of dimensions associated with the embedding.  In this case, a subset of the set of probabilities for a set of dimensions may be associated with a term and/or phrase (e.g., an embedding for basketball\nmay be associated with a term \"ball\" and a set of probabilities for a set of dimensions related to the term \"ball\").  Continuing still with the previous example, multimedia analysis platform 230 may determine a score (e.g., an average score, a weighted\naverage score, a weighted score, etc.) for each of the terms and/or phrases associated with the embedding based on the set of probabilities associated with the term.\n In some implementations, a score may be based on a context of an identified term and/or phrase (e.g., terms and/or phrases identified in text, objects and/or actions identified in video, etc.).  Additionally, or alternatively, a score may be\nbased on a frequency of the term and/or phrase being associated with the multimedia content (e.g., a frequency of the term and/or phrase being included in text, a frequency of an object identified by the term and/or phrase being included in video, etc.). Additionally, or alternatively, a score may be based on a result of performing machine learning and/or using artificial intelligence (e.g., determining that data being analyzed is similar to other data on which multimedia analysis platform 230 was\ntrained).  In this way, multimedia analysis platform 230 may determine a score that indicates a likelihood that a set of terms and/or phrases associated with an embedding identifies data for which the embedding was generated.\n In some implementations, multimedia analysis platform 230 may identify a set of terms and/or phrases associated with the first set of embeddings and/or the second set of embeddings that has a score that satisfies a threshold.  In this way,\nmultimedia analysis platform 230 may identify a set of terms and/or phrases that has the highest probability of identifying the multimedia content, the metadata, and/or the policy (e.g., relative to other terms associated with the first set of embeddings\nand/or the second set of embeddings).  For example, for a policy, multimedia analysis platform 230 may identify a set of terms and/or phrases that includes \"violence,\" \"nudity,\" and \"age\" as identifying the policy.\n Additionally, or alternatively, as an example for multimedia content, multimedia analysis platform 230 may identify a set of terms and/or phrases that includes \"fighting,\" \"cheering,\" and \"ball\" as identifying the multimedia content. \nAdditionally, or alternatively, and as an example for metadata, multimedia analysis platform 230 may identify a set of terms and/or phrases that includes \"Saudi Arabia\" and \"middle east\" as identifying a location of a user attempting to access the\ncontent, and \"nineteen\" as identifying an age of the user attempting to access the content.  This permits multimedia analysis platform 230 to quickly and efficiently determine whether multimedia content and/or access to the multimedia content violates a\npolicy by using a set of terms and/or phrases that identify the multimedia content, the user, and/or the policy with a threshold likelihood.\n In some implementations, when processing the first set of embeddings and/or the second set of embeddings using a knowledge graph, multimedia analysis platform 230 may populate the knowledge graph with information identifying the set of terms\nand/or phrases associated with the first set of embeddings and/or the second set of embeddings.  For example, multimedia analysis platform 230 may structure the set of terms and/or phrases into a set of hierarchical relationships using the knowledge\ngraph, where each term and/or phrase included in the set of terms and/or phrases is a node in the knowledge graph (e.g., a relationship of a parent and child nodes, such as \"sport\" and \"basketball\" nodes, where the \"basketball\" node is a child node of\nthe \"sport\" node).  This permits multimedia analysis platform 230 to quickly and efficiently analyze multimedia content and/or access to the multimedia content in an intelligent manner by permitting multimedia analysis platform 230 to differentiate\nbetween granular concepts, such as different types of sports, different contexts, such as an academic context or an entertainment context, and/or the like.\n In some implementations, multimedia analysis platform 230 may determine whether multimedia content and/or access by a user violates a policy.  For example, multimedia analysis platform 230 may determine whether multimedia content and/or access\nby a user violates a policy based on a result of using the knowledge graph.  In some implementations, multimedia analysis platform 230 may use edges of the knowledge graph to determine whether multimedia content and/or access by a user violates a policy. For example, the edges of the knowledge graph may indicate matches between the multimedia content, access by the user, and/or the policy.  For example, the edges of the knowledge graph may indicate that a user accessing the multimedia content is located\nin Saudi Arabia, that the multimedia content includes nudity, and that a policy prevents users located in Saudi Arabia from accessing multimedia content that includes nudity.  Continuing still with the previous example, multimedia analysis platform 230\nmay determine that the multimedia content and/or the access by the user violates a policy.\n In some implementations, multimedia analysis platform 230 may determine a score based on a result of using the knowledge graph to process the first set of embeddings and/or the second set of embeddings.  For example, multimedia analysis platform\n230 may use the score as a measure of confidence as to whether the multimedia content is correctly identified in the knowledge graph.  Continuing with the previous example, the score may be based on other scores previously determined by multimedia\nanalysis platform 230, may be based on, or may indicate, a degree to which information used to populate the knowledge graph matches nodes of the knowledge graph, may be based on, or may indicate, a degree to which nodes associated with multimedia content\nmatches nodes associated with a policy, may be based on, or may indicate, a degree to which multimedia content being analyzed matches known multimedia content, and/or the like.  In some implementations, multimedia analysis platform 230 may use the score\nwhen performing an action (e.g., may generate a particular recommendation based on a score satisfying a threshold).  In this way, multimedia analysis platform 230 may use a score to determine whether identified multimedia content is correctly identified.\n In this way, multimedia analysis platform 230 may process the first set of embeddings and/or the second set of embeddings using a knowledge graph to determine whether the multimedia content and/or access by the user violates the policy to permit\nand/or cause multimedia analysis platform 230 to perform an action based on a result of processing the first set of embeddings and/or the second set of embeddings.\n As further shown in FIG. 4, process 400 may include performing an action based on a result of processing the first set of embeddings and/or the second set of embeddings (block 460).  For example, multimedia analysis platform 230 may perform an\naction based on a result of processing the first set of embeddings and/or the second set of embeddings.\n In some implementations, multimedia analysis platform 230 may prevent access to the multimedia content.  Additionally, or alternatively, multimedia analysis platform 230 may provide a message and/or a notification to client device 210 indicating\nthat the user was denied access to the multimedia content.  In some implementations, multimedia analysis platform 230 may record information related to the attempted access (e.g., a date, a time, a username, etc., related to the attempted access). \nAdditionally, or alternatively, multimedia analysis platform 230 may permit access to the multimedia content.  Additionally, or alternatively, multimedia analysis platform 230 may provide a message and/or record information related to the attempted\naccess when multimedia analysis platform 230 permits access to multimedia content, in a manner similar to that described above.\n In some implementations, multimedia analysis platform 230 may generate a recommendation.  For example, multimedia analysis platform 230 may generate a recommendation related to whether access should be provided to the multimedia content. \nContinuing with the previous example, multimedia analysis platform 230 may determine that access to the content does not violate a policy and that the user attempting to access the content should be permitted to access the content.  In some\nimplementations, multimedia analysis platform 230 may provide the recommendation to an administrator (e.g., provide the recommendation for display via client device 210 associated with an administrator), such as to permit the administrator to make a\ndecision as to whether to permit the user to access the content.\n In some implementations, multimedia analysis platform 230 may obscure a portion of the multimedia content that violates the policy, such as by removing the portion, blurring a portion of video content or an image, removing a sound from audio\ncontent, replacing a sound in the audio content with another sound (e.g., a bleep), and/or the like.  Additionally, or alternatively, multimedia analysis platform 230 may provide a filtered version of the multimedia content (e.g., an existing filtered\nversion), such as a television version of an R-rated movie, a radio edit version of an explicit song, and/or the like.  In some implementations, multimedia analysis platform 230 may request permission to access the content from another user of another\nclient device 210 on behalf of the user requesting the multimedia content.\n In some implementations, multimedia analysis platform 230 may store information that identifies a result of an analysis.  For example, multimedia analysis platform 230 may store information that identifies whether multimedia content violates a\npolicy, whether a user is permitted to access multimedia content, and/or the like.  In some implementations, multimedia analysis platform 230 may use the stored information to quickly and efficiently determine whether multimedia content and/or access to\nthe content violates a policy, to improve future analyses, and/or the like.  This conserves processing resources of multimedia analysis platform 230 by reducing or eliminating a need for multimedia analysis platform 230 to process the same multimedia\ncontent multiple times, to analyze access to the multimedia content by the same user multiple times, and/or the like.\n In some implementations, multimedia analysis platform 230 may provide a result of an analysis for display via client device 210.  For example, multimedia analysis platform 230 may provide a result for display to permit a user of client device\n210 to perform a quality review of the analysis, to input information to modify a result of the analysis, and/or the like.  In some implementations, multimedia analysis platform 230 may use information received from client device 210 to modify access to\nmultimedia content, to perform machine learning to improve future analyses, and/or the like.\n In some implementations, multimedia analysis platform 230 may schedule a meeting.  For example, multimedia analysis platform 230 may schedule a meeting related to multimedia content, an attempted access to the multimedia content, and/or the like\n(e.g., among managers of an organization, lawmakers, etc.).  In some implementations, when scheduling a meeting, multimedia analysis platform 230 may use electronic calendars associated with potential meeting attendees to identify an available time for\nthe meeting and may generate a calendar item (e.g., a meeting invite, a meeting request, an appointment, etc.) for the meeting.\n In this way, multimedia analysis platform 230 may perform an action based on a result of processing the first set of embeddings and/or the second set of embeddings.\n Although FIG. 4 shows example blocks of process 400, in some implementations, process 400 may include additional blocks, fewer blocks, different blocks, or differently arranged blocks than those depicted in FIG. 4.  Additionally, or\nalternatively, two or more of the blocks of process 400 may be performed in parallel.\n Some implementations, described herein, provide a multimedia analysis platform that is capable of automatically analyzing multimedia content and determining whether a policy permits a particular user to access the multimedia content (e.g., based\non an age of the user, a location of the user, a type of multimedia content, an organization with which the user is associated, etc.).  In this way, the multimedia analysis platform may automatically and dynamically enforce a content-related policy. \nThis improves application of a policy to multimedia content and/or increases an efficiency of applying a policy to multimedia content.  In addition, this conserves processing resources that would otherwise be consumed due to an inefficient review of\ncontent.  Further, this permits review of an amount of content that cannot be processed manually or objectively by a human actor, or processed in a threshold amount of time by a human actor.\n The foregoing disclosure provides illustration and description, but is not intended to be exhaustive or to limit the implementations to the precise form disclosed.  Modifications and variations are possible in light of the above disclosure or\nmay be acquired from practice of the implementations.\n As used herein, the term component is intended to be broadly construed as hardware, firmware, and/or a combination of hardware and software.\n Some implementations are described herein in connection with thresholds.  As used herein, satisfying a threshold may refer to a value being greater than the threshold, more than the threshold, higher than the threshold, greater than or equal to\nthe threshold, less than the threshold, fewer than the threshold, lower than the threshold, less than or equal to the threshold, equal to the threshold, etc.\n It will be apparent that systems and/or methods, described herein, may be implemented in different forms of hardware, firmware, or a combination of hardware and software.  The actual specialized control hardware or software code used to\nimplement these systems and/or methods is not limiting of the implementations.  Thus, the operation and behavior of the systems and/or methods were described herein without reference to specific software code--it being understood that software and\nhardware can be designed to implement the systems and/or methods based on the description herein.\n Even though particular combinations of features are recited in the claims and/or disclosed in the specification, these combinations are not intended to limit the disclosure of possible implementations.  In fact, many of these features may be\ncombined in ways not specifically recited in the claims and/or disclosed in the specification.  Although each dependent claim listed below may directly depend on only one claim, the disclosure of possible implementations includes each dependent claim in\ncombination with every other claim in the claim set.\n No element, act, or instruction used herein should be construed as critical or essential unless explicitly described as such.  Also, as used herein, the articles \"a\" and \"an\" are intended to include one or more items, and may be used\ninterchangeably with \"one or more.\" Furthermore, as used herein, the term \"set\" is intended to include one or more items (e.g., related items, unrelated items, a combination of related and unrelated items, etc.), and may be used interchangeably with \"one\nor more.\" Where only one item is intended, the term \"one\" or similar language is used.  Also, as used herein, the terms \"has,\" \"have,\" \"having,\" or the like are intended to be open-ended terms.  Further, the phrase \"based on\" is intended to mean \"based,\nat least in part, on\" unless explicitly stated otherwise.", "application_number": "15639778", "abstract": " A device may receive multimedia data, metadata, and/or policy data. The\n     device may process the policy data using a first set of techniques to\n     determine a first set of embeddings for the policy data. The device may\n     process the multimedia data or the metadata using a second set of\n     techniques to determine a second set of embeddings for the multimedia\n     data or the metadata. The device may process the first set of embeddings\n     and the second set of embeddings using a knowledge graph to determine\n     whether the multimedia content or the access by the user violates the\n     policy. The device may perform an action based on a result of processing\n     the first set of embeddings and the second set of embeddings. The action\n     may relate to the multimedia content or the access to the multimedia\n     content by the user.\n", "citations": ["8762312", "20030126267", "20080228537", "20090016618", "20110093473", "20130006625", "20130347057", "20140007154", "20150215186", "20150249852", "20160071162", "20160180108", "20160295244", "20170078718", "20170272818"], "related": ["62504229"]}, {"id": "20180338032", "patent_code": "10298743", "patent_name": "Mobile terminal", "year": "2019", "inventor_and_country_data": " Inventors: \nBaek; Songyi (Seoul, KR), Chi; Jumin (Seoul, KR), Ham; Jihye (Seoul, KR)  ", "description": "CROSS-REFERENCE TO RELATED APPLICATIONS\n Pursuant to 35 U.S.C.  .sctn.  119(a), this application claims the benefit of earlier filing date and right of priority to Korean Patent Application No. 10-2017-0060896, filed on May 17, 2017, the contents of which are hereby incorporated by\nreference herein in its entirety.\nBACKGROUND\n The present invention relates to a mobile terminal and, more particularly, to a mobile terminal capable of recognizing loss thereof.  Artificial intelligence is computer engineering and information technology including methods of enabling a\ncomputer to perform thinking, learning, self-improvement, etc. capable of being performed by human intelligence, and enables a computer to imitate the intelligent behavior of human beings.\n In addition, artificial intelligence is directly or indirectly associated with other computer science fields.  In particular, today, attempts to introduce artificial intelligence elements into various fields of information technology and to\nsolve the problems of the fields have been actively conducted.\n Meanwhile, in the related art, context awareness technology for recognizing the situation of a user using artificial intelligence and providing desired information to the user in a desired form has been studied.\n As context awareness technology has been developed, demand for a mobile terminal capable of performing a function suitable for the situation of a user has increased.\n Recently, artificial intelligence technology has been applied to a mobile terminal.\n However, in the related art, if a mobile terminal is lost, a user may only access a server of a communication company to confirm the position of the mobile terminal, but cannot take other actions.\n Therefore, if the mobile terminal is lost, it is difficult for the user to find the mobile terminal.\nSUMMARY\n Accordingly, an object of the present invention is to address the above-noted and other problems.\n An object of the present invention is to recognize loss of a mobile terminal to restrict use of the mobile terminal by a finder.\n Another object of the present invention is to provide a mobile terminal capable of recognizing the loss state thereof to restrict use of the mobile terminal and to transmit information on the loss state of the mobile terminal to another mobile\nterminal.\n According to an embodiment of the present invention, a mobile terminal includes a wireless communication unit configured to perform wireless communication with an artificial intelligence device, an artificial intelligence unit configured to\nrecognize a loss state of the mobile terminal if information included in a signal received from the artificial intelligence device indicates the loss state of the mobile terminal and to generate a control signal for switching an operation mode of the\nmobile terminal according to the recognized loss state, and a controller configured to set the operation mode of the mobile terminal to a loss mode for restricting use of the mobile terminal according to the generated control signal.\n According to another embodiment, a mobile terminal includes a wireless communication unit configured to perform wireless communication with an artificial intelligence device, an artificial intelligence unit configured to collect context\ninformation of the mobile terminal, to recognize a loss state of the mobile terminal based on the collected context information and to generate a control signal for switching an operation mode of the mobile terminal according to the recognized loss\nstate, and a controller configured to set the operation mode of the mobile terminal to a loss mode for restricting use of the mobile terminal according to the generated control signal.\n Further scope of applicability of the present invention will become apparent from the detailed description given hereinafter.  However, it should be understood that the detailed description and specific examples, while indicating preferred\nembodiments of the invention, are given by illustration only, since various changes and modifications within the spirit and scope of the invention will become apparent to those skilled in the art from this detailed description. BRIEF DESCRIPTION\nOF THE DRAWINGS\n FIG. 1A is a block diagram of a mobile terminal in accordance with the present disclosure.\n FIGS. 1B and 1C are conceptual views of one example of the mobile terminal, viewed from different directions;\n FIG. 2 is a conceptual view of a deformable mobile terminal according to an alternative embodiment of the present disclosure.\n FIG. 3 is a conceptual view of a wearable mobile terminal according to another alternative embodiment of the present disclosure.\n FIG. 4 is a conceptual view of a wearable mobile terminal according to another alternative embodiment of the present disclosure.\n FIG. 5 is a ladder diagram illustrating a method of operating a mobile terminal according to an embodiment of the present invention.\n FIGS. 6 and 7 are diagrams illustrating a method of recognizing loss of a mobile terminal based on information received from an artificial intelligence device capable of performing communication with a mobile terminal according to an embodiment\nof the present invention.\n FIGS. 8 and 9 are diagrams illustrating operation performed in a loss mode of a mobile terminal according to an embodiment of the present invention.\n FIG. 10 is a ladder diagram illustrating a method of operating an artificial intelligence system according to another embodiment of the present invention.\n FIG. 11 is a diagram illustrating a process of sensing the loss state of a mobile terminal and transmitting information on loss of the mobile terminal to another mobile terminal.\n FIGS. 12a to 12c are diagrams illustrating an example of recognizing non-possession of a mobile terminal and automatically transmitting a notice to another mobile terminal according to an embodiment of the present invention.\n FIG. 13 is a diagram illustrating a process of performing automatic call forwarding to another phone number if an incoming call signal is received in a state in which a user does not possess a mobile terminal according to an embodiment of the\npresent invention.\n FIG. 14 is a diagram illustrating a process of remotely controlling a mobile terminal in a state in which a user does not possess the mobile terminal according to an embodiment of the present invention.\nDETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT\n Description will now be given in detail according to exemplary embodiments disclosed herein, with reference to the accompanying drawings.  For the sake of brief description with reference to the drawings, the same or equivalent components may be\nprovided with the same reference numbers, and description thereof will not be repeated.  In general, a suffix such as \"module\" and \"unit\" may be used to refer to elements or components.  Use of such a suffix herein is merely intended to facilitate\ndescription of the specification, and the suffix itself is not intended to give any special meaning or function.  In the present disclosure, that which is well-known to one of ordinary skill in the relevant art has generally been omitted for the sake of\nbrevity.  The accompanying drawings are used to help easily understand various technical features and it should be understood that the embodiments presented herein are not limited by the accompanying drawings.  As such, the present disclosure should be\nconstrued to extend to any alterations, equivalents and substitutes in addition to those which are particularly set out in the accompanying drawings.\n It will be understood that although the terms first, second, etc. may be used herein to describe various elements, these elements should not be limited by these terms.  These terms are generally only used to distinguish one element from another.\n It will be understood that if an element is referred to as being \"connected with\" another element, the element can be connected with the other element or intervening elements may also be present.  In contrast, if an element is referred to as\nbeing \"directly connected with\" another element, there are no intervening elements present.\n A singular representation may include a plural representation unless it represents a definitely different meaning from the context.  Terms such as \"include\" or \"has\" are used herein and should be understood that they are intended to indicate an\nexistence of several components, functions or steps, disclosed in the specification, and it is also understood that greater or fewer components, functions, or steps may likewise be utilized.\n Mobile terminals presented herein may be implemented using a variety of different types of terminals.  Examples of such terminals include cellular phones, smart phones, user equipment, laptop computers, digital broadcast terminals, personal\ndigital assistants (PDAs), portable multimedia players (PMPs), navigators, portable computers (PCs), slate PCs, tablet PCs, ultra books, wearable devices (for example, smart watches, smart glasses, head mounted displays (HMDs)), and the like.\n By way of non-limiting example only, further description will be made with reference to particular types of mobile terminals.  However, such teachings apply equally to other types of terminals, such as those types noted above.  In addition,\nthese teachings may also be applied to stationary terminals such as digital TV, desktop computers, and the like.\n Reference is now made to FIGS. 1A-1C, where FIG. 1A is a block diagram of a mobile terminal in accordance with the present disclosure, and FIGS. 1B and 1C are conceptual views of one example of the mobile terminal, viewed from different\ndirections.\n The mobile terminal 100 is shown having components such as a wireless communication unit 110, an input unit 120, an artificial intelligence unit (130), a sensing unit 140, an output unit 150, an interface unit 160, a memory 170, a controller\n180, and a power supply unit 190.  It is understood that implementing all of the illustrated components is not a requirement, and that greater or fewer components may alternatively be implemented.\n Referring now to FIG. 1A, the mobile terminal 100 is shown having wireless communication unit 110 configured with several commonly implemented components.  For instance, the wireless communication unit 110 typically includes one or more\ncomponents which permit wireless communication between the mobile terminal 100 and a wireless communication system or network within which the mobile terminal is located.\n The wireless communication unit 110 typically includes one or more modules which permit communications such as wireless communications between the mobile terminal 100 and a wireless communication system, communications between the mobile\nterminal 100 and another mobile terminal, communications between the mobile terminal 100 and an external server.  Further, the wireless communication unit 110 typically includes one or more modules which connect the mobile terminal 100 to one or more\nnetworks.  To facilitate such communications, the wireless communication unit 110 includes one or more of a broadcast receiving module 111, a mobile communication module 112, a wireless Internet module 113, a short-range communication module 114, and a\nlocation information module 115.\n The input unit 120 includes a camera 121 for obtaining images or video, a microphone 122, which is one type of audio input device for inputting an audio signal, and a user input unit 123 (for example, a touch key, a push key, a mechanical key, a\nsoft key, and the like) for allowing a user to input information.  Data (for example, audio, video, image, and the like) is obtained by the input unit 120 and may be analyzed and processed by controller 180 according to device parameters, user commands,\nand combinations thereof.\n The artificial intelligence unit 130 serves to process information based on artificial intelligence technology and may include one or more modules for performing at least one of information learning, information inference, information perception\nor natural language processing.\n The artificial intelligence unit 130 may perform at least one of learning, inference or processing of an enormous amount of information (big data) such as information stored in the mobile terminal, surrounding environmental information of the\nmobile terminal or information stored in an external storage capable of performing communication using machine learning technology.  The artificial intelligence unit 130 may control the mobile terminal to predict (or infer) operation of at least one\nexecutable mobile terminal and to perform operation having highest feasibility among one or more predicted operations using information learned using machine learning technology.\n Machine learning technology refers to technology for collecting and learning an enormous amount of information based on at least one algorithm and determining and predicting information based on the learned information.  Information learning\nrefers to operations for checking features, rules, judgement criteria, etc., of information, quantizing a relation between information and information and predicting new data using the quantized pattern.\n An algorithm using such machine learning technology may be based on statistics and may include, for example, a decision tree using a tree structure as a prediction model, a neural network for emulating the neural network structures and functions\nof living things, genetic programming based on evolutionary algorithms, clustering for dividing observed examples into subsets such as clusters, a Monte Carlo method of stochastically calculating a function value repeated random sampling, etc.\n Deep learning technology as machine learning technology refers to technology of performing at least one of information learning, judgement and processing using an artificial neural network.  The artificial neural network may have a structure for\nconnecting nodes and delivering data between nodes.  Such deep learning technology may learn an enormous amount of information through an artificial neural network using an optimized graphics processing unit (GPU) optimized for parallel arithmetic.\n Meanwhile, the artificial intelligence unit 130 may collect (sense, monitor, extract, detect, or receive) signals, data, information, etc. input to or output from components of the mobile terminal, in order to collect an enormous amount of\ninformation for applying machine learning technology.  In addition, the artificial intelligence unit 130 may collect (sense, monitor, extract, detect or receive) data, information, etc. stored in an external storage (e.g., a cloud server).  More\nspecifically, information collection may be understood as including operation for sensing information through a sensor, extracting information stored in the memory 170 or receiving information from the external storage through communication.\n The artificial intelligence unit 130 may sense internal information of the mobile terminal, surrounding environmental information of the mobile terminal and user information through the sensing unit 140.  In addition, the artificial intelligence\nunit 130 may receive broadcast signals and/or broadcast related information, wireless signals, wireless data, etc. through the wireless communication unit 110.  In addition, the artificial intelligence unit 130 may receive video information (or signal),\naudio signal (or signal), data or user input information from the input unit.\n The artificial intelligence unit 130 may collect an enormous amount of information on the background in real time, learn the information, process the information into an appropriate format (e.g., a knowledge graph, a command policy, a\npersonalization database, a dialog engine, etc.), and store the processed information in the memory 170.\n The artificial intelligence unit 130 may predict operation of the mobile terminal based on information learned using machine learning technology, control the components of the mobile terminal in order to perform the predicted operation or\ndeliver a control command for performing the predicted operation to the controller 180.  The controller 180 may control the mobile terminal based on the control command to perform the predicted operation.\n If specific operation is performed, the artificial intelligence unit 130 may analyze history information indicating the performed specific operation through machine learning technology and update previously learned information based on the\nanalyzed information.  The artificial intelligence unit 130 may improve information prediction accuracy.\n Meanwhile, in this specification, the artificial intelligence unit 130 and the controller 180 may be understood as the same component.  In this case, the function performed by the controller 180 described in this specification may be described\nas being performed by the artificial intelligence unit 130.  The controller 180 may be referred to as the artificial intelligence unit 130 or the artificial intelligence unit 130 may be referred to as the controller 180.\n In contrast, in this specification, the artificial intelligence unit 130 and the controller 180 may be understood as different components.  In this case, the artificial intelligence unit 130 and the controller 180 may exchange data with each\nother to perform a variety of control on the mobile terminal.  The controller 180 may perform at least one function on the mobile terminal and control at least one of the components of the mobile terminal, based on the result derived from the artificial\nintelligence unit 130.  Further, the artificial intelligence unit 130 may operate under control of the controller 180.\n The sensing unit 140 is typically implemented using one or more sensors configured to sense internal information of the mobile terminal, the surrounding environment of the mobile terminal, user information, and the like.  For example, in FIG.\n1A, the sensing unit 140 is shown having a proximity sensor 141 and an illumination sensor 142.\n If desired, the sensing unit 140 may alternatively or additionally include other types of sensors or devices, such as a touch sensor, an acceleration sensor, a magnetic sensor, a G-sensor, a gyroscope sensor, a motion sensor, an RGB sensor, an\ninfrared (IR) sensor, a finger scan sensor, a ultrasonic sensor, an optical sensor (for example, camera 121), a microphone 122, a battery gauge, an environment sensor (for example, a barometer, a hygrometer, a thermometer, a radiation detection sensor, a\nthermal sensor, and a gas sensor, among others), and a chemical sensor (for example, an electronic nose, a health care sensor, a biometric sensor, and the like), to name a few.  The mobile terminal 100 may be configured to utilize information obtained\nfrom sensing unit 140, and in particular, information obtained from one or more sensors of the sensing unit 140, and combinations thereof.\n The output unit 150 is typically configured to output various types of information, such as audio, video, tactile output, and the like.  The output unit 150 is shown having a display unit 151, an audio output module 152, a haptic module 153, and\nan optical output module 154.\n The display unit 151 may have an inter-layered structure or an integrated structure with a touch sensor in order to facilitate a touch screen.  The touch screen may provide an output interface between the mobile terminal 100 and a user, as well\nas function as the user input unit 123 which provides an input interface between the mobile terminal 100 and the user.\n The interface unit 160 serves as an interface with various types of external devices that can be coupled to the mobile terminal 100.  The interface unit 160, for example, may include any of wired or wireless ports, external power supply ports,\nwired or wireless data ports, memory card ports, ports for connecting a device having an identification module, audio input/output (I/O) ports, video I/O ports, earphone ports, and the like.  In some cases, the mobile terminal 100 may perform assorted\ncontrol functions associated with a connected external device, in response to the external device being connected to the interface unit 160.\n The memory 170 is typically implemented to store data to support various functions or features of the mobile terminal 100.  For instance, the memory 170 may be configured to store application programs executed in the mobile terminal 100, data or\ninstructions for operations of the mobile terminal 100, and the like.  Some of these application programs may be downloaded from an external server via wireless communication.  Other application programs may be installed within the mobile terminal 100 at\ntime of manufacturing or shipping, which is typically the case for basic functions of the mobile terminal 100 (for example, receiving a call, placing a call, receiving a message, sending a message, and the like).  It is common for application programs to\nbe stored in the memory 170, installed in the mobile terminal 100, and executed by the controller 180 to perform an operation (or function) for the mobile terminal 100.\n The controller 180 typically functions to control overall operation of the mobile terminal 100, in addition to the operations associated with the application programs.  The controller 180 may provide or process information or functions\nappropriate for a user by processing signals, data, information and the like, which are input or output by the various components depicted in FIG. 1A, or activating application programs stored in the memory 170.  As one example, the controller 180\ncontrols some or all of the components illustrated in FIGS. 1A-1C according to the execution of an application program that have been stored in the memory 170.\n The power supply unit 190 can be configured to receive external power or provide internal power in order to supply appropriate power required for operating elements and components included in the mobile terminal 100.  The power supply unit 190\nmay include a battery, and the battery may be configured to be embedded in the terminal body, or configured to be detachable from the terminal body.\n Referring still to FIG. 1A, various components depicted in this figure will now be described in more detail.  Regarding the wireless communication unit 110, the broadcast receiving module 111 is typically configured to receive a broadcast signal\nand/or broadcast associated information from an external broadcast managing entity via a broadcast channel.  The broadcast channel may include a satellite channel, a terrestrial channel, or both.  In some embodiments, two or more broadcast receiving\nmodules 111 may be utilized to facilitate simultaneously receiving of two or more broadcast channels, or to support switching among broadcast channels.\n The broadcast managing entity may be a server which generates and transmits a broadcast signal and/or broadcast associated information, or a server which receives a pre-generated broadcast signal and/or broadcast associated information, and\nsends such items to the mobile terminal.\n The broadcast signal may be implemented using any of a TV broadcast signal, a radio broadcast signal, a data broadcast signal, and combinations thereof, among others.  The broadcast signal in some cases may further include a data broadcast\nsignal combined with a TV or radio broadcast signal.\n The broadcast signal may be encoded according to any of a variety of technical standards or broadcasting methods (for example, International Organization for Standardization (ISO), International Electrotechnical Commission (IEC), Digital Video\nBroadcast (DVB), Advanced Television Systems Committee (ATSC), and the like) for transmission and reception of digital broadcast signals.  The broadcast receiving module 111 can receive the digital broadcast signals using a method appropriate for the\ntransmission method utilized.\n Examples of broadcast associated information may include information associated with a broadcast channel, a broadcast program, a broadcast event, a broadcast service provider, or the like.  The broadcast associated information may also be\nprovided via a mobile communication network, and in this case, received by the mobile communication module 112.\n The broadcast associated information may be implemented in various formats.  For instance, broadcast associated information may include an Electronic Program Guide (EPG) of Digital Multimedia Broadcasting (DMB), an Electronic Service Guide (ESG)\nof Digital Video Broadcast-Handheld (DVB-H), and the like.  Broadcast signals and/or broadcast associated information received via the broadcast receiving module 111 may be stored in a suitable device, such as a memory 170.\n The mobile communication module 112 can transmit and/or receive wireless signals to and from one or more network entities.  Typical examples of a network entity include a base station, an external mobile terminal, a server, and the like.  Such\nnetwork entities form part of a mobile communication network, which is constructed according to technical standards or communication methods for mobile communications (for example, Global System for Mobile Communication (GSM), Code Division Multi Access\n(CDMA), CDMA2000 (Code Division Multi Access 2000), EV-DO (Enhanced Voice-Data Optimized or Enhanced Voice-Data Only), Wideband CDMA (WCDMA), High Speed Downlink Packet access (HSDPA), HSUPA (High Speed Uplink Packet Access), Long Term Evolution (LTE),\nLTE-A (Long Term Evolution-Advanced), and the like).  Examples of wireless signals transmitted and/or received via the mobile communication module 112 include audio call signals, video (telephony) call signals, or various formats of data to support\ncommunication of text and multimedia messages.\n The wireless Internet module 113 is configured to facilitate wireless Internet access.  This module may be internally or externally coupled to the mobile terminal 100.  The wireless Internet module 113 may transmit and/or receive wireless\nsignals via communication networks according to wireless Internet technologies.\n Examples of such wireless Internet access include Wireless LAN (WLAN), Wireless Fidelity (Wi-Fi), Wi-Fi Direct, Digital Living Network Alliance (DLNA), Wireless Broadband (WiBro), Worldwide Interoperability for Microwave Access (WiMAX), High\nSpeed Downlink Packet Access (HSDPA), HSUPA (High Speed Uplink Packet Access), Long Term Evolution (LTE), LTE-A (Long Term Evolution-Advanced), and the like.  The wireless Internet module 113 may transmit/receive data according to one or more of such\nwireless Internet technologies, and other Internet technologies as well.\n In some embodiments, if the wireless Internet access is implemented according to, for example, WiBro, HSDPA, HSUPA, GSM, CDMA, WCDMA, LTE, LTE-A and the like, as part of a mobile communication network, the wireless Internet module 113 performs\nsuch wireless Internet access.  As such, the Internet module 113 may cooperate with, or function as, the mobile communication module 112.\n The short-range communication module 114 is configured to facilitate short-range communications.  Suitable technologies for implementing such short-range communications include BLUETOOTH.TM., Radio Frequency IDentification (RFID), Infrared Data\nAssociation (IrDA), Ultra-WideBand (UWB), ZigBee, Near Field Communication (NFC), Wireless-Fidelity (Wi-Fi), Wi-Fi Direct, Wireless USB (Wireless Universal Serial Bus), and the like.  The short-range communication module 114 in general supports wireless\ncommunications between the mobile terminal 100 and a wireless communication system, communications between the mobile terminal 100 and another mobile terminal 100, or communications between the mobile terminal and a network where another mobile terminal\n100 (or an external server) is located, via wireless area networks.  One example of the wireless area networks is a wireless personal area networks.\n In some embodiments, another mobile terminal (which may be configured similarly to mobile terminal 100) may be a wearable device, for example, a smart watch, a smart glass or a head mounted display (HMD), which is able to exchange data with the\nmobile terminal 100 (or otherwise cooperate with the mobile terminal 100).  The short-range communication module 114 may sense or recognize the wearable device, and permit communication between the wearable device and the mobile terminal 100.  In\naddition, if the sensed wearable device is a device which is authenticated to communicate with the mobile terminal 100, the controller 180, for example, may cause transmission of data processed in the mobile terminal 100 to the wearable device via the\nshort-range communication module 114.  Hence, a user of the wearable device may use the data processed in the mobile terminal 100 on the wearable device.  For example, if a call is received in the mobile terminal 100, the user may answer the call using\nthe wearable device.  Also, if a message is received in the mobile terminal 100, the user can check the received message using the wearable device.\n The location information module 115 is generally configured to detect, calculate, derive or otherwise identify a position of the mobile terminal.  As an example, the location information module 115 includes a Global Position System (GPS) module,\na Wi-Fi module, or both.  If desired, the location information module 115 may alternatively or additionally function with any of the other modules of the wireless communication unit 110 to obtain data related to the position of the mobile terminal.\n As one example, if the mobile terminal uses a GPS module, a position of the mobile terminal may be acquired using a signal sent from a GPS satellite.  As another example, if the mobile terminal uses the Wi-Fi module, a position of the mobile\nterminal can be acquired based on information related to a wireless access point (AP) which transmits or receives a wireless signal to or from the Wi-Fi module.\n The input unit 120 may be configured to permit various types of input to the mobile terminal 120.  Examples of such input include audio, image, video, data, and user input.  Image and video input is often obtained using one or more cameras 121. \nSuch cameras 121 may process image frames of still pictures or video obtained by image sensors in a video or image capture mode.  The processed image frames can be displayed on the display unit 151 or stored in memory 170.  In some cases, the cameras 121\nmay be arranged in a matrix configuration to permit a plurality of images having various angles or focal points to be input to the mobile terminal 100.  As another example, the cameras 121 may be located in a stereoscopic arrangement to acquire left and\nright images for implementing a stereoscopic image.\n The microphone 122 is generally implemented to permit audio input to the mobile terminal 100.  The audio input can be processed in various manners according to a function being executed in the mobile terminal 100.  If desired, the microphone 122\nmay include assorted noise removing algorithms to remove unwanted noise generated in the course of receiving the external audio.\n The user input unit 123 is a component that permits input by a user.  Such user input may enable the controller 180 to control operation of the mobile terminal 100.  The user input unit 123 may include one or more of a mechanical input element\n(for example, a key, a button located on a front and/or rear surface or a side surface of the mobile terminal 100, a dome switch, a jog wheel, a jog switch, and the like), or a touch-sensitive input, among others.  As one example, the touch-sensitive\ninput may be a virtual key or a soft key, which is displayed on a touch screen through software processing, or a touch key which is located on the mobile terminal at a location that is other than the touch screen.  On the other hand, the virtual key or\nthe visual key may be displayed on the touch screen in various shapes, for example, graphic, text, icon, video, or a combination thereof.\n The sensing unit 140 is generally configured to sense one or more of internal information of the mobile terminal, surrounding environment information of the mobile terminal, user information, or the like.  The controller 180 generally cooperates\nwith the sending unit 140 to control operation of the mobile terminal 100 or execute data processing, a function or an operation associated with an application program installed in the mobile terminal based on the sensing provided by the sensing unit\n140.  The sensing unit 140 may be implemented using any of a variety of sensors, some of which will now be described in more detail.\n The proximity sensor 141 may include a sensor to sense presence or absence of an object approaching a surface, or an object located near a surface, by using an electromagnetic field, infrared rays, or the like without a mechanical contact.  The\nproximity sensor 141 may be arranged at an inner region of the mobile terminal covered by the touch screen, or near the touch screen.\n The proximity sensor 141, for example, may include any of a transmissive type photoelectric sensor, a direct reflective type photoelectric sensor, a mirror reflective type photoelectric sensor, a high-frequency oscillation proximity sensor, a\ncapacitance type proximity sensor, a magnetic type proximity sensor, an infrared rays proximity sensor, and the like.  If the touch screen is implemented as a capacitance type, the proximity sensor 141 can sense proximity of a pointer relative to the\ntouch screen by changes of an electromagnetic field, which is responsive to an approach of an object with conductivity.  In this case, the touch screen (touch sensor) may also be categorized as a proximity sensor.\n The term \"proximity touch\" will often be referred to herein to denote the scenario in which a pointer is positioned to be proximate to the touch screen without contacting the touch screen.  The term \"contact touch\" will often be referred to\nherein to denote the scenario in which a pointer makes physical contact with the touch screen.  For the position corresponding to the proximity touch of the pointer relative to the touch screen, such position will correspond to a position where the\npointer is perpendicular to the touch screen.  The proximity sensor 141 may sense proximity touch, and proximity touch patterns (for example, distance, direction, speed, time, position, moving status, and the like).\n In general, controller 180 processes data corresponding to proximity touches and proximity touch patterns sensed by the proximity sensor 141, and cause output of visual information on the touch screen.  In addition, the controller 180 can\ncontrol the mobile terminal 100 to execute different operations or process different data according to whether a touch with respect to a point on the touch screen is either a proximity touch or a contact touch.\n A touch sensor can sense a touch applied to the touch screen, such as display unit 151, using any of a variety of touch methods.  Examples of such touch methods include a resistive type, a capacitive type, an infrared type, and a magnetic field\ntype, among others.\n As one example, the touch sensor may be configured to convert changes of pressure applied to a specific part of the display unit 151, or convert capacitance occurring at a specific part of the display unit 151, into electric input signals.  The\ntouch sensor may also be configured to sense not only a touched position and a touched area, but also touch pressure and/or touch capacitance.  A touch object is generally used to apply a touch input to the touch sensor.  Examples of typical touch\nobjects include a finger, a touch pen, a stylus pen, a pointer, or the like.\n If a touch input is sensed by a touch sensor, corresponding signals may be transmitted to a touch controller.  The touch controller may process the received signals, and then transmit corresponding data to the controller 180.  Accordingly, the\ncontroller 180 may sense which region of the display unit 151 has been touched.  Here, the touch controller may be a component separate from the controller 180, the controller 180, and combinations thereof.\n In some embodiments, the controller 180 may execute the same or different controls according to a type of touch object that touches the touch screen or a touch key provided in addition to the touch screen.  Whether to execute the same or\ndifferent control according to the object which provides a touch input may be decided based on a current operating state of the mobile terminal 100 or a currently executed application program, for example.\n The touch sensor and the proximity sensor may be implemented individually, or in combination, to sense various types of touches.  Such touches includes a short (or tap) touch, a long touch, a multi-touch, a drag touch, a flick touch, a pinch-in\ntouch, a pinch-out touch, a swipe touch, a hovering touch, and the like.\n If desired, an ultrasonic sensor may be implemented to recognize position information relating to a touch object using ultrasonic waves.  The controller 180, for example, may calculate a position of a wave generation source based on information\nsensed by an illumination sensor and a plurality of ultrasonic sensors.  Since light is much faster than ultrasonic waves, the time for which the light reaches the optical sensor is much shorter than the time for which the ultrasonic wave reaches the\nultrasonic sensor.  The position of the wave generation source may be calculated using this fact.  For instance, the position of the wave generation source may be calculated using the time difference from the time that the ultrasonic wave reaches the\nsensor based on the light as a reference signal.\n The camera 121 typically includes at least one a camera sensor (CCD, CMOS etc.), a photo sensor (or image sensors), and a laser sensor.\n Implementing the camera 121 with a laser sensor may allow detection of a touch of a physical object with respect to a 3D stereoscopic image.  The photo sensor may be laminated on, or overlapped with, the display device.  The photo sensor may be\nconfigured to scan movement of the physical object in proximity to the touch screen.  In more detail, the photo sensor may include photo diodes and transistors at rows and columns to scan content received at the photo sensor using an electrical signal\nwhich changes according to the quantity of applied light.  Namely, the photo sensor may calculate the coordinates of the physical object according to variation of light to thus obtain position information of the physical object.\n The display unit 151 is generally configured to output information processed in the mobile terminal 100.  For example, the display unit 151 may display execution screen information of an application program executing at the mobile terminal 100\nor user interface (UI) and graphic user interface (GUI) information in response to the execution screen information.\n In some embodiments, the display unit 151 may be implemented as a stereoscopic display unit for displaying stereoscopic images.  A typical stereoscopic display unit may employ a stereoscopic display scheme such as a stereoscopic scheme (a glass\nscheme), an auto-stereoscopic scheme (glassless scheme), a projection scheme (holographic scheme), or the like.\n In general, a 3D stereoscopic image may include a left image (e.g., a left eye image) and a right image (e.g., a right eye image).  According to how left and right images are combined into a 3D stereoscopic image, a 3D stereoscopic imaging\nmethod can be divided into a top-down method in which left and right images are located up and down in a frame, an L-to-R (left-to-right or side by side) method in which left and right images are located left and right in a frame, a checker board method\nin which fragments of left and right images are located in a tile form, an interlaced method in which left and right images are alternately located by columns or rows, and a time sequential (or frame by frame) method in which left and right images are\nalternately displayed on a time basis.\n Also, as for a 3D thumbnail image, a left image thumbnail and a right image thumbnail can be generated from a left image and a right image of an original image frame, respectively, and then combined to generate a single 3D thumbnail image.  In\ngeneral, the term \"thumbnail\" may be used to refer to a reduced image or a reduced still image.  A generated left image thumbnail and right image thumbnail may be displayed with a horizontal distance difference there between by a depth corresponding to\nthe disparity between the left image and the right image on the screen, thereby providing a stereoscopic space sense.\n A left image and a right image required for implementing a 3D stereoscopic image may be displayed on the stereoscopic display unit using a stereoscopic processing unit.  The stereoscopic processing unit can receive the 3D image and extract the\nleft image and the right image, or can receive the 2D image and change it into a left image and a right image.\n The audio output module 152 is generally configured to output audio data.  Such audio data may be obtained from any of a number of different sources, such that the audio data may be received from the wireless communication unit 110 or may have\nbeen stored in the memory 170.  The audio data may be output during modes such as a signal reception mode, a call mode, a record mode, a voice recognition mode, a broadcast reception mode, and the like.  The audio output module 152 can provide audible\noutput related to a particular function (e.g., a call signal reception sound, a message reception sound, etc.) performed by the mobile terminal 100.  The audio output module 152 may also be implemented as a receiver, a speaker, a buzzer, or the like.\n A haptic module 153 can be configured to generate various tactile effects that a user feels, perceive, or otherwise experience.  A typical example of a tactile effect generated by the haptic module 153 is vibration.  The strength, pattern and\nthe like of the vibration generated by the haptic module 153 can be controlled by user selection or setting by the controller.  For example, the haptic module 153 may output different vibrations in a combining manner or a sequential manner.\n Besides vibration, the haptic module 153 can generate various other tactile effects, including an effect by stimulation such as a pin arrangement vertically moving to contact skin, a spray force or suction force of air through a jet orifice or a\nsuction opening, a touch to the skin, a contact of an electrode, electrostatic force, an effect by reproducing the sense of cold and warmth using an element that can absorb or generate heat, and the like.\n The haptic module 153 can also be implemented to allow the user to feel a tactile effect through a muscle sensation such as the user's fingers or arm, as well as transferring the tactile effect through direct contact.  Two or more haptic modules\n153 may be provided according to the particular configuration of the mobile terminal 100.\n An optical output module 154 can output a signal for indicating an event generation using light of a light source.  Examples of events generated in the mobile terminal 100 may include message reception, call signal reception, a missed call, an\nalarm, a schedule notice, an email reception, information reception through an application, and the like.\n A signal output by the optical output module 154 may be implemented in such a manner that the mobile terminal emits monochromatic light or light with a plurality of colors.  The signal output may be terminated as the mobile terminal senses that\na user has checked the generated event, for example.\n The interface unit 160 serves as an interface for external devices to be connected with the mobile terminal 100.  For example, the interface unit 160 can receive data transmitted from an external device, receive power to transfer to elements and\ncomponents within the mobile terminal 100, or transmit internal data of the mobile terminal 100 to such external device.  The interface unit 160 may include wired or wireless headset ports, external power supply ports, wired or wireless data ports,\nmemory card ports, ports for connecting a device having an identification module, audio input/output (I/O) ports, video I/O ports, earphone ports, or the like.\n The identification module may be a chip that stores various information for authenticating authority of using the mobile terminal 100 and may include a user identity module (UIM), a subscriber identity module (SIM), a universal subscriber\nidentity module (USIM), and the like.  In addition, the device having the identification module (also referred to herein as an \"identifying device\") may take the form of a smart card.  Accordingly, the identifying device can be connected with the\nterminal 100 via the interface unit 160.\n If the mobile terminal 100 is connected with an external cradle, the interface unit 160 can serve as a passage to allow power from the cradle to be supplied to the mobile terminal 100 or may serve as a passage to allow various command signals\ninput by the user from the cradle to be transferred to the mobile terminal there through.  Various command signals or power input from the cradle may operate as signals for recognizing that the mobile terminal is properly mounted on the cradle.\n The memory 170 can store programs to support operations of the controller 180 and store input/output data (for example, phonebook, messages, still images, videos, etc.).  The memory 170 may store data related to various patterns of vibrations\nand audio which are output in response to touch inputs on the touch screen.\n The memory 170 may include one or more types of storage mediums including a Flash memory, a hard disk, a solid state disk, a silicon disk, a multimedia card micro type, a card-type memory (e.g., SD or DX memory, etc), a Random Access Memory\n(RAM), a Static Random Access Memory (SRAM), a Read-Only Memory (ROM), an Electrically Erasable Programmable Read-Only Memory (EEPROM), a Programmable Read-Only memory (PROM), a magnetic memory, a magnetic disk, an optical disk, and the like.  The mobile\nterminal 100 may also be operated in relation to a network storage device that performs the storage function of the memory 170 over a network, such as the Internet.\n The controller 180 may typically control the general operations of the mobile terminal 100.  For example, the controller 180 may set or release a lock state for restricting a user from inputting a control command with respect to applications if\na status of the mobile terminal meets a preset condition.\n The controller 180 can also perform the controlling and processing associated with voice calls, data communications, video calls, and the like, or perform pattern recognition processing to recognize a handwriting input or a picture drawing input\nperformed on the touch screen as characters or images, respectively.  In addition, the controller 180 can control one or a combination of those components in order to implement various exemplary embodiments disclosed herein.\n The power supply unit 190 receives external power or provide internal power and supply the appropriate power required for operating respective elements and components included in the mobile terminal 100.  The power supply unit 190 may include a\nbattery, which is typically rechargeable or be detachably coupled to the terminal body for charging.\n The power supply unit 190 may include a connection port.  The connection port may be configured as one example of the interface unit 160 to which an external charger for supplying power to recharge the battery is electrically connected.\n As another example, the power supply unit 190 may be configured to recharge the battery in a wireless manner without use of the connection port.  In this example, the power supply unit 190 can receive power, transferred from an external wireless\npower transmitter, using at least one of an inductive coupling method which is based on magnetic induction or a magnetic resonance coupling method which is based on electromagnetic resonance.\n Various embodiments described herein may be implemented in a computer-readable medium, a machine-readable medium, or similar medium using, for example, software, hardware, or any combination thereof.\n Referring now to FIGS. 1B and 1C, the mobile terminal 100 is described with reference to a bar-type terminal body.  However, the mobile terminal 100 may alternatively be implemented in any of a variety of different configurations.  Examples of\nsuch configurations include watch-type, clip-type, glasses-type, or as a folder-type, flip-type, slide-type, swing-type, and swivel-type in which two and more bodies are combined with each other in a relatively movable manner, and combinations thereof. \nDiscussion herein will often relate to a particular type of mobile terminal (for example, bar-type, watch-type, glasses-type, and the like).  However, such teachings with regard to a particular type of mobile terminal will generally apply to other types\nof mobile terminals as well.\n The mobile terminal 100 will generally include a case (for example, frame, housing, cover, and the like) forming the appearance of the terminal.  In this embodiment, the case is formed using a front case 101 and a rear case 102.  Various\nelectronic components are incorporated into a space formed between the front case 101 and the rear case 102.  At least one middle case may be additionally positioned between the front case 101 and the rear case 102.\n The display unit 151 is shown located on the front side of the terminal body to output information.  As illustrated, a window 151a of the display unit 151 may be mounted to the front case 101 to form the front surface of the terminal body\ntogether with the front case 101.\n In some embodiments, electronic components may also be mounted to the rear case 102.  Examples of such electronic components include a detachable battery 191, an identification module, a memory card, and the like.  Rear cover 103 is shown\ncovering the electronic components, and this cover may be detachably coupled to the rear case 102.  Therefore, if the rear cover 103 is detached from the rear case 102, the electronic components mounted to the rear case 102 are externally exposed.\n As illustrated, if the rear cover 103 is coupled to the rear case 102, a side surface of the rear case 102 is partially exposed.  In some cases, upon the coupling, the rear case 102 may also be completely shielded by the rear cover 103.  In some\nembodiments, the rear cover 103 may include an opening for externally exposing a camera 121b or an audio output module 152b.\n The cases 101, 102, 103 may be formed by injection-molding synthetic resin or may be formed of a metal, for example, stainless steel (STS), aluminum (Al), titanium (Ti), or the like.\n As an alternative to the example in which the plurality of cases form an inner space for accommodating components, the mobile terminal 100 may be configured such that one case forms the inner space.  In this example, a mobile terminal 100 having\na uni-body is formed in such a manner that synthetic resin or metal extends from a side surface to a rear surface.\n If desired, the mobile terminal 100 may include a waterproofing unit (not shown) for preventing introduction of water into the terminal body.  For example, the waterproofing unit may include a waterproofing member which is located between the\nwindow 151a and the front case 101, between the front case 101 and the rear case 102, or between the rear case 102 and the rear cover 103, to hermetically seal an inner space if those cases are coupled.\n The mobile terminal includes a display unit 151, a first and a second audio output modules 151a/151b, a proximity sensor 141, an illumination sensor 142, an optical output module 154, a first and a second cameras 121a/121b, a first and a second\nmanipulation units 123a/123b, a michrophone 122, interface unit 160 and the like.\n It will be described for the mobile terminal as shown in FIGS. 1B and 1C.  The display unit 151, the first audio output module 151a, the proximity sensor 141, an illumination sensor 142, the optical output module 154, the first camera 121a and\nthe first manipulation unit 123a are arranged in front surface of the terminal body, the second manipulation unit 123b, the michrophone 122 and interface unit 160 are arranged in side surface of the terminal body, and the second audio output modules 151b\nand the second camera 121b are arranged in rear surface of the terminal body.\n However, it is to be understood that alternative arrangements are possible and within the teachings of the instant disclosure.  Some components may be omitted or rearranged.  For example, the first manipulation unit 123a may be located on\nanother surface of the terminal body, and the second audio output module 152b may be located on the side surface of the terminal body.\n The display unit 151 outputs information processed in the mobile terminal 100.  The display unit 151 may be implemented using one or more suitable display devices.  Examples of such suitable display devices include a liquid crystal display\n(LCD), a thin film transistor-liquid crystal display (TFT-LCD), an organic light emitting diode (OLED), a flexible display, a 3-dimensional (3D) display, an e-ink display, and combinations thereof.\n The display unit 151 may be implemented using two display devices, which can implement the same or different display technology.  For instance, a plurality of the display units 151 may be arranged on one side, either spaced apart from each\nother, or these devices may be integrated, or these devices may be arranged on different surfaces.\n The display unit 151 may also include a touch sensor which senses a touch input received at the display unit.  If a touch is input to the display unit 151, the touch sensor may be configured to sense this touch and the controller 180, for\nexample, may generate a control command or other signal corresponding to the touch.  The content which is input in the touching manner may be a text or numerical value, or a menu item which can be indicated or designated in various modes.\n The touch sensor may be configured in a form of a film having a touch pattern, disposed between the window 151a and a display on a rear surface of the window 151a, or a metal wire which is patterned directly on the rear surface of the window\n151a.  Alternatively, the touch sensor may be integrally formed with the display.  For example, the touch sensor may be disposed on a substrate of the display or within the display.\n The display unit 151 may also form a touch screen together with the touch sensor.  Here, the touch screen may serve as the user input unit 123 (see FIG. 1A).  Therefore, the touch screen may replace at least some of the functions of the first\nmanipulation unit 123a.\n The first audio output module 152a may be implemented in the form of a speaker to output voice audio, alarm sounds, multimedia audio reproduction, and the like.\n The window 151a of the display unit 151 will typically include an aperture to permit audio generated by the first audio output module 152a to pass.  One alternative is to allow audio to be released along an assembly gap between the structural\nbodies (for example, a gap between the window 151a and the front case 101).  In this case, a hole independently formed to output audio sounds may not be seen or is otherwise hidden in terms of appearance, thereby further simplifying the appearance and\nmanufacturing of the mobile terminal 100.\n The optical output module 154 can be configured to output light for indicating an event generation.  Examples of such events include a message reception, a call signal reception, a missed call, an alarm, a schedule notice, an email reception,\ninformation reception through an application, and the like.  If a user has checked a generated event, the controller can control the optical output unit 154 to stop the light output.\n The first camera 121a can process image frames such as still or moving images obtained by the image sensor in a capture mode or a video call mode.  The processed image frames can then be displayed on the display unit 151 or stored in the memory\n170.\n The first and second manipulation units 123a and 123b are examples of the user input unit 123, which may be manipulated by a user to provide input to the mobile terminal 100.  The first and second manipulation units 123a and 123b may also be\ncommonly referred to as a manipulating portion, and may employ any tactile method that allows the user to perform manipulation such as touch, push, scroll, or the like.  The first and second manipulation units 123a and 123b may also employ any\nnon-tactile method that allows the user to perform manipulation such as proximity touch, hovering, or the like.\n FIG. 1B illustrates the first manipulation unit 123a as a touch key, but possible alternatives include a mechanical key, a push key, a touch key, and combinations thereof.\n Input received at the first and second manipulation units 123a and 123b may be used in various ways.  For example, the first manipulation unit 123a may be used by the user to provide an input to a menu, home key, cancel, search, or the like, and\nthe second manipulation unit 123b may be used by the user to provide an input to control a volume level being output from the first or second audio output modules 152a or 152b, to switch to a touch recognition mode of the display unit 151, or the like.\n As another example of the user input unit 123, a rear input unit (not shown) may be located on the rear surface of the terminal body.  The rear input unit can be manipulated by a user to provide input to the mobile terminal 100.  The input may\nbe used in a variety of different ways.  For example, the rear input unit may be used by the user to provide an input for power on/off, start, end, scroll, control volume level being output from the first or second audio output modules 152a or 152b,\nswitch to a touch recognition mode of the display unit 151, and the like.  The rear input unit may be configured to permit touch input, a push input, or combinations thereof.\n The rear input unit may be located to overlap the display unit 151 of the front side in a thickness direction of the terminal body.  As one example, the rear input unit may be located on an upper end portion of the rear side of the terminal body\nsuch that a user can easily manipulate it using a forefinger if the user grabs the terminal body with one hand.  Alternatively, the rear input unit can be positioned at most any location of the rear side of the terminal body.\n Embodiments that include the rear input unit may implement some or all of the functionality of the first manipulation unit 123a in the rear input unit.  As such, in situations where the first manipulation unit 123a is omitted from the front\nside, the display unit 151 can have a larger screen.\n As a further alternative, the mobile terminal 100 may include a finger scan sensor which scans a user's fingerprint.  The controller 180 can then use fingerprint information sensed by the finger scan sensor as part of an authentication\nprocedure.  The finger scan sensor may also be installed in the display unit 151 or implemented in the user input unit 123.\n The microphone 122 is shown located at an end of the mobile terminal 100, but other locations are possible.  If desired, multiple microphones may be implemented, with such an arrangement permitting the receiving of stereo sounds.\n The interface unit 160 may serve as a path allowing the mobile terminal 100 to interface with external devices.  For example, the interface unit 160 may include one or more of a connection terminal for connecting to another device (for example,\nan earphone, an external speaker, or the like), a port for near field communication (for example, an Infrared Data Association (IrDA) port, a Bluetooth port, a wireless LAN port, and the like), or a power supply terminal for supplying power to the mobile\nterminal 100.  The interface unit 160 may be implemented in the form of a socket for accommodating an external card, such as Subscriber Identification Module (SIM), User Identity Module (UIM), or a memory card for information storage.\n The second camera 121b is shown located at the rear side of the terminal body and includes an image capturing direction that is substantially opposite to the image capturing direction of the first camera unit 121a.  If desired, second camera\n121a may alternatively be located at other locations, or made to be moveable, in order to have a different image capturing direction from that which is shown.\n The second camera 121b can include a plurality of lenses arranged along at least one line.  The plurality of lenses may also be arranged in a matrix configuration.  The cameras may be referred to as an \"array camera.\" If the second camera 121b\nis implemented as an array camera, images may be captured in various manners using the plurality of lenses and images with better qualities.\n As shown in FIG. 1C, a flash 124 is shown adjacent to the second camera 121b.  If an image of a subject is captured with the camera 121b, the flash 124 may illuminate the subject.\n As shown in FIG. 1B, the second audio output module 152b can be located on the terminal body.  The second audio output module 152b may implement stereophonic sound functions in conjunction with the first audio output module 152a, and may be also\nused for implementing a speaker phone mode for call communication.\n At least one antenna for wireless communication may be located on the terminal body.  The antenna may be installed in the terminal body or formed by the case.  For example, an antenna which configures a part of the broadcast receiving module 111\nmay be retractable into the terminal body.  Alternatively, an antenna may be formed using a film attached to an inner surface of the rear cover 103, or a case that includes a conductive material.\n A power supply unit 190 for supplying power to the mobile terminal 100 may include a battery 191, which is mounted in the terminal body or detachably coupled to an outside of the terminal body.  The battery 191 may receive power via a power\nsource cable connected to the interface unit 160.  Also, the battery 191 can be recharged in a wireless manner using a wireless charger.  Wireless charging may be implemented by magnetic induction or electromagnetic resonance.\n The rear cover 103 is shown coupled to the rear case 102 for shielding the battery 191, to prevent separation of the battery 191, and to protect the battery 191 from an external impact or from foreign material.  If the battery 191 is detachable\nfrom the terminal body, the rear case 103 may be detachably coupled to the rear case 102.\n An accessory for protecting an appearance or assisting or extending the functions of the mobile terminal 100 can also be provided on the mobile terminal 100.  As one example of an accessory, a cover or pouch for covering or accommodating at\nleast one surface of the mobile terminal 100 may be provided.  The cover or pouch may cooperate with the display unit 151 to extend the function of the mobile terminal 100.  Another example of the accessory is a touch pen for assisting or extending a\ntouch input to a touch screen.\n FIG. 2 is a conceptual view of a deformable mobile terminal according to an alternative embodiment of the present invention.  In this figure, mobile terminal 200 is shown having display unit 251, which is a type of display that is deformable by\nan external force.  This deformation, which includes display unit 251 and other components of mobile terminal 200, may include any of curving, bending, folding, twisting, rolling, and combinations thereof.  The deformable display unit 251 may also be\nreferred to as a \"flexible display unit.\" In some implementations, the flexible display unit 251 may include a general flexible display, electronic paper (also known as e-paper), and combinations thereof.  In general, mobile terminal 200 may be\nconfigured to include features that are the same or similar to that of mobile terminal 100 of FIGS. 1A-1C.\n The flexible display of mobile terminal 200 is generally formed as a lightweight, non-fragile display, which still exhibits characteristics of a conventional flat panel display, but is instead fabricated on a flexible substrate which can be\ndeformed as noted previously.\n The term e-paper may be used to refer to a display technology employing the characteristic of a general ink, and is different from the conventional flat panel display in view of using reflected light.  E-paper is generally understood as changing\ndisplayed information using a twist ball or via electrophoresis using a capsule.\n If in a state that the flexible display unit 251 is not deformed (for example, in a state with an infinite radius of curvature and referred to as a first state), a display region of the flexible display unit 251 includes a generally flat\nsurface.  If in a state that the flexible display unit 251 is deformed from the first state by an external force (for example, a state with a finite radius of curvature and referred to as a second state), the display region may become a curved surface or\na bent surface.  As illustrated, information displayed in the second state may be visual information output on the curved surface.  The visual information may be realized in such a manner that a light emission of each unit pixel (sub-pixel) arranged in a\nmatrix configuration is controlled independently.  The unit pixel denotes an elementary unit for representing one color.\n According to one alternative embodiment, the first state of the flexible display unit 251 may be a curved state (for example, a state of being curved from up to down or from right to left), instead of being in flat state.  In this embodiment, if\nan external force is applied to the flexible display unit 251, the flexible display unit 251 may transition to the second state such that the flexible display unit is deformed into the flat state (or a less curved state) or into a more curved state.\n If desired, the flexible display unit 251 may implement a flexible touch screen using a touch sensor in combination with the display.  If a touch is received at the flexible touch screen, the controller 180 can execute certain control\ncorresponding to the touch input.  In general, the flexible touch screen is configured to sense touch and other input while in both the first and second states.\n One option is to configure the mobile terminal 200 to include a deformation sensor which senses the deforming of the flexible display unit 251.  The deformation sensor may be included in the sensing unit 140.\n The deformation sensor may be located in the flexible display unit 251 or the case 201 to sense information related to the deforming of the flexible display unit 251.  Examples of such information related to the deforming of the flexible display\nunit 251 may be a deformed direction, a deformed degree, a deformed position, a deformed amount of time, an acceleration that the deformed flexible display unit 251 is restored, and the like.  Other possibilities include most any type of information\nwhich can be sensed in response to the curving of the flexible display unit or sensed while the flexible display unit 251 is transitioning into, or existing in, the first and second states.\n In some embodiments, controller 180 or other component can change information displayed on the flexible display unit 251, or generate a control signal for controlling a function of the mobile terminal 200, based on the information related to the\ndeforming of the flexible display unit 251.  Such information is typically sensed by the deformation sensor.\n The mobile terminal 200 is shown having a case 201 for accommodating the flexible display unit 251.  The case 201 can be deformable together with the flexible display unit 251, taking into account the characteristics of the flexible display unit\n251.\n A battery (not shown in this figure) located in the mobile terminal 200 may also be deformable in cooperation with the flexible display unit 261, taking into account the characteristic of the flexible display unit 251.  One technique to\nimplement such a battery is to use a stack and folding method of stacking battery cells.\n The deformation of the flexible display unit 251 not limited to perform by an external force.  For example, the flexible display unit 251 can be deformed into the second state from the first state by a user command, application command, or the\nlike.\n In accordance with still further embodiments, a mobile terminal may be configured as a device which is wearable on a human body.  Such devices go beyond the usual technique of a user grasping the mobile terminal using their hand.  Examples of\nthe wearable device include a smart watch, a smart glass, a head mounted display (HMD), and the like.\n A typical wearable device can exchange data with (or cooperate with) another mobile terminal 100.  In such a device, the wearable device generally has functionality that is less than the cooperating mobile terminal.  For instance, the\nshort-range communication module 114 of a mobile terminal 100 may sense or recognize a wearable device that is near-enough to communicate with the mobile terminal.  In addition, if the sensed wearable device is a device which is authenticated to\ncommunicate with the mobile terminal 100, the controller 180 may transmit data processed in the mobile terminal 100 to the wearable device via the short-range communication module 114, for example.  Hence, a user of the wearable device can use the data\nprocessed in the mobile terminal 100 on the wearable device.  For example, if a call is received in the mobile terminal 100, the user can answer the call using the wearable device.  Also, if a message is received in the mobile terminal 100, the user can\ncheck the received message using the wearable device.\n FIG. 3 is a perspective view illustrating one example of a watch-type mobile terminal 300 in accordance with another exemplary embodiment.  As illustrated in FIG. 3, the watch-type mobile terminal 300 includes a main body 301 with a display unit\n351 and a band 302 connected to the main body 301 to be wearable on a wrist.  In general, mobile terminal 300 may be configured to include features that are the same or similar to that of mobile terminal 100 of FIGS. 1A-1C.\n The main body 301 may include a case having a certain appearance.  As illustrated, the case may include a first case 301a and a second case 301b cooperatively defining an inner space for accommodating various electronic components.  Other\nconfigurations are possible.  For instance, a single case may alternatively be implemented, with such a case being configured to define the inner space, thereby implementing a mobile terminal 300 with a uni-body.\n The watch-type mobile terminal 300 can perform wireless communication, and an antenna for the wireless communication can be installed in the main body 301.  The antenna may extend its function using the case.  For example, a case including a\nconductive material may be electrically connected to the antenna to extend a ground area or a radiation area.\n The display unit 351 is shown located at the front side of the main body 301 so that displayed information is viewable to a user.  In some embodiments, the display unit 351 includes a touch sensor so that the display unit can function as a touch\nscreen.  As illustrated, window 351a is positioned on the first case 301a to form a front surface of the terminal body together with the first case 301a.\n The illustrated embodiment includes audio output module 352, a camera 321, a microphone 322, and a user input unit 323 positioned on the main body 301.  If the display unit 351 is implemented as a touch screen, additional function keys may be\nminimized or eliminated.  For example, if the touch screen is implemented, the user input unit 323 may be omitted.\n The band 302 is commonly worn on the user's wrist and may be made of a flexible material for facilitating wearing of the device.  As one example, the band 302 may be made of fur, rubber, silicon, synthetic resin, or the like.  The band 302 may\nalso be configured to be detachable from the main body 301.  Accordingly, the band 302 may be replaceable with various types of bands according to a user's preference.\n In one configuration, the band 302 may be used for extending the performance of the antenna.  For example, the band may include therein a ground extending portion (not shown) electrically connected to the antenna to extend a ground area.\n The band 302 may include fastener 302a.  The fastener 302a may be implemented into a buckle type, a snap-fit hook structure, a Velcro.RTM.  type, or the like, and include a flexible section or material.  The drawing illustrates an example that\nthe fastener 302a is implemented using a buckle.\n FIG. 4 is a perspective view illustrating one example of a glass-type mobile terminal 400 according to another exemplary embodiment.  The glass-type mobile terminal 400 can be wearable on a head of a human body and provided with a frame (case,\nhousing, etc.) therefor.  The frame may be made of a flexible material to be easily worn.  The frame of mobile terminal 400 is shown having a first frame 401 and a second frame 402, which can be made of the same or different materials.  In general,\nmobile terminal 400 may be configured to include features that are the same or similar to that of mobile terminal 100 of FIGS. 1A-1C.\n The frame may be supported on the head and defines a space for mounting various components.  As illustrated, electronic components, such as a control module 480, an audio output module 452, and the like, may be mounted to the frame part.  Also,\na lens 403 for covering either or both of the left and right eyes may be detachably coupled to the frame part.\n The control module 480 controls various electronic components disposed in the mobile terminal 400.  The control module 480 may be understood as a component corresponding to the aforementioned controller 180.  FIG. 4 illustrates that the control\nmodule 480 is installed in the frame part on one side of the head, but other locations are possible.\n The display unit 451 may be implemented as a head mounted display (HMD).  The HMD refers to display techniques by which a display is mounted to a head to show an image directly in front of a user's eyes.  In order to provide an image directly in\nfront of the user's eyes if the user wears the glass-type mobile terminal 400, the display unit 451 may be located to correspond to either or both of the left and right eyes.  FIG. 4 illustrates that the display unit 451 is located on a portion\ncorresponding to the right eye to output an image viewable by the user's right eye.\n The display unit 451 may project an image into the user's eye using a prism.  Also, the prism may be formed from optically transparent material such that the user can view both the projected image and a general visual field (a range that the\nuser views through the eyes) in front of the user.\n In such a manner, the image output through the display unit 451 may be viewed while overlapping with the general visual field.  The mobile terminal 400 may provide an augmented reality (AR) by overlaying a virtual image on a realistic image or\nbackground using the display.\n The camera 421 may be located adjacent to either or both of the left and right eyes to capture an image.  Since the camera 421 is located adjacent to the eye, the camera 421 can acquire a scene that the user is currently viewing.  The camera 421\nmay be positioned at most any location of the mobile terminal.  In some embodiments, multiple cameras 421 may be utilized.  Such multiple cameras 421 may be used to acquire a stereoscopic image.\n The glass-type mobile terminal 400 may include user input units 423a and 423b, which can each be manipulated by the user to provide an input.  The user input units 423a and 423b may employ techniques which permit input via a tactile input. \nTypical tactile inputs include a touch, push, or the like.  The user input units 423a and 423b are shown operable in a pushing manner and a touching manner as they are located on the frame part and the control module 480, respectively.\n If desired, mobile terminal 400 may include a microphone which processes input sound into electric audio data, and an audio output module 452 for outputting audio.  The audio output module 452 may be configured to produce audio in a general\naudio output manner or an osteoconductive manner.  If the audio output module 452 is implemented in the osteoconductive manner, the audio output module 452 may be closely adhered to the head if the user wears the mobile terminal 400 and vibrate the\nuser's skull to transfer sounds.\n A communication system which is operable with the variously described mobile terminals will now be described in more detail.  Such a communication system may be configured to utilize any of a variety of different air interfaces and/or physical\nlayers.  Examples of such air interfaces utilized by the communication system include Frequency Division Multiple Access (FDMA), Time Division Multiple Access (TDMA), Code Division Multiple Access (CDMA), Universal Mobile Telecommunications System (UMTS)\n(including, Long Term Evolution (LTE), LTE-A (Long Term Evolution-Advanced)), Global System for Mobile Communications (GSM), and the like.\n By way of a non-limiting example only, further description will relate to a CDMA communication system, but such teachings apply equally to other system types including a CDMA wireless communication system as well as OFDM (Orthogonal Frequency\nDivision Multiplexing) wireless communication system.  A CDMA wireless communication system generally includes one or more mobile terminals (MT or User Equipment, UE) 100, one or more base stations (BSs, NodeB, or evolved NodeB), one or more base station\ncontrollers (BSCs), and a mobile switching center (MSC).  The MSC is configured to interface with a conventional Public Switched Telephone Network (PSTN) and the BSCs.  The BSCs are coupled to the base stations via backhaul lines.  The backhaul lines may\nbe configured in accordance with any of several known interfaces including, for example, E1/T1, ATM, IP, PPP, Frame Relay, HDSL, ADSL, or xDSL.  Hence, the plurality of BSCs can be included in the CDMA wireless communication system.\n Each base station may include one or more sectors, each sector having an omni-directional antenna or an antenna pointed in a particular direction radially away from the base station.  Alternatively, each sector may include two or more different\nantennas.  Each base station may be configured to support a plurality of frequency assignments, with each frequency assignment having a particular spectrum (e.g., 1.25 MHz, 5 MHz, etc.).\n The intersection of sector and frequency assignment may be referred to as a CDMA channel.  The base stations may also be referred to as Base Station Transceiver Subsystems (BTSs).  In some cases, the term \"base station\" may be used to refer\ncollectively to a BSC, and one or more base stations.  The base stations may also be denoted as \"cell sites.\" Alternatively, individual sectors of a given base station may be referred to as cell sites.\n A broadcasting transmitter (BT) transmits a broadcast signal to the mobile terminals 100 operating within the system.  The broadcast receiving module 111 of FIG. 1A is typically configured inside the mobile terminal 100 to receive broadcast\nsignals transmitted by the BT.\n Global Positioning System (GPS) satellites for locating the position of the mobile terminal 100, for example, may cooperate with the CDMA wireless communication system.  Useful position information may be obtained with greater or fewer\nsatellites than two satellites.  It is to be appreciated that other types of position detection technology, (i.e., location technology that may be used in addition to or instead of GPS location technology) may alternatively be implemented.  If desired,\nat least one of the GPS satellites may alternatively or additionally be configured to provide satellite DMB transmissions.\n The location information module 115 is generally configured to detect, calculate, or otherwise identify a position of the mobile terminal.  As an example, the location information module 115 may include a Global Position System (GPS) module, a\nWi-Fi module, or both.  If desired, the location information module 115 may alternatively or additionally function with any of the other modules of the wireless communication unit 110 to obtain data related to the position of the mobile terminal.\n A typical GPS module 115 can measure an accurate time and distance from three or more satellites, and accurately calculate a current location of the mobile terminal according to trigonometry based on the measured time and distances.  A method of\nacquiring distance and time information from three satellites and performing error correction with a single satellite may be used.  In particular, the GPS module may acquire an accurate time together with three-dimensional speed information as well as\nthe location of the latitude, longitude and altitude values from the location information received from the satellites.\n Furthermore, the GPS module can acquire speed information in real time to calculate a current position.  Sometimes, accuracy of a measured position may be compromised if the mobile terminal is located in a blind spot of satellite signals, such\nas being located in an indoor space.  In order to minimize the effect of such blind spots, an alternative or supplemental location technique, such as Wi-Fi Positioning System (WPS), may be utilized.\n The Wi-Fi positioning system (WPS) refers to a location determination technology based on a wireless local area network (WLAN) using Wi-Fi as a technology for tracking the location of the mobile terminal 100.  This technology typically includes\nthe use of a Wi-Fi module in the mobile terminal 100 and a wireless access point for communicating with the Wi-Fi module.\n The Wi-Fi positioning system may include a Wi-Fi location determination server, a mobile terminal, a wireless access point (AP) connected to the mobile terminal, and a database stored with wireless AP information.\n The mobile terminal connected to the wireless AP may transmit a location information request message to the Wi-Fi location determination server.  The Wi-Fi location determination server extracts the information of the wireless AP connected to\nthe mobile terminal 100, based on the location information request message (or signal) of the mobile terminal 100.  The information of the wireless AP may be transmitted to the Wi-Fi location determination server through the mobile terminal 100, or may\nbe transmitted to the Wi-Fi location determination server from the wireless AP.\n The information of the wireless AP extracted based on the location information request message of the mobile terminal 100 may include one or more of media access control (MAC) address, service set identification (SSID), received signal strength\nindicator (RSSI), reference signal received Power (RSRP), reference signal received quality (RSRQ), channel information, privacy, network type, signal strength, noise strength, and the like.\n The Wi-Fi location determination server may receive the information of the wireless AP connected to the mobile terminal 100 as described above, and may extract wireless AP information corresponding to the wireless AP connected to the mobile\nterminal from the pre-established database.  The information of any wireless APs stored in the database may be information such as MAC address, SSID, RSSI, channel information, privacy, network type, latitude and longitude coordinate, building at which\nthe wireless AP is located, floor number, detailed indoor location information (GPS coordinate available), AP owner's address, phone number, and the like.  In order to remove wireless APs provided using a mobile AP or an illegal MAC address during a\nlocation determining process, the Wi-Fi location determination server may extract only a predetermined number of wireless AP information in order of high RSSI.\n Then, the Wi-Fi location determination server may extract (analyze) location information of the mobile terminal 100 using at least one wireless AP information extracted from the database.\n A method for extracting (analyzing) location information of the mobile terminal 100 may include a Cell-ID method, a fingerprint method, a trigonometry method, a landmark method, and the like.\n The Cell-ID method is used to determine a position of a wireless AP having the largest signal strength, among peripheral wireless AP information collected by a mobile terminal, as a position of the mobile terminal.  The Cell-ID method is an\nimplementation that is minimally complex, does not require additional costs, and location information can be rapidly acquired.  However, in the Cell-ID method, the precision of positioning may fall below a desired threshold if the installation density of\nwireless APs is low.\n The fingerprint method is used to collect signal strength information by selecting a reference position from a service area, and to track a position of a mobile terminal using the signal strength information transmitted from the mobile terminal\nbased on the collected information.  In order to use the fingerprint method, it is common for the characteristics of radio signals to be pre-stored in the form of a database.\n The trigonometry method is used to calculate a position of a mobile terminal based on a distance between coordinates of at least three wireless APs and the mobile terminal.  In order to measure the distance between the mobile terminal and the\nwireless APs, signal strength may be converted into distance information, Time of Arrival (ToA), Time Difference of Arrival (TDoA), Angle of Arrival (AoA), or the like may be taken for transmitted wireless signals.\n The landmark method is used to measure a position of a mobile terminal using a known landmark transmitter.\n In addition to these position location methods, various algorithms may be used to extract (analyze) location information of a mobile terminal.  Such extracted location information may be transmitted to the mobile terminal 100 through the Wi-Fi\nlocation determination server, thereby acquiring location information of the mobile terminal 100.\n The mobile terminal 100 can acquire location information by being connected to at least one wireless AP.  The number of wireless APs required to acquire location information of the mobile terminal 100 may be variously changed according to a\nwireless communication environment within which the mobile terminal 100 is positioned.\n As previously described with regard to FIG. 1A, the mobile terminal may be configured to include short-range communication techniques such as Bluetooth.TM., Radio Frequency Identification (RFID), Infrared Data Association (IrDA), UltraWideband\n(UWB), ZigBee, Near Field Communication (NFC), Wireless USB (Wireless Universal Serial Bus), and the like.\n A typical NFC module provided at the mobile terminal supports short-range wireless communication, which is a non-contactable type of communication between mobile terminals and generally occurs within about 10 cm.  The NFC module may operate in\none of a card mode, a reader mode, or a P2P mode.  The mobile terminal 100 may further include a security module for storing card information, in order to operate the NFC module in a card mode.  The security module may be a physical medium such as\nUniversal Integrated Circuit Card (UICC) (e.g., a Subscriber Identification Module (SIM) or Universal SIM (USIM)), a secure micro SD and a sticker, or a logical medium (e.g., embedded Secure Element (SE)) embedded in the mobile terminal.  Single Wire\nProtocol (SWP)-based data exchange may be performed between the NFC module and the security module.\n In a case where the NFC module operates in a card mode, the mobile terminal may transmit card information on a general IC card to the outside.  More specifically, if a mobile terminal having card information on a payment card (e. g, a credit\ncard or a bus card) approaches a card reader, a short-range mobile payment may be executed.  As another example, if a mobile terminal which stores card information on an entrance card approaches an entrance card reader, an entrance approval procedure may\nstart.  A card such as a credit card, a traffic card, or an entrance card may be included in the security module in the form of applet, and the security module may store card information on the card mounted therein.  Card information for a payment card\nmay include any of a card number, a remaining amount and usage history, and the like.  Card information of an entrance card may include any of a user's name, a user's number (e.g., undergraduate number or staff number), an entrance history, and the like.\n If the NFC module operates in a reader mode, the mobile terminal can read data from an external tag.  The data received from the external tag by the mobile terminal may be coded into the NFC Data Exchange Format defined by the NFC Forum.  The\nNFC Forum generally defines four record types.  More specifically, the NFC Forum defines four Record Type Definitions (RTDs) such as smart poster, text, Uniform Resource Identifier (URI), and general control.  If the data received from the external tag\nis a smart poster type, the controller may execute a browser (e.g., Internet browser).  If the data received from the external tag is a text type, the controller may execute a text viewer.  If the data received from the external tag is a URI type, the\ncontroller may execute a browser or originate a call.  If the data received from the external tag is a general control type, the controller may execute a proper operation according to control content.\n In some cases in which the NFC module operates in a P2P (Peer-to-Peer) mode, the mobile terminal can execute P2P communication with another mobile terminal.  In this case, Logical Link Control Protocol (LLCP) may be applied to the P2P\ncommunication.  For P2P communication, connection may be generated between the mobile terminal and another mobile terminal.  This connection may be categorized as a connectionless mode which ends after one packet is switched, and a connection-oriented\nmode in which packets are switched consecutively.  For a typical P2P communication, data such as an electronic type name card, address information, a digital photo and a URL, a setup parameter for Bluetooth connection, Wi-Fi connection, etc. may be\nswitched.  The P2P mode can be effectively utilized in switching data of a small capacity, because an available distance for NFC communication is relatively short.\n Further preferred embodiments will be described in more detail with reference to additional drawing figures.  It is understood by those skilled in the art that the present features can be embodied in several forms without departing from the\ncharacteristics thereof.\n Hereinafter, a method of operating an artificial intelligence system according to an embodiment of the present invention will be described.\n Assume that the artificial intelligence system includes a mobile terminal 100 and an artificial intelligence device 500.\n The artificial intelligence device 500 may include all the components of the mobile terminal 100 described with reference to FIGS. 1a to 1c.\n FIG. 5 is a ladder diagram illustrating a method of operating a mobile terminal according to an embodiment of the present invention.\n In particular, FIG. 5 shows an embodiment in which the mobile terminal 100 senses that the mobile terminal 100 is lost based on information received from the artificial intelligence device 500 and switches an operation mode of the mobile\nterminal 100 to a loss mode.\n Referring to FIG. 5, the wireless communication unit 110 of the mobile terminal 100 receives context information of the mobile terminal 100 from the artificial intelligence device 500 (S501).\n The wireless communication unit 110 may periodically receive signals from the artificial intelligence device 500.  The received signals may include the context information of the mobile terminal 100.\n In one embodiment, the context information of the mobile terminal 100 may include information indicating the loss state of the mobile terminal 100.\n In one embodiment, the artificial intelligence device 500 may receive a voice command of a user and acquire loss information based on the received voice command.  For example, if voice \"My mobile phone was lost\" is received, the artificial\nintelligence device 500 may acquire the received voice as loss information.  The above voice is merely exemplary and voice related to loss of the mobile terminal 100, such as \"Please find my mobile phone\", may be used.\n The artificial intelligence device 500 may determine whether a speaker of the received voice corresponds to the user of the mobile terminal 100 or not.  The artificial intelligence device 500 may determine whether the received voice is\npre-registered user voice using one or more of the tone and pattern of the received voice.\n To this end, the artificial intelligence device 500 may store the tone and pattern of the pre-registered user voice.\n The artificial intelligence device 500 may transmit loss information indicating that the mobile terminal 100 has been lost to the mobile terminal 100, if the received voice is the pre-registered user voice.\n As another example, if voice \"My mobile phone was lost\" is received, the artificial intelligence device 500 may transmit the voice to the mobile terminal 100 without an authentication process.\n In one embodiment, the context information of the mobile terminal 100 may include a text message received from the artificial intelligence device 500 or another mobile terminal.\n The artificial intelligence unit 130 of the mobile terminal 100 determines whether the mobile terminal 100 is lost, based on the received context information of the mobile terminal 100 (S503).\n In one embodiment, the artificial intelligence unit 130 may continuously collect the received context information and determine whether the mobile terminal 100 is lost based on the result of collection.\n For example, the artificial intelligence unit 130 may recognize that the mobile terminal 100 is lost, if the voice received from the artificial intelligence device 500 is the voice of the user of the mobile terminal 100 and indicates the loss\nstate of the mobile terminal.\n As another example, the artificial intelligence unit 130 may sense the loss state of the mobile terminal 100 based on the text message received from another mobile terminal.  More specifically, the artificial intelligence unit 130 may recognize\nthe loss state of the mobile terminal 100, if the user of another mobile terminal is pre-registered and the text message indicates that the mobile terminal is lost.\n The text message may include information indicating the loss state of the mobile terminal \"My mobile phone was lost.  Please contact this phone number.  010-XXXX-XXXX\".\n The artificial intelligence unit 130 may recognize the loss state of the mobile terminal 100, if the text included in the text message indicates that the mobile terminal 100 is currently lost.\n Although the text message is described herein, the present invention is not limited thereto and is applicable to a message received through a message application.\n The artificial intelligence unit 130 of the mobile terminal 100 switches the operation mode of the mobile terminal 100 to a loss mode, upon determining that the mobile terminal 100 is lost (S505).\n The operation mode of the mobile terminal 100 may include a normal mode and a loss mode.\n In one embodiment, in the loss mode of the mobile terminal 100, since the mobile terminal 100 is lost, use of the mobile terminal 100 may be restricted.\n In one embodiment, the artificial intelligence unit 130 may maintain the lock state of the mobile terminal 100 in the loss mode of the mobile terminal 100.  The lock state of the mobile terminal 100 means a state for restricting use of the\nmobile terminal 100 and the artificial intelligence unit 130 may maintain display of the lock screen of the mobile terminal 100.\n That is, the artificial intelligence unit 130 may maintain display of the lock screen, even if a touch pattern, fingerprint or password input for releasing the lock state matches a pre-stored touch pattern, fingerprint or password.\n In another embodiment, the loss mode of the mobile terminal 100 may refer to a mode for blocking a specific function of the mobile terminal 100.  The specific function may include one or more of a function for blocking use of data necessary to\naccess a server through an application, a function for blocking a voice call and a function for blocking a video call.\n Therefore, use of the mobile phone by another person which may incur expenses may be restricted if the mobile terminal 100 is lost.\n In another embodiment, the artificial intelligence unit 130 may change the power supply state of the mobile terminal 100 in the loss mode of the mobile terminal 100.  For example, the artificial intelligence unit 130 turns the mobile terminal\n100 off in the loss mode of the mobile terminal 100.\n As another example, the artificial intelligence unit 130 may change the power supply state of the mobile terminal 100 to a power saving state in which the mobile terminal 100 uses minimum power, in the loss mode of the mobile terminal 100.\n The power saving state of the mobile terminal 100 may indicate that power for executing a currently executed application is applied or the brightness of the screen of the display unit 151 is minimally maintained.\n In another embodiment, the artificial intelligence unit 130 may transmit a notice including the loss state of the mobile terminal 100 and the current position of the mobile terminal 100 to the pre-registered mobile terminal of another person, if\nthe mobile terminal 100 enters the loss mode.\n In another embodiment, the artificial intelligence unit 130 may activate the first camera 121a if power-off of the mobile terminal 100 is sensed through a power button in the loss mode of the mobile terminal 100.  The artificial intelligence\nunit 130 may capture the image of another person who finds the mobile terminal 100 through the first camera 121a and transmit the captured image of another user to the mobile terminal of a protector of the user who lost the mobile terminal 500 or the\nartificial intelligence device 500.\n The normal mode of the mobile terminal 100 may refer to an operation mode other than the loss mode of the mobile terminal 100.\n The artificial intelligence unit 130 may directly change the operation mode of the mobile terminal 100 or instruct the controller 180 to change the operation mode of the mobile terminal 100, upon determining that the mobile terminal 100 is lost.\n For example, the artificial intelligence unit 130 may generate a control signal for switching the operation mode of the mobile terminal 100 to the loss mode, upon determining that the mobile terminal 100 is lost.  The artificial intelligence\nunit 130 may transmit the generated control signal to the controller 180.  The controller 180 may switch the operation mode of the mobile terminal 100 from the normal mode to the loss mode according to the control signal received from the artificial\nintelligence unit 130.\n As another example, the artificial intelligence unit 130 may autonomously switch the operation mode of the mobile terminal 100 from the normal mode to the loss mode, upon determining that the mobile terminal 100 is lost.\n Meanwhile, the artificial intelligence unit 130 maintains the operation mode of the mobile terminal 100 in the normal mode, upon determining that the mobile terminal 100 is not lost (S507).\n Hereinafter, the embodiment of FIG. 5 will be described in detail.\n FIGS. 6 and 7 are diagrams illustrating a method of recognizing the loss state of a mobile terminal based on information received from the artificial intelligence device capable of performing communication with the mobile terminal according to\nan embodiment of the present invention.\n Referring to FIG. 6, the artificial intelligence device 500 may receive voice from the user of the mobile terminal 100.\n The artificial intelligence device 500 may transmit the received voice to the mobile terminal 100.\n In one embodiment, the artificial intelligence device 500 may transmit the received voice to the mobile terminal 100, if the received voice includes a command for transmission to the mobile terminal 100.  For example, if the received voice\nincludes \"I lost my mobile phone.  Please transmit this information to my mobile phone\", since the command \"Please transmit this information to my mobile phone\" is included, the artificial intelligence device 500 may transmit the voice to the mobile\nterminal 100.\n In another embodiment, the artificial intelligence device 500 may transmit automatically received voice to the mobile terminal 100 if the received voice includes text related to the loss state of the mobile terminal 100.  For example, if the\nreceived voice includes \"I lost my mobile phone\", the artificial intelligence device 500 may recognize information related to the loss state of the mobile terminal 100 from the text \"mobile phone\" and \"lost\" included in the received voice and transmit\nuser voice to the mobile terminal 100.\n The artificial intelligence unit 130 of the mobile terminal 100 may recognize the loss state of the mobile terminal 100 based on the context information of the mobile terminal 100 received from the artificial intelligence device 500.\n For example, if the context information of the mobile terminal 100 received from the artificial intelligence device 500 includes voice, the artificial intelligence unit 130 may analyze one or more of the waveform and tone of the voice to\ndetermine whether the voice is the voice of the user of the mobile terminal.\n The artificial intelligence unit 130 may extract features from the waveform of the voice.  The features may be reference points used to distinguish the voice of the user of the mobile terminal 100 from voices of the other users.\n The artificial intelligence unit 130 may determine that the received voice is the voice of the user of the mobile terminal 100, if the features extracted from the received voice match a plurality of pre-stored features by a predetermined ratio.\n The artificial intelligence unit 130 may recognize the loss state of the mobile terminal 100, if the voice is pre-registered and the voice includes the loss state of the mobile terminal 100.\n The artificial intelligence unit 130 may automatically switch the operation mode of the mobile terminal 100 from the normal mode to the loss mode, upon recognizing that the mobile terminal 100 is lost.\n Next, FIG. 7 will be described.\n By referring to FIG. 7, the mobile terminal 100 may receive a text message 700 from another mobile terminal 501.\n The artificial intelligence unit 130 of the mobile terminal 100 may recognize that the mobile terminal 100 is lost, if the destination of the text message 700 is a pre-registered person and the text message 700 indicates the loss state of the\nmobile terminal.\n The artificial intelligence unit 130 may recognize that the mobile terminal 100 is lost, if the text message 700 is received from a pre-registered person B and the sentence included in the text message 700 includes information indicating the\nloss state of the mobile terminal 100.\n The artificial intelligence unit 130 may extract a keyword included in the text message 700 to recognize the loss state of the mobile terminal 100.\n For example, the artificial intelligence unit 130 may extract keywords \"mobile phone\" and \"loss\" included in the text message 700 and recognize the loss state of the mobile terminal 100.\n The artificial intelligence unit 130 may set the operation mode of the mobile terminal 100 to the loss mode, upon recognizing the loss state of the mobile terminal 100.\n Next, a process of automatically recognizing and coping with the non-possession or loss state of the mobile terminal 100 will be described.\n FIGS. 8 and 9 are diagrams illustrating operation performed in a loss mode of a mobile terminal according to an embodiment of the present invention.\n First, FIG. 8 will be described.\n The artificial intelligence unit 130 may switch the operation mode of the mobile terminal 100 to the loss mode, upon determining that the mobile terminal 100 is lost.\n The artificial intelligence unit 130 may lock personal information to prevent the other persons from using the personal information stored in the mobile terminal 100 in the loss mode of the mobile terminal 100.\n For example, the artificial intelligence unit 130 may set a contact application including contact information to lock state.  That is, the artificial intelligence unit 130 may make a request for inputting a password for execution of the contact\napplication, if a request for executing the contact application is received.  Although the contact application is described herein, the present invention is not limited thereto and is similarly applicable to an application including personal information\nsuch as a social networking service application, a messaging application, or a gallery application for providing a video/image.\n As another example, the artificial intelligence unit 130 may automatically set a lock function of an initial screen if a lock function is not set on the initial screen for accessing the home screen of the mobile terminal 100.  The artificial\nintelligence unit 130 may change the mobile terminal 100 to the lock state using one of a touch pattern and password for unlocking, which has been used by the user.\n As another example, the artificial intelligence unit 130 may block use of data for accessing an external server in the loss mode of the mobile terminal 100.\n As another example, the artificial intelligence unit 130 may change the power mode of the mobile terminal 100 to the power saving mode in the loss mode of the mobile terminal 100.\n To this end, in the loss mode of the mobile terminal 100, personal information can be prevented from being leaked, expenses incurred due to data use can be reduced, and the power of the mobile terminal can be maintained for a long time in order\nto find the mobile terminal 100.\n Next, FIG. 9 will be described.\n Referring to FIG. 9, the artificial intelligence unit 130 may transmit loss information indicating the loss state of the mobile terminal 100 to another mobile terminal 501 in the loss mode of the mobile terminal 100.\n The other mobile terminal 501 may be the mobile terminal of the family member of the user included in the contact information stored in the mobile terminal 100.\n The other mobile terminal 501 may display the loss information received from the mobile terminal 100.\n The loss information may include text indicating the loss state of the mobile terminal 100, a point of time when the mobile terminal 100 is lost, and the position information of the mobile terminal 100.\n The position information of the mobile terminal 100 may include the address and map of a place where the mobile terminal 100 is located at the point of time when the mobile terminal 100 is lost.\n The artificial intelligence unit 130 may collect a point of time when the loss state of the mobile terminal 100 is sensed and the loss information of the position of the mobile terminal 100 at a point of time when the loss state of the mobile\nterminal is sensed.\n The artificial intelligence unit 130 may control the wireless communication unit 110 to transmit the collected loss information to the other mobile terminal 501.  The other mobile terminal 501 may be the mobile terminal of another person who\nmost frequently contacts the user of the mobile terminal 100 within a week.\n Thus, the person close to the user who has lost the mobile terminal 100 may rapidly recognize and cope with the loss state of the mobile terminal 100.\n Next, an example in which the mobile terminal 100 autonomously senses loss thereof will be described.\n FIG. 10 is a ladder diagram illustrating a method of operating an artificial intelligence system according to another embodiment of the present invention.\n In FIG. 10, portions equal to those of FIG. 5 will be omitted.\n The artificial intelligence unit 130 of the mobile terminal 100 collects the context information of the mobile terminal 100 (S1001).\n The artificial intelligence unit 130 determines whether the mobile terminal 100 is lost based on the collected context information (S1003).\n In one embodiment, the context information of the mobile terminal 100 may include one or more of the position pattern and user information of the mobile terminal 100.\n The memory 170 may pre-store the position pattern of the mobile terminal 100.\n The artificial intelligence unit 130 may collect places where the mobile terminal 100 is located during a predetermined period.  The predetermined period may be a week, but this is merely an example.\n The artificial intelligence unit 130 may collect the positions of the mobile terminal 100 acquired by the position information module 115 during the predetermined period and generate the position pattern using the collected positions.\n The position pattern may indicate the movement path of the mobile terminal 100 obtained by connecting the collected positions by lines.\n The user information of the mobile terminal 100 may include one or more of the facial image and voice of the user.\n The artificial intelligence unit 130 may periodically activate the first camera 121a and capture the image of the front side of the mobile terminal 100.\n The artificial intelligence unit 130 may check whether the face of the user who is the owner of the mobile terminal 100 is included in the images captured for the predetermine period.\n The artificial intelligence unit 130 may periodically collect voice input through the microphone 122.\n The artificial intelligence unit 130 may store the tone and voice pattern of the user who is the owner of the mobile terminal 100.\n The artificial intelligence unit 130 may determine whether the voice of the user of the mobile terminal 100 is not present in the collected voce for the predetermined period.\n The artificial intelligence unit 130 may determine that the mobile terminal 100 is lost, if the position pattern of the mobile terminal 100 is different from a pre-stored position pattern and the facial image or voice of the user is not acquired\nfor a predetermined time.\n The artificial intelligence unit 130 switches the operation mode of the mobile terminal 100 to the loss mode, upon determining that the mobile terminal 100 is lost (S1005).\n The artificial intelligence unit 130 transmits loss information indicating the loss state of the mobile terminal 100 to another mobile terminal 501 (S1007).\n In one embodiment, the loss information may include a point of time when loss of the mobile terminal 100 is sensed (or a point of time when the mobile terminal is lost) and the position of the mobile terminal 100 at the point of time when the\nmobile terminal 100 is lost.\n The user of the other mobile terminal 501 may be an acquaintance pre-registered by the user of the mobile terminal 100 or a family member or friend of the user of the mobile terminal 100, which is autonomously set by the artificial intelligence\nunit 130 using contact information.\n Meanwhile, the artificial intelligence unit 130 may maintain the operation of the mobile terminal 100 in the normal mode, upon determining that the mobile terminal is not lost (S1009).\n FIG. 11 is a diagram illustrating a process of sensing the loss state of a mobile terminal and transmitting loss information of the mobile terminal to another mobile terminal.\n Referring to FIG. 11a, the artificial intelligence unit 130 may compare the pre-stored position pattern 1103 of the mobile terminal 100 with a current position pattern 1105.\n In addition, the artificial intelligence unit 130 may periodically collect images captured through the first camera 121a.  The artificial intelligence unit 130 may determine whether the face of the user is included in the images collected during\nthe predetermined period.\n The artificial intelligence unit 130 may determine that the mobile terminal 100 is lost if the current position pattern 1105 of the mobile terminal 100 does not match the pre-stored position pattern and the face of the user is not included in\nthe images collected during the predetermined period.\n The artificial intelligence unit 130 may determine that the mobile terminal 100 is lost, if the current position pattern 1105 of the mobile terminal 100 does not match the pre-stored position pattern by a predetermined ratio and the face of the\nuser is not included in the images collected during the predetermined period.\n The artificial intelligence unit 130 may switch the operation of the mobile terminal 100 to the loss mode.\n The artificial intelligence unit 130 may display a power off screen 1110 for turning the mobile terminal 100 off, if a command for turning the mobile terminal 100 off is received in the loss mode of the mobile terminal 100.\n The artificial intelligence unit 130 may not turn the mobile terminal 100 off while displaying the power off screen 1110, in order to track the finder 1101 of the mobile terminal 100.\n The artificial intelligence unit 130 may change the power mode of the mobile terminal 100 to the power saving mode while displaying the power off screen 1110.\n Thereafter, the artificial intelligence unit 130 may acquire the photo of the finder 1101 captured through the first camera 121a if the mobile terminal 100 is lost.\n The artificial intelligence unit 130 may transmit loss information 1130 including the photo 1131 of the finder 1101, the point of time when the mobile terminal 100 is lost, and the position information 1133 of the mobile terminal to the other\nmobile terminal 501.\n The other mobile terminal 501 may display the loss information 1130 received from the mobile terminal 100 through the display unit 151.\n To this end, the owner of the mobile terminal 100 may rapidly check the loss state of the mobile terminal 100 using the loss information received by the other mobile terminal 501.\n Next, operation performed if the user of the mobile terminal 100 recognizes non-possession of the mobile terminal 100 will be described.\n FIGS. 12a to 12c are diagrams illustrating an example of recognizing non-possession of a mobile terminal and automatically transmitting a notice to another mobile terminal according to an embodiment of the present invention.\n The artificial intelligence unit 130 may compare the pre-stored use pattern of the mobile terminal 100 with a current use pattern.\n The artificial intelligence unit 130 may determine that the user does not possess the mobile terminal 100, if the pre-stored use pattern of the mobile terminal 100 does not match the current use pattern.\n The pre-stored use pattern may include one or more of whether the display unit 151 is turned on or off and execution of an application installed in the mobile terminal 100.\n The artificial intelligence unit 130 may determine that the user does not possess the mobile terminal 100, if a command for turning the display unit 151 on or off is not received for a predetermined period.\n The artificial intelligence unit 130 may determine that the user does not possess the mobile terminal 100, if a command for executing the application is not received for a predetermined period.\n The artificial intelligence unit 130 may determine that the user does not possess the mobile terminal 100, if the position of the mobile terminal 100 is fixed for a predetermined period.\n The artificial intelligence unit 130 may transmit information 1201 indicating that the user of the mobile terminal 100 does not possess the mobile terminal 100 to the other mobile terminal 100 of another person, as shown in FIG. 12a, if the user\ndoes not possess the mobile terminal 100.\n Here, another person may be a person who most frequently contacts the user of the mobile terminal 100 during a predetermined period, which is merely exemplary.\n As shown in FIG. 12b, the artificial intelligence unit 130 may display missed call information 1203, if an incoming call signal is received from another person in a state in which the user does not possess the mobile terminal 100. \nSimultaneously, the artificial intelligence unit 130 may additionally transmit the missed call information 1203 to the other mobile terminal 501.\n Therefore, the other mobile terminal 501 may display missed call notification information 1205 based on the received missed call information 1203.\n As shown in FIG. 12c, the artificial intelligence unit 130 may display missed call information 1211, if an incoming call signal is received from the mobile terminal 501 of another person in a state in which the user does not possess the mobile\nterminal 100.\n The artificial intelligence unit 130 may transmit non-possession information 1213 indicating that the user does not possess the mobile terminal 100 to the mobile terminal 501 of another person.\n Therefore, the counterpart may recognize that the owner of the mobile terminal 100 does not possess the mobile terminal 100.\n FIG. 13 is a diagram illustrating a process of performing automatic call forwarding to another phone number if an incoming call signal is received in a state in which a user does not possess a mobile terminal according to an embodiment of the\npresent invention.\n The artificial intelligence unit 130 may switch the power mode of the mobile terminal 100 to the power saving mode, upon sensing that the user does not possess the mobile terminal 100.  Therefore, it is possible to prevent unnecessary power\nwaste if the user does not possess the mobile terminal 100.\n The artificial intelligence unit 130 may perform call forwarding to a pre-registered phone number if an incoming call signal is received in a state in which the user does not possess the mobile terminal.\n Here, the pre-registered phone number is a phone number of a company of the user of the mobile terminal 100, which is merely exemplary.\n Therefore, the user may contact a caller even if the user does not possess the mobile terminal 100.\n FIG. 14 is a diagram illustrating a process of remotely controlling a mobile terminal in a state in which a user does not possess the mobile terminal according to an embodiment of the present invention.\n Referring to FIG. 14, the artificial intelligence unit 130 may receive a remote control command from the other mobile terminal 501.  The remote control command may be a command for enabling the other mobile terminal 501 to remotely control\noperation of the mobile terminal 100.\n Referring to FIG. 14, the artificial intelligence unit 130 of the unpossessed mobile terminal 100 may receive the remote control command from the other mobile terminal 501.\n The artificial intelligence unit 130 may determine whether the identification information of the other mobile terminal 501 included in the remote control command is pre-registered identification information.\n The artificial intelligence unit 130 may accept remote control if the identification information of the other mobile terminal 501 included in the remote control command is the pre-registered identification information, in order to prevent the\nnon-registered mobile terminal of another person from remotely controlling the mobile terminal 100 of the user.\n The artificial intelligence unit 130 may extract message summary information from a message window 1401 received by the mobile terminal 100, if the remote control command requests the message summary information of the message received by the\nmobile terminal 100.\n The artificial intelligence unit 130 may acquire, from the message window 1401, messages received from a counterpart and extract only the questions of the counterpart.\n The artificial intelligence unit 130 may transmit the message summary information including the questions of the counterpart to the other mobile terminal 501.\n The other mobile terminal 501 may display the received message summary information 1403.\n The other mobile terminal 501 may receive the voice response of the user, convert the voice response into text and transmit the converted text response 1405 to the mobile terminal 100.\n Therefore, the user may remotely control the mobile terminal 100 even in a state in which the user does not possess the mobile terminal 100 and thus conveniently respond to the message received by the mobile terminal 100.\n Although operation performed in a state in which the mobile terminal 100 is not possessed is described, the present invention is not limited thereto and is applicable to operation performed even if the mobile terminal 100 is lost.\n That is, the artificial intelligence unit 130 may automatically transmit a notice to the pre-registered mobile terminal 501 in the loss mode of the mobile terminal 100, as shown in FIGS. 12a to 12c.\n In addition, the artificial intelligence unit 130 may perform call forwarding to the pre-registered phone number if an incoming call signal is received in the loss mode of the mobile terminal 100, as shown in FIG. 13.\n According to the embodiments of the present invention, even if the mobile terminal is lost, use of the mobile terminal by a finder is restricted, such that data charges caused by indiscriminate use of the mobile terminal or leakage of personal\ninformation may be prevented.\n According to the embodiments of the present invention, since the mobile terminal autonomously senses the loss state thereof and changes the operation mode thereof to a loss mode, use of the mobile terminal by the finder may be automatically\nrestricted without action of the user.\n According to the embodiment of the present invention, the mobile terminal, which has sensed the loss state, may transmit loss information to another terminal, thereby easily finding the mobile terminal.\n The present invention mentioned in the foregoing description may be implemented using a machine-readable medium having instructions stored thereon for execution by a processor to perform various methods presented herein.  Examples of possible\nmachine-readable mediums include HDD (Hard Disk Drive), SSD (Solid State Disk), SDD (Silicon Disk Drive), ROM, RAM, CD-ROM, a magnetic tape, a floppy disk, an optical data storage device, the other types of storage mediums presented herein, and\ncombinations thereof.  If desired, the machine-readable medium may be realized in the form of a carrier wave (for example, a transmission over the Internet).  The processor may include the controller 180 of the mobile terminal.\n The foregoing embodiments are merely exemplary and are not to be considered as limiting the present disclosure.  This description is intended to be illustrative, and not to limit the scope of the claims.  Many alternatives, modifications, and\nvariations will be apparent to those skilled in the art.  The features, structures, methods, and other characteristics of the exemplary embodiments described herein may be combined in various ways to obtain additional and/or alternative exemplary\nembodiments.\n As the present features may be embodied in several forms without departing from the characteristics thereof, it should also be understood that the above-described embodiments are not limited by any of the details of the foregoing description,\nunless otherwise specified, but rather should be considered broadly within its scope as defined in the appended claims, and therefore all changes and modifications that fall within the metes and bounds of the claims, or equivalents of such metes and\nbounds, are therefore intended to be embraced by the appended claims.", "application_number": "15725090", "abstract": " A mobile terminal includes a wireless communication unit configured to\n     perform wireless communication with an artificial intelligence device, an\n     artificial intelligence unit configured to recognize a loss state of the\n     mobile terminal if information included in a signal received from the\n     artificial intelligence device indicates the loss state of the mobile\n     terminal and to generate a control signal for switching an operation mode\n     of the mobile terminal according to the recognized loss state, and a\n     controller configured to set the operation mode of the mobile terminal to\n     a loss mode for restricting use of the mobile terminal according to the\n     generated control signal.\n", "citations": ["7783281", "8864847", "20040180673", "20110047033", "20120252411", "20140057597", "20150207917", "20150235058", "20160343235"], "related": []}, {"id": "20180367484", "patent_code": "10348658", "patent_name": "Suggested items for use with embedded applications in chat conversations", "year": "2019", "inventor_and_country_data": " Inventors: \nRodriguez; Adam (Singapore, SG), Chen; Rendong (Singapore, SG), Horn; Thomas (Singapore, SG), Lei; Florbela (Singapore, SG), Launay; Yohan (Singapore, SG), Chuang; Chen-Ting (Singapore, SG), Zhu; Bin (Singapore, SG)  ", "description": "CROSS REFERENCE TO RELATED APPLICATIONS\n Co-pending U.S.  patent application Ser.  No. 15/624,637, filed Jun.  15, 2017, entitled, \"Embedded Programs and Interfaces for Chat Conversations,\" is related to the present application, and is incorporated by reference herein in its entirety.\nBACKGROUND\n The popularity and convenience of digital mobile devices as well as the widespread of use of Internet communications have caused communications between user devices to become ubiquitous.  For example, users commonly use their devices to send\nelectronic messages to other users as text messages, chat messages, email, etc. In a chat conversation between user devices, for example, users post text, images, and other types of content data to a chat interface, and the posted data is displayed in\nthe chat interfaces displayed on the other user devices participating in the chat conversation.\n The background description provided herein is for the purpose of generally presenting the context of the disclosure.  Work of the presently named inventors, to the extent it is described in this background section, as well as aspects of the\ndescription that may not otherwise qualify as prior art at the time of filing, are neither expressly nor impliedly admitted as prior art against the present disclosure.\nSUMMARY\n Implementations of this application relate to embedded programs and interfaces for chat conversations.  In some implementations, a computer-implemented method to provide suggested items includes causing a chat interface to be displayed by a\nfirst user device, where the chat interface is generated by a messaging application, and the chat interface is configured to display one or more messages in a chat conversation.  The one or more messages are provided by a plurality of user devices\nparticipating in the chat conversation over a network.  The method causes an embedded interface to be displayed associated with the chat interface, where the embedded interface is provided by a first embedded application that executes at least in part on\nthe first user device in association with the messaging application.  The method determines that a suggestion event has occurred in association based on based on received data that indicates that a particular event has occurred at one or more of the\nplurality of user devices that are participating in the chat conversation, where the one or more of the plurality of user devices are different than the first user device.  The method obtains one or more suggested response items based on the suggestion\nevent, and causes the one or more suggested response items to be displayed by the first user device.\n Various implementations and examples of the method are described.  For example, in some implementations, the particular event includes user input received by an associated chat interface of the one or more of the plurality of user devices, user\ninput received by an associated embedded interface of the one or more of the plurality of user devices, and/or an embedded application event occurring in an associated embedded application executing on the one or more of the plurality of user devices. \nIn some implementations, the received data that indicates that the particular event has occurred indicates at least one of: a type of content feature displayed in a media item being played by the one or more of the plurality of user devices, achievement\nof a predefined objective in a game of the one or more of the plurality of user devices, user input received by a receiving user device participating in the chat conversation, where the user input is provided in a different embedded interface displayed\non the receiving user device, and initiation of the embedded application from a particular user device of the plurality of user devices.  In some implementations, the particular event includes one or more messages input to the chat conversation by\nrespective users of the one or more of the plurality of user devices, where the suggestion event is determined based on detecting at least one of one or more predefined topics, words, and phrases in the one or more messages.\n In some implementations, obtaining one or more suggested response items based on the suggestion event includes determining at least one suggested response item of the one or more suggested response items based on at least one of: one or more\npredefined associations of the at least one suggested response item with the suggestion event; a model trained with particular suggestion events and responses input in response to the particular suggestion events; and one or more of a plurality of rules\nand objectives used in a game provided by the embedded application.  In some implementations, the method further includes receiving user input indicative of user selection of a selected suggested response item from the one or more suggested response\nitems; and outputting a selected message in the chat conversation, the selected message associated with the selected suggested response item.\n In some examples, the one or more suggested response items can include one or more text messages that indicate user reactions to the suggestion event.  The method can further include receiving user input indicative of selection of a selected\nsuggested response item from the one or more suggested response items, and in response to the user input, providing a command to the first embedded application, where the command is associated with the selected suggested response item.  In some examples,\nthe command to the first embedded application includes at least one of: a first command to perform an action in a game implemented by the first embedded application, a second command to change a playback position in playback of media data by the first\nembedded application, and a third command to change a shared object provided by the first embedded application.  In additional examples, the one or more suggested response items include one or more suggested commands to the first embedded application and\none or more suggested messages to be output to the chat conversation in the chat interface, where the one or more suggested commands are visually distinguished from the one or more suggested messages when displayed in the chat interface.  In some\nimplementations, the one or more suggested response items are determined by at least one of the first embedded application, the messaging application, and a remote server in communication with the messaging application over the network.\n In further examples, the first embedded application is a media player application causing displayed playback of video data in the embedded interface, wherein the embedded interface is configured to receive input from a user that controls the\ndisplayed playback of the video data, and wherein at least one of the one or more suggested response items includes one or more suggested playback commands operative to adjust the displayed playback of the video data.  In further examples, the first\nembedded application is a game application causing display of game data in the embedded interface, wherein the embedded interface is configured to receive input from a user that changes one or more states of the game application, and wherein at least one\nof the one or more suggested response items includes one or more suggested commands operative to modify at least one state of the game application.  In further examples, the first embedded application is a shared document application causing display of a\nshared document in the embedded interface, wherein the embedded interface is configured to receive input from a user that changes one or more items of the shared document, and wherein at least one of the one or more suggested response items includes one\nor more suggested commands operative to modify the shared document.\n In some implementations, the embedded interface can be output on a subset of the plurality of user devices participating in the chat conversation, wherein the subset includes user devices that have received second user input in an associated\nchat interface that causes the user devices to join an embedded session initiated by the first embedded application.  The first embedded application can display output data that is generated by the first embedded application, and/or received from a\nserver in connection with the first user device over the network.\n In some implementations, a system includes a memory and at least one processor configured to access the memory and configured to perform operations including causing a chat interface to be displayed by a first user device, where the chat\ninterface is generated by a messaging application.  The chat interface is configured to display one or more messages in a chat conversation, where the one or more messages are provided by a plurality of user devices participating in the chat conversation\nover a network.  The operations include causing an embedded interface to be displayed associated with the chat interface, where the embedded interface is provided by a first embedded application executing in association with the messaging application,\nand the first embedded application executes at least in part on the first user device.  The operations include determining that a suggestion event has occurred in association with use of the first embedded application based on at least one of: user input\nreceived by the embedded interface, and event information from the first embedded application indicating that the suggestion event has occurred in the first embedded application.  The operations include obtaining one or more suggested response items\nresponsive to the suggestion event, and causing to be displayed the one or more suggested response items by the first user device.  The one or more suggested response items are each selectable by a first user of the first user device to cause the\nmessaging application to provide an associated command to change one or more states of the first embedded application, and/or cause an associated message to be displayed in the chat conversation by the plurality of user devices participating in the chat\nconversation.\n Various implementations and examples of the system are described.  For example, in some implementations, the operation of determining that the suggestion event has occurred is based on determining that a second user device of the plurality of\nuser devices has initiated a second embedded application corresponding to the first embedded application to join an embedded session provided for the first embedded application.\n In some implementations, a non-transitory computer readable medium has stored thereon software instructions that, when executed by a processor, cause the processor to perform operations.  The operations include determining that a suggestion\nevent has occurred in association with use of at least one embedded application of a plurality of corresponding embedded applications executed at least in part on associated user devices, where each embedded application executes in association with a\nmessaging application executing on the associated user device.  Each messaging application is configured to cause an associated chat interface to be displayed by the associated user device, where the associated chat interface is configured to display one\nor more messages in a chat conversation, and the associated user devices are participating in the chat conversation over a network.  The operations include obtaining one or more suggested response items responsive to the suggestion event, and causing to\nbe displayed the one or more suggested response items by at least one associated user device of the associated user devices.  The one or more suggested response items are each selectable by user input from a respective user of the at least one associated\nuser device to provide an associated command to change one or more states of the plurality of corresponding embedded applications, and/or cause display of an associated message in the chat conversation by the associated user devices participating in the\nchat conversation.  In some implementations, the one or more states are one or more second states, and the operation of determining that the suggestion event has occurred is based on at least one of: user input received by at least one of the\ncorresponding embedded applications, and user input received by at least one of the messaging applications that changes one or more first states of the corresponding embedded applications. BRIEF DESCRIPTION OF THE DRAWINGS\n FIG. 1 is a block diagram of example systems and a network environment which may be used for one or more implementations described herein;\n FIG. 2 is a flow diagram illustrating an example method to provide a program embedded in a chat conversation, according to some implementations;\n FIG. 3 is a flow diagram illustrating an example method to enable a user to join an embedded session, according to some implementations;\n FIG. 4 is a flow diagram illustrating an example method in which a first device processes an event in which a second device has joined an embedded session, according to some implementations;\n FIG. 5 is a flow diagram illustrating an example method to provide output of a media item using an embedded application in association with a messaging application, according to some implementations;\n FIG. 6 is a flow diagram illustrating another example method to provide output of an embedded application in association with a messaging application, according to some implementations;\n FIG. 7 is a flow diagram illustrating an example method to provide suggested response items in association with an embedded application in a chat conversation, according to some implementations;\n FIGS. 8A-8E are graphic representations of example user interfaces displayed by user devices, according to some implementations;\n FIGS. 9A-9C are graphic representations of additional example user interfaces displayed by user devices, according to some implementations;\n FIGS. 10A-10D are graphic representations of additional example user interfaces displayed by user devices, according to some implementations;\n FIG. 11 is a diagrammatic illustration of an example sequence including initiation of an embedded application and a user joining an embedded session; and\n FIG. 12 is a block diagram of an example device which may be used for one or more implementations described herein.\nDETAILED DESCRIPTION\n One or more implementations described herein relate to embedded programs and interfaces for chat conversations.  In various implementations, a first embedded application can be initiated by user input in a first chat interface provided by a\nfirst messaging application and displayed on a first user device.  The first chat interface is configured to receive user input from a first user and display messages received from one or more other user devices participating in a chat conversation over\na network and associated with a respective chat user.  The other user devices can connect to an embedded session that is associated with the embedded application, where users of the other devices are designated as member users.\n For example, in some implementations, a notification can be provided on other chat devices in the chat conversation, indicating that the embedded session is active.  Chat users of the other chat devices can provide input to their user devices\n(e.g., select a join control displayed in the chat interfaces of their devices) to cause their devices to join the embedded session and become member device.  Chat identities of the chat users can be provided to the embedded application and use in the\nembedded application.\n The first embedded application can cause an embedded interface to be displayed by the user device in association with the chat interface, e.g., as part of the chat interface, within or adjacent to the chat interface, etc. The embedded interface\ncan receive user input from the associated user of the user device, e.g., via selection of displayed interface elements, a keyboard, other input device, etc., and the input is provided to the first embedded application.  The first embedded application\ncan also communicate with the first messaging application and the chat interface, e.g., receiving data from chat messages and output chat messages displayed in the chat interface and/or in the chat conversation.  The first embedded application can\ncommunicate with the other member devices that have joined the embedded session, e.g., via the first messaging application, where the other devices can be associated with respective embedded applications and messaging applications.  The first embedded\napplication can communicate with the first messaging application and one or more servers that communicate with the first messaging application.  Indications of events occurring in the embedded application or at other connected devices (e.g., server) can\nbe displayed in the chat interface (e.g., in the chat conversation such that they are visible by chat users) and/or displayed in the embedded interface.\n Embedded applications of various types can be used, e.g., embedded media player applications to play media items (videos, images, audio segments, etc.), embedded game applications to output game data, embedded applications to output content\ndocuments (e.g., item lists, spreadsheets, etc.), embedded applications to create and output reservations or travel itineraries, etc. In some implementations, embedded applications can be downloaded and executed on the first user device when initiation\nof the embedded application is selected in the chat interface.  In some implementations, embedded applications can be standard, standalone applications that can be configured, or their output processed, to provide displayed output in an embedded manner,\ne.g., display output in an embedded interface in association with a chat interface of a messaging application.\n Various other features are described.  For example, in some implementations, suggested response items can be generated or obtained by the messaging application or embedded application and displayed in the chat interface or embedded interface. \nSome suggested response items, when selected by user input, can provide one or more suggested commands to the embedded application, messaging application, or first user device.  Some suggested response items, when selected by user input, can provide one\nor more suggested messages to the chat interface, e.g., to the chat conversation.  Suggested response items can be determined based on suggestion events occurring in the embedded application, messaging application, and/or other connected devices, thus\nproviding relevant suggested input options for a user.\n In various implementations, features can include member users being assigned respective user statuses (e.g., user roles) in the embedded session, e.g., a controlling user of media item playback, a player role or observer role in a game, etc. In\nsome examples, an embedded application can receive user comments in the embedded interface during a first playback of a media item, and can display those user comments in the embedded interface during a later, second playback of that media item for chat\nusers that participated in the chat conversation at the time of the first playback.  Displayed output data of embedded applications can be synchronized among the member devices of a particular embedded session.  For example, a playback position adjusted\nby user input on one member device causes the other member devices to adjust the playback positions in the associated embedded interfaces.  Bots can be provided in the chat interface and can interface with embedded applications, e.g., initiate embedded\napplications to process content items selected by user input, etc.\n One or more features described herein enable users to interact with embedded applications in the context of a chat conversation in which multiple user devices participate.  Furthermore, multiple chat users can interact in an embedded\napplication.  Features allow chat users to share functions and output of applications in a chat conversation without the chat users having to exit from the chat interface to display an interface of a separate application.  Thus, chat messages can be\ninput and displayed in the chat interface concurrently with the display of output from an embedded application, allowing chat users to feel connected in concurrent use of the application.  Such features allow group, shared, expressive experiences among\nchat users in an embedded session using embedded applications.  In some examples, in an embedded session playing a media item such as video, features enable chat users to watch videos together and comment on the video in the chat conversation.  If a\nfirst chat user is currently watching a video, a second chat user in that conversation has the option to watch the video with the first chat user, similarly to sitting on the couch watching television together in an analogous physical environment.  In\nthis shared experience, both users can control the playback, control a playlist of media items to watch, chat about features and events in the video, and/or post likes into the video feed.\n Communication between an embedded application and a chat interface allows compelling and efficient integration of chat and application functions.  For example, an embedded application can send events or notifications to a message application,\nand the message application can display the events or notifications in the chat conversation and chat interfaces of the chat users, thus being visible to chat users who are not currently participating in an embedded session.  This can improve the\nengagement, retention, and awareness of users about the occurrence of and events within embedded sessions.  Similarly, chat messages input to the chat interface and chat conversation can be input to an embedded application, allowing the embedded\napplication to respond to commands and messages from the chat interface and chat conversation.\n Use of such embedded applications in association with a chat interface allows chat users to interact with each other and an embedded application with reduced user input and reduced time to interact when using a chat interface and an application,\nthus reducing consumption of device resources that would otherwise be needed to process, display, and receive user input in applications that are separate from a messaging application.  For example, disclosed features reduce consumption of device\nresources needed to enable and process communications between multiple users simultaneously using chat interfaces and application functions (e.g., reducing switching displays of different interfaces for different applications on a user device, reducing\ncopying and pasting displayed data in one application to another application or interface, reducing the repeating of commands or data input to applications, etc.).  Furthermore, embedded applications can be downloaded and installed on a user device\nseamlessly based on selections within a chat interface, reducing system resources needed to present additional download and installation interfaces.  In addition, embedded applications can be of small data size, e.g., can use one or more functions of the\nassociated messaging application such as communication and network functions, chat interface functions, etc., and so can omit such functions themselves.  This allows specific application functions (e.g., playing media items, providing a game or shared\nlist, etc.) to be implemented in a smaller application that consumes fewer bandwidth and storage resources than independently-executed applications that each include a complete set of functions.  In some implementations, a standard API can be used\nbetween messaging application and embedded application, allowing a large variety of embedded applications to be provided by many different providers.\n Consequently, a technical effect of one or more described implementations is that chat communication between user devices in device-implemented conversations is provided in conjunction with displayed and interactive features from applications\nwith less computational time and resources expended to obtain results.  For example, a technical effect of described techniques and features is a reduction in the consumption of system processing resources utilized to provide chat communication and\napplication functions as compared to a system that does not provide one or more of the described techniques or features, e.g., a system that uses prior techniques of displaying application output in separate applications unassociated with chat interface\ncommunication in which additional computational resources are needed to run the separate application and cause a user to switch between application and chat conversations, provide additional input to applications, etc. In another example, a technical\neffect of described techniques and features is a reduction in the consumption of system processing resources utilized by a system that does not provide one or more of the described techniques or features, e.g., a system that uses only bots in chat\ninterfaces to provide application functions and interface with chat users, since such bots may present output in a chat interface that is confusing to multiple chat users participating and providing messages in a chat conversation, leading to inefficient\nuse of system resources (e.g., repeated viewing, scrolling, and input of commands from users to display previously-output messages and output provided by the bot, etc.).  Thus, a technical effect of described techniques and features is a reduction in the\nconsumption of system processing resources utilized to receive input from multiple users as compared to a system that does not provide one or more of the described techniques or features.\n In some implementations, suggested response items may be automatically provided in response to suggestion events occurring in embedded applications, messaging applications, or other devices.  The suggested response items are appropriate and\nrelevant as commands or messages to the suggestion event.  For example, suggested message items may cause appropriate messages to be displayed in the chat conversation as reactions to particular events occurring in an embedded application or other\nprogram (e.g., events based on user input from other users, or events in played content data), as if a user were creating the messages.  Suggested command items may cause suggested relevant commands to be sent to the embedded application or other\napplication/device in response to events in an embedded application or other program, e.g., a command to change media playback in response to a detection of features in the played media item, a command to perform a game action in response to an\nopponent's detected action in a game, etc. These suggested items provide users with more flexible options in determining their responses to events in applications and may provide options for a user who is otherwise unable to respond to an event in an\napplication adequately.  For example, at various times during a message conversation, users may not be able to provide sufficient attention and/or focus to respond to events with pertinent commands or messages, and/or may not be able to provide detailed\nuser input to a user device to create such pertinent commands or messages.  In some examples, a user may be performing an activity or be in an environment where appropriate responses are not possible or more difficult for the user to provide.\n One or more features described herein advantageously provide automatic selectable suggested response items for a user in response to particular events in the embedded application or messaging application.  The suggested response items are\nrelevant to detected events on the device.  Selection of such suggested items allows a user to simply and quickly respond to events with reduced user input and reduced time to compose the responses on a device, thus reducing consumption of device\nresources that would otherwise be needed to display a large set of possible responses and/or reducing consumption of device resources needed to enable and process added input from the user to compose responses, search through, edit, or complete\nresponses, and/or otherwise participate in conversations held via electronic devices.  Consequently, a technical effect of one or more described implementations is that creation and transmission of responses in device-implemented conversations is\nachieved with less computational time and resources expended to obtain results.  For example, a technical effect of described techniques and features is a reduction in the consumption of system processing resources utilized to display, create, and send\nmessage responses and command responses as compared to a system that does not provide one or more of the described techniques or features, e.g., a system that uses prior techniques in which additional computational resources are needed to receive input\nfrom the user used to specify, select, edit, or create responses that a user is to transmit.\n Described techniques provide additional advantages.  In some examples, user statuses or roles can be automatically assigned efficiently based on information passed between messaging application and embedded application without receiving\nadditional user input.  In some examples, user commands and comments can be sent to a server, and server events can be received and presented by an embedded application on a user device, without the user having to access the server by exiting a chat\ninterface and executing a separate application on the user device.  Bots can be used in conjunction with embedded applications, e.g., to search for content items to be played by an embedded application and without the user expending system resources to\nperform manual searching, selection, and initiation of applications.  In other examples, user comments can be stored that were previously input by users in association with content items previously played in an embedded application.  Such stored comments\ncan be displayed in association with the a later playback of the content items if the playback occurs in the same chat conversation (or group of chat users), thus saving chat users from re-entering comments during the later playback and reducing\nexpenditure of system resources.  Synchronization techniques allow playback of content items on user devices of an embedded session to be synchronized automatically and to be receptive to input from multiple users.  Such techniques allow chat users to\nshare an experience in a chat conversation without the users using additional system resources to execute separate applications or manually indicate playback positions to each other.\n In situations in which certain implementations discussed herein may collect or use personal information about users (e.g., user data, information about a user's social network, user's location and time at the location, user's biometric\ninformation, user's activities and demographic information), users are provided with one or more opportunities to control whether information is collected, whether the personal information is stored, whether the personal information is used, and how the\ninformation is collected about the user, stored and used.  That is, the systems and methods discussed herein collect, store and/or use user personal information specifically upon receiving explicit authorization from the relevant users to do so.  For\nexample, a user is provided with control over whether programs or features collect user information about that particular user or other users relevant to the program or feature.  Each user for which personal information is to be collected is presented\nwith one or more options to allow control over the information collection relevant to that user, to provide permission or authorization as to whether the information is collected and as to which portions of the information are to be collected.  For\nexample, users can be provided with one or more such control options over a communication network.  In addition, certain data may be treated in one or more ways before it is stored or used so that personally identifiable information is removed.  As one\nexample, a user's identity may be treated so that no personally identifiable information can be determined.  As another example, a user device's geographic location may be generalized to a larger region so that the user's particular location cannot be\ndetermined.\n An image as referred to herein is a digital image having pixels with one or more pixel values (e.g., color values, brightness values, etc.).  An image can be a still image or single image, or can be an image included in a series of images, e.g.,\na frame in a video sequence of video frames, or an image in a different type of sequence or animation of images.  A video includes a sequence of multiple images.  For example, implementations described herein can be used with single images or static\nimages (e.g., a photograph, an emoji, or other image), videos, or animated images (e.g., cinemagraphs or other animated image that includes motion, a sticker that includes animation and audio, etc).  Text, as referred to herein, can include alphanumeric\ncharacters, emojis, symbols, or other characters.\n FIG. 1 illustrates a block diagram of an example environment 100 for providing messaging services that enable embedded applications as described herein.  In some implementations, automatic assistive agents, e.g., bots, can be provided.  The\nexemplary environment 100 includes messaging server 101, one or more client devices 115a, 115n, server 135, embedded application server 150, session server 152, content server 154, suggestion server 156, and network 140.  Users 125a-125n may be\nassociated with respective client devices 115a, 115n.  Server 135 may be a third-party server, e.g., controlled by a party different from the party that provides messaging services.  In various implementations, server 135 may implement bot services, as\ndescribed in further detail below.  In some implementations, environment 100 may not include one or more servers or devices shown in FIG. 1 or may include other servers or devices not shown in FIG. 1.  In FIG. 1 and the remaining figures, a letter after\na reference number, e.g., \"115a,\" represents a reference to the element having that particular reference number.  A reference number in the text without a following letter, e.g., \"115,\" represents a general reference to implementations of the element\nbearing that reference number.\n In the illustrated implementation, messaging server 101, client devices 115, server 135, and servers 150-156 are communicatively coupled via a network 140.  In various implementations, network 140 may be a conventional type, wired or wireless,\nand may have numerous different configurations including a star configuration, token ring configuration or other configurations.  Furthermore, network 140 may include a local area network (LAN), a wide area network (WAN) (e.g., the Internet), and/or\nother interconnected data paths across which multiple devices may communicate.  In some implementations, network 140 may be a peer-to-peer network between user devices and/or between other devices, e.g., using peer-to-peer data exchange techniques\nenabling offline communication (and usage of embedded applications as described below).  Network communication can include use of appropriate communication protocols (e.g., web sockets allowing communication of bi-directional, full duplex messages\nbetween devices such as a client (user) device and server).  Network 140 may also be coupled to or include portions of a telecommunications network for sending data in a variety of different communication protocols.  In some implementations, network 140\nincludes Bluetooth.RTM.  communication networks, Wi-Fi.RTM., or a cellular communications network for sending and receiving data including via short messaging service (SMS), multimedia messaging service (MMS), hypertext transfer protocol (HTTP), direct\ndata connection, email, etc. In other examples, sound waves can be used in network 140 for data and state exchange between devices.  Although FIG. 1 illustrates one network 140 coupled to client devices 115, messaging server 101, server 135, and servers\n150-156, in practice one or more networks 140 may be coupled to these entities.\n Messaging server 101 may include a processor, a memory, and network communication capabilities.  In some implementations, messaging server 101 is a hardware server.  In some implementation, messaging server 101 may be implanted in a virtualized\nenvironment, e.g., messaging server 101 may be a virtual machine that is executed on a hardware server that may include one or more other virtual machines.  Messaging server 101 is communicatively coupled to the network 140 via signal line 102.  Signal\nline 102 may be a wired connection, such as Ethernet, coaxial cable, fiber-optic cable, etc., or a wireless connection, such as Wi-Fi, Bluetooth, or other wireless technology.  In some implementations, messaging server 101 sends and receives data to and\nfrom one or more of client devices 115a-115n, server 135, one or more of servers 150-156, and bot 113 via network 140.  In some implementations, messaging server 101 may include messaging application 103a that provides client functionality to enable a\nuser (e.g., any of users 125) to exchange messages with other users and/or with a bot.  Messaging application 103a may be a server application, a server module of a client-server application, or a distributed application (e.g., with a corresponding\nclient messaging application 103b on one or more client devices 115).\n Messaging server 101 may also include database 199 which may store messages exchanged via messaging server 101, group data, e.g., indicating which users are in which messaging groups (e.g., chat conversations) and which content data (video data,\nimages, audio data with group likes, game data, content documents, etc.) is associated with which messaging groups, data and/or configuration of one or more bots, information provided by content classifier 130, and user data associated with one or more\nusers 125, all upon explicit permission from a respective user to store such data.  In some embodiments, messaging server 101 may include one or more assistive agents, e.g., bots 107a and 111.  In other embodiments, the assistive agents may be\nimplemented on the client devices 115a-n and not on the messaging server 101.\n In some implementations, messaging server 101, messaging application 103, and/or embedded applications running in association with a messaging application can provide messages in one messaging group (e.g., chat conversation), and/or can send\nmessages to one or more other messaging groups (chat conversations) implemented by the messaging server 101 (or other messaging server).\n Messaging application 103a may be code and routines operable by the processor to enable exchange of messages among users 125 and one or more bots 105, 107a, 107b, 109a, 109b, 111, and 113.  In some implementations, messaging application 103a may\nbe implemented using hardware including a field-programmable gate array (FPGA) or an application-specific integrated circuit (ASIC).  In some implementations, messaging application 103a may be implemented using a combination of hardware and software.\n In various implementations, when respective users associated with client devices 115 provide consent for storage of messages, database 199 may store messages exchanged between one or more client devices 115.  In some implementations, when\nrespective users associated with client devices 115 provide consent for storage of messages, database 199 may store messages exchanged between one or more client devices 115 and one or more bots implemented on a different device, e.g., another client\ndevice, messaging server 101, and server 135, etc. In the implementations where one or more users do not provide consent, messages received and sent by those users are not stored.\n In some implementations, messages may be encrypted, e.g., such that only a sender and recipient of a message can view the encrypted messages.  Some implementations can store encrypted data generated from users, e.g., user-generated content data\nsuch as messages, comments, recorded user actions, etc. The user-generated content data can be encrypted locally on the user device (e.g., in local execution of an embedded application) and on a server system (e.g., for information communicated for a\nshared embedded application and an embedded session).  In some implementations, messages are stored.  In some implementations, database 199 may further store data and/or configuration of one or more bots, e.g., bot 107a, bot 111, etc. In some\nimplementations when a user 125 provides consent for storage of user data (such as social network data, contact information, images, etc.) database 199 may also store user data associated with the respective user 125 that provided such consent.\n In some implementations, messaging server 101 and messaging application 103 can be implemented on one or more chat devices (e.g., user devices) participating in a chat conversation as described herein.\n In some implementations, messaging application 103a/103b may provide a user interface that enables a user 125 to create new bots.  In these implementations, messaging application 103a/103b may include functionality that enables user-created bots\nto be included in chat conversations between users of messaging application 103a/103b.\n Servers 150-156 may include one or more processors, memory and network communication capabilities.  In some implementations, the servers are hardware servers.  Signal lines 160, 162, 164, and 166 connect the servers 150, 152, 154, and 156 to the\nnetwork 140, respectively, and may be wired connections, such as Ethernet, coaxial cable, fiber-optic cable, etc., or wireless connections, such as Wi-Fi, Bluetooth, or other wireless technology.  In some implementations, servers 150-156 sends and\nreceive data to and from one or more of the messaging server 101 and the client devices 115a-115n via the network 140.  Although servers 150-156 are illustrated as being individual servers, one or more of the servers 150-156 can be combined and/or\nincluded in one or more other components of the environment 100, e.g., messaging server 101, client device 115a, server 135, etc. Any or all of servers 150-156 may be managed by the same party that manages the messaging server 101, or may be managed by a\nthird-party.  In some implementations, a server 101, 135, and 150-156 can be considered a remote server relative to a particular device when the server communicates over the network 140 with that particular device.  Similarly, in some implementations,\nclient devices 115 can be considered remote from a particular device if communicating over the network 140 with that particular device.  In various implementations, functions of any of servers 101, 135, and 150-156 can be implemented on one or more\nclient devices 115.\n Embedded application server 150 is communicatively coupled to the network 140 via signal line 160 and serves embedded applications as described herein that are requested by a device for install on the device.  For example, a user device can\nrequest to download an embedded application in as a background process of an operating system or messaging application, and the embedded application server 150 sends the data to the client device to execute the embedded application on the client device. \nIn some implementations, the embedded application server 150 can execute embedded applications in whole or in part and can send output data from the embedded application to client devices for display by one or more client devices.\n Session server 152 is communicatively coupled to the network 140 via signal line 162 and can be used in some implementations to handle and coordinate data sent to and from multiple client devices participating in an embedded session that\nconnects multiple embedded applications (e.g., a shared embedded application) executing on client devices connected to the network 140.  For example, session server 152 can be used to manage an embedded session, e.g., including one or more of\nsynchronizing the states of embedded applications in an embedded session, determine and distribute events on a corresponding embedded application executing on the session server, etc. In some implementations, the session server can determine one or more\nevents for the embedded session, one or more of which may occur at the session server or other server in communication with the session server.  For example, an event may include a server receiving user selection, comments, or ratings associated with\ncontent items served by the server to user devices, and those content items may be played in an embedded session, such that the event can be conveyed to the embedded session.  In some implementations, the session server 152 can be included in or be the\nsame as messaging server 101 or other server of the environment 100.\n Content server 154 is communicatively coupled to the network 140 via signal line 164 and can be used to send content data to one or more devices connected to the network 140.  For example, the content data can be data provided for output by\nembedded applications that display embedded interfaces on client devices as described herein.  The content data can include video data, image data, audio data, document data, webpage data, game data, bot data or information, map data or other geographic\ninformation, etc. The content data can be included in different content items (media items), e.g., videos or movies, images, games, documents, webpages, etc.\n Suggestion server 156 is communicatively coupled to the network 140 via signal line 166 and can be used to determine and provide suggested response items in reply to messaging applications and/or embedded applications for display and selection\nby users at client devices.  For example, a messaging application running at a client device 115 can send descriptive information to the suggestion server 156, and the suggestion server 156 can consult databases, knowledge graph, and/or other data\nresources to determine one or more suggested response items (also referred to as \"suggested items,\" \"suggested input items,\" or \"suggested input items\" herein) in response to the descriptive information.  Such suggested items can be suggested commands\nand suggested messages as described with reference to FIG. 7.  In some implementations, the functions of suggestion server 156 can be included within a messaging application and/or an embedded application.\n In some implementations, the suggestion server 156 and/or other components of the environment 100 can use machine learning, e.g., use a machine learning model that utilizes machine learning to determine suggested items.  For example, in a\ntraining stage the suggestion server (or other component) can be trained using training data (e.g., message training data) of actual or generated messages in a messaging application context, and then at an inference stage can determine suggested items to\nnew messages or other data it receives.  For example, the machine learning model can be trained using synthetic data, e.g., data that is automatically generated by a computer, with no use of user information.  In some implementations, the machine\nlearning model can be trained based on sample data, e.g., sample message data, for which permissions to utilize user data for training have been obtained expressly from users providing the message data.  For example, sample data may include received\nmessages and responses that were sent to the received messages.  Based on the sample data, the model can predict message responses to received messages, which may then be provided as suggested items.  In some implementations, the suggestion server 156\n(or other component) can use one or more of a deep learning model, a logistic regression model, a Long Short Term Memory (LSTM) network, supervised or unsupervised model, etc. Some implementations can also detect image features in images or videos and\ndetermine suggested items (e.g., message responses) based on the image features.  For example, image features can include people (without determining identity of the people), animals, objects (e.g., articles, vehicles, etc.), particular monuments,\nlandscape features (e.g., foliage, mountains, lakes, sky, clouds, sunrise or sunset, buildings, bridges, etc.), weather, etc. Various image recognition and detection techniques can be used (e.g., machine learning based on training images, comparison to\nreference features in reference images, etc.) to detect image features.\n In some implementations, one or more of the servers 150-156 host bots.  The bots may be computer programs that perform specific functions to provide suggestions, for example, a reservation bot makes reservations, an auto-reply bot generates\nreply message text, a scheduling bot automatically schedules calendar appointments, etc. A server 150-156 may provide or refer one or more bots as its output to a messaging application 103.  For example, the code for the bot may be incorporated into the\nmessaging application 103, or the messaging application 103 may send requests to a bot implemented at a server 150-156.  In some implementations, the messaging application 103 acts as an intermediary between the user 125 and a server 150-156 by providing\nthe server 150-156 with bot commands and receiving output (e.g., embedded application data, session data, content data, suggested items, etc.) based on the bot commands.\n In some implementations, one or more of the servers 150-156 (e.g., content server 154 and/or suggestion server 156) may maintain an electronic encyclopedia, a knowledge graph, one or more databases, a social network application (e.g., a social\ngraph, a social network for friends, a social network for business, etc.), a website for a place or location (e.g., a restaurant, a car dealership, etc.), a mapping application (e.g., a website that provides directions), etc. For example, content server\n154 may receive a request for information from a messaging application 103, perform a search, and provide the information in the request.  In some implementations, content server 154 may include classifiers of particular types of content in images, and\ncan determine whether any of particular classes are detected in the content (e.g., pixels) of the image.  In some examples, the messaging application 103 may request driving directions or an estimated time of arrival from a mapping application accessed\nby content server 154.\n In some implementations, if a user consents to the use of such data, one or more servers 150-156 (e.g., content server 154) may provide a messaging application 103 with profile information or profile images of a user that the messaging\napplication 103 may use to identify a person in an image with a corresponding social network profile.  In another example, content server 154 may provide the messaging application 103 with information related to entities identified in the messages used\nby the messaging application 10, if user consent has been obtained for provided user data.  For example, the content server 154 may include or access an electronic encyclopedia that provides information about landmarks identified in the images, an\nelectronic shopping website that provides information for purchasing entities identified in the messages, an electronic calendar application that provides, subject to user consent, an itinerary from a user identified in a message, a mapping application\nthat provides information about nearby locations where an entity in the message can be visited, a website for a restaurant where a dish mentioned in a message was served, etc. In some implementations, the content server 154 may communicate with the\nsuggestion server 156 to obtain information.  The content server 154 may provide requested information to the suggestion server 156.\n In some implementations, one or more servers 150-156 may receive information from one or more messaging applications 103, e.g., to update databases used or maintained by these modules.  For example, where the content server 154 maintains a\nwebsite about a restaurant, the messaging application 103 may provide the content server 154 with updated information about the restaurant, such as a user's favorite dish at the restaurant.\n Server 135 may include a processor, a memory and network communication capabilities.  In some implementations, server 135 is a hardware server.  Server 135 is communicatively coupled to the network 140 via signal line 128.  Signal line 128 may\nbe a wired connection, such as Ethernet, coaxial cable, fiber-optic cable, etc., or a wireless connection, such as Wi-Fi, Bluetooth, or other wireless technology.  In some implementations, server 135 sends and receives data to and from one or more of\nmessaging server 101 and client devices 115 via network 140.  Although server 135 is illustrated as being one server, various implementations may include one or more servers 135.  Server 135 may implement one or more bots as server applications or server\nmodules, e.g., bot 109a and bot 113.\n In various implementations, server 135 may be part of the same entity that manages messaging server 101, e.g., a provider of messaging services.  In some implementations, server 135 may be a third party server, e.g., controlled by an entity\ndifferent than the entity that provides messaging application 103a/103b.\n In some implementations, server 135 provides or hosts bots.  A bot is an automated service, implemented on one or more computers, that users interact with primarily through text, e.g., via messaging application 103a/103b.  Bots are described in\ngreater detail below.\n Client device 115 may be a computing device that includes a memory and a hardware processor, for example, a camera, a laptop computer, a tablet computer, a mobile telephone, a wearable device, a mobile email device, a portable game player, a\nportable music player, a reader device, head mounted display or other electronic device capable of wirelessly accessing network 140.\n In the illustrated implementation, client device 115a is coupled to the network 140 via signal line 108 and client device 115n is coupled to the network 140 via signal line 110.  Signal lines 108 and 110 may be wired connections, e.g., Ethernet,\nor wireless connections, such as Wi-Fi, Bluetooth, or other wireless technology.  Client devices 115a, 115n are accessed by users 125a, 125n, respectively.  The client devices 115a, 115n in FIG. 1 are used by way of example.  While FIG. 1 illustrates two\nclient devices, 115a and 115n, the disclosure applies to a system architecture having one or more client devices 115.\n In some implementations, client device 115 may be a wearable device worn by a user 125.  For example, client device 115 may be included as part of a clip (e.g., a wristband), part of jewelry, or part of a pair of glasses.  In another example,\nclient device 115 can be a smartwatch.  In various implementations, user 125 may view messages from the messaging application 103a/103b on a display of the device, may access the messages via a speaker or other output device of the device, etc. For\nexample, user 125 may view the messages on a display of a smartwatch or a smart wristband.  In another example, user 125 may access the messages via headphones (not shown) coupled to or part of client device 115, a speaker of client device 115, a haptic\nfeedback element of client device 115, etc.\n In some implementations, messaging application 103b is stored on a client device 115a.  In some implementations, messaging application 103b (e.g., a thin-client application, a client module, etc.) may be a client application stored on client\ndevice 115a with a corresponding a messaging application 103a (e.g., a server application, a server module, etc.) that is stored on messaging server 101.  For example, messaging application 103b may transmit messages created by user 125a on client device\n115a to messaging application 103a stored on messaging server 101.\n In some implementations, messaging application 103a may be a standalone application stored on messaging server 101.  A user 125a may access the messaging application 103a via a web page using a browser or other software on client device 115a. \nIn some implementations, messaging application 103b that is implemented on the client device 115a may include the same or similar modules as that are included on messaging server 101.  In some implementations, messaging application 103b may be\nimplemented as a standalone client application, e.g., in a peer-to-peer or other configuration where one or more client devices 115 include functionality to enable exchange of messages with other client devices 115.  In these implementations, messaging\nserver 101 may include limited or no messaging functionality (e.g., client authentication, backup, etc.).  In some implementations, messaging server 101 may implement one or more bots, e.g., bot 107a and bot 111.\n In various implementations, messaging application 103 may include one or more embedded applications 117 that are executed in association with and dependent on the messaging application 103.  An embedded application 117 can interact with a chat\ninterface of the messaging application and provide one or more of its own embedded interfaces.  Examples of various features of embedded applications are described herein.\n In some implementations, messaging application 103a/103b may provide one or more suggested items, e.g., suggested commands, messages, or responses, to users 125 via a user interface, e.g., as a button, or other user interface element.  Suggested\nitems may enable users to respond quickly to messages when the user selects a suggested item, e.g., by selecting a corresponding user interface element on a touchscreen or via other input device.  Suggested items may enable faster interaction, e.g., by\nreducing or eliminating the need for a user to type a response.  Suggested items may enable users to respond to a message or provide a command quickly and easily, e.g., when a client device lacks text input functionality (e.g., a smartwatch that does not\ninclude a keyboard or microphone).  In some implementations, suggested responses may be generated using predictive models, e.g., machine learning models, that are trained to generate responses.  Suggested items are described in greater detail below with\nrespect to FIG. 7.\n FIG. 2 is a flow diagram illustrating an example method 200 to provide a program embedded in a chat conversations, according to some implementations.  In some implementations, method 200 can be implemented, for example, on a server system, e.g.,\nmessaging server 101, as shown in FIG. 1.  In some implementations, some or all of the method 200 can be implemented on a system such as one or more client devices 115 as shown in FIG. 1, and/or on both a server system and one or more client systems.  In\ndescribed examples, the implementing system includes one or more processors or processing circuitry, and one or more storage devices such as a database or other accessible storage.  In some implementations, different components of one or more servers\nand/or clients can perform different blocks or other parts of the method 200.\n In block 202, it is checked whether user consent (e.g., user permission) has been obtained to use user data in the implementation of method 200.  For example, user data can include messages sent or received by a user, e.g., using messaging\napplication 103, user preferences, user biometric information, user characteristics (identity, name, age, gender, profession, etc.), information about a user's social network and contacts, social and other types of actions and activities, content,\nratings, and opinions created or submitted by a user, a user's current location, historical user data, images generated, received, and/or accessed by a user, videos viewed or shared by a user, etc. One or more blocks of the methods described herein may\nuse such user data in some implementations.\n If user consent has been obtained from the relevant users for which user data may be used in the method 200, then in block 204, it is determined that the blocks of the methods herein can be implemented with possible use of user data as described\nfor those blocks, and the method continues to block 208.  If user consent has not been obtained, it is determined in block 206 that blocks are to be implemented without use of user data, and the method continues to block 208.  In some implementations, if\nuser consent has not been obtained, blocks are to be implemented without use of user data and with synthetic data and/or generic or publicly-accessible and publicly-usable data.\n Implementations described herein may provide embedded application programs that are executed in association with messaging applications providing chat conversations in chat interfaces.  The messaging application may be a messaging application\n103 as described above, and may execute wholly or in part on a user device (e.g., client device or other device), or may execute wholly or in part on a server (e.g., messaging server 101) and provide data to the user device to display at the user device.\n In some examples, messaging application 103 may be an instant messaging application, a social network application, an email application, a multimedia messaging application, and the like.  For example, if the messaging application is an instant\nmessaging application, messages may be received as part of an instant messaging communication between a particular user 125a and one or more other users 125 of participating devices, e.g., in a messaging session (e.g., chat, group, or \"chat\nconversation\") having two or more participants, etc. A chat, or chat conversation, is a messaging session in which multiple participating users communicate messages (e.g., including various types of content data) with each other.  In some\nimplementations, users may send messages to other users by inputting messages into a chat conversation implemented by a messaging application.  In some implementations, users may send messages to particular other users by messaging a phone number (e.g.,\nwhen the messaging application 103 works over SMS, or another messaging application that utilizes phone numbers) or selecting a recipient user from a contacts list (e.g., when the messaging application 103 works over rich communications services (RCS) or\nanother chat interface).  In some implementations, messaging application 103 may provide real-time communication between participating devices, e.g., audio (voice) calls and/or video calls, where real-time audio data and/or video data is captured at one\nor more participating devices (e.g., using a camera and microphone), and this data is sent to and output in chat interfaces (and speakers) of the other devices participating in a chat conversation (e.g., conference call).\n The messaging application 103 may display a chat interface that displays messages of the chat conversation, e.g., messages sent by the user of the user device, and messages received by the user device from other devices.  As referred to herein,\na chat interface has a user interface component that is displayed (e.g., including displayed interface elements), as well as having a data reception and exchange component that performs receiving of user input, sending and receiving messages, displaying\nof messages, etc.\n In implementations described herein, one or more embedded applications can be executed in association with a messaging application.  An embedded application herein refers to a messaging-embedded or chat-embedded program, e.g., a program that is\nexecuted in association with, or within a context of, a running messaging application and is dependent on the execution of the messaging application.  For example, an embedded application is code (e.g., a script) able to be executed on a device if a\nmessaging application is currently providing output (e.g., a chat interface) on the device, and the embedded application is exited (e.g., no longer executing on the device) if the messaging application is exited and no longer provides output on the\ndevice.  For example, in some implementations, an embedded application can receive input provided to or in an embedded interface, e.g., selections of user interface elements, text, or other displayed objects in the embedded interface, gestures of an\ninput device (such as taps, tracings of shapes, curves, or lines, etc. on a touch-sensitive input surface of a touchscreen, touchpad, etc.), and/or other user input received by the embedded interface via touchscreen or other input device.  As referred to\nherein, an embedded interface includes a displayed component, which can include a user interface component (e.g., displayed user interface elements) as well as displayed output data including content data, e.g., media item content data, game data,\ndocument data, user comments, messages, etc. The embedded interface also includes a data reception component that performs receiving of user input at the user device, e.g., via a touchscreen, keyboard, or other input device by a user.  In some\nimplementations, the embedded interface is displayed in association with the chat interface.  For example, the embedded interface can be displayed as part of the chat interface, e.g., within the borders or display area of the chat interface, partially\noverlapping the chat interface, or adjacent to the chat interface.  In some implementations, at least a portion of the chat interface is visible and the embedded interface is visible on the display.\n In some implementations, the embedded application can receive input provided in the associated chat interface for the associated chat conversation, e.g., as text commands or selection of chat interface elements.  In some implementations, an\nembedded application provides output in the embedded interface, and/or can provide output in the chat interface, e.g., provides output as text to a chat conversation displayed in the chat interface or provides other chat interface output.  For example,\noutput of the embedded application to the embedded interface can be referred to as embedded output data, and can include data generated by the embedded application and/or data that the embedded application has received from one or more other sources via\nthe associated messaging application (e.g., sources including a different application program executing on the first device, one or more servers connected over the network, one or more chat devices in a chat conversation of the associated messaging\napplication, etc.).  Data sent from the embedded application to other application programs and/or other devices can be provided from the embedded application to the messaging application, which then conveys the data (or a processed form of the data) to\nother programs or devices.  Similarly, data received at a user device from other programs or devices, to be provided to an embedded application executing for that user device, can be received by the messaging application and provided from the messaging\napplication to the embedded application.\n Some examples of embedded applications are described throughout the description.  In some example implementations, an embedded application can be code executed within a sandboxed display procedure to display application data and an embedded\ninterface, e.g., within display area of a chat interface or in other visual association with a chat interface.  For example, in some implementations, an embedded application can implement a web interface view to provide an embedded interface within the\nchat interface, where the view can display data from a web page and/or implement code that executes in connection with web pages (Javascript, CSS, HTML, etc.).  In some implementations, embedded applications can be supplied by a third party, e.g., a\ndifferent party than the party that supplies the associated messaging application and/or other components communicating with the embedded application.  In some implementations, buffering (e.g., double buffering techniques) can be provided when\ntransitioning from the display of one embedded application to a different embedded application.  For example, embedded application code and/or output data for a second embedded application that is not being displayed can be downloaded, installed, and/or\nloaded into memory for display in the background of an operating system of a device while a first embedded (or other) application and embedded interface are displayed and used in the foreground.  The first, displayed embedded interface can be swapped\nwith the second embedded interface via displayed animations and user interface transitions.  In some examples, when an embedded application is being downloaded, a loading indicator can be displayed while the code is being downloaded and/or installed on\nthe user device in the background.\n In some examples, the embedded application can communicate and interface with a framework implemented by the messaging application using an application program interface (API) that provides a set of predefined protocols and other tools to enable\nthe communication.  For example, the API can be used to communicate particular data from devices participating in an embedded session (e.g., member devices), where the data connects and synchronizes the embedded application programs (e.g., shared\nembedded application) of the embedded devices.  In some implementations, the messaging application framework can provide a set of APIs to provide embedded-application developers capabilities to enable interactions between a chat conversation, a device,\nan embedded session, and the embedded application.\n In some example implementations, if a member user exits an embedded application, that embedded application can be kept executing in the background of the member device of that member user, e.g., the background of the messaging application or\noperating system of the member device, for a period of time (e.g., 20 seconds, 1 minute, etc.).  For example, this can be performed in case the user switches back to using the embedded application soon after interacting with the chat interface.  In some\nexamples, the user can open an embedded game application, input a move to the game, close the embedded game application and embedded interface (so that they execute in the background), send or attach a file into the chat conversation in response to\nanother user's request, and immediately re-open the embedded game application and embedded interface to switch them to the foreground.  The background-running embedded application can be terminated if the chat conversation is ended, e.g., if the user\ncloses the messaging application or, in some implementations, switches to a different chat conversation.  Similarly, if the messaging application is closed or exited by the user, the messaging application can be sent to the background of the user device\nfor a particular time period, and the active embedded application can be paused during this time.  The user can switch to using another application, and then resume use of the embedded application when the messaging application is selected to execute in\nthe foreground.  Thus, embedded applications can be tied to the lifecycle of the chat conversation.\n In various implementations, one or more of the users may be bots.  For example, a chat user or member user as referred to herein can be a bot in some implementations.  In such a case, the \"user device\" associated with the \"chat user\" (bot) can\nbe a server device that provides the bot.  In some examples, the bot can be an assistant bot, a shopping bot, a search bot, etc. In some implementations, bots may be automated agents that are implemented in software and/or hardware.  In some\nimplementations, bots may represent or be associated with cameras (e.g., security cameras, web cameras, surveillance cameras, etc.), appliances (e.g., a smart refrigerator, an alarm device, an industrial appliance, etc.), imaging devices (e.g.,\nmicroscopes, medical imaging devices, etc.) and send one or more images via messaging application 103.  In the implementations that include one or more of the users that are bots, consent is obtained from an owner or operator of the bot to use messages\ngenerated by the bot.  In some implementations, consent may be specified as a configuration parameter of the bot, camera, appliance, etc. and be provided to the messaging application 103 upon the bot interacting with the messaging application 103.\n In block 208, a first messaging application is caused to display a first chat interface on a first user device.  The chat interface is a user interface configured to receive information and output information related to one or more chat\nconversations in which the first device can participate.  For example, the chat interface can be displayed on a display device of the first device, e.g., a display screen, in response to user input from a first user using or otherwise associated with the\nfirst device or other input.  In some implementations, the first messaging application executes on the first device, while in other implementations, the first messaging application can execute partially or wholly on a different device connected to the\nfirst device via a network (e.g., a messaging server) and can send data to the first device to be displayed by the first device.\n The first user and first device can join one or more chat conversations using the chat interface.  For example, user input from a user of the first device can initiate a new chat conversation with one or more other users using user devices,\nand/or join an existing chat conversation with one or more other users and devices communicating over a network.  Each chat conversation includes a group of chat users (and their associated user devices) who are participating in that particular chat\nconversation.  In some implementations, a user can be concurrently participating in multiple different chat conversations.  In some implementations, each chat user is associated with a respective chat identity in a chat conversation.  A chat identity can\nrefer to a visual representation of the user in the chat interface (e.g., depiction of a user's head, username, user image, etc.).  The chat identity includes text, image, or multimedia data effective to depict the chat user (e.g., a user photo, a user\nicon, a username, an animation depicting the chat user, etc.) A chat identity can also or alternatively refer to a user identifier (e.g., identification (ID) data, ID number, etc.).  The chat identity includes a unique identifier for the chat user in the\ncontext of the messaging application.  For example, the unique identifier may be the username.  In another example, the unique identifier may be an identifier for a database of chat users (e.g., a primary key for a database table that stores user\nidentities).  In providing a chat identity to an embedded application, the database identifier or username may be provided.\n In some implementations, a chat conversation can be considered a chat group of users, and may have an ongoing existence whether or not its users are currently participating in the chat conversation or are currently logged in or using a user\ndevice.  For example, if all the chat users exit the chat conversation, a conversation identifier (ID), and the chat identities and user identities of the chat conversation can be stored by the messaging application, and the chat conversation can be\nresumed when one or more of the chat users log in or resume the conversation using the chat interface.  Some implementations of the messaging application can assign and store a conversation ID that identifies a particular chat conversation, and store\nidentities of the associated current chat users (and/or chat devices) in association with the conversation ID.  In some implementations, new users can be added to a chat conversation, and one or more users can be removed from a chat conversation.\n The chat interface can display information of each chat conversation in which the first user is participating, including messages received by the first device from other chat devices and chat users participating in the chat conversation, as well\nas information related to statuses of chat users, options for message functions, events occurring in the chat conversation, etc. The chat interface can include interface features allowing a user of or associated with the first device (e.g., a first user)\nto provide user input, including interface elements to enable such features.  For example, a user can compose messages on the first device and input the messages to a chat conversation.  In some examples, a user can provide user input to the chat\ninterface via touchscreen, physical buttons or keyboard, voice input, or other types of input.\n In block 210, input is received by the first device that selects a first embedded application to initiate (e.g., to open or execute the embedded application on the first device) and to be shared with chat users.  The input can be user input\nreceived from the first user locally at the first device.  In some examples, a menu or other set of options listing available embedded applications can be displayed (or otherwise presented or output) by the first device, and user input can select the\nfirst embedded application from the options.  The first embedded application is associated with a particular chat conversion that is currently active in the message application.  For example, in some implementations, one or more options to select and\nopen the first embedded application can be displayed or otherwise enabled in the chat interface displaying a particular chat conversation, and selection of such an option causes the first embedded application to run in association with that chat\nconversation.  In some additional examples, the user input can include one or more text commands input in the chat interface as if part of a composed message, where the text commands are received as a command to select the first embedded application to\ninitiate.  In some examples, text preceded by a particular syntax character(s) or other predefined format can indicate such a command.\n In some implementations, the input that selected the first embedded application to open can also specify that the first embedded application is to be shared with one or more other chat users and user devices participating in an associated chat\nconversation.  This sharing selection allows an embedded session to be created (see block 214 below) between the first device and one or more other devices executing local versions of the embedded application and participating in the chat conversation. \nIn some implementations, the first embedded application can be alternately selected to open without sharing with other chat users, which would cause the first embedded application to be executed without initiating an embedded session and without other\ndevices receiving data related to the first embedded application.  In some implementations, a selection to initiate an embedded application causes a shared embedded session to be initiated automatically (or based on stored user preference data).  In some\nimplementations, an embedded application does not output display (e.g., in an embedded interface as in block 212 below) until at least one other chat user has joined the embedded session associated with the first embedded application.\n In block 212, the first embedded application is downloaded over the network and/or executed on the first device and an embedded interface is displayed in association with the chat interface.  For example, the first embedded application can be\ndownloaded in response to the selection received in block 210.  In some implementations, the first embedded application can be downloaded from a server system hosting embedded applications, e.g., embedded application server 150.  In some implementations,\nthe download of the first embedded application can occur in the background of the operating environment of the first device, e.g., as a background process of the messaging application and/or operating system of the first device.  This allows programs and\nfeatures (such as the chat interface of the messaging application) to execute, provide output on the first device, and/or be used by the first user during the download.  After (and/or during) the first embedded application is downloaded, it can be\nexecuted by the first device.  In some implementations, after the first embedded application has been downloaded, it can remain stored in local non-volatile memory of the first user device, such that upon subsequent initiations of the first embedded\napplication, it is available from the local memory and is not downloaded (e.g., unless updates are available for the first embedded application).  In some implementations, or more parts or components of the first embedded application can be executed by\none or more other devices in communication with the first device, e.g., one or more server systems.  The embedded application code may be provided (e.g., transmitted) specifically to those devices of users that have provided consent for the system to\naccess user data.  In some implementations, because the embedded application is downloaded when initiated, regional and/or time-sensitive embedded applications can be provided from the embedded application server in a particular download, e.g.,\nspecialized for current holidays or calendar events, geographical regions, etc. Access to specialized embedded applications can be restricted based on the particular geographical region, language, and/or other characteristics or restrictions of the user\ndevice and user that is receiving the download.\n In some implementations, the first embedded application causes an embedded interface to be displayed by the first device.  The embedded interface is a user interface to the embedded application allowing user input to be received in the embedded\ninterface to be provided to the first embedded application, and allowing output from the first embedded application to be displayed by the first device.  In some implementations, information from the first embedded application that is to be output to\nother devices over the network can be provided from the embedded application to the messaging application, and the messaging application can send the information from the first device over the network to other devices.\n In some examples, the embedded interface can be displayed in association with the chat interface on a display device of the first device such that at least a portion of the chat interface is visible, e.g., one or more chat messages (such as\nnewly input chat messages) in the chat interface remain visible during display of the embedded interface.  For example, the embedded interface can be displayed as part of the chat interface, e.g., within a display area or borders of the chat interface,\npartially overlapping the chat interface, or adjacent to the chat interface in some implementations.  For example, the embedded interface can include an embedded view displayed within the chat interface.  In some examples, the embedded view can be a\nportion of the chat interface that is dedicated to displaying output data of running embedded application(s) such as the first embedded application and/or dedicated to receiving input to running embedded application(s) such as the first embedded\napplication.  In some examples, the embedded view can be a rectangular or other shaped window or other portion of the display screen.\n In various implementations, an embedded interface can be displayed based on an orientation of the user device, e.g., where a rotation of the user device to landscape mode causes the embedded interface to expand to a fullscreen display, e.g., to\nallow a more immersive game experience, more screen space for an editing task, etc. In some implementations, if an embedded interface is in a fullscreen mode providing a fullscreen display (and not in a partial-screen mode), chat messages input in the\nchat conversation by chat users can be displayed within the fullscreen embedded interface, e.g., as notifications (if the first user has selected to display such chat messages in this way).  In some implementations, suggested responses (described with\nrespect to FIG. 7) can also be displayed within the fullscreen embedded interface.\n In some implementations, a size and/or shape of the embedded interface can be based on a type of the first user device, and/or a type of information displayed by the first embedded application in the embedded interface.  In some examples, if the\nfirst user device is a type that is a mobile device or other device having a small display area (e.g., display screen), the embedded interface can be sized to occupy a particular percentage of the display area and allow at least one chat message to be\ndisplayed in the chat interface.  For a type of device having a larger display area (e.g., desktop computer, larger tablet device, etc.), the embedded interface can be displayed to occupy a smaller percentage of the display area, allowing a larger area\nfor chat messages.  In some implementations, in response to user input selecting a displayed control, the embedded interface can be maximized to occupy the entire display area.  In additional examples, if the embedded application is a media player\napplication that displays a particular type of content data such as movie data and image data (or a game application displaying a particular type of game, e.g., with graphical environments), the embedded interface can be sized larger to view the content\ndata, while if the embedded application displays a type of content data such as lists of text items or outputs audio from audio data, the embedded interface can be sized smaller.  In some implementations, the embedded interface can be displayed as a\nthree-dimensional (3D) display by a display screen, virtual reality display device, etc., or can be displayed in augmented reality display device, etc.\n In some implementations, setup parameters can be input to the embedded application by the first user prior to initiating the embedded session and prior to sharing the embedded application with other chat users.  For example, the first user can\ncreate a new game instance of an embedded game application and set up parameters of the game, select a player name, game board size, player color, the number of players allowed to join that particular session, etc. In further examples, the user can\nsearch a media item menu for a video to play, select a travel destination, flight, hotel, etc. online using the embedded application before sharing the selected options, pre-select shopping items (clothes, furniture) before asking for opinion/votes by\nother member users, etc.\n In block 214, a shared embedded session is initiated.  The embedded session is a designated event dedicated to shared use of a shared embedded application by a group of particular chat devices that are participating in the embedded session and\nin a chat conversation over the network, e.g., a shared communication of information between the particular chat devices and the shared embedded application.  In some implementations, each chat device of the embedded session executes, at least in part,\nan instance of the shared embedded application (e.g., a corresponding embedded application).  The group of associated chat devices, and a group of the chat users that use the associated chat devices, share user interactions with and output of the shared\nembedded application.  For example, one or more states of the corresponding embedded applications executing on the user devices of the group of chat users are synchronized.  In some implementations, the embedded session includes only one or more chat\ndevices from the chat conversation with which the first embedded application is associated, e.g., the chat conversation in which the first embedded application was opened in block 210.  Chat users that have joined an embedded session are \"member users\"\nand their chat devices are \"member devices,\" which are a subset of the chat devices that are participating in the embedded session.  In some implementations, the embedded session can be associated with a particular embedded session identifier (ID) that\nis associated with the identifier of the associated chat conversation.  The session ID can be associated with the current user members of that embedded session.  For example, the user of the first device can be added to a session user list for the\nembedded session, which lists the members of the embedded session.  The first device and/or session server can store and update such a session user list.\n In some implementations, the embedded session can be managed by a server, e.g., (embedded) session server 152, which can be a server system that receives data from one or more member devices participating in the embedded session, synchronizes\nstates of the embedded applications on the member devices, and outputs data to one or more member devices.  In this example, the first device (e.g., the first messaging application) can send data to the session server in response to receiving the input\nin block 210 and/or in response to executing the first embedded application in block 212, where the data indicates to the session server that the first embedded application has been initiated.  In some implementations, the session server can initiate the\nembedded session in response to receiving this data and can execute application(s) that can perform some processing for the embedded session (e.g., store objects, perform processes, etc.).  In some implementations, one or more client devices, e.g., the\nfirst device that initiated the embedded application of an embedded session, can manage the embedded session, and/or member devices can communicate messages between each other without server processing.  For example, data can be sent from each member\ndevice to a message router that relays the data to the other member devices without processing that influences the embedded session.\n In block 216, notification information is transmitted to other devices of the chat conversation indicating that the embedded session is active.  In some implementations, the notification information can be transmitted from the session server to\nother devices participating in the chat conversation (\"chat devices\") over the network, e.g., to all chat devices.  In some implementations, notification information is generated at the session server (or at the first message application) and sent to the\nother chat devices via the session server.  The notification information can include the identification of the embedded session and the identification of the chat conversation, as well as the list of identifications of session users who are currently\nmembers of the embedded session (which in this example is currently the first user).\n In block 218, chat users are enabled to join and/or leave the embedded session, and chat devices can be notified of such joining and leaving events.  For example, the first device can transmit join information to the one or more other user\ndevices participating in the chat conversation (e.g., which can be included in the notification of block 216 in some implementations, or can be transmitted separately from the notification).  The join information can cause display of an interface element\nat the other chat devices that is receptive to user input at each chat device.  In response to receiving user input at the interface element on a particular chat device, the particular device is designated as a member device of the embedded session.  In\nsome example implementations, chat users are enabled to join the embedded session as member users by selecting a displayed \"join\" button presented on their user devices.  In some implementations, a chat user can input a command to join an embedded\nsession, e.g., in response to a notification of block 216 being displayed on the chat user's device.  Some examples of a device joining an embedded session are described below with respect to FIGS. 3 and 4.\n Member users can leave the embedded session by exiting or closing the embedded application and/or by exiting or closing the messaging application.  In some implementations, exiting or closing the messaging application and/or embedded application\ndoes not trigger the user losing membership in the embedded application or session.  In such an implementations, users are still members of the embedded application, and are simply indicated as being offline, e.g., in a chat interface or embedded\ninterface displayed on other member devices.  In examples of such implementations, users can leave the embedded session (discontinue embedded application membership) by leaving the chat conversation the embedded application is associated with.  In some\nexamples, chat devices are sent a notification indicating that a user has joined or left an embedded session and the identity of the user (or user device).  The notification can be sent to chat devices that may or may not be member devices of the\nembedded session.  In some implementations, only the devices of member users are sent the notification in response to a member user of the embedded session leaving the session.\n Each of the member devices that has joined the embedded session downloads and/or executes a respective embedded application (corresponding to the first embedded application and the embedded session) that provides a respective embedded interface\nassociated with a respective chat interface displayed on the respective particular user device.  The embedded applications of the member devices communicate with each other in the embedded session over the network.\n In block 220, the first embedded application executing on the first device operates to receive input, update states, and provide output.  For example, the input received by the first embedded application can include user input provided from the\nuser of the first device.  In some examples, the user input can be received in a displayed embedded interface, e.g., to select elements displayed within the embedded interface, specify directional data with a controller or gesture, specify text data,\netc. In additional examples, the user input can be received in the chat interface, e.g., text commands input as messages or selection of interface elements in the chat interface.  Input received in the chat interface can be processed and/or conveyed from\nthe messaging application to the embedded application.  In some implementations, the embedded application can have access to chat conversation information (e.g., user names or chat identities, user icons, etc.) and/or access to user profile information\n(user name, profile picture, chat obfuscated ID) which can allows for a personalized experience.  In some implementations, the embedded application does not have access to the chat stream (e.g., the chat messages input in the chat conversation).  In\nother implementations, the embedded application can have full access to the chat stream, and/or the embedded application can request higher-level access to chat input (to be permitted by the user), and/or can be provided summaries by the messaging\napplication of chat messages input by chat users in the chat conversation.  In additional examples, the embedded application can directly read the input chat messages from a chat conversation database associated with the chat conversation (e.g., stored\non the user device or a server), and/or can receive chat messages and other user input in the chat interface via a server or bot.  The user input can be touchscreen input, text input provided from a displayed keyboard or physical keyboard, commands\nprovided from voice input, or other types of input.\n Furthermore, the first embedded application can receive input (data) including event information received over the network from member devices of the embedded session.  For example, event information can include user input provided by other\nmember users to associated embedded interfaces displayed on their devices by embedded applications executing for those devices and corresponding to the first embedded application.  This user input and/or other data (e.g., updates to or states of the\nembedded application based on that user input), can be sent to the first user device over the network.\n In addition, input can include event information indicating one or more events and/or application states occurring in one or more corresponding embedded applications executing on one or more other member devices (and/or a session server), and\nthis event information can be sent to the first user device over the network.  For example, the event information can describe events occurring within a corresponding embedded application running on a particular device based on user input received by\nthat embedded application, and this information is sent to the other member devices to synchronize the embedded applications of the embedded session.  In some implementations, the event information from other member devices can be conveyed by a server\nsystem to the first embedded application (e.g., the messaging server, session server, etc.).  In some implementations, input is received by the first messaging application of the first device, which provides the information to the first embedded\napplication.\n Furthermore, the first embedded application can receive content data from one or more other devices (e.g., member devices, server systems, etc.) over the network.  For example, if the first embedded application is a video playing application, it\ncan receive a stream of video data from a content server over the network and display the video data in the embedded interface.  If the first embedded application is a game, it can receive game data from other member devices and/or a game server and\ndisplay game data in the embedded interface.  If the first embedded application displays a shared content document (e.g., list of items or events, text document, etc.), travel itinerary, or other information, it can receive the content data of these\nitems from other member devices and/or a server system and display the content data in the embedded interface.  In some implementations, the content data is received by the first messaging application of the first device, which provides the content data\nto the first embedded application.\n In some examples, the first embedded application is updated based on the input.  For example, the first embedded application can receive data based on input data received from the first user and/or one or more member devices (and/or server\nsystem) that, e.g., change one or more states of the embedded application.  In some examples of changed states, playback of a media item may be updated to a different point in the media item (e.g., a different time point in a video), a game may be\nupdated to one or more different game states (e.g., a state of the game indicating a different position of a game piece or player character in a game environment, a state of the game indicating a win, loss, or change in a player's score, etc.), a shared\nlist may be updated to add, remove, or change one or more items on the list, a reservation of a theater seat may be input, changed in time or place, or removed, etc. Similarly, input can include event information describing an event of the embedded\nsession (e.g., an event in a corresponding embedded application of a member device or a server providing data in the embedded session), such as an event occurring in a game based on user input (e.g., a game event determined by game code running at a\ndifferent member device or a server).  This event information can cause the first embedded application to update its state to synchronize with the embedded session, e.g., synchronize with one or more corresponding embedded applications.  In some\nimplementations, other embedded applications of other member devices can be similarly updated based on the input received at the respective member devices and event information.\n Output provided by the embedded application be based on received input and/or events occurring in the embedded application.  For example, the output can include output data displayed in the embedded interface.  For example, the embedded\ninterface can display interface elements, controls and other features provided by the embedded application.  In another example, an embedded application can cause data to be displayed in the embedded interface to provide a graphical game environment,\nthree dimensional (3-D) environment, etc. (such as display on a display screen(s), display by virtual reality and/or augmented reality display devices, etc.).  In some implementations, the output can include data received by the first embedded\napplication and the first device from a different device, e.g., content data received from a server such a content server 154 or from a member device of the embedded session.\n In some implementations, the output data displayed by the embedded interface can be customized for the particular user of the device on which it is displayed.  For example, the first user may have stored user preferences indicating a location,\nsize, shape, etc. of the embedded interface on the display, and a user-preferred color scheme or other visual features of output data displayed in the embedded interface.  In some examples, the first user may have stored user preferences specific to a\nparticular embedded application, e.g., preferences indicating a user-preferred user status or role in a specified embedded application, a user-preferred interface element layout for particular types of embedded applications, etc. Such preferences can,\nfor example, be provided from the messaging application to the embedded application and implemented by the embedded application.\n In some examples, the output provided from the first embedded application can include visual updates to the embedded interface (e.g., embedded view) displayed by the first device.  For example, media data playable (e.g., image or video data) at\na new time point can be displayed based on an update to that time point, one or more changes to a displayed game can be displayed, changes to a content document (list, document, chart, etc.) can be displayed, etc.\n In further examples, the first embedded application can process content data from the chat conversation (e.g., parse and/or otherwise process chat messages provided by the messaging application) to present output data in the embedded interface,\nand/or an embedded interface itself, that is contextually relevant to one or more topics mentioned in the associated chat conversation.  For example, chat messages in the conversation may have included indications of a particular media item to play, and\nthat media item can be automatically selected and played by the embedded application.  In another example, the chat messages can indicate that a user desires to play on a particular side or team in a game, and the user can be automatically assigned to\nthat team by the embedded application.  Multiple visual appearances of the embedded interface (e.g. colors, visual styles, etc.), and/or different available selections or controls displayed in the embedded interface, may be provided in different versions\nof the embedded interface.  For example, the embedded interface may be displayed in different versions for different member devices, based on a user status or role of the user, type of media item played (e.g., video vs.  image), etc.\n In additional examples, an embedded application can summarize conversations and automatically store important information that can be displayed at a later time.  For example, a user can open a \"summary\" embedded application and the embedded\napplication can display a summary overview of the messages input by chat users of a conversation over a period of time specified by the user and/or a period of time that the user was offline, instead of (or in addition to) displaying a record of many\nchat messages to read.\n In additional examples, the embedded application can be a \"lightweight\" (e.g., reduced-feature or reduced-functionality) version of a full application that executes on the user device or on a different devices.  The lightweight version is a\nversion of the full application that requires less storage space, memory to execute, etc., can be executed without launching the full application, and may have a subset (e.g., fewer than the full set) of the features and/or functions of the full\napplication.  In some examples, a full game application can be installed on a different device of the first user's (e.g., a desktop or laptop computer) and a lightweight version of the game application can be executed as an embedded application on the\nuser device.  Such a lightweight game application can allow the first user to provide user input in the embedded interface to change game settings or game data relating to the first user or the first user's account used in the full game application. \nSuch changes can include managing game resources of the first user used in the full game application, e.g., organizing inventory of game items, buying or selling items within the game, allocating points for particular game abilities of the first user's\naccount, changing preferences or display settings, perform simple game actions, etc. Changes made by the user with the lightweight application are saved to the full game application, e.g., saved in the user's account at a server, and accessed by the full\ngame application when that full application is executing and can, e.g., synchronize with the server.  Similarly, changes can be made with a lightweight version for a media item player program, graphical editing program, word processor, or other\napplication program.  In some implementations, other member users in an embedded session can share the lightweight application to adjust their data and settings of the full application and/or change the first user's data and settings, e.g., when the\nfirst user is not online.  In some examples, the lightweight version embedded application can provide a shared, second-screen experience on user devices of an application that executes on different devices.\n In some implementations, an embedded application can trigger initiation and execution of an external application, e.g., a different embedded application or other application external to the messaging application, on the first user device or\nother device in communication with the first user device.  For example, the external application can be a full application corresponding to a lightweight version implemented by the embedded application, similarly as described above.  In some examples,\nthe embedded application can initiate the external application to complete processing a task (e.g., determine the result of a transaction, provide a rendered display bitmap, etc.).  In some implementations, an embedded application can provide selectable\ndata (e.g., words, links, buttons, etc.) to be displayed in the chat interface and chat conversation.  For example, the embedded application can provide the selectable data to the messaging application, and the messaging application can display the\nselectable data in the chat interface and chat conversation.  The selectable content is selectable by user input, e.g., can be clicked on via a touchscreen or other input device.  When selected by user input, the selectable data can cause a command to be\nsent to the operating system of the device to initiate and execute the external application (e.g., on the user device), where the other application can be identified in or associated with the selectable data (e.g., a default application associated with a\ntype of the selectable data).\n In further examples, when initiating the external application as described above, associated data such as parameters, configuration settings, and/or arguments can be passed to the external application to present particular data from the external\napplication.  In some examples, the embedded application can generate a map icon associated with a specific address, and the icon is displayed in the chat interface.  When the icon is selected by user input, a map application program is initiated and\ndisplays a map in a view that includes a selected location of the associated address.  In additional examples, an external shopping application can be initiated by selection of an item description sent to the chat interface by the embedded application,\nand a checkout shopping display of the external shopping application can include information of the user automatically filled into fields of the shopping display.  In further examples, users select a hotel location and room using a shared embedded\nreservation application, and selecting a displayed control (e.g., \"reserve hotel room\" button) in the chat interface or embedded interface opens an external full reservation application on the user device.  The full reservation application displays a\nreservation interface that is automatically filled in with the reservation information from the embedded application, and a user can select a \"pay\" button in the reservation interface to provide payment to the hotel for the reservation.\n In some implementations, the first embedded application can detect one or more external applications that are installed on the first user device and modify content data that is displayed on the first user device by one or more of the external\napplications.  For example, the embedded application may be a lightweight version (e.g., lightweight application) of a full version (external) application (full application).  For example, if the lightweight application on the first user device detects\nthat the corresponding full application is installed on the first user device, then the lightweight application can enable special features (e.g., additional controls, displayed data, etc.) in the full application by communicating with the full\napplication, e.g., via the messaging application.  In some implementations, a complementary relationship can be provided between the full application and the lightweight application (embedded application).  In some examples, a full application can\ncommunicate data in the background (e.g., not displayed by the first user device), such that the lightweight application can directly communicate with the full application in the background to access or exchange data.  This communication can trigger\ndisplayed data and options in the lightweight application and/or full application, e.g., with no need for additional authentication between the lightweight application and an authentication server.  For example, a login process on a full reservation\napplication can be triggered by the corresponding lightweight application using the full application account of the user registered within the full application on the user device and not requiring the user re-authenticate within the lightweight\napplication to perform a reservation.\n In some implementations, the output from the first embedded application can include data provided to the chat interface of the first device and/or provided to the chat conversation, e.g., data that is displayed on the chat interfaces of chat\ndevices participating in the chat conversation.  For example, the first embedded application can provide data to the first messaging application, which outputs the data in the chat interface and/or to the other chat devices.  In some examples, the\nembedded application can send data describing events or notifications to the message application (and/or the message application can receive data describing events and notifications from other devices), and the message application can display the data\ndescribing the events or notifications in the chat conversation to improve the engagement, retention, and awareness of users about the occurrence and events within embedded sessions.  For example, output to the chat interface can be provided as text or\nother type of chat message (or image, video, audio, etc.).  The output to the chat conversation can be transmitted to all chat devices such that it is displayed in the chat interface of all the user devices participating in the chat conversation.  In\nsome implementations, the output to the chat conversation can be transmitted to a subset of the participating chat devices to display the output.  For example, the output can be provided in the chat conversation and visible to member users, or to\nparticular chat users selected or designated by the first user.  In some implementations, such data output to the chat interface and chat conversation can be shared outside of the messaging application, e.g., to other applications executing on the first\nuser device.  In some examples, such data shared outside the messaging application can, when selected by a user or processed by a different application, cause the messaging application to open or move to the foreground on the user device, and can cause\nthe embedded application to execute and the user to join the embedded session.\n In some examples, notifications output to the chat conversation can include descriptions of events occurring in the embedded session related to user actions, content data, events in a game, etc. provided by the embedded application in the\nsession.  For example, such notifications can include, \"User1 and User2 are watching the party scene movie Adventure in Time,\" (triggered by the scene being played), \"User1 re-took the lead score in the Toads game!\" (triggered by this game event), or\n\"User2 just reserved tickets to the Jubilee concert!\" (triggered by the user reserving these items in the embedded application).  In some implementations, the notifications can include text, images, videos, audio output, cinemagraphs, or other types of\ncontent data.  In some implementations, such descriptions of events can be displayed on interactive cards, described below, which can allow a chat user to provide input to the notification/card to join the embedded session or provide input to the\nembedded application.  In additional examples, the embedded application can have access to a list of the chat users (e.g., obfuscated chat IDs) in the chat conversation and can issue invites to the chat conversation to bring non-member chat users to join\nthe embedded session, where, e.g., the non-member chat users can select the invite to cause their device to join the embedded session.\n In some examples, the embedded application can send chat messages to the chat conversation on behalf of the user with user's approval, as described below).  In some implementations, the attribution of such messages can be to the embedded\napplication and not to the user's chat identity, e.g., the embedded application can assume a chat identity of its own as the originator of its chat messages.  Such chat messages can include themes, text, images, videos, or other type of content data\nresulting from editing performed by the user in the embedded application, web content snippets edited by a user in the embedded application, etc. In some examples, the attribution or name of the embedded application, as displayed in the chat interface of\nother chat devices, can be selectable by user input, and if selected, causes the embedded application to open to edit the same shared content data or edit another content data item.\n The embedded interface can present displayed options to modify, setup or enable additional features to the chat conversation.  For example, such features can include an auto-reply feature (selecting a random suggested reply or auto-answering\nquestions with prior answers from the first user to prior similar questions), censoring or replacing particular words before the words are sent to the chat conversation or received for display on the first device, etc. For example, the embedded\napplication can use APIs to modify the chat conversation experience for the user.  Thus, in some implementations, the embedded application provides a visual interface enabling modified settings and/or provided features within the messaging application\n(e.g., providing messaging application features instead of--or in addition to--providing shared content data for chat users).\n In additional examples, notifications (or other data) output to the chat conversation can include messages based on chat messages input by chat users.  For example, the first embedded application can check a list of chat messages input by a user\nin the chat conversation, determine usage of slang vs.  proper vocabulary in the chat messages (e.g., by consulting a stored dictionary, using a machine learning model trained with slang and vocabulary, etc.), and output a rating in a notification\nindicating a score for the user's writing in the chat.  The embedded application can determine and indicate in notifications, e.g., the frequency of usage of particular words by the user in chat messages, or a challenge to the user to use particular\nwords (and can verify that the user has used the particular words in later notifications).  In some implementations, notifications output to the chat conversation can include messages from one or more member users in the embedded session to a different\nmember user as an indication of events occurring in the embedded session, e.g., invitations, challenges to other users (\"beat my score,\" \"guess my drawing,\" \"I built a tower, try to destroy it\"), or boasting/whining messages or notifications (e.g., a\nplayer killed another player's character, a player complaining that another player stole their pet, etc.).\n In some additional examples, an embedded application having access to the chat conversation message stream can provide information related to the chat conversation, e.g., a high-level summary of the conversation, a record or list of content data\nthat was shared in the chat conversation by chat users, displaying (e.g., pinning) controls or other options in the chat interface or embedded interface allowing a user to select collected topics, information, or shared content data from the chat\nconversation to save for later, to save on a favorites list of the user, etc. For example, the embedded application (e.g., a particular \"summarizer\" type of embedded application) can detect topics shared in the chat conversation and provide reminders,\nsummaries, and/or actions related to those topics.  In some examples, User 1 tells User 2, \"don't forget Kenny's birthday on Saturday\" and the embedded application, when initiated, automatically adds a calendar event to the calendars of User 1 and User 2\nstored on their user devices, or displays a suggested command or notification that causes the embedded application to add the calendar event when the command or notification is selected by the user.  In another example, User 3 inputs a message in the\nchat conversation that includes an address or location of a restaurant for next week's reunion dinner, and the embedded application auto-pins (persistently displays) in the chat interface, or otherwise stores, the location as a summary so that the first\nuser does not have to review and scroll through old chat messages a week later to find the location.  For example, summaries can include descriptive and identifying information (e.g., times, locations, names, phone numbers, addresses, etc.) input into\nthe chat conversation, and/or can include messages that received reactions from other chat users in the chat conversation (e.g., user approvals, exclamatory messages, etc.), In another example, User 1 shares images captured from the latest company trip\nin the chat conversation, and the embedded application automatically (with user consent) uploads the images to a shared group photo album \"Company trip to Vegas\" that is shared with all the chat users (e.g., stored on a server over the network).  In some\nimplementations, if an embedded application does not have the capability to execute such actions based on chat conversation content data, it can trigger the initiation and execution of a different embedded application (or other application) that has that\ncapability, and which executes the actions.  In some implementations, one or more such actions of the embedded application can be presented as suggested commands and/or suggested messages which can be selected by the first user to implement the selected\ncommands, as described herein.\n In some implementations, the notification to the chat conversation can be provided in response to one or more particular events or particular types of events that have occurred on a server (e.g., server events) in communication with the\nmessaging application and/or embedded application over the network, e.g., events that have been processed in a server application executing on the server.  The server can send server event information to one or more of the member devices such that the\nmessaging application displays the server event information in the chat conversation.  In some examples, this server can manage second content data for other network users, where the second content data is the same or related to first content data that\nthe server is providing to the embedded application on the first user device (e.g., media item data, game data, etc.).  For example, the server may send the second content data over a network to one or more other users accessing the content data on other\nuser devices (e.g., that are not participating in the chat conversation) via other embedded applications and/or other mechanisms (e.g., web sites, dedicated viewing applications on user devices, etc.).  The server can detect whether the second content\ndata is the same or similar to the first content data provided to the embedded application (e.g., same title of media item, game, etc.).  Furthermore, the server can detect user actions associated with the first content data and second content data,\nwhich may qualify as server events.  For example, the server can detect that the second content data has received a number of favorable user ratings from other users, e.g., over a threshold number of favorable ratings.  This can be predefined as a server\nevent, causing the server to send server event information (e.g., \"this movie has received 1 million approvals!\") to the first embedded application to display in the embedded interface, and/or to the messaging application to display in the chat\ninterface.\n In some implementations, if user consent has been obtained, output provided from the embedded application can include input information based on user input received from the first user in the embedded interface, where the messaging application\nsends the input information over the network to a server, e.g., for display at other member devices, etc. The messaging application can also or alternatively determine and send input information to the server (if user consent has been obtained), where\nthe input information is based on user input that was received in the chat interface outside the embedded interface.  For example, the user input can be a chat message, user comments or ratings associated with a content item (e.g., media item or game)\nbeing played in the embedded session, etc. In some implementations, these comments or ratings can be received by the server and associated with the content item that the server provides to devices.  The server can provide the comments or ratings to user\ndevices accessing the content item on the server.  For example, other user devices that are not participating in the chat conversation may access that content item on the server, e.g., via different embedded sessions or other applications.  Such other\nuser devices can also access the comments or ratings provided by the first user.  For example, user input such as user comments or ratings can be sent to the other user devices and displayed in embedded interfaces or other application programs on the\nother user devices.\n In some implementations, if user consent has been obtained, the input information sent to the server can include information indicating the particular content items that have been played in embedded applications on the first device.  The server\ncan update a stored history of user selections of content items based on the input information, where the history is associated with the first user of the first device.  The stored history can also indicate user selections of content items stored by the\nserver and viewed in other applications used by the first user.  In some implementations, the server can use the stored history to determine suggested content items for the user that are related (e.g., in genre, viewings by other users, etc.) to content\nitems previously accessed on the server by the first user.  In some implementations, such server-suggested content items can be displayed in the chat interface and/or by an embedded application that plays the type of the content items.\n In addition, information generated by an embedded application, bot (see bot descriptions below), or server can be updated by the embedded application, bot, or server if new conditions are triggered.  In an example, User 1 uses an embedded media\nplayer application to share a video in an embedded session in the chat conversation.  User 2 opens a corresponding embedded media player application at his or her device and comments on or rates the video with a rating.  The embedded application, bot, or\nserver can update the shared video, and update a notification describing the video that is displayed in the chat conversation to reflect and propagate the new information to the chat users (e.g., display an indication that the video now has +1 user\napprovals, +1 user comments).  In some implementations, updatable \"interactive cards\" about an embedded session are shared in the chat conversations, which can be controlled by a server, bot or embedded application.\n In some implementations, when output data from the first embedded application is to be output to the chat conversation and/or to a server for display, a prompt is displayed (e.g., with a user-selectable option) to allow the first user to select\nwhether to allow the data to be output in the chat conversation or not.  For example, the prompt can display the message that is to be sent to the chat conversation or server if approved by the user.  In some implementations, the prompt can be displayed\nfor particular types of output data to the chat conversation (e.g., particular types of messages, having predefined keywords, indicating events of particular types, output data that is a response to a particular type of action or command provided by a\nmember user to the embedded application, etc.) and not displayed for other types.  The prompt can be displayed for particular types of embedded applications, and not displayed for other types.  In some implementations, the first user may have stored\naccessible user preferences that are accessed to determine the conditions under which a prompt is to be displayed.  For example, user preferences can indicate that the prompt should ask for user consent to send output from a game types of embedded\napplications to the chat conversation, and not provide the prompt for a media player types of applications or other types of applications.\n Output can also include data transmitted to corresponding embedded applications executing on other member devices over the network, e.g., via an associated messaging application and/or a server system.  For example, the transmitted data can\ninclude user input (or data derived therefrom) received by the first embedded application from the first user, e.g., selecting elements or changing states in the first embedded application.  The transmitted data can include event information indicating\none or more events occurring in the first embedded application (e.g., based on user input from the first user, program events based on execution of code and particular conditions, or other events occurring local to the first embedded application),\nsimilarly as described above.  The data can be provided to the first messaging application, which can transmit the data (or a processed form of the data) over the network to other devices.  For example, the data can be sent to a message router, e.g., a\nsession server as described above, which can send the data to other member devices, and/or the data can be sent directly to one or more member devices.  The other member devices can receive and use the user input and/or event information to update the\nstates of the corresponding embedded applications to synchronize with the first embedded application.\n In some examples, the output from the embedded application can include event data that indicates one or more particular events (or types of events) that have occurred in the embedded session.  The event data can be provided from the embedded\napplication to the messaging application, which can generate a notification based on the event data and causes the notification to be displayed in the chat interface and sent over the network to be displayed in the chat conversation (e.g., chat\ninterfaces) for member devices and/or chat devices.  For example, the events may have occurred in the first embedded application.  In various examples, the events in the embedded session may have occurred in response to the first user input from the\nfirst user and other user input received from the one or more member users of the embedded session.  In some examples, types of events may be changing a playback position of a media item, or an initiation or ceasing of playback of a media item in an\nembedded media player application; achieving an objective, ending a game or a particular phase or portion of game duration, or other game event in a game provided by an embedded game application; modifying a content document (e.g., an item in a shared\nlist) in an embedded list application; reserving a seat via an embedded reservation application; etc. In some implementations, the events in the embedded session may have occurred on a server managing the embedded session (e.g., a session server), which\nprovides the event data to the messaging application, which can generate a notification for display in the chat conversation.  Types of events on such a server may include reception of input data at the server over the network from other users external\nto the chat conversation, availability of a new content item related to a content item playing in the embedded session (e.g., the new media item has the same artist, author, actors, production company, genre, etc. as the playing media item, or the new\ncontent item is known to be on a favorites list of one or more member users, if they have provided consent), a disconnection of one or more member users from the server, etc.\n In some examples, the embedded application can output control information (e.g., via the messaging application) to an external application executing (or initiated) on the first user device or executing on a different device in communication with\nthe first user device over the network.  In some examples, the embedded application can output control information to applications that control home appliances, vehicle functions (lock/unlock doors, change thermostat, etc.), or other types of devices. \nFor example, such control information can be output in response to selection of displayed controls in the embedded interface (e.g., buttons, dials, etc.), or automatically in response to chat messages input by the first user (e.g., \"open my front door\",\ncausing output of control information to unlock the door of the user's house) or based on chat messages in the chat conversation input by chat users (e.g., messages discussing going into a car, causing the embedded application to send control signals to\nunlock doors of that car).  In another example, the embedded application can send application data to affect game states of a console/desktop game in response to receiving input from the first user and/or other chat users in the embedded interface or in\nthe chat conversation.  For example, the chat users may select controls in the embedded interface to send data to a server, where the data is read by a game application on a different game device to change game resources (as described above), or start a\ngame on the game device, perform game actions in the game, etc. In another example, the embedded application can automatically send information to the game that causes game characters of the chat users to be formed into a group in the game based on chat\nmessages from the chat users in the chat conversation (e.g., \"let's join up in the game\").  The chat conversation provided on the chat devices can act as a chat interface between players of the game executing on the game device.  In some implementations,\ncontrol information can be presented in the chat interface as suggested commands for the external application, as described below, which can be sent to the external application in response to selection by user input.\n In additional examples, the embedded application can provide data to the messaging application to permanently or temporarily modify a visual representation of the first user in the chat conversation.  For example, a user status or role can be\ndisplayed next to every message or user name (e.g., \"User1--player\") or a title bestowed by the embedded application (e.g., \"User1--the Conqueror\" for a game).  In additional examples, a filter can be applied to modify a user profile image appearing in\nthe chat interface and/or modify chat messages input by the first user (e.g., highlighted with colors, borders, frames, stickers, etc.).  Such modifications can be provided while the first user is participating in the embedded session and/or as rewards\nfor achievements within the embedded application.  Rewards can include unlocking other embedded applications for use by all chat users.  The integration between the messaging application and the embedded application via exposed APIs can allow such\nfeatures.\n Some embedded applications can change a displayed keyboard layout, displayed buttons, or other displayed controls in the chat interface (or controls provided by the operating system or other interface displayed by the user device) based on the\ntype of embedded application and/or in response to events in the embedded application or chat conversation.  For example, a layout of a keyboard displayed in the chat interface can be changed to match a game controller specialized for a particular\nembedded game application.  In another example, events such as achieving an objective in a game, or being defeated in a game, can cause one or more displayed buttons to be added to or removed from the chat interface in accordance with increased or\ndecreased options in the game for the user.  In another example, when a user in inputting a text query to perform a search for an item (e.g., a product), as more characters are input, the embedded application can cause additional suggested characters for\nthe query that do not match any search results to be removed from display, or displayed with lower visibility than matching characters (e.g., grayed out).\n In additional examples, embedded applications of various types can execute in association with the messaging application.  For example, an embedded media player application can receive user input from member users in an associated embedded\ninterface and/or chat interface.  The input can select playback functions of media items such as videos, images, audio data, documents, etc., and output from the embedded application can provide playback of the media item.  An embedded game application\ncan receive user input from member users which can change game states, and output of the application can include visual, auditory, and tactile output indicating a game environment and game states.  An embedded shared document application can receive user\ninput from members users which can change a shared content document (e.g., list or other document) and output of the embedded application can display the updated content document.  An embedded travel itinerary application can receive user input from\nmembers users which can select travel destinations, routes, times, and other travel-related preferences, and the output of the embedded application can include display of available travel routes, options to select particular routes, times, and method of\ntravel with user input, etc. An embedded reservation application can display selectable options allowing member users to reserve places or attendance at hotels, restaurants, movies, conventions, rides, parks, entertainment performances, and other future\nevents.  For example, such an embedded application can receive user input from member users that specifies times, places, and events at which to reserve places (e.g., including user selections of seats, tables, or other places in a graphical display),\nand the embedded application can output suggested events and dates/times, indications of reservations being successfully made or denied, etc. In some implementations, various types of embedded applications (e.g., having different functions or purposes)\ncan be combined into a single embedded application.\n In block 222, it is determined whether the first messaging application has been exited at the first device.  For example, user input received by the first device may close the messaging application.  If the first embedded application has been\nexited, then in block 224, the chat interface is removed from the display of the first device, including removal of the embedded interface.  In some implementations, the embedded application is also removed from memory of the first device.  In addition,\ninformation can be sent to a device managing the embedded session, e.g., the session server, to indicate that the first device has exited the embedded session.  In some implementations, the managing device (e.g., session server) updates the session user\nlist to remove the first user and provides this update to other member devices of the embedded session.\n If the messaging application has not been exited, then in block 226 it is checked whether the first embedded application has been closed at the first device.  For example, user input can be received at the first device from the first user that\nselects a displayed element in the chat interface or embedded interface to close the embedded application.  If the first embedded application has been closed, then in block 228 the embedded interface is removed from the display of the first device.  In\nsome examples, the removal of the embedded interface allows the chat interface to occupy the area formerly occupied by the embedded interface.  In addition, information can be sent to a device managing the embedded session (e.g., session server) to\nindicate that the first device has exited the embedded session, similarly as described for block 224.\n In block 230, in some implementations, an indication is displayed in the chat interface indicating that the embedded application is in use by one or more other member devices, if such a condition exists.  For example, the indication can be an\nicon or other element displayed in the chat interface while the embedded application is continued to be used in the embedded session that was exited by the first user.  In some implementations, the indication can be selected by user input to cause the\nfirst user and first device to re-join the embedded session of the embedded application, e.g., re-execute the embedded application from code stored in memory of the first device and/or download and execute the embedded application.  Some implementations\ncan remove the displayed indication if all users have exited the embedded application, and/or if no member users have interacted with (e.g., provided input to) the embedded session or embedded applications after a threshold period of time.\n In some implementations, multiple embedded applications can be executing simultaneously and multiple embedded interfaces can be displayed by the first user device in association with the chat interface (e.g., similarly as for the embedded\ninterface as described above).  In some examples, multiple embedded applications can execute simultaneously on the first user device.  In some implementations, the multiple embedded applications can communicate data to each other.  For example, one\nembedded application can send a command to the other embedded application to cause the other embedded application to update one or more states, remove or add a member user, etc.\n In some implementations, the messaging application can manage a conversation \"currency.\" This can be a virtual currency that can be generated by embedded applications, earned by chat users, and exchanged with other chat users within the embedded\napplications and/or within the messaging application.  In some examples, a user can receive tokens or \"coins\" when performing particular tasks or goals in embedded applications, e.g., completing an objective in a game, editing a video or image for a\nthreshold amount of time or threshold number of edit operations, playing a media item for a threshold amount of time, editing a shared list, completing a travel reservation, etc. In various implementations, the currency can be spent by a user to obtain\nrewards, e.g., the ability to change visual features of the chat interface, select new options, or use other features of the chat interface.  In additional examples, the currency can be spent to enable use of (e.g., unlock access to) visual text\ncharacters, icons, or images that can be inserted into chat conversations (e.g., \"sticker packs\"), use of particular conversation features (e.g., change message text fonts, add colors or titles to chat identities, use a particular image or avatar in the\nchat conversation, etc.), use of particular embedded applications, etc. In some implementations, the currency can be used to unlock or improve features in embedded applications (e.g., use of particular avatars or characters in games, access to particular\nmedia items with a monetary discount, use of different displayed appearances for embedded interfaces, etc.).  In another example, a user can send a particular amount of the currency to a different chat user, or can transfer (or copy) obtained rewards or\naccess rights to a different chat user in exchange for an amount of the currency.  In some implementations, the currency can be converted to a real currency, allowing for person-to-person payments (e.g., splitting a bill at a restaurant, or small debt\nsettling).\n FIG. 3 is a flow diagram illustrating an example method 300 to enable a user to join an embedded session, according to some implementations.  In some examples, method 300 can be included in block 218 of FIG. 2.  In some implementations, method\n300 can be implemented, for example, on a server system, e.g., messaging server 101, as shown in FIG. 1.  In some implementations, some or all of the method 300 can be implemented on a system such as one or more client devices 115 as shown in FIG. 1,\nand/or on both a server system and one or more client systems.  In described examples, the implementing system includes one or more processors or processing circuitry, and one or more storage devices such as a database or other accessible storage.  In\nsome implementations, different components of one or more servers and/or clients can perform different blocks or other parts of the method 300.\n In some implementations, method 300 can be implemented on a different user device than the method 200 of FIG. 2, e.g., a second user device.  For example, the second device can communicate with the first device of FIG. 2 over a network.  The\nsecond device is participating in a chat conversation, and the embedded session has been initiated by a chat user of a different device participating the chat conversation.  In the example of FIG. 3, the embedded session was initiated in association with\nthe first embedded application of the first device as described with reference to FIG. 2, and the second device is participating in the same chat conversation in which the first user and first device are participating.\n In block 302, it is checked whether user consent (e.g., user permission) has been obtained to use user data in the implementation of method 300.  For example, user data can include messages sent or received by a user, e.g., using messaging\napplication 103, user preferences, user biometric information, user characteristics (identity, name, age, gender, profession, etc.), information about a user's social network and contacts, social and other types of actions and activities, content,\nratings, and opinions created or submitted by a user, a user's current location, historical user data, images generated, received, and/or accessed by a user, videos viewed or shared by a user, etc. One or more blocks of the methods described herein may\nuse such user data in some implementations.\n If user consent has been obtained from the relevant users for which user data may be used in the method 300, then in block 304, it is determined that the blocks of the methods herein can be implemented with possible use of user data as described\nfor those blocks, and the method continues to block 308.  If user consent has not been obtained, it is determined in block 306 that blocks are to be implemented without use of user data, and the method continues to block 308.  In some implementations, if\nuser consent has not been obtained, blocks are to be implemented without use of user data and with synthetic data and/or generic or publicly-accessible and publicly-usable data.\n In block 308, an indication (e.g., notification) is received at a second messaging application of a second device that an embedded session for an embedded application is active.  In some examples, the indication can be data indicating the\nparticular embedded application that has been initiated and indicating that an associated embedded session has been initiated.  In some implementations, the indication can include an identification of the chat user from the chat conversation that\ninitiated the embedded session.  In some implementations, the data of the indication may have been sent by a session server, e.g., after the embedded session was initiated.\n In some implementations, identifications (e.g., user IDs) of the chat users who are currently members of the embedded session (e.g., who have joined the embedded session) are included in the received indication.  For example, the list of session\nusers described with reference to FIG. 1 can be provided in the indication.  Alternatively, the current number of member users can be indicated without the identifications of the member users.  In some implementations, the received indication can include\nadditional information.  For example, the information can include the amount of time since the embedded session was initiated, one or more current states of the embedded application(s) in the embedded session, and/or other information related to the\nembedded session.  In some examples, the additional information can include the name of media item(s) currently being played, in queue to be played, or otherwise being processed for the embedded session, the current number of items of a shared list (or\nother shared document data) being edited in the embedded session, statuses of one or more of the member users with regard to the embedded session (e.g., identifying users as controllers, players, observers, etc., and/or identifying a user who is\ncurrently editing a document or winning a game implemented in the embedded session, etc.), a list of objectives so far achieved in the embedded session (e.g., finished playing a video, game objectives completed, a list is finalized, etc.), a specific\nrequest from one or more current member users of the session for additional user members to join the embedded session, etc. In some implementations, the additional information can include messages from one or more member users in the embedded session to\nthe second user, or to other member users as an indication of events occurring in the embedded session, e.g., invitations, challenges to other users (\"beat my score,\" \"guess my drawing,\" \"I built a tower try to destroy it\"), boasting or whining\nnotifications (e.g., a player killed another player's character, a player complaining that another player stole their pet, etc.).  In some implementations, updates to this information can be received by the second device at later times, e.g., updates to\nthe current number of member users, a current state of the embedded application, etc.\n In block 310, the second messaging application causes display (or other type of presentation or output) of a notification and a join option by the second device.  The notification can include one or more items of information that were received\nin block 308, e.g., descriptive information about the active embedded session indicated in block 308.  For example, such items of information can include the name or identifier of the embedded application used in the embedded session, content data being\nshared (e.g., media item played, content document edited, etc.), current member users who have joined the embedded session, statuses of one or more of the member users with regard to the embedded session, current state(s) of a participating embedded\napplication, messages from member users, and/or other information received in the indication of block 308.  In additional examples, information in the notification can refer to content data or states of the embedded application (playback position of\nmedia item, high scores in a game, game state information) or content data of the chat conversation (as allowed by user privacy controls).  Further, notification information can include embedded application download size and application description,\nindication that user permission is requested for use of user data and the types of data (e.g., user data) the embedded application will access, a list of users that have already joined the embedded session, etc. In some implementations, the displayed\nnotification can take the form of an \"interactive card\" as described herein.\n The join option is a selectable option that, if selected by user input, allows the second user to join the active embedded session.  In some implementations, the join option can be an interface element displayed in the chat interface of the\nmessaging application where data of the chat conversation is being displayed.  For example, the join option can be displayed as a selectable control (e.g., button) in the notification displayed in the chat interface.\n In some implementations, the join option can be displayed, or can be highlighted (e.g., displayed in a bright color, flashing, etc.) or otherwise emphasized in the display, if one or more conditions are met.  For example, the conditions can be\nspecified in stored user preferences of the second user.  Such conditions can include, e.g., the embedded session is a specified type or is associated with a specified embedded application, one or more specified users are currently member users of the\nsession, a specified user status (e.g., user role) is still available for the second user in the embedded session (e.g., not yet assigned to other member users), the embedded session has not been active for more than a specified period of time (e.g., the\ntime period after initiation of the embedded application by the first user of FIG. 2), the output of a sequential media item being played in the embedded session (e.g., video or audio data) has not yet progressed past a specified percentage of the total\nplaying time, or other conditions.\n In some implementations, the join option can include one or more displayed additional options in addition to join the embedded session (e.g., displayed user interface elements corresponding to the additional options).  For example, the\nadditional options can include an option to select a command or request for one or more particular user statuses (e.g., user roles) that the second user can or is to be assigned in the embedded session.  The user statuses or roles can be selected by the\nsecond user from a set of predefined user statuses or roles.  A user status can define one or more functions of the embedded application able to be activated by the user participating in the embedded session and/or can define the capability of the user\nto modify data and/or states of the embedded application and embedded session.  Examples of user statuses or roles can include a controller (e.g., a user able to modify data such as a content document (e.g., list), or control playback of a media item), a\nplayer in a game (e.g., able to be winner or loser in the game), an observer of application events (e.g., view displayed playback of a media item or playing of a game and not able to affect media item playback or input game actions), a referee in a game\n(e.g., adjudicating rules of the game, initiating next rounds of play in the game, etc.), an administrator (e.g., initiating and stopping embedded application output or processing such as media item playback or games, assigning or dividing resources to\nplayers in a game, etc.), a leader of a team of multiple users in a game, a user status or role allowing the user to modify operation of the embedded application (e.g., designate rules or conditions of a game, control playback of media data, or other\noutput), etc. Some user statuses or roles can be restricted or limited, e.g., only qualifying users can be assigned these roles (e.g., a maximum number of players in a game may restrict additional users from being assigned a player role if the maximum\nnumber has been reached), while other statuses or roles can be unlimited (e.g., observers of a game).  In some implementation, a join option can include a request for permission from the user for the embedded application to use particular types of user\ndata associated with the second user.\n In block 312, user input is received by the second device and the second messaging application, the user input instructing that the second user and second device join the embedded session.  For example, the user input can be selective user input\nthat selects a displayed join option (as described for block 310) via a touchscreen or other input device, input a text or menu command, etc. In some implementations, the second user and second device can join the embedded session without explicitly\nreceiving user input instructing to do so.  For example, one or more conditions can be specified in stored user preferences of the second user that may instruct to automatically join an embedded session if one or more of the conditions are met.  For\nexample, the conditions can include one or more conditions as described above for block 310.\n In some implementations, user input can be received by the second messaging application which specifies additional input related to the joining of the second user to the embedded session.  For example, the user input can select a status of the\nsecond user in the embedded session.  In some examples, the user input can select or specify a user status (e.g., role) in the embedded session to be assigned to (or requested by) the second user, as described above.  In other implementations, the user\nstatus or role can be selected by the second user, by the second embedded application, or by the session server after joining the embedded session.  In some implementations, the user input can include other designations or selections, such as a time at\nwhich the user wishes to exit the embedded session, a selection of an icon, avatar, image, or other data to be associated with the user while being a member of the embedded session, etc. In some implementations, the user status or other designations can\nbe selected, by user input, from a menu or other list of options displayed in the chat interface.\n In some implementations, the second user and/or one or more other chat users can be automatically joined to an active embedded session, or can be automatically provided a customized option (e.g., in block 310) to join an active embedded session. In some examples, such chat users may have been detected to be interested in the embedded application based on their input messages in the chat conversation.  In an example, three chat users input chat messages to the chat conversation describing a\nvacation to a particular geographic location (e.g., Hawaii).  One of the chat users issues a command in a chat interface to start an embedded travel itinerary application, and the other two users are automatically joined to the associated embedded\nsession, or are provided a notification that indicates that the embedded session relates to their discussed travel destination (e.g., Hawaii), based on the messaging application or other device application parsing the messages input by these users in the\nchat conversation (e.g., parsing the location name, etc.).\n In block 314, a second embedded application is downloaded and/or executed for the second device, in response to the user input to join the active embedded session of block 312, and the embedded session is notified of the joining event.  For\nexample, the second device can send a request to and download the second embedded application can be downloaded from a server system hosting embedded applications similarly as described for FIG. 1.  The download of the second embedded application can\noccur in the background of the operating environment of the first device, e.g., similarly as described above.  After (and/or during) the second embedded application is downloaded, it can be executed on the second device.  In some implementations, one or\nmore parts or components of the second embedded application can be executed by one or more other devices in communication with the second device, e.g., one or more server systems.  In some implementations, the second embedded application corresponds to\nthe first embedded application and to embedded applications executed on other member devices of the embedded session.  For example, the second embedded application can include the same code (or substantially the same code) as the first embedded\napplication of FIG. 1.\n The second messaging application can send notification information to the embedded session indicating that the second user has selected to join the embedded session.  For example, the notification information can be sent to the session server,\nwhich conveys the joining event to the other member devices (including the first device) by sending the notification information (or information derived therefrom) to those member devices.  In some implementations, the session server can update the\nsession user list for the embedded session stored by the session server, e.g., add the second user to its session user list.  Some implementations can send the notification information directly to one or more other member devices of the embedded session. In some implementations, the second device sends a request to join the embedded session, and a session server (or embedded device) can send back a response to the second device that causes the second device to connect to the embedded session (and/or\ninitiate the execution of second embedded application on the second device, if it is not already executing).\n In block 316, the second messaging application provides user information to the second embedded application on the second device.  In some implementations, the second embedded application can request such user information.  For example, the user\ninformation can include a member update, e.g., an update of the members in the embedded session.  In some examples, the user information can include user identities of the member users of the embedded session, which, for example, can be the chat\nidentities of the users and user devices participating in the chat conversation.  In this example, the chat identity of the first user can be provided, and chat identities of any other member users.  The second messaging application can provide the user\nidentities, e.g., where the second messaging application received the user identities in block 308 and/or has previously received from a messaging server for the chat conversation implemented by the second messaging application.\n In some implementations, the user information for a particular user (e.g., the first user) can be provided by the second messaging application to the second embedded application if the second messaging application has obtained consent from the\nsecond user to do so.  For example, such consent can be included in user information provided for the chat conversation, in some implementations.  If no consent from the user has been obtained, then the user information is not provided to the second\nembedded application.\n In some implementations, the second messaging application can, prior to providing the user identity to the second embedded application, anonymize the user information (e.g., anonymize or obfuscate one or more user identities) so that the user\ninformation does not describe the particular user nor can be associated with the particular user (or only partially describes or is associated with the particular user).  In some examples, if a chat identity is a user name, a \"dummy\" or \"temporary\" name\nmay be assigned for use by the embedded application and in the embedded session.  For example, the first user chat identity can be changed to a generic identifier or name such as \"User 1,\" \"Friend 1\", the initials of the first user, the first name of the\nfirst user, etc. For example, a name such as \"John Doe\" may be replaced with a temporary pseudonym (e.g. \"Kenny\").  The anonymized user information can be provided to the second embedded application as one or more user identities.\n In some implementations, the second messaging application also provides additional user information, e.g., a user status or role to be assigned to the second user in the embedded session.  In some implementations or cases, this status or role\nmay have selected by the second user, e.g., in the embedded interface or chat interface.  The second embedded application can update its output based on the added second user.  For example, the user identity of the second user can be added to a user list\ndisplayed in the embedded interface.\n In some implementations, the user status or role is determined by the messaging application (or embedded application) based on chat messages provided by the second user in the chat conversation (and, e.g., this user status or role need not be\nselected by the second user).  For example, if the second user has previously input a chat message saying, \"I want to play the white side\" in the chat conversation about starting an embedded chess game application, then the messaging application can\ndetect this message, determine its meaning (e.g., via comparisons to keywords or key phrases and/or using machine learning techniques) and automatically assign the second user to a user role of the white pieces player after the second user joins the\nembedded chess game session.  The messaging application can provide this user status to the embedded application to assign the second user control of the white pieces in the game.\n In some implementations, a particular user role in the embedded session may not be available to a member user (e.g., the second user) being added to the embedded session.  For example, the user information may indicate that the second user has\nselected to be a player role in an embedded game application, where such a player role allows a user to affect game states in the game (e.g., input game actions to affect game state).  If the number of player roles in the application is limited (e.g., a\nchess game application limited to two players), there may not be a player role slot available to the second user if, for example, the second user is joining an active embedded session in which other member users have previously joined the embedded\nsession and occupied all the available player roles.  If such unavailability occurs, then the second embedded application can determine a different user status (e.g., player role) for the second user and can inform the messaging application of the\ndifferent user status.  For example, the second user can be assigned an observer role, in which the second user is not enabled to input actions to change particular states of the embedded application or session.  In some examples, an observer player for\nthe embedded game application can be allowed to display game states on their device but not affect game states with user input.  An observer role for a media player or shared content document can be allowed to view a media item or list but not affect\nmedia playback or list items.\n In some implementations, if the second user's selected role is unavailable, the second user can be provided with a menu of available role(s) in the embedded session, and the second user can provide user input to select one or more of the\navailable roles.  In some implementations, if a user role becomes available during an embedded session (e.g., a member user exits the embedded session or changes roles within an embedded session, such as changing from player to observer), then member\nusers who were not assigned requested user roles can be offered such a newly-available user role.  In some implementations, a particular user role may be removed or changed during an embedded session as determined by an embedded application.  For\nexample, a player role may change to an observer role after a game concludes or after a particular round or phase of the game is over.  User roles may also be changed or reassigned during an embedded session by member users having the ability or\nauthorization to perform such changes in an embedded session.  In some implementations, one or more of the various changes of user roles during an embedded session can be indicated in data provided to the messaging application by the embedded\napplication, and these changes can be indicated by the messaging application in the chat conversation of the chat interface (e.g., output as text descriptions by devices of all chat users, or only of member users).\n In some implementations, an indication that the second user has been assigned the different (newly-assigned) user status, indications of removal or additions of user statuses or roles to the embedded session, and/or indications of any other\nchange in user statuses for member users in the embedded session, are output to the chat interface to be displayed on the chat devices participating in the chat conversation, or alternatively displayed on the member devices of member users of the\nembedded session.\n In an example, the first user initiates an embedded chess game application and the first and second users agree via chat messages input in the chat conversation that the second user will start the game as the player controlling the white pieces. The messaging application parses the chat messages to determine that the first user is assigned a player role to play black pieces, and provides this user status to the embedded chess application to start a game with the white pieces controlled by the\nsecond user.  A third user (another member user) then joins the embedded chess session, and the messaging application (or the embedded chess application) automatically assigns a user role of observer to the third user for the embedded session, since\nthere are only two player roles provided for the game session and those roles are occupied by the first and second users.\n In some implementations, the user information can include other information related to the member users of the session, if user consent has been provided by participating users.  For example, geographic locations of the devices used by the\nmember users can be obtained by the second messaging application, e.g., from the chat conversation, if the chat users (including the member users) have provided consent to obtain such geographical information (e.g., geographical coordinates, labels of\ngeographic regions or areas, etc. as determined by GPS or other location sensors of the member devices).  In an example using the embedded travel itinerary application described above for three chat users, the messaging application can specify to the\nembedded application that the travel context is \"vacation in Hawaii mid-June\" based on the users' chat messages, and specify the respective geographic locations of the three chat user devices so that the embedded application can determine travel\nitineraries (e.g., travel routes, airplane flights, scheduled visits, etc.) to the destination location based on the users' device locations.  In some implementations, location information can specify a location as an associated category such as \"home\"\nor \"work\" for a user.  In some implementations, the user information can include a local time (based on local time zone) of each member user.  For example, the local times can be similarly used by an embedded travel itinerary application to determine a\ntravel itinerary for each user.  In some examples, the embedded application can use location and/or time information of member users to determine output information displayed in the embedded interface, e.g., travel itineraries, reservations at events,\nwritten language of text output (e.g., subtitles, messages, etc.), regional customizations in output, notifications based on time, etc. In various examples, access to member users' locations (with user consent) can enable embedded applications to use\nshared locations to provide output, e.g., as status updates of member user locations to a group of member users that are waiting for other member users at a restaurant, to provide user locations in races and fitness challenges, and/or to provide\nlocations of children users for viewing by parent user who are parents of those children users.\n In some implementations, the user information can include current or available data bandwidths over the network of one or more of the member devices used by member users, e.g., bandwidth, lag, or other network connection characteristics of each\nmember device to a session server implementing the embedded session and/or to one or more member devices implementing the embedded session.  In some examples, the embedded application can use the bandwidth information to cause a modification of\ntransmission of input data received at the first user device and the first embedded application over the network.  For example, the modified transmission can include a different amount of data to be transmitted over the network to the first user device,\ncausing a different rate of data transfer over the network to the first user device, and/or causing compression of the transmitted data from other devices to the first user device (e.g., a content data stream from a content server).  In some examples,\nbandwidth information can be used by the embedded application to assign particular user statuses (e.g., roles) to particular member users in the embedded session.  For example, high bandwidth (e.g., over a threshold) may be suited to roles requiring\nsending data from the user device to other devices, e.g., particular player users in a real-time game, while lower bandwidth may be more appropriate for observer players in particular game applications.\n Other user information provided to the second embedded application can include other information if consent by the associated chat users for access and use of such data has been obtained, including social network information (e.g., posts,\nratings, comments, etc. posted to a social networking service), history of the use of the embedded application by the member users (e.g., particular media items previously viewed in the embedded application, games previously played by member users and\nthe roles assigned to member users in those games, the win/loss record of member users in previous games, dates and times of previous embedded application use, etc.).\n In block 318, syncing information can be received at the second device (e.g., from the session server and/or other member devices such as the first device) and the second embedded application is synchronized with the embedded session.  In some\nimplementations, the syncing information can be received in block 308 with the indication of the embedded session.  For example, the syncing information can include one or more states of corresponding embedded applications executing on member devices of\nthe embedded session, reference time information from a session server, etc. In some implementations, the syncing information can be received from the session server or other device that can provide the syncing information to all the member devices of\nthe embedded session to enable the embedded applications to be synchronized with each other.  In some examples, the session server receives updates of the embedded applications of the member devices based on user input (e.g., user playback commands of\nmedia items, game actions, modification of shared content documents, itineraries, etc.) or application events (e.g., a timer expiring, a new media item in a queue starting to be played, a game event occurring, etc.) at those member devices, and sends\nsyncing information to the other member devices.  In some implementations, the member devices can update a global synchronizing value or timer stored on an accessible device such as a server to maintain synchronization with each other, an example of\nwhich is described with reference to FIG. 5.\n In some examples, the syncing information can include the time point (or change in time point) of a video or audio data segment that is currently being played in the embedded session, a change in state of a game played in the embedded session,\nan update to a shared content document in the embedded session, etc. In another example, the syncing information can include clock information for other member devices that can be used for synchronization of timed events in the embedded session.  For\nexample, spawning of enemies or treasures in a game may be synchronized across all member devices at a particular time, or registering the member user inputs at (or relative to) a particular time to cause an effect in a game.  Registering the\nsynchronized times at which states occur or change in the embedded session may allow accurate replaying of a recorded version of the embedded session at a later time by a different member user.  Precisely timing events among the member devices also\nallows for competition between users (e.g., to determine which member user is the first player in a game to collect or select a specific item in the game environment).\n In some implementations, a leader device can be elected among the member devices (e.g., randomly, or based on network characteristics, e.g., the device having the lowest latency to a server handling synchronization of the user devices, etc.) to\nallow synchronization of real-time events and output among embedded applications in an embedded session.  For example, the leader device can be the authority for validating and propagating state changes to the other member devices, e.g., instead of using\na server system for such tasks.  If the leader device goes offline, the online member devices elect a new leader device, and when the former leader device comes back online, it will be considered a regular (non-leader) device since its leader status was\nrevoked.  Such a leader device configuration can allow multi-device shared experiences over a network without the use of server-side synchronization and authority.\n In block 320, the embedded interface of the second embedded application is caused to be displayed by the second device.  For example, the embedded interface can be displayed in association with the chat interface of the second messaging\napplication similarly as described above.  The embedded interface is a user interface to the second embedded application and allows output from the second embedded application be displayed by the second device (e.g., in an embedded view of the embedded\ninterface) and allows user input to be input in the embedded interface to be provided to the second embedded application.\n The embedded interface displays output from the second embedded application that is synchronized with the other embedded applications of the member devices in the embedded session.  For example, if a media item is being played in the embedded\nsession, then a media item being played by the corresponding embedded applications on other member devices is also played by the second embedded application, synchronized with the member devices.  Thus, if the second user has joined an embedded session\nin which a media item has previously been playing for some time and is currently halfway through its duration, the second embedded application plays the media item starting at the halfway point.  For example, this synchronization can provide to the\nsecond user an experience of sharing media items and other output of an embedded application with other member users.\n In an example using an embedded travel itinerary application, the embedded application can provide an embedded interface that displays information about airplane flights departing from the geographic locations of the member users' devices to the\ntravel destination for specified dates.  In some implementations, customized airplane flight information can be displayed in the embedded interface for each member user, where each embedded interface shows only the airplane flight information for the\nlocation of the user viewing the information in the embedded interface.\n In block 322, an indication is sent over the network that the second user has joined the embedded session.  For example, the second messaging application can send the indication of a join event to a session server, and the session server can\nsend the indication of the join event to member devices (and not to chat devices that are not member devices).  For example, the session server can send the indication to the first device, as described below with respect to FIG. 4.  In some\nimplementations, the second messaging application can send the indication directly to member devices.\n During the embedded session, the second device can receive user input from the second user to the embedded application via the embedded interface and/or to the messaging application via the chat interface as described herein.  Input can also be\nreceived from the other member devices, e.g., via a session server, and/or from one or more servers (e.g., synchronization and other updates to the embedded application from the session server, content data from a content server, etc.).  Output can be\ndisplayed in the embedded interface and/or in the chat interface.\n If the second user has been assigned a user status (e.g., role) within the embedded application and embedded session, then the second user can be enabled by the embedded application to activate one or more functions of the embedded application\nthat are associated with the assigned user status.  For example, in an embedded game application, a user assigned a player role can be enabled to perform game actions with user input to affect the state(s) of the game.  A user assigned an observer role\ncan be enabled to provide input to perform view functions in the game, e.g., cause display of different player scores, attributes, etc. with user input, but not affect the game states.  A user assigned an administrator role or referee role can be enabled\nto initiate a game, assign player users to different teams, adjudicate a game, etc., but not affect game states.  In some implementations, a member user can be assigned a role of a passive observer or an active observer.  For example, an active observer\nstatus may allow a user to modify states in the embedded application (e.g., game states, playback position of a media item, etc.), or modify some types of states (e.g., guide certain types of game objects, add comments but not change playback position,\netc.), while a passive observer may be prevented from modifying any such states.  In some examples, a single-player game may allow the first user to be the main character moving through a game environment, facing obstacles, etc. The other member users\ncan be assigned passive observer roles by default, or one or more passive observer users could select to be assigned active observer roles allowing them to affect game states, e.g., instruct enemies in the game to attack or assist the first player's\ncharacter, directly assist the first player's character, tap on the first player's character to slow it down, etc.\n In additional examples, a particular graphical interface for the embedded interface can be displayed on a member device based on a role that has been assigned to the user of that member device, while other member devices of the embedded session\nmay display other graphical interfaces based on different roles (or lack of roles) of their member users.  For example, different controls and functions can be displayed in different graphical interfaces for different roles.  In some examples, only\nparticular roles can have access to certain features of the embedded application.  In an example of an embedded reservation application (e.g., similarly as described above), the first user initiates the embedded reservation application and thus has a\n\"reserver\" status allowing that user to finalize the reservation (e.g., pay a reservation fee to reserve a hotel room).  The first user selects reservation information such as a destination and itinerary, and the embedded application shares the selected\nreservation information with the rest of the member devices (e.g., including the second user's device) for display in their embedded interfaces of the embedded reservation application.  The second user assists the first user select a hotel room to\nreserve by selecting or changing reservation information (which is synchronized on all member devices).  In this example, only the first user device of the member devices displays a \"pay\" button in its embedded interface, because the first user set up\nthe embedded application/session and has reserver status.  When the pay button is selected by user input, data is caused to be sent to a server to finalize the reservation.\n In various implementations, \"interactive cards\" can be displayed in the chat interface of chat devices of the chat conversation, such as the second user device.  An interactive card can be a displayed information area or window that displays\ninformation and controls (user input options) related to events, output data (e.g., content data), and input from members users provided in an embedded session of the chat conversation.  The information can include text, images, videos, audio output,\ncinemagraphs, or other types of content data (e.g., captured and stored images of the output displayed in the embedded interface).  In some implementations, the interactive card can be displayed in the chat interface of the chat devices, regardless of\nwhether or not a chat device is a member device of the embedded session.\n An interactive card can also include one or more displayed input controls which can be selected by user input to cause association actions to be performed by the embedded application.  When the control is selected by user input, arguments or\nparameters of the selected control and/or the interactive card are provided to the embedded application and the embedded application performs the related action(s).  In some examples, the notification indicating the initiation of an embedded session as\ndisplayed in block 310 can be an interactive card that displays the information described above, including the join option (for non-member users) that, if selected, causes the user device to execute the embedded application and join the embedded session. In some implementations, one or more actions are executed in the embedded application in response to the user joining the embedded session, as described above.  In another example, an interactive card can include a displayed control that, when the\ncontrol is selected by user input, causes the embedded application to start playing a media item, trigger an in-game action (e.g., accept a different user's challenge, unlocking an in-game item, etc.), and/or other actions.  The embedded application can\ndisplay a request in the interactive card for confirmation from the user to perform the association action(s).  In some implementations, the information and options in the interactive card can be continually updated based on the current input received\nfrom member users (and/or chat users), current output provided by the embedded application, and current processing/states of the embedded application.\n In some examples, a media item is being played in an embedded session for a media player embedded application, and an interactive card is displayed in the chat interface of the chat devices related to the playing of the media item.  The\ninformation and options can be continually updated based on the playback of and commenting on that media item by the chat users of the chat conversation.  For example, the interactive card can display an identification of each media item being played or\nin queue for playback.  The information and options in the interactive card can be continually updated based on the playback of and commenting on that media item by the chat users of the chat conversation.  For example, the card can display one or more\nof the following features: identifications (or the number) of the chat users who have watched the media item (if consent from those users has been obtained), identifications (or the number) of the chat users who have commented on the media item (as in\nblock 518 below, if consent from those users has been obtained), a displayed option to comment on the media item for member devices (e.g., as in block 518), and a displayed option to make the card displayed persistently on the user device, e.g., as a\nreminder to the user to play the media item.  If multiple interactive cards are made persistent, a carousel display of cards can be displayed which can be scrolled or rotated to view the multiple cards, e.g., one at a time.  Causing a media item to be\nplayed in the chat conversation, or commenting on the media item (e.g., as in block 518), updates a corresponding interactive card stored on each chat device, which can be displayed in the chat interface of each chat device of the chat conversation.  The\ninteractive card indicates the latest plays and comments for the media item by chat users.  In some implementations, selection of the comment option of the interactive card by user input can cause an input field to be displayed, allowing the user to\ninput comments to be displayed in the embedded interface during current and later playbacks of the media item (e.g., as in block 518), and or displayed in the chat interface of the chat conversation during current and later playback of the media item.\n In some implementations, interactive cards related to other types of embedded applications can be displayed in the chat interface.  For example, an embedded game application and session can cause an interactive card to be displayed indicating\none or more of the following, if user consent has been obtained: the number of players and observers in the game, identifications of member users, current scores of players, game objectives achieved, one or more images showing screenshots or portions\nthereof of game events or objects, a displayed option to input a comment or rating to the game or to a particular player of the game (e.g., to be displayed in the chat interface, and/or in the embedded interface of member devices), a displayed option to\nmake the card displayed persistently on the user device, e.g., as a reminder to the user to join the embedded session, etc. Similar information and displayed options can be displayed in interactive cards for embedded sessions providing content documents,\ntravel itineraries, reservations, shopping, etc.\n In some implementations, the second user exiting the embedded session can be implemented similarly to joining the embedded session.  For example, the second messaging application receives an indication that the second user is exiting the\nembedded session, e.g., by exiting the second embedded application, by exiting the chat conversation, or by exiting the second messaging application.  The second messaging application sends information to the other member devices (e.g., via a server)\nindicating that the second user is exiting, so that the other (corresponding) embedded applications executing for the other member devices can update their states (e.g., remove the user identity of the second user from their embedded interfaces, remove\nthe second user as a participant in the embedded session, etc.).\n FIG. 4 is a flow diagram illustrating an example method 400 in which a first device of FIG. 2 processes an event in which a second device has joined an embedded session.  In this example, method 400 illustrates blocks performed at the first\ndevice in response to the second user joining the embedded session as described with reference to FIG. 3.\n In block 402, it is checked whether user consent (e.g., user permission) has been obtained to use user data in the implementation of method 400.  For example, user data can include messages sent or received by a user, e.g., using messaging\napplication 103, user preferences, user biometric information, user characteristics (identity, name, age, gender, profession, etc.), information about a user's social network and contacts, social and other types of actions and activities, content,\nratings, and opinions created or submitted by a user, a user's current location, historical user data, images generated, received, and/or accessed by a user, videos viewed or shared by a user, etc. One or more blocks of the methods described herein may\nuse such user data in some implementations.\n If user consent has been obtained from the relevant users for which user data may be used in the method 400, then in block 404, it is determined that the blocks of the methods herein can be implemented with possible use of user data as described\nfor those blocks, and the method continues to block 408.  If user consent has not been obtained, it is determined in block 406 that blocks are to be implemented without use of user data, and the method continues to block 408.  In some implementations, if\nuser consent has not been obtained, blocks are to be implemented without use of user data and with synthetic data and/or generic or publicly-accessible and publicly-usable data.\n In block 408, an indication is received over the network at the first device that indicates the second user and the second device have connected to (joined) the embedded session.  For example, the indication may indicate that the second device\nhas downloaded and/or executed a corresponding embedded application for the embedded session (or is in process thereof), e.g., as in block 314 of FIG. 3, and/or that embedded application has synchronized (or is synchronizing) with the embedded session,\ne.g., as in block 318 of FIG. 3.  For example, the indication can include, or be derived from, notification information as described for block 314 of FIG. 3.  In some implementations, the first messaging application can receive the indication, which can\ninclude notification information in some implementations, as described.  The indication can include information indicating the joining event, a user identity of the second user, and any other options selected by or provided for the second user (e.g.,\nuser status in the embedded application, etc.).  In some implementations, the first messaging application previously obtained user identity information of the second user for the chat conversation, e.g., to display the user identity of the second user in\nthe chat interface of the first device.  In some implementations, the indication is received from a server system, e.g., the session server that received the notification information from the second device in block 314 of FIG. 3.  In some\nimplementations, the indication is received at the first device directly from the second device (e.g., a peer-to-peer configuration).\n In block 410, the first messaging application provides a data update to the first embedded application, where the update indicates the joining of the second user to the embedded session.  The embedded application designates the second user as a\nmember user of the embedded session.  For example, the update can include user information such as a user identity of the second user, and an indication to add the second user as a participant in the embedded session.  In some implementations, the first\nmessaging application has obtained the user identity of the second user and other user information in the chat conversation.  In some examples, the user identity can be the chat identity of the second user participating in the chat conversation.\n The user information for a particular user (e.g., the second user) can be provided by the first message application to the first embedded application if the first messaging application has obtained consent from the second user to do so.  For\nexample, such consent can be included in user information provided for the chat conversation, in some implementations.  If no consent from the second user has been obtained, then the user information is not provided to the first embedded application.  In\nsome implementations, the first messaging application (or a server application) can anonymize, or partially anonymize, the user information such that the information does not describe the second user nor can be associated with the second user (or only\npartially describes or is associated with the second user).  For example, the second user chat identity can be changed to a generic name or placeholder name such as \"User 1,\" \"Friend 1\", the initials of the second user, the first name of the second user,\netc. In additional examples, the user identity can be the real name of the second user name, a user name previously registered by the second user in the messaging application, a user name previously registered in a contact list or other data stored on\nand obtained from the second user device (if consent from the second user has been obtained), a user name previously registered in the embedded application, a placeholder name, etc. This information can be provided to and/or determined by the embedded\napplication as a user identity for the embedded session.  In some examples, the application may choose the user name to display to other member users in one or more particular contexts of the embedded application and embedded session, where different\nuser names can be displayed in different contexts.  For example, contexts can include within the name used in the messaging application and chat interface, the name attached to the second user's messages composed within the messaging application, the\nname used within the embedded application and its content data, the name attached to chat messages or system messages generated by the embedded application, etc.\n In some implementations, the user information can include a user status (e.g., role) selected by or provided for the second user, geographic location of the second device, and/or other information related to the second user, if consent has been\nobtained from the second user for use of such information.  The first embedded application can update its processing based on the addition of the second user, e.g., display the user identity of the second user in the embedded interface, check for input\nfrom the second user, etc. In some implementations, the first messaging application also designates the second user as a member user of the embedded session, e.g., updates its own session user list for the embedded session to add the second user\nidentity.\n In block 412, the first embedded application (and/or first messaging application) may determine one or more user statuses for the second user.  In some implementations, the user status can be determined based on the user information received\nfrom the first messaging application, e.g., in block 410.  For example, the user information can include a user status for the second user in the embedded session.  The user status may have been selected by the second user, and information indicating\nthis selection may have been provided to the first embedded application by the first messaging application.  For example, a user role of \"observer\" for a game embedded application may have been selected by the second user, and the first embedded\napplication assigns this role to the second user.\n In some implementations, the first embedded application (and/or the corresponding embedded applications executing on other member devices) can generate the user status or role.  In some implementations, a server (e.g., a session server) can\ngenerate the user status or role and provide the determined user status to the first messaging application and first embedded application.  In some examples, the user status can be assigned based, at least in part, on one or more current states of the\nfirst embedded application and/or embedded session.  For example, if one or more roles are available to be assigned to a joining user, then the second user (joining user) can be automatically assigned to an available role (e.g., player in a game).  If\nthe embedded session does not have any selected roles available, the second user can be assigned to another (e.g., default) role.  For example, if the second user selected to be assigned a player role but no player roles are available, the second user\ncan be assigned an observer role.  In some implementations, particular states and/or data can be stored (e.g., on a server) from previous embedded sessions that a particular user has participated in, such as user statuses and roles previously assigned to\nthe particular user in previous embedded sessions using the same embedded application, e.g., if privacy and security measures are performed for such states and data.  In such implementations, a history of use of embedded applications of the second user\ncan be used in determining a user status or role for the second user, if second user consent has been obtained for such use of user data.  For example, if the second user has been a referee role in a threshold number of previous embedded sessions using\nthis embedded application, then that role can be automatically assigned to the second user.  In some implementations, the second device can be sent a prompt (e.g., by a member device and/or session server) to allow the second user to accept or reject a\nparticular user status or role that has been automatically assigned to the second user in the embedded session.\n In some implementations, other user statuses and roles can be determined for users in an embedded session.  For example, a user can be assigned a \"controller\" or \"leader\" role if the user initiates the embedded session (e.g., the first user),\nwhere the controller is the only user to be able to modify data processed in the embedded session, e.g., change the playback point in a media item, change data in a shared content document, etc. In some examples, the second user can be assigned a\nhandicap or advantage as a player in a game of the embedded application based on history of the second user in previous games similar to the embedded session.\n In some implementations, the user status can be determined based on messages and other user input provided in the chat conversation from which the embedded session was initiated.  For example, the messaging application can parse the second\nuser's text messages (or text messages derived from second user voice input) to match predefined words or phrases, and/or use other language processing techniques to determine semantic meanings of user input to the chat conversation.  For example, the\nsecond user may have previously input into the chat conversation, \"let's play!\" or \"I'll watch you two,\" indicating that the second user intends to be a player role or an observer role, respectively, in the initiated embedded application game.  The\nsecond user may have input, \"I'll show you the place in the video,\" indicating that the second user should be assigned a controller role that has control over a playback position in a played video in the embedded session.\n In some implementations, a member user that is assigned a particular status or role in the embedded session may have a status or role that is changed, and/or may select to change his or her status or role to a different status or role, while\ncontinuing to be a member user.  For example, a user that is assigned a player role in the embedded session may automatically become an observer user based on certain conditions, e.g., the user's player character has been eliminated in a game, becomes a\n\"ghost\" that can only observe and move within the game environment but not act to change objects within the environment, or, e.g., in a 2-player game, the player is offline for longer than a threshold period of time and loses player status, while an\nonline observer user is automatically assigned the player role by the embedded application.  In another example, in turn-based games where a player user is only active during their turn, the player may automatically become an observer player when their\nturn is not currently active.  In further examples, predefined additional user statuses or roles may become permanently or temporarily available in an embedded session, and/or existing user statuses or roles may become permanently or temporarily\nunavailable in the embedded session, during the embedded session.  For example, such user status or role changes can occur based on a change in the number of member users currently in the embedded session, a length of time the session has been active, a\nmember user indicating he or she is doing an external task such as answering a phone call, particular events occurring in the embedded application (game events, completion of a shared list, etc.), etc.\n In some additional example implementations, a lobby can be provided in the embedded application and embedded interface, e.g., a displayed area listing the member users who do not yet have an assigned role.  For example, such users can wait to be\nassigned a user status or role in a game, e.g., wait for a game to finish, wait for a player user to change to a non-player role, etc. In some examples, a subset of the member users can be associated with a sub-session within the embedded session, where\nthe subset of users interact via the embedded application within the sub-session and other member users have different ability or no ability to interact.  For example, an embedded session can include multiple sub-sessions, each sub-session associated\nwith a different subset of the member users.  In some examples, each sub-session can be a particular game within a tournament structure executed in the overall embedded session, where each game is executed simultaneously and the winner of each game plays\neach other in a new game.  In some examples, two or more users in a lobby can be associated with each other and/or be assigned particular roles in a sub-session (e.g., individual game) that is initiated in the embedded session.  Team-based\nimplementations can also be provided, where users that join the embedded session are assigned to an existing team in a particular game, and each team has multiple players.  In some examples, the embedded session can provide multiple games running in\nparallel, a single game with two players and N observers, a single game with two teams and N players per team, etc. In some implementations, multiple such sub-sessions can be implemented as separate concurrent embedded sessions, each embedded session\nprovided by a single embedded application.\n In block 414, the first embedded application updates its output in the embedded interface based on the addition of the second user.  For example, a second user identifier can be displayed in the embedded interface, as well as the second user\nstatus (user role, etc.).\n In block 416, the first embedded application provides determined user status information to the first messaging application.  For example, if the first embedded application determined a user role and/or other user status information, this is\nprovided to the first messaging application.  In some implementations, the first messaging application can update a session list of users, output the user status information in the chat interface, etc.\n In some implementations, the second user exiting the embedded session can be implemented similarly to joining the embedded session.  For example, the first device can receive an indication over the network that the second user is exiting the\nembedded session, e.g. by exiting the second embedded application, by exiting the chat conversation, or by exiting the second messaging application.  The first messaging application sends information to the first embedded application indicating that the\nsecond user is exiting, so that the first embedded application can update its states (e.g., remove the user identity of the second user from the embedded interface and no longer expect input from the second user).\n In some implementations, a user device can cache a state of the embedded application to enable offline mode, where the user device (and/or the messaging application) is no longer visible over the network to the other user devices and server\ndevices.  When the user device (and/or messaging application) goes back online, the cached state of the embedded application can be used to synchronize the embedded application with the other member devices and corresponding embedded applications, and\nprovide merge/conflict resolution.  In some examples, before the user device loses network connectivity, the embedded application can first write any state changes to storage on the user device.  While the user device is online, the local state is\nconstantly synchronized with a server global state stored on a server, where the global state is shared by the instances of the embedded application executing on the member devices in the same chat conversation.  On loss of network connectivity, the\nlocal state can be accessed and written to by the embedded application on the user device, but no client-to-server synchronization can occur.  Once network connectivity is restored to the user device, the local state is synchronized with the global state\n(e.g., pulling server-side changes and pushing client-side changes) and a merge occurs, using conflict resolution if needed (e.g., using a last-write-wins strategy or other merging strategy).  This offers limited offline support, e.g., it provides\nprotection against short-term network disconnections that are common among mobile devices during cell tower switches or environmental changes (such as going underground, etc.).  If users are offline for a longer period of time, the local state on the\nuser device may diverge significantly from the server global state.\n FIG. 5 is a flow diagram illustrating an example method 500 to provide output of a media item using an embedded application in association with a messaging application.  In some examples, method 500 can be implemented similarly as one or more\nblocks of FIG. 2 in which an embedded application is initiated from a chat conversation and data is processed in an embedded session involving chat users.\n In block 502, it is checked whether user consent (e.g., user permission) has been obtained to use user data in the implementation of method 500.  For example, user data can include messages sent or received by a user, e.g., using messaging\napplication 103, user preferences, user biometric information, user characteristics (identity, name, age, gender, profession, etc.), information about a user's social network and contacts, social and other types of actions and activities, content,\nratings, and opinions created or submitted by a user, a user's current location, historical user data, images generated, received, and/or accessed by a user, videos viewed or shared by a user, etc. One or more blocks of the methods described herein may\nuse such user data in some implementations.\n If user consent has been obtained from the relevant users for which user data may be used in the method 500, then in block 504, it is determined that the blocks of the methods herein can be implemented with possible use of user data as described\nfor those blocks, and the method continues to block 508.  If user consent has not been obtained, it is determined in block 506 that blocks are to be implemented without use of user data, and the method continues to block 508.  In some implementations, if\nuser consent has not been obtained, blocks are to be implemented without use of user data and with synthetic data and/or generic or publicly-accessible and publicly-usable data.\n In block 508, a selection of a media item by a chat user is obtained, the selection instructing playback (e.g., output) of the media item by a user device.  The chat user is participating in a chat conversation with one or more other chat users\nusing devices connected to the user device and each other by a network, where the chat conversation is displayed in a chat interface by a device similarly as the first and second users and first and second devices described above.  The media item can be\nselected in any of various ways.  For example, an identifier of the media item can be input via text, voice input, selection of a displayed element or an item from a displayed menu, etc. In some implementations, the media item is selected from within the\nchat interface, e.g., via text command, interface element displayed in the chat interface, etc. In some implementations, user input can also explicitly indicate to share the media item with the chat conversation by selecting or inputting a share option\n(e.g., an interface element, text command or parameter, etc.).  Sharing the media item can include making the media item available for playback on the other devices by offering an embedded session to the other chat users of the chat conversation.  Some\nimplementations can automatically offer the media item in an embedded session in the chat conversation when the media item is selected for output.  In some implementations, the media item can be selected within an embedded interface after an embedded\nmedia player application is initiated as described in block 510.  In some implementations, a bot can be used to select a media item, some examples of which are described below.  The method continues to block 510.\n In block 510, an embedded media player application is executed (e.g., initiated) in response to the selection of the media item.  The embedded media player application is configured to output the selected media item.  In some examples, the\nembedded media player application is downloaded to the user device from a server similarly as described above and executed at least partially on the user device.  In some implementations, the embedded media player application can be executed partially or\nin whole on a device in communication with the user device, e.g., a server, and provide data (e.g., data describing the embedded interface) for display on the user device.  The embedded media player application causes display of an associated embedded\ninterface on the user device in association with the chat interface (e.g., similarly as described above) such that one or more chat messages in the chat interface can be displayed during display of the embedded interface.  In some implementations, the\nembedded media player application can be initiated in response to user input instructing that the embedded application be executed without specifying the media item to be played.  For example, the media item can then be selected by user input provided to\nthe embedded interface, e.g., after user input browses displayed menus of available media items in the embedded interface, etc. The method continues to block 512.\n In block 512, other chat users are enabled to join or leave an embedded session that is initiated in association with the start of the embedded media player application, where the embedded session can include playback of the media item.  For\nexample, a notification can be sent to the chat devices participating in the chat conversation and one or more chat users can join the embedded session as member users, similarly as described above for FIGS. 2-4.  The method continues to block 514.\n In block 514, playback of the selected media item is performed in the embedded interface provided on a display of the user device by the embedded media player application.  For example, after selection of the media item in block 510, the\nplayback is initiated at the start of a sequence of content data of the media item, e.g., a first frame of a video data segment and/or first portion of an audio segment.  In some examples, the playback can include displaying frames of video data in the\nembedded interface and/or outputting corresponding audio data as audio from speakers of the user device.  In some examples, the playback can include playing a track for video and/or audio, and playback position can refer to the data which is currently\ndisplayed or output at a current time point in that track.  In some further examples, e.g., in displaying a virtual reality environment or other displayed 3D environment, playback may have a playback position that refers to direction and/or orientation\nof a displayed point of view (or other reference view) in a 3D environment, in addition or alternatively to a time point.  In some cases, the media item content data can be obtained from local storage of the user device, e.g., memory and other storage\nincluded in the user device.  In some cases, the media item content data can be received over the network, e.g., as a stream of data from another device such as content server 154, a different member device, etc. In some implementations, playback of a\nvideo media item can default to muted audio and displaying captions for speech (e.g., where caption data is included in received video data), where the user can select to enable audio output for the video data.\n Playback of the media item occurs on the other member devices of the embedded session.  In some implementations, a server (e.g., session server or content server) can manage playback of the media item in the embedded session.  In some\nimplementations, the content server can stream the content data to a session server or other server, which manages the embedded session and sends an individual stream of the content data to each member device.  For example, the session server can send\nany commands received from the member devices to the content server or can process the commands into particular command for the content server.  In some implementations, a content server can stream the content data independently to each member device of\nthe embedded session, and can receive commands based on user input from one or more of the member devices.  Changes made to the playback based on the commands (e.g., in later blocks of method 500) can be provided in all the streams to the member devices.\n Playback is synchronized among the member devices.  In some implementations, the synchronization can be managed by one or more servers.  For example, a content server providing the media item as a stream of data to each member device can\nsynchronize each stream of data using synchronization data, e.g., a reference position, such that the playback on each member device is synchronized.  In some implementations, a session server can store a reference position (or other synchronization\ndata), such as a reference frame or time position within the media item data sequence, that can be accessed by member devices and/or sent to member devices by the session server.  For example, the playback of the media data can be set to a frame (or time\npoint) on each member device equal to the reference position.  In some implementations, the synchronization can be managed by one or more of the member devices.  For example, one of the member devices (e.g., the user device) can be a reference device\nthat sends synchronization data (e.g., reference positions) to the other member devices that indicates the current position in the media data being played on the user device.  The other member devices can adjust the playback of the media item on those\ndevices to synchronize with the reference position of the reference device.\n In some examples, a shared playback timer can be used which can be stored and accessed on a database of a device accessible to all member devices (e.g., a session server, content server, or one of the member devices).  For example, a single\nreference value can act as an authoritative playback position (e.g., a global playback position) to which all member devices should synchronize.  In some implementations, a tolerance (e.g., threshold amount of time) can be allowed between the global\nplayback position and the playback positions on the member devices, e.g., plus or minus 5 seconds.  If a member device's playback position is at a time greater than the tolerance amount, that playback position is updated to the global playback position.\n In some implementations, multiple or all member devices can update the shared timer (global playback position) in an averaging manner.  For example, during playback, each member device attempts to synchronize with the server each second (or\nother time unit), which is a \"sync point.\" At each sync point, each member device attempts to increase the global playback position.  In some examples, the amount of increase can be determined using the following formula:\n '.times..times..times..times..times..times..times..times..times..times..t- imes..times..times..times..times..times..times..times..times..times.  ##EQU00001## where g is the global playback position and g' is the increased global playback\nposition.  For example, the device previous playback position is at the previous sync point, and the total number of member devices is the number of member devices in the embedded media session (e.g., if all member devices are able to update the global\nplayback position).  Thus, each device updates the global playback position by a fractional amount that is based on the increase in playback position of that device since its last sync point, divided by the number of member devices.  If the difference\nbetween the increased playback position g' and the [Device current playback position] is greater than the tolerance value, the device maintains or changes the playback position to g (and does not commit g').  Otherwise, the device commits g' to the\ndatabase.  The method continues to block 516.\n In block 516, user input is received from a member user in an embedded interface.  For example, the user input can be received from the user of the member device that initiated the embedded session in block 508, or the user input can be received\nfrom a different member user of a different member device.  The user input can be received in the embedded interface, e.g., selection of a displayed interface element such as a button, icon, slider control, etc. In some cases, the user input can be\nreceived, or partially received, in the chat interface.  For example, user input can select an option to input a user comment directly onto the displayed media item, and then further user input can input the user comment in the chat interface, e.g., as\ntext, emoji, rating symbol, etc. The method continues to block 518.\n In block 518, it is checked if the received user input is a user comment associated with the playing media item.  For example, such an associated user comment is input in the embedded interface such that the user comment is displayed during\ndisplay of the media item.  For example, the user comment can be displayed during the playback for a predefined amount of time after being input.  In some examples, the user comment can be superimposed on a display of video data, or otherwise displayed\nin the embedded interface.  In some implementations, the content of the user comment can be input in the chat interface, as described above, and then displayed in the embedded interface.  For example, the user input can select an option in the embedded\ninterface to input the comment, and then the comment is input via the chat interface.  In some implementations, the user input can also select or point to a particular location at which to display the user comment in a displayed area of the embedded\ninterface, such as a location in a display window or view of the media item.  For example, the location can be specified as coordinates of the location of the comment in the displayed view of a video, image, or other media item.  In further examples, the\nuser can toggle (e.g., tap) or continuously select one or more of multiple controls displayed in the chat interface or embedded interface that are associated with an emotion or mood of the user (e.g., emoticon buttons).  This selection causes an\nassociated image or icon (e.g., emoticon) to be displayed in the embedded interface (as described below) while the user selects the control or until the user again toggles (e.g., taps) the control, which represents the user's emotion at that time in\nresponse to the playback of the media item.\n If the user input is a user comment, then in block 520, the user comment is stored in association with an identification of the associated portion of the media item that was displayed when the comment was input and/or displayed.  In some\nimplementations, the identification of the associated portion includes an identification of the media item (e.g., title, identification value, etc. of the media item).  For example, the identification of the associated portion of the media item can\nindicate the media item context of the user comment.  In some examples, if a user comment was input at a time of 1 minute after the start of playback of a 5-minute video segment, then the associated portion of the media item is the content data of the\nmedia item at the 1 minute time point of the video segment (e.g., a threshold amount of data before and/or after the 1 minute time point).  In some examples, this portion can be identified by the time point, e.g., indicating where the portion starts or a\nmiddle point of the portion.  In some implementations, the identification can include a duration of the portion.  In some implementations, an identification of the user who provided the comment can be stored (e.g., an identification of the user as\ndisplayed in the embedded session, or a chat identity in the chat conversation).  In some implementations, an indication of a location of the user comment relative to a displayed area of the media item can be stored in association with the user comment. \nFor example, coordinates referencing the area of a displayed embedded interface window, or the area of a media display window in the embedded interface, can indicate the location where the user comment was positioned by the user.  The method continues to\nblock 522.\n In block 522, identification of the associated chat conversation is stored in association with the user comment.  This allows, for example, the user comment (and other user comments input and stored for embedded applications associated with the\nchat conversation) to be retrieved and displayed during later playback of the media item for users of the same chat conversation (e.g., for member users from the same group of chat users who participated in the chat conversation when the user comment was\ninput).  The user comment will not be retrieved and will not be displayed for future playback of the media item in an embedded interface associated with a different, second chat conversation.  For example, that second chat conversation can have its own\nset of associated user comments to display during playback of the media item, which were input to an embedded media player application associated with the second chat conversation.  In some implementations, the user comment is not retrieved and displayed\nfor later embedded playback of the media item in other chat conversations or by other users.\n In block 524, the user comment is displayed in the embedded interface of the member device at which the user comment was received, and the user comment is sent (e.g., via the messaging application on the user device) to other member devices\nduring the current playback such that the user comment is displayed in the embedded interfaces of the other member devices.  In some examples, the user comment is displayed for a predetermined period of time during the playback of the media item before\nbeing removed from display in the embedded interface.  The period of time can be determined based on user preferences, settings for the embedded session, etc. In some implementations, the user comment is displayed in response to selection of an\nassociated control by the user, e.g., a toggle or continuous pressing of a displayed control as described above, and the user comment is removed from display when the user stops selecting the control or again toggles the control.  The method can then\ncontinue to block 512 to enable users to join or leave the embedded session and perform playback of the media item in block 514.\n If the user input is not a user comment as determined in block 518, then the method continues to block 526 where it is checked whether the user input is a selection of a seek point in the media item.  In some examples, the user can select a seek\npoint on a graphical progress bar or timeline displayed in the embedded interface which represents the entire play length of the media item.  In other implementations, the seek point can be selected in other ways, e.g., by receiving input specifying a\nparticular time along the timeline, by receiving selection of a fast forward or rewind control that moves the playback forward or back along the timeline, etc.\n If the user input is a selection of a seek point, then in block 528, the playback position of the media item is updated to the seek point.  For example, in some cases where the seek point has moved the playback position from its former position,\nthe playback of the media item can be discontinued from its former position and continue at the updated seek point.  The method continues to block 530.\n In block 530, the seek point information is sent to synchronize playback of devices.  For example, information describing the seek point, such as a time value of the adjusted seek point or other value, etc., can be sent to the other member\ndevices (e.g., via the messaging application) to synchronize the playback of the media item on those devices with the playback on the member device that received the updated seek point from a user.  In some implementations using a shared timer and/or\nglobal playback position as described above, the member device overwrites the value of the global playback position (g) with the new value of the playback position at the seek point.  The method can then continue to block 512 to enable users to join or\nleave the embedded session and perform playback of the media item in block 514.\n If the user input does not select a seek point as determined in block 526, then the method continues to block 532, where one or more device or application functions can be performed based on the received user input, if applicable.  The method\ncan continue to block 512 in some cases.  In some implementations, the method continues to block 534 in response to a later playback of the media item.\n In block 534, associated user comment(s) that were stored in association with the media item and chat conversation are displayed during later playback of the media item in the same (e.g., ongoing) chat conversation.  The associated user comments\ncan be displayed in the embedded interface during the later playback.  For example, in some implementations, if the media item received one or more user comments which were stored in association with the media item in blocks 518 to 524, then the\nassociated user comments can be displayed in conjunction with later playback of the associated portions of the media item by a chat user.  In some examples, if the media item is played in the embedded interface in the chat conversation (e.g., having the\nsame conversation ID, or having the same participating chat users as when the comment was originally input, or having a subset of the original chat users), and a portion of the media item that is associated with one or more user comments is displayed,\nthen the associated user comments are retrieved from storage and also displayed in the embedded interface along with the associated media item portions.  In some examples, the associated user comments can be displayed at the same location in the embedded\ninterface at which they were input, e.g., same coordinates in a playback window, if the interface locations of user comments has been stored.  In some examples, after a predetermined time duration or amount of media item data has been played, the\nassociated user comment(s) can be removed from the embedded interface.  In some implementations, the user comments can also or alternatively be displayed in the chat conversation of the chat interface during the later playback, e.g., as chat messages. \nIn some implementations, the user comments may be displayed as a list below or next to the display of the media item during playback or accessible via a control on a playback interface in the embedded interface (e.g., switching between comment view and\nplayback view).\n In further examples, as described above, user comments may have been stored in association with a period of time of playback of the media item while the user continuously selected or toggled a control, and these user comments can represent the\nuser's emotion or thoughts during the corresponding portion of the media item playback (e.g., via emoticons, images, etc.).  With multiple such user comments stored from input from multiple users who previously viewed the media item, this allows\nsynchronization of the thoughts and emotional states of member users who were concurrently watching and commenting on the media item.  The user comments for corresponding portions of the media item can be displayed as markers or icons on a time bar or as\noverlays on the display of the media item (e.g., displayed over video frames), where a user comment can be displayed in full upon selection of an associated marker or icon.  Such user comments can register the thought and emotional state changes in users\nduring replay (later playback) of the media item by same or different users within the same chat conversation.\n In some implementations, a save feature can be provided to allow a user to save a playing media item for later playback.  This can be useful for media items with relatively long durations (e.g., movies or TV shows) that may not often be viewed\nin chat conversations.  In some examples, a user can be allowed, via provided user input, to access an associated repository of unplayed media items, e.g., stored on the user device and/or on a server (e.g., session server or messaging server), allowing\nthat user to play the media items at a desired time, all in succession at one time, etc. For example, the saved media items may have been previously played by other chat users, and the user did not participate in those embedded sessions.\n In some examples, each media item played in an embedded session (e.g., initiated by a different chat user) can be associated with a displayed option (e.g., UI element) in the chat interface and/or embedded interface to save the media item for\nlater playback.  For example, this option can be displayed in association with a notification received by the user device that indicates that an embedded session is active (and which also can include a join option), as described with reference to FIGS.\n2-3.  This option can also or alternatively be displayed within an embedded interface, e.g., next to or otherwise in association with a displayed list of identifications of media items playing and to be played in the embedded application.  Selection of\nthe option by user input causes that media item to be stored into a queue associated with the selecting user.  In some examples, the media item can be stored in a different chat conversation in which the user and a particular bot participate.  In some\nimplementations, a user can request to automatically save all media items initiated by other chat users in embedded sessions in a chat conversation.  Any media item stored in the queue can be later selected and played in an embedded session for the\nembedded media player application that is then initiated to play the media item.  In some implementations, the embedded interface and/or chat interface can receive comments from the user during the later playback, where the comments are displayed in the\nembedded interface and/or sent and provided to the chat conversation in which the media item was originally played.  In some examples, when a media item from the queue is selected for later playback, a comment control is displayed to enable comments on\nthe selected media item to be input by the user.  In some implementations, selection of the comment control can display the originating chat conversation, insert a media item link as a message in the chat conversation, and enable the user to compose\ncomment messages in the chat conversation.  In some implementations, a link to a saved or played media item can be displayed as a selectable message in the chat conversation (e.g., an image of a video with a play button, or a text message), such that\nselection of the message by a chat user causes the media player embedded application to be initiated and the selected media item to be played.  Some implementations can provide a similar save feature for other types of content data output in other types\nof embedded applications, e.g., game data displayed over time (e.g., to record a particular play of a game), document data, travel itineraries, etc.\n In various example implementations that can use features described above, a selection of a media item is received in a first chat conversation that includes a plurality of participants.  For example, the chat conversation can be implemented on\nmultiple user devices connected by a network.  The media item can be provided by a user device and/or a server connected to one or more of the user devices by the network.  Playback of the media item is initiated in the first chat conversation such that\nthe media item is displayed by the user devices of the participants.  A comment is received from a first participant, the comment being associated with the media item and associated with a particular portion of the media item.  The comment is stored,\nand, after storing the comment, a user device of a second participant is detected displaying the particular portion of the media item.  In response to detecting that the user device of the second participant is displaying the particular portion, the\ncomment is caused to be displayed by the user device of the second participant.\n Features for these implementations can additionally include displaying the comment by the second participant's user device in response to determining that the second participant is a participant of the first chat conversation in which the first\nparticipant was participating in when the comment was received.  Storing the comment can include storing identification information that is associated with the comment, where the identification information includes an identification of the first chat\nconversation.  In some examples, the identification information can also include an identification of the media item, and an identification of the particular portion of the media item.  The comment can include a rating and/or a user comment.  In some\nexamples, the playback of the media item in the first chat conversation is synchronized among the user devices of the participants, e.g., by providing a reference number accessible by the user devices over a network, where each of the user devices\nattempts to synchronize with the reference number at regular time intervals.\n In further examples, initiating the playback can be performed on a first user device of the user devices, and a notification can be provided to one or more other user devices that the playback of the media item has been initiated by the first\nuser device.  The other user devices send data (and/or request) to join the playback, and the other user devices are added to an embedded session including the first client device.  In response to adding the user devices, the playback of the media item\nis synchronized on the other user devices to a current playback position on the first client device.  In another example, initiating the playback of the media item includes displaying data of the media item in an embedded interface displayed on each of\nthe user devices in association with the chat conversation provided by a messaging application.  The embedded interface is provided by an embedded application executing in association with the messaging application.\n In some example implementations, a selection of a media item is received on a first client device from a first participant of the participants in a chat conversation.  A first command is sent to one or more client devices associated with the\nparticipants to initiate playback of the media item.  User input is received, indicative of a seek point in the media item on a second client device from a second participant.  In response to the user input, a second command is sent to the one or more\nclient devices associated with the plurality of participants to update playback of the media item to the seek point.\n For example, the media item can be a first media item.  Input can be received from the second participant selecting a second media item, and the second media item can be added to a playlist queue indicating an order of playback of the first\nmedia item and the second media item.  A synchronization of the playback of the media item can be performed for the first client device and the one or more client devices, including updating a global value indicative of the position of the playback.  In\nsome implementations, a notification is provided to the one or more client devices associated with the plurality of participants that the playback of the media item has been initiated, an indication is received that the one or more client devices are to\njoin the playback, the one or more client devices are added to an embedded session including the first client device, and, in response to adding the one or more client devices to the session, the playback of the media item is synchronized on the one or\nmore client devices to a current playback position on the first client device.  In some example, initiating the playback of the media item in the first chat conversation includes displaying data of the media item in an embedded interface displayed on\neach of the user devices in association with the chat conversation provided by a messaging application.\n FIG. 6 is a flow diagram illustrating another example method 600 to provide output of an embedded application in association with a messaging application.  In some examples, method 600 can be implemented similarly as one or more blocks of FIG. 2\nin which an embedded application is initiated from a chat conversation and data is processed in an embedded session involving chat users.\n In block 602, it is checked whether user consent (e.g., user permission) has been obtained to use user data in the implementation of method 600.  For example, user data can include messages sent or received by a user, e.g., using messaging\napplication 103, user preferences, user biometric information, user characteristics (identity, name, age, gender, profession, etc.), information about a user's social network and contacts, social and other types of actions and activities, content,\nratings, and opinions created or submitted by a user, a user's current location, historical user data, images generated, received, and/or accessed by a user, videos viewed or shared by a user, etc. One or more blocks of the methods described herein may\nuse such user data in some implementations.\n If user consent has been obtained from the relevant users for which user data may be used in the method 600, then in block 604, it is determined that the blocks of the methods herein can be implemented with possible use of user data as described\nfor those blocks, and the method continues to block 608.  If user consent has not been obtained, it is determined in block 606 that blocks are to be implemented without use of user data, and the method continues to block 608.  In some implementations, if\nuser consent has not been obtained, blocks are to be implemented without use of user data and with synthetic data and/or generic or publicly-accessible and publicly-usable data.\n In block 608, an embedded application has been initiated and executes to provide display output in an embedded interface on a user device in association with a messaging application providing a chat conversation, where the user device is one of\nmultiple member devices participating in the chat conversation which have joined an embedded session associated with the embedded game application, similarly as described above for FIGS. 2-4.  The method continues to block 610.\n In block 610, it is determined whether local input has been received to the embedded application executing on the user device.  For example, local input can include user input received from the user of the user device on which the embedded\napplication executes.  If local input has been received, the method continues to block 612, where one or more state(s) of the embedded application are updated based on the local input.  For example, the local input may include one or more commands to the\nembedded application which cause one or more application states to be changed.  In this example, an application state can include a state that is updated in all the embedded applications of the member devices of the embedded session.  For example, in a\ngame application, the update can be to a game state, e.g., a change in position of one or more displayed game objects, a change in score, etc. In a media player application, the update can be to a playback characteristic of a playing media item, e.g.,\nchange playback position, stop, pause, rewind, fast forward, etc. In some cases, a state updated at the user device may not be updated at the other member devices, e.g., a change in a view of an environment as displayed in the embedded interface (e.g., a\ngame environment) may not be updated at the other member devices if the view on each member device is independently controlled by the user of that device in that particular embedded application.  The method continues to block 614.\n In block 614, the updated state is sent to the other member devices of the embedded session, if the updated state is of a type that is updated among the member devices.  For example, the embedded application can provide data describing the\nupdated state to the messaging application, and the messaging application can transmit the data to the other member devices of the embedded session.  The method continues to block 616.\n If local input was not received in block 610, or after block 614, the method continues to block 616, where it is determined whether input has been received from one or more of the other member devices of the embedded session over the network\n(e.g., remote input).  If such input has been received, the method continues to block 618, where the application state is updated based on the input.  For example, the input may indicate an updated application state of the corresponding embedded\napplication on a different member device, and the application state of the embedded device on the user device is updated to synchronize with the other member devices.  The method continues to block 620.\n If input was not received in block 616, or after block 618, the method continues to block 620, where it is determined whether one or more message conditions have occurred in the embedded application or the messaging application for one or more\nnon-user application objects provided in an application environment.  For example, the application environment can be a game environment, and the one or more non-user application objects can represent, for example, characters or entities within the game\nenvironment that are not controlled by any of the player users in the embedded session, e.g., a non-player game entity that the player users of the game can interact with in the game environment.  In another example, application objects (game objects)\ncan include non-player game characters playing with or against the player users of the game, such as a simulated opponent or team member.  In another example, the application environment can be a media player environment displaying a media item, and a\nnon-user application object can be a character in displayed content data (e.g., a character in a movie or image).  In some examples, the application object can be an assumed character or persona adopted by a bot and displayed as a character.\n A non-user application object can be enabled to output a message.  For example, if the application object is a character, the message can be in accordance with the character's actions or role in the embedded application and session.  In some\nimplementations, multiple different application objects can be implemented in the embedded session that may output different types of messages.\n One or more message conditions can be predefined and stored, and can be checked during embedded application execution.  The message conditions may cause an associated application object to output a chat message if the message condition occurs. \nFor example, a particular application object can be associated with one or more message conditions, such that if the one or more message conditions occur (or, alternatively, if any one of the one or more message conditions occur), the associated\napplication object(s) are designated to output a predefined message or a message to the chat conversation determined based on a current application state, current member users, etc. A variety of message conditions and/or types of message conditions can\nbe defined to trigger the output of a message by one or more associated non-user application objects.  A message condition can include, for example, an event occurring in the application environment which affects the application object (e.g., a\nplayer-controlled object colliding with the application object, a user inputting a request addressed to an application character, etc.).  A message condition can include, for example, one or more member users joining or exiting the embedded session.  A\nmessage condition can include, for example, a user action against or assisting a particular character in the application environment.\n In some implementations, message conditions can include the input of particular chat messages or types of chat messages by member users in the chat conversation.  For example, the input chat messages may refer to an application object.  In some\nexamples, if a player user in a game session inputs a message in the chat conversation such as \"let's go after that enemy on the left,\" this message can be parsed and analyzed by the messaging application (or embedded application, other application, or\nserver) and determined to refer to a specific game object.  This can satisfy a message condition associated with that game object that is triggered when a player message refers to the game object.  If no message condition has occurred, the method can\ncontinue to block 608.\n If a message condition has occurred as determined in block 620, the method continues to block 622, in which object identities of the application objects which are associated with the message condition are provided from the embedded application\nto the messaging application.  For example, the object identities can be labels that provide names of the application objects as displayed by the embedded application and session, or can be other types of identifiers of application objects.  In addition,\nin some implementations, message data associated with the application object and with the message condition can be provided from the embedded application to the messaging application in block 620.  The message data can be text or other form of message\n(e.g., image, symbols, audio message, etc.) or can be an indication of the message (e.g., a flag or value indicating a particular message or type of message).  In some implementations, the embedded application can provide the message condition(s) that\nwere detected to have occurred to the embedded application.  The method continues to block 624.\n In block 624, the messaging application assigns a chat identity to each application object.  In some examples, the chat identity is a chat name that is provided by the embedded game application, or the application object can be assigned a chat\nname by the messaging application based on data received from the embedded application.  In some implementations, the messaging application can determine the message for the application object based on message data provided by the embedded application. \nFor example, the messaging application can receive a value from the embedded application that corresponds to a particular text message in a look-up table or other data structure.  In some implementations, the messaging application can receive one or more\nmessage conditions that were detected to have occurred in the embedded application, and determine associated messages based on the message conditions.  In some examples, the messaging application can send data descriptive of the occurring message\nconditions to a server that can determine a message for the application object and send the determined message to the messaging application.  The method continues to block 626.\n In block 626, a message associated with each determined chat identity for an application object is output in the chat interface in the chat conversation.  Each chat identity represents an associated application object that provides a message in\nthe chat conversation to chat users (e.g., member users only in some implementations).  This allows the member devices of the embedded session to display the message in their chat interfaces and the member users to view the message.  In some\nimplementations, the messaging application can output the message in the chat interface of one or more particular member devices and not in the chat interface of other member devices, e.g., a direct message output in the chat interface of one or more\nparticular member devices.  For example, if a particular application object is to provide a message to a subset of the member users due to being interacted with by those users in the application environment (e.g., via user-controlled application objects\nor by direct input from the member users), then the message can be sent to and displayed by the member devices of that subset of users.\n The output message can take a variety of forms and can be based on the message condition and a set of stored messages associated with that application object.  For example, if the message condition is met by one or more member users joining an\nembedded game session and the associated game object is a player-enemy game object (e.g., character), the output message for that game object can be, \"Another victim!\" A similar message condition can be associated with a friendly-to-players game object\n(e.g., character) which can provide a message such as, \"Welcome, User.\" In another example, in response to a message condition being met by a new user joining the embedded session, an associated application object (e.g., character) can provide assistance\nto the new user by sending one or more messages including hints or tips for application user (e.g., how to play the game) that are displayed only in the new player's chat interface.  In another example, if a message condition is met by an event in the\nembedded application occurring to an application object, the associated message can include a description of the status of the application object (e.g., a game character that says, \"I need help from all of you\" or \"beware, I am stronger\").  In another\nexample, if a message condition is met by a user action against a non-user application character, the non-user character can provide messages such as taunts (e.g., in a game, \"your efforts are useless\").  In another example, if a message condition is met\nby user actions that assist a non-user application object (e.g., a character), the application object can provide messages that thank the assisting users (e.g., \"your help was greatly needed\").  In another example, a message condition can be met by the\ninput of a particular user chat message from a member user in the chat conversation, where the chat message refers to a game object (e.g., \"let's go after that enemy on the left\").  In response to the message condition being met, the associated message\noutput by a chat identity representing the game object can include text such as, \"You will find me a worthy opponent.\" In some implementations, such messages can include images, emojis, or other types of data.\n Examples of Providing Suggested Items in Association with Embedded Applications\n FIG. 7 is a flow diagram illustrating an example method 700 to provide suggested response items in association with an embedded application in a chat conversation.\n Suggested response items (also referred to as \"suggested items\" herein) may be generated and provided in the chat interface and/or the embedded interface for selection by a user in a variety of contexts.  Suggested response items may be\ngenerated and provided to the user automatically, upon consent from the user and one or more other users that sent and/or received the image.\n Suggested response items can be determined based on one or more techniques that can provide relevant results to a set of data.  In some implementations, suggested response items can be determined based on stored (e.g., in a look-up table or\ndatabase), predefined associations of suggested response items with particular suggestion events and/or types of suggestion events that have occurred, as described below.  Suggested response items can be or include one or more of different content data\ntypes, including text, images, animated images (e.g., cinemagraphs), audio segments, video, other types of documents (e.g., spreadsheets, tables, etc.), etc. Suggested response items can also include or be presented as interactive cards.  In some\nimplementations, suggested response items can include attached (e.g., lightweight) embedded applications which perform one or more actions or provide output, as described below.  Multiple types and suggested responses of these types and features can be\npresented in a menu (e.g., drop down menu, carousel selection menu, etc.).\n In some examples, suggested response items can be determined using one or more machine learning techniques.  In some examples, messaging application 103a/103b may implement machine learning or interface with a machine-learning application, e.g.,\na deep learning model, that can enhance user interaction with messaging application 103.  Suggestion server 156, for example, can utilize machine learning to provide suggestions to a messaging application.  In some implementations, machine learning can\nimplemented on one or more other components of the environment 100 and, for example, not using suggestion server 156.\n A machine learning model can be created, based on training data, prior to receiving a suggestion event for which suggested response items are to be generated, so that upon receiving the indication of an suggestion event, suggested response items\ncan be generated using the existing model.  Machine-learning models may be trained using synthetic data or test data, e.g., data that is automatically generated by a computer, with no use of user information.  Synthetic data can be based on simulated\nevents occurring in embedded applications and embedded sessions, and responsive commands and messages, where no human users are participants.  In some implementations, machine-learning models may be trained using sample data or training data, e.g.,\ncommands and messages actually provided by users in response to embedded application and session events and who consent to provide such data for training purposes.  Training data is treated before use to remove user identifiers and other user-related\ninformation.  In some implementations, machine-learning models may be trained based on sample data for which permissions to utilize user data for training have been obtained expressly from users.  After the machine learning model is trained, a\nnewly-occurring set of data can be input to the model and the model can provide suggested items based on its training with the sample data.  Based on the sample data, the machine-learning model can predict messages and commands to occurring events in an\nembedded session, which may then be provided as suggested response items.  User interaction is enhanced, e.g., by reducing burden on the user to determine a command or compose a message to an application event, by providing a choice of response items\nthat are customized based on the occurring event and the user's context.  Some examples of machine-learning application and machine-learning features are described below with reference to FIG. 12.\n In some examples, when users provide consent, suggested response items may be customized based on the user's prior activity, e.g., earlier messages provided in a conversation, messages in different conversations, earlier commands provided by the\nuser to the embedded application or to a different embedded application program, etc. For example, such activity may be used to determine an appropriate suggested item for the user, e.g., a playful message or command, a formal message, etc. based on the\nuser's interaction style.  In another example, when the user specifies one or more user-preferred languages and/or locales, messaging application 103a/103b may generate suggested items in the user's preferred language.  In various examples, suggested\nitems may be text messages, images, multimedia, encoded commands, etc.\n In some implementations, machine learning may be implemented on one or more components of environment 100, e.g., suggestion server 156, messaging server 101, client devices 115, either or both messaging server 101 and client devices 115, etc. In\nsome implementations, a simple machine learning model may be implemented on client device 115 (e.g., to permit operation of the model within memory, storage, and processing constraints of client devices) and a complex machine learning model may be\nimplemented on messaging server 101, suggestion server 156, and/or a different server.  If a user does not provide consent for use of machine learning techniques, such techniques are not implemented.  In some implementations, a user may selectively\nprovide consent for machine learning to be implemented only on a client device 115.  In these implementations, machine learning may be implemented on client device 115, such that updates to a machine learning model or user information used by the machine\nlearning model are stored or used locally, and are not shared to other devices such as messaging server 101, servers 135 and 150-156, or other client devices 115.\n For the users that provide consent to receiving suggestions, e.g., based on machine-learning techniques, suggestions may be provided by messaging application 103.  For example, suggestions may include suggestions of content (e.g., movies, books,\netc.), schedules (e.g., available time on a user's calendar), events/venues (e.g., restaurants, concerts, etc.), and so on.  In some implementations, if users participating in a chat conversation via their devices provide consent to use of conversation\ndata, suggestions may include suggested message responses to messages input by other chat users that are based on conversation content.  For example, if a first user of two users that have consented to suggestions based on conversation content, sends a\nmessage \"do you want to grab a bite? How about Italian?\" a response may be suggested to the second user, e.g., \"@assistant lunch, Italian, table for 2.\" In this example, the suggested response includes a bot (identified by the symbol @ and bot handle\nassistant).  If the second user selects this response, an assistant bot is added to the chat conversation and the message is sent to the bot.  A response from the bot may then be displayed in the conversation, and either of the two users may send further\nmessages to the bot.  In this example, the assistant bot is not provided access to the content of the chat conversation, and suggested items are generated by the messaging application.\n In certain implementations, the content of a suggested item may be customized based on whether a bot is already present in a chat conversation or is able to be incorporated into the chat conversation.  For example, if it is determined that a\ntravel bot could be incorporated into the messaging application, a suggested message response to a question about the cost of plane tickets to France could be \"Let's ask travel bot!\"\n In different implementations, suggestions, e.g., suggested messages as described herein, may include one or more of: text (e.g., \"Terrific!\"), emoji (e.g., a smiley face, a sleepy face, etc.), images (e.g., photos from a user's photo library),\ntext generated based on templates with user data inserted in a field of the template (e.g., \"her number is &lt;Phone Number&gt;\" where the field \"Phone Number\" is filled in based on user data, if the user provides access to user data), links (e.g.,\nUniform Resource Locators), message stickers, etc. In some implementations, suggested messages may be formatted and/or styled, e.g., using colors, fonts, layout, etc. For example, a suggested message that includes a movie recommendation may include\ndescriptive text about the movie, an image from the movie, and a link to buy tickets.  In different implementations, suggested messages may be presented as different types of user interface elements, e.g., text boxes, interactive cards, etc.\n In various implementations, users are offered control over whether they receive suggested items, what types of suggested items they receive, a frequency of the suggested items, etc. For example, users may decline to receive suggested items\naltogether, or may choose specific types of suggested items, or to receive suggested items only during certain times of day.  In another example, users may choose to receive personalized suggested items.  In this example, machine learning may be used to\nprovide suggested items, based on the user's preferences relating to use of their data and use of machine learning techniques.\n In block 702, it is checked whether user consent (e.g., user permission) has been obtained to use user data in the implementation of method 700.  For example, user data can include messages sent or received by a user, e.g., using messaging\napplication 103, user preferences, user biometric information, user characteristics (identity, name, age, gender, profession, etc.), information about a user's social network and contacts, social and other types of actions and activities, content,\nratings, and opinions created or submitted by a user, a user's current location, historical user data, images generated, received, and/or accessed by a user, videos viewed or shared by a user, etc. One or more blocks of the methods described herein may\nuse such user data in some implementations.\n If user consent has been obtained from the relevant users for which user data may be used in the method 700, then in block 704, it is determined that the blocks of the methods herein can be implemented with possible use of user data as described\nfor those blocks, and the method continues to block 708.  If user consent has not been obtained, it is determined in block 706 that blocks are to be implemented without use of user data, and the method continues to block 708.  In some implementations, if\nuser consent has not been obtained, blocks are to be implemented without use of user data and with synthetic data and/or generic or publicly-accessible and publicly-usable data.\n In block 708, a (first) embedded application is executed for a first user device in association with a (first) messaging application providing a (first) chat conversation, where the first user device is one of multiple user devices participating\nin the chat conversation and which have joined an embedded session created in association with the chat conversation, similarly as described above for FIGS. 2-4.  The user devices are member devices running corresponding embedded applications as\ndescribed above.\n In block 710, it is determined whether a suggestion event has occurred, e.g., an event that can cause one or more suggested response items to be generated as described for FIG. 7.  In some implementations, a suggestion event can include an event\nthat occurs in association with an embedded application.  Such suggestion events can occur in an embedded application of the embedded session, in the messaging application, in a server used in the embedded session, and/or in a different application or\ndevice in communication with the embedded session, member devices, or chat devices.  Suggestion events can include reception of user input at a device, a change in state in a program, communication of data between devices, connection of a user device\nwith an embedded session or a device over the network, reception or transmission of data at a device (e.g., between programs, over a network, etc.), detection of content features in media data or other data, detection of particular topics in chat\nmessages, one or more of these events occurring at a previous time (e.g., indicated in historical data), etc. For example, in some implementations, suggestion events can be checked for occurrence in the messaging application (e.g., chat conversation),\nsuch as chat users entering or leaving a chat conversation, particular detected content in chat messages input to the chat conversation by chat users, etc. Other examples are described herein.  A suggestion event triggers generation of one or more\nsuggested response items that can be presented on a member device (or chat device) in the chat conversation or chat interface and are selectable for use by the user of the device, as described below.\n In various implementations, the embedded application and/or messaging application can determine whether an event qualifies as a suggestion event.  In some implementations, a suggestion event can be any of multiple predefined types of events in\nembedded applications, messaging applications, and/or other applications and devices that have been designated to be suggestion events that cause suggested response items to be generated for a user.\n In some implementations, suggestion events can be determined based on one or more particular events that occur at one or more of the chat devices that are participating in the chat conversation.  For example, such particular events can include\nuser input received by a chat interface and/or by an embedded interface of the first user device and/or of other member devices or chat devices.  In some examples, such particular events can be chat user actions and/or member user actions performed in\nthe embedded application based on the user input received by one or more member devices from associated chat user(s) and/or member user(s).  The first user device (e.g., first messaging application) can receive indications that particular events have\noccurred at other devices (e.g., member devices, chat devices, and/or other devices), e.g., by receiving data over the network indicating and/or identifying the particular events.\n For example, a suggestion event can be a user joining or leaving the embedded session based on user input at a member device or chat device, or a different action based on user input and affecting one or more states of the embedded session\n(e.g., affecting one or more states of the corresponding embedded applications running on the member devices for the embedded session).  A suggestion event can be an embedded application being selected by a user in the chat interface and/or the embedded\napplication being initiated by a member user.  A suggestion event can be an action performed by a member user in the embedded application (e.g., starting playback of a media item, performing a game action, adding or modifying a displayed, shared object\nprovided in the embedded application and displayed in the embedded interface, e.g., a document object, game object, etc.).  A suggestion event can be the current display (e.g., in the embedded interface) of a particular user comment that was previously\nadded to a media item by a member user (e.g., as described with reference to FIG. 5).  A suggestion event can be a change of an item or element in a shared content document based on local (first user device) or remote (other member/chat device) user\ninput, or other shared data being edited in an embedded application of the embedded session.  A suggestion event can be a selection or confirmation of a particular travel itinerary by a member user in an embedded travel itinerary application.\n In some implementations, suggestion events can be determined based on one or more particular events that are embedded application events occurring at one or more member devices, e.g., within one or more embedded applications of the embedded\nsession.  In some implementations, some types of embedded application events are based on user input, and some types of embedded application events are not based on user input.  In some examples, a suggestion event can be based on an embedded application\nevent such as particular content data (e.g., media item data) being played in the embedded application.  For example, the particular content data can be one or more content features (or types of content features) that are detected in visual content\n(e.g., an image or video), or audio content (e.g., an audio segment of data) of a played media item using one or more image recognition and/or voice/audio recognition techniques.  For example, the content features can include visual content features\n(e.g., faces if user consent has been obtained, landscape features, monuments, objects, etc.) and audio content features (e.g., voices, particular sound effects, etc.).  The content features can include geographic locations, e.g., provided as metadata\nwith content items such as videos, images, audio segments, games, etc. A suggestion event can be the attainment of (or failure to attain) a particular objective or score in an embedded game application, or attainment of a score difference between a\nfirst-place player and the next-place player in a game that is greater than a predefined threshold.  A suggestion event can be an opportunity for a player to make a particular action or move in an embedded game application (e.g., a game application\nwaiting on a player action to advance the game state, triggered by a different member user having performed a game action, or triggered by time, non-player events in the game, etc.).\n In some implementations, suggestion events can be embedded application events of particular types.  For example, user member events can include a chat user starting a new embedded session (e.g., by initiating an embedded application at a chat\ndevice), a chat user joining an active embedded session, a member user exiting an active embedded session, or the last member user of an embedded session exiting embedded session and/or otherwise ending an embedded session.  Certain designated embedded\napplication events can include starting or ending events, e.g., a game is won by a member user (causing a game to be completed), a content document is created or deleted, a new video starts or ends within a media player application, etc. Synchronize\nevents can include events in which data is provided to member devices to synchronize the embedded applications on the member devices (e.g., an input user action changes a game state, a state is changed of a playback position of a media item played in a\nmedia player application, an item on a shared list is changed, etc.).  Suggestion events can be defined to be the occurrence of one or more of such defined types of embedded application events.\n In some implementations, event information can be sent between applications and/or between chat devices to indicate that particular events (and/or suggestion events) have occurred.  For example, event information can be passed from the embedded\napplication to its associated messaging application upon occurrence of a particular event that has occurred in the embedded application.  In some implementations or cases, the particular event has occurred in the embedded application, and the event\ninformation can indicate the occurrence of the particular event and/or can identify the particular event or the type of the event.  For example, the event information can indicate a type of the particular event (e.g., a game action has been performed in\na particular game, playback of a media item has been paused or changed to a different playback position, etc.), data associated with the particular event (e.g., time of occurrence of the event, a type of feature being displayed in a played media item\nthat was detected as the event, etc.), circumstances of the particular event (e.g., which member users caused the event or had a role, status, or score affected by the event, etc.).  Such particular events can include any of the events described herein,\ne.g., embedded application events, messaging application events, server events, etc.\n In some implementations, event information can be sent from one device to another device over the network, e.g., from a member device (or chat device) to a server device, from a server device to one or more member devices (or chat devices), etc.\nFor example, embedded event information can be from an embedded application indicating an event occurring in the embedded application, server event information can be from a server indicating an event occurring at the server, etc. In some examples, the\nmessaging application on a member device can receive event information from a different device that indicates an particular event has occurred on a different device.  In some implementations or cases, the messaging application can determine if an event\nindicated by the received event information is a suggestion event.  In some implementations or cases, the messaging application can pass the event information to an embedded application executing in association with that messaging application, where the\nembedded application determines if the event indicated by the event information is a suggestion event (and can communicate any resulting suggestion event back to the messaging application).\n In some implementations, event information received by the messaging application can indicate that the particular event is a suggested event.  For example, the source of the event information may have determined that the particular event\nqualifies as a suggestion event.  In some implementations, the event information can indicate the particular event and the messaging application can determine whether the particular event qualifies as a suggestion event.  In some implementations, the\nmessaging application can use such event information to obtain suggested items, e.g., determine one or more suggested items itself and/or send the event information (or a request derived from the event information) to a different program or device to\ndetermine suggested items which can be sent back to the messaging application.\n In some implementations, suggestion events can be particular events in or provided to the messaging application.  For example, a user entering (e.g., logging on, going online) or exiting the chat conversation can be a suggestion event.  One or\nmore particular chat messages input by member users in the chat conversation (or a particular set or sequence of chat messages) can be considered a suggestion event.  For example, chat messages can be parsed and one or more predetermined topics, words or\nkeywords, and/or phrases can be detected to determine a suggestion event (e.g., \"go out to eat,\" \"let's invite User4,\" \"let's watch MovieA,\" a particular name, address, location, etc.).  In some implementations, a machine learning model can be trained in\na training stage with synthetic or actual training data and, in an inference stage used in method 700, can be used to process a set of chat messages to predict if a suggestion event has occurred.  For example, the model can predict if any particular user\nactions (e.g., commands or further messages) are likely to be initiated or provided by users based on the content of the set of chat messages, and if such commands are within a threshold probability of occurrence, the input of the set of chat messages\n(e.g., the input of the last chat message of the set) can be considered a suggestion event.\n In some implementations, suggestion events can be particular events in or provided to the messaging application that change one or more states of an embedded application executing on the first device (e.g., which in turn causes a change in one\nor more states of the corresponding embedded applications executing on the other member devices).  For example, user input provided to the messaging application via the chat interface can direct commands to the embedded application (e.g., change playback\nstate of a playing media item, perform a game action, etc., based on text commands input as chat messages in the chat interface).  In some implementations, input in the chat interface that are not direct commands to the embedded application can change\none or more states of the embedded application.  For example, a selection of options for an embedded application may be performed via a menu or interface elements displayed in the chat interface external to the embedded interface.  In additional\nexamples, chat messages input in the chat interface may be parsed by an executing embedded application and may cause the embedded application to change one or more of its states to determine or process data, display particular output data, and/or\nretrieve data from storage, a different device, a network site, etc., as examples described herein.\n In some implementations, a suggestion event can be a particular event occurring within a different chat conversation provided by the messaging application that provides the first chat conversation.  For example, the particular event can be a\nuser message input in the different chat conversation, particular content of that user message matching predefined words or phrases (or other matches based on machine learning techniques), a user joining or exiting the different chat conversation, etc.\nIn some examples, a chat user of a different chat conversation can input a message indicating interest in joining the first chat conversation, and this message can be detected as a suggestion event to generate a suggested command (e.g., a command to add\nthe users of the different chat conversation to the first chat conversation and/or to the embedded session) or suggested message (e.g., a message to be output in the different chat conversation that greets the other users, invites the users of the\ndifferent chat conversation to the first chat conversation, etc.).\n In some implementations, a suggestion event can be a particular event that is a user action (e.g., input command) instructing the system to generate one or more suggested response items for the user.  For example, the user can select an\ninterface element in the embedded interface or the chat interface that is a command to generate one or more general suggested messages or suggested commands.  In some examples, such suggested messages can include comments about the embedded application\nor comments about one or more member users, or such suggested commands can include commands to exit an embedded session, resign a game, etc.\n In some implementations, a suggestion event can be based on received data from a different application executing on the user device that is different than the messaging application and embedded application, where the data indicates a particular\nevent has occurred in the different application.  For example, a videoconference application can send an indication (e.g., data) to the messaging application indicating that the videoconference has ended, a particular user has joined or exited the\nvideoconference, etc. In some implementations, a suggestion event can be a particular event that has occurred on a server or other device (e.g., in an application executing on the server or other device) that is in communication with the user device over\nthe network and, e.g., is providing data used in the embedded session.  For example, the particular event can include user input (e.g., comments or ratings for stored media items) the server has obtained from other user devices not participating in the\nchat conversation, e.g., user requests for second content data the same as first content data the server is providing to the embedded session, user comments or ratings for the second content data that is being sent to the other user devices, etc.\n A suggestion event can include a particular event occurring on the server, e.g., a game event (such as a change in game state, etc.) in a game executed by the server and streamed to member devices over the network, a particular type of content\nfeature being displayed in a media item being streamed to member devices from the server, etc. The messaging application can receive server event information from the server that indicates or describes the particular server event that has occurred, which\ncan qualify as a suggestion event for the messaging application or embedded application.\n A suggestion event can include a particular event that occurred at a previous time, if user consent has been obtained.  For example, the occurrence of one or more of the particular events described herein can be indicated or described in data\nstored on one or more devices, with user consent (e.g., a description of device communication, content of chat messages, etc.), and a device can search for such previous particular events in this data.\n In block 712, it is determined whether the detected suggestion event is an event (and/or has a type) that is associated with one or more suggested commands for the embedded application.  For example, some types of suggestion events can cause\ngeneration of suggested response items that are suggested commands for the embedded application.\n A suggested command for an embedded application can be, for example, a command to modify one or more states of the embedded application.  For example, the suggested command can be a suggested playback command to set a playback position or change\na current playback position to a new playback position of a media item that is being played in the embedded application.  In further examples, a suggested command can be a command to modify a game state in an embedded game application, e.g., perform a\nparticular game action such as move a game piece or character, change a game display viewpoint, select a game object, etc. A suggested command can be to add, modify or remove a particular item or element of a shared content document (e.g., shared list)\nprovided in the embedded application.  Suggestion event types that can provide suggested commands can include, for example, a joining or exiting of one or more member users to or from the embedded session, an action made by a player in a game embedded\napplication, a state of a game in which the game is waiting for an action or move from a player, an event in which a player or player's objects in a game are being interacted with by a different player or player's objects (e.g., an attack or conflict\nbetween objects, an offer or request for help from one player to another player, etc.), a detected change in visual or audio content of a media item (e.g., a change from a scene with low movement of objects to a scene having moving objects over a\nthreshold movement speed, a change from a talking scene to an action scene, a change of audio output from lower volume or constant sound to a higher volume over a threshold or sound of rapidly-changing amplitude over a threshold frequency, etc.), a\nchange performed to a shared content document by a different member user (e.g., a suggestion to prompt the user for additional changes or to change back the performed change), a change in the playback position of a played media item, etc.\n If the detected suggestion event is of a type that provides one or more suggested commands for the embedded application, then the method continues to block 714, in which one or more suggested commands are obtained and are based, at least in\npart, on the suggestion event.  In some implementations, suggested commands can be based on other factors in addition to or alternative to the suggestion event, e.g., one or more current state(s) of the embedded application and/or messaging application\n(e.g., current time position of playback in a media item, current game score, current number or identifications of the member users and/or chat users, time of day, etc.).  In some examples, the one or more suggested response items may be determined by\nthe embedded application and/or the chat application based on the suggestion event.  In some implementations, a user may be provided an option (e.g., displayed interface element) that causes suggested response items to be disabled, e.g., not generated\nand/or not displayed.\n In some implementations, the suggested commands can be determined based on stored, predefined associations of suggested commands with particular suggestion events and/or types of suggestion events.  For example, one or more particular types of\nsuggestion events may be associated with one or more particular commands, e.g., in a look-up table or database, such that the associated commands are retrieved based on the detected suggestion event.\n In some implementations, the embedded application may implement a recommendation engine (e.g., based on rules used in determining outcomes in the embedded application, other rule sets, decision trees, etc.) to determine the suggested response\nitems.  In one example, the suggested response items may be determined based on a decision tree (or other decision construct) in which the states of one or more conditions at nodes of the tree determine the path through the tree to determine suggestion\nitems as end results, where the conditions can include suggestion events and other conditions.  In another example, when the embedded application is a rules-based game (e.g., chess, poker, etc.), the suggested response items may be determined based on\nrules and/or objectives of the game.  For example, the suggested response item may be a move (e.g., Bg7, which indicates a movement of a bishop to chess board location g7) in the game.  For example, the ruleset or decision tree may determine the move\nbased on predetermined probabilities of winning.  In another example, when users permit analysis of user data, the suggested response item may be based on the user's prior accepted suggestions (e.g., the user normally opens a chess game with the move d4,\nindicating moving the pawn to location d4).\n In some implementations, when user permits use of user profile data, the suggested response items may be determined based on user profile data, e.g., user preferences or defaults directly specified by user input and/or inferred based on previous\nuser actions.  For example, if the embedded application is a shopping application (e.g., travel app, e-commerce app, etc.), and the suggestion event is that the user has navigated to a \"purchase\" user interface, the suggested response items may include\n\"pay with credit card,\" \"pay with bank account,\" \"pay with cash,\" etc. based on the user's profile.  Similarly, if user consent has been obtained, other user profile data such as a user's contact information (e.g., address, phone number, email, etc.) may\nbe included in the suggested response items.  In some implementations, e.g., when the embedded application is a \"quiz\" application, the embedded application may provide suggested response items as the multiple choices to answer a quiz question.\n In some implementations, suggested commands can be determined using a trained model for the embedded application and/or machine learning techniques as described herein.  For example, suggested commands can be a result of an inference stage of\nthe machine-learning model, where the model has been previously trained with particular suggestion events and types of suggestion events and with particular commands input by users in response to those suggestion events as training data, such that the\nmodel can produce likely or relevant suggested commands at the inference stage based on a newly-occurring suggestion event.  In some implementations, the suggested response items may be determined by a machine-learning application that is trained to\nprovide response items for the specific embedded application.  In some examples, a machine learning model used for suggested responses can be trained to recognize various game actions that can be performed in a particular embedded application, where the\nmachine learning model can provide suggested commands in the embedded application that triggers a responsive action in that embedded application, e.g., \"build a tower\", \"move pawn to space D1\", \"play this video next\", etc.\n In various implementations, the suggestion event may be analyzed to determine the one or more suggested commands (or other response items).  In some implementations, analysis of the suggestion event may include looking up a rule set or decision\ntree for the suggestion event.  In some implementations, analysis of the suggestion event may include determining a state of the embedded application (e.g., a progress level in a game, a user setting related to assistance during a game, etc.) to\ndetermine context of the suggestion event.\n In some example implementations, the embedded application is playing a media item, and the suggested commands can be provided to modify the playing of the media item.  For example, if the suggestion event is a chat user joining the embedded\nsession to view the media item, then a suggested command can be to restart the playback from the start of the media item (e.g., start a video or audio segment from the beginning of the segment), which can allow the new member user to view the media item\nfrom the beginning.  In some implementations, such a \"restart\" suggested command can be generated in response to the media item being at a current playback point that is a threshold time period after its start, such that the restart suggested command is\nnot generated if the playback position has been playing for a time period more than the threshold time period (e.g., since the existing member users may not wish to restart a media item that has been playing for a long time).  In another example, if the\nsuggestion event is a particular detected type of scene or sequence (e.g., a detected action scene or sequence) that has been detected to have occurred in the media item, suggested commands can include rewinding the playback position a particular amount\nof time to replay the detected scene or sequence from the start of that scene.  Another example suggested command can cause the embedded application to display a prompt or input field to allow the user to input a user comment that is then displayed\n(e.g., superimposed) on the displayed scene of the played media item (e.g., as described with reference to FIG. 5).  In another example, if the suggestion event is a change in the playback position of the media item by another member user, a suggested\ncommand can be to undo or revert the change in the playback position to the position it held prior to the change.  In another example, if the suggestion event is a change in the playback position, the suggested commands can include one or more further\nchanges in the playback position to the nearest (in one direction or either direction) scene transition, chapter division, segment division, or other marked division in the media file (e.g., commands such as, \"adjust to nearest previous chapter heading?\"\nand \"adjust to nearest later chapter heading?\").  In another example, if the suggestion event is a change in the audio volume output by the media item (e.g., by a threshold amount, or to a volume level above a threshold level), suggested commands can be\nto raise or lower the volume.\n In some example implementations, the embedded application is providing a game, and the suggested commands can be directed to the game.  For example, if the suggestion event is a game action by a second player of the game, a suggested command for\nthe first player can cause input of one or more possible actions to the game in response to the game action, e.g., as determined by game algorithms that can evaluate game strategy and actions.  In some examples, if the suggestion event is a move of a\ngame piece by an opponent player, the suggested commands can include various possible responding moves of the user's pieces in the game (e.g., in an asynchronous or turn-based game).  In another example, if the suggestion event is an invasion of a user's\ngame territory by enemy game pieces, the suggested commands can include commands to move pieces to intercept invaders, defend an invaded location with a player's pieces at the invaded location, etc. In another example, if the suggestion event is another\nplayer joining the game, suggested commands can include requesting the new player to join the user's team in the game, and requesting a different player to form a team against the new player.  In another example, if the suggestion event is a member user\nexiting the game (or changing of a player role to an observer role in the game), a suggested command can be to request that the game be paused until a replacement player joins the game.  In another example, if the suggestion event is the user's score (or\nother game objective) differing from (e.g., trailing or leading) other players' scores or objectives by a threshold amount, suggested commands can include sending hints to other players or requesting other players to provide hints to the user, requesting\nother players to handicap or boost themselves in the game, restarting the game, etc.\n In some example implementations, the embedded application is providing a shared object, e.g., a game object in a game, a shared content document such as a list or other document of items, etc. Suggested commands can be commands to change or\naffect the shared object, e.g., edit, remove, etc. the shared object, change one or more of its characteristics (e.g., type, color, size or dimensions, etc.).  For example, if the suggestion event is the initiation of a shared list in the embedded\napplication, a suggested command can be to add, delete, or change an item on the list.  In another example, if the suggestion event is the addition of an item to the shared list by a different member user, a suggested command can be to highlight or\notherwise graphically indicate the added item in the embedded interface.  In some implementations, the highlighting command can be accompanied by suggested text that can be output in the embedded interface or in the chat interface, e.g., \"do we need this\nitem?\" or \"I suggest removing this\" (e.g., a suggested message as described below).  In another example, if the suggestion event is removal of an item, a suggested command can include outputting text in the embedded interface (or chat interface), e.g.,\n\"why did you remove that?\" In another example, in response to a suggestion event of highlighting a particular item by the user, suggested commands can include moving the suggested item within the list one, two, or more places, up or down, to the\nbeginning or end, etc.\n In additional examples that can be applied to many types of embedded applications, if the suggestion event is a member user joining the embedded session, a suggested command can instruct that control over specified functions of the embedded\napplication be assigned to the new member user, if the new member user has been assigned a user status (e.g., user role) in the embedded application that allows such assigning of control.  For example, the suggested command can be to allow the new user a\nstatus able to change a playback position of a media item played in the embedded application, change items on a shared list, etc. In some implementations, a suggested command can be to remove such control abilities from a designated user (if the\ncommanding user is able to do so), e.g., if abusive or inappropriate language has been detected by the messaging application (or other program or device) in messages from the designated user in the chat conversation.\n Suggested commands can, if user consent has been obtained, also or alternatively be based on a history of prior actions and/or messages performed by the user in the same or similar types of embedded applications and/or for same or similar types\nof output data (e.g., media items, games, etc.).  For example, if the user has previously rewound or skipped the playback of a particular scene or type of scene in a video, a suggested command can be generated which causes the playback position of a\ncurrently playing media item to be rewound or moved in the same manner.\n Suggested commands can also or alternatively be based on member user input in the (first) chat conversation, as displayed in the chat interface of the messaging application.  In various implementations, this user input can considered part of the\ndetected suggestion event, or can be considered external to (e.g., supplementary to) the detected suggestion event.  For example, a suggested command can be generated to initiate an embedded session involving an embedded application that implements a\nmedia player, a game, a content document, or other functions, based on chat messages that mention such an activity (e.g., \"let's watch Funny Movie 2,\" \"let's play Chess,\" or \"we need a shopping list\", etc.).  In another example, a suggested command can\nbe generated to assign a user status or role to a particular member user in the embedded session if that member user has sent a message in the chat conversation indicating a desired user role (e.g., \"I'll play\" to indicate a player role, or \"I only want\nto watch\" to indicate an observer role).  In another example, a suggested command can be generated that moves a playback position for a played media item past a current scene or to a particular later scene if one or more member users have input messages\nin the chat conversation such as, \"let's skip this scene\" or \"let's watch the car chase.\"\n In some implementations, user data can be used to determine suggested commands and/or suggested messages, if consent to use such user data has been obtained from the user(s) related to or associated with such user data.  For example, the current\ngeographic locations of member users or chat users (or their devices) can be obtained by the messaging application and/or provided to the embedded application for use in determining suggested response items.  For example, suggested commands can refer to\nthe current geographic locations of users (with user consent), e.g., to instruct an embedded travel itinerary application to provide an itinerary based on current user location.  Other types of user data can cause generation of suggested items, if user\nconsent has been obtained.  In additional examples, user data such as a list of applications installed on the device (e.g., applications external to the messaging application) can be used to generate suggested items.  For example, suggested messages can\nbe generated that recommend one or more applications for use by other chat users in the chat conversation.\n In additional examples, user data such as SMS messages and phone calls received by the user device can be used to generate suggested commands to forward or annotate messages.  For example, the messaging application intercepts that an SMS message\nhas been received and can generate forwarding or annotation capabilities, e.g., a suggested command and/or suggested message to share that SMS message with the chat conversation, a suggested command to annotate an SMS image to another message (e.g., chat\nmessage, email message, etc.) and send it to another user, etc. In another example, the messaging application can detect that a phone call is being received while a user is actively inputting messages in the chat conversation, and the embedded\napplication can generate suggested messages to the chat conversation (e.g., \"I'm answering a phone call, will be back\").  User data such as fitness tracker information can be used to provide suggested messages that challenge other chat users in the chat\nconversation to fitness exercises.  User data such as current system audio volume/mute state on the user device can be used to provide suggested commands that automatically adjust media playback volume/on-off state before or during playback of the media\nitem.  User data such as browsing history can be used to generate suggested commands to display shopping suggestions for the user, e.g., in a browser embedded application.  User data such as user account information (e.g., login/password) to particular\nsystems or network sites can be used to provide suggested commands that access those systems in an embedded application.  User data such as recently-added users to a contact list (e.g., added within a threshold amount of time from the current time) can\nbe used to provide a suggested command to add one or more of those users to the chat conversation or a suggested command to invite one or more of those users to join a particular embedded session in which the user is participating.\n In some implementations, suggested commands can be to the embedded application to cause the embedded application control an external application executing on the first user device or on a different device in communication with the first user\ndevice over the network.  Some examples of such control are described herein.\n Some suggestion event types can cause generation of both suggested commands from blocks 714-718 and suggested messages from blocks 722-724.  In some implementations, suggested command items that provide suggested commands can be visually\ndistinguished from suggested message items that provide suggested messages to be output in the chat interface.  For example, different colors or highlighting can be used for these types of suggested response items to assist a user in distinguishing the\ndifferent types of suggested response items.\n In some implementations, a plurality of suggested commands can be generated and can be ranked based on one or more predetermined criteria.  For example, a ranking score for a suggested command can be based on how likely the suggested command is\nto be selected in response to the detected suggestion event, e.g., as indicated by the machine learning model and its training based on synthetic or actual data of prior commands provided in response to prior suggestion events.  In some implementations,\nsuggested responses can be provided with confidence ratings or rankings which indicate how likely that the suggested response will be selected by the user.  For example, such confidence ratings can be based on prior selections by the user (or multiple\nusers) of suggested responses provided for the same or similar suggestion events, if consent from such users has been obtained.  In some implementations in which machine learning models are used, the suggested responses may have rankings indicating which\nresponses are most likely to be selected as determined based on the training of the model.  A particular number of top-ranked suggested commands can be selected for output on a user device.  In some implementations, suggested responses can be displayed\nin order of their ranking or confidence rating.  In an example, a suggested response having a high confidence rating (e.g., over a threshold rating, or within a threshold number of top ratings) may be selected by user input as a command or message that\ntriggers an action in an associated embedded application.  In some implementations, after the high-confidence suggested response is generated and before it is selected by user input, the associated embedded application can be pre-loaded (e.g., downloaded\nin the background and stored in local storage) to the user device and its code initiated.  This allows an instant display of output from the embedded application on the user device after the high-confidence suggested response is selected by user input. \nSuch pre-loading of embedded applications can be omitted for lower-confidence (e.g., below threshold) suggested responses since they are less likely to be selected.\n In some implementations in which multiple embedded applications can be simultaneously executing for the first user device as described above, one or more suggested commands can be commands provided to multiple or different executing embedded\napplications.\n In some implementations, one or more of the suggested commands can be commands to the messaging application.  For example, a suggested command can include a command to the messaging application to add one or more particular chat users to the\nembedded session, where the particular chat users are specified in the suggested command.  For example, such a suggested command can be generated based on a suggestion event that is detected to have occurred by the messaging application (or embedded\napplication) based on one or more chat messages being input by the first user and/or other member user of the member session (e.g., \"let's ask Kenny if he wants to join\", etc.), or by one or more chat messages input by chat users that are not member\nusers (e.g., \"maybe we should join that movie\").  The suggested command can be presented to a member user, and/or to a chat user who is not a member user.  In some implementations, to determine such a suggestion event, particular chat user names, member\nuser names, names of content data, general words relating to an embedded session (e.g., \"movie,\" \"game,\" etc.) and other words can be detected in chat messages in conjunction with predefined keywords and key phrases, and/or parsing and analysis of chat\nmessages via machine learning techniques.  In some implementations, the suggestion event can also or alternatively be a detection by the messaging application that a content item (e.g., a played media item, a game, a content document, etc.) provided in\nthe embedded session (a first embedded session) is the same as a content item being provided to other chat users in a second embedded session concurrently with the display of the content data in the first embedded session.  This can cause the messaging\napplication to generate a suggested command to add the member users of the second embedded session to the first embedded session.  User comments associated with the content item by chat users outside the embedded session can also be a basis for a\nsuggestion event similarly as described below.\n In some implementations, a suggested command to the messaging application can include a command to add a user to the chat conversation of block 708 (e.g., first chat conversation), or add the user to the first chat conversation and the embedded\nsession of block 708 (e.g., the first embedded session).  For example, the suggested command can instruct that an external user who is currently not participating in the first chat conversation be added.  Such a command, when executed, causes the\nexternal users' devices to join the chat conversation.  In some examples, the external user can be participating in a different chat conversation that is provided by the first messaging application or provided by a different messaging application in\ncommunication with the first messaging application.  In some cases, one or more member users of the first embedded session may be participating in the different chat conversation simultaneously with participating in the first embedded session and the\nfirst chat conversation, and such member users can be provided the suggested command (e.g., in blocks 716 and 718) to add one or more external users from the different chat conversation.  For example, such a suggested command can be generated based on a\nsuggestion event that is detected to have occurred by the messaging application (or embedded application) based on one or more chat messages being input by one or more chat users of the first chat conversation (similarly as described above) and/or by one\nor more external users of the different chat conversation.\n In some implementations, a suggestion event can also or alternatively be a detection by the messaging application that a content item (e.g., a played media item, a game, a content document, etc.) provided in the embedded session is the same as\nan external content item concurrently being provided to the external users.  This causes the messaging application to suggest to add the external users to the first chat conversation.  In some implementations, the suggested command can be to add the\nexternal users to the first embedded session.  In some examples, the suggestion event can be triggered if a current playback position of the content item output by the embedded application is within a threshold amount (of time, data, etc.) of a current\nplayback position of the external content item output to the external users.  In some implementations, a suggestion event can also or alternatively be a detection by the messaging application that one or more user comments have been input by one or more\nexternal users associated with an external content item being output to the external users, and the external content item is the same as a content item being output by the embedded application.  In some examples, a suggestion event can be a detection of\na particular type of user comment by the external users, e.g., approval ratings by users for the content data, a reference to one or more of the member users of the first embedded session, etc. Such user comments can cause the messaging application to\ngenerate a suggested command to add such external users to the first embedded session (as member users) and/or first chat conversation (as chat users).\n In some implementations, if user consent from the relevant users has been obtained, user data can be used to determine a suggestion event causing generation of a suggested command to the messaging application (or other application) to add one or\nmore users (e.g., chat users or external users) to the embedded session or to the chat conversation as described above.  For example, user data (e.g., accessible database data or contact list data) can indicate particular users that have one or more\nparticular social relationships with one or more member users (or chat users), and such particular users can be selected to be included in a suggested command to add these users to the embedded session or chat conversation.  For example, such particular\nsocial relationships can include users that are in the contact lists of member/chat users, users that are in user groups created by member/chat users (e.g., social networking user groups), users within a particular degree of separation from member/chat\nusers, etc.\n In some implementations, one or more of the suggested commands can be commands to the first user device to perform one or more tasks external to the messaging application.  For example, the tasks can include opening and executing an application\nexternal to the messaging application on the first user device, and/or controlling an external application executing on a remote device in communication with the first user device.  For example, such an application can include a web browser application,\na communication application, a calendar application, a word processing application, a specific application to access a particular Internet service, etc. If the external application is executing on the first user device, it can provide display output on\nthe first user device.  The display output can include, for example, information received from a server device in communication with the first user device.  In some examples, a suggested command can include a command to open a web browser application,\ndisplay a user interface of the web browser application on the first user device, instruct the web browser application to access a web page at a particular link, and display the contents of a web page at the particular link in the user interface of the\nweb browser.  Use of appropriate communication protocols can be included (e.g., websockets).  The particular link can be a link determined based on the suggestion event.  In another example, the opened application can be a shopping application that\noutputs data received from a server indicating an item or service to purchase and providing one or more controls receptive to user input to purchase the item or service, where the item or service is related to the suggestion event.\n In some examples, the command can instruct an application to open or initiate on the first user device to display information related to one or more content features included in a content item that is displayed in the embedded interface on the\nfirst user device.  For example, a suggested command to open or execute an application can be generated based on a suggestion event that is detected to have occurred by the messaging application (or embedded application) based on the output data of the\nembedded session and/or based on one or more chat messages being input by one or more chat users of the first chat conversation, similarly as described above.  In some examples, if user consent has been obtained, content features including visual\nfeatures (e.g., faces, animals, landscape features, objects, etc. using image recognition techniques), audio features (speech, types of sounds, etc. using audio recognition techniques), or topics (e.g., a subject of a presentation, geographical location\nof an image or a scene in a video, etc.) can be detected by a system in output data of the embedded application and similar content features can be searched for on accessible sites and databases over the network.  For example, if a chat message from a\nmember user includes \"I like that coat she's wearing\" and the embedded application is displaying output data of a video or image depicting a person with a coat, this combination of the chat message related to the visual feature displayed in the embedded\napplication output can be a suggestion event.  This causes the messaging application or embedded application to search for similar coat items in accessible web pages or shopping sites over the network, e.g., by using an image search or text labels\ndetermined from one or more image recognition techniques.  One or more network sites having matching items can be included in one or more suggested commands.  For example, a suggested command can be generated for one or more member users, where the\nsuggested command (if selected) instructs the first user device to open a browser application that displays one of the matched coat items.  In some examples, the suggested command can be displayed as a text description of the action(s) it commands, e.g.,\n\"Display a website showing the coat.\"\n In other examples, a suggestion event can be detected when the embedded application displays a person giving a lecture, where an identity of the person can be determined (e.g., in metadata of the output data, by detecting a subtitle for the\nspeaker, etc.), and items related to the speaker are searched for and found in content items accessible over the network.  In one example, a book written by the speaker is found on a website.  A suggested command can be generated for one or more member\nusers, where the suggested command (if selected) instructs the first user device to open a shopping application that displays a description of the book with a selectable option (e.g., displayed element) to purchase the book by the user.\n In additional examples, suggested commands can be determined based on one or more images, videos, audio data segments, or other types of content data.  For example, a number of images (e.g., representations of media items, items to purchase,\netc.) can be displayed in an embedded application, and a suggested command can be to select one or more of the images, where the suggested command is determined based on previous images selected by the user.  In another example, a suggested command can\nbe to select a particular color of a clothing item from a list of multiple colors of that clothing item, where the suggestion is based on previous user selections (e.g., where the user previously selected a particular color more frequently than other\ncolors).\n In additional examples, a suggestion event can be a user joining a chat conversation, triggering generation of a suggested command in the joining user's chat interface to launch a \"discovery embedded application\" that (with consent of the chat\nusers) stores and displays, in the chat interface or embedded interface, a history of actions taken and/or content output in the embedded session by the chat users prior to when the joining user joined the chat conversation.  For example, the history can\ninclude identifications of other embedded applications that have previously been used in the chat conversation (including which embedded sessions are currently ongoing, ongoing shared lists, etc.).  In some implementations, the embedded application can\ndisplay a launch/join control for each of these other embedded applications that initiates the associated other embedded application, allowing the joining user to re-initiate a previous embedded session, join/continue a previous embedded session if it is\nongoing, etc. This can allow a new user to discover and catch up on the activities that the group of users in the chat conversation has been doing prior to the new user joining the chat conversation.\n In additional examples, a suggestion event can be an external application (e.g., executing on the first user device or a remote device) sending data to the messaging application while a chat conversation is active (e.g., the data can be an\nindication of an occurrence of an event in a communication application, calendar application, etc.), and a suggested item can be displayed in the chat interface providing a suggested command or suggested message (see below) based on the data (e.g., a\nsuggested command to save the data indicating the event in storage for later review by the user, and/or a suggested message describing the event in text form, which can be selected to be output in the chat conversation).  A suggestion event can be a user\nchanging the group information for the chat conversation (e.g., a visual background displayed in the chat interface (wallpaper), chat conversation name or group name, group icon, etc.), and this can trigger a suggestion command to initiate an image\nediting embedded application that provides controls to the chat users that can create a new wallpaper, e.g., by editing an image in a shared editing interface with the other chat members.\n In some implementations, recommendations can be generated in block 714 instead of or in addition to the suggested commands.  For example, such a recommendation can describe a particular action that is suggested for the user, but does not\ninitiate a command to cause that action to be performed if the recommendation is selected by the user.  Thus, the user would perform the recommended action manually if the user chose to follow the recommendation.\n In some implementations, as in the example of FIG. 7, suggested commands can be determined by the embedded application and displayed by the embedded application, e.g., in the embedded interface and/or provided to the messaging application to be\ndisplayed in the chat interface.  In some implementations, other components of the system can determine suggested commands.  For example, the messaging application can determine suggested commands based on, e.g., events in the chat interface and/or based\non event information from the embedded application.  In some implementations, a server (e.g., messaging server, suggestion server, etc.) can be provided event information and can determine suggested commands.  For example, the messaging application can\nsend descriptive information indicating the suggestion event to a server, and the server can determine one or more suggested commands that are provided back to the messaging application.\n In some implementations, e.g., when the embedded application does not include suggestion features, the messaging application may provide the suggested response items.  For example, if the embedded application is an image-viewer application, the\nmessaging application may provide a suggested command to enhance the image, to download the image, to execute an image search for the image, etc. In some implementations, the suggested commands may be provided by both the embedded application and the\nchat application.  The method continues to block 716.\n In block 716, the one or more suggested commands determined in block 714 are provided from the embedded application to the messaging application (if the embedded application generated the suggested commands) on the first user device.  In some\nimplementations, the embedded application can send data to the messaging application from which the messaging application can determine one or more suggested commands, instead of or in addition to sending suggested commands to the messaging application. \nFor example, the embedded application can send data describing the suggestion event (e.g., an indication of a user action in the embedded application, a game score, an indication of success or failure achieving a game objective, etc.), and/or can send\ndata providing a context to the suggestion event (e.g., video data, audio data, states of a game, portions of a shared list, etc.).  In some implementations, the messaging application can determine suggested commands from this data, and/or can send the\ndata to a suggestion engine, e.g., suggestion server 156, to determine and send back suggested commands based on the data to the messaging application.  The method continues to block 718.\n In block 718, the messaging application outputs the suggested commands in the chat interface (and/or the embedded application outputs the suggested commands in the embedded interface) for selection, such that the first device displays the\nsuggested commands.  In some implementations, the suggested commands can be each displayed as a description of the actions that the suggested command instructs (e.g., \"Skip this scene,\" \"Move pawn 3 forward,\" \"add User4 to this movie session\", \"Display\noption to purchase a coat\", etc.).  For example, the suggested commands can be displayed in a list in the chat interface, as selectable messages from a bot, etc. The individual suggested commands can be selectable by the user, e.g., via touchscreen input\nor other user input.  In some implementations, the suggested commands can be displayed as interface elements in the chat interface, e.g., as buttons, in a drop-down menu, etc. Some implementations can display the suggested commands in a persistent area\nof the chat interface so that the commands do not scroll off the display screen when additional chat messages are displayed in the chat interface.  In some implementations, the suggested commands are not displayed to the other chat users of the chat\nconversation and the member users of the embedded session, e.g., the commands are specific to the first user of the first device.  In some implementations, one or more of the suggested commands can be displayed in the chat conversation to all chat\ndevices or member devices, or to a subset of the chat devices or member devices which may have been found relevant to the suggested commands.  In some implementations, if one user selects a suggested command, that command is removed from the suggested\nlist of the chat interfaces of all the chat devices.\n In some implementations, the suggested commands can have a limited display time before automatically being removed from the chat interface by the messaging application.  For example, a predefined time limit can be used, such that the suggested\ncommands are removed after the time limit expires.  In some implementations, the suggested commands can be removed based on one or more conditions being met.  For example, if a suggestion event or application state caused particular suggested command(s)\nto be determined and displayed, and that suggestion event or application state is no longer relevant, pending, or valid, then those particular suggested command(s) can be removed from the chat interface.  In some examples, if suggested commands present\nactions in a game responding to another player's action, and the user performs a different action, then those suggested commands are no longer relevant and can be removed.  In another example, if an event in a played media item is no longer being\ndisplayed, e.g., after a predefined amount of time, then suggested commands reacting to that event can be removed.  In some examples, the embedded application can inform the messaging application of new events and states and/or when suggestion events are\nno longer valid or pending, such that the messaging application can determine when to remove suggested commands from display.  The method continues to block 720, described below.\n If the suggestion event type does not provide suggested commands as determined in block 712, or after block 718, the method continues to block 720, where it is determined whether the detected suggestion event is an event (e.g., has a event type)\nthat is associated with one or more suggested messages for the chat conversation.  For example, some types of suggestion events can provide suggested response items that include suggested messages to be output in the chat conversation.  A suggested\nmessage can be, for example, a text message that is displayed in the chat interface as if the selecting user has input the message as a chat conversation message.  In some implementations, suggestion event types that can provide suggested messages can\ninclude one or more of the types of suggested events that can provide suggested commands as described above.  In some examples, suggestion event types that can provide suggested messages can include the event examples described above for suggested\ncommands.  In some implementations, some types of suggestion events can be designated as providing one or more suggested messages and not providing one or more suggested commands, and vice-versa.\n If the detected suggestion event is of a type that provides one or more suggested messages for the chat conversation, then the method continues to block 722, in which one or more suggested messages are obtained based on the suggestion event\nand/or other conditions.  For example, similarly as described for suggested commands (e.g., block 714), the messaging application can determine one or more suggested messages, and/or can send descriptive data to a suggestion server (or other server)\nwhich determines and sends back one or more suggested messages to the messaging application, etc. In some implementations, the embedded application can determine one or more suggested messages.\n In some implementations, the suggested messages can be determined based on stored, predefined associations of suggested messages with particular suggestion events and/or types of suggestion events, determined using a recommendation engine,\ndetermined using a trained model and machine learning techniques, and/or determined using other techniques and features similarly as described herein for suggested commands, e.g., as described for block 714.\n In some examples, a suggested message can be a description of the suggestion event that caused the suggested message, or a description of an action that caused the suggestion event.  For example, if the first user performs an action in the\nembedded application that is detected as a suggestion event (e.g., a game action in a game, setting a playback position in a media player application, etc.), the embedded application can provide a suggested message (or provide an indication of the event\nto the messaging application that provides a suggested message) that describes the action.  For example, the suggested message can describe the action performed by the user as \"User1 has skipped past the current scene\" or \"User1 has moved a knight.\" If\nthe suggested message is selected by the first user (e.g., in block 730 below), the suggested message is output to the chat conversation, e.g., to share the user action with the embedded session and/or the chat conversation.\n In some implementations, if the suggestion event is a user action that instructs the system to generate one or more message suggestions about the embedded application and/or embedded session, then various suggested messages can be generated\nwhich describe the embedded application and/or the embedded session, e.g., outside the context of a particular user action or particular type of suggestion event.  For example, suggested messages can include general comments about the embedded\napplication, e.g., \"This game is great!\" or \"Check this movie out!\" etc. Such general messages can also be generated in response to other types of suggestion events.\n In some examples, suggested messages can indicate or describe user reactions to a suggestion event.  In some implementations, such a suggested message, when selected by a user, can be displayed as a chat message in the chat conversation.  Such\nsuggested messages can be text messages, for example.  In some examples, suggested messages that describe user reactions can be determined as stored predefined messages that are associated with particular topics, words, or phrases occurring as a basis of\nthe suggestion event, or are associated with particular actions used to detect the suggestion event.  In some implementations, a machine learning model can be used that is trained based on synthetic or actual training data, e.g., predetermined synthetic\nmessages or previous actual messages provided by users in reaction to predetermined or previous suggestion events that are the same as the detected suggestion event (e.g., the same types of user actions, chat input, events, etc. that were detected as\nprevious suggestion events).\n In some example implementations in which the embedded application is playing a media item, suggested messages can be determined based on the suggestion event.  For example, if the suggestion event is a chat user named User2 joining the embedded\nsession to view the media item, then a suggested message can be \"welcome User2!\".  In another example, if the suggestion event is a particular detected type of visual or audio feature or type of scene or sequence (e.g., a detected action scene or\nsequence) that has been detected in the played media item, suggested messages can include exclamations or observations relevant to the detected type of feature or scene, e.g., \"Wow that was great!\" or \"Cute baby.\" For example, such message suggestions\ncan be determined based on predefined associations between messages and content features and scenes, and/or with a machine learning model that has been trained using synthetic or actual data including prior content features and prior user comments or\nresponses in reaction to those features.  In another example, if the suggestion event is a change in the playback position or output characteristics (e.g., audio volume or visual brightness or contrast) of the media item caused by another member user, a\nsuggested message can be a reaction to the change, such as \"good, I was tired of that\" or \"wait, I wanted to watch that!\" In another example, a server event may have been received by the messaging application from a server as described above.  In one\nexample, the played media item in the embedded session is a video created by a first member user, User1, the media item has been available for viewing by users from the server, and the server event indicates that a threshold number of approvals by users\nhave been received by the server for the media item.  A suggested message for the member users other than the first member user can be \"congratulations, User1\" or similar praise for the first member user.\n In some example implementations in which the embedded application provides a game, the suggested messages can be determined based on the suggestion event related to a game event.  For example, if the suggestion event is a game action by a\ndifferent player of the game, a suggested message can be one or more possible or predefined reaction messages to the game action, e.g., \"great move!\" or \"great answer!\" (e.g., to a selection in a quiz game providing a choice of multiple answers).  In\nanother example, if the suggestion event is an invasion of a user's game territory by enemy game pieces, the suggested messages can include reactions such as \"Hey, don't come after me\" or \"You'll regret that move!\" In another example, if the suggestion\nevent is the user's score becoming different from (e.g., trailing or leading) other players' scores or objectives by a threshold amount or more, suggested messages can include boasts or laments by the player, e.g., \"You guys will never catch me.\" In\nanother example, if the suggestion event is another player joining the game, suggested messages can include a message request to the new player to join the user's team in the game, and/or sending a message request to a different player to form a team\nagainst the new player.  In another example, if the suggestion event is a member user exiting the game (or has changed from a player role to an observer role in the game), a suggested message can be \"Thanks for playing.\" Such suggested messages can be\ndetermined similarly as described above.\n In some example implementations in which the embedded application is providing a shared content document (e.g., shared list) of items, the suggested commands can be determined based on the suggestion event related to the list or document.  For\nexample, if the suggestion event is the addition of an item (e.g., \"steak\") to a food list by a different member user, the suggested messages can include messages that inquire about the item or request to remove the item from the list, e.g., \"do we need\nthe steak?\" or \"I suggest removing the steak.\" In some implementations, the suggested message can be associated with a highlighting command to highlight the item as described above.  In another example, if the suggestion event is removal of an item, a\nsuggested message can include outputting text in chat interface, e.g., \"why did you remove that item?\" Such suggested messages can be determined similarly as described above.\n In some example implementations in which the embedded application provides travel itineraries, if the suggestion event is the output of multiple travel itineraries from which to select, suggested messages can include descriptions of the\nitineraries, requests to other chat users for comments or suggestions as to which itinerary to select, etc.\n In another example that can be applied to many types of embedded applications, if the suggestion event is a member user joining the embedded session, a suggested message can be to ask the new member user whether he or she wants control over\nfunctions of the embedded application (if the first user has been assigned a user status or role in the embedded application that allows such assigning of control).  In some implementations, a suggested message can be to ask a member user to stop using\nabusive or inappropriate language if the messaging application (or other program or device) has detected such language in messages from the designated user in the chat conversation.\n In additional examples, a suggestion event can be the user device receiving a phone call while the user is participating in the chat conversation, and a suggested message can be \"Hang on, let me take this call from my boss\" for display in the\nchat conversation.  A suggestion event can be the user device receiving a Short Message Service (SMS) message while the user is participating in the chat conversation, and a suggested message can include the content of the SMS message for display in the\nchat conversation.  A suggestion event can be the user device going offline, and a suggested message can include \"sorry guys, my phone lost connectivity for a minute.\" A suggestion event can be the user device alternating between online/offline status,\ne.g., for a threshold number of times within a threshold time period, during display of a video in an embedded session, and a suggested message can include, \"sorry guys my connection is very choppy, I'll watch that video later.\" A suggestion event can be\na chat user exiting a chat conversation (e.g., detected as a server event), and this can trigger generation of a suggested messages to all the other (remaining) chat users, where the suggested messages relate to that user quitting the chat conversation\n(e.g., \"shall we stop the movie?\", etc.).\n Suggested messages can, if user consent has been obtained, also or alternatively be based on a history of prior messages and/or actions performed by the user in embedded sessions involving that same or similar types of embedded applications\nand/or for same or similar types of content items (e.g., media items, games, etc.).  For example, if the user has previously reacted to a change in playback position of a particular scene or type of scene in a video by sending a message of \"Please change\nit back,\" then a similar suggested message can be generated in response to the occurrence of a similar suggestion event, where the suggested message provides a similar reaction.\n Suggested messages can also or alternatively be based on member user input in the chat conversation, as displayed in the chat interface of the messaging application.  For example, if during a game, a member user inputs a message in the chat\nconversation that says, \"help me get this objective,\" suggested messages can be generated that are \"sure, will do\" or \"I can't help you right now.\" In another example, if during playback of a media item in the embedded interface, member user inputs,\n\"what time should I start at?\" then suggested messages can be generated that indicate various times, such as different chapter headings (e.g., \"go to 28:30--The Visit.\") or a suggested message can indicate the time or position of a known favorite scene\nof the user, if user priority history can be accessed with user consent.\n Some suggested messages can be associated with one or more suggested commands that have been determined in block 714.  If a suggested command is selected by the user, a suggested message associated with that command can be output in the chat\ninterface while the command is provided to the embedded application.  In some implementations, such a suggested message can relate to the associated suggested command, e.g., emphasize or explain the command provided.  For example, if a suggested command\nis selected to take an action responding to a different player's previous action in a game, then one or more associated suggested messages can be determined that can accompany that suggested command to emphasize it, such as \"Take that!\" and \"I'm not\ndefeated yet.\" In another example, if a suggested command is selected to change a playback position for a media item to a new scene, an associated suggested message can explain that change in playback (e.g., \"I'm skipping past that scene\" or \"I got bored\nwith that scene\").  Some implementations can similarly display one or more suggested commands in response to a selection of a suggested message.  In another example, if there is a discussion or argument between chat users in the chat conversation, one or\nmore of the chat users can be presented with a suggested command to an embedded application to generate a visualization of the discussion, e.g., using other types of content data.  For example, the embedded application can generate a cinemagraph or other\nanimated data with a visual representation of the discussion using user profile pictures.  Editing options can be presented (e.g., to the user that selected the suggested command) that allow a user to edit the animated data.  The embedded application can\ngenerate a suggested message that, if selected by the user, causes the animated data to be displayed in the chat conversation as a chat message.\n Some suggestion event types can cause generation of both suggested commands from blocks 714-718 and suggested messages from blocks 722-724.  In some implementations, suggested commands for the embedded application can be visually distinguished\nfrom suggested messages when displayed in the chat interface.  In some examples, suggested commands can be displayed with a common first visual feature, and suggested messages can be displayed with a common second visual feature that is different than\nthe first visual feature.  For example, a particular color, typeface, highlighting (e.g., boldface text) or other visual feature can be applied to suggested commands, and a different color, typeface, highlighting, or other visual feature can be applied\nto suggested messages.\n In some implementations, a plurality of suggested messages can be generated and can be ranked based on one or more predetermined criteria, similarly to suggested commands as described above.  A particular number of top-ranked suggested messages\ncan be selected for output on a user device.  The method continues to block 724.\n In block 724, the one or more suggested messages obtained in block 722 are output by the messaging application in the chat interface and/or embedded interface.  For example, the suggested messages can be displayed in a list in the chat interface\nand can be selectable by the user, similarly as described above for suggested commands.  Some implementations can display the suggested messages in a persistent area of the chat interface so that the suggested messages do not scroll off the display when\nadditional chat messages are displayed in the chat interface.\n In some implementations, the suggested messages can have a limited display time before automatically being removed from the chat interface by the messaging application, similarly as described above for the suggested commands.  For example, the\nsuggested messages can be removed after a time limit expires and/or based on one or more conditions being met.  For example, if a suggestion event or application state caused particular suggested messages(s) to be determined and displayed, and that\nsuggestion event or application state is no longer relevant, pending, or valid, then those particular suggested messages(s) can be removed from the chat interface.  In some examples, if suggested messages are in response to a particular action performed\nby a different user in the embedded application, and that different user performs a different action, then those suggested messages may no longer be relevant and can be removed.  In some examples, the embedded application can inform the messaging\napplication of new events and states and/or when suggestion events are no longer valid or pending, such that the messaging application can determine when to remove suggested messages from display.  The method continues to block 726, described below.\n If the suggestion event type does not provide suggested messages as determined in block 720, or after block 724, the method continues to block 726, where it is determined whether a selection of one or more suggested command items is received in\nthe chat interface.  For example, the selection can be received based on user input provided by the user in the chat interface to select displayed suggested command item(s).  In some implementations, the embedded interface can display suggested items in\nthe embedded interface where they are responsive to user input for selection, as described above for block 718.\n In some implementations, multiple suggested command items can be selected by user input.  In some examples, as described above, a selection can be received for a suggested command item, which causes a list of suggested message items to be\ndisplayed that are associated with the suggested command item.  The user can then select one (or more) of the associated suggested message items.  In some implementations, a selection can be received for a suggested message item, which causes a list of\nsuggested command items to be displayed that are associated with the suggested message item.  The user can then select one (or more) of the associated suggested command items.\n If a selection of one or more suggested command items is received as determined in block 726, then the method continues to block 728, where the messaging application provides one or more selected command(s) to the embedded application which are\nassociated with the selected command item(s).  In some cases, one or more selected commands associated with the selected command item are provided to the messaging application and/or the first user device as described above, instead of or in addition to\ncommands provided to the embedded application.  In some implementations, multiple commands are associated with a single selected suggested command item.  The embedded application, message application, and/or first user device implements the provided\ncommand(s).  For example, the command may cause a change in state of the embedded application (e.g., change in playback position of a media item, change in position or status of a user-controlled game piece, change in a shared document, etc.).  In some\nimplementations, as described herein, the change in state is sent or indicated to one or more servers and/or to the other member devices over the network to allow synchronization of embedded application states in the embedded session.  The method\ncontinues to block 730.\n If a selection of one or more suggested command items is not received as determined in block 726, or after block 728, the method continues to block 730, where it is determined whether a selection of one or more suggested message items is\nreceived in the chat interface.  For example, the selection can be received based on user input provided by the user in the chat interface to select a displayed suggested message item.  In some implementations, multiple suggested message items can be\nselected by user input.  Some implementations can select one or more suggested message items in association with one or more suggested command items as described above.\n If a selection of one or more suggested message items is received as determined in block 730, then the method continues to block 732, where the messaging application outputs selected message(s) corresponding to the selected message item to the\nchat conversation in the chat interface.  In some implementations, multiple messages can correspond to a single message item.  In some examples, the selected message(s) can be displayed in the chat interface of the (first) device receiving the user\nselection of the message item, and the selected message(s) can be transmitted over the network to the chat devices participating in the chat conversation and displayed in the chat interfaces of those devices.  In some implementations, the selected\nmessage(s) can be transmitted to a particular subset of the chat devices for display in their associated chat interfaces, e.g., to member devices in the same embedded session as the first device, to devices of particular chat users based on user\nselection or preferences, to devices of particular user statuses or roles in the embedded session, to devices on the same team or opposing team in a game of an embedded session, etc.\n In some implementations, the number of suggested messages output in the chat conversation can be limited, e.g., so as to reduce the effect of many chat messages crowding the chat interface.  For example, the messaging application can impose a\nmaximum number of selected suggested messages that can be output to the chat conversation within a particular period of time, or can impose a minimum amount of time between the output of successive selected suggested messages from a particular member\nuser or from all chat users.  Thus, if the selected suggested message does not qualify for output, then the message is not output to the chat conversation in block 732.\n If a selection of one or more suggested message items is not received as determined in block 730, or after block 732, the method can continue to block 708 to continue execution of the embedded application.\n In some implementations, a particular embedded application can be opened (e.g., minimized in a corner) to continually parse and/or otherwise process chat messages in the chat conversation and provide suggested commands and/or messages based on\nthe chat messages.  For example, the embedded application can present suggested messages that are topics, videos, images, cinemagraphs, audio data segments, etc., based on the content of the chat conversation.  This can assist a user to post content\nrelevant to the chat conversation.  In some of these implementations, suggestion events can be continually being detected based on current chat messages being input by chat users in the chat conversation, and new suggested commands and/or messages can be\ncontinually being provided for user selection in the chat interface and/or embedded interface.\n In some implementations, suggested responses can be determined and presented to multiple member users (and/or chat users) based on the suggestion event, e.g., displayed on each member device or chat device (or selected member devices or chat\ndevices).  In some examples, the suggested responses can be the same suggested commands and/or suggested messages provided on each of the multiple member devices, or different suggested responses can be provided to each of two or more of the member\ndevices.  In some examples, member devices of Users 1, 2 and 3 receive a chat message from User 4 of the chat conversation, and each member device (or multiple member devices) of Users 1, 2, and 3 may present a different set of suggested responses (e.g.,\neach set having at least one response different from one or more of the other sets).  Each set of suggested responses can be based on the context of the member device presenting the set, e.g., based on the associated chat messages input by the associated\nuser in the chat conversation, a history of chat messages of the associated user (if user consent has been obtained), and/or other factors as described herein.  In some examples, the set of suggested responses can be based on one or more particular\nembedded applications the associated user has been using.  For example, if User 4 inputs a chat message, \"We need to buy drinks for the party tonight,\" and if User 2 is a frequent user of a shared list embedded application, the user device of User 2 can\nprovide a suggested command that (when selected) adds an item `Buy Drinks` to a shopping list in that application.  Selecting that suggested command can cause the shared list embedded application to open and can cause the item \"Buy Drinks\" to be\nautomatically to the shared list of the shared list embedded application.  User 3, however, is not a frequent user of the shared list embedded application (e.g., has not used the application more than a threshold number of times within a particular\namount of time), and the user device of User 3 does not display that suggested command in this example.  The user device (e.g., messaging application) of User 3 may display a different suggested command for a different embedded application that is\nfrequently used by User 3, e.g., a map application that displays a geographical location of businesses at which to purchase drinks.  In another example, User 3's device can provide a suggested message that is the same as a previous message previously\ninput or selected by User 3 in response to a similar chat message displayed in the chat conversation (or, alternatively, in any chat conversation) on a previous occasion (e.g., the previous chat message having one or more of same words, the same semantic\nmeaning, etc.).\n In some implementations, relevant suggested responses can be determined that are related to one or more particular content items provided in the embedded interface.  For example, if member users are using (or have previously used) a shared list\nembedded application to generate a first list for travel preparation and a second list for party organizing, suggested responses can refer to one of these lists and not the other list, e.g., based on content of chat messages and/or one or more other\nsuggestion events.  In some examples, in response to a chat message such as \"don't forget to buy drinks for tonight,\" a machine learning model and/or knowledge graph can determine that \"tonight\" and \"buy drinks\" refers to a \"party\" that the chat members\nare organizing, and provide a suggested response that (when selected) adds an item \"buy drinks\" to the corresponding second list for party organizing.  In response to a chat message such as \"have you booked the plane tickets for our trip,\" the machine\nlearning and knowledge graph can determine that \"plane tickets\" and \"trips\" refer to travel, and provide a suggested response that (when selected) adds an item \"book plane tickets\" to the corresponding first list for travel preparation.\n In some implementations, a suggested response, when selected, can trigger a display of identifications of multiple associated embedded applications, e.g., to allow a user to select a desired embedded application to be executed or to receive data\nbased on the suggested response, thus reducing ambiguity.  In some implementations, a suggested response, when selected by a user, can cause display of a request as to which embedded application should be triggered by the selection of the suggested\nresponse.  In some cases, multiple possible embedded applications that are suitable for a selected suggested response can be displayed, and user input can indicate the embedded application to execute.  For example, if a selected suggested response adds\nan item to a shared list, and two different available embedded applications provide such lists, identifications of these two embedded applications can be displayed and the user can select one of the embedded applications to use.\n In some implementations, suggested responses can be displayed within the embedded interface and receive selection from user input while displayed in the embedded interface (e.g., via touch input on a touchscreen, or via a user-controlled cursor\npositioned in the embedded interface).  For example, suggested responses can be displayed in response to occurrence of suggestion events in the embedded application.  In some examples, a suggestion event in a game can be a user player causing an opponent\nplayer's avatar or game piece to be defeated or removed, and a first set of suggested messages can be displayed on the winning player's member device (e.g., messages bragging about the move), and a second set of suggested messages can be displayed on the\nlosing player's member device (e.g., messages whining about the loss).  When selected by user input, such suggested messages cause chat messages indicating the desired content to be provided in the chat conversation.  In another example, the suggestion\nevent is a notification in the embedded application and embedded interface of a first user's device that a second user has joined the game, and a suggested response, when selected, causes a taunt or challenge to be output to the second player's user\ndevice (\"Let's go, User 2,\" \"I'm gonna beat you at this game\", etc.) and/or to all member devices (or, alternatively, to all chat devices).\n In some implementations, embedded applications (e.g., scripts) can be attached to (associated with) a suggested response by, e.g., being encoded directly in the data of a suggested response or in a user-selectable link (or other displayed\nselectable control) included in a suggested response.  For example, small embedded applications of few hundred kilobytes can be so attached.  When the suggested response is selected by the user, the attached embedded application is triggered to open\n(after decoding), e.g., instead of downloading the embedded application from a source (e.g., embedded application server 150) over the network.  In some cases, this feature can eliminate or reduce additional download time of embedded application data,\nand can provide an immediate execution and display of an embedded interface.  In some implementations, the attached embedded application can be a lightweight version and is executed in addition to triggering downloading a more full version of the\nembedded application in the background of the device that can replace the lightweight version on the device after the downloading has occurred.\n In some implementations, an attached embedded application can execute in the background of an operating system (or other software environment) of the device (e.g., in a \"headless mode\"), which does not cause an embedded interface or other output\nto be displayed by the embedded application.  For example, the attached embedded application can generate suggested responses displayed in the chat interface.  Attached embedded applications can provide instant extensions and capabilities to the\nmessaging application that can be created on-the-fly during a chat conversation, e.g., based on messages of a chat conversation and/or associated suggested responses selected by a user.  The messaging application can use APIs and capabilities of attached\nembedded applications without needing to download and execute a corresponding version of the embedded application that, e.g., displays its own associated embedded interface.\n In some implementations, an attached embedded application can be associated with or attached to a chat conversation for which it is executed, e.g., in an ongoing association with the chat conversation.  For example, the attached embedded\napplication can act as an extension to the chat conversation, e.g., where the embedded application executes in the background while the chat conversation is open and/or displayed on the device.  Such an attached embedded application can process chat\nmessages that are input to the chat conversation and perform actions in response to chat messages, e.g., provide auto-replies to certain types of chat messages, auto-translate chat messages into a different language, provide always-on location sharing\n(indicating a current geographic location of the user device) while the messaging application is open (with user consent), add displayed graphical interface elements to the chat interface, etc. In some examples, the chat messages of the chat conversation\ncan be auto-translated into and displayed as messages in a different language, e.g., by having an attached embedded translation application as an extension of the messaging application.  Extension embedded applications can augment a camera application or\nimage picker application, e.g., to modify media items such as images before such media items are shared with other devices in a chat conversation or via a different embedded application.  In further examples, an extension embedded application can\nanonymize a chat conversation or particular users in the chat conversation (e.g., change the displayed user identifiers of the chat users), reorder the display of chat messages in the conversation (e.g., group the messages by theme, by threads including\nsuccessive replies to an original chat message, etc.), enable functions in the chat interface accessible to user input (e.g., displayed controls to pin chat messages or media items posted in the chat conversation to be persistently displayed in a display\narea of the chat interface, or controls to attach comments to media messages that are stored, e.g., in a shared database).\n In some implementations, an attached embedded application provides a one-time execution for the chat conversation, e.g., to modify and re-share an image input to the chat conversation, or a one-time translation of a chat message.  In some\nimplementations, such embedded application extensions and/or one-time scripts can be downloaded from a server, or can be directly attached to user-selectable chat messages or suggested responses (links) (e.g., as encoded scripts provided as a parameter\nof a web address (e.g., Uniform Resource Locator (URL)).\n In a similar manner, attached embedded applications (e.g., small-sized applications that use a few hundred KB of memory) can be encoded in the data of a selectable link in a chat message or in the data for an interactive card that is displayed\nin the chat interface or chat conversation, and can be executed upon selection of that chat message link or control in the interactive card.\n In some implementations, suggested responses can be generated based on suggestion events occurring at external devices or programs outside the messaging application and the chat conversation, e.g., as described above, and the suggested responses\ncan provide commands and/or messages output to the external devices or programs.  For example, an embedded application can communicate with an online game (e.g., a game executing on a server), and commands input to the embedded application can affect the\nstate of the game, e.g., be conveyed to the game server.  If an event occurs in the game which can be responded to by the first user providing input to the embedded application, a suggested response can include an action providing input to the embedded\napplication that will cause a responsive user action in the online game.\n In some implementations, suggested responses can be generated based on suggestion events and/or particular events that occurred in the past, if consent of the applicable users has been obtained to do so.  The previous events can be indicated or\ndescribed in stored historical data, for example.  In some examples, a log of previous chat messages input in a previous chat conversation between two users can be used to determine messages to be included as newly-generated suggested responses for a\ncurrent chat conversation that includes the same two users.  A suggested response can be based on a prior history describing a particular user's performance in an embedded game application, such as previous objectives or scores attained by the user in\nthe game.  For example, a suggested message of \"You're doing better than before!\" can be displayed for User 1 based on User 2's current score in the game being higher as compared to User 2's previous score in a previous game in that embedded game\napplication.  Recorded previous interactions between users in embedded applications can also be used to generate suggested responses, if user consent has been obtained.  For example, previous user interactions in a game application can be used to\ngenerate a new suggested game action that was successful in a previous game, and/or generate a suggested message (e.g., \"I'm doing better than you this time\"), or previous user contributions to a shared document application can be used to generate a\nsuggested command to add the same contributions to a new shared document, etc. In another example, if it is determined that an addition of a list item was previously made by a user to a first shared list in an embedded list application, as described in\nstored historical data, a suggested command can be generated for that user with respect to a newly-created, second shared list that, e.g., has the same or similar label or title as the first list.  For example, the suggested command, when selected, adds\nthe same list item to the second list, where the second list has been determined to have the same label (e.g., \"Shopping List\") as the first list.\n In some implementations, the number of displayed suggested responses can be limited, e.g., by consolidating multiple suggested responses into a single suggested response that is displayed.  For example, a single suggested response can, when\nselected, cause multiple actions or inputs to one or more applications (embedded applications, messaging application, server, etc.).  In some examples, a suggestion event is the input of a chat message \"Don't forget Timmy's birthday on Friday\" by a\ndifferent chat user in the chat conversation.  Determined suggested responses to this chat message can include a suggested message \"don't worry\" (chat message), a suggested command to add `Timmy's Birthday` to the first user's calendar, a suggested\ncommand to set an alarm or reminder on the first device to activate on Timmy's birthday, and a suggested command to add an item `Buy a present for Timmy's birthday` to a shopping list in a list embedded application.  These three suggested commands\ntrigger actions in embedded applications and/or bots.  However, the three suggested commands may occupy a large amount of screen real estate and so can be combined into a displayed consolidated suggested command, e.g., \"Set reminders for Timmy's\nbirthday.\" When selected by user input, this consolidated suggested command causes a selection menu to be displayed that includes the three individual suggested commands.  Any or all of these individual suggested commands can be selected by user input.\n In some implementations, suggested response items determined for a first chat conversation can, when selected by user input, provide commands or messages to a different, second chat conversation, e.g., implemented by the messaging application\n(or a different application).  Users may be participating in multiple chat conversations in parallel, and selected suggested responses in one chat conversation can trigger actions in a different chat conversation.  For example, in a first chat\nconversation between User 1 and User 2, User 1 inputs the chat message, \"User 2, don't forget Timmy's present for his birthday.\" This message may be a suggestion event that causes a suggested response (command) to be generated on the device of User 2. \nWhen this suggested response is selected by user input, an item \"Buy present for Timmy\" is added to a birthday list that has been created in an embedded application in a second chat conversation in which User 2 is participating.  In another example, User\n1 and User 2 are inputting chat messages in a first chat conversation about a game that User 1 is playing in a second group conversation, and a suggested response is generated that, when selected by User 1, causes a particular chat message (or other\ndisplayed prompt) to be output in the first chat conversation.  For example, the particular chat message can invite User 2 to join the game in the first chat conversation, or cause display of an interactive card on User 2's device that includes a\ndisplayed join control.\n In some implementations, suggested responses may be customized based on embedded applications that have been executed at least once by the first device.  In some implementations, suggested responses may be customized for a chat conversation\nbased on embedded applications that have been executed at least once by the first device in that same chat conversation.  In some implementations, suggested responses may be customized based on embedded applications that have been executed at least once\nby a device of one or more member users, or in some implementations by the device of one or more chat users.  For example, a command or message to an embedded application can be provided as a suggested response on a device (in response to a suggestion\nevent) if that embedded application has been executed previously by that device.  In some examples, if the first device has previously executed a shared list embedded application, then when a chat user inputs chat messages in the chat conversation that\ninclude particular items (e.g., words, images, or other content data) related to event organizing or recommendations, a suggested response can be generated on the first device that is a command to add those particular items to an existing or new list in\nthe shared list embedded application.  In some examples, member devices that did not previously execute the shared list embedded application would not display the suggested response related to the shared list embedded application.  In some\nimplementations, if any member device (or chat device) has previously executed the embedded application, the suggested command or message can be provided on one, some, or all member devices (or chat devices).\n In some implementations, one or more suggested response items can be generated based on invoking or requesting information from one or more bots accessible to the device providing the suggested response items.  For example, if the suggested item\ngeneration component (e.g., suggestion server or other component of the system) determines that an appropriate suggested item would be particular information relevant to the content of a played media item, a bot that can obtain that information can be\ndetermined and a command and/or request to the bot can be generated as a suggested item.\n In some implementations, if a selected suggested response item is a command or request to a bot, the bot can be added to the chat conversation and obtains and displays requested information in the chat interface, and the users can send further\nmessages to the bot.\n In various example implementations that use features described above, a chat interface can be displayed by a first user device as generated by a messaging application as described above.  The chat interface is configured to display one or more\nmessages provided in a chat conversation configured to display messages provided by user devices participating in the chat conversation over a network.  An embedded interface is provided by a first embedded application and displayed in the chat\ninterface.  A suggestion event is determined to have occurred in association with the first embedded application (or the messaging application), e.g., based on received data that indicates that a particular event has occurred at one or more of the\nplurality of user devices that are participating in the chat conversation, wherein the one or more of the plurality of user devices are different than the first user device.  The first user device (e.g., the messaging application) obtains one or more\nsuggested response items based on the suggestion event, and the suggested response items are displayed by the first user device.\n For example, the particular event can include user input received by an associated chat interface of the one or more of the plurality of user devices, user input received by an associated embedded interface of the one or more of the plurality of\nuser devices, an embedded application event occurring in an associated embedded application executing on the one or more of the plurality of user devices, etc. The received data can indicate a type of content feature displayed in a media item being\nplayed by the first embedded application, achievement of a predefined objective in a game of the first embedded application, user input received by a receiving user device participating in the chat conversation, initiation of the embedded application\nfrom a particular user device of the plurality of user devices, etc. The received data can include chat messages.  The suggestion event can be based on, e.g., user input received in the chat interface and/or in the embedded interface, and/or based on\nevent information from the first embedded application indicating that a particular event has occurred in the first embedded application (and/or occurred in a corresponding embedded application executing on a different chat device).  The one or more\nsuggested response items can be determined by the embedded application, the messaging application, and/or a remote server in communication with the messaging application over the network.  In some examples, the embedded interface can be output on a\nsubset of the user devices participating in the chat conversation, where the user devices received a user selection to join an embedded session involving the first embedded application.  The first embedded application can display embedded output data\nthat is generated by the first embedded application and/or received from a server over the network.\n Features for such implementations can include receiving user input selecting a selected suggested response item from the suggested response items, outputting a chat message associated with the selected suggested response item in the chat\nconversation of the chat interface of the first user device, e.g., such that the chat message is displayed by one or more other user devices of the user devices participating in the chat conversation.  For example, the suggested response items can\ninclude one or more chat messages that indicate user reactions to the suggestion event, and/or can include one or more commands provided to the embedded application and/or to the messaging application.  In some examples, a command to the embedded\napplication can include a command to perform an action in a game implemented by the embedded application, a command to change a playback position in a playing of media data by the embedded application, and/or a command to change a shared object provided\nby the embedded application.  In some examples, the suggested response items providing suggested commands are visually distinguished from the suggested response items providing suggested messages when displayed in the chat interface.\n In further examples, the first embedded application can be a media player application causing display of video data (or other content data) in the embedded interface that is displayed by a subset of the user devices participating in the chat\nconversation, where the embedded interface is configured to receive user input controlling the playback of the video data, and the suggested response items can include one or more suggested playback commands operative to adjust the playback of the video\ndata.  In another example, the first embedded application can be a game application causing display of game data in the embedded interface that is displayed by a subset of the user devices participating in the chat conversation, where the embedded\ninterface is configured to receive user input that changes one or more states of the game application, and the suggested response items can include one or more suggested commands operative to modify at least one state of the game application.  In another\nexample, the first embedded application can be a shared document application causing display of a shared content document (e.g., list of items) in the embedded interface that is displayed by a subset of the user devices participating in the chat\nconversation, where the embedded interface is configured to receive user input that changes one or more items in the shared document, and the suggested response items include one or more suggested commands operative to modify the shared document.\n In additional examples, a server device can receive join indications (or requests) over the network from chat devices that have initiated an embedded application associated with an existing embedded session.  The join indications are sent by the\nserver device to other chat devices (member devices) over the network that are in the embedded session to synchronize the joining devices with the member devices of the embedded session.\n The methods, blocks, and operations described herein can be performed in a different order than shown or described, and/or performed simultaneously (partially or completely) with other blocks or operations, where appropriate.  Some blocks or\noperations can be performed for one portion of data and later performed again, e.g., for another portion of data.  Not all of the described blocks and operations need be performed in various implementations.  In some implementations, blocks and\noperations can be performed multiple times, in a different order, and/or at different times in the methods.\n In some implementations, some or all of the methods can be implemented on a system such as one or more client devices.  In some implementations, one or more methods described herein can be implemented, for example, on a server system, and/or on\nboth a server system and a client system.  In some implementations, different components of one or more servers and/or clients can perform different blocks, operations, or other parts of the methods.\n In various implementations, a messaging application, an embedded application running in association with a messaging application, and/or a server in communication with the messaging application may perform one or more blocks of the described\nmethods.\n FIGS. 8A-8E are graphic representations of example user interfaces displayed by user devices, e.g., client devices.  In these examples, a messaging application is used for a chat conversation between users and an embedded media player\napplication is used in association with the messaging application.\n In FIG. 8A, an example of a first chat interface 800 of a first messaging application is displayed on a first user device operated by a first user (e.g., \"Sarah C.\").  In this example, a chat conversation \"HanginOut\" has been initiated that\nincludes participation of the first user and first device, as well as participation of one or more other users and user devices.  Chat messages from the chat users to the chat conversation can be input at respective user devices, sent to the first user\ndevice if applicable, and displayed in the chat interface by the messaging application.  For example, chat messages from the first user can be input in an input field 802 of the chat interface 800 (e.g., via input devices such as a physical keyboard,\ndisplayed touchscreen keyboard, voice input, etc.).  The chat messages can be displayed in a message display area 804 the chat interface 800.\n In this example, the first user has input a command in the input field 802, where the received command is displayed as message 806 in the chat interface after being input.  This command specifies an embedded interface to be displayed in\nassociation with the chat conversation.  In this example, the embedded interface is provided by an embedded application that is a video player \"vidplay\" executing on the first device.  The command also specifies a media item to play with the video player\napplication, a video segment \"MusicVidA.\" The command also has a \"share\" parameter to cause the playing of the video to be shared in the chat conversation as an embedded session.  In some implementations, the media player application can be selected to\ndisplay the embedded interface without sharing its output in an embedded session associated with the chat conversation.  In some implementations, instead of inputting the command shown, the first user can select to play the media item by browsing a menu\nthat displays media items and selecting the desired media item, or selecting the media item in a different way.  In some implementations, the first user can execute the embedded application without selection of a media item, and then select a media item\nusing a menu of the embedded application, e.g., in the embedded interface.\n In response to the command 806, a response message 808 can be displayed in the chat interface by a bot or the messaging application, e.g., as if the bot or messaging application were a chat user.  Response message 808 indicates that the selected\nembedded video player application is being executed.  In some cases or implementations, the selection of the embedded application (and/or media item) causes the embedded application to be downloaded to the first device in the background, e.g., from an\nembedded application server over the network connected to the first device.\n The embedded application executes on the first device and displays an embedded interface 810.  In this example, embedded interface 810 is displayed within chat interface 800, e.g., within message display area 804 of the chat interface.  In some\nimplementations, the embedded interface 810 is displayed based on data received by the first device over the network, e.g., from an embedded application at least partially executing on a remote session server or other device connected over the network. \nIn this example, the embedded interface 810 is displayed such that message display area 804 is at least partially displayed, e.g., allowing one or more chat messages in the message display area 804 to be simultaneously displayed with the embedded\ninterface 810.\n Embedded interface 810 includes a display area 812 for displaying content data of a media item (e.g., video data, image data, etc.), a seek control 814 allowing the first user to provide user input to change the playback position of the media\nitem along a timeline (e.g., moving the control left or right), and a full screen control 816 allowing user input to enlarge the embedded interface 810 to the entire screen (or other display area) of the first device.  A user identifier 818 can be\ndisplayed to indicate which user of the chat initiated the embedded session associated with the interface 810.  The user identifier 818 is shown as an identifying image, but can also or alternatively be text (e.g., name), etc. A list 820 can show the\nmedia item that is current playing in the display area 812 and other media items that are queued to play after the media item is completed.  A close control 822 allows user input to close the embedded interface 810 and close the embedded application,\ne.g., so that it is no longer executing on the first device.\n In some implementations, one or more embedded application indicators 824 can be displayed in the chat interface to indicate which embedded sessions are current active.  Some implementations can also display an indicator in the chat interface to\nindicate updates to the embedded session (e.g., user input to a corresponding embedded interface) since the last time that the first user was a member user of that embedded session.  For example, such an indicator can be an indicator 826 displayed in\nassociation with (e.g., adjacent to or overlapping) the embedded application indicator 824.\n In FIG. 8B, an example of a second chat interface 840 of a second messaging application is displayed on a second user device operated by a second user (e.g., \"Emily D.\").  In this example, the second user is a participant in the chat\nconversation \"HanginOut\" that includes the first user and first device as described for FIG. 8A.  Chat messages can be input by the second user in the chat interface 840, e.g., via the input field 842 and an input device (e.g., on-screen keyboard, etc.). In this example, the second user has input a message 844 to the chat conversation, which is displayed in a display area 848 of chat interface 840.  In response to message 844, the first user has input a message 846 to the chat conversation, as displayed\nin display area 848 of chat interface 840.\n In addition, an embedded session notification 850 is displayed in chat interface 840.  Notification 850 indicates that an embedded session is active, which in this example utilizes the embedded video player application that was executed by the\nfirst user in FIG. 8A.  Notification 850 can be displayed in chat interface 840 after the second device receives information from a session server or a chat device that the embedded session has been initiated and is active.  Notification 850 is displayed\nin this example below the display area 848 in the chat interface 840, or can be displayed in other areas of the chat interface 840 (or outside the chat interface 840) in other implementations.\n In this example, notification 850 includes a designation 852 (e.g., icon or symbol) of the embedded video player application that was executed by the first user in FIG. 8A to initiate the associated shared embedded session.  Notification 850\nalso includes the name 854 of the media item that is being played in the embedded session.  Notification 850 also includes a list 856 of the chat users who have joined the embedded session, e.g., are currently member users of the embedded session. \nNotification 850 also includes a join control 858 which is receptive to user input to cause the second user and second device to join the embedded session if selected.  Notification 850 also includes a close control 860 which is receptive to user input\nto cause the notification 850 to be removed from the chat interface 840.\n In FIG. 8C, the chat interface 840 of FIG. 8B is shown with an example update after the second user has selected to join the embedded session indicated in the notification 850 of FIG. 8B.  For example, the second user may have selected the join\ncontrol 858 of FIG. 8B.  In FIG. 8C, an embedded interface 870 has been displayed in the chat interface 840.  For example, embedded interface 870 can be provided by a second embedded application that has been downloaded (in some cases) and executed by\nthe second device in response to the second user joining the embedded session.  In other implementations, the embedded interface 870 is displayed based on data received over the network, e.g., from an embedded application at least partially executing on\na session server or other device.  The second embedded application corresponds to the video player application that provides the embedded interface 870 on the first device of FIG. 8A.  In this example, the embedded interface 870 is displayed such that\nmessage display area 848 is at least partially displayed, e.g., allowing one or more chat messages in the message display area 848 to be displayed during display of the embedded interface 870.\n After the second user and second device have joined the embedded session, the playback position of the media item displayed in the embedded interface 870 is synchronized with the current playback position of the embedded session, e.g., as shown\non the embedded interface 810 of the first device of FIG. 8A.  Thus, the second user starts viewing the playing media item at a playback position after the start of the media item, and shares the viewing experience with the other member users of the\nembedded session.  The embedded interface 870 includes features similar to embedded interface 810, e.g., a display area 872, a seek control 874 allowing the second user to provide user input to change the playback position of the media item along a\ntimeline, a full screen control 876, a user identifier 878 that indicates the user who initiated the embedded session associated with interface 870, and a close control 882.  A list 880 shows the media item 884 that is current playing in the display area\n872 as well as an identifier of the user that instructed to play media item 884.  In this example, list 880 also includes a second media item 886 that is queued to play after the media item 884 completes playing, and includes an identifier of the user\nthat instructed to play media item 886.\n In this example, the second user has been permitted control over playback functions in the embedded session, and the second user can manipulate the seek control 874 to change the current playback position of the media item, e.g., to a position\nbefore or after the current position.  Such a change is imposed on all the embedded interfaces displayed by the member devices of the embedded session, such that the media item viewing is synchronized among member devices.  In other cases or\nimplementations, the second user may not have been provided permission or privileges (e.g., from the first user that activated the embedded session) and would not be able to adjust the playback position of the media item.\n In FIG. 8D, the chat interface 840 of FIG. 8C is shown with an example update after the second user has selected to input a new message in the chat conversation of the chat interface.  For example, the second user may have selected input field\n842 of chat interface 840 during playback of the media item in embedded interface 870.  In some implementations, this can cause a displayed keyboard 890 to be displayed in or adjacent to the chat interface 840 (e.g., such that at least a portion of the\nmessage display area 848 is still displayed), which can be used to input a new message such as text 892.  Text 892 can be displayed as chat message 894 when the text 892 is entered (e.g., via a return control in keyboard 890).  Some implementations can\nprovide message suggestions 896 as responses to the last chat message 846, any of which can be selected by user input to provide a chat message having the selected message suggestion 896 in message display area 848 and to the chat conversation.\n In some implementations, the embedded interface 870 can be resized to allow display space for both the keyboard 890 and chat messages of the chat conversation.  In this example, embedded interface 870 has been reduced in size as shown.  Chat\nmessage 846 and new chat message 894 can be displayed alongside the embedded interface 870.\n In FIG. 8E, chat interface 840 is shown similarly as in FIG. 8C, with an example user comment displayed in the embedded interface 870 and associated with the playing media item.  At the current playback position of the media item displayed in\nthe embedded interface 870, a user comment 897 is displayed.  In various implementations, displayed user comment 897 can include a comment as well as an identification of the user that input the comment.\n In some implementations or cases, the information of user comment 897 is input by a member user who is currently viewing the media item in the embedded session.  For example, the user comment 897 can be displayed at a location in the display\narea of the embedded interface that is pointed to by the user, for a predetermined amount of time and then removed from the display.  In some cases, user comment 897 may have been previously input by a chat user while the chat user was viewing the same\nmedia item on a previous occasion while participating in the chat conversation (which, for example, can be an ongoing chat group).  For example, the media item may have been played in a previous embedded session in the chat conversation by a chat user,\nwho input the user comment at a particular playback position and location in the display area.  The user comment was stored in association with the chat conversation, media item, and playback position.  When the media item is displayed in the current\nembedded session, the user comment 897 is displayed when the playback position is at (or near, e.g., within a threshold time of) the same position where the user comment was previously input.\n In some implementations, one or more suggested response items 898 can be displayed in chat interface 840 and/or in embedded interface 870.  In this example, suggested response items 898 are displayed in response to a detected suggestion event\nwhich is the display of the user comment 897 in the embedded interface.  In this example, a suggested command has been determined by the embedded application (e.g., as generated by a machine-learning model or other technique) and provided to the\nmessaging application for display in chat interface 840.  The suggested command would, if executed, cause the embedded application to skip the video playback of the media item to the next user comment stored for the playing video.  If the user selects\nthe suggested command, the command is sent to the embedded application and playback in embedded interface 870 is changed to a playback position at the next user comment.  A suggested message has also been determined, in this example in reaction to user\ncomment 897 as determined by the system (e.g., machine learning model based on training data providing sample user messages to similar comments).  Other suggested messages can be determined in response to any member user actions in the embedded session,\none or more chat messages in the chat interface, etc. If the user selects the suggested message, the message is displayed as a chat message in chat interface 840, e.g., in message display area 848.  In some implementations, a selected suggested message\ncan be added to the media item as a user comment similar to user comment 897.  For example, the selected message can be positioned within the embedded interface 870 and/or display area 872 adjacent to (e.g., below) existing user comment 897 as a default\nposition.  In some implementations, suggested response items such as suggested commands and/or suggested messages can be displayed in association with a displayed keyboard or other input controls, e.g., similarly to message suggestions 896 shown in FIG.\n8D.\n FIGS. 9A-9C are graphic representations of additional example user interfaces displayed by user devices.  In these examples, a messaging application is used for a conversation between users and an embedded game application is used in association\nwith the messaging application.\n In FIG. 9A, an example of a first chat interface 900 of a first messaging application is displayed on a first user device operated by a first user (e.g., \"Sarah C.\").  In this example, a chat conversation \"HanginOut\" has been initiated that\nincludes participation of the first user and first device similarly as described for FIG. 8A.  Chat messages from the first user can be input in an input field 902 of chat interface 900 (e.g., using an input device or keyboard similar to keyboard 890\nabove) and the chat messages can be displayed in a message display area 904 of chat interface 900.\n In this example, the first user previously input a command in input field 902, and the received command is displayed as message 906 in the chat interface after being input.  This command specifies an embedded interface to be displayed in\nassociation with the chat conversation.  In this example, the embedded interface is provided by an embedded application that is a game \"Toads\" which can execute on the first device in some implementations.  The command also has a \"share\" parameter to\ncause the game to be shared in the chat conversation as an embedded session.  In some implementations, the game application can be selected to display the embedded interface without sharing its output in an embedded session associated with the chat\nconversation.  In other implementations, the first user can select to initiate the embedded game application by selecting the embedded game application from a menu, list, etc.\n In response to command 906, a response message 908 can be displayed in the chat interface by a bot or the messaging application, e.g., as if the bot or messaging application were a chat user.  Response message 908 indicates that the selected\nembedded game application is being executed.  In some cases or implementations, the selection of the embedded game application causes the embedded game application to be downloaded to the first device in the background, e.g., from an embedded application\nserver over the network connected to the first device.\n After the embedded application (or a portion thereof) has been stored on the first device, the embedded application executes and displays an embedded interface 910.  In this example, embedded interface 910 is displayed within chat interface 900,\ne.g., within message display area 904 of the chat interface such that a portion of the message display area 904 remains displayed.  In some implementations, the embedded interface 910 is displayed based on data received by the first device over the\nnetwork, e.g., from an embedded application executing on a remote session server or other device connected over the network.\n Embedded interface 910 includes a display area 912 for displaying output of the game.  The display area 912 can be receptive to user input, e.g., selections of locations within the display area as commands to or actions within the game, in this\nexample.  A full screen control 914 allows user input to enlarge the embedded interface 910 to the entire screen (or other display area) of the first device.  A user identifier 916 can be displayed to indicate the first user of the chat interface as a\nplayer of the game.  In some implementations, a game score 918 can be displayed next to (or otherwise associated with) the user identifier 916 to indicate the current score of the identified user.  A close control 920 allows user input to close the\nembedded interface 910 and close the embedded game application, e.g., so that it is no longer executing on the first device.\n In some implementations, one or more embedded application indicators 924 can be displayed in the chat interface to indicate which embedded sessions are current active, similarly as described above.\n In FIG. 9B, an example of a second chat interface 940 of a second messaging application is displayed on a second user device operated by a second user (e.g., \"Emily D.\").  In this example, the second user is a participant in the chat\nconversation \"HanginOut\" that includes the first user and first device as described for FIG. 9A.  Chat messages can be input by the second user in the chat interface 940, e.g., via the input field 942.  In this example, the second user has input a\nmessage 944 to the chat conversation, which is displayed in a message display area 948 of chat interface 940.  In response, the first user has input a message 946 to the chat conversation, as displayed in display area 948 of chat interface 940.\n An embedded session notification 950 is displayed in chat interface 940.  Notification 950 indicates that an embedded session has been initiated and/or is active, which in this example utilizes the embedded game application that was executed by\nthe first user in FIG. 9A.  Notification 950 can be displayed in chat interface 940 after the second device receives information from a session server or a chat device that the embedded session has been initiated and is active.  Notification 950 can be\ndisplayed in display area 948 or other areas of chat interface 940, or outside the chat interface 940 in various implementations.\n In this example, notification 950 includes a designation 952 (e.g., icon or symbol) of the embedded game application that was executed by the first user in FIG. 9A to initiate the associated shared embedded session.  Notification 950 also\nincludes a list 956 of the chat users who have joined (e.g., are currently member users of) the embedded session.  Notification 950 also includes a join control 958 which is receptive to user input to cause the second user and second device to join the\nembedded session, if selected.  Notification 950 also includes a close control 960 which is receptive to user input to cause notification 950 to be removed from the chat interface 940.\n In FIG. 9C, chat interface 940 of FIG. 9B is shown with an example update after the second user has selected to join the embedded session indicated in the notification 950 of FIG. 9B, e.g., by selecting join control 958 of FIG. 9B.  In FIG. 9C,\nan embedded interface 970 is displayed in chat interface 940 such that a portion of the message display area 948 (and one or more chat messages) remain displayed.  For example, embedded interface 970 can be provided by an embedded game application that\nhas been downloaded (in some cases) and executed by the second device in response to the second user joining the embedded session.  In other implementations, embedded interface 970 is displayed based on data received over the network, e.g., from an\nembedded application executing on a session server or other device.  The embedded game application corresponds to the embedded game application that provides embedded interface 910 on the first device of FIG. 9A.\n After the second user and second device have joined the embedded session, the game states of the embedded interface 970 are synchronized with the current game states of the embedded session as implemented on the corresponding embedded game\napplications.  In some implementations, a session server or other server can synchronize game states on the member devices during the game by continually receiving game states from the member devices and sending synchronization updates to the member\ndevices.  Some implementations can provide peer-to-peer synchronization between the member devices.\n The embedded interface 970 includes features similar to embedded interface 910 of FIG. 9A, including a display area 972 to display output of the game and which can be receptive to user input.  A full screen control 974 allows user input to\nenlarge the embedded interface 970 to the entire screen (or other display area) of the first device.  A close control 976 allows user input to close the embedded interface 970 and close the embedded game application, e.g., so that it is no longer\nexecuting on the second device.  A user identifier 978 can be displayed to indicate the second user of chat interface 940 as a player of the game.  In some implementations, a game score 980 can be displayed next to the user identifier 978 to indicate the\ncurrent score of the user.  In this example, other user identifiers 982 and associated game scores 984 are displayed to indicate the other player users participating in the embedded game session and their current scores in the shared game.  In some\nimplementations, member users having an observer role or referee role in the game can be displayed and visually differentiated in the display from users having player roles.\n In some implementations, the game provided in the embedded session can be a real-time game in which player users may provide user input (e.g., game actions) to the game at any time to change game states, and the embedded game application can\nprovide output continuously in real-time, e.g., based on its own events and/or in response to the user input.  Some implementations can provide an embedded game application that is asynchronous or turn-based, e.g., where the game waits for input from one\nplayer (or multiple players) to change the game state and/or output, and the game waits for the next input from the same or different player(s), etc. Game output is displayed in the embedded interfaces of observer users, where input that the observer\nusers are able to provide to the game is more restricted, e.g., does not affect game states.\n Output from the game can be displayed in embedded interface 970 and/or as messages displayed in message display area 948 of chat interface 940.  For example, messages from game objects can be displayed as chat messages in message display area\n948 as described above with respect to FIG. 6.\n In some implementations, one or more embedded application indicators 986 can be displayed in the chat interface to indicate which embedded sessions are current active, similarly as described above.\n FIGS. 10A to 10D are graphic representations of additional example user interfaces displayed by user devices.  In these examples, a messaging application is used for a chat conversation between users and an embedded application is used to\nprovide a shared content document in the form of a shared list of items.\n In FIG. 10A, an example of a chat interface 1000 of a messaging application is displayed on a first user device operated by a first user (e.g., \"Emily D.\").  In this example, a chat conversation \"HanginOut\" has been initiated that includes\nparticipation of the first user and first device similarly as described for FIGS. 8A and 9A.  Chat messages from the first user can be input in an input field 1002 of chat interface 1000 and the chat messages can be displayed in a message display area\n1004 of chat interface 1000.\n In this example, several chat users have input messages in the chat conversation, as shown by messages 1006 input from three chat users different than the first user and displayed in the display area 1004.  Chat messages 1008 have been input by\nthe first user and displayed in the chat interface in display area 1004.  In this example, chat messages from other users are identified by a graphical user identifier and chat messages input by the first user are displayed right-justified on the display\nscreen.\n An embedded session notification 1020 can be displayed in chat interface 1000.  Notification 1020 indicates that an embedded session is active, similarly to the notifications 858 and 958 described above for FIGS. 8 and 9.  In this example,\nnotification 1020 is displayed after the user \"Pamela\" has initiated the embedded session by selecting an embedded list application to execute and selecting to share a list provided by the embedded list application.  Notification 1020 can be displayed in\nchat interface 1000 after the first device receives information from a session server or a chat device that the embedded session has been initiated and is active.  Notification 1020 can be displayed in display area 1004 or other areas of chat interface\n1000, or outside the chat interface 1000 various implementations.\n In this example, notification 1020 includes a designation 1024 (e.g., icon or symbol) identifying the embedded list application used in the embedded session.  Notification 1020 also includes the amount 1026 of user-response items currently in\nthe list provided in the embedded session.  Notification 1020 also includes list 1028 of the chat users who have joined (e.g., are currently member users of) the embedded session.  Notification 1020 also includes a join control 1030 allowing the first\nuser to join the embedded session if selected, and a close control 1032 causing the notification 1020 to be removed from the chat interface 1000 if selected.\n In some implementations, such as the example of FIG. 10A, a notification 1022 can be displayed in the chat interface, e.g., in display area 1004, to indicate that a user has initiated an embedded session.  Notification 1022 can include a\ndescription of data provided in the embedded session that was contributed or selected by member users, such as the number of items in a list in this example.\n In FIG. 10B, chat interface 1000 of FIG. 10A is shown with an example update after the first user has selected to join the embedded session indicated in the notification 1020 of FIG. 10A.  For example, the first user may have selected the join\ncontrol 1030 of FIG. 10A.  In FIG. 10B, an embedded interface 1040 has been displayed in the chat interface 1000 such that a portion of the message display area 1004 (and one or more chat messages) remain displayed.  For example, an embedded interface\n1040 can be provided by an embedded application that has been downloaded (in some cases) and executed by the first device in response to the first user joining the embedded session, similarly as described above.  In other implementations, the embedded\ninterface 1040 is displayed based on data received over the network, e.g., from an embedded application executing on a session server or other device.  The embedded application is the same list application initiated by the user Pamela to create the\nembedded session indicated in notification 1020 of FIG. 10A.\n In this example, embedded interface 1040 includes a name 1042 of a shared list, here based by default on the chat conversation name.  Embedded interface 1040 displays items 1044 in a list, where an item 1044 can be selected by member user input\nto indicate the item has been completed or is no longer relevant.  An add control 1046 allows user input to add a new item to the list and input text to name the new item.  Completed items 1048 can be displayed with a visual indication that they have\nbeen selected by user input to be considered completed or achieved.  A close control 1050 can also be provided.  Other features such as an indication or list of member users who have contributed particular items to the list can be displayed in various\nimplementations.\n The list of items displayed in embedded interface 1040 are synchronized with the embedded interfaces of all the member devices such that each member device displays the same items and statuses of items in the list.  In some implementations, one\nor more member users can be assigned an observer status, e.g., by the member user that initiated the embedded session, where observer users cannot modify the list.\n In FIG. 10C, chat interface 1000 of FIG. 10B is shown with an example update after the first user has selected to input a new message in the chat conversation of the chat interface.  For example, the first user may have selected input field 1002\nof chat interface 1000.  In some implementations, this can cause a displayed keyboard 1060 to be displayed in or adjacent to the chat interface 1000, which can be used to input a new message.  Some implementations can provide message suggestions 1062 as\nselectable elements responsive or related to one or more chat messages, e.g., the last chat message 1064.  Similarly as in FIG. 8D, some implementations can resize the embedded interface 1040 as shown in the example of FIG. 10C (e.g., shrink the size of\nthe embedded interface) to allow display space for both the keyboard 1060 and chat messages of the chat conversation in display area 1004 of the chat interface 1000, such that keyboard 1060, embedded interface 1040, and a portion of display area 1004 are\nsimultaneously displayed.\n In FIG. 10D, chat interface 1000 of FIG. 10B is shown with an example update after the first user has selected to provide input to the embedded interface and to the embedded application.  For example, the first user may have selected the add\ncontrol 1046 of embedded interface 1040 of FIG. 10B, e.g. using touchscreen input or input from another pointing input device, or may have selected a different option in the embedded interface that can accept text input from a keyboard.  In other cases,\nthe user may have selected a control in the embedded interface.  In some implementations, this user input to the embedded interface can cause a displayed keyboard 1070 to be displayed in or adjacent to the chat interface 1000, where keyboard 1070 can be\nused to input text (including characters, emoji, symbols, etc.).  For example, the first user can use the keyboard 1070 to input a text name or phrase for the added item in the list.\n Some implementations can provide one or more suggestions 1072 as suggested input for the embedded interface, e.g., based on an interface element selected by the user input in the embedded interface and/or based on an event of the embedded\napplication.  In this example, suggestions 1072 were determined based on the selection of the add control 1046.  For example, the embedded application (and/or the messaging application or suggestion server, after being sent the list items) can examine\nthe existing list items to determine suggested new list items that are related to the existing items.  For example, stored knowledge base or databases (e.g., hierarchical graph of related concepts) can be examined to find words that are related to words\nin the list.\n In some implementations, as shown in FIG. 10D, while keyboard 1070 is displayed to allow input to the embedded interface 1040, the embedded interface 1040 can be maintained in its normal size as shown (or can be displayed in a larger size than\nthe small-sized embedded interface 1040 of FIG. 10C).  The display area 1004 that displays chat messages can be removed from the chat interface 1000 while keyboard 1070 is displayed to enable input to the embedded interface 1040, as shown.\n Some implementations can display a keyboard similar to keyboard 1070 for input to the embedded interface, and/or can display suggestions to be input to the embedded interface similar to suggestions 1072, for other types of embedded applications,\ne.g., media players, games, document processing applications, etc. as described herein for other implementations.\n Examples of Bot Implementations\n A bot is an automated service, implemented on one or more computers, that users interact with primarily through text, e.g., via messaging application 103a/103b.  A bot may be implemented by a bot provider such that the bot can interact with\nusers of various messaging applications.  In some implementations, a provider of messaging application 103a/103b may also provide one or more bots.  In some implementations, bots provided by the provider of messaging application 103a/103b may be\nconfigured such that the bots can be included in other messaging applications, e.g., provided by other providers.  In some implementations, one or more bots can be provided by, generated by, and/or included in an embedded application.\n A bot may provide several advantages over other modes.  For example, a bot may permit a user to try a new service (e.g., a taxi booking service, a restaurant reservation service, etc.) without having to install an application on a client device,\nor accessing a website.  Further, a user may interact with a bot via text, which requires minimal or no learning compared with learning to use a website, software application, a telephone call to, e.g., an interactive voice response (IVR) service, or\nother manners of interacting with a service.  Incorporating a bot within a messaging service or application may also permit users to collaborate with other users to accomplish various tasks such as travel planning, shopping, scheduling events, obtaining\ninformation, etc. within the messaging service, and eliminate cumbersome operations such as switching between various applications (e.g., a taxi booking application, a restaurant reservation application, a calendar application, etc.) or web sites to\naccomplish the tasks.\n A bot may be implemented as a computer program or application (e.g., a software application) that is configured to interact with one or more users (e.g., any of the users 125a-n) via messaging application 103a/103b to provide information or to\nperform specific actions within the messaging application 103.  As one example, an information retrieval bot may search for information on the Internet and present the most relevant search result within the messaging app. As another example, a travel bot\nmay have the ability to make travel arrangements via messaging application 103, e.g., by enabling purchase of travel and hotel tickets within the messaging app, making hotel reservations within the messaging app, making rental car reservations within the\nmessaging app, and the like.  As another example, a taxi bot may have the ability to call a taxi, e.g., to the user's location (obtained by the taxi bot from client device 115, when a user 125 permits access to location information) without having to\ninvoke or call a separate taxi reservation app. As another example, a coach/tutor bot may tutor a user to instruct the user in some subject matter within a messaging app, e.g., by asking questions that are likely to appear on an examination and providing\nfeedback on whether the user's responses were correct or incorrect.  As another example, a game bot may play a game on the opposite side or the same side as a user within a messaging app. As another example, a commercial bot may provide services from a\nspecific merchant, e.g., by retrieving product information from the merchant's catalog and enabling purchase through a messaging app. As another example, an interface bot may interface a remote device or vehicle so that a user of a messaging app can chat\nwith, retrieve information from, and/or provide instructions to the remote device or vehicle.\n A bot's capabilities may include understanding a user's intent and executing on it.  The user's intent may be understood by analyzing and understanding the user's conversation and its context.  A bot may also understand the changing context of a\nconversation or the changing sentiments and/or intentions of the users based on a conversation evolving over time.  For example, if user A suggests meeting for coffee but if user B states that he does not like coffee, then a bot may assign a negative\nsentiment score for coffee to user B and may not suggest a coffee shop for the meeting.\n Implementing bots that can communicate with users of messaging application 103a/103b may provide many advantages.  Conventionally, a user may utilize a software application or a website to perform activities such as paying bills, ordering food,\nbooking tickets, etc. A problem with such implementations is that a user is required to install or use multiple software applications, and websites, in order to perform the multiple activities.  For example, a user may have to install different software\napplications to pay a utility bill (e.g., from the utility company), to buy movie tickets (e.g., a ticket reservation application from a ticketing service provider), to make restaurant reservations (e.g., from respective restaurants), or may need to\nvisit a respective website for each activity.  Another problem with such implementations is that the user may need to learn a complex user interface, e.g., a user interface implemented using multiple user interface elements, such as windows, buttons,\ncheckboxes, dialog boxes, etc.\n Consequently, an advantage of one or more described implementations is that a single application enables a user to perform activities that involve interaction with any number of parties, without being required to access a separate website or\ninstall and run software applications, which has a technical effect of reducing consumption of memory, storage, and processing resources on a client device.  An advantage of the described implementations is that the conversational interface makes it\neasier and faster for the user to complete such activities, e.g., without having to learn a complex user interface, which has a technical effect of reducing consumption of computational resources.  Another advantage of the described implementations is\nthat implementing bots may enable various participating entities to provide user interaction at a lower cost, which has a technical effect of reducing the need for computational resources that are deployed to enable user interaction, such as a toll-free\nnumber implemented using one or more of a communications server, a web site that is hosted on one or more web servers, a customer support email hosted on an email server, etc. Another technical effect of described features is a reduction in the problem\nof consumption of system processing and transmission resources used for completing user tasks across communication networks.\n While certain examples herein describe interaction between a bot and one or more users, various types of interactions, such as one-to-one interaction between a bot and a user 125, one-to-many interactions between a bot and two or more users\n(e.g., in a group messaging conversation), many-to-one interactions between multiple bots and a user, and many-to-many interactions between multiple bots and multiple users are be possible.  Further, in some implementations, a bot may also be configured\nto interact with another bot (e.g., bots 107a/107b, 109a/109b, 111, 113, etc.) via messaging application 103, via direct communication between bots, or a combination.  For example, a restaurant reservation bot may interact with a bot for a particular\nrestaurant in order to reserve a table.\n In certain embodiments, a bot may use a conversational interface, such as a chat interface, to use natural language to interact conversationally with a user.  In certain embodiments, a bot may use a template-based format to create sentences with\nwhich to interact with a user, e.g., in response to a request for a restaurant address, using a template such as \"the location of restaurant R is L.\" In certain cases, a user may be enabled to select a bot interaction format, e.g., whether the bot is to\nuse natural language to interact with the user, whether the bot is to use template-based interactions, etc.\n In cases in which a bot interacts conversationally using natural language, the content and/or style of the bot's interactions may dynamically vary based on one or more of: the content of the conversation determined using natural language\nprocessing, the identities of the users in the conversations, and one or more conversational contexts (e.g., historical information on the user's interactions, connections between the users in the conversation based on a social graph), external\nconditions (e.g., weather, traffic), the user's schedules, related context associated with the users, and the like.  In these cases, the content and style of the bot's interactions is varied based on only such factors for which users participating in the\nconversation have provided consent.\n As one example, if the users of a conversation are determined to be using formal language (e.g., no or minimal slang terms or emojis), then a bot may also interact within that conversation using formal language, and vice versa.  As another\nexample, if a user in a conversation is determined (based on the present and/or past conversations) to be a heavy user of emojis, then a bot may also interact with that user using one or more emojis.  As another example, if it is determined that two\nusers in a conversation are in remotely connected in a social graph (e.g., having two or more intermediate nodes between them denoting, e.g., that they are friends of friends of friends), then a bot may use more formal language in that conversation.  In\nthe cases where users participating in a conversation have not provided consent for the bot to utilize factors such as the users' social graph, schedules, location, or other context associated with the users, the content and style of interaction of the\nbot may be a default style, e.g., a neutral style, that doesn't require utilization of such factors.\n Further, in some implementations, one or more bots may include functionality to engage in a back-and-forth conversation with a user.  For example, if the user requests information about movies, e.g., by entering \"@moviebot Can you recommend a\nmovie?\", the bot \"moviebot\" may respond with \"Are you in the mood for a comedy?\" The user may then respond, e.g., \"nope\" to which the bot may respond with \"OK.  The sci-fi movie entitled Space and Stars has got great reviews.  Should I book you a\nticket?\" The user may then indicate \"Yeah, I can go after 6 pm.  Please check if Steve can join\".  Upon user's consent to the bot accessing information about their contacts and upon the friend Steve's consent to receiving messages from the bot, the bot\nmay send a message to user's friend Steve and perform further actions to book movie tickets at a suitable time.\n In certain embodiments, a user participating in a chat conversation may be enabled to invoke a specific bot or a bot performing a specific task, e.g., by typing a bot name or bot handle (e.g., taxi, @taxibot, @movies, etc.), by using a voice\ncommand (e.g., \"invoke bankbot\", etc.), by activation of a user interface element (e.g., a button or other element labeled with the bot name or handle), etc. Once a bot is invoked, a user 125 may send a message to the bot via messaging application\n103a/103b in a manner similar to sending messages to other users 125.  For example, to order a taxi, a user may type \"@taxibot get me a cab\"; to make hotel reservations, a user may type \"@hotelbot book a table for 4 at a Chinese restaurant near me.\"\n In certain embodiments, a bot may automatically suggest information or actions within a chat conversation without being specifically invoked.  That is, the users may not need to specifically invoke the bot.  In these embodiments, the bot may\ndepend on analysis and understanding of the chat conversation on a continual basis or at discrete points of time.  The analysis of the chat conversation may be used to understand specific user needs and to identify when assistance should be suggested by\na bot.  As one example, a bot may search for some information and suggest the answer if it is determined that a user needs information (e.g., based on the user asking a question to another user, based on multiple users indicating they don't have some\ninformation).  As another example, if it is determined that multiple users have expressed interest in eating Chinese food, a bot may automatically suggest a set of Chinese restaurants in proximity to the users, including optional information such as\nlocations, ratings and links to the web sites of the restaurants.\n In certain embodiments, rather than automatically invoking a bot or waiting for a user to explicitly invoke a bot, an automatic suggestion may be made to one or more users in a messaging conversation to invoke one or more bots.  In these\nembodiments, the chat conversation may be analyzed on a continual basis or at discrete points of time, and the analysis of the conversation may be used to understand specific user needs and to identify when a bot should be suggested within the\nconversation.\n In the embodiments in which a bot may automatically suggest information or actions within a chat conversation without being specifically invoked, such functionality is disabled, e.g., if one or more users participating in the chat conversation\ndo not provide consent to a bot performing analysis of the user's conversation.  Further, such functionality may also be disabled temporarily based on user input.  For example, when the users indicate that a chat conversation is private, analysis of\nconversational context is suspended until users provide input for the bot to be activated.  Further, indications that analysis functionality is disabled may be provided to participants in the chat conversation, e.g., with a user interface element.\n In various implementations, a bot may be implemented in a variety of configurations.  For example, as shown in FIG. 1, bot 105 is implemented on client device 115a.  In this example, the bot may be a module in a software application that is\nlocal to client device 115a.  For example, if a user has installed a taxi hailing application on client device 115a, bot functionality may be incorporated as a module in the taxi hailing application.  In this example, a user may invoke a taxi bot, e.g.,\nby sending a message \"@taxibot get me a cab.\" Messaging application 103b may automatically cause the bot module in the taxi hailing application be launched.  In this manner, a bot may be implemented locally on a client device such that the user can\nengage in conversation with the bot via messaging application 103.\n In another example shown in FIG. 1, bot 107a is shown implemented on client device 115a and bot 107b is shown as implemented on messaging server 101.  In this example, the bot may be implemented, e.g., as a client-server computer program, with\nportions of the bot functionality provided by each of bot 107a (server module) and bot 107b (client module).  For example, if the bot is a scheduling bot with the handle @calendar, user 125a may schedule a reminder, by typing \"@calendar remind me to pick\nup laundry in the evening,\" which may be handled by bot 107b (client module).  Continuing with this example, if user 125a tells the bot \"check if Jim is free to meet at 4,\" bot 107a (server module) may contact user Jim (or Jim's scheduling bot) to\nexchange messages, and provide a response to user 125a.\n In another example, bot 109a (server module) is implemented on server 135 and bot 109b (client module) is implemented on client devices 115.  In this example, the bot functionality is provided by modules implemented on client devices 115 and\nserver 135, which is distinct from messaging server 101.  In some implementations, a bot may be implemented as a distributed application, e.g., with modules distributed across multiple client devices and servers (e.g., client devices 115, server 135,\nmessaging server 101, etc.).  In some implementations, a bot may be implemented as a server application, e.g., bot 111 that is implemented on messaging server 101 and bot 113 that is implemented on server 135.\n Different implementations such as client-only, server-only, client-server, distributed, etc. may provide different advantages.  For example, client-only implementations permit bot functionality to be provided locally, e.g., without network\naccess, which may be advantageous in certain contexts, e.g., when a user is outside of network coverage area or in any area with low or limited network bandwidth.  Implementations that include one or more servers, such as server-only, client-server, or\ndistributed configurations may permit certain functionality, e.g., financial transactions, ticket reservations, etc. that may not be possible to provide locally on a client device.\n While FIG. 1 shows bots as distinct from messaging application 103, in some implementations, one or more bots may be implemented as part of messaging application 103.  In the implementations in which bots are implemented as part of messaging\napplication 103, user permission is obtained before implementing bots.  For example, where bots are implemented as part of messaging application 103a/103b, messaging application 103a/103b may provide bots that can perform certain activities, e.g., a\ntranslation bot that translates incoming and outgoing messages, a scheduling bot that schedules events on a user's calendar, etc. In this example, translation bot is activated only upon user's specific permission.  If the user does not provide consent,\nbots within messaging application 103a/103b are not implemented (e.g., disabled, removed, etc.).  If the user provides consent, a bot or messaging application 103a/103b may make limited use of messages exchanged between users via messaging application\n103a/103b to provide specific functionality, e.g., translation, scheduling, etc.\n In some implementations, third parties distinct from a provider of messaging application 103a/103b and users 125, may provide bots that can communicate with users 125 via messaging application 103a/103b for specific purposes.  For example, a\ntaxi service provider may provide a taxi bot, a ticketing service may provide a bot that can book event tickets, a bank bot may provide capability to conduct financial transactions, etc.\n In implementing bots via messaging application 103, bots are permitted to communicate with users only upon specific user authorization.  For example, if a user invokes a bot, the bot can reply, e.g., based on the user's action of invoking the\nbot.  In another example, a user may indicate particular bots or types of bots that may contact the user.  For example, a user may permit travel bots to communicate with her, but not provide authorization for shopping bots.  In this example, messaging\napplication 103a/103b may permit travel bots to exchange messages with the user, but filter or deny messages from shopping bots.\n Further, in order to provide some functionality (e.g., ordering a taxi, making a flight reservation, contacting a friend, etc.), bots may request that the user permit the bot to access user data, such as location, payment information, contact\nlist, etc. In such instances, a user is presented with options to permit or deny access to the bot.  If the user denies access, the bot may respond via a message, e.g., \"Sorry, I am not able to book a taxi for you.\" Further, the user may provide access\nto information on a limited basis, e.g., the user may permit the taxi bot to access a current location only upon specific invocation of the bot, but not otherwise.  In different implementations, the user can control the type, quantity, and granularity of\ninformation that a bot can access, and is provided with the ability (e.g., via a user interface) to change such permissions at any time.  In some implementations, user data may be processed, e.g., to remove personally identifiable information, to limit\ninformation to specific data elements, etc. before a bot can access such data.  Further, users can control usage of user data by messaging application 103a/103b and one or more bots.  For example, a user can specify that a bot that offers capability to\nmake financial transactions require user authorization before a transaction is completed, e.g., the bot may send a message \"Tickets for the movie Space and Starts are $12 each.  Shall I go ahead and book?\" or \"The best price for this shirt is $125,\nincluding shipping.  Shall I charge your credit card ending 1234?\" etc.\n In some implementations, one or more suggested response items generated by systems described above can be messages used to invoke or command a bot, e.g., request a bot for information.  For example, the system can determine whether a received\nimage includes content that would be assisted by information and/or one or more functions provided by a bot.  In some examples, the first user in a conversation may send a text message to the second user, saying, \"Guess where?\" The first user then sends\nan image to the second user's device.  The system can determine that a phrase including the word \"guess\" indicates that a suggested response can be a request to find out the information related to the image content.  (Other words can also indicate such a\nrequest, such as \"where,\" \"what,\" etc.) In response, the system can generate a suggested response that is a request or command to an appropriate bot that can provide the relevant information.  For example, a suggested response can be a request to a\nmapping bot to provide the name of the location depicted in the image (e.g., where the bot can determine such information using web searches, maps and atlases, geographic location metadata of the received image, etc.).  In some implementations, the\ntraining message data can be used (if user consent has been obtained) to determine appropriate types of bots for particular types of image content that has been detected in the image (e.g., using one or more image detection or recognition techniques, if\nuser consent has been obtained), and/or rules-based grammars can determine which types of bots to invoke in a suggested response based on the types of image content and/or concepts detected in the image content.\n In some implementations, if the bot-related suggested item is displayed for the user as a possible response and the user selects that suggestion, the bot can be added to the chat conversation and messaging interface, e.g., represented with a bot\nname.  The bot can determine and output the requested information in the conversation, e.g., as messages to both the first and second users in a chat interface.\n In another example, the first user inputs an image depicting text that is in a language foreign to the second user to a chat conversation or embedded application.  The system can determine that an appropriate suggested item is a request to a\nlanguage bot to translate the text in the image and to output the translated text in the chat interface or embedded interface.\n In another example, the first user inputs an image depicting a food item to a chat conversation.  The system can determine that an appropriate suggested response is a request to a bot to check one or more accessible data sources (e.g., on the\nInternet) to determine the ingredients and/or calories typically or specifically included in the food item, and to output that information in the message conversation interface.\n Implementations described herein generally relate to messaging applications.  Certain implementations may automatically (e.g., without user intervention) analyze image content of one or more chat conversations and/or user information to\nautomatically provide suggested items to a user within a messaging application.  In certain examples, the automatic suggested items may be selected by a user to respond in the embedded application or in the messaging application, and/or may be\nautomatically provided as one or more appropriate responses on behalf of a user.  In certain other examples, the suggestions may automatically incorporate particular non-messaging functionality into the messaging application.\n Suggested items may be provided based on any type of media content that is received in a chat conversation.  For example, such media content may include stickers (e.g., in a chat application), animated images (e.g., cinemagraphs, GIF images,\netc.), and videos.  Further, various types of items may be suggested.  For example, suggested messages may include one or more of an image, a sticker, an animated image (e.g., cinemagraph, GIF image, etc.) and a video.  To provide these suggestions, a\ncomparison can be made of identified concepts in a received item (e.g., text, image, video, sticker, animated image, etc.) with concepts in different types of responses, and select a suitable response.  In different implementations where users provide\nconsent, the type of suggested item may be selected or prioritized based on context, e.g., a sticker may be provided as a suggested item for a particular action in a game, a second video may be provided as a suggested item for a playing video, etc.\n Certain implementations enable messaging with human users and/or chat bots.  In certain implementations, automatic suggested items may be customized based on whether a chat bot is participating in the chat conversation.  In some examples, a\nfirst set of automatic suggested items may be provided if a chat bot is absent in a messaging conversation, while a second set of automatic suggested items may be provided if a chat bot is present in the chat conversation, where the first and second sets\nof suggested items are at least partially different.  For example, these implementations may employ conversational rules followed by the chat bot, and suggest items to a user based on the rules.  This can mitigate challenges that users may have in\ncommunicating with chat bots in a language and in a format that is easily understood by the chat bots.\n Some implementations can include determining one or more trending responses (e.g., messages including popular message content sent by many different users) based on other messages in at least one of a region, market, and country related to a\nlocation of a user.  One or more determined suggested items may include one or more trending responses.  In some implementations, a user context, e.g., a geographic location, holiday or an event, etc., can be used to generate and determine for\npresentation one or more of the suggested items.\n In some implementations, determining one or more suggested response items may be based on using machine learning to develop a personalized model for a user.  Determining suggested items may be based on preferences of the user and/or prior\nactions of the user in communications (if user consent for use of such actions and data has been obtained).  For example, user preferences may include a whitelist indicating particular words which can be included and/or a blacklist indicating particular\nwords which cannot be included in message suggestions.  If user consent has been obtained, suggested items can be generated or modified based on one or more of punctuation use, emoji use, or other content provided by the user on previous occasions.\n Machine learning models used to provide suggested items may be implemented by a client device 115 and/or a server, e.g., messaging server 101 and/or a server 150-156.  In some implementations, chat conversations may be encrypted such that only\nclient devices of participants in the chat conversation can access conversation content.  In these implementations, models implemented by a respective client device may be used to provide suggested response items and models implemented by a server are\nnot used.  Models implemented by a client device may also be used, e.g., when the user does not provide consent for use of models implemented by a server.  In some implementations, client implemented models may be based on or derived from server\nimplemented models.  In some implementations, server models may be used and client models may not be used, e.g., when a client device lacks capability to implement client models.  In some implementations, a combination of client and server models may be\nused.\n While the examples described in this document utilize concepts illustrated in English, suggestions may be provided in any language, e.g., a language, locale or other geographic configured for a client device 115, a language selected based on a\nuser preference, etc. In some implementations, where users provide consent for analysis of context of a chat conversation, a language that is used in various conversations (e.g., in recent messages) involving the user may be detected and message\nsuggestions can be provided in that language.\n Further Examples of Bots and Embedded Applications\n In some implementations, bots as described above can be used by a user in conjunction with the use of an embedded application in one or more chat conversations implemented on a user device.\n In some bot implementations, a user can participate in a chat conversation with a bot and without other human users participating in the chat conversation.  For example, an assistant bot can be provided by a local program executing on the user\ndevice, or provided by a server (e.g., server application program).  In some examples, a new, bot chat conversation can be opened including the user and one or more bots, in response to the user inputting a bot command in a first chat conversation (e.g.,\ncommand chat message), selecting a bot activation control in the chat interface, etc. In the bot chat conversation, the user can input information (e.g., questions, commands, statements) as messages that can be processed by the bot, and the bot can send\ncommands or data to embedded applications based on those messages.  For example, a bot can initiate an appropriate embedded application associated with a chat interface based on a command in the chat interface from a user to the bot to join a particular\nembedded session, playback a media item, play a game, create a shared list, etc. In some implementations, bots can determine and present the suggested response items described herein, e.g., upon a command provided to the bot from user input in the chat\ninterface, and, e.g., simultaneously with execution of an embedded application and embedded session to which the suggested response items are relevant.  In some implementations, bots can receive information related to events occurring in embedded\napplications and can generate and cause to be displayed interactive cards describing the events in embedded sessions, as described herein.\n In some examples, in a chat conversation with a bot having media player functionality, a user can input questions in the chat interface or conversation, and the bot can initiate a media player embedded application that plays a particular media\nitem in an embedded interface displayed in association with the bot chat conversation or in association with a different chat conversation (e.g., a chat conversation including other human chat users).  For example, if played in association with a\ndifferent chat conversation, the bot chat conversation can be hidden from the display and the different chat conversation can be displayed in the chat interface, with the embedded interface displayed in association with the chat interface.\n In an example in which the embedded application can play a media item, e.g., as described in implementations herein, a media player bot can be invoked in a chat conversation by a user.  For example, the bot can be invoked by inputting a command\nin the chat interface similarly to a chat message, and including command syntax (e.g., a symbol before the command), or by inputting a different type of selection (e.g., selecting a displayed interface element).  The media player bot can support queries\nfrom the user related to media items played or playable by the embedded application.\n In some implementations, a media player bot can be used to search for stored media items or other types of content items that have been saved in accessible storage by the user and/or by other chat users, and/or have had links to the content\nitems posted in the chat conversation.  In some examples, a query for the bot can be input to the chat interface as a chat message.  For example, a search query can be provided as \"@mediaplay cat videos\" to cause the media player bot to search for cat\nvideos in a collection of stored media items.  Specific titles of media items can also be queried.  In some implementations, a conversational search can be provided.  For example, a search query can be input as \"show me the most recent episode of\nGameshow\" or \"play the most popular Gameshow video\" or \"show me five videos you recommend for me to watch\" or \"send me a new cat video each day.\" In some implementations, the recommended media items can be provided by a content server which may have\ntracked previous user viewings and preferences (with user consent) and ranked media items for the user.\n Some implementations can provide one or more bots to receive user input forming a search query used in searching for other data items or content items to be used in embedded applications.  For example, such items can include items to be added to\na shared list, a time in a particular time zone to be used in a travel itinerary, a name of a restaurant for a reservation, etc. The bot can search for such items on databases, servers, or other resources accessible on the user device and over the\nnetwork.  Items found by the bot in response to a search query can be presented in the chat interface for selection by the user, or can be provided automatically to the associated embedded application by the bot (and, in some implementations, the bot can\ninitiate the embedded application if the embedded application is not already executing).  Items provided to the embedded application can be displayed in the embedded interface, if appropriate.\n In some implementations, as described above, playback of media items in an embedded session can be saved for later for a particular user to enable that user to play the media item at a later time, e.g., in a new embedded session.  In some\nimplementations, such saved media items can include recordings (e.g., captured video data, screen capture images, etc.) of games or other user and application activity/events that occurred in one or more embedded sessions with chat users, if consent has\nbeen obtained from the chat users.\n In some implementations, media items can be saved for the user in a dedicated separate chat conversations.  For example, a user can provide, in the chat conversation, user input to save a particular media item.  In response, a second chat\nconversation is identified that is associated with the user and a bot is associated with the media item.  The media item is inserted into the second chat conversation, and previous saved media items are also listed in the second chat conversation.  The\nmessaging application can receive input from the user that is associated with the media item, e.g., to play a selected media item.  In response, the first chat conversation is updated by playing the selected media item using an embedded application.\n In some additional examples, a save control can be provided in a notification indicating that a media player embedded session is active, or in association with a media item link displayed in the chat conversation.  If, for example, a user\nreceives a notification that a media player embedded session is active (or sees a link to a media item in the chat interface), the user can select the save control.  This selection can cause a second chat conversation (e.g., media selection chat\nconversation) to be displayed in which only the user and the bot are participating.  A link to the media item in the embedded session can be displayed in the media selection chat conversation.  Furthermore, any saved media items from previous embedded\nsessions can be included in the media selection chat conversation as previous messages.  Thus, the user can open the media selection chat conversation to select and play saved media items associated with the user.  In some implementations, an option can\nbe provided to automatically insert a user's media items into a media selection chat conversation, e.g., to allow viewing and playing of the media items from a single collection.\n In some implementations, the system (e.g., a bot) provides selections in the chat interface for additional media items that are determined to be similar to a media item that was shared in an embedded session.  For example, the additional media\nitems can have the same genre, were authored by or feature the same artists, etc.\n In some implementations, a share option can be provided, which if selected causes a media item being played in an embedded interface to be shared to a different chat conversation or share to a new chat conversation that is created for the\nselected sharing action.  For example, to cause the share option to be displayed, a user can touch the embedded interface playing the media item for over a threshold amount of time to cause the share option to be displayed, or can select a particular\ndisplayed interface element.\n In some implementations, the user can select a comment control to enable inputting a comment to the chat conversation associated with the embedded session playing the media item.  In some implementations, a link to play the media item is posted\nin the chat conversation, followed by a user comment input by the user.  If the comment control is selected while viewing a saved media item viewed in a different chat conversation (e.g., a media selection chat conversation as described above), the\nselected comment control can cause the originating chat conversation to be opened and displayed, allowing the person to comment in the originating chat conversation where the media item was played in an embedded session.\n In some implementations, suggested responses as described herein can be used with bots.  For example, selection of a suggested response can trigger a bot associated with the suggested response to perform one or more actions in an embedded\napplication, messaging application, or other program or device.  In some examples, a bot can perform an action on embedded application content without opening the embedded application.  For example, the embedded application can allow a user to provide\ncommands to the embedded application in the chat interface (e.g., by providing a command to a bot), and allow users to interact with the embedded application visually via the embedded interface.  In some examples in which an embedded application has or\ncommunicates with a server component (e.g., a game where game states are managed by a server), commanding the bot via a chat message to perform actions in the embedded application and embedded interface also causes the actions to be performed in the\nembedded interfaces displayed by other member devices.  In another example, a shared list embedded application causes list data to be stored on a server.  The first user opens the shared list embedded application and adds items to the list or removes\nitems from the list.  Other users' devices can display suggested responses to add items to the list via bot commands to the embedded application using the chat interface.  For example, the suggested responses can be in response to chat messages such as\n\"don't forget to buy drinks for the party\" (e.g., a suggested response of \"Add `Buy drinks` to the `Tonight's Party` list\") or in response to chat messages such as \"I just bought the sodas for tonight's party\" (e.g., a suggested command of \"Remove `Buy\nSodas` from the `Tonight's Party` list\").  In some implementations, these example suggested responses, when selected by the associated user, can call the bot to send the command to the embedded application, and, e.g., would not provide the command to the\nembedded interface.  In some implementations, an advantage of providing a bot interface (via chat interface) in addition to the embedded interface is that a visual interface need not be displayed on a device, thus saving device processing, memory, and\nscreen space resources, and enabling quick user interaction with content data managed by the embedded application.\n FIG. 11 is a diagrammatic illustration of an example sequence including initiation of an embedded application and a user joining an embedded session, according to some implementations.  In this example, a first user 1102 (\"User 1\") is operating\na first user device that executes messaging application 1104 which displays a first chat interface.  A second user 1106 (\"User 2\") is operating a second user device that executes second messaging application 1108 which displays a second chat interface, A\nchat conversation is provided over a network between the messaging applications 1104 and 1108 in which the users 1102 and 1106 (and their devices) are participating, where the users are inputting messages in their respective chat interfaces to be\ndisplayed by the participating user devices.\n In 1110, user 1102 selects to open an embedded application 1114 by providing user input to messaging application 1104.  At 1114, first messaging application 1104 initiates the selected embedded application 1112.  In some implementations, the\nfirst embedded application 1112 is downloaded from a server, e.g., embedded application server 150 (not shown).  At 1116, the initiated first embedded application 1112 causes an embedded interface to be displayed by the first user device, e.g., in,\nadjacent, or otherwise in association with the first chat interface, and which is viewed by first user 1102.\n In addition to starting the embedded application, at 1118 the first messaging application sends information over the network indicating an initiate event (or join event of the first user) based on the initiation of the first embedded application\n1112.  In this example, the event at 1118 is sent to a message router 1120.  Message router 1120 can be a session server 152 (e.g., included in a messaging server 101) or other server, in various examples.  An embedded session can be created at the\nmessage router in response to the initiate event some implementations, or can be created by the first user device, etc. Message router 1120 sends the join event at 1118 to the second messaging application 1108 operating on the second user device.  At\n1122, the second messaging application 1108 causes a notification to be displayed by the second user device to be viewed by the second user, e.g., in the second chat interface, indicating that the first user 1102 has initiated an embedded session.  In\nthis example, the notification includes a displayed join option (e.g., interface element) for the second user to select to join the embedded session.\n At 1124, the second user 1106 selects the displayed join option to join the embedded session, where this selection is provided to the second messaging application 1108.  In response to the join selection, at 1126 the second messaging application\n1108 causes initiation of a second embedded application 1128 on the second user device (e.g., downloaded in some implementations).  At 1130, the second embedded application causes display of a second embedded interface on the second user device.  In\naddition to initiating the second embedded application, at 1132 the second messaging application 1108 sends information over the network indicating a join event of the second user to the embedded session in response to the join selection at 1124.  In\nthis example, the join event 1132 is sent to message router 1120.\n The join event 1132 is sent by the message router 1120 over the network to the first messaging application 1104 on the first user device.  In response, at 1122, the first messaging application 1104 sends a member update to the first embedded\napplication 1112 for the second user joining the embedded session.  For example, the member update can include data indicating that the second user has joined, the chat identity of the second user, a selected user status of the second user, etc., as\ndescribed herein.  In response to the member update, the first embedded application 1112 updates the display of the first embedded interface at 1136, as viewed by the first user 1102.  For example, the chat identity (or other user identity) of the second\nuser can be displayed in the embedded interface.\n In various example implementations of systems, methods, and non-transitory computer readable medium having stored software instructions, an embedded application is provided in association with a messaging application.  A first embedded\napplication can be initiated in association with a first chat interface displayed by a first messaging application that executes at least in part on a first user device.  The first chat interface is configured to receive first user input from a first\nuser and display messages originating from one or more other user devices participating in a chat conversation over a network.  The one or more other user devices are associated with one or more chat users, the chat users associated with respective chat\nidentities.  An indication is received over the network indicating that one or more particular user devices of the other user devices have connected to an embedded session associated with the first embedded application.  In response to the indication,\none or more chat identities are provided from the first messaging application to the first embedded application, which are associated with one or more particular chat users of the particular user devices.  The particular chat users are designated as one\nor more member users of the embedded session, and the first embedded application is updated based on data received from at least one of the particular user devices of the embedded session.\n In further examples, the first embedded application receives the first user input from the first user to an embedded interface displayed as part of the first chat interface, and the first user input includes selection of an interface element in\nthe embedded interface.  The first embedded application is updated in response to the first user input.  In some examples, each of the particular user devices of the embedded session execute a respective embedded application that provides a respective\nembedded interface associated with a respective chat interface displayed on the respective particular user device, and the first embedded application is updated based on the data received from at least one of the particular user devices of the embedded\nsession.  The data can include other user input received by at least one of the respective embedded interfaces from at least one of the one or more member users of the embedded session.  Output data can be obtained from the first embedded application to\nbe displayed in the first chat interface, and, in response to receiving the output data, a prompt is displayed by the first user device, the prompt including a user-selectable control to consent to display of the output data in the chat interface.  Prior\nto providing the one or more chat identities to the first embedded application, one or more of the chat identities can be anonymized to create anonymized chat identities, such that the anonymized chat identities are provided to the first embedded\napplication.\n In further examples, information is transmitted to the other user devices participating in the chat conversation, which causes display of an interface element by the other user devices, the interface element receptive to selective user input at\neach of the other user devices, and in response to receiving the selective user input at the interface element on a particular user device, designating the particular user device as a member device of the embedded session.  One or more characteristics of\nthe one or more particular user devices can be determined and provided to the first embedded application, where the characteristics include a geographic location of at least one of the particular user devices, a local time for at least one of the\nparticular user devices, and/or an available data bandwidth over the network for at least one of the particular user devices, where the characteristics are used by the first embedded application to determine output information to be displayed by the\nparticular user devices, and/or cause a modification of transmission of input data to the first user device from one or more other devices over the network.  The indication can be received from a server that manages the embedded session, and updates to\nthe first embedded application can be received from the server, where the updates are based on the other user input from the particular chat users, the other user input provided to the server.\n In further examples, a notification is output, to be displayed in respective chat interfaces of the one or more other user devices, where the notification indicates that the particular user devices are member devices of the embedded session.  In\nsome implementations, a notification is output to the chat conversation in response to one or more particular types of events that occur in the embedded session, including events that occur in the first embedded application and include initiation of\nplayback of a media item, ceasing of the playback of the media item, a change in the playback of the media item based on the first user input and/or the other user input, a game event occurring in a game of the embedded session, an update to the game\nbased on the first user input and/or the other user input, and/or an update to a shared content document based on the first user input and/or the other user input.  In some implementations, a notification is output to the chat conversation in response to\nreceiving server event information indicating that one or more events have occurred on a server, and providing the server event information to the first embedded application to display on the first user device.\n In further examples, a suggestion event in the embedded session is determined based on receiving second user input received from the first user in the first chat interface, embedded event information from the first embedded application, and/or\nserver event information from a server coupled to the network, obtaining one or more suggested response items based on the suggestion event, and causing to be displayed the one or more suggested response items by the first user device.  For example,\nthird user input indicative of a selected suggested response item can be received, causing output of a chat message associated with the selected suggested response item in the first chat interface displayed on the first user device and at the other user\ndevices participating in the chat conversation.  In another example, third user input selecting a selected suggested response item, and provide one or more commands associated with the selected suggested response item.  In various examples, the commands\ncan include at least one first command provided to the first embedded application, at least one second command provided to the first messaging application, and/or at least one third command provided to the first user device to cause display of output\ninformation from a different application executing on the first user device.  The output information can include information received from a server device in communication with the first user device.  In further examples, the commands can include a first\ncommand to add a first set of one or more other users to the embedded session in response to determining that a second content item is the same as a first content item being output in an embedded interface of the embedded application on the first user\ndevice, where the second content item is displayed on a first set of one or more user devices not participating in the chat conversation.  The command can include a second command to add a second set of one or more other users to the embedded session in\nresponse to determining that the second set of one or more other users have input user comments associated with a fourth content item that is the same as a third content item being output in the embedded interface on the first user device, where the\nfourth content item is displayed on a second set of one or more user devices not participating in the chat conversation.\n In further examples, content data can be received at the messaging application over a network from a server application executing at a server device and provided to the embedded application to cause output information to be displayed by the\nembedded application for the embedded session in a embedded interface displayed on the first user device and the one or more particular user devices.  Event information is received over the network indicating a suggestion event occurring at the server\napplication, where the suggestion event is based on input received by the server application by one or more network users different than the one or more chat users.  One or more suggested response items responsive to the suggestion event are obtained,\nand the suggested response items are displayed in the first chat interface on the first user device.  Suggested response items are selectable by user input to cause a message to be displayed in the chat interface, and/or a command to be sent to one of\nthe embedded application and the messaging application.\n In further examples, prior to the one or more particular chat users being designated as the one or more member users of the embedded session, selection input is received from the particular chat users selecting one or more associated user roles\nfor the embedded session.  The associated user roles are selected from a plurality of predefined user roles.  In response to receiving the selection input, each of the one or more particular chat users is enabled to be assigned, in the embedded session,\na respective associated user role selected by the particular chat user.  Each particular chat user is enabled to activate one or more functions of the embedded application associated with the respective associated user role selected by the particular\nchat user.  Indications of the associated user roles and the particular chat users are output by the first messaging application to the other user devices to be displayed.\n In further examples, a first user device joins a chat conversation provided, at least in part, by a first messaging application that executes at least in part on the first user device, and a first chat interface is configured to receive first\nuser input from the first user and display messages originating from one or more other chat users of one or more other chat devices participating in the chat conversation over a network.  Each chat user is associated with a chat identity displayed in the\nfirst chat interface.  Second user input is received from the first user to the first chat interface and, in response to the second user input, a first embedded application is initiated in association with the first chat interface.  The first embedded\napplication provides output information in an embedded interface that is displayed by the first user device such that least a portion of the first chat interface is visible.  The chat identity associated with the first user is provided from the first\nmessaging application to the first embedded application, where the first user is designated as a member user of an embedded session associated with the first embedded application, and the first embedded application is updated based on third user input\nreceived from the first user to the embedded interface.  In some examples, the other chat users include a bot executing on at least one of the first user device and a server device connected to the first user device by a network.  Based on fourth user\ninput received from the first user in the first chat interface, the bot causes a display of content data in the embedded interface.\n In further examples, first user input is received from a first user to a first embedded interface displayed in association with a first chat interface by a first embedded application that executes in association with a first messaging\napplication on a first user device.  The first user input is received by the first embedded application, the first chat interface is displayed by the first messaging application executing at least in part on the first user device, and the first chat\ninterface is configured to receive second user input from the first user and display messages received from one or more other user devices participating in a chat conversation over a network.  Each of the other user devices is associated with a\nrespective chat user, each chat user associated with a chat identity displayed in the first chat interface.  Other user input is received from one or more member users of one or more member devices of the one or more other user devices, where the member\ndevices are included in an embedded session associated with the first embedded application.  One or more states of the embedded application are updated based on the first user input and based on the other user input, where the updating includes assigning\na respective user status to each of the member users.  Information indicating the respective user status of the member users is provided to the first messaging application to be displayed by the other user devices participating in the chat conversation.\n FIG. 12 is a block diagram of an example device 1200 which may be used to implement one or more features described herein.  In one example, device 1200 may be used to implement a client device, e.g., any of client devices 115 shown in FIG. 1. \nAlternatively, device 1200 can implement a server device, e.g., messaging server 101 and/or other servers of FIG. 1.  Device 1200 can be any suitable computer system, server, or other electronic or hardware device as described above.\n One or more methods described herein can be run in a standalone program that can be run on any type of computing device, a program run on a web browser, a mobile application (\"app\") run on a mobile computing device (e.g., cell phone, smart\nphone, tablet computer, wearable device (wristwatch, armband, jewelry, headwear, virtual reality goggles or glasses, augmented reality goggles or glasses, etc.), laptop computer, etc.).  In one example, a client/server architecture can be used, e.g., a\nmobile computing device (as a client device) sends user input data to a server device and receives from the server the final output data for output (e.g., for display).  In another example, all computations can be performed within the mobile app (and/or\nother apps) on the mobile computing device.  In another example, computations can be split between the mobile computing device and one or more server devices.\n In some implementations, device 1200 includes a processor 1202, a memory 1204, and input/output (I/O) interface 1206.  Processor 1202 can be one or more processors and/or processing circuits to execute program code and control basic operations\nof the device 1200.  A \"processor\" includes any suitable hardware and/or software system, mechanism or component that processes data, signals or other information.  A processor may include a system with a general-purpose central processing unit (CPU),\nmultiple processing units, dedicated circuitry for achieving functionality, or other systems.  Processing need not be limited to a particular geographic location, or have temporal limitations.  For example, a processor may perform its functions in\n\"real-time,\" \"offline,\" in a \"batch mode,\" etc. Portions of processing may be performed at different times and at different locations, by different (or the same) processing systems.  A computer may be any processor in communication with a memory.\n Memory 1204 is typically provided in device 1200 for access by the processor 1202, and may be any suitable processor-readable storage medium, such as random access memory (RAM), read-only memory (ROM), Electrical Erasable Read-only Memory\n(EEPROM), Flash memory, etc., suitable for storing instructions for execution by the processor, and located separate from processor 1202 and/or integrated therewith.  Memory 1204 can store software operating on the server device 1200 by the processor\n1202, including an operating system 1208, messaging application 1210 and other applications (or engines) 1212 such as a data display engine, web hosting engine, image display engine, notification engine, social networking engine, etc. In some\nimplementations, the messaging application 1210 can include instructions that enable processor 1202 to perform functions described herein, e.g., some or all of the methods of FIGS. 2-7.  For example, messaging application 1210 can communicate with other\napplications and devices as described herein.  In some implementations, messaging application 1210 may include or be associated with one or more embedded applications 1216, which can provide features as described herein and can communicate with the\nmessaging application 1210 to send and receive information from other applications 1212 and devices connected via a network.  In some implementations, messaging application 1210 can include one or more modules, such as user interaction module 1218\nmanaging a chat interface and/or other user interfaces, and/or these modules can be implemented in other applications or devices in communication with the device 1200.  The user interaction module 1218 and/or one or more of the other applications 1212\ncan, for example, provide a displayed user interface responsive to user input to display selectable options or controls, and data based on selected options.\n Other applications 1212 can include, e.g., image editing applications, media display applications, communication applications, web hosting engine or application, etc. One or more methods disclosed herein can operate in several environments and\nplatforms, e.g., as a stand-alone computer program that can run on any type of computing device, as a web application having web pages, as a mobile application (\"app\") run on a mobile computing device, etc.\n In various implementations, the other applications 1212 can include a machine-learning application 1230 which can be used in various implementations described herein.  For example, machine-learning application 1230 may utilize Bayesian\nclassifiers, support vector machines, neural networks, or other learning techniques.  In some implementations, machine-learning application 1230 may include a trained model, an inference engine, and data.  In some implementations, the data may include\ntraining data, e.g., data used to generate the trained model.  For example, the training data may include any type of data such as text, images, audio, video, etc. Training data may be obtained from any source, e.g., a data repository specifically marked\nfor training, data for which permission is provided for use as training data for machine-learning, etc. In implementations where one or more users permit use of their respective user data to train the trained model, training data may include such user\ndata.  In implementations where users permit use of their respective user data, the data may include permitted data such as images (e.g., photos or other user-generated images), communications (e.g., e-mail; chat data such as text messages, voice, video,\netc.), documents (e.g., spreadsheets, text documents, presentations, etc.)\n In some implementations, the data may include collected data such as map data, image data (e.g., satellite imagery, overhead imagery, etc.), game data, etc. In some implementations, training data may include synthetic data generated for the\npurpose of training, such as data that is not based on user input or activity in the context that is being trained, e.g., data generated from simulated conversations, computer-generated images, etc. In some implementations, machine-learning application\n1230 excludes the data.  For example, in these implementations, the trained model may be generated, e.g., on a different device, and be provided as part of machine-learning application 1230.  In various implementations, the trained model may be provided\nas a data file that includes a model structure or form, and associated weights.  The inference engine may read the data file for the trained model and implement a neural network with node connectivity, layers, and weights based on the model structure or\nform specified in the trained model.\n The machine-learning application also includes a trained model.  In some implementations, the trained model may include one or more model forms or structures.  For example, model forms or structures can include any type of neural-network, such\nas a linear network, a deep neural network that implements a plurality of layers (e.g., \"hidden layers\" between an input layer and an output layer, with each layer being a linear network), a convolutional neural network (e.g., a network that splits or\npartitions input data into multiple parts or tiles, processes each tile separately using one or more neural-network layers, and aggregates the results from the processing of each tile), a sequence-to-sequence neural network (e.g., a network that takes as\ninput sequential data, such as words in a sentence (e.g., in a chat message), frames in a video, etc. and produces as output a result sequence), etc. The model form or structure may specify connectivity between various nodes and organization of nodes\ninto layers.  For example, nodes of a first layer (e.g., input layer) may receive data as input data or application data.  Such data can include, for example, one or more pixels per node, e.g., when the trained model is used for image analysis. \nSubsequent intermediate layers may receive as input output of nodes of a previous layer per the connectivity specified in the model form or structure.  These layers may also be referred to as hidden layers.  A final layer (e.g., output layer) produces an\noutput of machine-learning application 1230.  For example, the output may be a set of labels for an image, a representation of the image that permits comparison of the image to other images (e.g., a feature vector for the image), an output sentence in\nresponse to an input sentence, one or more categories for the input data, etc. depending on the specific trained model.  In some implementations, model form or structure also specifies a number and/or type of nodes in each layer.\n In different implementations, the trained model can include a plurality of nodes, arranged into layers per the model structure or form.  In some implementations, the nodes may be computational nodes with no memory, e.g., configured to process\none unit of input to produce one unit of output.  Computation performed by a node may include, for example, multiplying each of a plurality of node inputs by a weight, obtaining a weighted sum, and adjusting the weighted sum with a bias or intercept\nvalue to produce the node output.  In some implementations, the computation performed by a node may also include applying a step/activation function to the adjusted weighted sum.  In some implementations, the step/activation function may be a nonlinear\nfunction.  In various implementations, such computation may include operations such as matrix multiplication.  In some implementations, computations by the plurality of nodes may be performed in parallel, e.g., using multiple processors cores of a\nmulticore processor, using individual processing units of a GPU, or special-purpose neural circuitry.  In some implementations, nodes may include memory, e.g., may be able to store and use one or more earlier inputs in processing a subsequent input.  For\nexample, nodes with memory may include long short-term memory (LSTM) nodes.  LSTM nodes may use the memory to maintain \"state\" that permits the node to act like a finite state machine (FSM).  Models with such nodes may be useful in processing sequential\ndata, e.g., words in a sentence or a paragraph, frames in a video, speech or other audio, etc.\n In some implementations, the trained model may include embeddings or weights for individual nodes.  For example, a model may be initiated as a plurality of nodes organized into layers as specified by the model form or structure.  At\ninitialization, a respective weight may be applied to a connection between each pair of nodes that are connected per the model form, e.g., nodes in successive layers of the neural network.  For example, the respective weights may be randomly assigned, or\ninitialized to default values.  The model may then be trained, e.g., using the data described above, to produce a result.\n For example, training may include applying supervised learning techniques.  In supervised learning, the training data can include a plurality of inputs (e.g., a set of images) and a corresponding expected output for each input (e.g., one or more\nlabels for each image).  Based on a comparison of the output of the model with the expected output, values of the weights are automatically adjusted, e.g., in a manner that increases a probability that the model produces the expected output when provided\nsimilar input.\n In some implementations, training may include applying unsupervised learning techniques.  In unsupervised learning, only input data may be provided and the model may be trained to differentiate data, e.g., to cluster input data into a plurality\nof groups, where each group includes input data that are similar in some manner.  For example, the model may be trained to differentiate images such that the model distinguishes abstract images (e.g., synthetic images, human-drawn images, etc.) from\nnatural images (e.g., photos).\n In another example, a model trained using unsupervised learning may cluster words based on the use of the words in input sentences.  In some implementations, unsupervised learning may be used to produce knowledge representations, e.g., that may\nbe used by machine-learning application 1230.  In various implementations, a trained model includes a set of weights, or embeddings, corresponding to the model structure.  In implementations where the data is omitted, machine-learning application 1230\nmay include the trained model that is based on prior training, e.g., by a developer of machine-learning application 1230, by a third-party, etc. In some implementations, the trained model may include a set of weights that are fixed, e.g., downloaded from\na server that provides the weights.\n Machine-learning application 1230 also may include an inference engine.  The inference engine is configured to apply the trained model to data, such as application data, to provide an inference.  In some implementations, inference engine may\ninclude software code to be executed by processor 1202.  In some implementations, the inference engine may specify circuit configuration (e.g., for a programmable processor, for a field programmable gate array (FPGA), etc.) enabling processor 1202 to\napply the trained model.  In some implementations, the inference engine may include software instructions, hardware instructions, or a combination.  In some implementations, the inference engine may offer an application programming interface (API) that\ncan be used by operating system 1208 and/or other applications 1212 to invoke the inference engine, e.g., to apply the trained model to application data to generate an inference.\n Machine-learning application 1230 may provide several technical advantages.  For example, when the trained model is generated based on unsupervised learning, the trained model can be applied by inference engine to produce knowledge\nrepresentations (e.g., numeric representations) from input data, e.g., application data.  For example, a model trained for image analysis may produce representations of images that have a smaller data size (e.g., 1 KB) than input images (e.g., 10 MB). \nIn some implementations, such representations may be helpful to reduce processing cost (e.g., computational cost, memory usage, etc.) to generate an output (e.g., a label, a classification, a sentence descriptive of the image, etc.).  In some\nimplementations, such representations may be provided as input to a different machine-learning application that produces output from the output of the inference engine.  In some implementations, knowledge representations generated by machine-learning\napplication 1230 may be provided to a different device that conducts further processing, e.g., over a network.  In such implementations, providing the knowledge representations rather than the images may provide a technical benefit, e.g., enable faster\ndata transmission with reduced cost.  In another example, a model trained for clustering documents may produce document clusters from input documents.  The document clusters may be suitable for further processing (e.g., determining whether a document is\nrelated to a topic, determining a classification category for the document, etc.) without the need to access the original document, and therefore, save computational cost.\n In some implementations, machine-learning application 1230 may be implemented in an offline manner.  In these implementations, the trained model may be generated in a first stage, and provided as part of machine-learning application 1230.  In\nsome implementations, machine-learning application 1230 may be implemented in an online manner.  For example, in such implementations, an application that invokes machine-learning application 1230 (e.g., operating system 1208, one or more of other\napplications 1212) may utilize an inference produced by machine-learning application 1230, e.g., provide the inference to a user, and may generate system logs (e.g., if permitted by the user, an action taken by the user based on the inference; or if\nutilized as input for further processing, a result of the further processing).  System logs may be produced periodically, e.g., hourly, monthly, quarterly, etc. and may be used, with user permission, to update the trained model, e.g., to update\nembeddings for the trained model.\n In some implementations, machine-learning application 1230 may be implemented in a manner that can adapt to particular configuration of device 1200 on which machine-learning application 1230 is executed.  For example, machine-learning\napplication 1230 may determine a computational graph that utilizes available computational resources, e.g., processor 1202.  For example, if machine-learning application 1230 is implemented as a distributed application on multiple devices,\nmachine-learning application 1230 may determine computations to be carried out on individual devices in a manner that optimizes computation.  In another example, machine-learning application 1230 may determine that processor 1202 includes a GPU with a\nparticular number of GPU cores (e.g., 1,000) and implement the inference engine accordingly (e.g., as 1,000 individual processes or threads).\n In some implementations, machine-learning application 1230 may implement an ensemble of trained models.  For example, the trained model may include a plurality of trained models that are each applicable to same input data.  In these\nimplementations, machine-learning application 1230 may choose a particular trained model, e.g., based on available computational resources, success rate with prior inferences, etc. In some implementations, machine-learning application 1230 may execute\nthe inference engine such that a plurality of trained models is applied.  In these implementations, machine-learning application 1230 may combine outputs from applying individual models, e.g., using a voting-technique that scores individual outputs from\napplying each trained model, or by choosing one or more particular outputs.  Further, in these implementations, machine-learning application may apply a time threshold for applying individual trained models (e.g., 0.5 ms) and utilize only those\nindividual outputs that are available within the time threshold.  Outputs that are not received within the time threshold may not be utilized, e.g., discarded.  For example, such approaches may be suitable when there is a time limit specified while\ninvoking machine-learning application 1230, e.g., by operating system 1208 or one or more applications 1212.\n In different implementations, machine-learning application 1230 can produce different types of outputs.  For example, machine-learning application 1230 can provide representations or clusters (e.g., numeric representations of input data), labels\n(e.g., for input data that includes images, documents, etc.), phrases or sentences (e.g., descriptive of an image or video, suitable for use as a response to an input sentence, etc.), images (e.g., generated by machine-learning application 1230 in\nresponse to input), audio or video (e.g., in response an input video, machine-learning application 1230 may produce an output video with a particular effect applied, e.g., rendered in a comic-book or particular artist's style, when the trained model is\ntrained using training data from the comic book or particular artist, etc. In some implementations, machine-learning application 1230 may produce an output based on a format specified by an invoking application, e.g. operating system 1208 or one or more\napplications 1212.  In some implementations, an invoking application may be another machine-learning application.  For example, such configurations may be used in generative adversarial networks, where an invoking machine-learning application is trained\nusing output from machine-learning application 1230 and vice-versa.\n Any of software in memory 1204 can alternatively be stored on any other suitable storage location or computer-readable medium.  In addition, memory 1204 (and/or other connected storage device(s)) can store one or more messages, one or more\ntaxonomies, electronic encyclopedia, dictionaries, thesauruses, message data, grammars, user preferences, and/or other instructions and data used in the features described herein.  Memory 1204 and any other type of storage (magnetic disk, optical disk,\nmagnetic tape, or other tangible media) can be considered \"storage\" or \"storage devices.\"\n I/O interface 1206 can provide functions to enable interfacing the server device 1200 with other systems and devices.  Interfaced devices can be included as part of the device 1200 or can be separate and communicate with the device 1200.  For\nexample, network communication devices, storage devices (e.g., memory and/or database 106), and input/output devices can communicate via I/O interface 1206.  In some implementations, the I/O interface can connect to interface devices such as input\ndevices (keyboard, pointing device, touchscreen, microphone, camera, scanner, sensors, etc.) and/or output devices (display device, speaker devices, printer, motor, etc.).\n Some examples of interfaced devices that can connect to I/O interface 1206 can include one or more display devices 1220 that can be used to display content, e.g., images, video, and/or a user interface of an output application as described\nherein.  Display device 1220 can be connected to device 1200 via local connections (e.g., display bus) and/or via networked connections and can be any suitable display device.  Display device 1220 can include any suitable display device such as an LCD,\nLED, or plasma display screen, CRT, television, monitor, touchscreen, 3-D display screen, or other visual display device.  For example, display device 1220 can be a flat display screen provided on a mobile device, multiple display screens provided in a\ngoggles device, or a monitor screen for a computer device.\n The I/O interface 1206 can interface to other input and output devices.  Some examples include one or more cameras which can capture images.  Some implementations can provide a microphone for capturing sound (e.g., as a part of captured images,\nvoice commands, etc.), audio speaker devices for outputting sound, or other input and output devices.\n For ease of illustration, FIG. 12 shows one block for each of processor 1202, memory 1204, I/O interface 1206, and software blocks 1208, 1210, and 1212.  These blocks may represent one or more processors or processing circuitries, operating\nsystems, memories, I/O interfaces, applications, and/or software modules.  In other implementations, device 1200 may not have all of the components shown and/or may have other elements including other types of elements instead of, or in addition to,\nthose shown herein.  While some components are described as performing blocks and operations as described in some implementations herein, any suitable component or combination of components of environment 100, device 1200, similar systems, or any\nsuitable processor or processors associated with such a system, may perform the blocks and operations described.\n Methods described herein can be implemented by computer program instructions or code, which can be executed on a computer.  For example, the code can be implemented by one or more digital processors (e.g., microprocessors or other processing\ncircuitry) and can be stored on a computer program product including a non-transitory computer readable medium (e.g., storage medium), such as a magnetic, optical, electromagnetic, or semiconductor storage medium, including semiconductor or solid state\nmemory, magnetic tape, a removable computer diskette, a random access memory (RAM), a read-only memory (ROM), flash memory, a rigid magnetic disk, an optical disk, a solid-state memory drive, etc. The program instructions can also be contained in, and\nprovided as, an electronic signal, for example in the form of software as a service (SaaS) delivered from a server (e.g., a distributed system and/or a cloud computing system).  Alternatively, one or more methods can be implemented in hardware (logic\ngates, etc.), or in a combination of hardware and software.  Example hardware can be programmable processors (e.g. Field-Programmable Gate Array (FPGA), Complex Programmable Logic Device), general purpose processors, graphics processors, Application\nSpecific Integrated Circuits (ASICs), and the like.  One or more methods can be performed as part of or component of an application running on the system, or as an application or software running in conjunction with other applications and operating\nsystem.\n Although the description has been described with respect to particular implementations, these particular implementations are merely illustrative, and not restrictive.  Concepts illustrated in the examples may be applied to other examples and\nimplementations.\n In situations in which certain implementations discussed herein may collect or use personal information about users (e.g., user data, information about a user's social network, user's location and time, user's biometric information, user's\nactivities and demographic information), users are provided with one or more opportunities to control whether the personal information is collected, whether the personal information is stored, whether the personal information is used, and how the\ninformation is collected about the user, stored and used.  That is, the systems and methods discussed herein collect, store and/or use user personal information specifically upon receiving explicit authorization from the relevant users to do so.  In\naddition, certain data may be treated in one or more ways before it is stored or used so that personally identifiable information is removed.  As one example, a user's identity may be treated so that no personally identifiable information can be\ndetermined.  As another example, a user's geographic location may be generalized to a larger region so that the user's particular location cannot be determined.\n Note that the functional blocks, operations, features, methods, devices, and systems described in the present disclosure may be integrated or divided into different combinations of systems, devices, and functional blocks as would be known to\nthose skilled in the art.  Any suitable programming language and programming techniques may be used to implement the routines of particular implementations.  Different programming techniques may be employed such as procedural or object-oriented.  The\nroutines may execute on a single processing device or multiple processors.  Although the steps, operations, or computations may be presented in a specific order, the order may be changed in different particular implementations.  In some implementations,\nmultiple steps or operations shown as sequential in this specification may be performed at the same time.", "application_number": "15624638", "abstract": " Implementations relate to suggested items for use with embedded\n     applications in chat conversations. In some implementations, a method\n     includes causing a chat interface to be displayed by a first user device,\n     the chat interface generated by a messaging application. The chat\n     interface is configured to display, in a chat conversation, messages\n     provided by user devices participating in the chat conversation over a\n     network. An embedded interface is associated with the chat interface and\n     displayed by an embedded application associated with the messaging\n     application. A suggestion event is determined to occur in association\n     with the embedded application based on received data that indicates that\n     a particular event has occurred at one or more other user devices\n     participating in the chat conversation. Suggested response items are\n     obtained based on the suggestion event, and the suggested response items\n     are displayed by the first user device.\n", "citations": ["6092102", "7603413", "8391618", "8423577", "8515958", "8589407", "8650210", "8688698", "8825474", "8938669", "8996639", "9191786", "9213941", "9230241", "9674120", "9715496", "9805371", "9807037", "9817813", "10146748", "20030105589", "20030182374", "20060029106", "20060156209", "20070030364", "20070094217", "20070162942", "20070244980", "20080120371", "20090076795", "20090119584", "20090282114", "20100228590", "20100260426", "20110074685", "20110107223", "20110164163", "20110252108", "20120030289", "20120033876", "20120041941", "20120042036", "20120131520", "20120224743", "20120245944", "20130050507", "20130061148", "20130073366", "20130260727", "20130262574", "20130346235", "20140004889", "20140035846", "20140047413", "20140067371", "20140088954", "20140108562", "20140163954", "20140164506", "20140171133", "20140189027", "20140189538", "20140201675", "20140228009", "20140237057", "20140317030", "20140337438", "20140372349", "20150006143", "20150032724", "20150058720", "20150095855", "20150100537", "20150171133", "20150178371", "20150178388", "20150207765", "20150227797", "20150248411", "20150250936", "20150286371", "20150288633", "20150302301", "20160037311", "20160042252", "20160043974", "20160072737", "20160140447", "20160140477", "20160162791", "20160179816", "20160210279", "20160224524", "20160226804", "20160234553", "20160283454", "20160342895", "20160350304", "20160378080", "20170075878", "20170093769", "20170098122", "20170134316", "20170142046", "20170149703", "20170153792", "20170171117", "20170180276", "20170180294", "20170187654", "20170250930", "20170250935", "20170250936", "20170293834", "20170308589", "20170324868", "20170344224", "20170357442", "20180004397", "20180005272", "20180012231", "20180013699", "20180060705", "20180083894", "20180083898", "20180083901", "20180090135", "20180109526", "20180137097", "20180196854", "20180210874", "20180293601", "20180309706", "20180316637", "20180322403", "20180336226", "20180336415", "20180367483", "20180367484", "20180373683"], "related": []}, {"id": "20190034068", "patent_code": "10318140", "patent_name": "Third part action triggers", "year": "2019", "inventor_and_country_data": " Inventors: \nHurley; Fergus Gerard (San Francisco, CA), Dua; Robin (San Francisco, CA)  ", "description": "BACKGROUND\n The advent of cloud based services, search engines, mobile applications, and location-aware devices have drastically expanded the utility of mobile user devices over the last decade.  Many such user devices now provide context-aware services and\napplications in addition to voice and data access.  Furthermore, with the recent advances in processing systems, many previously disparate applications and services are now able to work together to provide enhanced capabilities and user experiences.\n Many of these application services available to users are instantiated by use of command inputs.  One such service is creating action triggers.  For example, a user may speak (or type) the input [remind me to buy milk this evening] into a\nsmartphone, and the smartphone, using a command parsing application (or, alternatively, communicating with a command parsing service) will invoke an action trigger process that may solicit additional information from the user.  Such information may be a\ntime, if the user desires for the action trigger to be performed at a certain time, and/or a location, if the user desires for the action trigger to be performed when the user arrives at the location.  While the setting of such action triggers are very\nuseful in a relatively fluid user experience, the users often forget to do the things they wanted to do because they cannot setup action triggers that are based on the context that they need to be in to be able to complete the task at hand.\nSUMMARY\n This specification relates to action triggers, which are associations of user defined events and actions, that are generated by use of an action trigger prompt rendered with a resource.  In an example implementation, application pages and\nweb-based resources include an action trigger prompt though which a user may set up a notification that is specific to the user.  The notification is subject to parameter value criteria set by the user.  The publishers of the applications and web-based\nresources, which are generally referred to herein as \"third parties,\" may then provide a notification to the user once the criteria for the notification are met.  Alternatively, the third parties may provide data to a system that, independent of the\nthird party, manages the action triggers for multiple different third parties.  The third parties may provide the data to the system automatically, or the data may be requested, searched, or otherwise gathered by the system.\n In additional implementations, the action trigger prompts and the action triggers created by use of the action trigger prompts may be implemented entirely independent of third parties.  For example, the systems described in this specification\nmay be configured to identify the parameters rendered on a resource, and for at least one of the parameter values displayed on the resource, render an action trigger prompt with the rendered resource.  This frees the third parties of back-end system\ndevelopment and resource layout design and coding efforts.  However, the developers of the third-party content will still benefit for increased user engagement, as described in more detail below.\n The action that can be taken in response to an event occurring can be user defined.  In some implementations, the action, when taken, may or may not enable the user to take subsequent actions.  In the case of the former, for example, the action\nmay be a notification that an event has occurred, or, alternatively, the action may be performed by the user device, or by some other device or party, without notification to the user.  In the case of the latter, for example, the action may be a\nconfirmation prompt to take a subsequent action.  For example, the user may have set an action trigger with an event of Company X stock falling below $620 a share, and may have set the action to be a presentation of a confirmation prompt to purchase 10\nshares of Company X stock when the event occurs.  The user may then accept or reject the confirmation when presented with the confirmation prompt when the action occurs.  Should the user accept the confirmation, a subsequent action that results in the\npurchase of the stock can be performed (or, alternatively, the user may be presented with a resource through which the user may purchase the stock).\n In some implementations, the notification is provided to a software assistant for the user.  For example, the third party may have a proprietary data source which when a user defined event is met the third party system notifies the personal\nassistant application.  Alternatively, the personal assistant application may monitor the data source, or content published by a resource, and detects the occurrence of the event.  The software assistant then determines the importance of this\nnotification and surfaces it to the user in the best determined way, e.g., according to one or more conditions that are either automatically determined by the assistant or set by the user.\n The notification may be informative only, or may alternatively be interactive.  In the latter case, if the user interacts with the notification, the user may be linked back to the initial application or web page from which the user set the\nnotification, or some other resource that permits the user to execute an action related to the notification.  This allows for a very efficient engagement model with the user and that brings the user to the third party service at a desired time.\n Implementations of the subject matter described below allows for an intuitive, more accurate, and more tailorable user experience when creating action triggers.  The creation of an action trigger that is contextually relevant to content that the\nuser is viewing allows the user to set a reminder or action and be alerted of events relevant to that content.  This provides a very fluid and intuitive interaction model for the user that makes it easier for the user to set up an alert or action.\n For example, a user may be viewing a resource through which tickets for concerts may be purchased.  The user may search for a particular artist and may learn that the artist is not currently on tour.  By use of the action trigger, however, the\nuser may set up a reminder to be notified when new tour dates are published for the artist, or to even automatically purchase tickets for a particular tour venue when the tickets become available This reduces the necessity for a user to keep a particular\nschedule or check the resource frequently, and can facilitate notifying the user of user defined events or performing user defined actions when the user defined event has occurred.  The notifications may be made in real-time to better equip the user to\nrespond in a timely and efficient manner.  Additionally, in some implementations, the user can select for the user defined action to be completed once the user defined event has been determined to occur.\n A variety of notification types may be used, including a mobile operating system (OS) push notification, text message, email, a web or digital assistant alert, or other form of notification.  As described above, in the case of a digital\nassistant alert, the digital assistant may determine the proper time, place, and method to notify the user or take action on the user's behalf.  For example, if the user is in a business meeting, the digital assistant may wait to notify the user until\nthe user is out of the meeting, or after the user's work day is completed.\n The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below.  Other features, aspects, and advantages of the subject matter will become\napparent from the description, the drawings, and the claims. BRIEF DESCRIPTION OF THE DRAWINGS\n FIG. 1 is a block diagram of an example environment in which command inputs are processed for action triggers.\n FIG. 2A is an illustration of an interface of a resource in a portion of a web page.\n FIG. 2B is an illustration of an interface of a resource in a portion of a web page after selecting the action trigger prompt.\n FIG. 2C is an illustration of an interface of a resource in a portion of a web page in selecting the second parameter value in the action trigger menu.\n FIG. 2D is an illustration of an interface at a user.\n FIG. 2E is a flow diagram of an example process for creating an action trigger and performing an action based on the action trigger.\n FIG. 2F is an illustration of an action selection menu.\n FIG. 3 is a flow diagram of another example process for creating an action trigger.\n FIG. 4A is an illustration of a user interface at a user device in which an action trigger prompt is provided along with a resource.\n FIG. 4B is an illustration of a user interface where an action trigger menu is provided.\n FIG. 4C is an illustration of a user interface providing a notification that the value of the first parameter is the parameter value of value input.\n FIG. 5 is a block diagram of an example mobile computing device.\n Like reference numbers and designations in the various drawings indicate like elements.\nDETAILED DESCRIPTION\n An action trigger processing system facilitates action triggers including user defined events.  In operation, a resource, e.g., a web page resource or a particular application page in an application, includes instructions that cause the user\ndevice to display an action trigger prompt that is rendered in an initial state.  The resource is provided to the user device, and the resource and action trigger prompt are rendered at the user device.  An action trigger menu that includes an input for\na user defined event may be provided if the action trigger prompt is selected at the user device.  The action trigger menu may also define one or more actions that can be taken when the event occurs.  The association of the action and the events is\nreferred to as an \"action trigger,\" and the action trigger may be stored in an action trigger data store.\n The action trigger processing system obtains updates and information associated with resources from event data (e.g., by searching or receiving data of the resources), and when the action trigger processing system determines that the user\ndefined event of the action trigger has occurred, the action trigger processor may perform the action when the event occurs.  The action may be an action that is performed automatically with or without a notification to the user, or may be the providing\nof a notification to the user, or may be the providing of a confirmation prompt to take a subsequent action.  In some implementations, the user defined action for the action trigger may be initiated when the action trigger processing system determines\nthe user defined event has occurred.  In some implementations, an automatic operation may be completed after the user defined event has occurred (e.g., automatic ticket purchase when concert tickets go on sale).  Further, in some implementations the\naction trigger processing system may receive a selection by the user of one or more trigger conditions that indicate a condition to be satisfied in determining that the user defined event has occurred.\n In some implementations, code for the action trigger prompt may be included in the web page resource or application page.  In other implementations, the code may be inserted into the web page resource or application page.  In still other\nimplementations the action trigger prompt may be rendered as an overlay to the web page resource or application page.\n The action trigger processing system can be implemented in the user device, or in a computer system separate from user device, such as a server system.  In the case of the latter, the server system receives input from the user device and sends\ndata to the user device for processing.  These features and additional features are described in more detail below.\n FIG. 1 is a block diagram of an environment 100 in which command inputs are processed for action triggers including user defined events.  A computer network 102, such as the Internet, or a combination thereof, provides for data communication\nbetween electronic devices and systems.  The computer network 102 may also include, or be in data communication with, one or more wireless networks 103 by means of one or more gateways.\n A resource publisher 104 includes one or more resources 105.  There may be one or more resource publisher 104.  The resource publisher 104 may be a website publisher, an application publisher, among others.  In implementations where the resource\npublisher 104 is a website publisher, the one or more resources 105 are associated with a domain and hosted by one or more servers in one or more locations.  Generally, a resource publisher website is a collection of web pages formatted in hypertext\nmarkup language (HTML) that can contain text, images, multimedia content, and programming elements.  Each website is maintained by a content publisher, which is an entity that controls, manages and/or owns the website.  A web page resource is any data\nthat can be provided by a publisher resource 104 over the network 102 and that has a resource address, e.g., a uniform resource locator (URL).  Web resources may be HTML pages, images files, video files, audio files, and feed sources, to name just a few. The resources may include embedded information, e.g., meta information and hyperlinks, and/or embedded instructions, e.g., client-side scripts.  More generally, a \"resource\" is anything identifiable over a network.\n A resource, as used in this specification, may also be a particular application page of a stand-alone application that is run separate from a web browser.  For example, a ticket selling agency may publish an application that sells tickets to\nvarious events.  The application may have various pages, such as a page for musical artists, another page for sporting events, and another page for plays, for example.\n User device 106 is an electronic device that is under the control of a user and is capable of requesting and receiving resources over the network 102, also capable of performing other actions.  Example user devices 106 include personal\ncomputers, mobile communication devices, wearable devices, and other devices that can send and receive data over the network 102.  In the example of FIG. 1, the user device 106 is a smartphone.  An example smartphone is described with reference to FIG. 5\nbelow.  The user device 106 may communicate over the networks 102 and 103 by means of wired and wireless connections with the networks 102 and 103, respectively.  The user device 106 may also be a device that accesses the network 102 by means of an\nintermediate device and a short range wireless transceiver, such as a WiFi, Bluetooth, etc. As described with reference to FIG. 5, a user device may be able to perform a set of device actions for various programs and capabilities.\n The user device 106 is associated with a user account, such as an account hosted by a cloud service provider 112 that provides multiple services.  These services may include search (web, intranet, documents, and applications, among others), web\nmail, calendar, social networking, messaging, documents storage and editing, an electronic assistant service, etc. The account data 114 may store data specific to the account of the user device 106.  Further, although only one user device 106 is shown in\nFIG. 1, a plurality of user devices 106 may be included.\n In some implementations, the user may invoke actions and events that cause the actions to be performed by means of an action trigger prompt provided with resources.  The generation of action triggers, which are an association of user defined\nevents and user defined actions, and the processing of such items are described in more detail below.  An action trigger processing system 120 receives the action triggers, either from the third party publishers or user devices, and stores the action\ntriggers in an action trigger data store 126.  While the action trigger processing system 120 is shown as a separate entity in FIG. 1, the action trigger processing system 120 can be implemented in the cloud service provider 112, or alternatively as a\nseparate system by each third party.  In certain embodiments, action trigger processing system 120, or subparts thereof, may form a portion of or be stored on user device 106.\n Action triggers that are stored in the action trigger data store are the association of actions and events.  The action trigger processor 124 accesses event data 128 to determine which events are met, and whether to perform the actions\nassociated with the events that are determined to be met.  As used herein, event data 128 is any data that can be used to determine whether an event has occurred.  The event data may be data that is preprocessed and sent to the action trigger processor,\nor may be other data that can be searched by or on behalf of the action trigger processor 124.  For example, the event data 128 may be data that is provided to the action trigger processing system 120 by the third parties, or may be data that is\nrequested by the action trigger processing system periodically, or may even be an indexed corpus of resources, knowledge graph, or any other data source that may be searched for parameter values to determine whether an event has occurred.\n The action trigger processor 124 will store the action trigger in an action trigger data store 126.  There may be a plurality of action triggers AT1, AT2, .  . . ATn stored in action trigger data 126.  A group of action triggers (e.g., those\ncreated or previously implemented by the user) may be viewed at the user device 106, and the user may make changes (e.g., add, delete, modify) at the user device 106 to the action triggers at any time.  Each of the plurality of action triggers may have\none or more user defined events E1, E2, .  . . En associated with an action A. Upon the occurrence of a user defined event, the action trigger processor 124 may cause the action to be performed.  The action may be one or more of: the presentation of a\nnotification that the event occurred; an action that is performed partly or fully automatically and separate from the user device; or the presentation of a prompt to perform another action.  For example, based on the example above, the user selects or\nspecifies a user defined action, such as \"buy tickets,\" to take place when the user defined event occurs.\n Additionally, in some implementations, a user may provide trigger conditions Tc1, Tc2, .  . . Tcn associated with the one or more action triggers.  Generally, the trigger conditions are additional conditions or events that are not available in\nthe action trigger prompt menu but that a user can associate with an action.  A trigger condition specifies a condition to be satisfied in determining that the user defined event indicated in the action trigger has occurred, or that may be required to be\nsatisfied before the user is notified of the events being met.  For example, suppose a user has set an action trigger to be notified when tickets become available for an artist.\n The user may also specify that notifications should be provided only during appropriate times during which the user is able to review and respond to the notifications.  For example, the user may also set a condition that the user is not to be\nnotified of tickets coming available based on the user's availability according to the user's calendar.  Thus, the presentation of the notification may be delayed when the user's calendar indicates the user is currently engaged in an appointment, e.g.,\nif the user is in a meeting as scheduled on the user's work calendar.\n Trigger conditions may be one or more time period condition, location area condition, or person proximity condition, among others.  A time period condition may be a date, a date range, a time of day, or a time of day range, among others.  For\nexample, time period condition may be a default or user set time range (e.g., 1 PM-5 PM) on a particular Saturday (e.g., the next Saturday), every Saturday, selected Saturdays, or a pattern of Saturdays (e.g., the first Saturday of every month).\n A location area condition may be an area around a particular location (e.g., house address) or type of location (e.g., grocery store, airport, hospital) that the user device 106 is to be within or near for the user defined event to be met.  For\nexample, the location area condition may be \"At Home,\" which may be defined by the user on the user device 106.  Additionally, \"near\" can be a particular distance from (e.g., feet or miles) or amount of time away by different modes of transportation\n(e.g., by car, public transportation, walking) from the identified location.  Particular location types may be specified by the user, or may be inferred.  For example, in the case of the latter, the location at which a user is most often located during\nthe evening and through mid-morning on weekdays may be inferred as \"Home,\" and the location that the user is most often located during late morning and afternoons on weekdays, if different from the inferred \"Home\" location, may be inferred as \"Work.\"\n Additionally, a trigger condition may be a person proximity condition.  A person proximity condition may be met if the user device 106 of the user is within a threshold distance from an identified user device of a particular person or group.\n In some implementations, when creating an action trigger, a user defined event may be created based on the content of the resources 105, and a parameter may be selected or input at the user device 106.  An example of this is illustrated with\nreference to FIGS. 2A-2D.  The flow through FIGS. 2A-2D is described with reference to FIG. 2E, which is a flow diagram of an example process 250 for creating an action trigger and performing an action based on the action trigger.  The process 250 may be\nimplemented in one or more computers that are used to realize the action trigger processing system 120.\n The process 250 receives data responsive to a selection of an action trigger prompt at a user device (252).  An action trigger prompt is an element, such as a user interface element in the form of a button, pull-down menu, audible cue, etc. that\nprovides a user with the option to initiate setting an action trigger.  The action trigger prompt is rendered or provided at the user device with a resource and displayed with the resource, and is rendered or provided in an initial state.  Upon selection\nor activation of the action trigger prompt the user device displays an action trigger menu that facilitates the input of multiple parameter values for an event.  This selection and menu for the action trigger prompt is shown with reference to FIGS.\n2A-2C.  In particular, FIG. 2A is an illustration of an interface 202a of a resource in a portion of a web page.  The web page may include a list and chart of multiple stocks.  The web page 202a includes an action trigger prompt 204 in an initial state\n(e.g., prior to selection) and a first parameter value 206 of a first parameter.  In this case, the first parameter is the stock price of the stock \"XCOMPANY\" and the first parameter value is $624.36.\n FIG. 2B is an illustration of an interface 202b of a resource in a portion of a web page after selecting the action trigger prompt.  Upon selection (e.g., by the pointer) of the action trigger prompt, an action trigger menu is provided that\nincludes an input for a user defined event.  The selection generates an action trigger menu that facilitates the input of multiple different possible parameter values for an event.\n For example, the user may desire an action trigger for a certain stock price going above or below a certain price, or a certain stock price staying at a value for a period of time, or a certain stock price at a particular time, among others.  In\nthe current example, action trigger is related to a certain stock going above a certain price.  The user may select a stock ticker 208, which in the current implementation is the particular stock of XCOMPANY, which may be changed by the user.  Also, a\nsecond parameter value 210 may be selected by the user.  In some implementations, a default value may be provided, which could be based on the user history, selections made by other users, or default values, among others.  In the current example, a\ndefault value of $630 is provided.\n In certain embodiments, one or more parameters are automatically selected based on context, such as the content of the web page with which the action trigger prompt 204 is associated or on which it is displayed.  For example, when viewing the\nweb page 202a showing stock prices for Company X, selection of action trigger prompt 204 displayed with the parameter value 206 automatically provides the ticker symbol XCOMPANY for Company X as a parameter.  While this parameter may be changed by the\nuser, the context of the display, user actions, environment, and so on may be used to automatically (and at least initially) set one or more parameters.  It is contemplated herein that this functionality can be provided in any or all embodiments in which\nthe action trigger prompt is included in the web page resource or application page, inserted into the web page resource or application page, and/or is rendered as an overlay to the web page resource or application page.\n In the case of an overlay, various processes can be used to detect parameters and generate an overlay.  For example, a document object model of a resource may be processed to identify predefined parameter types and values, e.g., stock symbols\nand prices, and an overlay may be rendered by use of iFrame or any other process by which content of a displayed resource may be augmented with additional content.  For application pages, a textview object may be extracted from displayed data and\nprocessed to identify the predefined parameter types and values, and an overlay can be generated by a separate process that displays additional content in an active region that defines only the action trigger in the viewport.  The overlay is generated by\nuse of instructions provided to the user device and that are separate from the instructions that are used to render the resource.\n In variations of this implementation, image processing can be used to identify textual content of an image and provide an action menu for certain textual content.  In particular, the action trigger processing system 120 may receive data\ndescribing textual content recognized for an image taken by a user device and displayed on the user device.  The user device may perform the textual recognition and generate data describing the textual content and the location of the textual content in\nthe image and send the data to the action trigger processing system 120.  Alternatively, the user device may send the image to another system that processes the image that provides the textual data and location data to the action trigger processing\nsystem 120.  A variety of text recognition processing techniques can be used.\n For example, suppose a user takes a photograph of a tour poster for an artist.  The action trigger menu will receive textual data describing content of the tour poster, and may generate an action menu near the text describing the tour.  The\naction menu may include notifications actions to inform the user of various tour dates when the tour is announced, purchase actions to purchase tickets when the tickets go on sale, and so on.  The action menu is rendered over the image displayed on the\nuser device.\n To generate the instructions for the overlay, the system 120, or, alternatively the user device 106, may process the rendered resource as described above.  Upon the detection of certain parameter values, e.g., parameter values for predefined\nparameters, such as stock symbols, prices, dates, and the like, the system 120, or the user device 106, generates the overlay instructions for processing by the user device.\n In some implementations, certain resources may be identified in advance of rendering and a list of the identified resources may be provided to the system 120 or user device 106.  Thereafter, when one of the identified resources is rendered at\nthe user device 106, the user device may notify the system 120 and the system 120 may generate the overlay instructions.  Alternatively, the user device may generate the overlay instructions.\n FIG. 2C is an illustration of an interface 202c of a resource in a portion of a web page, illustrating selecting the second parameter value 210 in the action trigger menu.  In the current implementation, upon selection (e.g., by the pointer) of\nthe second parameter value 210, a second parameter value option list 212 may be provided.  The second parameter value option list may include user-selectable options for the second parameter value.  The list of options may be based on the user history\n(e.g., selected a particular stock price or certain percentage above the current stock price in the past), context (e.g., other data being shown in interface 202c), other selections provided by other users, or a default set of values, among others.  The\nuser may select one of the second parameter value options from the option list 212, or select to \"Set Amount,\" where the user may provide the second parameter value option by inputting a value.  In the current example, the user has created a user defined\nevent with a first parameter of \"XCOMPANY stock,\" and a second parameter value of $630.\n Upon selecting 630, an action trigger is created.  The action trigger in this case is a notification that Company X stock is above $630 a share, and the event that triggers the notification is the value of Company X stock exceeding $630 a share. The action trigger is thus an association of the user defined event and the action to be taken in response to the occurrence of the user defined event.  It will be appreciated that additional fields for user-selectable parameters may be provided.  For\nexample, a field may be provided permitting a user to specify what action from multiple different actions to take when Computer Co.  stock is above $630 per share, such as place a sell order for [x] shares, or send an email to the user's stockbroker, and\nso on.\n In some implementations, actions can be automatically executed by a third party system by use of an API to the action trigger processing system 120 or personal assistant service.  The action trigger processing system 120 or personal assistant\nservice may store credentials (e.g., usernames, passwords, account information, etc.).  These third party actions may also make use of an electronic wallet.  Such third party API level integrations allow the action trigger processing system 120 or\npersonal assistant service to complete financial transactions, e.g., stock trade, ticket purchases, etc., on behalf of the user.\n The process 250 stores the data specifying the user defined event and the action associated with the user defined event as an action trigger in the action trigger data store (254).  Thereafter, the process 250 will periodically monitor for the\noccurrence of the user defined event (256), and in response to the occurrence of the user defined event, cause the action associated with the user defined event to be performed (258).\n Continuing with the example, the action trigger processor 124 will monitor the stock price of Company X. The monitoring can be done in a variety of ways.  For example, the action trigger processor 124 may request to be notified by a third party\nservice when the stock exceeds the value of $630.  Alternatively, the action trigger processor 124 may periodically search a web corpus or other data source to determine whether the event has occurred.\n FIG. 2D is an illustration of an interface 202d at a user device 106 when there has been an occurrence of the user defined event provided in FIG. 2C.  In the current implementation, after creating the action trigger and storing the association\nof the action and the event in the action data store 126, the action trigger processing system 120 may receive updates and/or check data sources associated with the user defined event and action trigger to determine if the user defined event has\noccurred.  In this example, the action trigger processor 126 will determine whether the value of the first parameter, the stock price, meets the second parameter value of $630.\n In the current example, the action trigger processing system 120 has made the determination that there has been an occurrence of the user defined event, and a notification 214 is provided to and/or by the user device 106.  Additionally, in some\nimplementations, the notification may be provided by the action trigger processor 124, and the user of the user device 106 may access information related to the user defined action.  The notifications the user receives may be notifications provided on a\nmobile device (e.g., Android notifications, In-app notifications, etc.), or notifications on a desktop (e.g., browser notifications, OS notifications, etc.).  Within the notification, a link or button to perform an operation (or user defined action)\nassociated with the action trigger may be provided.\n In the current example, the action specified is a notification action.  In some implementations, the action trigger prompt 204 may also include an action selection menu that includes a list of selectable actions.  The actions may include, for\nexample, actions that can be taken on behalf of the user with or without notification.  The actions may be contextual in that the actions available are dependent on the type of content shown in the resource.  The system 120 may have data pre-associating\nparticular actions with particular content types.  For example, in addition to the \"Let me know\" selection shown in FIGS. 2A-2C, other actions may be \"Purchase stock,\" \"Sell stock,\" etc., by means of an action selection menu 205 as shown in FIG. 2F. \nSimilarly, if the resource was a ticket purchasing resource, actions may include notifications when tickets go on sale, notifications when an artist announces tour dates, and the purchasing of tickets.\n By way of a further example, suppose FIG. 2A depicts not a web page but an image of a stock ticker report taken by the user device.  Using the image processing techniques described above, the same action trigger prompt 204 can be generated and\noverlaid on the image as shown.\n In some implementations, selecting the notification may cause the user device to display the resource from which the action trigger was set.  Alternatively, if the action was, for example, to purchase stock, selection of the notification may\ncause the user device to display an environment from which the user may purchase the stock or view confirmation that the stock was purchased.\n In some implementations, a notification may be subject to additional presentation criteria.  The presentation criteria may be selected, for example, by a software assistant, or alternatively selected by a user.  In the case of the former, with\nrespect to notifications, a software assistant may determine from a user's schedule data whether to present a notification to a user, i.e., a notification may only be presented if the user is not in a meeting, etc. In some implementations, the volatility\nof the underlying data related to the event that caused the notification may be a factor considered by the software assistant.  For example, the software assistant may override a user's meeting status and present a notification while a user is in a\nmeeting if, for example, the notification relates to an option to purchase stock and a price specified by the user.  Because the volatility of the stock price may change rapidly, the user may benefit from an immediate notification.  Conversely,\nnotifications for events for which the underlying data changes less frequently may be delayed and presented to the user at a more convenient time for the user.\n By way of another example, when a user selects the action trigger prompt on the user device 106, the action trigger menu may be provided that includes an input for a user defined event.  When a resource has multiple different parameters\navailable, the user may be able to select a first parameter.  Example parameters may be topics, titles, categories, or other information provided within the content of the resource 105.  For example, if the user is viewing a resource related to tickets\nfor Artist A's concerts and the user selects the request prompt provided on the resource, parameters associated with the resource may be concert tickets, locations of concerts, dates of a concert tour, or any other information provided within the content\nof resource 105.  Within the request prompt, the user may input or select a parameter for the user defined event.\n The action trigger processor 124 may determine a value for the parameter based on the content provided in the resource.  For example, if the user selected concert tickets as the first parameter, then the value for that parameter may indicate the\nstatus of the concert tickets, which may be \"Not available,\" or some other indication, from the content of the resource, that the tickets are not available (e.g., sold out or not yet on sale).\n After selecting or inputting a parameter, the user may input a parameter value for the indicated parameter to define the user defined event of the action trigger.  The user provided parameter value may be different from the parameter value\ndisplayed in the resource.  For example, the user may create a user defined event to be notified when the concert tickets go on sale, or are otherwise available.  Thus, the parameter value specified by the user would be \"available.\"\n Additionally, in some implementations, an action trigger may include multiple parameters.  For example, based on the example above, the action trigger may include a second parameter related to ticket sections or ticket price ranges for Artist\nA's concerts.  The second parameter may be related to, for example, Artist A's concert ticket prices, and the parameter value provided by the user may be, for example, \"less than $40.\" In the current example, the user defined event would not be met until\nthe user provided parameter values for the first parameter and the second parameter occur, e.g., tickets are available and less than $40.\n The action trigger is then created and stored in the action trigger data 126.  After the action trigger is created, the action trigger processor monitors for the occurrence of the event(s) associated with the action.  Action trigger processor\n124 may continuously or periodically (e.g., poll the event data 128 at regular or irregular intervals) search the content, or register for a notification from a third party of data satisfying the event, etc.\n As described above, event data may include resources 105.  Some content of resources 105 may be information that is generally available and accessible from the resource publisher 104 (e.g., application publisher or website publisher).  These\nresources 105 may be, for example, related to weather, news, entertainment, or any other type of resource that does not require a subscription or authentication to see the information and content of the resource 105.\n Further, some content of resource 105 may be information that is not personalized to a particular user, but the content nonetheless may require the user to sign in, or otherwise provide authorization, for access to the content of the resource\n105.  These resources 105 may be, for example, related to newspapers, journals, or blogs that require a subscription or other type of verification or login from the user in order to access all of the content of the resource.\n Moreover, some content of resource 105 may be information that is personalized, and possibly private, to a particular user.  These resources may be, for example, related to bank account information or social media websites or applications. \nThese resources 105 may be proprietary to the resource publisher 104 and can also require login or authentication information from the user to access the personalized information and content of the user.  The resource publisher 104 may, in some\nimplementations, select or enable individual users to decide if they want their personalized content associated with the resources 105 to be accessible to the action trigger processor 124.\n Content of resources 105 that require authorization may not be included in the event data 128 unless the action trigger processing system 120 has access to the authorization information for the user of the user device 106.  The authentication\ninformation for a particular user of the action trigger processing system 120 may be stored and accessed in different ways, and different amounts of authentication information may be stored for each user depending on, for example, selections indicated by\nthe user (e.g., a user may select for the action trigger processing system 120 to not store or access any authentication and login information for that particular user).  For example, a user may provide authentication information, where required, to the\naction trigger processing system 120 for the resources 105 associated with the resource publishers 104 they would like to be included in the event data 128 (i.e., the data that is accessible to the action trigger processor 124).\n Additionally, in some implementations, the user may enable the action trigger processing system 120 to access and keep track of authentication information for resource publishers 104 that the user device 106 of the user has accessed (e.g., a\nuser may enable the action trigger processor 124 to store the authentication information that accesses resources related to bank account information of the user at a publisher resource 104 of a particular bank).\n Moreover, a user history related to previous actions performed by the user on the user device 106 may be included in action trigger data 126.  For example, previous action triggers, user defined events, and user defined actions may be stored in\nthe action trigger data 126 to enable default, favorite, or common action triggers, user defined events, and/or user defined actions, for the particular user, a group of users, and/or a subset of users, to be selected by the user.  Additionally, a\ncontext of a user may change or adjust the suggested or favorite action triggers, user defined events, and/or user defined actions.  The time of day, week, month, location of the user, frequency and regularity of setting up particular action triggers,\namong others may factor in to the context of the user.  However, in the example above, the user can change the parameters and parameter values that are initially suggested or provided to the user.  Additionally, in some implementations, a user calendar\non a user device 106 and user interests provided, implicitly (e.g., based on a user history) or explicitly, to the action trigger processing system 120 may factor in to determining suggested action triggers, user defined events, user defined actions,\nparameters and parameter values.  Moreover, the order or ranking of suggested action triggers provided (e.g., as the user is inputting an action trigger) may also adjust and change based on context, prior action triggers, or other information.\n FIG. 3 is a flow diagram of an example process 300 for creating an action trigger from a resource 105 of a resource publisher 104.  The process 300 can, for example, be implemented by the action trigger processor 124.  In some implementations,\nthe operations of the example process 300 can be implemented as instructions stored on a non-transitory computer readable medium, where the instructions cause a data processing apparatus to perform operations of the example process 300.\n A resource 105 that includes a first parameter value of a first parameter and instructions that cause the user device 106 to display an action trigger prompt that is rendered in an initial state is provided (302).  Each resource 105 may include\ncontent and the action trigger processor 124 may determine one or more parameters from the content provided in the resource 105.  For example, the first parameter may be topics, titles, categories, or other information provided within the content of the\nresource 105.  The first parameter value may indicate information provided in the content of the resource 105 related to the first parameter.  For example, the first parameter value may indicate availability, cost, or a weather condition, among others.\n The action trigger prompt may be displayed along with or within the resource 105.  The action request prompt may be included from the resource publisher 104 when the resource 105 is provided to the user device 106, or in other implementations,\nthe action trigger processing system 120 may provide the action request prompt to the user device 106, where the action request prompt is provided with the resource 105 or overlaid over the resource 105.  If the action request prompt is included from the\nresource publisher 104, the action trigger processing system 120 may provide the required scripting to the resource publishers 104 that include the action triggers (e.g., via API 107).\n Instructions are provided that render the resource 105 at the user device 106 (304).  The resource 105 may then be rendered at the user device along with the action trigger prompt.  A selection of the action trigger prompt at the user device 106\nis received (306).\n Instructions that cause the user device 106 to display an action trigger menu that includes an input for a user defined event are provided (308).  After selection of the action trigger prompt, the action trigger menu may be provided where the\nuser, via the user device 106, can select or input a user defined event to be notified of when the event occurs, or in some implementations, a user defined action may be initiated or completed when the particular user defined event occurs.\n Input of the first parameter of the user defined event in the action trigger menu is received, where the resource 105 includes a first parameter value for the first parameter (310).  The user defined event may include a first parameter that is\ninput or selected by the user at the user device 106, as previously described.  The first parameter includes a first parameter value, based on the content provided in the resource 105.  For example, the first parameter value may indicate a status,\navailability, cost, or a weather condition, among others.  In an example of a particular stock quote shown at a resource 105 (e.g., a web page or mobile application) including information of a particular stock, a first parameter may be, for example, the\nstock price, earnings, opening and closing price, or other type of information that may be obtained from the resource 105.\n For example, if the first parameter is the stock price, and the first parameter value may be the current price or what is listed as the current price at the resource 105, for example $600 per share.  Likewise, in a resource 105 that includes\nconcert tickets for a particular artist, the first parameter may be ticket availability (e.g., a new tour for the artist), ticket price, seat location, concert location, or other type of information that may be obtained from the resource 105.  In the\ncurrent example, the first parameter is ticket availability, and the first parameter may indicate that concert tickets for the particular artist are unavailable.\n A value input as a second parameter value for the first parameter of the user defined event is received, wherein the second parameter value only specifies a parameter value that is different from the first parameter value (312).  The user\ndefined event may also include a second parameter value that is input or selected by the user at the user device 106.  The second parameter value may indicate the event related to the identified first parameter that the user would like to be notified of,\nor a user defined action to take place when the second parameter value is reached by the first parameter, based on the content of the resource 105.\n The action trigger processor 124 may search the event data 128 to determine a parameter value of the identified parameter.  Based on the stock price example above, the second parameter value of the stock price may be provided by the user, and\nmay be, for example, $630 per share.  When the action trigger processor 124 determines from the data provided in the resource 105 and/or the event data 128 that the value of the first parameter has met the value of the second parameter value, then a\nnotification of such may be provided to the user device 106 (e.g., stock of Company X has reached $630) or a user defined action may be set to take place (e.g., the action trigger system 120 may be directed to sell the stock).  Likewise, with respect to\nthe concert ticket example above, a second parameter value of ticket availability may be, for example, a new tour (i.e., tickets for sale).  When the action trigger processor 124 determines from the data provided in the resource 105 and/or the event data\n128 that the value of the first parameter (ticket availability) has met the value of the second parameter value (tickets for sale), a notification of such may be provided to the user device 106 (e.g., tickets for Artist A are now on sale) or a user\ndefined action may be set to take place (e.g., the action trigger system 120 may be directed to purchase tickets).\n For example, FIG. 4A is an illustration of a user interface 402a at a user device 400 in which an action trigger prompt is provided along with a resource 105.  Within resource 105, which in the current example may be an event ticket selection\nand purchase web site or application, a search input 404 may be provided.  In the current example, \"Artist A\" has been provided in the search input 404, and the resource 105 has indicated there are not currently any events for \"Artist A.\" Also, a\nlocation identifier 405 is provided in the resource 105, which may be a current location of a user device 106 of the user or a location the user has provided or input.  Additionally, an action trigger prompt 406 is provided in the resource 105 to enable\nthe user of the user device 106 to create an action trigger.\n In FIG. 4B, an illustration of a user interface 402b provided where the action trigger prompt 406 has been selected, and an action trigger menu 408 is provided.  In the current example, based on the content provided in resource 105, the action\ntrigger processor 124 may provide a suggested user defined event and the user may be able to input or provide a different selection for the user defined event (e.g., by use of the arrows in action trigger menu 408).  The suggested user defined event may\nbe based on the information of action trigger data 126 and/or event data 128.  In the current example, a suggested user defined event of \"When Artist A has a new tour\" is provided.  The first parameter may be determined to be related to Artist A's tours,\nand based on the location content provided in resource 105, the first parameter may be related to Artist A's tours in the \"SF Bay Area, Calif.\"\n The first parameter value, in the current example, may indicate that the concert and tour information for the first parameter is not available.  In some implementations, the first parameter value may be input or provided by the user of the user\ndevice.  At value input 410, the user may provide a parameter value of the first parameter to complete a definition of the user defined event, and when the first parameter at resource 105 has a value of the user provided parameter value, then the user\ndefined event may be met.  In the current example, the user provided parameter value is related to Artist A's \"tour,\" and specifically a new tour.  In some implementations, based on the content of resource 105 and the information of action trigger data\n126 and/or event data 128, which may include, for example, interests and a user history, of the user of user device 106, a suggested or hierarchy of user provided values to include in value input 410 may be provided.\n In FIG. 4C, an illustration of a user interface 402c is provided showing a notification that the value of the first parameter has met the parameter value of value input 410, or a second parameter value.  As previously described, action trigger\nprocessor 124 obtains updates and status information associated with resource 105 from event data 128, and when the action trigger processing system 120 determines the user defined event has occurred, the action trigger processor may provide a\nnotification that the value of the first parameter is the second parameter value to the user device, or in some implementations, a user defined action may be complete when the action trigger processing system 120 determines the user defined event has\noccurred.  In the current example, FIG. 4C indicates that the user defined event has occurred, and a user defined event notification 412 is provided to the user device 106 of the user.  In some implementations, a user may select the user defined event\nnotification 412 and the user may be provided with information associated with the user defined event, and in the current example, the user may be directed to the resource 105 to see information related to Artist A's tour and see ticket purchase\nselections.\n In some implementations, the user of the user device 106 may select or provide a user defined action to occur once the action trigger processing system 120 determines there has been an occurrence of the user defined event.  For example, based on\nthe example provided in FIGS. 4A-4C, the user may provide a user defined action to the action trigger processing system 120, via the user device 106, to purchase tickets when resource 105 indicates that Artist A has a new tour (and tickets are\navailable).  Additionally, the user defined action may specify, for example, a number of tickets, a date or date range, a price range, and a section in which to purchase the tickets.  Additionally, a notification may also be provided to indicate that the\nuser defined action has been performed (and a notification may also be included to indicate the user defined event has occurred before or while in the process of performing the user defined action).\n Moreover, in some implementations, the presentation of a notification may be provided to a device other than user device 106.  For example, the presentation may be provided to a device that is determined to be close to the user or a device that\nthe user will see or is looking at. For example, if the user device 106 of the user is not currently visible to the user, but the user is viewing another device, the action trigger processing system 120 may determine to present the notification to the\ndevice the user is viewing.\n Action trigger prompts can also be provided for many other types of content that have varying parameter values.  For example, an action trigger prompt may be provided for an advertisement.  Selection of the prompt may enable the user to select\nvarious actions and events related to the product or service being advertised.  By way of example, for an advertisement advertising a product that is yet to be released, the user may set a notification of the release that triggers on the release of the\nproduct.  Conversely, for a product that is currently for sale, the user may set an action to purchase the product when the product goes on sale, e.g., 10% off, by a particular retailer.  In this latter example, the user may specify a particular sale\nprice as the trigger, or the user may specify any sale event as a trigger.\n In situations in which the systems discussed herein collect personal information about users, or may make use of personal information, the users may be provided with an opportunity to control whether programs or features collect user information\n(e.g., information about a user's social network, social actions or activities, profession, a user's preferences, a user's current location, location trajectory, inferred locations such as home/work, inferred context, calendar data, upcoming\nevents/bookings from email, etc.), or to control whether and/or how to receive content from the content server that may be more relevant to the user.  In addition, certain data may be treated in one or more ways before it is stored or used, so that\npersonally identifiable information is removed.  For example, a user's identity may be treated so that no personally identifiable information can be determined for the user, or a user's geographic location may be generalized where location information is\nobtained (such as to a city, ZIP code, or state level), so that a particular location of a user cannot be determined.  Thus, the user may have control over how information is collected about the user and used by a content server.\n FIG. 5 is a block diagram of example mobile computing device.  In this illustration, the mobile computing device 510 is depicted as a handheld mobile telephone (e.g., a smartphone, or an application telephone) that includes a touchscreen display\ndevice 512 for presenting content to a user of the mobile computing device 510 and receiving touch-based user inputs.  Other visual, tactile, and auditory output components may also be provided (e.g., LED lights, a vibrating mechanism for tactile output,\nor a speaker for providing tonal, voice-generated, or recorded output), as may various different input components.\n The mobile computing device 510 may include mechanical or touch sensitive buttons 518a-d. Additionally, the mobile computing device may include buttons for adjusting volume output by the one or more speakers 520, and a button for turning the\nmobile computing device on or off.  A microphone 522 allows the mobile computing device 510 to convert audible sounds into an electrical signal that may be digitally encoded and stored in computer-readable memory, or transmitted to another computing\ndevice.  The mobile computing device 510 may also include a digital compass, an accelerometer, proximity sensors, and ambient light sensors.\n The mobile computing device 510 may present a graphical user interface with the touchscreen 512.  A graphical user interface is a collection of one or more graphical interface elements and may be static (e.g., the display appears to remain the\nsame over a period of time), or may be dynamic (e.g., the graphical user interface includes graphical interface elements that animate without user input).\n The mobile computing device 510 may include other applications, computing subsystems, and hardware.  A voice recognition service 572 may receive voice communication data received by the mobile computing device's microphone 522, and translate the\nvoice communication into corresponding textual data or perform voice recognition.\n The mobile computing device 510 may communicate wirelessly with one or more networks to provide a variety of services, such as voice and data services.\n Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and\ntheir structural equivalents, or in combinations of one or more of them.  Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions,\nencoded on computer storage medium for execution by, or to control the operation of, data processing apparatus.\n A computer storage medium can be, or be included in, a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them.  Moreover, while a\ncomputer storage medium is not a propagated signal, a computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal.  The computer storage medium can also be, or be included\nin, one or more separate physical components or media (e.g., multiple CDs, disks, or other storage devices).\n The operations described in this specification can be implemented as operations performed by a data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.  The term \"data processing\napparatus\" encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or multiple ones, or combinations, of the foregoing.  The apparatus can\ninclude special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).  The apparatus can also include, in addition to hardware, code that creates an execution environment for the\ncomputer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them.  The\napparatus and execution environment can realize various different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.\n Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.  Generally, a processor will receive\ninstructions and data from a read only memory or a random access memory or both.  The essential elements of a computer are a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and\ndata.  Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.  However, a computer\nneed not have such devices.\n Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that\nincludes a front end component, e.g., a user computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more\nsuch back end, middleware, or front end components.  The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network.  Examples of communication networks include a local area network\n(\"LAN\") and a wide area network (\"WAN\"), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).\n While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any inventions or of what may be claimed, but rather as descriptions of features specific to particular\nembodiments of particular inventions.  Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment.  Conversely, various features that are described in the\ncontext of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination.  Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or\nmore features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.\n Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations\nbe performed, to achieve desirable results.  In certain circumstances, multitasking and parallel processing may be advantageous.  Moreover, the separation of various system components in the embodiments described above should not be understood as\nrequiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.\n Thus, particular embodiments of the subject matter have been described.  Other embodiments are within the scope of the following claims.  In some cases, the actions recited in the claims can be performed in a different order and still achieve\ndesirable results.  In addition, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results.  In certain implementations, multitasking and parallel\nprocessing may be advantageous.", "application_number": "16151101", "abstract": " Methods, systems, and apparatus, for providing notifications based on\n     third party data. In an aspect, a method includes receiving first\n     instructions that cause a user device to render a resource that includes\n     a first parameter value of a first parameter and display the resource and\n     generating second instructions that cause the user device to display an\n     action trigger prompt that is rendered with the resource and in an\n     initial state, and wherein the action trigger prompt is rendered to\n     augment the resource with additional content not provided by rendering of\n     the resource according to the first instructions.\n", "citations": ["6654025", "7334000", "8751602", "8766793", "20040019603", "20040030753", "20050251558", "20080004926", "20090157672", "20100214428", "20100271202", "20120293341", "20140067649", "20140100904", "20140247383", "20150006632"], "related": ["16020616", "15264023", "15197301", "14752369", "62167726"]}, {"id": "20190042559", "patent_code": "10366161", "patent_name": "Anaphora resolution for medical text with machine learning and relevance\n     feedback", "year": "2019", "inventor_and_country_data": " Inventors: \nAllen; Corville O. (Morrisville, NC), DeLima; Roberto (Apex, NC), Ezen Can; Aysu (Cary, NC), Sizemore; Robert C. (Fuquay-Varina, NC)  ", "description": "BACKGROUND\n The present disclosure relates generally to natural language processing, and more particularly to resolving anaphora within natural language text.\n Once words or phrases (i.e. entities) are recognized in a span of natural language text, it is useful to delve into semantics, a deeper level than syntax processing, so that the relationships between entities and the overall concepts of the\nauthor could be understood by an intelligent system.\nBRIEF SUMMARY\n Embodiments of the present invention disclose a method, a computer program product, and a system.\n According to an embodiment, a method for directing a computer processor to resolve an anaphor in electronic natural language text.  The method detects multiple entities and an anaphor in a span of parsed natural language text including one or\nmore sentences.  The method extracts pairs of related entities among the multiple entities, based on domain knowledge.  The method further constructs a set of tuples, wherein each tuple is a data type including an anaphor, an antecedent entity (AE)\nappearing before the anaphor in the span of parsed natural language text, and an entity (E) appearing after the anaphor in the span of parsed natural language text, wherein the anaphor refers to the AE and relates the AE to the E, and wherein the\nconstructing is based on initial training data and the extracted pairs of related entities.  The method resolves the anaphor by determining which entity in the multiple entities the anaphor references, using the constructed set of tuples, and selecting\nan AE among one or more candidate AEs.\n According to another embodiment, a computer program product for directing a computer processor to resolve an anaphor in electronic natural language text.  The storage device embodies program code that is executable by a processor of a computer\nto perform a method.  The method detects a plurality of entities and an anaphor in a span of parsed natural language text including one or more sentences.  The method extracts pairs of related entities among the multiple entities, based on domain\nknowledge.  The method further constructs a set of tuples, wherein each tuple is a data type including an anaphor, an antecedent entity (AE) appearing before the anaphor in the span of parsed natural language text, and an entity (E) appearing after the\nanaphor in the span of parsed natural language text, wherein the anaphor refers to the AE and relates the AE to the E, and wherein the constructing is based on initial training data and the extracted pairs of related entities.  The method resolves the\nanaphor by determining which entity in the multiple entities the anaphor references, using the constructed set of tuples, and selecting an AE among one or more candidate AEs.\n According to another embodiment, a system for directing a computer processor to resolve an anaphor in electronic natural language text, includes one or more computer devices each having one or more processors and one or more tangible storage\ndevices.  The one or more storage devices embody a program.  The program has a set of program instructions for execution by the one or more processors.  The program instructions include instructions for detecting a plurality of entities and an anaphor in\na span of parsed natural language text including one or more sentences.  The program instructions include instructions for extracting pairs of related entities among the multiple entities, based on domain knowledge.  The program instructions include\ninstructions for constructing a set of tuples, wherein each tuple is a data type including an anaphor, an antecedent entity (AE) appearing before the anaphor in the span of parsed natural language text, and an entity (E) appearing after the anaphor in\nthe span of parsed natural language text, wherein the anaphor refers to the AE and relates the AE to the E, and wherein the constructing is based on initial training data and the extracted pairs of related entities.  The program instructions include\ninstructions for resolving the anaphor by determining which entity in the multiple entities the anaphor references, using the constructed set of tuples, and selecting an AE among one or more candidate AEs. BRIEF DESCRIPTION OF THE DRAWINGS\n FIG. 1 illustrates a computing environment, in accordance with an embodiment of the present invention.\n FIGS. 2A-2E are flowcharts illustrating the operation of anaphora resolution program 136 of FIG. 1, in accordance with embodiments of the present invention.\n FIG. 3 is a diagram graphically illustrating the hardware components of a computing environment of FIG. 1, in accordance with an embodiment of the present invention.\n FIG. 4 depicts a cloud computing environment, in accordance with an embodiment of the present invention.\n FIG. 5 depicts abstraction model layers of the illustrative cloud computing environment of FIG. 4, in accordance with an embodiment of the present invention.\nDETAILED DESCRIPTION\n Anaphora resolution is an important part of natural language processing (NLP) in order to derive a clearer understanding from natural language text.  An anaphor refers to a segment of natural language text, such as a word or a phrase, that\nrefers to another segment of natural language text.  Determining what other segment of natural language text the anaphor refers to may require examining the anaphor's context.  The anaphor's context may include one or more segments of text surrounding\nthe anaphor.  In one embodiment, the anaphor refers to a segment of text that appears before the anaphor's mention in a span of natural language text under consideration.  In another example, the anaphor may be a pronoun or a generalization, and its\ncontext may be words in the same sentence or in one or more of its surrounding sentences (e.g. 2 to 3 sentences before and/or 2 to 3 sentences after).  A pronoun is a word that substitutes for a noun or a noun phrase, most typically a pronoun.  The use\nof pronouns often involves anaphora, where the meaning of the pronoun is dependent on an antecedent.  For example, in the sentence That poor man looks like he needs a new coat, the antecedent of the pronoun he is the noun phrase that poor man.  A\ngeneralization is the formulation of general concepts from specific instances by abstracting common properties.  Generalizations posit the existence of a domain or set of elements, as well as one or more common characteristics shared by those elements. \nFor example, animal is a generalization of bird because every bird is an animal, and there are animals which are not birds.\n Anaphora analysis in natural language text often involves analysis of mentions of entities in the natural language text.  Analyzing entity-mentions enables semantic analysis, which goes deeper than syntactic analysis.  Semantic analysis enables\nidentifying relationships between entities in the span of natural language text and the concepts that the text conveys.\n Entities are words or phrases that have an associated classification, according to some knowledge base (which may be as simple as a list or a dictionary).  A span of natural language text can be examined to identify the entities it contains. \nThese entities may relate, or refer, to one another in some form.  An anaphor can be considered as an entity that refers back to another entity in the span of natural language text.  The entity to which the anaphor refers back may be called an antecedent\nentity (AE).  The anaphor may relate the AE to another entity (E) in the span of natural language text, which may appear after the anaphor.\n In some spans of natural language text, where there are multiple entities before and/or after the mention of the anaphor, it may be unclear which AE or AEs the anaphor refers to.  Consequently, it also may be unclear which Es appearing after an\nanaphor's mention are related to which AEs appearing before the anaphor's mention.  For any E in a span of natural language text under consideration, there may be multiple AEs (i.e. candidate AEs) that are potentially related to the E via the anaphor. \nThe process of determining which AE or AEs are related to the E via the anaphor is called anaphor resolution.\n One context in which anaphora resolution may be useful is in medical text analysis.  One example of medical text is an electronic medical record (EMR).  In medical text analysis, performing accurate anaphora resolution may enable improving the\naccuracy of treatment recommendations.\n Anaphora resolution, according to various embodiments of the invention may be discussed in the context of medical text analysis.  Two illustrative example of medical text (i.e. spans of natural language text) that may be found in an EMR include\nEXAMPLE 1--\"Tumors were noted in the brain and liver.  They measured 6 cm and 5 cm.\", and EXAMPLE 2--\"8/20 lymph nodes had evidence of metastatic spread.  Tumors were noted in the brain and liver.  They measured 6 cm and 5 cm.\" In each example, the\nanaphor requiring disambiguation may be they.  In each example, there may be one or more Es and AEs that will be used to resolve the anaphor.  These will be discussed in more detail in connection with embodiments of the invention.\n For instance, in EXAMPLE 1, it is not self-evident that the anaphor they refers to tumors.  Without a contextual understanding of the text's semantics, they may equally refer to brain and liver.  Therefore, the anaphor they is ambiguous and must\nbe resolved.  This fact highlights one reason why embodiments of the present invention disclose a technical solution to a technical problem of processing natural language text electronically.  A human being may read the sentences in EXAMPLE 1 and\nintuitively conclude that they refers to tumors.  However, a computer has no intuition and requires a technical solution for the problem of anaphor resolution.  Embodiments of the invention provide such a technical solution.\n Similarly, in EXAMPLE 2, it is not evident whether they refers to lymph nodes or tumors.  Correctly resolving the anaphor requires application of a non-trivial technical solution.\n The current state of the art in anaphora resolution either addresses this problem by employing full manual annotations or using only machine learning, which are both time consuming ventures in obtaining training data.  The present invention\ndiscloses a framework that combines the strengths of both machine learning and manual annotations to resolve anaphora.\n Hereinafter, exemplary embodiments of the present invention will be described in detail with reference to the attached drawings.\n The present invention is not limited to the exemplary embodiments below, but may be implemented with various modifications within the scope of the present invention.  In addition, the drawings used herein are for purposes of illustration, and\nmay not show actual dimensions.\n FIG. 1 is a functional block diagram of a computing environment 100, according to an embodiment of the present invention.  Computing environment 100 includes computing device 110, database server 120, and data processing system 130 all connected\nvia network 102.  The setup in FIG. 1 represents an example embodiment configuration for the present invention, and is not limited to the depicted setup in order to derive benefit from the present invention.\n In the example embodiment, computing device 110 contains user interface 112, and source document 114.  In various embodiments, computing device 110 may be a laptop computer, tablet computer, netbook computer, personal computer (PC), a desktop\ncomputer, a personal digital assistant (PDA), a smart phone, or any programmable electronic device capable of communicating with database server 120 and data processing system 130 via network 102.  Computing device 110 may include internal and external\nhardware components, as depicted and described in further detail below with reference to FIG. 3.  In other embodiments, computing device 110 may be implemented in a cloud computing environment, as described in relation to FIGS. 4 and 5, herein. \nComputing device 110 may also have wireless connectivity capabilities allowing it to communicate with database server 120, data processing system 130, and other computers or servers over network 102.\n User interface 112 is a computer program which allows a user to interact with computing device 110 and other connected devices via network 102.  For example, user interface 112 may be a graphical user interface (GUI).  In addition to comprising\na computer program, user interface 112 may be connectively coupled to hardware components, such as those depicted in FIG. 3, for receiving user input.  In the example embodiment, user interface 112 is a web browser, however in other embodiments user\ninterface 112 may be a different program capable of receiving user interaction and communicating with other devices.\n Source document 114, in the example embodiment, may be a collection (corpus) of one or more documents containing spans of natural language text related to the medical domain.  For example, the source document 114 can be an electronic medical\nrecord (EMR) of a patient, and may include one or more sentences.  The one or more sentences may include an anaphor that requires resolution.  Other examples of source document 114 may include medical journal articles, clinical trial studies, and other\nnatural language text documents that discuss information relevant to the medical domain.  In alternative embodiments, source document 114 may include natural language text documents related to other knowledge domains.\n In the example embodiment, database server 120 includes medical database 122, domain knowledge database 124, knowledge graph database 126, and tuple database 128 and may be a laptop computer, tablet computer, netbook computer, personal computer\n(PC), a desktop computer, a personal digital assistant (PDA), a smart phone, a server, or any programmable electronic device capable of communicating with computing device 110 and data processing system 130 via network 102.  While database server 120 is\nshown as a single device, in other embodiments, database server 120 may be comprised of a cluster or plurality of computing devices, working together or working separately.\n Medical database 122, in the example embodiment, may include a stored list, or lists, of medical terminology, or medical entities (i.e. words or phrases), from medical dictionaries and medical literature.  Examples of such medical entities\nstored in medical database 122 may include: &lt;knee&gt;, &lt;joint pain&gt;, &lt;brain&gt;, &lt;liver&gt;, &lt;endocrine system&gt;, &lt;cataracts&gt;, &lt;flu&gt;, &lt;disease&gt; and so forth.  In alternative embodiments, medical entities may be\ndetermined by input from subject matter experts (SMEs).  The medical entities may be categorized according to body parts, body systems, symptoms, diseases, or in any other way known to one of ordinary skill in the art.  In other embodiments, the lists\nmay be specific to other knowledge domains.\n Domain knowledge database 124, in the example embodiment, includes a list of pairs of related medical entities.  These lists may be generated automatically, manually, or via a combination of both.  For example, the list may be generated randomly\nand improved through machine learning techniques.  The list may be generated, or improved, using manual curation by a subject matter expert (SME).  The list of pairs of related entities may be derived from the individual entities in medical database 122. For example, a stored list of pairs of related entities may be represented as Pair1-PairN (P1-PN), where N represents an integer greater than 1, and be listed as follows in domain knowledge database 124: P1&lt;knee, joint pain&gt;, P2&lt;brain,\norgan&gt;, P3&lt;tumor, measurement&gt;, P4&lt;stomach, acidity&gt;, and so on.\n In the example embodiment, an individual entity that may be determined to be related to multiple other entities are listed as individual data objects, such as: P1&lt;lymph node, organ&gt;, P2&lt;lymph node, white blood cell&gt;, P3&lt;lymph\nnode, centimeter&gt;, P4&lt;lymph node, infection&gt;, and so on.  In alternative embodiments, the related pairs of entities may be scored, or assigned a value, to denote the strength of the relationship between the two entities.  For example, the\nstrength of a relationship between paired entities may denote the commonality of the two entities being used together in a given span of natural language text.  For example, &lt;brain, neuron&gt; may have a stronger relationship than &lt;brain,\norgan&gt;.\n Knowledge graph database 126, in the example embodiment, logically depicts the interrelationships between the related pairs of entities stored in domain knowledge database 124.  For example, the entity \"brain\" may be determined by an SME, in the\ntraining data, to be related (either tenuously, significantly, or somewhere in between) to \"body\", \"organ\", \"disease\" and various other entities.  Knowledge graph database 126 reflects the logical relationships, and their strengths, between \"brain\",\n\"organ\", \"disease\" and so forth.  For example, the knowledge graph may be in the form of a line graph or tree structure.\n Tuple database 128, in the example embodiment, includes stored data objects, known as tuples, that contain a finite, and immutable, ordered list of related entity pairs, associated with an anaphor, from initial training data.  A tuple, in the\nexample embodiment, contains an antecedent entity (AE) and an entity (E), associated with an anaphor, which may be represented in the following sequence (AE, anaphor, E), (AE, E), or (AE, E, anaphor).  Where the anaphor is not expressly stored as part of\nthe tuple, the anaphor may be associated with the tuple using any known means in the art.  The determined relationship between a specific AE, E and an anaphor may be based on the results of initial training data.  Initial training data may include sets\nof training tuples constructed using a set of training sentences, wherein each training tuple includes AEs and Es known to be related via corresponding anaphora.  The sets of training tuples may be compiled via an automated process (e.g. machine learning\nprocess) or through a manual process (e.g. SME review).  For example, an SME may be presented with a set of training sentences that include multiple entities connected via an anaphor.  The SME may be prompted to identify which candidate AE, of multiple\nAEs, corresponds to a particular E under consideration via an anaphor.\n In natural language processing, there is typically an order wherein an AE, E and anaphor are identified in a multiple sentence structure.  For any E under consideration that appears after an anaphor, it is connected to at least one AE which\nappears before the anaphor.  The E under consideration and the AE are connected via the anaphor.  For an E under consideration there may be multiple such AEs where each AE may be considered a candidate AE.  Some such AEs may be more likely to be\nconnected to the E under consideration via the anaphor whereas other candidate AEs may be less likely to be connected to the E under consideration via the anaphor.  These various relationships are represented as tuples in tuple database 128.\n With reference to EXAMPLE 1, the resulting set of training tuples may be depicted in tuple database 128 as follows: (tumors, 6 cm, they), (tumors, 5 cm, they), (brain, 6 cm, they), (brain, 5 cm, they), (liver, 6 cm, they), (liver, 5 cm, they). \nAn SME's denotation of (tumors, 6 cm, they) and (tumors, 5 cm, they) as resolving the anaphor they correctly may be noted in tuple database 128 as a score or value of correctness.  A lower score or value may be denoted next to tuples that do not\ncorrectly resolve the anaphor they.  The resulting initial training data may assist the computer program in learning both correct, and incorrect, associations (based on an assigned score or value) between related entities and an anaphor.  The greater the\namount of initial training data stored in tuple database 128, the higher the probability of success (based on the normalization of tuples) the program will have in identifying correct anaphora resolutions on newly presented natural language text data.\n In alternative embodiments, tuple database 128 may also store the tuples that incorrectly pair an entity with an antecedent entity and anaphor, in order to derive correct associations between entities based on tuples that are known to be\nincorrect.  For example, the incorrectly paired tuples (brain, 6 cm, they), (brain, 5 cm, they), (liver, 6 cm, they), and (liver, 5 cm, they) may be known to be incorrect (based on lower assigned scores or values), and therefore by the process of\nelimination (or by identifying the tuple with a higher assigned score or value), the tuples (tumors, 6 cm, they), and (tumors, 5 cm, they) may be identified as being correct.\n Data processing system 130, in the example embodiment, may be a laptop computer, tablet computer, netbook computer, personal computer (PC), a desktop computer, a personal digital assistant (PDA), a smart phone, or any programmable electronic\ndevice capable of communicating with computing device 110 and database server 120 via network 102.  While data processing system 130 is shown as a single device, in other embodiments, data processing system 130 may be comprised of a cluster or plurality\nof computing devices, working together or working separately.  In the example embodiment, data processing system 130 includes processor 132, NLP pipeline 134, and anaphora resolution program 136.  Anaphora resolution program 136 contains instruction sets\nwhich may be described using a set of functional modules.  Processor 132 executes anaphora resolution program's 136 instruction sets.\n NLP pipeline 134 may be a software application which is capable of receiving, analyzing, and understanding natural language text.  In the example embodiment, NLP pipeline 134 may comprise features such as dictionaries, syntax and semantics\nrules, statistical models, and relational databases in order to perform a natural language text analysis of data, such as the data contained in source document 114.  Anaphora resolution program 136, in the example embodiment, complements existing NLP\npipeline 134 features.\n Anaphora resolution program 136, in the example embodiment, operates within NLP pipeline 134 and may be stored on a storage device of data processing system 130.  Anaphora resolution program 136 may be a computer program configured to detect one\nor more entities and an anaphor in a parsed natural language text source document 114 that includes one or more sentences, extract one or more pairs of related entities amongst the one or more entities using domain knowledge, and construct a set of\ntuples based on initial training data and the extracted pairs of related entities.  Based on the constructed set of tuples, anaphora resolution program 136 resolves the anaphor in a given span of parsed natural language text by determining which entity,\nof the one or more entities, the anaphor references and selects an antecedent entity (AE) among one or more candidate AEs.\n With continued reference to FIG. 1, the functional modules of anaphora resolution program 136 include entity and anaphora detector 138, entity pair extractor 140, tuple constructor 142, and anaphora resolver 144.\n Entity and anaphora detector 138 includes a set of programming instructions, in anaphora resolution program 136, to detect multiple entities and an anaphor in parsed natural language text comprising one or more sentences.  The set of programming\ninstructions is executable by processor 132.  In the exemplary embodiment, entity and anaphora detector 138 contains program instructions to detect entities from medical database 122, within a span of natural language text of source document 114.\n In the example embodiment, entity and anaphora detector 138 receives input from computing device 110 and database server 120, and is capable of parsing a span of natural language text within a received source document 114, via text parsing tools\nknown to one of ordinary skill in the art.  A sub-task of parsing may include the process of tokenization, which demarcates, or classifies, sections of a string of parsed natural language text.  The resulting tokens may include entities (i.e. words or\nphrases) that match known normalized entities stored in medical database 122.  The identified entities in the span of natural language text may then be assigned a value, at the discretion of the user, for temporary identification and storage purposes\nwithin anaphora resolution program 136.\n In the example embodiment, anaphora may comprise a pronoun (i.e. such as him, her, they, it) and/or a generalization (i.e. such as site or any other word or phrase that, absent other information, relates an E to more than one candidate AE). \nPronouns may be detected by using an array of NLP rules, known to one of ordinary skill in the art, such as a dictionary that contains an exhaustive list of known pronouns.  Anaphora generalizations, on the other hand, may be detected according to any\nknown method in the art (i.e. such as domain knowledge, or entity detection).\n In an exemplary embodiment, entity and anaphora detector 138 may detect the following entities (Es) and anaphor, and assign an identification number, from the following parsed sentences (EXAMPLE 1) from source document 114: \"Tumors were noted in\nthe brain and liver.  They measured 6 cm and 5 cm.\" (Tumors &lt;E1&gt;, brain &lt;E2&gt;, liver &lt;E3&gt;, 6 cm &lt;E4&gt;, 5 cm &lt;E5&gt;, They &lt;anaphor&gt;).  The words were, noted, in, the, and, measured, and may be hidden in the analysis since\nthese words are not recognized as entities within medical database 122, nor recognized as pronouns or generalizations.  Measurements, such as centimeters (cm), inches (in), feet (ft), may be included in medical database 122 since these are common in\nmedical diagnostics and analysis, as determined by initial training data.  Furthermore, measurements oftentimes may be written in abbreviation or shorthand in natural language text.  For example, the word \"centimeter\" may be depicted as centimeter,\ncentimeters, cm, centro, cmeter, and as such may be stored in medical database 122 under all of the above-referenced variations.  In the exemplary embodiment, entity and anaphora detector 138 may be capable of storing the identified Es and anaphora\nwithin a span of natural language text, and outputting same to entity pair extractor 140.\n Entity pair extractor 140 includes a set of programming instructions in anaphora resolution program 136, to extract pairs of related entities among the multiple entities, within the natural language text of source document 114, based on domain\nknowledge.  The set of programming instructions is executable by processor 132.  In the exemplary embodiment, entity pair extractor 140 utilizes initial training data from domain knowledge database 124 and knowledge graph database 126 to determine\nrelatedness between one or more detected entities of a span of natural language text in source document 114.\n In the exemplary embodiment, a span of parsed natural language text may cover one, two, or three sentences within a source document 114.  Out of the detected multiple entities located within a span of natural language text, entity pair extractor\n140 is capable of determining a pair of related entities (i.e. an entity and its antecedent entity), based on initial training data contained within domain knowledge database 124.\n With continued reference to the above EXAMPLE 1 sentences, entity pair extractor 140 may determine that related entities for a span of natural language text, based on domain knowledge database 124, include: &lt;tumor, brain&gt;, &lt;tumor,\nliver&gt;, &lt;tumor, 6 cm&gt;, &lt;tumor, 5 cm&gt;.  The identified relationships between entities in a span of natural language text may be tenuous or strong.  The determination of relationship strength may be based on the location of one entity in\nrelation to another related entity within the span of natural language text.  In the exemplary embodiment, the extracted pairs of related entities may be temporarily stored in entity pair extractor 140 and used in constructing sets of tuples.\n Tuple constructor 142 includes a set of programming instructions in anaphora resolution program 136, to construct a set of tuples, wherein each tuple is a data type comprising an anaphor, an antecedent entity (AE) appearing before the anaphor in\nthe span of parsed natural language text, and an entity (E) appearing after the anaphor in the span of parsed natural language text, wherein the anaphor refers to the AE and relates the AE to the E, and wherein the constructing is based on initial\ntraining data from tuple database 128, and the extracted pairs of related entities from entity pair extractor 140.  The set of programming instructions is executable by processor 132.\n In the exemplary embodiment, tuple constructor 142 receives input from entity pair extractor 140 and database server 120.  Tuple constructor 142 utilizes initial training data from tuple database 128 (e.g. training tuples) to determine known\nrelationships between AEs, Es, and anaphora for the extracted pairs of related entities from entity pair extractor 140.  For any E under consideration that appears after an anaphor, it is connected to at least one AE which appears before the anaphor. \nThe E under consideration and the AE are connected via the anaphor.  For an E under consideration there may be multiple such AEs where each AE may be considered a candidate AE.  Some such AEs may be more likely to be connected to the E under\nconsideration via the anaphor whereas other candidate AEs may be less likely to be connected to the E under consideration via the anaphor.\n Tuple constructor 142 may construct only two tuples with regards to the span of natural language text in EXAMPLE 1: \"Tumors were noted in the brain and liver.  They measured 6 cm and 5 cm.\" The first constructed tuple, via tuple constructor 142,\nmay be: (tumors, 6 cm, they) and wherein the second constructed tuple, via tuple constructor 142, may be: (tumors, 5 cm, they).  Since there is only one AE (\"tumors\"), for both Es (\"6 cm\" and \"5 cm\"), in the initial training data stored in tuple database\n128 associated with the anaphor (\"they\"), there is no basis to perform disambiguation logic.  Disambiguation logic may be utilized when there are multiple AEs for any given E under consideration in a span of natural language text.\n In the exemplary embodiment, tuple constructor 142 may determine the entities of a given tuple (AE, E, anaphor) based upon the identified Es and anaphora of entity and anaphora detector 138, the extracted pairs of related Es from entity pair\nextractor 140, and based upon a set of rules contained within anaphora resolution program 136.  One such rule may direct tuple constructor 142 to find the identified anaphor within a given span of natural language text, and scan to the right of the\nanaphor to find the identified E. Another rule may direct tuple constructor 142 to scan to the left of the anaphor to find the identified AE.  Based upon the order, or pattern, of an AE(s), E(s), and anaphor(s) for a given span of natural language text,\ntuple constructor 142 constructs a set of tuples.  Tuple constructor 142 may compare a set of constructed tuples to the training tuples in tuple database 128.\n In the exemplary embodiment, tuple constructor 142 may construct multiple sets of tuples where there is more than one AE in the span of parsed natural language text that is associated with the anaphor.  With reference to EXAMPLE 2, \"8/20 lymph\nnodes had evidence of metastatic spread.  Tumors were noted in the brain and liver.  They measured 6 cm and 5 cm.\", tuple constructor 142 may construct the following sets of tuples: Tuple1 (tumors, 6 cm, they), Tuple2 (tumors, 5 cm, they), Tuple3 (lymph\nnodes, 6 cm, they), Tuple4 (lymph nodes, 5 cm, they), Tuple5 (brain, 6 cm, they), Tuple6 (brain, 5 cm, they), Tuple7 (liver, 6 cm, they), and Tuple8 (liver, 5 cm, they).  Tuples 5-8 may be eliminated, or filtered, based on training tuples in tuple\ndatabase 128 which indicate that brain and liver are typically not associated with a measurement.  On the other hand, training tuples in tuple database 128 may indicate that tumors and lymph nodes are typically associated with a measurement.  Therefore,\nthe anaphor, they, potentially links 6 cm and 5 cm to multiple candidate AEs, tumors and lymph nodes.  As such, disambiguation logic is required in order to resolve which candidate AE they refers to.\n Anaphora resolver 144 includes a set of programming instructions in anaphora resolution program 136, to resolve the anaphor by determining which entity of the multiple entities is referenced by the anaphor, using the constructed set of tuples,\nand selecting an AE among one or more candidate AEs.  The set of programming instructions is executable by processor 132.\n In an exemplary embodiment, anaphora resolver 144 may determine, using the constructed set of tuples from tuple constructor 142, that there are multiple candidate AEs which the anaphor may reference, in relation to an entity under consideration. Based on this determination, anaphora resolver 144 may resolve the anaphor in a variety of ways.  One such method to resolve the anaphor may include performing disambiguation logic.  In an exemplary embodiment, performing disambiguation logic may include\nmachine learning using probabilistic reasoning to determine which AE, among multiple candidate AEs, an anaphor references in relation to an E under consideration.\n With continued reference to EXAMPLE 1: \"8/20 lymph nodes had evidence of metastatic spread.  Tumors were noted in the brain and liver.  They measured 6 cm and 5 cm\", anaphora resolver 144 may utilize machine learning to resolve the anaphor They\nas referring to either lymph nodes or tumors.\n Machine learning generally refers to algorithms that can learn from and make predictions on data.  These algorithms operate differently from strictly static program instructions by making data-driven predictions or decisions, through building a\nmodel from sample inputs, called training data.  In embodiments of the invention, machine learning may include the process of analyzing initial training data that includes sets of training sentences, also referred to as sentence clusters.  For each\nsentence cluster in the initial training data, a corresponding set of training tuples contains at least one AE known to be correct (i.e. the AE is known to be related to an E via an anaphor).\n As more sentence clusters and their corresponding sets of training tuples (constructed based on known information) are processed and analyzed by the machine learning engine of anaphora resolution program 136, the program extracts features from\nthe sentence clusters, and determines their corresponding weights.  The weighted features are used to generate a predictive statistical data model that can be applied to new sentence clusters for which no known information exists.  The statistical data\nmodel is a practical tool for predicting anaphora resolution in these new sentence clusters.  The statistical data model may include a set of training tuples, which can be added to tuple database 128, for future use as predictors in resolving anaphora\nambiguities in a span of parsed natural language text containing constructed tuples with candidate AEs, where it is not immediately known which candidate AE is the correct AE.\n In the exemplary embodiment, anaphora resolver 144 may employ machine learning to identify a set of features in a span of parsed natural language text comprising constructed tuples corresponding to the initial set of training tuples in tuple\ndatabase 128.  A set of features may refer to identifying characteristics between a related pair of entities in a constructed tuple.  For example, a set of training tuples in tuple database 128 may include the entity pairings &lt;tumors,\nmeasurement:cm&gt;, and &lt;lymph nodes, measurement:count&gt;.  In this example, each of the tumor and lymph node entities is paired with an entity of type measurement.  However, a feature of these pairings is that for tumor, the measurement is of type\ncm, whereas for lymph nodes, the measurement is of the type count.  Anaphora resolver 144 may generate a statistical data model using a machine learning engine based on these features (i.e., tumor measurements are frequently stated in sentence clusters\nusing a metric unit, and lymph nodes are frequently stated in sentence clusters using an integer count).  The frequency with which these features occur can be built into the statistical model using weighting techniques.\n Based on the identified pairs of related entities (AE and E) from the initial set of training tuples in tuple database 128, anaphora resolver 144 may resolve an anaphor having no known resolution.  In EXAMPLE 2, for instance, the anaphor they\nmay be resolved using initial training data as follows.  Anaphora resolver 144 may determine that the Es under consideration are measurement entities 6 cm and 5 cm.  These measurement entities are under consideration because they are related to they,\nwhich is an anaphor without a known AE.  For example, it is not self-evident whether they refers to tumors or to lymph nodes.  If the statistical data model, generated using the initial training data, strongly suggests that tumors are measured in\ncentimeters but lymph nodes are not, then anaphora resolver 144 may determine that the more likely AE to be correct, among the candidate AEs, is tumors, not lymph nodes.  Note that this analysis is a technical processing of textual data as it is stored\nand processed on a computer.  The statistical model takes into account how natural language text is expressed in electronic documents.  There is no guarantee about the veracity of the information.  The solution is purely technical and limited to the\nfield of electronic information processing.\n In another embodiment, the feature that may have classified tumors as the correct candidate AE in one instance, and lymph nodes as the correct candidate AE in another instance may be that tumors co-occurred in a span of parsed natural language\ntext with entities that identified as being measured, while lymph nodes co-occurred in a span of parsed natural language text with entities that identified as being counted.\n In the exemplary embodiment, anaphora resolver 144 may utilize relevance feedback to ask the user which AE is more suitable to resolve the current anaphor for a given entity, or list of entities, in a span of parsed natural language text. \nRelevance feedback of a user may be used to check the accuracy of the program by confirming, distinguishing, clarifying, or negating an identified AE by anaphora resolution program 136.\n In the example embodiment, anaphora resolver 144 may expand the initial training data of tuple database 128 based on the information obtained either from performing disambiguation logic or utilizing relevance feedback, or both.  Expanding the\ninitial training data of tuple database 128 increases the ability of anaphora resolution program 136 to disambiguate instances of anaphora with greater accuracy and without the oversight of an SME.\n In the example embodiment, network 102 is a communication channel capable of transferring data between connected devices and may be a telecommunications network used to facilitate telephone calls between two or more parties comprising a landline\nnetwork, a wireless network, a closed network, a satellite network, or any combination thereof.  In another embodiment, network 102 may be the Internet, representing a worldwide collection of networks and gateways to support communications between\ndevices connected to the Internet.  In this other embodiment, network 102 may include, for example, wired, wireless, or fiber optic connections which may be implemented as an intranet network, a local area network (LAN), a wide area network (WAN), or any\ncombination thereof.  In further embodiments, network 102 may be a Bluetooth network, a WiFi network, or a combination thereof.  In general, network 102 can be any combination of connections and protocols that will support communications between\ncomputing device 110, database server 120, and data processing system 130.\n FIGS. 2A-2E are flowcharts depicting operational steps of a method for directing a computer processor to resolve an anaphor in electronic natural language text, according to an embodiment of the present invention.  The methods of FIGS. 2A-2E are\ndiscussed further in reference to an illustrative example.\n Referring now to FIGS. 1 and 2A, anaphora resolution program 136, via processor 132, detects a plurality of entities and an anaphor in a parsed natural language text comprising one or more sentences (step 202).  For example, with reference to\nEXAMPLE 1, anaphora resolution program 136 receives the span of text in EXAMPLE 1, and detects the anaphor they and its entities; i.e., tumor, brain, liver, 6 cm, 5 cm, based on these entities appearing in medical database 122.  As a further example,\nwith reference to EXAMPLE 2, anaphora resolution program 136 receives the span of text in EXAMPLE 2, and detects the anaphor they, and some if its other entities; i.e., 8/20, lymph nodes, metastatic spread, tumors, brain, liver, 6 cm, and 5 cm.  Some\nentities may be filtered out, i.e., not considered, if they are not included in medical database 122 (or filtered based on other criteria).\n With continued reference to FIGS. 1 and 2A, anaphora resolution program 136, via processor 132, extracts pairs of related entities among the plurality of entities, based on a domain knowledge.  Domain knowledge includes a collection of known\npairs of related entities, or a logical knowledge graph including known pairs of related entities, or a set of natural language statements about the domain, or any combination thereof.  (step 204).  For example, with reference to EXAMPLE 1, anaphora\nresolution program 136 extracts &lt;tumors, 6 cm&gt;, &lt;tumors, 5 cm&gt;, &lt;tumors, brain&gt;, and &lt;tumors, liver&gt; as pairs of related entities among the plurality of entities, based on known related entity pairs from domain knowledge database\n124.  The related entity pairs may have been automatically generated or manually selected by an SME before being stored in domain knowledge database 124.  As a further example, with reference to EXAMPLE 2, anaphora resolution program 136 extracts\n&lt;lymph nodes, 8/20&gt;, &lt; lymph nodes, metastatic spread&gt;, &lt;tumors, 6 cm&gt;, &lt;tumors, 5 cm&gt;, &lt;tumors, brain&gt;, &lt;tumors, liver&gt;.\n With continued reference to FIGS. 1 and 2A, anaphora resolution program 134, via processor 132, constructs a set of tuples, wherein each tuple is a data type comprising an anaphor, an AE appearing before the anaphor in the natural language text,\nand an E appearing after the anaphor in the natural language text, wherein the anaphor refers to the AE and relates the AE to the E, and wherein the constructing is based on initial training data and the extracted pairs of related entities.  The initial\ntraining data includes a set of training tuples constructed using a set of training sentences, each tuple including an anaphor in the set of training sentences, an antecedent entity (AE) in the set of training sentences, and an entity (E) in the set of\ntraining sentences, wherein the anaphor is known to reference the AE, and the anaphor is known to relate the AE to the E. (step 206).  For example, with reference to EXAMPLE 1, anaphora resolution program 136 constructs a set of tuples from the span of\nnatural language text, including an E, an AE, and an anaphor that relates the E to the AE, i.e. (6 cm, tumors, they) and (5 cm, tumors, they).  The constructed set of tuples are based on known related entity pairs from domain knowledge database 124 and\nknown anaphora relationships between related entity pairs, as may have been automatically generated or manually selected by an SME before being stored in tuple database 128.  As a further example, with reference to EXAMPLE 2, anaphora resolution program\n136 constructs a set of tuples from the span of natural language text, i.e. (8/20, lymph nodes, they), (6 cm, tumors, they), (5 cm, tumors, they).\n With continued reference to FIGS. 1 and 2A, anaphora resolution program 134, via processor 132, resolves the anaphor by determining which entity in the plurality of entities the anaphor references, using the constructed set of tuples, and\nselecting an AE among one or more candidate AEs (step 208).  For example, with reference to EXAMPLE 1, anaphora resolution program 136 resolves the anaphor they as referring to tumors.  Tumors is the only AE with a known relationship with both 6 cm and 5\ncm.  As a further example, with reference to EXAMPLE 2, anaphora resolution program 136 resolves the anaphor they as referring to tumors, as opposed to lymph nodes, since tumors are associated with a measurement such as 6 cm and 5 cm and lymph nodes are\nassociated with a count such as 8/20.\n Referring now to FIGS. 1 and 2B, anaphora resolution program 134, via processor 132, determines, using the constructed set of tuples, that the anaphor potentially refers to only one candidate AE (step 210).  For example, with reference to\nEXAMPLE 1, anaphora resolution program 134 determines that they potentially refers to only tumors.  There are no other candidate AEs in this example.\n Based on the determination that the anaphor potentially refers to only one candidate AE, anaphora resolution program 134 determines a first instance of the only one candidate AE, appearing prior to the anaphor, as the entity in the plurality of\nentities to which the anaphor refers (step 212).  With continued reference to EXAMPLE 1, anaphora resolution program 134 determines a first instance of tumors by searching backwards from the location of the anaphor, they.\n Referring now to FIGS. 1 and 2C, anaphora resolution program 134, via processor 132, determines, using the constructed set of tuples, that there are multiple candidate AEs which the anaphor may reference, in relation to an entity under\nconsideration (step 214).  For example, with reference to EXAMPLE 2, anaphora resolution program 134 determines that they potentially refers to tumors or lymph nodes, the two candidate AEs in this example.\n Based on the determination that there are multiple candidate AEs which the anaphor may reference, anaphora resolution program 134 performs disambiguation logic to determine which entity, among the multiple candidate AEs, the anaphor references\nin relation to the entity under consideration (step 216).  With continued reference to EXAMPLE 2, anaphora resolution program 134 performs disambiguation logic to determine which candidate AE, tumors or lymph nodes, the anaphor, they, references in\nrelation to the measurements 6 cm and 5 cm.  Initial training data indicates that measurements can be associated with tumors, lymph nodes, or margins.  Anaphora resolution program 134 searches backwards from they to find tumors in the prior sentence, and\nlymph nodes two sentences back.  Anaphora resolution program 134 then performs disambiguation logic to determine whether tumors or lymph nodes is the referenced AE.  The entity margins are eliminated because it is not found within the proximity, at least\ntwo or three sentences, of the anaphor, they.  Tuple database 128 denotes that lymph nodes are connected to its count, while tumors are connected to a measurement (such as cm).  Therefore, they (anaphor) connects 6 cm and 5 cm (Es) to tumors (candidate\nAE), not lymph nodes (candidate AE).\n Referring now to FIGS. 1 and 2D, anaphora resolution program 134, via processor 132, identifies a set of features in a set of training data comprising training tuples corresponding to a training set of natural language text, each training tuple\ncomprising an unresolved anaphor, a set of candidate AEs, and an E, wherein at least one candidate AE is known to resolve the unresolved anaphor (step 218).  For example, with reference to EXAMPLE 1 and EXAMPLE 2, anaphora resolution program 134\nidentifies that tumors in training data may co-occur in sentences with entities that identify as tissue-based organs, while lymph nodes may co-occur in sentences with entities that identify as muscle-based organs.\n With continued reference to FIGS. 1 and 2D, anaphora resolution program 134, via processor 132, generates a statistical model of the training tuples wherein the statistical model is based on features of the training tuples that most accurately\npredict the at least one candidate AE known to resolve the unresolved anaphor (step 220).  With reference to EXAMPLE 2, anaphora resolution program 134 generates a percentage of likelihood that tumors is associated with 6 cm and 5 cm versus lymph nodes\nbeing associated with 6 cm and 5 cm, based on assigning values to features of the respective candidate AEs.  In EXAMPLE 2, tumors being located in a proximity sentence to entities naming tissue-based organs may be a 45% predictor that tumors is the\ncandidate AE known to resolve the unresolved anaphor.\n In another embodiment, anaphora resolution program 134, via processor 132, generates a statistical model of the training tuples wherein the statistical model is further based on features of the training tuples that most accurately predict at\nleast one candidate AE known not to resolve the unresolved anaphor.\n In an exemplary embodiment, performing disambiguation logic further includes applying the statistical model to the constructed set of tuples to select an AE among the one or more candidate AEs.\n With continued reference to FIGS. 1 and 2D, anaphora resolution program 134, via processor 132, performs disambiguation logic by applying the statistical model to the constructed set of tuples to select an AE among the one or more candidate AEs\n(step 222).  In an exemplary embodiment, anaphora resolution program 134 generates scores for the constructed set of tuples and selects a candidate AE among the one or more candidate AEs based on the selected candidate AE having a score within a\nthreshold of a desired score.\n Referring now to FIGS. 1 and 2E, anaphora resolution program 134, via processor 132, obtains relevance feedback, from a user, to evaluate the AE selected among the one or more candidate AEs (step 224).  With reference to EXAMPLE 2, anaphora\nresolution program 134 generates a percentage of likelihood that tumors is associated with 6 cm and 5 cm versus lymph nodes being associated with 6 cm and 5 cm, based on assigning values to features of the respective candidate AEs.  In EXAMPLE 2, tumors\nbeing located in a proximity sentence to entities naming tissue-based organs may be a 45% predictor that tumors is the candidate AE known to resolve the unresolved anaphor.\n With continued reference to FIGS. 1 and 2E, anaphora resolution program 134, via processor 132, updates the statistical model based on the relevance feedback (step 226).\n With continued reference to FIGS. 1 and 2E, anaphora resolution program 134, via processor 132, expands the initial training data based on the information obtained either from performing disambiguation logic or utilizing relevance feedback, or\nboth (step 228).\n FIG. 3 is a block diagram depicting components of a computing device (such as computing device 110, database server 120, or data processing system 130, as shown in FIG. 1), in accordance with an embodiment of the present invention.  It should be\nappreciated that FIG. 3 provides only an illustration of one implementation and does not imply any limitations with regard to the environments in which different embodiments may be implemented.  Many modifications to the depicted environment may be made.\n Computing device 110 may include one or more processors 902, one or more computer-readable RAMs 904, one or more computer-readable ROMs 906, one or more computer readable storage media 908, device drivers 912, read/write drive or interface 914,\nnetwork adapter or interface 916, all interconnected over a communications fabric 918.  Communications fabric 918 may be implemented with any architecture designed for passing data and/or control information between processors (such as microprocessors,\ncommunications and network processors, etc.), system memory, peripheral devices, and any other hardware components within a system.\n One or more operating systems 910, and one or more application programs 911, such as anaphora resolution program 136, may be stored on one or more of the computer readable storage media 908 for execution by one or more of the processors 902 via\none or more of the respective RAMs 904 (which typically include cache memory).  In the illustrated embodiment, each of the computer readable storage media 908 may be a magnetic disk storage device of an internal hard drive, CD-ROM, DVD, memory stick,\nmagnetic tape, magnetic disk, optical disk, a semiconductor storage device such as RAM, ROM, EPROM, flash memory or any other computer-readable tangible storage device that can store a computer program and digital information.\n Computing device 110 may also include a R/W drive or interface 914 to read from and write to one or more portable computer readable storage media 926.  Application programs 911 on computing device 110 may be stored on one or more of the portable\ncomputer readable storage media 926, read via the respective R/W drive or interface 914 and loaded into the respective computer readable storage media 908.\n Computing device 110 may also include a network adapter or interface 916, such as a TCP/IP adapter card or wireless communication adapter (such as a 4G wireless communication adapter using OFDMA technology).  Application programs 911 on\ncomputing device 110 may be downloaded to the computing device from an external computer or external storage device via a network (for example, the Internet, a local area network or other wide area network or wireless network) and network adapter or\ninterface 916.  From the network adapter or interface 916, the programs may be loaded onto computer readable storage media 908.  The network may comprise copper wires, optical fibers, wireless transmission, routers, firewalls, switches, gateway computers\nand/or edge servers.\n Computing device 110 may also include a display screen 920, a keyboard or keypad 922, and a computer mouse or touchpad 924.  Device drivers 912 interface to display screen 920 for imaging, to keyboard or keypad 922, to computer mouse or touchpad\n924, and/or to display screen 920 for pressure sensing of alphanumeric character entry and user selections.  The device drivers 912, R/W drive or interface 914 and network adapter or interface 916 may comprise hardware and software (stored on computer\nreadable storage media 908 and/or ROM 906).\n The programs described herein are identified based upon the application for which they are implemented in a specific embodiment of the invention.  However, it should be appreciated that any particular program nomenclature herein is used merely\nfor convenience, and thus the invention should not be limited to use solely in any specific application identified and/or implied by such nomenclature.\n Referring now to FIG. 4, illustrative cloud computing environment 50 is depicted.  As shown, cloud computing environment 50 includes one or more cloud computing nodes 10 with which local computing devices used by cloud consumers, such as, for\nexample, personal digital assistant (PDA) or cellular telephone 54A, desktop computer 54B, laptop computer 54C, and/or automobile computer system 54N may communicate.  Nodes 10 may communicate with one another.  They may be grouped (not shown) physically\nor virtually, in one or more networks, such as Private, Community, Public, or Hybrid clouds as described hereinabove, or a combination thereof.  This allows cloud computing environment 50 to offer infrastructure, platforms and/or software as services for\nwhich a cloud consumer does not need to maintain resources on a local computing device.  It is understood that the types of computing devices 54A-N shown in FIG. 4 are intended to be illustrative only and that computing nodes 10 and cloud computing\nenvironment 50 can communicate with any type of computerized device over any type of network and/or network addressable connection (e.g., using a web browser).\n Referring now to FIG. 5, a set of functional abstraction layers provided by cloud computing environment 50 (FIG. 4) is shown.  It should be understood in advance that the components, layers, and functions shown in FIG. 5 are intended to be\nillustrative only and embodiments of the invention are not limited thereto.  As depicted, the following layers and corresponding functions are provided:\n Hardware and software layer 60 includes hardware and software components.  Examples of hardware components include: mainframes 61; RISC (Reduced Instruction Set Computer) architecture based servers 62; servers 63; blade servers 64; storage\ndevices 65; and networks and networking components 66.  In some embodiments, software components include network application server software 67 and database software 68.\n Virtualization layer 70 provides an abstraction layer from which the following examples of virtual entities may be provided: virtual servers 71; virtual storage 72; virtual networks 73, including virtual private networks; virtual applications\nand operating systems 74; and virtual clients 75.\n In one example, management layer 80 may provide the functions described below.  Resource provisioning 81 provides dynamic procurement of computing resources and other resources that are utilized to perform tasks within the cloud computing\nenvironment.  Metering and Pricing 82 provide cost tracking as resources are utilized within the cloud computing environment, and billing or invoicing for consumption of these resources.  In one example, these resources may include application software\nlicenses.  Security provides identity verification for cloud consumers and tasks, as well as protection for data and other resources.  User portal 83 provides access to the cloud computing environment for consumers and system administrators.  Service\nlevel management 84 provides cloud computing resource allocation and management such that required service levels are met.  Service Level Agreement (SLA) planning and fulfillment 85 provide pre-arrangement for, and procurement of, cloud computing\nresources for which a future requirement is anticipated in accordance with an SLA.\n Workloads layer 90 provides examples of functionality for which the cloud computing environment may be utilized.  Examples of workloads and functions which may be provided from this layer include: mapping and navigation 91; software development\nand lifecycle management 92; virtual classroom education delivery 93; data analytics processing 94; transaction processing 95; and controlling access to data objects 96.\n The present invention may be a system, a method, and/or a computer program product at any possible technical detail level of integration.  The computer program product may include a computer readable storage medium (or media) having computer\nreadable program instructions thereon for causing a processor to carry out aspects of the present invention.\n The computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device.  The computer readable storage medium may be, for example, but is not limited to, an electronic\nstorage device, a magnetic storage device, an optical storage device, an electromagnetic storage device, a semiconductor storage device, or any suitable combination of the foregoing.  A non-exhaustive list of more specific examples of the computer\nreadable storage medium includes the following: a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a static random access memory (SRAM), a\nportable compact disc read-only memory (CD-ROM), a digital versatile disk (DVD), a memory stick, a floppy disk, a mechanically encoded device such as punch-cards or raised structures in a groove having instructions recorded thereon, and any suitable\ncombination of the foregoing.  A computer readable storage medium, as used herein, is not to be construed as being transitory signals per se, such as radio waves or other freely propagating electromagnetic waves, electromagnetic waves propagating through\na waveguide or other transmission media (e.g., light pulses passing through a fiber-optic cable), or electrical signals transmitted through a wire.\n Computer readable program instructions described herein can be downloaded to respective computing/processing devices from a computer readable storage medium or to an external computer or external storage device via a network, for example, the\nInternet, a local area network, a wide area network and/or a wireless network.  The network may comprise copper transmission cables, optical transmission fibers, wireless transmission, routers, firewalls, switches, gateway computers and/or edge servers. \nA network adapter card or network interface in each computing/processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium\nwithin the respective computing/processing device.\n Computer readable program instructions for carrying out operations of the present invention may be assembler instructions, instruction-set-architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, firmware\ninstructions, state-setting data, configuration data for integrated circuitry, or either source code or object code written in any combination of one or more programming languages, including an object oriented programming language such as Smalltalk, C++,\nor the like, and procedural programming languages, such as the \"C\" programming language or similar programming languages.  The computer readable program instructions may execute entirely on the user's computer, partly on the user's computer, as a\nstand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server.  In the latter scenario, the remote computer may be connected to the user's computer through any type of network,\nincluding a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).  In some embodiments, electronic circuitry including, for\nexample, programmable logic circuitry, field-programmable gate arrays (FPGA), or programmable logic arrays (PLA) may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to\npersonalize the electronic circuitry, in order to perform aspects of the present invention.\n Aspects of the present invention are described herein with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems), and computer program products according to embodiments of the invention.  It will be\nunderstood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer readable program instructions.\n These computer readable program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute\nvia the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.  These computer readable program instructions may also be\nstored in a computer readable storage medium that can direct a computer, a programmable data processing apparatus, and/or other devices to function in a particular manner, such that the computer readable storage medium having instructions stored therein\ncomprises an article of manufacture including instructions which implement aspects of the function/act specified in the flowchart and/or block diagram block or blocks.\n The computer readable program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other device to cause a series of operational steps to be performed on the computer, other programmable apparatus or\nother device to produce a computer implemented process, such that the instructions which execute on the computer, other programmable apparatus, or other device implement the functions/acts specified in the flowchart and/or block diagram block or blocks.\n The flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present invention. \nIn this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of instructions, which comprises one or more executable instructions for implementing the specified logical function(s).  In some alternative\nimplementations, the functions noted in the blocks may occur out of the order noted in the Figures.  For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse\norder, depending upon the functionality involved.  It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special\npurpose hardware-based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.\n Based on the foregoing, a computer system, method, and computer program product have been disclosed.  However, numerous modifications and substitutions can be made without deviating from the scope of the present invention.  Therefore, the\npresent invention has been disclosed by way of example and not limitation.", "application_number": "15666694", "abstract": " The program directs a computer processor to resolve an anaphor in\n     electronic natural language text. The program detects a plurality of\n     entities and an anaphor in a span of parsed natural language text\n     comprising one or more sentences, and extracts pairs of related entities\n     based on domain knowledge. The program constructs a set of tuples,\n     wherein each tuple is a data type comprising an anaphor, an antecedent\n     entity (AE) appearing before the anaphor in the span of parsed natural\n     language text, and an entity (E) appearing after the anaphor in the span\n     of parsed natural language text, wherein the anaphor refers to the AE and\n     relates the AE to the E. The program resolves the anaphor by determining\n     which entity in the plurality of entities the anaphor references, using\n     the constructed set of tuples, and selecting an AE among one or more\n     candidate AEs.\n", "citations": ["7475010", "7813916", "9542393", "10019434", "20050216443", "20050234709", "20080243479", "20120158400", "20150046150", "20160042058", "20160098396"], "related": []}, {"id": "20190095830", "patent_code": "10373091", "patent_name": "Systems and methods for investigating and evaluating financial crime and\n     sanctions-related risks", "year": "2019", "inventor_and_country_data": " Inventors: \nEpstein; Matthew (Santa Monica, CA), Schmidt; Benjamin (Santa Monica, CA)  ", "description": "COPYRIGHT NOTICE\n The present application includes material that is subject to copyright protection.  The copyright owner does not object to the facsimile reproduction of the application by any person as the application appears in the records of the U.S.  Patent\nand Trademark Office, but otherwise reserves all rights in the copyright.\nFIELD OF INVENTION\n The present invention is generally directed to systems and methods for investigating and evaluating financial crime and sanctions-related risks.  More particularly, the present invention is related to systems and methods involving an internal\nproduction environment configured to interact with analysts and an external production environment configured to interact with customers.  The combination of the two environments and other mechanisms help maintain and evaluate financial crime and\nsanctions-related risks.\nBACKGROUND OF THE INVENTION\n Today's banking and financial systems are subject to complex regulations that can subject those institutions to significant fines, other regulatory penalties, and reputational risk.  Various procedures and systems have been developed to assist\nthese institutions in monitoring, investigating, and avoiding risks addressed by such regulations.  Financial crime and sanctions-related risks can arise from directly or indirectly conducting or supporting financial transactions with entities under\nsanctions or otherwise engaged in illicit activity.  These regulations can be complex and can involve identified entities and other entities that fall under sanctions by association.  There are a number of existing systems but they have been found thus\nfar to be inadequate in providing the institution with a cogent and quick representation of the risk as new situations arise.  There is a tremendous amount of information, and understanding and assessing the information down to relevant parts has been\ndifficult to achieve.  The efficiency in providing such services and the way the information is interactively made available have also been inadequate.\nSUMMARY OF THE INVENTION\n In accordance with principles of the invention, a computer implemented system for providing a visual interactive software tool that permits users to investigate and evaluate financial crime and sanctions-related risks is contemplated.  In one\nembodiment, the system may comprise a graph database configured to store edges, nodes, and properties, a first subsystem configured to store and publish a style guide over a network, a document library, an internal production environment configured to\ninteract with analysts, and an external production environment configured to interact with customers.  The graph database is located in nonvolatile memory or stores nodes, edges, and properties in nonvolatile memory.  The first subsystem is implemented\non a computer configured with volatile and non-volatile memory and configured to communicate over a network.  The style guide includes a plurality of data models that are identified in the style guide to model different categories of situations and\ncontains requirements for a structure and format of data used to enter nodes, edges, and properties for a particular data model.  The document library is also implemented on a computer configured with volatile and non-volatile memory and to communicate\nover a network.  The document library can communicate with different sources to receive information to be reviewed by analysts and save received information that is used by analysts to enter nodes, edges, and properties.  Each of the internal production\nenvironment and the external production environment is implemented on one or more servers configured with volatile and non-volatile memory and configured to communicate over a network.  The internal production environment can communicate with analysts'\ncomputers over a network.  The external production environment can communicate with customers' computers over a network.\n The internal production environment implements an electronic online workspace for analysts.  The internal production environment configures the electronic online workspace to provide analysts with a software interface to enter a first category\nof information to identify and input individual nodes and to enter a second category of information specifying edges (i.e., different types of relationships) between the nodes in the first category.  The inputted nodes include source nodes and corridor\nendpoint nodes.  The internal production environment, in response, stores the entered nodes, edges, and properties in the graph database.\n The internal production environment also comprises a software engine that finds corridors.  The software engine, in response to the entered nodes, edges, and properties, automatically traverses pathways which are available starting from each\nsource node through connected edges and nodes.  The software engine identifies any new pathways and traverses the pathways until it reaches a corridor endpoint node before a maximum number of degrees of node-traversal as specified by the software engine\nhas been reached, or it reaches a node or edge in the traversal process that has no risk relevance or low risk relevance for the purpose of corridor generation.\n The software engine then eliminates some of the identified pathways.  The elimination process includes eliminating pathways that include the nodes or edges that have no risk relevance or low risk relevance for the purpose of corridor generation\nand the pathways that include a total number of nodes at or above the maximum number of degrees of node-traversal.  The software engine stores a set of weights in correspondence with the different types of edges and assigns weights based on the type of\nedge or connected node to the edges in each pathway.  In response to eliminating pathways and assigning weights, the software engine identifies the remaining pathways to be the resulting corridors for the source node and further based on a cumulative\nvalue of weights on edges in each resulting corridor specifies a variable degree of relevance to the resulting corridors.\n The system also comprises a second subsystem configured to perform a publish process.  The publish process approves and transmits the resulting one or more corridors and the nodes, edges, and properties to an external production environment. \nThe second subsystem is implemented on a computer configured with volatile and nonvolatile memory to communicate over a network.\n The external production environment is configured to receive and store the one or more resulting corridors and the nodes, edges, and properties in an external graph database which aggregates corridors and nodes, edges, and properties and\nmaintains edge connections between nodes in different corridors.\n The external production environment comprises an electronic online customer platform.  The platform includes a visual interactive interface having interactive tools.  The tools include a search engine that provides keyword searching capability\nthat surfaces possible matching nodes in the external graph database, that provides the user with the ability to add an identified node to the visual workspace, that generates a visual graphic as representation of the identified node in the workspace,\nthat communicates that the identified node has a number of connections (connected nodes) that include a number of corridors, and that permits the user to select one of the connections or corridors to add to the visual workspace.  In response to adding\nthe connection or corridor to the visual workspace, the engine displays additional nodes and edges that are part of the connection or corridor using separate visual graphical elements for nodes and edges that visually illustrate the connection or\ncorridor.\n The external production environment can communicate a relative relevance of a plurality of corridors based on the weights.  The external production environment is further configured to display underlying properties for the nodes and edges by\nretrieving the underlying properties from the external graph database.\n The electronic online customer platform is further configured to permit the customer to add a node to the workspace that already contains the identified node.  In response to the addition, the workspace automatically displays a graphic\nrepresentation of a connection between the added node and the identified node, if a relationship between the two nodes exists.\n The electronic online customer platform is further configured to permit the customer to add a corridor to the workspace that already contains the identified node.  In response to the addition, the workspace automatically displays graphic\nrepresentations of nodes and edges of the added corridor.\n The electronic online customer platform is further configured to provide the customer with option to interactively select a node displayed on the workspace.  In response to the selection, the workspace communicates corridors associated with the\nselected node that can be added to the workspace.\n The electronic online customer platform is further configured to provide the customer with option to interactively select a node displayed on the workspace.  In response to the selection, the workspace communicates other nodes associated with\nthe selected node that can be added to the workspace.\n In accordance with principles of the invention, a computer implemented method for investigating and evaluating financial crime and sanctions-related risks is contemplated.  In one embodiment, the method comprises storing edges, nodes, and\nproperties in a graph database, storing and publishing a style guide over a network, receiving information in a document library to be reviewed by analysts and saving received information in the document library that is used by analysts to enter nodes,\nedges, and properties, implementing an internal production environment configured to interact with analysts, and implementing an external production environment configured to interact with customers.  The graph database is located in nonvolatile memory. \nThe style guide is stored in and published via a first subsystem implemented on a computer configured with volatile and non-volatile memory and to communicate over a network.  The style guide includes a plurality of data models that are identified in the\nstyle guide to model different categories of situations and contains requirements for a structure and format of data used to enter nodes, edges, and properties for a particular data model.  The document library is also implemented on a computer\nconfigured with volatile and non-volatile memory and to communicate over a network.  The document library can communicate with different sources to receive information to be reviewed by analysts and save received information that is used by analysts to\nenter nodes, edges, and properties.  Each of the internal production environment and the external production environment is implemented on one or more servers configured with volatile and non-volatile memory and to communicate over a network.  The\ninternal production environment can communicate with analysts' computers over a network.  The external production environment can communicate with customers' computers over a network.\n The internal production environment implements an electronic online workspace for analysts.  The workspace is configured to provide analysts with a software interface to enter a first category of information to identify and input individual\nnodes or groups of nodes and to enter a second category of information specifying edges (i.e., different types of relationships) between the nodes in the first category.  The inputted nodes include source nodes and corridor endpoint nodes.  In response\nto entering the first and second category of information, the method stores the entered nodes, edges, and properties in the graph database.\n The internal production environment implements a software engine that finds corridors.  In response to the stored nodes, edges, and properties, the method, via the software engine, automatically traverses pathways which are available starting\nfrom each source node through connected edges and nodes.  The method identifies, via the software engine, any new pathway and traverses the pathways until it reaches a corridor endpoint node before a maximum number of degrees of node-traversal as\nspecified by the software engine has been reached, or it reaches a node or edge in the traversal process that has no risk relevance or low risk relevance for the purpose of corridor generation.\n The method then eliminates, via the software engine, some of the identified pathways.  The elimination process includes eliminating pathways that include the nodes or edges that have no risk relevance or low risk relevance for the purpose of\ncorridor generation and the pathways that include a total number of nodes at or above the maximum number of degrees of node-traversal.  The method stores, via the software engine, a set of weights in correspondence with the different types of edges and\nassigns weights based on the type of edge or connected node to the edges in each pathway.  In response to eliminating pathways and assigning weights, the method, via the software engine, identifies the remaining pathways to be the resulting corridors for\nthe source node and further based on a cumulative value of weights on edges in each resulting corridor specifies a variable degree of relevance to the resulting corridors.\n The method also includes a publication process that approves and transmits the resulting one or more corridors and the nodes, edges, and properties to an external production environment.  The publication process is performed by a second\nsubsystem that is implemented on a computer configured with volatile and non-volatile memory and to communicate over a network.\n The method also includes receiving and storing, via the external production environment, the one or more resulting corridors and the nodes, edges, and properties in an external graph database which aggregates corridors and nodes, edges, and\nproperties and maintains edge connections between nodes in different corridors.\n The method includes implementing an electronic online customer platform in the external production environment.  The platform provides a visual interactive interface that comprises a visual workspace and interactive tools.  The tools include a\nsearch engine that is configured to provide keyword searching capability that surfaces possible matching nodes in the external graph database, that provides the user with the ability to add an identified node to the visual workspace, that generates a\nvisual graphic as representation of the identified node in the workspace, that communicates that the identified node has a number of connections that include a number of corridors, and that permits the user to select one of the connections or corridors\nto add to the visual workspace.  In response to adding the connection or corridor to the visual space, the method, via the engine, displays additional nodes and edges that are part of the connection or corridor using separate visual graphical elements\nfor nodes and edges that visually illustrate the connection or corridor.\n The method further comprises communicating, via the external production environment, a relative relevance of a plurality of corridors based on the weights.\n The method further comprises displaying, via the external production environment, underlying properties for the nodes and edges by retrieving the underlying properties from the external graph database.\n The method further comprises permitting, via the electronic online customer platform, the customer to add a node to the workspace that already contains the identified node.  In response to the addition, the method automatically displays, via the\nelectronic online customer platform, a graphic representation of a connection between the added node and the identified node, if a relationship between the two nodes exists.\n The method further comprises permitting, via the electronic online customer platform, the customer to add a corridor to the workspace that already contains the identified node.  In response to the addition, the method automatically displays, via\nthe electronic online customer platform, graphic representations of nodes and edges of the added corridor.\n The method further comprises providing, via the electronic online customer platform, the customer with option to interactively select a node displayed on the workspace.  In response to the selection, the method communicates, via the electronic\nonline customer platform, corridors associated with the selected node that can be added to the workspace.\n The method further comprises providing, via the electronic online customer platform, the customer with option to interactively select a node displaced on the workspace.  In response to the selection, the method communicates, via the electronic\nonline customer platform, other nodes associated with the selected node that can be added to the workspace.\n In accordance with principles of the invention, a computer readable data medium storing computer-executable instructions that, when executed by a processor, cause the processor to perform a method for investigating and evaluating financial crime\nand sanctions-related risks is contemplated.  The method comprises establishing a graph database configured to store edges, nodes, and properties in nonvolatile memory, storing and publishing a style guide, receiving information in a document library to\nbe reviewed by analysts and saving received information in the document library that is used by analysts to enter nodes, edges, and properties, communicating with an internal production environment configured to interact with analysts, and communicating\nwith an external production environment configured to interact with customers.  The graph database is located in nonvolatile memory.  The style guide is stored in and published via a first subsystem implemented on a computer configured with volatile and\nnon-volatile memory and to communicate over a network.  The style guide includes a plurality of data models that are identified in the style guide to model different categories of situations and contains requirements for a structure and format of data\nused to enter nodes, edges, and properties for a particular data model.  The document library is also implemented on a computer configured with volatile and nonvolatile memory and to communicate over a network.  The document library can communicate with\ndifferent sources to receive information to be reviewed by analysts and save received information that is used by analysts to enter nodes, edges, and properties.  Each of the internal production environment and the external production environment is\nimplemented on one or more servers configured with volatile and non-volatile memory and to communicate over a network.  The internal production environment can communicate with analysts' computers over a network.  The external production environment can\ncommunicate with customers' computers over a network.\n The internal production environment implements an electronic online workspace for analysts.  The workspace is configured to provide analysts with a software interface to enter a first category of information to identify and input individual\nnodes or groups of nodes and to enter a second category of information specifying edges (i.e., different types of relationships) between the nodes.  The inputted nodes include source nodes and corridor endpoint nodes.  In response entering the first and\nsecond category of information, the method stores the entered nodes, edges, and properties in the graph database.\n The internal production environment implements a software engine that finds corridors.  In response to the stored nodes, edges, and properties, the method, via the software engine, automatically traverses pathways which are available starting\nfrom each source node through connected edges and nodes.  The method identifies, via the software engine, any new pathway and traverses the pathways until it reaches a corridor endpoint node before a maximum number of degrees of node-traversal as\nspecified by the software engine has been reached, or it reaches a node or edge in the traversal process that has no risk relevance or low risk relevance for the purpose of corridor generation.\n The method then eliminates, via the software engine, some of the identified pathways.  The elimination process includes eliminating pathways that include the nodes or edges that have no risk relevance or low risk relevance for the purpose of\ncorridor generation and the pathways that include a total number of nodes at or above the maximum number of degrees of node-traversal.  The method stores, via the software engine, a set of weights in correspondence with the different types of edges and\nassigns weights based on the type of edge or connected node to the edges in each pathway.  In response to eliminating pathways and assigning weights, the method identifies, via the software engine, the remaining pathways to be the resulting corridors for\nthe source node and further based on a cumulative value of weights on edges in each resulting corridor specifies a variable degree of relevance to the resulting corridors.\n The method also includes a publication process that approves and transmits the resulting one or more corridors and the nodes, edges, and properties to an external production environment.  The publication process is performed by a second\nsubsystem that is implemented on a computer configured with volatile and non-volatile memory and to communicate over a network.\n The method also includes receiving and storing, via the external production environment, the one or more resulting corridors and the nodes, edges, and properties in an external graph database which aggregates corridors and nodes, edges, and\nproperties and maintains edge connections between nodes in different corridors.\n The method includes implementing an electronic online customer platform in the external production environment.  The platform provides a visual interactive interface that comprises a visual workspace and interactive tools.  The tools include a\nsearch engine that is configured to provide keyword searching capability that surfaces possible matching nodes in the external graph database, that provides the user with the ability to add an identified node to the visual workspace, that generates a\nvisual graphic as representation of the identified node in the workspace, that communicates that the identified node has a certain number of connections that include a number of corridors, and that permits the user to select one of the connections or\ncorridors to add to the visual workspace.  In response to adding the connection or corridor to the visual workspace, the method, via the engine, displays additional nodes and edges that are part of the connection or corridor using separate visual\ngraphical elements for nodes and edges that visually illustrate the connection or corridor.\n The method further comprises communicating, via the external production environment, a relative relevance of a plurality of corridors based on the weights.\n The method further comprises displaying, via the external production environment, underlying properties for the nodes and edges by retrieving the underlying properties from the external graph database.\n The method further comprises permitting, via the electronic online customer platform, the customer to add a node to the workspace that already contains the identified node.  In response to the addition, the method automatically displays, via the\nelectronic online customer platform, a graphic representation of a connection between the added node and the identified node, if a relationship between two nodes exists.\n The method further comprises permitting, via the electronic online customer platform, the customer to add a corridor to the workspace that already contains the identified node.  In response to the addition, the method automatically displays, via\nthe electronic online customer platform, graphic representations of nodes and edges of the added corridor.\n The method further comprises permitting, via the electronic online customer platform, the customer with option to interactively select a node displayed on the workspace.  In response to the selection, the method communicates, via the electronic\nonline customer platform, corridors associated with the selected node that can be added to the workspace. BRIEF DESCRIPTION OF THE DRAWINGS\n The nature and various advantages of the present invention will become more apparent upon consideration of the following detailed description, taken in conjunction with the accompanying drawings, in which like reference characters refer to like\nparts throughout, and in which:\n FIG. 1 depicts an illustrative computer-implemented system for providing a visual interactive software tool that permits user to investigate and evaluate financial crime and sanctions-related risks in accordance with some embodiments of the\npresent invention;\n FIG. 2 depicts illustrative nodes, properties, and an edge and their respective roles in accordance with some embodiments of the present invention;\n FIG. 3 depicts illustrative corridors in accordance with some embodiments of the present invention;\n FIG. 4 depicts an illustrative process for finding corridors in accordance with some embodiments of the present invention;\n FIG. 5 depicts illustrative steps that determine whether pathways have satisfied certain automated rules to be qualified as corridors in accordance with some embodiments of the present invention;\n FIG. 6 depicts illustrative visual graphical elements with color indicating a particular type of node and symbol indicating a particular type of subject in accordance with some embodiments of the present invention;\n FIGS. 7-11, 13, 14A, 15, and 16 depict illustrative screenshots of a graphical user interface, an interactive window(s), and related displayed features generated by the system through which the user interacts on their computer or PC to use the\nadvantages provided by the system in accordance with some embodiments of the present invention;\n FIG. 12 depicts additional illustrative corridors and an illustrative corridor edge in accordance with some embodiments of the present invention;\n FIG. 14B depicts an illustrative structure for constructing the visual graphic of an illustrative insight in accordance with some embodiments of the present invention;\n FIG. 17 depicts another illustrative computer-implemented system for providing a visual interactive software tool that permits user to investigate and evaluate financial crime and sanctions-related risks in accordance with some embodiments of\nthe present invention;\n FIG. 18 depicts additional illustrative corridors that have certain types of relationships in accordance with some embodiments of the present invention; and\n FIG. 19 depicts an illustrative visual graphic of one or more identified nodes in accordance with some embodiments of the present invention.\nDETAILED DESCRIPTION OF THE INVENTION\n In accordance with principles of the present invention, a system is provided that can efficiently identify for users direct and indirect relationships between an actor that does not appear on a sanctions list and other actors that do appear on a\nsanctions list using, among other things, a graph database and a style guide.  The style guide allows many analysts to incrementally add to the data in the graph database in a consistent, structured, and connected way.  As new nodes are added to the\nsystem, a computer implemented process is implemented that traverses pathways from the new node to a node that is identified on a sanctions list and which relies on rules that identify only a subset of pathways for use by the customers.  The system\neliminates pathways that have a degree of separation at or above a predetermined threshold and implements other rules such as rules eliminating pathways that include nodes or edges that have no risk relevance or low risk relevance for the purpose of\ncorridor generation.  The resulting pathways that are found are considered \"corridors\" and are saved by the system in the graph database.  Thus, for example, in some embodiments, the system incrementally adds corridors and stores them instead of\ntraversing the entire graph.  This technique also supports an interactive graphical interface of the system which provides an interactive tool for users to quickly and cogently understand risk using a visualization of one or more corridors.  The\ninterface provides a visual workspace that allows the user to search by keywords and the interface, in response, lists search results for possible matching nodes in the database.  The interface further permits the user to select a node from the list of\nsearch results (e.g., a person) and in response, the interface or system generates a related list that includes a list of corridors (pathways in the graph database from that starting node to a node that appears on a sanctions list, also referred to as a\ncorridor endpoint node) and other connected nodes under subject groupings.  The user can interact with the interface to select a corridor and add it to the workspace.  The user can select other corridors from the list and add to the workspace and display\nit.  This provides a powerful and elegant solution to the needs of the institution with respect to financial crime and sanctions compliance.  The interface permits the user to select individual nodes that are in the workspace and in response, lists of\nrelated information including corridors that are associated with that node, and other connected nodes, and the user can similarly add those to the workspace.  This technique quickly adapts the source data and in operation, quickly surfaces the relevant\ninformation in a visual way for understanding by those in charge of evaluating financial crime and sanctions compliance issues at institutions.  The known prior art is not able to accomplish such powerful results.\n Embodiments of the present invention are directed to a software platform that allows analysts and customers to research and map the commercial, financial, and facilitation networks of sanctioned or other actors that may be associated with\nillicit activity.  Through embodiments to the present invention, financial institutions, multinational corporations, legal professionals, and other stakeholders can access visual graphs depicting relationships between sanctioned and non-sanctioned actors\nin order to evaluate their possible exposure to financial crime or sanctions-related risks.  Embodiments illustrated herein are directed to financial crime and sanctions-related solutions and have particular suitability to solving and assisting users\nwith financial crime and sanctions compliance related issues.  It should be understood that described features or systems can have broader or different applications.  Sanctions refers to laws or regulations issued by a government that places commercial\nrestrictions on certain identified entities or entities that are described in more general terms or indirectly (e.g., all subsidiaries of an entity that is specifically identified) and also place restrictions on commercial activities involving the\nrestricted entities.  Individuals, companies, or organizations that violate the restrictions are subject to civil or criminal penalties, which can be significant source of risk, typically arising from being a primary or secondary actor in a financial\ntransaction.  In the United States, government organizations involved in issuing and enforcing such regulations include the Department of the Treasury, the Department of State, and the Department of Justice, but others exist in the U.S.  and in other\ncountries.\n FIG. 1 depicts an illustrative computer-implemented system 100 for providing a visual interactive software tool that permits users to evaluate financial crime and sanctions-related risks.  The system 100 comprises a graph database 105 configured\nto store nodes, edges, and properties a document library 110 configured to communicate with different sources over a network to receive financial crime and sanctions-related risk information and save received information used by analysts to create nodes,\nedges, and properties, an internal production environment 115 configured to interact with analysts, and an external production environment 120 configured to interact with customers.  The graph databases 105 include an internal graph database and an\nexternal graph database.  The internal graph database is configured to store nodes, edges, and properties inputted into the internal production environment 115 by analysts, store nodes, edges, and properties for access and use by analysts in the internal\nproduction environment 115, and store other information entered and produced from the internal production environment 115.  The information produced from the internal production environment 115 may be information created using the nodes, edges, and\nproperties (e.g., connections and corridors).  The external graph database is configured to store nodes and edges for customers' use in the external production environment 120 and store visual graphs produced and saved by customers in the external\nproduction environment 120.  The graph database 105 may be a component separate from the internal production environment 115 and the external production environment 120 or be integrated with the internal production environment 115 and the external\nproduction environment 120 (e.g., with the internal graph database built into the internal production environment 115 and with the external graph database built into the external production environment 120).  The system 100 is configured to logically\ndivide (e.g., by a mechanism such as user authentication executed by subsystem 155) between the internal graph database and the external graph database, wherein customers are not permitted to access or interact with nodes or edges in the internal graph\ndatabase.\n A node represents a subject.  The system 100 is configured to allow for the subject to be an individual, entity, postal address, e-mail address, event, number, telephone number, campaign, or other subject.  The subject may be one that appears on\na sanctions list or one that has a criminal record (sanctioned subject), one that is materially associated with a sanctioned subject but does not appear on a sanctions list (associated subject), or one that is neutrally associated with a sanctioned\nsubject or an associated subject and does not appear on a sanctions list (neutral subject).  A node is described by properties that include one or more words, numbers, symbols, or a combination thereof by which the underlying subject is known or referred\nto.  When the subject (or property) is a number, the number may be a phone number, fax number, driver license number, social security number, passport number, bank account number (including credit card and debit card account numbers), identification\nnumber used by the system 100, or other number.\n An edge represents a relationship between two nodes.  The relationship may be a business relationship (e.g., creditor, supplier, joint-venture, etc.), an ownership relationship (e.g., majority shareholder, minority shareholder, subsidiary,\netc.), a position relationship (e.g., director, manager, owner, etc.), a banking relationship (e.g., account holder, mortgagor, etc.), a familial relationship (e.g., father, daughter, cousin, etc.), and other relationships.  An edge is described by\nproperties that include one or more words, numbers, symbols, or a combination thereof by which the underlying subject is known or referred to.  FIG. 2 depicts an illustrative node, property, and edge and their respective roles.  FIG. 2 depicts a\ngraphical structure of the node, property, and edge that is presented to user when he or she assembles them through the workspace in the internal or external production environment.  The graph database 105 stores nodes, edges, and information from the\nenvironments 115, 120 in nonvolatile memory.  Nodes and edges are part of a graph database and can also have a graphical representation as part of a user interface.\n The internal production environment 115 includes an electronic online workspace for analysts 130 and a software engine 135.  The workspace 130 provides a software interface that allows an analyst to enter a first category of information to input\nindividual nodes or group of nodes.  The nodes may be inputted into or created in the internal production environment 115 from information stored in the document library 110.  Analysts can access the internal production environment 115 over network\nconnections as over an Internet connection.  The document library 110 communicates with different data sources over a network to receive financial crime and sanctions-related information.  Data sources may include government sanctions lists, court\nrecords, records of incorporation, corporate filings, shipping records, news, websites, public social media postings, multi-media, and other sources.  An analyst may review the stored information, determine subjects from the stored information, and\nenters each determined subject and relevant information into the internal production environment 115 to create or input a node and related properties.  The information relied on by the analyst to create nodes can be saved in the document library so it is\navailable for review by the analyst and other analysts and users.  In this way, analysts and users can check accuracy of the entered nodes and properties, and review the actual received information directly (instead of just the nodes, edges, and\nproperties) if necessary.  For example, the received information may include an article from a newspaper (or other source), a link to the article, or information about the article, and the analysts and users can review that article.  An analyst may also\nobtain financial crime and sanctions-related information from places other than the document library and use the internal production environment 115 to merely input or create nodes.  Once an inputted node is identified by the system or an analyst as\nappearing on a sanctions list, the system designates it as a corridor endpoint node.  The designation imparts to the inputted node different characteristics and effects that enables the software engine 135 to find paths and corridors (further discussed\nbelow) accordingly and that allows the electronic online platform of the external production environment 120 to generate a visual graphic as a distinctive representation.  A node can also be created to represent an article or other data that serves as a\nsource for additions made by an analyst.  This type of node is configured not to have a graphical node inserted into a workspace.\n The software interface also allows the analyst to enter a second category of information specifying an edge and related properties between inputted nodes.  The software interface provides a list of available edges, and the analyst can select and\nspecify an edge from the list.  The analyst may also create additional edges from the software interface if they are not in the list.  In some embodiments, inputting a certain node and/or specifying the type and direction of the relationship between two\nnodes may cause the system 100 to automatically select a certain relationship between that certain inputted node and another inputted node.  The relationship selected by the system 100 may be a mandatory relationship.  In some embodiments, specifying a\ncertain relationship may cause the system 100 to automatically change or specify the type of inputted node (e.g., source or corridor endpoint node).  The type of node selected by the system 100 may be a mandatory type of node.\n The first category of information and the second category of information are stored in the graph database 105 and can be accessed by the workspace 130.\n The internal production environment 115 or the system 100 includes a first subsystem that is configured to store and provide a style guide.  The style guide includes a plurality of data models defined by analysts or administrators to model\ndifferent situations and contains requirements for a structure and format of data used to enter nodes, edges, and properties for a particular data model.  For example, the situations may include arrest, investigation, and litigation situations, and the\ndata model can specify how the information of each situation should be entered into the internal production environment 115.  For instance, in an arrest situation, the data model may specify the types of nodes that should be used, the edges that should\nbe used for each type of node, the properties that should be used for each type of node and edge, and the format that the node, edge, and property should be entered to clearly convey the information represented by the nodes, edges, and properties.  For\nexample, the data model may require the analyst to enter the nodes, edges, and properties in an \"Arrest of [Person A] in [Country], who is [relationship] of [Person B]\" format with the nodes, edges, and properties inserted in the corresponding bracket. \nThe text in the format (e.g., \"Arrest of,\" who is,\" and \"of\") may be entered by the analyst or automatically generated by the environment 115 when the analyst selects the data model of the arrest situation to be used for the information he is about\nenter.  The data model may have similar specifications for investigation, litigation, and other situations.  In this way, analysts can follow the same rules and requirements for each situation and build the information for each situation in the same\nmanner.\n A data model can govern what and how each piece of information is entered into the internal production environment 115 in each situation, relate the entered information to a node in the graph database 105 based on information in the entered\ninformation, and determine how the entered information should be linked to or integrated with the node if the entered information is related.  In some embodiments, a data model may be available as a template for entering the first and/or second category\nof information.  In some embodiments, an analyst can enter the first and/or second category of information and the system 100 can automatically select the appropriate data model to add or integrate the information in the data model into the graph\ndatabase 105.  The data model may include the corresponding fields and the required data structure and format so that the system 100 or the graph database 105 can be built and maintained uniformly across all analysts.  The data models can also be created\nfor other situations.\n The development and publication of the style guide can be an important component of the system 100.  A rigorous set of requirements and definitions can be specified in the style guide for a wide range of situations, subjects, or relationships. \nThis approach embeds structured consistency in the analyst-created nodes, edges, properties, graphs, or related information (the creation of which may involve many different analysts and can involve many different sources of information or types of\nsources) that permits the database to grow efficiently.  The system 100 can, in using the created information, automatically identify edges or relationships between subjects.  This can for example result in avoiding having multiple representations of the\nsame subject in the database which can create disconnects or result in duplicate or redundant paths.  Other advantages are also gained from such a system.  Note that this is one option and in some embodiments other variations can be implemented.\n The system is configured to allow collaboration.  The system can be configured to have many analysts interact and save their work on the system.  The system can be configured to allow analysts to see the work of other analysts on the system and\nto build on that work (efficiently building a larger and more complex set of information).  The system can be configured to allow analysts to view the work saved by other analysts or can have restrictions in place that limit the access of analysts to\nnodes, edges, or properties that have been approved and added to the external production environment.  For example, the internal production environment 115 allows analysts to collaborate and build a database storing information that visually identifies\ndirect and indirect relationships between an actor that does not appear on a sanctions list and other actors that do appear on a sanctions list.  Analysts (e.g., other analysts) can then add additional actors or nodes to the database to continue the\ndirect and indirect relationships and/or new actors or nodes to the database to create new direct and indirect relationships that are not already in the database.  The internal production environment 115 also includes an approval process for new\ninformation that is being added by an analyst.  The approval process can be configured as an automated process involving a workflow in which the contributing analyst specifies to the system that a new set of subjects or relationships are being added to\nthe system and in response, an automated approval process is implemented that can involve approval by peers through the system or by supervisory reviewers.\n The software engine 135 includes a set of computer implemented rules (generally referred to as rules) that are configured to traverse paths and find (\"surface\") a subset of the paths that meet certain rules (and therefore, eliminate other\npaths).  The subset that meets the rules is referred to herein as corridors.  The software engine 135 is configured to find or identify corridors based on the stored first and second category of information.  A corridor is a pathway connecting any node\nthat is not a corridor endpoint node to a node that is designated as a corridor endpoint node via one or more edges and/or one or more connecting nodes.  A connecting node can be any node except a node designated as a corridor endpoint node and/or other\nnodes deemed to have no (or low) illicit finance or sanctions-related risk relevance for the purpose of corridor generation.  As a matter of clarification, at the time an analyst creates a new node for a subject, the system 100 may not be configured to\nrequire that the analyst specifies whether it is a corridor endpoint node or some other kind of node (e.g., a node that has no risk relevance or low risk relevance for the purpose of corridor generation, or any further specificity with respect to the\ntype of node can for example be determined by the software engine 135 based on the edges or paths connected to that node).  At a base level, a corridor is determined (found) by identifying a node (that is not a corridor endpoint node) from the first\ncategory of information and connecting the identified node to a designated corridor endpoint node in the first category of information via a direct edge connection or via other nodes in the first category of information and the edges in the second\ncategory of information.  Rules are incorporated into the software engine 135 that traverses or finds paths from identified nodes to designated corridor endpoint nodes and finds corridors as a subset of all paths where a path from an identified node can\nbe traversed by the software engine 135 to reach a designated corridor endpoint node.  In operation, the software engine 135 automatically traverses from the identified node through connected nodes and edges until it reaches a designated corridor\nendpoint node.  A corridor endpoint node is a type of node that is specified or designated within the system as having different properties than other nodes.  A corridor endpoint node cannot be the starting point of a corridor and is only available to\nend a pathway or corridor.  The corridor endpoint node is defined by the system 100 (e.g., by rules, configurations, or data structures) to function that way with respect to identified nodes.  A corridor is found when the identified node is connected to\na corridor endpoint node and also certain rules are satisfied.  A corridor can be graphically illustrated by the system 100 (e.g., as part of the graphical user interface) using a graphical structure comprising the connected nodes, edges and corridor\nendpoint node that form the corridor.  Subject to meeting certain additional rules (discussed herein) a corridor can be identified by the software engine 135 whenever an identified node can reach a corridor endpoint node.  Multiple corridors can be\ngenerated for an identified node.  The multiple corridors may be pathways between an identified node and a corridor endpoint node, between an identified node and multiple corridor endpoint nodes, or a combination thereof.\n FIG. 3 depicts illustrative corridors 350, 355, 360, 364, 368.  As mentioned above, a corridor is a pathway between a source node and a corridor endpoint node that meets certain rule requirements.  For discussion purposes, in connection with\nFIG. 3, corridors are discussed in more general terms and additional specificity is discussed further below.  The corridors 350, 355, 360 represent pathways between a source node 305 and a corridor endpoint node 310.  The corridor 350 includes an edge\n315 joining the source node 305 and a connecting node 320, a connecting node 320, and an edge 325 joining the connecting node 320 and the corridor endpoint node 310.  The corridor 355 is an edge 357 between the source node 305 and corridor endpoint node\n310.  The corridor 360 includes an edge 330 joining the source node 305 and a connecting node 335, a connecting node 335, an edge 340 joining the source node 335 and a connecting node 345, a connecting node 345, and an edge 348 joining the connecting\nnode 345 and the corridor endpoint node 310.  The corridor 364 represents a pathway between the same source node 305 and a different corridor endpoint node 370.  The corridor 368 represents a pathway between the same source node 305 and another different\ncorridor endpoint node 375.  The software engine 135 may not generate corridors for some of the source nodes since not all the source nodes can be connected to a corridor endpoint node.\n The software engine 135 is programmed with a maximum number of degrees of node-traversal.  The maximum number of degrees of node-traversal refers to the number of nodes in a pathway the software engine 135 is permitted (by rule) to traverse to\nreach a corridor endpoint node from a particular source node.  The software engine 135 will end the node traversal, exits the traversed pathway, and eliminates that traversed pathway from qualifying as a corridor when the software engine 135 determines\nthat the current traversed node is equal to the maximum number of degrees of node-traversal.  The software engine 135 is configured to eliminate a pathway when the number of nodes in the path (after the source node) reaches a maximum.  The maximum can be\nspecified internal to the system and not otherwise visible to users or analysts when interacting with the system.  The maximum can for example be selected to be one of 4, 5, 6, 7, or 8 nodes, which can have advantages in processing, relevancy, and\npresentation.  Other maximums are contemplated.  With this rule, the software engine 135 prevents pathways that exceed the maximum number of degrees of node-traversal from qualifying to be corridors.  If the software engine 135 has reached the maximum\nnumber and no corridor endpoint node is found, then the software engine 135 moves onto traversing another pathway (e.g., which can start from the same source node again if that source node has other pathways or from the next source node) to find a\ncorridor endpoint node.  Since the graph database 105 may store a large number of nodes and edges, their combinations may produce an enormous number of pathways.  The maximum number can control the number of iteration performed by the software engine 135\nto save processing time and power consumption.  For brevity, the term maximum number of degrees of node-traversal is sometimes referred to or abbreviated as the maximum number, the maximum number of node traversal, the maximum number of node traversal or\ndegrees of traversal, or the maximum number of traversal nodes.\n FIG. 4 depicts an illustrative process 400 for finding corridors.  In addition to the programmed maximum number, the software engine 135 is configured to apply a process that identifies and eliminates pathways that include a node in the path\n(between a source node and a corridor endpoint node) that has been designated as having no risk relevance or low risk relevance for the purpose of corridor generation when traversing through that node to a corridor endpoint node.  The determination by\nthe system of which nodes (or edges) are ones that have no risk relevance or low risk relevance for the purpose of corridor generation can be performed in one or more ways (or combinations thereof) that are described herein as illustrative examples.  A\nnode (or edge) can be designated as having no risk relevance or low risk relevance by an analyst via the electronic online analyst workspace.  The software engine 135 can also determine whether a node is one that has no risk relevance or low risk\nrelevance, without an analyst identifying it as such.  The software engine 135 can make the determination by analyzing the edge connecting the two nodes, the properties of the edge and two nodes, the number of edges the node has, or a combination\nthereof, and determining that the inclusion of the node, or the path associated with the node, would indicate a neutral or primarily neutral relationship between the two nodes.  If desired, a software module that incorporates artificial intelligence or a\nset of rules can be implemented in some embodiments that determines whether a node or edge has no or low risk relevance for the purpose of corridor generation.  Low risk relevance is mentioned to explain that a low level of relevance would still not be\nsufficient to reach a threshold value for the purpose of corridor generation.  The software engine 135 is configured to recognize nodes that have no risk relevance or low risk relevance (e.g., government node).\n This configuration improves the technical and informational aspects of the resulting corridors.  For example, a node for a government entity is removed by this rule because its inclusion provides very limited or no relevance for the purpose of\nevaluating financial crime and sanctions-related risks for two distinct nodes that connect through the government node.  In operation, in one embodiment, the system implemented a process in which as part of the automated node traversal process, it\ndetermines whether each node is identified as a type of node that the system has stored with a designation that this type of node has no risk relevance or low risk relevance for the purpose of corridor generation.  If the system identifies a match, the\nsystem would eliminate that pathway and would discontinue traversing that pathway.  The node and related connection would be available and displayed (if desired) in a workspace to provide additional information to a user.\n FIG. 5 depicts illustrative steps 405 and 410 of FIG. 4.  In this example, a low maximum number (e.g., 3) is used for explanatory purposes.  FIG. 5 shows a graphical illustration of the process in order to further elaborate about its operation. \nIf desired, the graphical illustration could be incorporated into an interactive graphical interface.  In FIG. 5, node 505 may have been recently added to the internal production environment and approved to be added to the external production\nenvironment.  At this point, for example, the system processes the addition as part of the already existing nodes and edges in the already existing external production environment.  The system will automatically start at node 505 and traverse connections\n(e.g., one by one, at the same time, or some variation thereof) and applies automated rules to select corridors.  In this case, node 505 has connections to node 510 (a node on a sanctions list and thus a corridor endpoint node) through nodes 520, 521,\n525, 527, 530, 532, 534, 522, 524, and 529.  The software engine 135 would traverse connected edges and nodes until reaches node 510 or the node traversal is terminated before it reaches node 510 when the software engine 135 determines that the current\npath it is traversing has reached a maximum number of nodes before reaching a corridor endpoint node or a node or edge (reached in the traversal process) that is in the category of nodes or edges that have no risk relevance or low risk relevance for the\npurpose of corridor generation.  In the illustration, for convenience, lines 552, 554, 556, 558, 560, and 562 are provided as a way to mark and discuss an associated set of nodes and edges that (based on information stored in the environment) establish a\npath from node 505 to node 510.  Those lines are markers for discussion purposes.  For example, in this case, using step 405 of FIG. 4, the software engine 135 identifies pathways, referenced using associated markers, 552, 554, 556, 558, 560, 562 that\nconnect node 505 to corridor endpoint node 510 and eliminates pathway 558 because pathway 558 involves more than 3 connecting nodes (e.g., 520, 530, 532, and 534).  In operation, the process would stop the traversal process when it reaches the maximum\nnumber of traversal nodes (3 in this example) and the system would not identify or \"know\" that there is a pathway 558 to node 510 because it would have stopped the traversal process when it reached the maximum (i.e., node 532).  By performing step 410 of\nFIG. 4, the software engine 135 identifies that node 522 has no risk relevance or low risk relevance for the purpose of corridor generation, when it reaches that node in the node traversal process, and eliminates pathway 560 associated with node 522. \nSimilarly, in operation, the process would stop the traversal process when it reaches the node 522 (determines node has no risk relevance or low risk relevance for the purpose of corridor generation) and at that point the system would not identify or\n\"know\" that there is a pathway 560 to node 510 because it would have stopped the traversal process when it reached node 522.  As a result, the software engine 135 keeps only pathways 552, 554, 556 and 562 after performing steps 405 and 410 and the kept\npathways are determined to be corridors associated with node 505 in relation to node 510 (since there can be other relations).  Corridors are automatically added to the external production environment when or in response to being found by the software\nengine 135.  Other implementations are also contemplated.\n The predetermined node type for non-traversal, the type of node that the process should identify and use to eliminate path being traversed (no risk relevance or low risk relevance), and the predetermined number of connecting nodes (maximum\nnumber of node traversal or degrees of traversal) may be inputted by the analyst into the software interface but is preferably pre-programmed in the software engine 135 (in the implementation of the overall system).\n The process 400 (FIG. 4) further comprises determining a degree of relevance for the corridors 415.  The step 415 comprises storing a set of weights in correspondence with the different types of edges 420, assigning weights based on the type of\nedges in each corridor 425, calculating a cumulative value of weights for each found corridor based on the assigned weights 430, and specifying a degree of relevance to other corridors for each corridor 435 based on the cumulative value.  The types of\nedges may be based on whether the relationship is a shareholder one, an employment one, a banking one, a familial one, or other one and/or the rank in the relationship (majority shareholder vs.  minority shareholder, director vs.  associate, son vs. \ncousin, etc.).  Each relationship type and rank has an assigned weight, and certain relationships types and ranks have a weight higher than other relationship types and ranks.  Higher weight may refer to a higher assigned number or a higher multiplier in\nthe calculation step 430 or the formula used in the calculation step 430.  The calculation step 430 may be based on adding all the assigned weights of the connecting edges in a corridor.  The total sum or cumulative value may be used as the only\nparameter in the specifying step 435 to determine the degree of relevance.  The cumulative value may also be used with other parameters or in another procedure to determine the relevance in step 435.  A higher cumulative value may represent that the\ncorridor has a higher relevance compared to other corridors.  In some embodiments, a lower cumulative value may represent that the corridor has a higher relevance compared to other corridors.  The degree of relevance may be output as a number on a scale\nconfigured to convey such degree.  For example, the corridor with the strongest degree of relevance may be marked with \"1\", the corridor with the second strongest degree of relevance may be marked with \"2,\" and so forth.  Other numbers, letter,\ncharacters, or orders (e.g., from large number to small number) may also be used.  The number may be different from the cumulative value.  The system can be configured to only assign weights that are equal or greater than 1 (with 1 indicating most\nrelevant and ascending numbers, totals, indicating lower relevance, in relation to a weight of 1).\n If desired as an option, the step 415 may further comprise a step 440 that eliminates corridors based on relevance determined from calculating weights.  The step 440 may eliminate corridors based on assigned weights in step 425 and before the\ncalculation step 430.  In this case, steps 430 and 435 calculate a cumulative value and specify a degree of relevance for the remaining corridors.  The step 440 may eliminate corridors based on the calculated cumulative value in step 430.  The step 440\nmay eliminate corridors that do not have a cumulative value that is above, below, or matches a predetermined cumulative value.  In this case, step 435 specifies a degree of relevance for the remaining corridors.  The predetermined cumulative value may be\nprovided by analyst from software interface or be pre-programmed in the software engine 135 and be subsequently modified to a different number by analyst from the software interface.\n The step 415 may be performed after the step 405 is executed, after the step 410 executed, or after both steps 405 and 410 are executed.  In any of these scenarios, the step 415 may operate to determine a degree of relevance for corridors kept\nby step 405 and/or 410 without the step 440.  The step 415 may also operate to further eliminate one or more corridors kept by step 405 and/or 410 and determine a degree of relevance for the remaining corridors.  In other words, the step 440 may be\noptional in some embodiments.\n In some embodiments, the step 415 may be configured to perform before steps 405 and/or 410.  In that situation, steps 405 and/or 410 may further eliminate corridors kept by the step 415 (whether step 415 performs the eliminating step 440 or\nnot).  In some embodiments, the step 415 may be configured as an alternative step to steps 405 and 410.  The process 400 may operate based on the step 415 to eliminate corridors without steps 405 and 410.\n All the data available to (e.g., all the predetermined numbers, assigned weights, etc.) and produced by the software engine 135 (e.g., the generated corridors, the cumulative values, kept/resulting corridors, etc.) may be stored in the internal\ngraph database.\n The system 100 includes a second subsystem that is configured to approve and provide the kept or resulting corridors and the associated nodes, edges, and properties from the internal production environment 115 (or the internal graph database) to\nthe external production environment 120 (or the external graph database) via link 133 (FIG. 1).  In one embodiment, the second subsystem may provide a multi-stage approval process.  In the first stage, the second subsystem may transmit the work added by\nan analyst (e.g., a set of new nodes and edges that are related to a sanctioned subject based on a court record) to the computers of peer analysts for peer analysts' review (which is understood to include web browser implementations, which should be\ngenerally understood with respect to other aspects as well).  The second subsystem may require a certain number of analysts to approve the work (added research) in order to advance to the next stage.  Upon approval by the required number of analysts, the\nsecond subsystem may transmit the analyst-approved work to the computer of a senior analyst for the senior analyst's review.  Upon approval by the senior analyst, the work (a particular set of nodes, edges, or properties) are provided (inserted and\nsaved) to the external production environment 120.  The approval process may have additional or fewer stages.  During any of the stages, any rejected work will not be transferred to the external production environment 120.  The kept or resulting\ncorridors are used interchangeably throughout this application to refer to corridors that remain after an eliminating step is performed, which may be step 405, 410, and/or 415.  The second subsystem is configured to automatically feed approved work to\nthe external production environment 120.  The second subsystem also provides approved work to the third subsystem.\n The external production environment 120 (FIG. 1) is configured to receive and store the resulting corridors and the associated nodes, edges, and properties in the external graph database.  The external graph database can aggregate resulting\ncorridors and associated source nodes (e.g., adding current resulting corridors and associated nodes, edges and properties to resulting corridors and associated nodes, edges, and properties previously stored in the external graph database) and maintain\nedge connections between nodes in different corridors.\n The external production environment 120 includes an electronic online customer platform 140 (and is accessible via a web portal 1044 as shown in FIG. 17) and a software engine 145.  The customer platform 140 provides a visual interactive\ninterface that allows a customer to enter a keyword search, search for nodes with properties that match the keyword search in the external graph database, add a node that matches the keyword search to an electronic workspace, view information\naccompanying a node in a side panel, and add additional nodes and/or a corridor to the workspace.  The customer platform 140 perform its functions through the software engine 145.  As the customer enters a keyword search through the interface, the\nsoftware engine 145 searches indexed properties on nodes in the external graph database and identifies nodes that match or are similar to the search query.  When the customer selects and adds an identified node to the workspace, the software engine 145\nadds the identified node to the workspace by generating and displaying a visual graphical element as representation of the identified node in the workspace.  The software engine 145 also retrieves the accompanying information (which includes properties)\nof the identified node from the external graph database and displays that information through the visual interactive interface.  The displayed or accompanying information may include the relevant information entered by analyst during the input step,\nother nodes directly connected to the identified node, and a number of corridors.  Upon the customer selecting and adding one of the corridors, the software engine 145 generates and displays a visual graphic as representation of the corridor in the\nworkspace.  The visual graphic for the corridor is generated and displayed by the nodes and edges that form the corridor.  Customers can access the external production environment 120 over network connections such as over an Internet connection.\n The software engine 145 is configured to generate and display a plurality of visual graphical elements.  The software engine 145 may generate a circle as the visual graphical element for a node (see FIGS. 2, 3, and 5).  The circle may have a\ndifferent size or color to indicate whether the node is identified as a particular type of node, including, for example, a corridor endpoint node.  The circle may be further produced with a symbol indicating a particular type of subject.  FIG. 6 depicts\nillustrative visual graphical elements 600 of a node with color 605 indicating a particular type of node and symbol 610 indicating a particular type of subject.  The software engine 145 may generate a line as the visual graphical element for an edge (see\nFIGS. 2, 3, and 5).  The line may have a different thickness, length, or color to indicate the type of relationship.  The line may also have an arrow at either end to indicate the direction of the relationship.  The visual graphic of corridor can be\ngenerated based on the above circle and line.  A separate traversal line (e.g., arrow lines in FIGS. 3 and 5) can also be provided to illustrate the path of the corridor.  Visual graphic refers to a visual representation created by one or more visual\ngraphical elements.  The software engine 145 may also generate other shapes to represent node, edge, and corridor.\n FIGS. 7-11, 13, 14A, 15, and 16 depict illustrative screenshots of a graphical user interface 700 (FIG. 7) for customers and its operation through a software engine.  Referring to FIG. 7, the graphical user interface 700 comprises a visual\ninteractive interface 705.  The visual interactive interface 705 includes a first command 710 configured to enter keyword searches on the external production environment.  Customer conducts keyword searches for a node 712 by typing letters, numbers,\nsymbols, marks, or a combination thereof in the first command 710 and the interface 705 displays the search results 715 in categories.  The categories include an entity category, a person category, an address category, or other categories.  Each category\nincludes nodes of the corresponding category that match the keyword search.  The search results include nodes that are similar or match the keyword search node 712.  The interface 705 displays the number of results (or nodes) in each category and the\ntotal number of results (or nodes) of all the categories.  The results delivered in response to the keyword search 712 can include any type of node, including any corridor endpoint node.  Customer may select a category to view all the results 720 in that\ncategory from the interface 705.  In FIG. 7, the customer selects the person category and the interface 705 displays the nodes 720 in that category in a window 722.  The interface 705 includes buttons, tabs, windows, screens, and other mechanisms to\nreceive customer selection and to display search results.  The interface 705 also includes an electronic workspace 725 to which nodes can be added and displayed via visual graphical elements.  The interface 705 is software that is implemented and running\non a computer using computer instructions that are stored in memory that, when executed by a processor, displays or provides the graphical and user-interactive features described herein (wherein the user can interact with the interface using an input\ndevice of the computer such as a keyboard).  This includes the situation where the interface 705 is provided through a web browser such as when a customer or analyst logs into the platform from their Internet browser running on their computer over an\nInternet connection to the platform.  The interface 705 is configured to implement the various related features described herein.\n From the search results 720, the customer may select a displayed node 730 to view additional information 735 about the selected node 730 as shown in FIG. 8.  Additional information 735 helps the customer decide if the selected node 730 matches\nthe keyword search 712 or the node the customer intends to investigate for sanctions-related risks.  The nodes in each category have their corresponding additional information.  For example, additional information 735 of the person category may include a\ntotal number of connections (explained below), the real name, alias, date and place of birth, nationality, resident country, sanctions status of the person, and other information.  Sanctions status information may include the jurisdiction (e.g., country\nor state) that sanctioned the person or entity represented by the node, the date the person committed the illegal act, and other information.  Additional information 735 is displayed in another window 737 configured with one or more commands 739, 740 to\nadd the selected node 730 to the electronic workspace 725.  The command 739 may allow the customer to add and place a node on the workspace 725 immediately upon clicking the command 739.  The location of the node is determined by the software engine 145. The location can be subsequently changed by the customer if needed.  The command 740 may allow the customer to drag a node onto the workspace 725.  Upon clicking and holding the mouse on the command 740, the selected node may be affixed to the mouse\ncursor and be moved along with the mouse cursor.  The location of the node is determined by the customer and the customer can move the node to a location he or she desires.  The node can be placed at the desired location upon releasing the mouse at the\ndesired location.  The node that appears on the workspace 725 is the visual graphical element of the node generated by the software engine 145.  The visual graphical element of the node is referred to as node in the description of FIGS. 7-16 for the ease\nof reading the instant application.  Adding a node to the electronic workspace 725 (visual workspace) may also be referred to as adding a node to the workspace.\n FIG. 9 depicts an illustrative visual interactive interface displaying a node 745 (a node that has been displayed on the workspace 725 by the user interacting with the interface).  The customer may select another node 747 to add to the workspace\n725.  The node 747 may be one of the connections (explained below) of the node 745 or another node searched by the customer.  The customer may view the additional information 735 and decide whether to add the node 747 to the workspace 725 via the command\n739 or 740.\n FIG. 10 depicts adding additional nodes 778, 779 to a node 745.  Upon adding a node 745, the customer may view connection information 780 of the node 745 by selecting the node 745 on the workspace 725.  In this example, the subject of the node\n745 is a person.  The connection information 780 includes nodes connected to the node 745, and the visual interactive interface 705 also displays the connected nodes in categories 780.  A connected node is a node that has an edge connection to the\ndisplayed node.  The categories are based on the relationship of the connected nodes to the node 745 (displayed node that is clicked on/selected by the user resulting on the system retrieving relationship information, instantly).  The categories for\nrelevant nodes connected to a displayed node (e.g., node 745) may include a leadership & management category, a facilitation & support category, a litigation, regulation enforcement category, a business associates category, and a family members category. The interface 705 displays the number of relevant nodes in each category and the total number of relevant nodes of all the categories.  The total number of relevant nodes of all the categories is also the total number of connections the node 745 has. \nThe interface 705 also displays the additional information 737 about the added node 745 in conjunction with the connection information 780.  The customer may select a category to view all the relevant nodes 785 in that category from the interface 705. \nIn FIG. 10, the customer selects the facilitation & support category and the interface 705 displays the relevant nodes 785 in that category in a window 787.  To clarify, in the context of the interface 705, the system permits the user to search and add\nnodes to the workspace 725 that fall in the category of nodes that have no risk relevance for the purpose of corridor generation.  These can also be found as part of connected nodes to a displayed node (e.g., when a user clicks on the displayed node and\nthe system provides relationship information).  This can add another dimension of visual information that can aid the user.  With respect to associated corridors, corridors that are retrieved and listed for node 745 when the customer selects node 745,\ncontain only the nodes or edges in resulting pathways that were identified by the node traversal process to produce corridors, which was performed before this information is made available on the external production environment.\n From the connected nodes 785, the customer may select a node 790 to view additional information 789 contained in window 738 about the node 790.  The additional information 789 includes the name of the connected node 790 and a description of the\nrelationship between the connected node 790 and the node 745.  Additional information 789 helps the customer decide if the connected node 790 should be added to the workspace 725 to view their relationship in a graphical form.  Window 738 is configured\nwith one or more commands 739, 740 to add the connected node 790 to the workspace 725.  The commands 739, 740 operate in the same manners discussed above.  Relevant nodes 778, 779 are illustrative nodes added via the steps explained above.  When the\nnodes 778, 779 are added to the workspace 725, their corresponding edge connections 761-763 to the nodes 745, 778, and 779 are also added to the workspace 725.  The edge or edge connection that appears on the workspace 725 is the visual graphical element\nof the edge or edge connection generated by the software engine 145.  The visual graphical element of an edge or edge connection is referred to as an edge connection in the description of FIGS. 7-16 for the ease of reading the instant application.  The\nnodes 745, 778, 779 and the edge connections 761-763 create a visual graphic 792.  The visual graphic 792 can be created with any number of nodes and edge connections.  The structure or shape of the visual graphic 792 can be changed by moving a node to a\ndifferent location on the workspace 725 to arrange the nodes and their relationships in different manners and to facilitate the understanding of their relationships.  When a node is being moved to another area, the corresponding edge connection remains\nconnected to the displayed node during the moving process.  When the node is moved to another area, and the edge connection is also moved to another area.\n When a node 714 has corridors, the connection information 780 also includes corridor information and corridors are stored in a corridor category for selection and display as shown in FIG. 11.  When the customer selects the corridor category, the\ninterface displays corridors 794 in a window 788.  The corridors 794 are the resulting corridors from the second subsystem.  The corridors 794 may be ranked by the determined degree of relevance and be displayed based on the rank.  The corridors 794 may\nbe displayed with the one having the highest degree of relevance first and with the remaining ones arranged in a descending order.  The corridors 794 may also be displayed in other orders.  The interface may also display the corridors 794 with a\nmeasurement (e.g., (1), (2), (3), (4)) indicating the number of connecting nodes between the displayed node and the corridor endpoint node.\n From the displayed corridors 794, the customer may select and add a corridor to the workspace 725.  The selected corridor is visually illustrated in the workspace 725.  The corridor is generated and presented with nodes and edge connections and\ntheir corresponding graphical elements.  For example, when corridor A is added, corridor A is produced with edge connection E1, connecting node C1, edge connection E2, and corridor endpoint node T1.  For another example, when corridor B is added,\ncorridor B is produced with edge connection E1 (if not already produced or if corridor A is not added yet), connecting node C1 (if not already produced or if corridor A is not added yet), edge connection E3, connecting node C2, edge connection E4, and\ncorridor endpoint node T2.  Although the node 714 is not mentioned in the above corridors, it is also part of the corridor.  The line associated with the corridor is used to illustrate the path of the corridor and it may or may not be displayed on the\ninterface.  The interface allows the customer to add one or more corridors for the node 714.  The subject of the node 714 is an entity, and the customer can evaluate financial crime and sanctions-related risks associated with the entity from the visual\ngraphics 792.\n FIG. 12 depicts additional corridors 1 and 2 and corridor edge 3 to illustrate the concept of corridors.  Corridors 1 and 2 are pathways between node A and corridor endpoint node G. Corridor edge 3 is an edge that stores corridors 1 and 2 using\nnodes and properties.  For simplicity, only the nodes involved are mentioned (not the edge connections).  Corridor 1 includes node A, connecting node B, connecting node C, and corridor endpoint node G. Corridor 2 includes node A, connecting node D,\nconnecting node E, connecting node F, and corridor endpoint node G.\n Additional information depicted in 735 (FIG. 9), 789 (FIG. 10) and connection information depicted in 780 (FIG. 10) are derived from the accompanying information or the relevant information an analyst entered during the input step discussed\nabove.\n The visual interactive interface also allows the customer to save created visual graphics 800 in the external graph database, load previous created and saved visual graphics 800 from the external graph database, and share created visual graphics\n800 with other individuals as shown in FIG. 13.  Each saved visual graphic 800 may be referred to as a chart.  The visual interactive interface also allows the customer to export created visual graphics and the accompanying information to a particular\nfile format (e.g., PDF, Word, Powerpoint, etc.) for saving or communicating with another computer system.  The visual interactive interface also allows the customer to view charts or insights specifically created by an analyst.  An insight is a specific\nresearch conducted by analyst for a particular subject or certain relationships that are illustrative, instructive, interesting, or unique.  FIG. 14a depicts an illustrative insight 900.  The insight 900 includes a short summary 905 and a visual graphic\n910 representing the summary 905.  The insight may be provided to a customer only if an insight is relevant to or contains information that has appeared in the customer's search or work history or the customer's previous inquiries.  The insight may also\nalways be provided to customers whenever it is created by analyst.\n An insight is created through an insight node with insight edges and relevant nodes and properties.  An insight node is created by an analyst in the internal production environment and may be published to the external production environment.  An\ninsight is intended to provide a brief summary so the visual graphic should have only a certain amount of complexity.  An insight node and relevant nodes selected by analyst may have a limited number of edges or may be a node having a plurality of edges\nbut only a few of them are used by analyst.  The insight node, relevant nodes, and edges are created to form a narrative or description that corresponds to the brief summary.  FIG. 14b depicts an illustrative structure for constructing the visual graphic\nof an insight.\n The interface may provide insights to customer through two locations.  The customer may access insights from the connection information 780 as shown in FIG. 15.  The connection information 780 includes an insight category, and insights\nassociated with a node will be displayed in a window 796.  Upon selecting from the displayed insights in 796, the customer may view a short summary of the selected insight in another window 797 configured with a command 798 to add its visual graphic 799\nto the workspace 725.\n The customer may also access insights from a location 930 that stores and displays insights as shown in FIG. 16 among other things, without the customer first querying a node or obtaining the connection information.  The stored insights may be\ndivided into different categories and the customer may select a category 935 to view insights 940 in that category.  Each insight 945 may be depicted as a small window 945 displaying a portion of the brief summary.  The window 945 is configured with a\ncommand 950 that can access the entire brief summary and the associated visual graphic.\n The system 100 also includes a third subsystem 150 (FIG. 1) configured with a list graph that enables bulk exporting of tailored subsets of data and a fourth subsystem 155 (FIG. 1) configured to provide user authorization and secure\ncommunications.  The list graph in the third subsystem 150 receives data from the publish flow 133 (FIG. 1) from the internal production environment 115 (FIG. 1).  The list graph in the third subsystem 150 is configured to allow internal users to apply\nadvanced query searches to identify all nodes in the list graph that meet customer specified requirements.  For example, a customer may want a list of all nodes that are identified as majority owned by corridor endpoint nodes; or for example, a customer\nmay want a list of all nodes that are identified as officers or directors of corridor endpoint nodes.  To identify nodes matching customer specified requirements, the corridor endpoint node may serve as the starting node from which the relationships are\ntraversed (rather than querying a node that is related to a corridor endpoint node and that does not appear on a sanctions list).  The certain subset of information and work produced by executing customer specified queries on the third subsystem 150 can\nbe exported in bulk to a particular type of file and/or distributed by other means.  The exportation allows individuals to access the information without using the external production environment 120.\n The fourth subsystem 155 is configured to provide user authorization and secure communication.  The fourth subsystem 155 can determine whether a user is authorized to access the system 100 and the level of access of the user.  User can be\nanalyst, customer, administrator, or other individuals.  Analyst refers to individuals who use the internal production environment (including the internal graph database) or who build a database of financial crime and sanctions-related information. \nCustomer refers to individuals who access the external production environment (including the external graph database) via a web portal or who evaluate financial crime-related and sanctions-related risks based on the information built by analyst.  Analyst\nand customer may not have access to the other's environment and graph database.  Administrator refers to individuals who monitor and maintain the system 100 to ensure that the system 100 is operating in the correct manner.  Administrator may also be\nindividuals who can grant access to analyst and customer.  The technology of the fourth subsystem 155 may be based on security API and may include protocols such as RDS MySQL, LDAP, OAuth2, and other protocols.\n FIG. 17 depicts another illustrative computer-implemented system 1000 for providing a visual interactive software tool that permits user to investigate and evaluate financial crime and sanctions-related risks.  The system 1000 comprises an\ninternal production environment 1005 and an external production environment 1010.  The internal production environment 1005 includes an analyst toolset 1022 and an internal graph database 1024.  The analyst toolset 1022 includes the electronic online\nworkspace for analysts 130 in FIG. 1.  The internal production environment 1005 also includes the software engine 135 in FIG. 1, and the software engine 135 may be part of the analyst toolset 1022.  The analyst toolset 1022 communicates with the internal\ngraph database 1024 to save and access the information and work analyst produced from the workspace 130.  The internal graph database 1024 is configured to feed or publish the information and work the analyst produced to the external production\nenvironment 1010 or external graph databases 1040, 1042 (customer graph database).  The publication process is performed by the second subsystem, and the second subsystem may be part of the internal graph database 1024 or the internal production\nenvironment 1005.  The external graph databases may include one 1040 dedicated to customer web portal 1044 and one 1042 dedicated to bulk exporting of tailored subsets of data.  Customer may log into the external production environment 1010, or the\nplatform for customers 140 shown in FIG. 1, via the portal 1044.  The communication between customer and the platform 140 and between the customer web portal 1044 and the internal graph database 1024 may be based on an application programming interface\n(API) to determine whether a customer is an authorized user and to ensure that the communications are secure.  The customer platform 140 communicates with the external graph database 1040 to access the information and work analyst produced and to save\nthe work customer produced from the platform 140.  The external graph database 1042 is configured to allow bulk exporting of tailored subsets of data or files containing the information and work analyst produced.  The lists may be exported periodically\nfrom the database 1042 and for distribution.  Exported lists allow individuals to view information and work analyst produced without the individuals keyword searching through the external production environment 1010.\n In some embodiments, databases 1040 and 1042 may be one single database configured to be accessible by customer web portal 1044 and to export lists 1046.  The external production environment 1010 also includes the software engine 145 in FIG. 1\nto perform the operations of the platform 140.\n FIG. 18 depicts additional illustrative corridors that have certain types of relationships.  The types of relationships may include ownership structures & subsidiaries, family member & affiliated companies, entities with a shared director,\nfinanciers & fundraising campaigns, bank accounts & financial transactions, entities with a shared address, and other types of relationships.\n FIG. 19 depicts an illustrative visual graphic of one or more nodes.  The complexity of the visual graphic may depend on the number of nodes and edges involved.  From FIG. 19, the customer can visually determine the relationships between\ndifferent nodes, view their properties and evaluate financial crime and sanctions-related risk.  The visual graphic can include additional nodes and edges if there are additional relevant nodes or if the customer adds more nodes.\n Each of the system 100, the internal and external production environments, the graph databases, and the subsystems in FIG. 1 can be implemented on one or more computer systems and be configured to communicate via a network connection.  They all\nmay also be implemented on one single computer system.  In either situation, the computer system(s) can communicate with analysts' computers and customers' client devices.  The computer system(s) may also be referred to as servers.  The computer\nsystem(s), the analyst's computer, and the customer's client device may adopt the following computer system.\n In one embodiment, the computer system includes a bus or other communication mechanism for communicating information, and a hardware processor coupled with bus for processing information.  Hardware processor may be, for example, a general\npurpose microprocessor.\n The computer system also includes a main memory, such as a random access memory (RAM) or other dynamic storage device, coupled to bus for storing information and instructions to be executed by processor.  Main memory also may be used for storing\ntemporary variables or other intermediate information during execution of instructions to be executed by processor.  Such instructions, when stored in non-transitory storage media accessible to processor, render computer system into a special-purpose\nmachine that is customized to perform the operations specified in the instructions.\n Computer system further includes a read only memory (ROM) or other static storage device coupled to bus for storing static information and instructions for processor.  A storage device, such as a magnetic disk or optical disk, is provided and\ncoupled to bus for storing information and instructions.\n Computer system may be coupled via bus to a display, such as a cathode ray tube (CRT), for displaying information to a computer user.  An input device, including alphanumeric and other keys, is coupled to bus for communicating information and\ncommand selections to processor.  Another type of user input device is cursor control, such as a mouse, a trackball, or cursor direction keys for communicating direction information and command selections to processor and for controlling cursor movement\non display.  This input device typically has two degrees of freedom in two axes, a first axis (e.g., x) and a second axis (e.g., y), that allows the device to specify positions in a plane.\n Computer system may implement the techniques described herein using customized hard-wired logic, one or more ASICs or FPGAs, firmware and/or program logic which in combination with the computer system causes or programs computer system to be a\nspecial-purpose machine.  According to one embodiment, the techniques herein are performed by computer system in response to processor executing one or more sequences of one or more instructions contained in main memory.  Such instructions may be read\ninto main memory from another storage medium, such as storage device.  Execution of the sequences of instructions contained in main memory causes processor to perform the process steps described herein.  In alternative embodiments, hard-wired circuitry\nmay be used in place of or in combination with software instructions.\n The term storage media as used herein refers to any non-transitory media that store data and/or instructions that cause a machine to operation in a specific fashion.  Such storage media may comprise non-volatile media and/or volatile media. \nNon-volatile media includes, for example, optical or magnetic disks, such as storage device.  Volatile media includes dynamic memory, such as main memory.  Common forms of storage media include, for example, a floppy disk, a flexible disk, hard disk,\nsolid state drive, magnetic tape, or any other magnetic data storage medium, a CD-ROM, any other optical data storage medium, any physical medium with patterns of holes, a RAM, a PROM, and EPROM, a FLASH-EPROM, NVRAM, any other memory chip or cartridge.\n Storage media is distinct from but may be used in conjunction with transmission media.  Transmission media participates in transferring information between storage media.  For example, transmission media includes coaxial cables, copper wire and\nfiber optics, including the wires that comprise bus.  Transmission media can also take the form of acoustic or light waves, such as those generated during radio-wave and infra-red data communications.\n Various forms of media may be involved in carrying one or more sequences of one or more instructions to processor for execution.  For example, the instructions may initially be carried on a magnetic disk or solid state drive of a remote\ncomputer.  The remote computer can load the instructions into its dynamic memory and send the instructions over a telephone line using a modem.  A modem local to computer system can receive the data on the telephone line and use an infra-red transmitter\nto convert the data to an infra-red signal.  An infra-red detector can receive the data carried in the infra-red signal and appropriate circuitry can place the data on bus.  Bus carries the data to main memory, from which processor retrieves and executes\nthe instructions.  The instructions received by main memory may optionally be stored on storage device either before or after execution by processor.\n The computer system also includes a communication interface coupled to bus.  Communication interface provides a two-way data communication coupling to a network link that is connected to a local network.  For example, communication interface may\nbe an integrated services digital network (ISDN) card, cable modem, satellite modem, or a modem to provide a data communication connection to a corresponding type of telephone line.  As another example, communication interface may be a local area network\n(LAN) card to provide a data communication connection to a compatible LAN.  Wireless links may also be implemented.  In any such implementation, communication interface sends and receives electrical, electromagnetic or optical signals that carry digital\ndata streams representing various types of information.\n Network link typically provides data communication through one or more networks to other data devices.  For instance, network link may provide a connection through local network to a host computer or to data equipment operated by an Internet\nService Provider (ISP).  ISP in turn provides data communication services through the world wide packet data communication network now commonly referred to as the \"Internet.\" Local network and Internet both use electrical, electromagnetic or optical\nsignals that carry digital data streams.  The signals through the various networks and the signals on network link and through communication interface, which carry the digital data to and from computer system, are example forms of transmission media.\n Computer system can send messages and receive data, including program code, through the network(s), network link and communication interface.  In the Internet example, a server might transmit a requested code for an application program through\nInternet, ISP, local network and communication interface.\n The received code may be executed by processor as it is received, and/or stored in storage device, or other non-volatile storage for later execution.\n Embodiments described herein are primarily described in the context of providing a tool for evaluating prospective financial crime and sanctions-related risks for businesses considering commercial dealings, but it should be understood that\napplications in other areas including application of particular features in other areas are contemplated.\n It is understood from the above description that the functionality and features of the systems, devices, or methods of embodiments of the present invention include generating and sending signals to accomplish the actions.\n It should be understood that variations, clarifications, or modifications are contemplated.  Applications of the technology to other fields are also contemplated.\n Exemplary systems, devices, and methods are described for illustrative purposes.  Further, since numerous modifications and changes will readily be apparent to those having ordinary skill in the art, it is not desired to limit the invention to\nthe exact constructions as demonstrated in this disclosure.  Accordingly, all suitable modifications and equivalents may be resorted to falling within the scope of the invention.\n Thus, for example, any sequence(s) and/or temporal order of steps of various processes or methods (or sequence of device connections or operation) that are described herein are illustrative and should not be interpreted as being restrictive. \nAccordingly, it should be understood that although steps of various processes or methods or connections or sequence of operations may be shown and described as being in a sequence or temporal order, but they are not necessarily limited to being carried\nout in any particular sequence or order.  For example, the steps in such processes or methods generally may be carried out in various different sequences and orders, while still falling within the scope of the present invention.  Moreover, in some\ndiscussions, it would be evident to those of ordinary skill in the art that a subsequent action, process, or feature is in response to an earlier action, process, or feature.\n It is also implicit and understood that the applications or systems illustratively described herein provide computer-implemented functionality that automatically performs a process or process steps unless the description explicitly describes\nuser intervention or manual operation.\n It should be understood that claims that include fewer limitations, broader claims, such as claims without requiring a certain feature or process step in the appended claim or in the specification, clarifications to the claim elements, different\ncombinations, and alternative implementations based on the specification, or different uses, are also contemplated by the embodiments of the present invention\n It should be understood that combinations of described features or steps are contemplated even if they are not described directly together or not in the same context.\n The terms or words that are used herein are directed to those of ordinary skill in the art in this field of technology and the meaning of those terms or words will be understood from terminology used in that field or can be reasonably\ninterpreted based on the plain English meaning of the words in conjunction with knowledge in this field of technology.  This includes an understanding of implicit features that for example may involve multiple possibilities, but to a person of ordinary\nskill in the art a reasonable or primary understanding or meaning is understood.\n Software can be implemented as distinct modules or software applications or can be integrated together into an overall application such as one that includes the user interface and that handles other feature for providing the functionality to the\nuser on their device.\n It is intended that the specification and examples be considered as exemplary only, with a true scope being indicated by the claims and their equivalents.", "application_number": "16104706", "abstract": " A research, analysis, regulatory compliance and media platform that\n     connects customers to finished research and analysis produced by subject\n     matter experts is described. The platform facilitates research,\n     investigations, and analysis by creating a single environment in which a\n     group of distributed analysts conduct research and investigations, store\n     and retrieve documents and other sources, collaborate, and publish\n     findings. Consumers are able to query a published knowledge graph,\n     surface high value relationships, and access insights captured by analyst\n     through a customer web portal or external production environment. The\n     platform allows analysts and customers to research and map the\n     commercial, financial, and facilitation networks of sanctioned or other\n     actors that may be associated with illicit activity. Customers can access\n     visual graphs depicting relationships between sanctioned and\n     non-sanctioned actors in order to evaluate their possible exposure to\n     financial crime or sanctions-related risks.\n", "citations": ["7315626", "7315826", "9116975", "20040090472", "20040193572", "20050102273", "20100223276", "20110288905", "20120137367", "20130073387", "20130144957", "20150032468", "20150154281", "20150169746", "20150227559"], "related": ["15713260"]}, {"id": "20190099653", "patent_code": "10322330", "patent_name": "Systems, devices, and methods employing the same for enhancing audience\n     engagement in a competition or performance", "year": "2019", "inventor_and_country_data": " Inventors: \nWanke; Todd (Palos Verdes Estates, CA), Cleary; James P. (San Diego, CA), Nolan; James Scott (Del Mar, CA)  ", "description": "BACKGROUND\n The following description of the background of the disclosure is provided simply as an aid in understanding the disclosed technology and is not admitted to describe or constitute prior art to the appended claims herein.\n Entertainment and Athletic events are an important part of society.  From children to teenagers to adults, a large part of modern populations participate in one or more entertaining and/or athletic events.  And for those who do not actually\nparticipate in the events, the vast majority of them, nevertheless, enjoy watching and/or listening to those who do.  Entertainment events can involve various aesthetic activities, including listening to music, viewing art or theatrical performances,\nfashion, and beauty, as well as engaging in entertaining activities, such as gambling.  Performance events, such as dance or gymnastics or ballet, and the like may be considered as both an aesthetic and an athletic event.  Sports are athletic events and\ninclude ball games, such as: baseball, football, soccer, tennis, lacrosse, and the like; sports also include man vs.  man competitions, such as those events included in the Olympics, such as track and field, boxing, judo, golf, cycling, and the like; and\nadditionally, such events can include races, such as bike, horse, car, or boat races, and the like.  Additional sporting events include water based competitions such as swimming, diving, surfing, and the like.  A couple of common threads that connects\nall of these diverse activities is both an aesthetic of expertise, the spirit of fair competition, and the fact that each performer and/or competitor, team or individual, is judged and/or the competition itself is governed by a referee.\n A problem with such judging and/or refereeing is that reasonable minds may disagree as to whether the rules were followed by the competitors, whether the performance was at a certain level, whether a ruling was accurate or not, and the like. \nThese differences can be as mild as a difference in opinion, or may indicate a discrepancy that itself may be as mild as an implicit to as worrisome as an explicit bias.  In fact, these discrepancies become even more exacerbated when the specter of bias\nrears its ugly head.\n For instance, in certain scored events where a judge is tasked with scoring a competitor with respect to their ability to perform certain activities at a given skill level, such as a musical/theatrical performance or an Olympic competition, a\nsituation sometimes occurs where the scores of one judge seems to be out of line with the scores of one or more of the other judges.  In such instances, the viewing crowd may feel that the competition has become unfair and/or may lose interest in\ncontinued viewing of the event, which can have dire consequences for the promoters, sponsors, and/or advertisers of the event as well as for the sport itself.\n More particularly, crowd engagement is not only a good metric of the entertainment's or sport's popularity, it is also a necessary component for attracting the most skilled and competent participants, as the more fan engagement there is, the\nmore advertisers will be willing to pay to sponsor such events, and the more competitors may gain by their participation in the competition.  Likewise, the more competitive the performance or competition is, the more exciting it will be to watch, and,\nthus, the more fans will want to view the event, which in turn will lead to greater amount of advertiser dollars being spent to sponsor such events in the future.  However, where there is little to no audience interaction, there is likely going to be\ndeclining audience engagement.  And further, where there is a lack of real-time transparency in the judging process, coupled with a lack of uniformity between judge scoring, the competitors are left to the mercy of the judges, and audience engagement\nsuffers, along with sponsorship, due to perceived bias.\n Such bias comes in many forms.  For example, such bias may occur when one judge has a previous relationship with a participant; where a judge wishes to curry favor with a participant; where a sponsor has an undue influence on one or more judges. Or such bias may occur from those seeking financial, political, and/or for social gain, and in view of this a judge allows his or her scoring to be directed not exclusively by the particular participants' performance, but by other, external factors. \nSuch bias, e.g., due to secrecy and a lack of transparency in judging, leaves the performers and/or competitors at the mercy of the biasness of the judges.\n Accordingly, what is needed are the means and methods for increasing viewing engagement amongst a crowd of spectators, promoting fairness and enhancing sponsorship as well as more targeted advertising, while at the same time increasing\ntransparency in competition, and decreasing bias amongst the judging faculty.  In essence, entertainments, competitions, and the judging of such, needs to be modernized in a manner that more greatly involves the viewer, the participants, sponsors, and/or\nadvertisers and their interactions, as much more intimate and intrinsic partners in the event.  The present devices, systems, as well as their methods of use have been configured to overcome these and other such problems in the art.  Accordingly, the\ngoal of the technologies described herein is to solve these and other such problems faced by event organizers, sponsors, advertisers, and/or the audience of such events, for instance, to promote audience and participant engagement and minimize bias in\nthe judging of such events.\nSUMMARY\n Hence, in various instances, implementations of various aspects of the disclosure may include, but are not limited to: apparatuses, systems, and methods of their use including one or more features as described in detail herein, as well as\narticles that comprise a tangibly embodied machine-readable medium operable to cause one or more machines (e.g., computers, etc.) to result in operations described herein.  Similarly, computer systems are also described that may include one or more\nprocessors and/or one or more memories coupled to the one or more processors.  Accordingly, computer implemented methods consistent with one or more implementations of the current subject matter can be implemented by one or more data processors residing\nin a single computing system or multiple computing systems containing multiple computers, such as in a computing or supercomputing bank.\n Such multiple computing systems can be connected and can exchange data and/or commands or other instructions or the like via one or more connections, including but not limited to a connection over a network (e.g. the Internet, a wireless wide\narea network, a local area network, a wide area network, a wired network, a physical electrical interconnect, or the like), via a direct connection between one or more of the multiple computing systems, etc. A memory, which can include a\ncomputer-readable storage medium, may include, encode, store, or the like one or more programs that cause one or more processors to perform one or more of the operations associated with one or more of the algorithms described herein.\n The details of one or more variations of the subject matter described herein are set forth in the accompanying drawings and the description below.  Other features and advantages of the subject matter described herein will be apparent from the\ndescription and drawings, and from the claims.  While certain features of the currently disclosed subject matter are described for illustrative purposes in relation to an enterprise resource software system or other business software solution or\narchitecture, it should be readily understood that such features are not intended to be limiting.  The claims that follow this disclosure are intended to define the scope of the protected subject matter.\n Accordingly, in one aspect, presented herein are systems, apparatuses, and methods for implementing an interactive, crowd-sourced judging and/or scoring platform that is configured for both enabling and encouraging audience and/or participant\nengagement when sharing in, e.g., as an observer or performer, an athletic event, theatrical performance, and/or any other form of competitive interaction or source of entertainment.  Particularly, it is understood that many people enjoy watching\ncompetitions and/or entertaining performances, however, there are others who do not, or grow bored doing so throughout the course of the event.  Hence, crowd engagement with an observed event may start off strong, but through the course of the day, as\nthe event progresses, audience participation and engagement may begin to wane.  The devices, systems, and their methods of use, as provided herein, are adapted for enhancing and improving such engagement through providing mechanisms and methods for more\nimmediate and collective fan participation in the event, thereby promoting a social collaboration between participant actors, e.g., those performing the event, and participant scorers, e.g., those informally judging the event.  More particularly, such\ninteractions are enhanced through use of real-time polling, voting, and sharing of contemporaneous results data and commentary, and the predictive analysis, and/or betting opportunities proffered by the present systems.  Additionally, the devices,\nsystems, and methods of using the same, as disclosed herein, are adapted for making the judging and/or scoring of competitive events, be they sporting events, theatrical performances, or even election results (e.g., official and/or unofficial), more\nengaging, as real-time results are obtained in an ongoing manner, but also more transparent and accurate, such as by keeping the judging and/or voting system more honest.\n Accordingly, in one aspect, presented herein is a platform for generating user interaction with a system, such as where the system is directed to engage with or present an event, such as an athletic or other entertainment event.  In these\ninstances, a user may be an spectator, e.g., a fan, a participant or performer, an event organizer, a sponsor, an advertiser, or other interested third party, any of which can be termed an observer or participant or actor, based on the contextual\ncircumstances.  Consequently, the platform is organized in a manner so as to engage a passive spectator, or other observer, and convert them to being an engaged viewer, and further into an active influencer with respect to the platform, and/or into a\npassionate fan of the platform, its activities, and the events in which it engages.  In a manner such as this a spectator not only observes but becomes a fan of the event, such as by becoming more of a participant, such as a judge, a scorer, and/or a\ncommentator of the event through the mechanisms of the platform, its systems, and devices.  In essence, in certain instances, the goal of the platform is to allow observers to become participants in the events to which they watch, such as by becoming\npart of the action, such as through liver interaction and/or communication with the platform system.\n Further, in various instances, audience engagement may be enhanced through one or more rewards, betting, and/or gambling regimes, which are directed to rewarding audience participation, actor participation, and/or judging consistency, and/or for\nallowing the audience to be more intimately engaged in the course and outcome of the performance and/or event.  Additionally, one or more of the tools set forth herein can be used to promote fairness in judging, but also may be used to better promote\nmore relevant advertisement generation and distribution, such as to better target participant consumers and reward participant actors who are sponsored.  For instance, the identity as well as the coordinates of mobile devices, and their users, are very\npowerful tools that may be employed in conjunction with the systems and their methods of use disclosed herein so as to make the often-subjective judging of athletic and/or performance events less subjective, and more objective, which in turn, can provide\na more objective measure of sponsorship performance of the actor.\n Particularly, an analysis of the representation of the participants and their performance in the event, can better quantify the results of that sponsorship, allowing them to be identified as a high performer worthy to be awarded a higher level\nof sponsorship.  Thus, bringing more money to the participant performer as well as into the event.  For example, in accordance with the present disclosure, a mobile communication device, such as a handheld cellular telephone, may be used, by an observer\nof an event, such as an athletic or performance event and/or competition, to engage in, to score, or otherwise provide commentary on the event and/or its competitors, which input can be quantified and qualified by the system, and fed back to the event\norganizers and sponsors thereby allowing them to better determine fan engagement, participant following, and their return on sponsorship investment.\n As will be described in greater detail herein below, the present devices and methods allow for the observer to be more intimately involved in the event, its participants, organizers, sponsors, and advertisers, and their interactions.  Further,\nby use of the identity of the observer, the device, and/or its coordinates, user engagement may be enhanced and/or useful information may be provided for determining bias in the judging and/or scoring of various participants in the event.  Specifically,\nin various embodiments, those identities and coordinates may be used to identify subjective information that may then be run through an analytics module, such as an Artificial Intelligence (A/I) engine, which may be configured for performing both a\nlearning function, such as through review of historic data, and to generate rules by which to determine bias in future scoring patterns, and/or predict future outcomes of such events.  Likewise, the A/I engine may include a deep learning functionality\nthat is configured for determining pattern recognition, such as with respect to patterns of actions, actors, observers, faces, and/or design elements, such as logos and/or other elements capable of being trademarked.  The agents and/or elements can be\ntracked throughout the system relative to the event, the patterns identified, quantified, and qualified, statistically analyzed, and pertinent results presented, such as results pertinent to one or more sponsors, advertisers, and/or advertisements\ngenerated.  In a manner such as this, a sponsor or advertiser can track how well its brand and brand representatives are tracked through the event, such as via the system.\n For instance, personal data of the observer of an event, e.g., which may be private or publically available data, using the interactive, e.g., scoring, device, such as a suitably configured mobile telephone; as well as personal data of the event\nparticipants, including the competitors and the judges, may be collected, searched, and may be run through a suitably configured analytics module, such as an artificial intelligence engine, to identify subjective information from various different\nsources that may be in some way correlated with one another, and therefore, may be a source of potential bias in the judging and/or scoring regime.  Particularly, the data from all various sources may be collected and organized in a structure that is\nspecifically designed to pinpoint correlations between otherwise unknown relationships.  Such a relational architecture may take many forms, such as in the form of a Structured Query Language (SQL), Hierarchical Tree, or Knowledge Graph database. \nLikewise, the system may track and collect the number of instances where a sponsored representative or sponsor identifier is observed by the system, and further track fan engagement therewith, so as to quantify and qualify that representation.\n Collected information, for example, may be run through one or more computational and/or analytics regimes, as herein described, so as to identify pertinent known or inferred data points from which various relationships between participants\nand/or sponsors engaged with the system may be determined, and/or discrepancies in their performance, scoring, and/or commentaries on an event or performance may be identified, thereby making the judging of events more \"real-time\" transparent, less\nsecretive, and any bias relatively apparent.  Specifically, as explained below, in its basic form, the system includes a network associable server that is communicably coupled to one or more user devices, upon which a user can act, so as to participate\nin a viewable event.  More specifically, a user can engage a client device, such as a mobile computing device, by pulling up a downloadable application or \"APP\", logging in their to, selecting a geographical region within which to participate, and\nfurther select an event to watch and/or otherwise engage in. The server may identify the user by the login and/or the device by its identification, as well as their relative locations.  The system may then identify if the user is within the acceptable\ngeographical region, and if so may allow the user to engage with the event, such as by using the APP to score, comment on, or otherwise participate with the platform with respect to the selected event.  Particularly, once logged on, the user can be\npresented a current competitor or performer, and can evaluate the performance, such as by entering a score or other evaluation thereof.  This may be performed for a plurality of users.  Consequently, once a number of scores for the performance of\ncompetitors in an event are collected, they may be pooled, weighted, and tallied, as described below, such as by undergoing one or more analytic protocols and/or processing by the artificial intelligence engine disclosed herein.  From these analyses,\ncorrected data, such as mean weighted or average or other statistically relevant scores and evaluations may be generated for the performer and/or the event, which data may then be fed back into the system and/or displayed on the various different client\ndevices of those participating in the event.\n Accordingly, in one aspect, presented herein is a system including an interactive, communication platform that is adaptable so as to provide for crowd-source communications while concurrently participating in, e.g., as a judge or scorer, of an\nathletic event, or performance, or other competition.  In particular embodiments, the communications platform may be configured for allowing participation in, e.g., judging and/or scoring, an activity, such as an athletic activity, competition,\nperformance, or the like, by one or more participants of an event, such as an observer, a scorer, a competitor, or any other person in the crowd having access to the technology.  In such an instance, the platform may include one or more, e.g., a\nplurality, of client application programs, e.g., running on a mobile device, and a server system through which the client applications of the mobile devices may communicate with one another and/or the system.  The system may also include one or more of\nan analytics module, for performing data analysis; and an artificial intelligence module, for generating a searchable data structure, e.g., a knowledge graph, through which data may be correlated, relationships determined or inferred, and future\nbehaviors, e.g., bias, may be predicted.  Additionally, in certain instances, a targeted advertisement module may also be associated with and/or otherwise coupled to the analytics module, e.g., an inference and/or A/I engine, and associated media\ncontaining database, so as together to form a real-time advertisement generation mechanism, whereby one or more advertisements may be generated and transmitted to one or more users of the system, e.g., based on a user profile determined for them by the\nA/I system.  As indicated, these advertisements may be generated such as based on the need or payment of sponsors that seek to increase their presence in the event, or based on its relevance to the target recipient, e.g., determined by their use of the\nsystem, and/or may be determined by geographical relevance.\n Particularly, in one configuration, the system includes a plurality of client application programs, e.g., \"APPs,\" which may be provided and distributed to a corresponding plurality of mobile devices, where each of the devices has an interactive\ndisplay, such as a capacitive sensing touch screen display.  For instance, the various processes of the system may be implemented by a computing system that includes a back-end component, e.g., as a data server, being communicably associated with a\ndatabase, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of\nthe methods of the system, or any combination of such back end, middleware, or front end components.  For example, a computer system may be provided where the computer system includes a remote server system, e.g., which may include a server and/or a\ndesktop or laptop computer, and/or a client computer, such a mobile telephone device running a client application.  Typically, the server and client are remote from each other and generally interact through a communication network.  Hence, the components\nof the system can be interconnected by any form or medium of digital data communication, e.g., a communication network.  Examples of communication networks include a local area network (\"LAN\") and a wide area network (\"WAN\"), e.g., the Internet. \nParticularly, the relationship of server and client arises by virtue of computer programs or firmware, e.g., software programs and applications, or hardware configurations, running on the respective computers and having a client-server relationship to\neach other.  In this instance, the client application may be an application program including machine instructions that, when executed by a processor cause the processor to perform certain programmed functions, as herein described.\n More particularly, the client application may be a downloadable software application that is adapted for being downloaded onto a client device, and configured for directing a host processor(s) of that device to perform one or more predetermined\nfunctions, including the presentation of a client interface, through which interface data may be inputted, transmitted, and received.  For instance, the client application may be configured for directing data transmission, through the endogenous\ncommunications module of the client device, back and forth e.g., between the device and a host server via the application.  In certain embodiments, the system is configured for receiving and transmitting data to and from a plurality of client devices,\nsuch as a multiplicity of communication devices, e.g., desktop computers and/or handheld cellular phones, running the same or similar programming.  Hence, in such embodiments, one or more, e.g., each, of the software implementations, e.g., client\napplication programs, which may be run on a handheld communication device, may be configured with a device, e.g., mobile device, identifier (ID), for providing a unique device identifier for the device.  In particular instances, the client application\nprogram of the mobile device further includes one or more of a user ID of a user associated with the mobile device, information about the user, and/or location data representing a location of the user and/or mobile device.\n Specifically, in various use models, each client application program may be configured to generate an interactive user interface that may be configured for being displayed on the interactive display of the mobile device.  Specifically, in\ncertain instances, the interactive user interface may display one or more activities of one or more events to be or being observed.  More specifically, the client user interface may be a graphical display for presenting an input mechanism, e.g., a\nscoring matrix, to the user through which one or more inputs, e.g., scores or commentary, may be entered, via the user interacting with the score matrix graphic, and thereby being input into the system.  In particular embodiments, the graphical user\ninterface can be presented based on a determined language preference, such as English, Spanish, Portuguese, French, German, Italian, Polish, Japanese, Chinese, Korean, and the like.  Hence, in a manner such as this, users of the devices of the system may\nevaluate, e.g., score, the activities of one or more competitors or performers engaging in the event(s).  In such an instance, the evaluation may include a time and/or geographical stamp and/or judging data, e.g., scores or commentary, representing the\nscoring and/or judging of the activity by the user.  In certain instances, the scoring and/or commenting may be in accordance with one or more predetermined judging categories and/or scales, such as a matrix configured in the corresponding client\napplication program.\n Hence, in various embodiments, a method for receiving a score from a client device is provided.  In certain instances, the client device may include a downloadable application or APP that is configured for presenting a user interface for\nallowing a user to enter a score or an evaluation of a performer in a competition, such as where the quality and/or quantity of the score is at least partially dependent on the user's location.  Accordingly, the method may include locating an individual\nuser within a geographical region, where the method is to be implemented by the application running on the client device, such as a handheld mobile device where the handheld mobile device has a unique device identification (RFID) code.  In such an\ninstance, the method may include generating, e.g., by the application running on the handheld mobile device, an interactive user interface on the handheld mobile device being operated by a first user.  The method may further include receiving, over a\ncommunication network, via the interactive user interface on the handheld mobile device, an input, e.g., regarding an evaluation or score.  In various instances, the input may include other data, as well as the score, such as user identification data,\ndevice identification data, geolocation data, as well as other user characteristic and/or meta data associated therewith.\n Accordingly, once the score has been entered into the device, e.g., via the downloadable application, the method may include sending, over the communication network, by the application running on the handheld mobile device, the unique device\nidentification code to a server system for authenticating the user, the device, and/or the authenticity of the evaluations and/or scores being entered by the user, e.g., via the device identification code.  In such an instance, the method may include\nreceiving, over the communication network, by the server system, the input evaluation and/or score, the geolocation data locating the device and/or its user, such as within a predetermined geographical region; and associating, by at least one of the user\ndevice and the server system, the input data with time-stamp data so as to generate time and place data for the user, the device and the evaluation.  Likewise, the method may include transmitting, from the server system, over the communication network\nfor presentation to the user via the user interface on the mobile communication device, one or more of the user entered score, the average entered score of a collection of users, and/or the official score entered by one or more official judges of the\nevent, as well as one or more results of a statistical analysis thereof.  Various metadata may also be transmitted with respect to the data, such as time and place data with respect to score entry may also be included, although not necessarily displayed. Hence, in various instances, the method may include displaying, on the interactive user interface on the handheld mobile device, the entered evaluation or score.\n As indicated, the APP may be a downloadable application and as such the method may include downloading the application from a website associated with the server system.  In various instances, the APP may be downloaded on to a mobile device, such\nas a smart phone or watch.  Accordingly, the system may include a client device, such as where the client device may be any intelligent device such as a mobile phone, or watch, or bracelet, or the like, and thus, in various instances, the method may\ninclude entering data into the client device, such as via entry at a display of the mobile device, or via voice command.  For instance, the mobile device may include a voice activated input module that is configured for recognizing the voice of a user,\nthe voice prompting an activation of the device, which in response thereto activates the device to receive and record an input, in view of which input the device may interpret the input into a set of instructions directly, or may send the instructions to\na central server for interpretation, which will then send the instructions back to the client device for implementation.  In such an instance, the method may then include implementation of the instructions parsed from the received voice commands. \nParticularly, the method may include receiving a voice command, converting the voice command into instructions regarding an input, implementing those instructions, and transmitting the results thereof back to the central server.  In certain instances,\nthe input involves scoring a competitors actions in an event, which score then gets transmitted to the server.\n Accordingly, the system may include a server system, wherein a server of the system is connectable to one or more of the associated client, e.g., mobile, devices via the plurality of client application programs, such as over a communication,\ne.g., cellular and/or Internet network.  The connection may be such that it synchronizes the mobile device(s) with the server and at a time during which the activities of the event or performance are taking place.  In such an instance, the server system\nmay be configured for receiving one or more of the mobile device ID(s), the user ID(s), the user(s) information, and/or the location data for each user of the one more client programs.  Further, the server system may also be configured to authorize a\nnumber of users, so as to allow them to participate in, e.g., score, the event; receive the scores inputted from the authorized users; and for adjusting the scores to produce a fileted score, as necessary.  For example, filtering may occur so as to take\naccount of the timing and/or geography by which the score was entered, e.g., due to a time stamp associated with each score, and/or to take into account any determined bias of a score by an associated, authorized user.  Particularly, in certain\ninstances, the server may be configured for collecting, collating, and/or generating an aggregated score, such as from the filtered scores, for transmission to each of the plurality of client application programs and/or for display in the interactive\ndisplay with respect to the one or more activities of the event(s), such as via substantially \"real-time\" results transmission.\n In another aspect, an apparatus for evaluating, judging, and/or scoring and/or commentating on an activity, such as of a participant in a competition, is provided.  For example, the apparatus may be a computing device, such as a desktop, laptop,\nor mobile computing device, such as tablet computing device or handheld mobile communications device, or the like, which is configured so as to allow an observer to view, evaluate, score, judge, and/or comment on an activity or action of a participant of\nthe event being viewed.  In such an instance, the system may be configured such that a multiplicity of such apparatuses are capable of observing and/or voting, scoring, commenting on, and/or judging an event or competition, such as in conjunction with\none or more other apparatus possessed by other observers.  In such instances, each of the apparatuses may include a mobile electronic device that has at least one processor, a transceiver to communicate with a communications network, and a display.  In\nparticular instances, the apparatus may be communicably coupled to a server system, such as over a suitably configured communications network.\n Particularly, in various embodiments, the apparatus may be a mobile communications device, such as a hand held cellular phone, which in this instance, may be operable by an observer of an athletic event or performance.  The apparatus may include\nan application program that may be executed by at least one processor of the cellular phone.  In certain embodiments, the application is configured for executing one or more processes, such as including an authenticating and/or a locating process, such\nas for authenticating the operator of the mobile device, e.g., the observer of the event, based on one or more of a device identifier, associated with the mobile electronic device; a user identifier; associated with the observer; and/or a determined\ngeolocation of the mobile electronic device, the geolocation being defined by a geographic area for the activity.\n A further process for operation by the processor(s) of the cellular phone is the generating, e.g. on the display of the mobile electronic device, a graphical representation of a participant, e.g. competitor, of the activity; a scoring matrix,\nfor scoring the activities of the participant competitor; and/or a time-based component for the performance of that activity.  Particularly, the processor may generate, at a user interface, a graphical representation of an evaluation matrix or pad, for\npresentation on the display, through which evaluation interface or pad a user, e.g., the observer, may input an evaluation such as a score, the score representing an evaluation of the quality of the participant's activities.  Additionally, the processor\nmay further be configured for receiving an inputted evaluation, once it has been selected and/or entered by the user, and thereby inputted into the system; and may further be configured for transmitting, by the transceiver of the mobile electronic\ndevice, the score to a server, such as for aggregation with other evaluations or scores from other observers.  Likewise, once one or more evaluations or scores or comments have been entered into the system, aggregated, and/or corrected, e.g., by the\nserver system, they may be transmitted back to the various associated mobile devices, e.g., via a suitably configured receiver, for display, at the display of the mobile electronic device, where the evaluation or score may represent an aggregated\nevaluation or score for the activity based on the evaluations and scores received by the observer and the other evaluations or scores received from the other observers within the geographic and/or time-based component.\n More particularly, in various instances, the mobile communication device, may be a cellular telephone or other mobile computing device that is configurable for the scoring of a participant in a competitive activity.  For instance, the mobile\nscoring device may include one or more of the following.  The mobile scoring device may include a geolocation element, such as a GPS or other locating mechanism, which is configured for determining the location of the device.  An antenna may also be\nincluded for exchanging signals with one or more other communicable devices, such as over a communication, e.g., wireless, network.  The mobile communication device may also include an input device via which a user may input information.  The input\ndevice may be a real, e.g., a physical device, or virtual, e.g., a graphical representation of an input mechanism, e.g., such as a virtual button, toggle, or touch pad.  The mobile device may also include a display, such as a capacitive sensing touch\nscreen display for providing information, e.g., a user interface, to a user of the mobile scoring device.  A processor for sending and receiving signals, e.g., exchanged by the antenna, may also be included, where the processor is operably connected with\na memory for storing an application program, e.g., an \"APP.\" The \"APP\" may be an application that includes machine instructions such that when executed by the processor, cause the processor to perform the various process functions disclosed herein.\n These processes, for operation by the processor of the device, e.g., per instruction by the APP, may include: determining, via the GPS, the location of the device within a geographic area; generating, via the application running on the mobile\ndevice, a user interface for display on the device; prompting, via the user interface, the user to enter a score and/or commentary for an action performed by the participant in the competitive activity; and transmitting, via a suitably configured\ncommunications module, the input, and other associated data to a server of the system.  In various instances, the other associated data may include one or more of the identity of the device, e.g., Device ID, the identity of the user, e.g., User ID, and\nthe geographical location of the device.  Other data, such as time elapse data, and/or other related data, e.g., event data, may also be transmitted.  Hence, the mobile communication device may be configured for transmitting the geolocation of the mobile\ndevice, the user's identity, the device identity, and the inputted score and/or commentary from the user to a server system over the communication network via the device antenna.\n Accordingly, in various instances, the system may include a server that is configured for receiving, over the communications network, the information specifying the user's identity, the identity of the device, the geographical location, and the\nuser inputted score, and/or other associated data, e.g., event data, over the communications module of the device, e.g., via the APP. Once received, by the server system, the server may then evaluate the data, such as for bias, for instance, by\ndetermining if the inputted score fits within a mean score range to thereby determine bias.  If bias is determined, the server system may then adjust the score, such as based on the mean score range thereby rendering a final score by the user of the\naction performed by the participant.  Once a final score has been rendered by the system, the server may then transmit the rendered final score back to the user device, such as for presentation, via the user interface of the APP, on the display of the\ndevice.\n In various embodiments, the system may be configured for authenticating the user, e.g., via the user ID, for determining whether the user is authorized to score the event, and/or for authenticating whether the time stamp for the scoring, e.g.,\nvia a determined geolocation with time identifier, fits within a determined window of opportunity within which scores may be received and entered into the system for consideration by the server.  In such an instance, it is only after authentication,\ne.g., verification, of the user and/or the time stamp that the inputted scores will be considered by the system.  Further, as indicated, along with the user and device ID, geolocation, and time stamp data, other associated data may be entered into or\notherwise be received and/or considered by the system.  For instance, in various instances, the system may be configured for receiving, e.g., over the communications network.  Such data may be any useful data associated with the user, an event, and/or\none or participants and/or sponsors thereof.  For example, a database of the server system may receive or otherwise include social media data of the users, judges, competitors, event organizers, and/or sponsors of the system.  Such social media data may\nbe that which is directly entered into the system, e.g., by filling out electronic forms, or collected by the system through a search of readily available databases, such as those associated with FACEBOOK.RTM., INSTAGRAM.RTM., TWITTER.RTM., FLICKR.RTM.,\nPINTEREST.RTM., FOURSQUARE.RTM., and other online or public information storing resources.  Specifically, the system may be configured for performing a search of identified online accessible databases for information regarding participants in the system\nto identify data that may be relevant for determining relationships there between and/or for determining bias thereby.\n Correspondingly, another aspect of the disclosure is directed to a computer program product for implementing the various processes and sub-processes of the system.  For instance, a computer program may be provided wherein the program includes a\nmachine-readable medium storing machine instructions that, when executed by a scoring device or server of the system, e.g., having one or more programmable processors, cause the device and/or server system to perform the programmed functions.  These\nfunctions may include one or more of the following: the receiving, e.g., over a communication network, such as from a mobile device, and the processing of data.  The data may be score data, such as data pertaining to characterizing and/or evaluating a\nparticipant in an activity being performed in an event within a defined geographical area.  Other data that may be received and processed by the system includes user and mobile device identification data, as well as geolocation and/or time elapse data,\nso as to authenticate the user and/or the user's location.  Such data may be received at the mobile device automatically or via a user interface presented by an application, e.g., APP, that runs on the mobile device.  Accordingly, another process that\nmay be implemented by the system include authenticating the user and the user's location, e.g., based on the user and the mobile device identifier, to verify that the user is authorized to score the activity.\n Additionally, a computer program product of the system may be configured for causing the device and/or server system to access a database, e.g., storing information pertaining to the activity, where the accessing includes identifying a plurality\nof participants in the activity, as well as identifying one or more users scoring the actions of the participants, such as where the one or more users have a physical location within the defined geographical area and are authorized to score the activity,\nwhich authenticating, as indicated, may include validating the identity of the user, the identity of the user's device, and/or the geographic location of the user and/or their device.  Once accessed, this information, e.g., the user's identification, the\nidentification of the device, and/or the location, may be associated with other relevant information entered into or stored within the database.  The relevant information may also include data pertaining to the user, the participant, the event, one or\nmore judges of the event, such as social media data, such as postings, that include or reference one or more of these factors.  This data, in addition to the scoring data, may be evaluated, such as by the scoring device and/or server system may be\nretrieved from the database and evaluated such as for correlations and/or relationships between the data.  Such correlations and/or relationships may then be used to determine possible bias, such as where the evaluating may include determining a mean\nand/or average and further determining if the score by the user fits within the determined mean score range of the one or more inputted scores, such as from a selected number of users of the system.  Further, where such bias is determined the score may\nbe adjusted, e.g., by the scoring device and server system, so as to render a final evaluation and/or score of the participant's activities by the user.  Furthermore, once the final score has been rendered, such as by the server system, it may be\ntransmitted, e.g., over the communication network, such as for presentation to the user via the user interface on the mobile communication device, or vice versa, such as where the final score represents an adjusted and/or aggregated score of the\nactivity.\n Further, in accordance with the above, a method is provided for evaluating and/or scoring a participant in a competitive or entertaining activity.  The following steps of the method may be performed in any logical order, and may be performed\npursuant to machine instructions implemented by a processor the scoring device and/or the server system.  Accordingly, the method for execution by a device of the system may include receiving, e.g., at a server system, over a communication network, from\nthe mobile device, or vice versa, one or more user inputted preliminary evaluations or scores of the participant in the competitive activity, such as a competitive activity being performed within a defined geographical area.  In various instances, in\norder to evaluate and/or score participants in the competitive or performance activity, an evaluation and/or scoring matrix may be presented to the user, e.g., as a graphical interface, on a display of the mobile device and/or server system, whereby a\npreliminary evaluation or score may be entered into the device by a user, e.g., a scorer or judge of the event.  In various instances, the evaluation or scoring matrix may be generated by a user interface and may be formed as a template by which each\nparticipants' activities may be scored by the user.  Hence, the preliminary evaluation or score may be received via a user interface presented at a display of the device and/or server.  The method may further include authenticating, e.g., by the device\nand/or server system, user authentication data, so as to verify an identity of the user, such as by a user identifier or an identity of the mobile device; and may further include determining whether the scoring is taking place within a defined\ngeographical area.\n The method may then include accessing, e.g., by a device or the server system, a database storing the evaluation or scoring matrix, such as where the matrix includes one or more of an identity of the event, the competitive activity, and a\ncompetitor of the competitive activity, as well as an identity of the participants in the competitive activity, an identity of each authenticated user scoring the competitive activity, a list of actions by which each participant's actions are to be\nevaluated and scored, and the preliminary evaluations inputted to the matrix by each authenticated user evaluating and scoring the competitive activity.  Hence, the method may include aggregating, e.g., by the evaluation device and/or the server system,\nthe preliminary evaluations and scores inputted to the matrix, such as by each authenticated user, so as to generate a mean and/or average or other statistically relevant parameter score range and/or a final score or evaluation.  Once a mean, median,\nmode, and/or average evaluation or score is determined, the evaluation device and/or server system may evaluate the one or more preliminary evaluations or scores inputted by the user for bias, such as where the evaluating includes determining if the\npreliminary evaluation or score inputted by each user fits within the average and/or mean evaluation range to thereby determine bias.  Additionally, if bias is determined, the user's inputted preliminary evaluation or score may be adjusted to thereby\nrender a final evaluation of the participant's activities by the user, which adjusted final evaluation and/or score or commentary may be transmitted, over a communication network for presentation to the user via the user interface, e.g., on the scoring\ndevice and/or server system.\n Accordingly, in view of the above, the present disclosure is directed to devices, systems, and their method use for evaluating and/or scoring a performer's activities in an event, such as by a plurality of users employing a graphical user\ninterface of a plurality of client devices, such as where the scores may be aggregated, statistically analyzed, and/or checked for bias, such as where some evaluations or scores may discounted or discarded, e.g., where bias is determined, but where those\nthat are accepted are aggregated, and the results may be transmitted back to the client device for display.  Along with these scores the average of the crowd score and/or the judge's score may be sent as well.\n As discussed herein, bias may be determined by determining trends in the individual's evaluations and/or scoring that reveal bias, such as where the individual user always scores below or above the crowd, e.g., for some performers, or always\nvotes the same as a block of other users, votes from a distance from the event, or votes outside of a predefined time window, or has social connections with competitors or judges as determined by the system such as by image recognition of social media\nweb pages, etc. In certain instances, the scores may be discounted or discarded if they were entered into a device where the device is determined to be located outside of a predefined geographical region, or if they were entered at a time that is\ndetermined to be outside of the determined timeout window, such as where the timeout window may be determined manually, such as by a system administrator, automatically, based upon one or more pre-set conditions occurring, such as the initiation of an\nauxiliary machine, or dynamically determined by an artificial intelligence module of the system.\n For instance, the opening and the closing of the time window may be determined manually by a system administrator who is watching the event, and opens the scoring window when a performer begins their performance, and then closes the window when\nthe performer ceases their performance.  Alternatively, the system may open the timeout window in a more dynamic manner, such as where the system collects the entered scores as they come in, and it is determined at what time the highest number of scores\nare entered, e.g., at the greatest number or velocity, the system may then set the window parameters based on that time, and those scores entered too soon and too late, based on that selected time, may then be rejected.  In various instances, the system\nmay include a wearable user device, which may be worm by the performer and may be configured for detecting and/or determining the characteristics of the movements of the user, or the system may include a smart image capturing device, e.g., video camera,\nthat is configured for detecting the movement of a performer, such that when the characteristic movement for beginning a performance is detected the timeout window is opened, and when the characteristic movements for ending a performance are detected,\nthe timeout window is closed.  Additionally, the window can be opened and closed by a combination of the above.\n In various embodiments, the system may be configured for ensuring that only one person gets one vote, such as by identifying the user by their login, user or device identification, and/or their location, e.g., GPS, data, etc. The system may also\nbe configured for ensuring that the scores or votes being entered are accurate, and thus may include programming for appropriately weighing the scores along a variety of characteristics.  In particular instances, the system may be configured for\nreceiving and/or broadcasting a video feed from a user of a client device, such as a mobile smart phone, such as a video recorded on a phone where the phone processor accesses that recording and transmits it over network to an encoder, the encoder\nformats the captured video into a HTTP-based live stream, which then gets transmitted to the server and then directed to back to the recording and/or other client devices that have signed up to receive the stream.  This allows local users to film and\nbroadcast a local event, such as amongst friends.\n The summary of the disclosure described above is non-limiting and other features and advantages of the disclosed apparatus and methods will be apparent from the following detailed description of the disclosure, and from the claims. BRIEF\nDESCRIPTION OF THE DRAWINGS\n FIG. 1A shows a user interface in which the user of a downloadable client application enters an alpha and/or numerical, e.g., four-digit, pass code to log in as a user of the downloadable client application.\n FIG. 1B shows a user interface in which the user of the client application creates a user account.\n FIG. 1C shows a user interface for a user of a client application of the system of the disclosure.\n FIG. 1D shows a user interface in which the user of the client application is shown a welcome screen following the creation of an account with the application.\n FIG. 2 shows an event screen of a user interface for a user of a client application of a mobile device of the system of the disclosure.\n FIG. 3A shows a set-up screen for an event at a user interface of a client application of a mobile device of the system of the disclosure.\n FIG. 3B shows another version of the set-up screen of FIG. 3A for an event accessible via the user interface of the client application of the mobile device of the disclosure.\n FIG. 3C shows another version of the set-up screen of FIG. 3A for an event accessible via the user interface of the client application of the mobile device of the disclosure.\n FIG. 3D shows another version of the set-up screen of FIG. 3A for an event accessible via the user interface of the client application of the mobile device of the disclosure.\n FIG. 3E shows another version of the set-up screen of FIG. 3A for an event accessible via the user interface of the client application of the mobile device of the disclosure.\n FIG. 3F shows another version of the set-up screen of FIG. 3A for an event accessible via the user interface of the client application of the mobile device of the disclosure.\n FIG. 4A shows a set-up screen for judge of the system, where the set-up screen is accessible via the user interface of the client application of the mobile device of the disclosure.\n FIG. 4B shows another version of the set-up screen of FIG. 4A for a judge, where the set-up screen is accessible via the user interface of the client application of the mobile device of the system of the disclosure.\n FIG. 4C shows another version of the set-up screen of FIG. 4A for a judge, where the set-up screen is accessible via the user interface of the client application of the mobile device of the disclosure.\n FIG. 5A shows a set-up screen for a location of an event, where the set-up screen is accessible via the user interface of the mobile device of the system of the disclosure.\n FIG. 5B shows another version of the set-up screen of FIG. 5A for the location of the event, where the set-up screen is accessible via the user interface of the client application of the mobile device of the disclosure.\n FIG. 5C shows an event location identifier for the event, where the event location screen is accessible via the user interface of the client application of the mobile device of the disclosure.\n FIG. 5D shows a user interface in which the user may enable the client application to use the user's location to evaluate user bias.\n FIG. 6A shows a pipeline in which a user may utilize voice commands, e.g., via a watch, to transmit information to the application to be displayed on multiple devices.\n FIG. 6B shows a user interface for selecting a representation of a competitor in an event uploaded to the system.\n FIG. 7A shows a client interface for scoring the actions of a competitor in an event uploaded to the system, where the user interface is accessible via the client application of the mobile device of the system of the disclosure.\n FIG. 7B shows another user interface of FIG. 7A for scoring the actions of a competitor in an event uploaded to the system and accessible via the client application.\n FIG. 7C shows another user interface for scoring the actions of a competitor in an event uploaded to the system and accessible via the client application.\n FIG. 7D shows a user interface for commenting on the actions of a competitor in an event uploaded to the system and accessible via the client application.\n FIG. 7E shows a user interface illustrating the social media platforms of a competitor in an event uploaded to the system and accessible via the client application.\n FIG. 7F shows a user interface illustrating a selection of a potential winner of a competitor in an event uploaded to the system and accessible via the client application.\n FIG. 7G shows a user interface in which a portion of the window is configured for video streaming and another portion contains information about the next event.\n FIG. 7H shows a user interface that displays a competitor scoreboard.\n FIG. 7I shows a user interface for selecting a representation of a competitor in an event uploaded to the system.\n FIG. 7J shows a user interface for scoring a competitor selected by the user.\n FIG. 7K shows a user interface in which the user may view the competitors in relation to each other by score.\n FIG. 7L shows a user interface in which the user may view more information about a event and share the information on a social platform.\n FIG. 7M shows a user interface in which the user may view more information about the event as well as a real-time countdown displaying the remaining amount of time to score a competitor before the entered score is given a lower weight or\neliminated.\n FIG. 7N shows a user interface in which the user may view more information about the event as well as score the event using voice recognition.\n FIG. 7O shows a user interface in which the user may view more information about an event.\n FIG. 7P a user interface allowing a user to record the actions of a competitor in an event uploaded to the system and accessible via the client application.\n FIG. 8A shows a user interface in which the user may utilize screen swiping to view the score entered for a competitor in an event.\n FIG. 8B shows a user interface in which the user may swipe a touch screen device to view the results of an event.\n FIG. 8C shows a user interface in which the user may swipe a touch screen device to view the previous competitor to the current competitor in an event.\n FIG. 8D shows a user interface in which the user may swipe a touch screen device to view the next competitor in the event.\n FIG. 8E shows a scoring table in accordance with the teachings of the disclosure.\n FIG. 8F shows another view of a scoring table of FIG. 8A in accordance with the teachings of the disclosure.\n FIG. 8G shows one version of a scoring matrix indicating one instance of bias.\n FIG. 8H shows another version of a scoring matrix indicating another instance of bias.\n FIG. 9A shows various system components in accordance with the teachings of the disclosure.\n FIG. 9B shows a user interface in which the user may view the competitors in relation to each other by score in a mobile web browser.\n FIG. 9C shows a user interface to be used by official judges of a competition to score competitors.\n FIG. 10A shows a user interface in which the user can configure engagement with the application by setting up a user account and user profile with the client application.\n FIG. 10B shows a user interface in which the user can configure engagement with the client application by activating and deactivation various application settings.\n FIG. 10C shows a user interface in which the user can configure engagement with the client application by activating and deactivation various settings.\n FIG. 10D shows a user interface in which the user can view information about the events the user has signed up to score.\n FIG. 10E shows a user interface in which the user may choose to engage with international events.\n FIG. 10F shows a user interface in which the user may view more information about an event.\n FIG. 10G shows a user interface in which the user may view more information about the event as well as a real-time countdown displaying the remaining amount of time to score a competitor before the entered score is given a lower weight or\neliminated.\n FIG. 10H shows a user interface in which the user may view more information about the current competitor as well as score the event using voice recognition.\n FIG. 10I shows a user interface in which the user may view more information about an event including competitor scores and competitor information.\n FIG. 10J shows a user interface in which the user has shared scores of a particular competitor on INSTAGRAM.RTM..\n FIG. 11A displays a page navigation layout of the client application.\n FIG. 11B shows a REST table in accordance with the teachings of the disclosure.\nDETAILED DESCRIPTION OF THE DISCLOSURE\n The present disclosure is directed to systems and methods for allocating and distributing inventory, such as perishable products.\n As used herein, unless otherwise stated, the singular forms \"a,\" \"an,\" and \"the\" include plural reference.\n Accordingly, presented herein is an interactive, crowd-sourced evaluating, judging, scoring, and/or communicating platform that is configured for both enabling and encouraging audience engagement when observing and/or participating in an event. \nThe participation may be in the form as an observer, watching the event, as a judge, judging the event, or as a participant of the event.  In various instances, a sponsor and/or advertiser, a reporter, and/or journalist, or commentator, or other\ninterested party may also be a participant.\n Accordingly, an issue overcome by the devices, systems, and their methods of use herein disclosed may be used to enhance the engagement of those viewing the event, but not otherwise participating in it.  For instance, certain athletic and/or\nentertainment events may not particularly be engaging for the average viewer, e.g., when their child or team is not involved in the event or in a particular heat of the event.  Of course, in many instances, event announcers and/or commentators are\npresent to encourage and amplify fan engagement, but even with the aid of a commentator, often times fans simply lose interest when their favorite competitor or performer is not competing.  Further, the secrecy and lack of real time transparency of the\nscoring involved during the event and/or throughout the day, additionally leads to disenfranchised fans.\n The event may be an athletic event, such as a sporting event, for instance, a professional or collegiate or high school sporting event, e.g., a football, baseball, soccer, volleyball game, etc.; a professional or collegiate or high school\ncompetition, such as a boxing, martial arts, including jujitsu, Tae Kwon Do, judo, mixed martial arts (MMA), wrestling, tennis, surfing, track and field, dance competition, and the like; other competitive events, like Olympic, World Cup, Primer League,\nNFL, MLB, NBA, MSL, swimming, gymnastic competitions and similar type of events.  Further, the athletic event may be an event set up by a local group of user's who wish to organize their own activity and open it up to evaluation, such as by use of the\ndevices, systems, and methods disclosed herein.  Further, the event may be any form of entertainment event that is amenable to evaluation, which event may be viewed and opened, e.g., via the APP, to have one or more of its elements and/or participants\nevaluated.  Such entertainment events include theatrical performances, movies and TV shows, including award shows, reportings, and competitions, as well as music, concerts, speeches, debates, and the like.  The athletic and entertainment events and\nperformances may be global events, national events, statewide events, regional events, local events, and the like.  The performers may be professional, semi-professional, e.g., minor leagues, collegiate, high-school, or even junior league performers.  In\ncertain embodiments, the performers may simple be any collection of users of the APP who desire to organize an event that s open to viewing and/or evaluation through the APP.\n For instance, in one use model, the devices, systems, and methods herein disclosed may be used by those participating in, e.g., those competing, judging, observing, scoring, commenting and/or reporting on the event, and the event itself may be\nany suitable event amenable to evaluations, which for exemplary purposes herein may be a surfing, jujitsu, MMA, boxing competition, or the like, such as a professional or armature surf contest for adults or kids.  It is to be understood that although the\nfollowing is described with respect to a surf, jujitsu, or MMA competition, the devices, systems, and methods disclosed herein are useful for enhancing fan engagement as well as for ensuring objectivity of judging, e.g. through identifying bias, in any\nathletic or entertainment event, competition, or performance, such as those involving the judging of one competitor or performer against another.  Because of the informal nature of various surf competitions, as well as other amateur or local athletic\nevents, such as Olympic events, bias in judging the performance of athletes or competitors competing in such events is difficult to objectively determine, and even more difficult to counteract, and thus, the devices, methods, and systems of the present\ntechnologies are particularly useful in such instances.  Consequently, in various instances, the event may also be any form of contest where one team, group, or person is competing against another team, group, or person for votes, scores, accolades, and\nthe like.  Such events may include performances, such as theatrical performances, elections, debates, and/or any other form of competitive interactions.\n For example, in various embodiments, provided herein is a client application that may be a downloadable software application \"APP\" that is adapted for being downloaded onto and implemented by a client device.  A client device may be any form of\ncomputing device, such as a desktop or laptop computer; or a mobile computing device, such as a tablet computer or mobile communication device, such as a cellular phone.  The downloadable application may be configured for being downloaded onto the client\ndevice and once there may be configured for directing the processor(s) of the host device to perform one or more predetermined functions.\n Particularly, as can be seen with respect to FIG. 1A, the client application may be configured for generating a client interface for presentation on a display of the device through which interface data may be inputted, transmitted, and received. More particularly, as can be seen in FIG. 1A, the user interface may present a window whereby the user may enter a pass code, such as a four-digit, e.g., alpha-numeric, pass code to log into the application.  For instance, in one exemplary embodiment, in\norder to access the system the user may enter a passcode.  The passcode can be any form of security verification that the user enters so as to gain access to the system.  In certain instances, the passcode can be an alphanumeric number or word or mixture\nthereof, which may be entered via a prompt displayed on a user interface of the device, and/or may include the answering of one or more security questions.  Of course, access to the APP may be through a facial recognition or finger identification\nauthentication device, or other password generating apparatus.  As illustrated, in this instance, the passcode interface requests the user to enter a four-digit code to activate the client application log in screen.  However, in various instances, the\npasscode may be generated by a random number or word generator that may be sent to a key device (receiver), which may then be entered into the system via the user interface for access and authorization.  In other instances, an access device, such as a\ndongle may be used to gain access to the system, such as by bringing the dongle into proximity and/or into communication or contact with the computing device.  In certain instances, the user may access the system by logging into the APP by using their\nlogin information to another application, such as their login information to a social media platform.  Once the user is logged into the client application, they will then be enabled to engage the system and participate, e.g., as an evaluator or scorer,\nof a selected event.\n In a first step in engaging with the system, FIG. 1B shows a user interface in which the user of the client application creates a user account.  Particularly, if the user is a new user and does not have a pass code, the user is instead taken to\na sign up screen, which is shown in FIG. 1B, where the user can create an account by entering their name, e-mail address, and desired password of the user.  By creating an account with the application, the user also agrees to the application's conditions\nof use and privacy notice.  Once the account is created, a confirmation e-mail containing a temporary, e.g., four-digit, pass code may be sent to the user at the e-mail address the account was created with.  Once the user re-opens the application and\nenters the temporary pass code, the user may request a new pass code that may be used to log into the application on future occasions.\n For example, in one embodiment, a new user who has not created a pass code with the client application uses the application interface to enter the personal information required to create an account with the client application including but not\nlimited to: name, e-mail address, phone number, password, and the option to view the password decrypted.  Following the transmission of personal information to the client application in the new account creation interface, the user acknowledges an\nagreement to abide by the privacy policy and conditions of use by pressing or otherwise selecting a check box or button, which indicates acceptance of the terms of use of the system.  At the completion of entering user personal information and agreeing\nto the privacy policy and conditions of use, the user presses a submission button to initiate the creation of a new account with the client application for which the same user may repeatedly log in to the client application.  Once the user has created\nthe account, a verification instruction may be sent, such as via a text, SMS, phone call, email, or other means of communication transmission, the user then receives the communication via the mechanism of communication, e.g., e-mail account, associated\nwith the client application account, accesses the confirmation communication, verifying the user of the client application and assigning the user a temporary pass code to use for logging in. Once the user has obtained the temporary pass code, the user\nmay reopen the client application and enter the temporary pass code contained in the confirmation e-mail received to access the user's client application account.  Likewise, once the user has entered the temporary pass code to log into the newly created\naccount with the client application, the user will be prompted to create a permanent pass code, which may or may not be different from the temporary pass code, for which to log into the account for subsequent times logging in.\n As can be seen with respect to FIG. 1C, a participant of the system, e.g., an event organizer or user, may open and log on to the system via the application.  For instance, opening the application launches a user interface, such as an interface\nthat is configurable to the particular user.  Once logged on, the user screen of the user interface may allow the user to enter and may subsequently display their identifying information into the system, such as via the application.  Such identifying\ninformation may include the name and residency of the user among various other preferences, likes, and/or dislikes.\n FIG. 1D shows a user interface in which the user of a client application is shown a welcome screen following the creation of an account with the application.  Specifically, upon opening the application, the user may be taken to a welcome screen\nshown in FIG. 1D, in which the user of the application has the option to log in to an existing account or create a new account.  When the user creates the account the user acknowledges and agrees to the application's privacy policy and conditions of use. More specifically, after the user has opened the application, a new user may select a button to create a new account, and an existing user may select a button to log into an account that has been created.  If a new user selects the option to create a new\naccount, the user additionally and simultaneously agrees to the terms of use, privacy policy, and EULA of the client application.\n Additionally, the user interface may include toggles to allow the user to configure the functionality of the application.  These functionalities allow the user to demarcate to what extent the particular user is amenable to engaging publically or\nprivately with the system.  For example, the user may allow the system to access and/or integrate their various social media interfaces into a searchable database.  Such social media interfaces may include FACEBOOK.RTM., INSTAGRAM.RTM., TWITTER.RTM.,\nFLICKR.RTM., PINTEREST.RTM., FOURSQUARE.RTM., and other online or public information storing resources.  The access to such online content of, or related to.  the user is useful for determining the user's identity, authenticity, patterns of behavior,\ntruthfulness, especially with respect to their use of the system, such as for determining potential for bias.  Alternatively, the user may select an anonymous interaction, which will allow the user to interact with the system and/or participate in the\nevent in an anonymous manner.\n Further, the system may allow the user to participate as a competitor and/or performer, as a judge, as an evaluator and/or scorer, which selection will determine at least in part, the permissions and/or weighting of the participant's\ninteractions.  Furthermore, at the user interface, the user may enter, and/or the system may automatically determine, who the user is following and/or tracking and/or otherwise interested in, such as with respect to that person's public, e.g., social\nmedia presence.  Likewise, the user and/or system may determine who is friends with or is otherwise following the user.  The selectable events for participating in and/or viewing may also be presented at the user interface for selection by the user.\n Accordingly, as indicated, the present technology is directed to a communication platform that is configured for allowing participation in an activity, such as an event.  The participation may be as a judge, an observer, evaluator, and/or\nscorer, as a performer, as an event organizer, even as a sponsor or advertiser, or the like.  Specifically, as can be seen with respect to FIG. 2, the client application of the platform may be configured for generating a user interface so as to allow an\nevent organizer, with the appropriate authorizations, to create an event.  As indicated the organizer may be as large as a big corporate sponsor, or may be as small as a group of two local users wanting to judge each other's performance.  Accordingly,\nthe event may be of several different types of events.  For instance, the event may be a certified event, a public event, a private event, a requested event, or the like.  More specifically, as seen with respect to FIG. 2, the application, e.g., the\nclient application, may generate a user interface that presents an events window, by which an event organizer may organize an event for participation by one or more system users.\n In one instance, the event may be a certified event.  A certified event may be one of a series of events in a competition that spans a number of days, weeks, or months, and/or may be global, regional, or local in scope.  As such an event may be\ncertified when it is part of a regularly scheduled series of professional, amateur, or school based competitions, and the like that have sought and received certification for the event via the governing and/or regulating body of the event, e.g., sport,\nin question.  For instance, a weekly sporting event, such as a football, Australian rules football, Rugby, soccer, basketball, or baseball game, or the like, where several teams in one or more conferences play each other for an annual title, may be\ncertified as an event.  Such events may be a national, state, regional, or citywide event, and in some instances, the event may be televised, and as such, the pool of potential participants, e.g., evaluators and/or scorers, may be quite large.  In\nvarious instances, such events may not be nationally televised, but may be more than a local event, in terms of popularity.  Although in some instances, they may be big nationally televised or local events.  Accordingly, in such an instance, the system\nmay be adapted to flexibly adjust to local, regional, national, or global viewing parameters.\n A public event may typically be an event that is made available to a very large audience, but may not be regularly occurring and/or a seasonal event.  Such an event may be a boxing match, an Olympic event, a Word Cup event, Grand Tour, or the\nlike that may be nationally and/or internationally televised.  In such instances, the system may be adapted to receive input from a very large number of participants.  A private event, however, is an event that is neither nationwide, like a public event,\nnor is it typically of regional or even citywide interest.  Rather, a private event is typically one that is of interest to a local neighborhood or community interest.  For example, a private event may simply be a local event, like a little league\nbaseball or pop-warner football game.  In such an instance, the system may be adapted to adjust to local viewing parameters, such as for receiving inputs from a smaller number of users.\n Additionally, where an event is of such a nature as to not be of wide international, national, state, regional, and/or of citywide and/or of community interest, a user may still request that the event be opened for participation by users by\nsimply activating the request event functionality.  In such an instance, the event organizer simply need input the event specifications into the application interface, define the region of participation, and once authorized by the system the event will\nbe made available to other users of the system, in the local environment, such as by entering a search query into a search window.  Alternatively, once an event has been set up, either automatically by the system itself, such as for certified and public\nevents, or through creation by an event organizer, other potential users may be invited to participate in the event, such as by engaging the invite functionality to send an invite to selected users.  Further still, where a user desires to view a certain\nevent that a desired participant, such as a particular team or professional athlete, is participating in, the user may select to follow the participant via the application, and may then receive notifications via the system as to what events the selected\nteam or athlete participates in, so as to be able to observe the event and/or participate in as an observer and/or scorer.\n Particularly, as can be seen with respect to FIGS. 3A-3F, the client application can generate a series of screens via the user interface so as to prompt a user for entering the information that is fundamental to setting up and/or organizing an\nevent.  For instance, in setting up an event a number of parameters may be selected so as to determine the particulars of the event.  Some particulars may be optional, while others will likely be mandatory.  For example, as seen in FIG. 3A, the user may\nselect among a variety of optional particulars, such as whether or not public or private evaluation and/or scoring is allowed, or whether anonymous evaluating or scoring is allowed, whether the event is to be certified, and/or whether the event is open\nto be broadcast, such as via social media.  However, in some instances, it may be useful to limit those who may participate in an event, such as a judge or scorer, such as by geographical region, time zone, and the like.\n More particularly, as can be seen with respect to FIG. 3A, a user interface configured for being presented on a user's, e.g., a scorer's mobile device, may be presented, wherein the interface may include a number of options for setting up and\norganizing an event.  For instance, when an administrator or event organizer interacts with the system to set up a specific event, the user interface may present a proximity setting for allowing the organizer to determine a perimeter within which the\nevent interface may be accessed by other users of the system.  In such an instance, the system may be configured to allow certain users, e.g., within the perimeter settings, to score or otherwise evaluate a competitor and/or event, while excluding other\nusers, e.g., those outside of the perimeter setting from entering an evaluation or score.  Hence, given the geographical tagging and location services that are collected with respect to the devices of the various users of the system, scores entered into\ndevices identified as being within the perimeter may be entered, accepted, and/or used in the evaluation process, but scores attempted to be entered into devices outside of the perimeter, cannot be entered and/or accepted.  For instance, the user\ninterface may present a toggle for turning a proximity locator on, a location setting, demarcating where the event is to take place, and/or a distance input for entering a distance parameter within which scores and/or evaluations will be allowed to be\nentered, outside of which scores may not be entered.  However, in various instances, the commenting function may or may not be functional for all users regardless of geographical location.\n Further, the presented screen may include one or more toggles or switches that allow the event organizer to structure the parameters of the event, such as whether the event is open to scoring by the public, may only be scored by registered vent\nparticipants, may be anonymously scored, or the like.  As indicated, the location settings may be engaged to identify where the event is to take place and the proximity limiter may be configured so as to limit those potentially able to score the event,\nsuch as based on a selected or entered distance factor.  The presented screen may also present whether the event is to be certified or not, and/or what the social media factors to be employed are.  In this instance, the scoring proximity is delimited by\nboth a location, such as Huntington Beach, and further by a radius of 1 mile, such that those within the 1 mile radius may participate in the event.  Limiting participation to a specific region, while excluding those not within that region, may be useful\nin that it limits participation to those presumed to have a more vested interest in the outcome in the results, than those located further and further from the event.  This helps to maintain the relevancy and authenticity of the event and its various\nparticipants.\n Accordingly, at any particular event, the system may be configured for including and excluding certain regions, either by geographical location or by a certain predefined distance from the event.  So, if a given event or performance, such as a\nsurfing or jujitsu or MMA event, is occurring in a given location, such as Huntington Beach, Culver City, San Clemente, or the like, those allowed to participate in the scoring may be limited to those at the actual event, e.g., on the beach at that time,\nthose within the city, if regionally broadcast, or those within a broader range, if broadcast, e.g., televised, nationally, such as depending on whether the event is a public or private event.  For instance, for local, e.g., a requested or private event,\nit may be useful to limit those who can score the participants in the event to within a given distance from where the action is taking place, such as within 100-500 yards of the event, or within the event arena or stadium, or within 1 mile or 5 or 10, or\n25, or 50 or 100 miles or more from the event, because by limiting opinions to those actually at the event, or close by, such as to those actually watching the event, means that the scores entered will be more contemporaneous, more honest, and presumably\nbe less prone to bias and more valid.\n However, if the event is televised to a wider audience, such as a public event, it may then be useful to increase the proximity area for those allowed to score the event.  In such an instance, as described in detail herein, the further the\ndistance from the event, or the maximally approved region, the weighting and/or ranking of the scores may be different, taking into account how far away from the event's location the scoring is taking place.  Hence, if an event is taking place on the\nEast coast, but is being televised to the West Coast, e.g., on a 3 hour time delay, the application can be configured to account for where in the nation the scoring device is, and where and when the scoring is being entered, and this data may then be\nused to weight the received scores.  For instance, for the referenced East Coast event, those on the West Coast may be prevented from scoring the event until it is broadcast in the region where the mobile device is located.  Alternatively, in various\ninstances, devices located out of the identified time and/or region, in this instance, those on the West Coast, may be blocked from participating.  However, in other instances, scores may be accepted from any device regardless of region so long as the\nevaluation and scores being entered are contemporaneous with the action of the event or participant being evaluated.  Such contemporaneous scoring, such as for users outside of where the event is taking place, may in some instances, show that the scorer\nis highly invested in the outcome of the event, since they are viewing and engaging in the event despite being outside of its typical presentation.\n Specifically, the user interface may be configured for manually entering location data into the system, such as through entry into a user interface of the scoring device.  In various instances, the system may be configured for automatically\ndetermining the location of the event and/or the location of each of the scoring apparatuses being employed by the various users of the system.  Such automatic identification can be determined by the system in a variety of manners, such as via\ntriangulation, GPS tracking, cellular signal, and the like.  For instance, a suitably configured geolocation system may be provided.  For example, the geolocation system may include one or more technologies such as a Global Navigation Satellite System\n(GNSS).  Exemplary GNSS systems that enable accurate geolocation can include GPS in the United States, Globalnaya navigatsionnaya sputnikovaya sistema (GLONASS) in Russia, Galileo in the European Union, and/or BeiDou System (BDS) in China.  Hence, when a\nsystem administrator or event organizer sets up the event, and enters the event parameters into the system, e.g., for participation, evaluation, and/or scoring, the geographical region within which the users of the system may participate in the event,\ne.g., as scorers, may also be determined and/or otherwise defined.\n Consequently, once the geographical boundaries have been defined and set, the system may then regulate who is authorized, e.g., which devices, to score the event.  This may be done through an interactive user interface that translates the\nselection of a given region into geographical coordinates, e.g., GPS coordinates, setting forth the area, e.g., mapped to a very precise or even a broad location, in which scores will be considered, and the area in which scores will not be considered. \nParticularly, in various embodiments, scoring may be limited to a given location, such as an arena, or stadium, or the like, or may be opened to a larger area such as an entire city, region, or state.  In other embodiments, scoring may be limited so as\nto only allow scores from those deemed to have a vested interest in the outcome, as determined at least in part by geographical location.  As is explained in greater detail below, the geographical parameters of the system may be employed in determining a\nweighting regime by which evaluations being entered into the system may be appropriately ranked so as to ensure the authenticity of the evaluations entered into the system.\n Further, as can be seen with respect to FIGS. 3B and 3C, a number of event parameters may be entered into the system to define the event particulars, such as the event name or title and/or the date of the event.  An image may also be selected so\nas to represent and distinguish the various participants in the event, as seen with respect to FIG. 6.  Additionally, a time window representing a duration during which scores may be entered into the system may also be set, and timeout judges may be\ndemarcated.\n Also, if an event is one where there are many divisions, rounds, bouts, and/or heats these particulars may also be entered into the system.  The event start time and duration may also be entered into the system, and where there are various\nrounds or heats, the start time and/or duration of the rounds or heats may also be entered.  Likewise, the list of competitors in the event and/or rounds may also be entered into the system and/or the maximum number of entries per round may be set.  In\ncertain instances, as explained below, the system may be configured for determining and/or setting any of these parameters automatically.  Particularly, the system may include an artificial intelligence module that is configured for determining when an\nevent or an action within an event is starting, the duration thereof, and when the same is ending.  More particularly, the system may be configured for autonomously determining when a round is to begin, when a participant within a round is to begin an\nactivity, and/or who is next to perform in the round or event.  Furthermore, in various instances, the system is further configured for transmitting round order and other event particulars directly to the event participants or performers, such as over a\ncommunications network to a mobile computing device carried by the participant, such as to a mobile phone, smart watch, and the like.\n Likewise, with respect to FIG. 3C, once an event has been set up, a user can access their account where they may be presented with a graphical user interface that will allow them to select that newly created event from a list of events in which\nto participate.  At this point the user interface may provide a selection of data pertinent to the selected event.  Such data may include an indication of the selected event as well as one or more of a designation of the division, the round or heat\nnumber, the date of the event, start time, e.g., for the event or heat, the duration, which competitors are performing in the heat, the number of participants in the event or heat, a time period within which fan engagement may be accepted, and the like. \nThe user may also engage the settings menu so as to determine the various settings the event has been configured under.\n Further, as can be seen with respect to FIG. 3D, a search window may be opened.  For instance, one or more dropdown menus may be used to allow a user to perform a search wherefrom they can select an event from a list of events, and/or select\nfrom a list of other users to invite to view and/or participate in the event.  These parameters may further be limited by filtering by gender and/or type of event, as well as event classification.  In various embodiments, the event classification may\nallow one or more users to invite one or more other users or performers to participate in the event, e.g., either as an observer or as a participant.  For instance, a number of invitees may be selected for being invited to participate in an event, and\nfurther, those who seek to participate in the event may also send a request to the organizer for authorization to participate, e.g., as a scorer, of the event through the application.  In such an instance, those to be invited and/or requesting invitation\nto participate in the event will show up on the user interface as a list of potential invitees for approval or disapproval, such as by the event organizer.\n Further, as can be seen with respect to FIG. 3E, the user interface may display a list of events available for participation in, which events may be demarcated by type and by the gender of the various competitor events.  As illustrated, the\nlisted events are certified, public, private, invited, and may also include a list of broadcasts, e.g., televised, and completed events.  And as indicated in FIG. 3F the events may be listed in any order, such as alphabetically or by time of event or the\nlike, may be categorized, such as by event type and the like.\n As indicated above, another important aspect of the technology is a geographical limiter, by which the system may limit those who may participate in the event in various manners, such as by organizer choice, geographical region, time period, and\nthe like, as depicted in FIGS. 4 and 5.  For instance, as can be seen with respect to FIG. 4A, a list of participants may be presented at the user interface.  In this instance, the participants in consideration are those selectable as judges, such judges\nmay be official judges or unofficial judges, such as evaluators and/or scorers of the event.  Particularly, a list of potential judges may be presented, where the identity of the judge and their history of participation in the event may be listed.  In\nsuch an instance, the event organizer, or other authorized party, may be enabled to select those who may participate, e.g., as a judge or scorer, of the event.\n As can be seen with respect to FIG. 4B, the user interface may present a map of available events that are possible to attend and/or participate in, such as by geographical area, by date, by time, etc. And once the event is set up and entered\ninto the system, the present event screen may be presented, which screen may demarcate the region of the event as well as the event identification, the event/present date and time, and the selected judges for the event, as well as the scoreboard\ndetailing the scores of the judges/evaluators.  Also, where the user is an event organizer, e.g., an administrator, the user interface may include various control screen functionalities that allow the user to control the event particularities, such as\nwhen the event will begin, end, and/or may include a delete of event or event particularity function.\n Furthermore, as can be seen with respect to FIG. 4C, in various instances, the system may be configured for allowing the real or past time viewing of the event, in its entirety or by competitor, with pause, fast forward, and/or reverse\nfunctionalities, as appropriate.  For instance, the system may be configured for streaming the event, or a portion thereof, live.  Particularly, streaming multimedia, such as via a cellular or Internet network, is multimedia that may be broadcast by a\nstreaming provider to an end-user client device, such as a mobile telephone or tablet computer.  In various instances, dependent on the configuration of the system, the streaming may require more or less bandwidth and/or latency whereby the data may be\ncached locally.  A stream of media can be provided on-demand or live.  On demand streams may be stored on a server for a long period of time, and are available to be transmitted upon receiving a user's request.  Live streams may still use a server to\nbroadcast the event, but are typically only available at one particular time, such as a live sporting event, a theatrical performance, a political debate, an educational lecture, or a concert, and the like.  In various instances, live streams may be\nedited, compiled, and/or formatted real-time for distribution in a plurality of formats so as to be appropriately sized to be viewed on the screen of the mobile device to which it is being broadcast.  In certain instances, the live stream may be\nconverted into on demand stream for later content consumption, such as via a time-delay.  The live online presentation of content to large streaming audiences may be unidirectional or bi-directional so as to facilitate observer participation.  The\nsystem, therefore, may be configured for allowing one or more users to record the event, and play it back via the application.  Consequently, the real-time date of the event may be displayed along with the duration of the current heat and/or the event\nitself.  Additionally, the accepted judges and/or scorers may be selectable so as to see their scores, raw or corrected, per competitor per heat.  Likewise, the current leaderboard with current and past scores may also be presented for viewing.\n As indicated, a unique feature of the system is a geographic locator.  In one use model, as can be seen with respect to FIG. 5A, the geographic locator may be employed so as to search and locate events for viewing and/or participation, such as\nthrough a search functionality, whereby various events within the system may be searched by geographical region.  Specifically, the events may be searched by a drop down menu of regions hosting events, such as by continent, by country, by region, by\nstate, by county, by city, by town or municipality, and the like.  Alternatively, as can be seen with respect to FIG. 5B, at any point, an interactive geographical map may be presented for searching a specific region, which map may not only locate events\nwithin the region, but may also locate one or more of the users and/or other participants.  The map may be navigated via a capacitive sensing, touch screen interface allowing the user to touch and swipe their fingers across the screen as a means of\nnavigating the map.  Performing a pinching motion well compress the screen, while performing the opposite of a pinching movement, e.g., performing an opening movement, enlarges the screen.\n Additionally, the geographical indicator may be used to limit who can participant, such as an evaluator or scorer, in the event.  Specifically, the system may be configured to limit use by one or more of location and distance.  For instance, in\nvarious instances, it may be desirable to limit those who can access the system, e.g., the online system, such as via the application, to participate, e.g., as an evaluator, in an athletic event, such as an athletic event, competition, or performance of\nthe system.  Particularly, if an athletic event is being performed within a given location, such as within a given municipality, within a given city, within a given county, within a given state, within a given country, within a given nation, and such,\nthe system may be configured to limit those who can participate within the event, as an evaluator, to those within a certain region, such as within a given distance from the event, such as by one or more of the event location, municipality, city, county,\nstate, or country, or nation, and the like.\n For instance, as can be seen with respect to FIG. 5C, a regionally broadcast event may take place, where the system has set forth one or more perimeters within which one or more event participants may watch, evaluate, and score an event.  In\nthis instance, unlimited participation as an evaluator of the event may be limited to a region of the country, such as within a first perimeter, such as a county of California, like San Francisco.  A second perimeter may include the states of California,\nOregon, and Arizona.  Evaluators within this perimeter may be allowed to participate, but the evaluations they enter may be discounted based upon a determined negative geographical factor because they are deemed to not be located particularly close to\nthe event.  Other parameters may also be demarcated and further restricted, e.g., based on negative geographical and/or time factors and the like, or excluded from evaluating the event altogether.  These parameters are demarcated by the hashed lines. \nHence, observers of the event in regions outside of perimeters one and two, as indicated by the dashed line perimeters, would be more limited in their participation and scoring, e.g., at least their scoring would not be given as much weight, and those\noutside of a demarcated perimeters, would be excluded from scoring the event altogether, depending on how the parameters were set up by the event organizer.\n FIG. 5D shows a user interface in which the user may enable the client application to identify the user's location, e.g., so as to employ that location to evaluate for potential user bias.  As shown in FIG. 5D, the user may enter their location\nor the system may identify and/or determine the user's location automatically.  This may be done in a number of different manners, such as using cellular tower triangulation, GPS location, and/or through RFID determination.  For instance, in one\nembodiment, the user may enable the system to detect the user's location via the phone's Radio-frequency identification (RFID) technology, which utilizes electromagnetic fields to detect and track tags attached to objects.  Location identification can\nalso be determined using image recognition.  Particularly, in various instances, the application can then employ the user's location to evaluate bias, such as based on the user's proximity to the event.  As the proximity of the user to the event\nincreases, the higher the entered scores may be weighted with the system's A/I \"weighting\" module, as explained in greater detail below.\n As can be seen with respect to FIGS. 4C and 6A the system may be configured for broadcasting or otherwise displaying the audio and/or video of the event and/or the performances therein so that remote users may listen and/or view the event.  For\ninstance, the system may include, or may otherwise be configured for, receiving audio/video of the event, and for broadcasting that event for listening and display at remote client and/or recipient devices.  Specifically, the implementation of the system\nmay include capturing audio files and/or video images of performers in the competition or performance, transmitting such audio and/or video, e.g., in live-streaming fashion or as an audio/video file, to one or more of the associated client devices, and\nallowing the user to listen and/or view and score the performance remotely.\n More specifically, a user of the system may stream an event broadcast live, or may receive and watch a recording thereof, and may further be presented with an interface presented at the display of the device that not only shows the event but\nalso allows the user to engage in the event, such as by evaluating and/or scoring the performance, as described herein, or by messaging one or more of the other participants, or one or more of their friends with whom they are associated, such as by a\nsocial media platform.  In a manner such as this, the system is configured for allowing those outside of the local area where the event is being held to participate in the event.  However, as indicated, the system may take into account the user's\ndistance from the event, and/or the time lag between their observing the event and entering scores, when determining the appropriate weight to be given to the scores entered by remote observers.  In certain instances, the system may be configured such\nthat scores are to be entered and transmitted to the central server such as by engaging with a displayed key or number pad.  For instance, the system may include a communications module, which communications module may include a suitably configured\ntransmitter and/or receiver.  For example, a typical transmitter may be a radio frequency (RF) transmitter, a cellular transmitter, WIFI, and/or a Bluetooth.RTM., such as a low energy Bluetooth.RTM.  transmitter unit.  In some instances, a typical\nreceiver may include a satellite based geolocation system, e.g., a GPS, or other mechanism for determining the position of an object in three-dimensional space.\n In various instances, the system may be configured such that the scores may be entered and/or transmitted such as through initiation via a voice command.  In such an instance, as discussed in greater detail herein below, the system may be\nconfigured such that a user may engage an activation switch or button, either physically or via voice command, and once activated may speak into a microphone of the device, so as to verbally enter an evaluation or score for the performer and/or for the\nperformance.  Specifically, as explained below, in various instances, the system may include an artificial intelligence (A/I) module that may include a learning or training platform that is configured for learning the voice and words or phrases of a\nuser, and may further include an inference engine that is configured for predicting the meaning behind the words and/or phrases employed by users, especially with respect to their use of the system to engage in, e.g., evaluate and/or score, an event.  As\nsuch, the system is capable of receiving voice commands from a user in their scoring of the events of a competition or performance, and transmitting the same to a remote, e.g., central server, of the system.\n Particularly, as can be seen with respect to FIG. 6A, as indicated above, the system may include a downloadable \"APP\" that is capable of being downloaded and/or otherwise installed on a user device, e.g., a mobile computing device that may be a\nmobile phone or computing watch.  More particularly, FIG. 6A shows a pipeline in which a user may utilize voice commands to transmit information to the application to be entered into the system and considered thereby, and which, in certain embodiments,\nmay be re-sent for display by one or more other devices, such as a second device, e.g., mobile computing device, of the user.\n For example, a computing device configured as a mobile phone, or in this instance, a wrist-worn watch or bracelet may be provided, where the watch or bracelet includes a display screen upon which a user interface of the system may be presented. \nThe interface may include a screen or viewer upon which a performance in an event may be listened to or viewed, and/or a user dashboard may be presented, which dashboard, as disclosed herein, may display information about the event and its competitors or\nperformers, and may include a mechanism by which the user may engage in the event, such as an evaluator, scorer or commentator, either officially or unofficially.\n More particularly, in various embodiments, the dashboard, displayable on a display screen of the phone or watch, may include a user engagement interface that allows the user to activate the microphone of the device, such as through tapping or\notherwise activating the system, so as to receive a voice command from the user.  The voice command may be in natural language, and may be with reference to describing a competitor's performance in the event, such as by scoring the performance.  Upon\nreceiving a voice command, such as a score, the system, via the mobile computing device, may then transmit the voice command to a central server, such as to the A/I module of the system.  The A/I module may be configured to include a voice recognition\nand/or modulation module that is capable of receiving and determining the meaning behind a user's voice commands.  It is to be understood that although the immediate description is with respect to a voice command given to a smart watch or bracelet, any\nsuitably configured smart, computing device may be employed for these purposes.\n Accordingly, as can be seen with respect to FIG. 6A, the application is capable of running on multiple devices including smart phones and smart watches.  For instance, utilizing the core foundation technology, the present system has developed\ndevices, systems, and their methods of use so as to provide competitors and or fans real-time information to a wearable technology like smart phone, watch, athletic bracelet, and the like.  In such an instance, a competitor can have a wearable on their\nperson and be alerted to where he\\she or the device should physically be located, e.g., in order to begin their performance.  One such example would be using a wearable having an interface, e.g., display, that is changeable in color or presentation of a\ngraphic, such as a number, that may flag or indicate the position to be in, the routine to be followed, the action(s) to be taken, and/or the order or sequence of events to take place, as well as the order and sequence of performers.\n The device can also display which performer is performing, who just performed, who is on-deck, and may further display the characteristics and/or results of the performers previous performance, and may additionally display what scores need to be\nachieved in order to beat the previous performance and/or advance to the next round.  Additional information can also be sent from the server to the performer via the wearable device, which information may include real time messaging, information flags\nor placards setting forth useful information, alerts, calculations, statistics, etc. including time until up, place, distance traveled, schedule notifications, safety concerns, etc. For example, in the context of surfing, a competitor may be sitting in\nthe water and there are colored buoys or notification devices present (buoy, number signs, colors flags, etc.).  Using the present technology, the application could change the color of the watch face to indicate the color of flag they should be\nphysically next to and/or the order of performance, and when.\n As shown in FIG. 6A, the user may utilize voice commands to enter scores by tapping on the watch or speaking a key initiation phrase while the client application is running.  The voice commands of the user may identify the name of the event, the\nround or heat, the competitor or performer of choice, and the desired evaluation and/or score.  Each voice command may be interpreted by the system's artificial intelligence (A/I) modules, and the user's given evaluation or score is returned via the\napplication protocol interface (API).  Once the evaluation and/or score is received by the system's A/I \"weighing\" module, the evaluation may be parsed for meaning and/or scores may be weighted.  For instance, the evaluations and/or scores may be\nweighted and/or ranked according to several factors that may reveal the meaning and/or reasoning behind the evaluations and scores, which may include an analysis as to the presence of bias in the evaluations and/or scoring, such as where a higher level\nof bias will introduce a lower weight and/or rank to the entered evaluation or score, and a lower level of bias will introduce a higher weight to the evaluation or score.  As described herein, the system's A/I \"social\" module may be configured to\ndetermine a plurality of sources of bias, such as via one or more, e.g., all, social media platforms.\n In order to store information, in this instance, from the smart watch onto the smart phone, the watch and/or phone may include a storage resource, such as a memory, and/or may regularly communicate with the system server and database to sync the\ndevices.  The memory may be any suitable memory such as NAND, flash, DRAM, or FRAM, or the like.\n In various instances, the voice data may be received and/or entered into the system via a suitably configured application programming interface, API.  Once received by the system, the command may be interpreted by the system, e.g., a speech\nrecognition application, whereby the language will be parsed, and relevant data, e.g., evaluation and/or score data, may be entered into the system.  The system may then forward a confirmatory message back to the entry device, or secondary device, of the\nuser so as to allow the user to confirm that the system has correctly interpreted the voice command.  Where the user device is a watch, bracelet, or the like, e.g., smart glasses, the device may be configured for communicating directly with the system,\nor may be configured for communicating with the system through an intermediary device, such as a mobile phone.\n For example, the device may be \"online\" or \"offline,\" and when the device is offline, a user may record and/or otherwise store data entries that may be transmitted or entered into the system when the device is online, such as when the device,\ne.g., watch, is paired or synced with another device of the system, such as a mobile telephone, whereby the data, such as a verbal message, may then be transmitted to a server of the system.  Once received by the system, the system, e.g., an A/I module\nthereof, may then analyze and evaluate the data and/or weight and/or handicap the data, as disclosed herein, and/or may transmit the data to one or more social media platforms.  The results of the analysis may then be transmitted back to the recording\ndevice, which may be accompanied with one or more other system generated messages, which message may be an evaluation of the users evaluation.\n More specifically, with respect to FIG. 6A, once the user has decided to engage with a spectator event via a smart watch, or other voice activated computing device, the user may configure the application to begin processing a voice recording in\norder to engage with the event.  The user may be signaled by the application to initiate the recording.  The user may utilize recorded voice commands to interact with the client application.  Once the associated devices and client application process the\nvoice recognition interaction from the user, the user may view a visual representation of the recorded and processed voice command on a smart phone.  In a manner such as this, the user may engage with the system and participate in the event simply by\nemploying the audio (and/or video) mechanisms of the system, so as to participate more closely in the event.\n Accordingly, in various instances, the system may include one or more film or video cameras, which cameras may be configured for not only capturing images of the performer or competitor participating in the event, but may also capture images of\nthe crowd, judges, evaluators, scorers, and/or other spectators within view of a camera of the system.  In such an instance, one or more of the captured images may be transmitted to the server, such as to the A/I module, specifically, to an image\nrecognition module of the system.  Hence, in various embodiments, the system may include an suitably configured image recognition module.  The image recognition module may be configured for performing a plurality of tasks.  For instance, the collected\nimages may be examined so as to be employed in a process for determining bias.  Particularly, images may be analyzed by the system so as to identify people who may be associated with one another, such as by having been photographed together, e.g., such\nas attending an event together, being tagged together, commenting on one another's photos, having friends in common, and the like.  For example, images captured at an event can be run against a database, such as a publicly accessible image database,\ne.g., a database of FACEBOOK.RTM., INSTAGRAM.RTM., TWITTER.RTM., FLICKR.RTM., PINTEREST.RTM., or the like.  In various embodiments, the system itself may include a database of stored images, which may be used for determining various associations between\nparticipants.  More particularly, as explained in greater detail below, a knowledge graph of identified images and associated relationships between them may be generated and used by the system in one or more of its analyses steps.\n In other instances, the system may configure one or more image capturing devices, e.g., cameras, to collect images of various different articles of manufacture, such as articles that may include a logo, such as a logo or other identifying marks,\nsuch as a trademark, of one of the event and/or participant sponsors.  The logo may be on a sign, banner, article of clothing, including hats, shoes, shirts, etc., equipment, a tool, and the like.  The collection of logo images may be used and/or\nanalyzed for a variety of different purposes, such as for identifying how many times a given logo or other identifying mark appears in the images.  Particularly, in various embodiments, the cameras may be video cameras, the images captured may be from a\nstreaming video, such as a live feed of the event, and an analysis of the images to be performed may be for the purpose of identifying, qualifying, and/or quantifying the representation of a sponsor or advertiser of the event.  For instance, in various\ninstances, sponsors and/or advertisers may pay to have participants and/or other market influencers wear or use articles with their logo.  However, determining the return of such an investment is often hard to quantify.  Nevertheless, the system may be\nconfigured for determining the value of the sponsorship and/or advertising, such as based on the extent of the coverage, e.g., media coverage and/or airtime.  In such an instance, the number, time, location, prominence, etc. of the image may be used in\ndetermining a value.\n In a further embodiment, the system may be configured for communicating with the performers in the event.  For instance, images captured by one or more of the cameras may be analyzed, and one or more elements therein may be communicated to the\nperformers, such as to affect their performances, such as in the event.  Specifically, an analysis of the images obtained in a competition can be made and feedback as to how to increase the competitiveness of the competitors, or to indicate the\noccurrence, or predicted occurrence, of an event relevant to the competition or its competitors may be made and communicated to the competitors or performers.\n More specifically, in the context of a surfing competition, the system can analyze news feeds, weather feeds, as well as images displaying current conditions that are captured by its cameras, so as to predict a future event, such as the\noccurrence of an upcoming wave, and communicate that information to one or more of the surfers in the water, such as to allow them to adequately prepare for a performance.  Particularly, the system can determine when a wave is coming up, and send that\ninformation to a surfer in the line-up in the water, such as to a waterproof smart watch or phone of the surfer.  In a manner such as this, all the surfers in the competition can be given a smart watch for communicating with the system server, and can\nthereby be given commands or suggestions, or be informed as to upcoming events, such as current standings, wave order, and/or upcoming waves, so as to better inform them of the competition process.  More particularly, the next surfer in the wave order in\nthe lineup may be given an indication that a new wave is coming, from what direction, and/or be notified as to other characteristics of the upcoming event, e.g., wave.  It is to be understood that even though the preceding has been described with respect\nto the event of a surfing competition, the same processes may be applied to any of a number of competitions and/or performances, including but not limited to: combat arts competitions, such as Boxing, Jujitsu, Martial Arts (including MMA), as well as\nother such action sports.\n Additionally, as can be seen with respect to FIG. 6B, the user interface may allow a user to demarcate and/or distinguish between various competitors in an event in a variety of different manners.  For instance, as can be seen with respect to\nFIG. 6B, competitors and/or other participants may be distinguished from one another by different uniforms, such as jerseys, or different icons or avatars, and/or may be selected from one or more images, e.g., a digital photograph, of the participant. \nOther such designators may also be used, such as in a GIF or JPEG image, animation, and the like.  Further, when using a voice initiated engagement the user may designate a performer and/or competitor by name, jersey, avitar, or other such designator by\nvoice.\n Likewise, as indicated, a unique feature of the system is the enhancement of engagement of the audience of an event, such as by allowing individual members of the observing crowd to participate in the event, such as by allowing a user to score\nthe event and/or the event's competitors or performers.  Particularly, event organizers often seek out sponsorships when putting on an event.  Such sponsorship is important for attracting the best competitors and/or performers for participation in the\nevent, because the more money and/or goods that can be provided to the competitors/performers, e.g., the bigger the purse, the more high profile competitors will want to compete in the event.  Likewise, the bigger the purse, the more the event sponsors\ncan pay out, and the more competitive the actors will be, and of course, the more engaging the audience will find the event.  Consequently, as discussed in greater detail herein, audience engagement in the competitions and performances the event sponsors\norganize is an important metric for how effective the event will be for the sponsors present and future collaborations with the event organizers, promoters, competitors, performers, and the like.\n More particularly, watching sports is a favorite past time for many people.  However, participating in athletic events is even more engaging, and yet not everyone can compete on a level to be showcased.  Therefore, the present devices, systems,\nand their methods of use disclosed herein are configured for building audience engagement in the event in a more intimate manner, over just passively watching the event.  Specifically, where watching a performance, e.g., by competitors, may at first be\ninteresting and enthralling, over time those watching may lose interest or grow bored doing so throughout the course of the event.  This is concerning to sponsors because they depend on audience engagement to sell their brands, which sales provide the\nimpetus to sponsor the event and/or competitors and/or performers therein.  As such, audience engagement is of primary interest to event organizers and competitors and performers.  Accordingly, the devices and methods disclosed herein, which are\nconfigured to increase and maintain a high level of crowd engagement throughout the course of the event, are very useful tools of event organizers, participants, and others such as sponsors.  Likewise, as described above, to further assist in determining\nthe effectiveness of the referenced sponsorships and/or advertising, the system can be configured to identify, recognize, collect, analyze, qualify, and/or otherwise quantify the instances wherein the sponsors or advertisers images, logos, and/or\nrepresentatives thereof are captured by the system.  This information can then be transmitted or otherwise communicated to the sponsor or advertiser so as to facilitate and/or calculate their return on their investment in the event.\n Client interactions with the systems herein described are an important feature of the technology.  Consequently, the devices, systems, and their methods of use, as provided herein, are adapted for enhancing and improving crowd engagement through\nproviding apparatuses, means, and methods that are adapted for more immediate and collective fan participation in an event, such as by allowing observers of the event to actually participate in how the event progresses.  Specifically, the devices and\nmethods of the system may be employed for judging and/or scoring of competitive events, be they sporting events, theatrical performances, or even election results (e.g., official and/or unofficial), in a real-time and in an ongoing manner, but also in a\nmanner to make the judging and/or scoring process more transparent, accurate, and honest.\n In certain instances, the participation and/or voting may be directed towards selecting an action of a list of actions that may be performed by the participants.  Particularly, an action menu may be provided to the user interface of a user\ndevice, where the menu includes a list of a variety of different actions from which a user can select an action to be performed.  Once selected, the action may be sent to the central server, whereby the server will collect all entries, aggregate the\nselections, and based on the results will determine which action the performer should take.  These actions may include plays that may be run in a game, moves to be taken by a player, statements made by an orator, songs or lines to be performed by a\nperformer, such as a musical performer.  For instance, in this instance, the actions can be a list of surfing maneuvers, such as a cut-back, a lip-smack, a floater, and the like.  In the instance of mixed martial arts, the list of actions may be fighting\nmaneuvers, such as a roundhouse, take down, rear-naked-choke.  Specifically, the system can present the various suggested actions to the crowd for selection, the crowd can then make its selection, the results can be tabulated by the system, the highest\nscoring action can then be transmitted to the performer, such as for presentation to an electronic device thereof, e.g., a smart watch or phone, and the performer can then implement the winning action.  In various instances, the event may be a speech,\nand the list of actions may be arguments, comments, and/or other dictum to be made during the speech.\n For instance, in various embodiments, a system for aggregating and/or electronically displaying a judging, e.g., crowdsourced judging, of an activity is provided.  In various embodiments, the system includes a client computing device for\nimplementing a client application, such as for execution on one or more, e.g., a plurality, of client computing devices.  In such instances, each of the client computing device may have an interactive display, where the client application generates an\ninteractive user interface on the display.  Particularly, the interactive user interface provides a plurality of input prompts, for instance, where each of the plurality of input prompts represents a qualitative and/or quantitative judgment of the\nactivity, and in some instances, the input prompts may be individually selectable by a user of each of the plurality of client computing devices to input a user judgment of the activity.\n The system may also include an aggregator server or computer that may be connected with each of the plurality of client computing devices, such as via a communication network.  The aggregator computer may further be configured to receive the\nindividual selections of the input prompts by the user of each of the plurality of client computing devices so as to generate the qualitative and/or quantitative judgment.  The aggregator computer may also be configured to generate, for transmission to\nthe interactive display of selected ones of the plurality of client computing devices, a display of an aggregated qualitative and/or quantitative judgment of the activity, such as based on at least some of the received individual selections of the input\nprompts by the user of each of the plurality of client computing devices.  In various embodiments, the aggregator computer may further be configured to receive, e.g., via the communication network, an official qualitative and/or quantitative judgment\nfrom an official application executed on an official computing device, such as that of an official judge of the competition.  In such an instance, the aggregator computer may then be configured to generate, for transmission to the interactive display of\nthe selected ones of the plurality of client computing devices, a display of the official qualitative and/or quantitative judgment, such as from the official judges of the competition.  The system may additionally include an administrator computer that\nis in communication with at least the aggregator computer, where the administrator computer is configured to receive an input representing a qualitative and/or quantitative limit and/or time limit for users to enter their individual selections of the\nqualitative and/or quantitative judgment of the activity, beyond which limit scores will be discounted or discarded altogether.\n More specifically, FIGS. 7A-7P provides a client application of a user computing device, e.g., a mobile telephone, smart watch, and the like, that allows a user to engage in an athletic event in a more intimate and significant manner.  For\ninstance, the client application of the platform may be configured for generating a user interface so as to allow a user, with the appropriate authorizations, to participate in, e.g., evaluate, score, a performance or competition, and thereby become more\ninvested in the outcome of the performance and more intrinsically engaged with the event itself.  In certain embodiments, the client application can generate a series of screens via the user interface so as to prompt the user for entering the information\nthat is fundamental to setting up and/or organizing their user profile.  The user interface will prompt the user to enter information about themselves, including their name, residence, as well as whether they wish to interact with the system anonymously\nor not.  Likewise, the system may allow the user to select their category and/or level of participation, for instance, as a competitor, a judge, an organizer, a scorer, commentators, and the like.\n As can be seen with respect to FIG. 7A, the user interface may present an interactive window e.g., for evaluating, scoring, making a selection, and the like, whereby the user may take an action, such as select a score, make a judgement or\nsuggestion, or evaluation, or the like, with respect to the event or a competitors performance in the event, such as where the window presents to the user interface one or more categories to select from.  As depicted in FIG. 7A, a scoring matrix is\nprovided, where the scoring matrix is configured as a number pad.  In this instance, the scoring may be input into the system by the user interacting with the number pad to select a number, e.g., from a range of numbers, such as from 1 to 10, by which to\njudge a competitor's performance.  It is to be noted that although a numeric interface is depicted, any reasonable mechanism for judging, scoring, evaluating, commenting, etc. may be employed, such as a scale, for instance a sliding scale, an alphabetic\nscale, a scale of icons, such as from happy to sad faces, and the like.\n In various instances, as depicted in FIG. 7B, a list of activities to be performed, such as particular routine in a list of routines, for example, in a professional, semi-professional, or amateur sporting event like gymnastics, X-games, other\nextreme sports, a boxing or MMA match, or a list of maneuvers in a surfing, skating, skate-boarding, biking routine, and the like.  As depicted, a list of surfing maneuvers are set forth, but any list of maneuvers for a given activity may be set forth so\nas to describe the activities that may be scored in relation to the underlying activity.  In various embodiments, the maneuver selected may be a suggestion for the participant to perform the selected maneuver, or the maneuver selected may represent an\naction that describes or otherwise characterizes the maneuver the actor actually performed.\n Specifically, as indicated, the evaluation and/or scoring may be performed in relation to one or more categories of activities, such as maneuvers that may inherently have point modifiers that automatically adjust a predetermined or previously\nentered score, or may be entered to justify the score given.  In certain instances, the list may include typical characteristics of the actual performer, such as their sportsmanship or their attitude while performing the maneuver or event generally, and\nthe selection may be used to automatically add or subtract from an entered score or evaluation.  A description of the activities and/or their difficulty levels and/or how to employ them in a scoring methodology may also be described and presented at the\nuser interface.\n Additionally, as set forth in FIGS. 7C and 7D, if desired, the user may tag the score, or an image captured, for entry into the system and transmission to others.  For instance, as can be seen with respect to FIG. 7D, the user may add notes to\ntheir evaluation and/or score for entry into the system, if desired.  Such notes may also be tagged, e.g., by the user, and/or connected to a specific performance and/or score entered by the user.  Further, with respect to FIG. 7E, the user may also\nselect to give the system privileges to interact with one or more of their social media platforms(s).  For instance, the user may allow the system access to their FACEBOOK.RTM., TWITTER.RTM., INSTAGRAM.RTM., FLICKR.RTM., PINTEREST.RTM., SNAPCHAT.RTM.,\nWHATSAPP.RTM., and other like social media accounts.\n Likewise, the user can specify for the system the other users, e.g., judges, competitors, and/or other scorers who they follow, or the system may determine the same autonomously and/or automatically, as described in greater detail herein below\nwith respect to a suitably configured API, web-crawler, spider, robot, bot, and/or skimmer of the Artificial Intelligence Module, that is used to gather and/or harvest online information, which information may be employed by the system to average,\nweight, rank, adjust, and/or otherwise perform a statistical analysis on the evaluations and/or scores entered by various users, such as in the case of determining and/or predicting bias.\n Information about the online usage and practices of a user of the system may be gathered based on what sites various users of the system visit, how they comment and/or interact with those sites and/or other users on those sites, such as the\nmessages they send, texts or images or other data they post, as well as the types of engagements and/or relationships they form thereon.  This data may be collected by the system and may be fed into the A/I module, e.g., a machine learning and/or image\nprocessing platform, and may then be used as data points to form and/or structure a searchable database and/or image graph of the system.  In particular instances, the data may be text data, email data, photo or other image data, data pertaining to\nonline usage, postings, searches, metadata pertaining thereto, and the like.  In further instances, the data may be communicated by the user and/or their engagement with a social media platform, such as FACEBOOK.RTM., TWITTER.RTM., INSTAGRAM.RTM.,\nSNAPCHAT.RTM., WHATSAPP.RTM., and the like.\n In various instances, the user may allow the system to notify them when particular events occur, and/or when particular participants are competing, in which case the user may be granted access to the event information, so as to encourage the\nuser to attend and/or otherwise participate in the event.  The user may also be notified when other users, e.g., those they follow, attend events, and/or participate, such as by entering evaluations and/or scores into the system, in which case, the user\nmay be notified when other users post evaluation, scores, or comments and/or when those evaluations, scores, and/or comments evidence bias.  This may be useful for keeping evaluators, scorers, commentates, and/or judges honest, such as when they know\ntheir entries may be published, broadcast, or otherwise transmitted to others.\n More particularly, as described in greater detail herein below, once access to the user's online usage characteristics, including social media usage, has been granted, the system may then be configured to automatically access these platforms,\nand mine them for data that may be useful to the present system and its users, such as for determining authenticity and/or bias.  For example, the system may analyze the various social media of the participants, with respect to their followers, so as to\ndetermine relationships between the various participants, such as from identifying common followers, and/or the commentary thereof, posted on social media.  Once set up and authorized via the graphical user interface of the client device, the user\nthereof may then select from the available events to participate in.\n Accordingly, a feature of enhancing fan engagement allows for leveraging each fan's social network because of the fan's involvement in the event.  Particularly, the more involved a fan is in the event, the more likely they will be to post about\ntheir experiences on one or more social media platforms, thereby peaking the interest of others that may not be present at the event, and/or indicating their increased level and engagement with the event.  In such an instance, interested users viewing\nand/or receiving the posting who are not at the event may then be motivated to either go to the event and/or watch or listen to a broadcast of the event, such as on the application or other available media.  As described herein, an event to be entered\ninto the system may be configured by the system to be live streamed contemporaneously via the application interface.  In a manner such as this, a positive feedback loop may be generated wherein the audience engagement is promoted, the more the audience\ngrows, and the more the audience grows the more audience engagement there will be, and consequently, the more sponsors will be willing to invest in such events and/or competitors, e.g., those who are trending, and/or organizers of such events.  Hence, a\nuseful implementation of the system is promoting a social collaboration between participant actors, e.g., those performing the event, and participant evaluators and/or scorers, e.g., those informally judging the event, and as well as their friends and\nfollowers.\n More particularly, event participation may be enhanced by encouraging users to engage their social media platforms, with respect to their participation in the event, in a more meaningful way by rewarding them for doing so.  For instance, any\nsuitable rewarding scheme may be employed so as to embolden social communication of the event between those at the event and those not at the event, such as by granting points for every appropriate post, which points may be redeemed for coupons for\npurchase discounted goods from the sponsors or others, or for free products, cash rewards, and the like.  Accordingly, an element of the system is the use of real time polling and/or voting, which polling may be used to determine audience engagement,\naudience opinion, audience projections, audience interpretations, predictions, and the like.  Such polling may be in answer to a prompt, may be an electronic survey, a questionnaire, in short answer format, and the like, which prompt is generated and\nprovided to the user s by the system.  Through the use of real-time polling and/or the sharing of contemporaneous results data and commentary, a predictive model of participant behavior may be determined, an analysis made, and feedback presented of the\npredictive analysis.\n Further, as described in detail herein, the user may give the system access to one or more of their social media platforms that they use such as for maintaining social contact and connectivity their social base.  This is a useful feature of the\nsystem because it will allow the system to better determine possible bias in scoring activity.  This may be done for one or more, e.g., all, of the participants in the event, such as all of judges, competitors/performers, evaluators, and scorers.  As\ndepicted, a screen for a competitor is set forth in FIG. 7F, where the screen displays the competitors identity, designator, bio, and other information about the competitor.  An additional feature of the system is a screen that will allow the user to\npredict or otherwise select a winner of the round or entire event, and/or may allow the user to predict the score or the reasoning behind what the event outcome will be.  A time left window may also be displayed, for instance, for indicating for how much\nlonger the voting window will remain open and votes will be received and counted.  The designator for which round of the event the competitors are in may also be displayed.\n FIG. 7G shows a user interface in which a portion of the window is configured for video streaming and another portion contains information about the current or a future, e.g., next, event.  For instance, as shown in FIG. 7G, the user may view a\nreal-time display of the current event such that a portion of the user's display is configured for streaming audio and/or video, e.g., of a participant presently competing in a heat or round of the event, and another portion may be configured for\ndisplaying a real-time countdown until the heat or round is over and/or until when the next heat or event is to begin.\n Particularly, in using the APP, the user may opt to view a live stream or previously recorded spectator event, such as by selecting a toggle presented at the user dashboard of the client device.  The display may be presented in a number of sizes\nor display formats, such as where the display takes up the entire screen, or a substantial portion thereof, so as to allow the user to watch the event on the display of their computing device.  However, in various instances, a split screen may be\nprovided whereby the user can both watch the event, and participate with the system in a number of different ways, such as by evaluating and/or scoring the event, making notes and messages regarding the event, and/or receiving the same.  In this\ninstance, a real-time countdown until the next spectator event is presented.  However, as shown in FIG. 7H, the present score board may be presented on a split screen or as an entire screen of the device.\n As indicated, the system may be configured for providing a live stream of an event.  Particularly, the system may include an imaging mechanism, such as a camera and/or video camera, for capturing images of the event, its competitors and\nperformers, and/or spectators, which images may then be transmitted over a network, such as the Internet or a cellular network, to a server of the system, which server may then broadcast the images to one or more other client devices connectable to\nand/or associated with the system.  The camera may be a professional high-definition, high-grade digital video camera, and the like, or may be a simple, low resolution video recording camera, such as the type endogenous to a mobile computing device, such\nas a video camera inherent to a mobile telephone, tablet computing device, and/or smart watch.  Particularly, in one embodiment, the camera may be a camera configured for broadcasting the event, which camera is made accessible to the system server, such\nas by connection therewith via a suitably configured internet connection.  In other instances, the camera recorder may be a camera of a mobile telephone, which may not otherwise be configured for broadcasting the event, but through its connection through\nthe internet or cellular network, it can now be used to broadcast an event to other users of the system.\n Accordingly, in a manner such as this, the system may be configured for interacting with a previously configured broadcasting system so as to broadcast an event, e.g., nationwide, or the system may be configured for interacting with the\nendogenous camera of a typical cellular phone, so as to allow local system users to broadcast an event that would otherwise not be configured for being broadcast.  Specifically, the system may include a camera that is capable of capturing or otherwise\nrecording images, and is connectable itself or through the endogenous components of the mobile phone, to a network whereby the captured images may be transmitted to an encoder, in a manner so that the captured images may be suitably encoded, and once\nencoded the images may then be processed, e.g., via an HTTP-based live streaming (HLS) or other similar communications protocol, and transmitted and/or broadcast to various of the user client devices of the system, such as through the APP. It is to be\nnoted that the HLS stream is particular to APPLE.RTM.  devices, but other such streams may be configured for transmitting the captured images over other user devices, such as with respect to MICROSOFT.RTM., GOOGLE.RTM., SAMSUNG.RTM., FIREFOX.RTM., and\nthe like.  In a manner such as this the HLS may be presented at the user interface presented on the display of the client device through the downloadable APP, such as presented at FIG. 7G.\n FIG. 7H shows a user interface that displays a competitor scoreboard.  As shown in FIG. 7H, the user may view a real-time display of the current event, e.g., a scoreboard showing the current standings of the participants in the event and their\nscores, such that a portion of the user's display is configured for showing the scoreboard, and as indicated above, a portion of the screen may be streaming a real-time video of the event.  Particularly, the user may view a competitor scoreboard.  In the\npresent example, the scores displayed include: the user's entered score, the average unofficial score entered by users of the client application, and the score given by the judges.  The system allows the crowd attending the event to be engaged in the\nscoring, and enables alignment of individual user scores to crowd scores and judge scores.  Further, as described herein, the entered scores may be corrected for bias, such as based on the manner by which the scores are entered, e.g., are they\nconsistently higher or lower than the crowd and/or judges, such as with respect to given performers or competitors; based on the identity of the user and/or device, e.g., have scores from the same person or device been entered more than once; based on\nthe timing of score entry, e.g., have the scores been entered to long before or too long after a given timing window for entering scores, such as too long before or after an event has begun; and/or based on the geographical location of the user or client\ndevice, such as determined by the client device GPS or cellular triangulation, and the like.  Additional methods for determining bias may also be employed by the system so as to correct for potential bias by the various evaluators, scorers, and/or\ncommenters of the system.  For instance, as described in greater detail herein below, the system's A/I module may be configured for evaluating the scores, location, and the timing of their entry for bias, and may correct or discard the scores\naccordingly.\n Likewise, with respect to FIG. 7H, the user may engage a toggle via the user dashboard whereby the user opts to view a competitor scoreboard of the event of their choice, and may choose to have a live stream of the event also presented at the\nuser interface.  However, in various instances, the user may opt to have the scoreboard displayed in its entirety, such as in a power savings mode, or the user may elect a partial screen mode.  In certain instances, the user may toggle between screen\nportions, such as to bring up a scoring interface whereby the user can both watch and score the event at the same time.  If the user has scored the event, the user may view the score recorded by the user, the score given by the official judges, and the\nscore given by the crowd, e.g., via the scoreboard interface portion.\n FIG. 7I shows a user interface for selecting a representation of a competitor in an event for uploading into the system.  As can be seen with respect to FIG. 7I, a portion of the user interface may allow a user to demarcate and/or distinguish\nbetween various competitors by different selectable uniforms, e.g., jersey, pictures, avatars, colors, designs, and the like.  Another portion of the display may allow the user to view specific information about the competitors, events, judges, including\nthe name of the event, the current time, the heat, the competitor level (e.g., GROM, junior men's, senior men's, junior women's, senior women's'), and the like.\n FIGS. 7I and 7J shows a user interface for scoring and/or evaluating a competitor selected by the user.  For instance, the user may toggle between screens displaying a wide variety of ways in which to represent the competitors participating\nwithin an event, and once the competitor/performer has been demarcated, here by jersey color, the user may engage the system and participate in the event by recording a score.  The scoring window as depicted in FIG. 7J may include the competitor's name,\nthe competitor's social media, e.g., INSTAGRAM.RTM., username, the heat status, e.g., in this instance, wave number (in a surfing competition), and the representation, e.g., jersey color, selected by the user in the previous screen.  Particularly, once\nthe user has selected a participant to score, the user will be led to a screen for which to enter an evaluation, in this instance a score, via an alphanumeric keypad, voice recording, tick box screen, or other evaluating and/or scoring interface.  Once\nthe user has entered or recorded the score for a participant, the user may view the entered score on the screen, confirm, and submit or re-submit the score into the APP, which then gets transmitted to the central sever of the system.\n FIG. 7K shows a user interface in which the user may view the competitors in relation to each other by score.  As seen in FIG. 7K, the user has the option to view a window displaying a competitive scoreboard.  A portion of the screen displays\ninformation about the competitive event including but not limited to the name of the event, the event sponsor, the event category, the heat, and the scoring data.  For each competitor listed on the scoreboard, the scoreboard displays the competitor's\nname, the top three scores received by the competitor averaged together, and the competitor's ranking in the competition.  For all competitors that are not the top competitor, the number of points necessary to win the competition is listed with the\ncompetitor's information.  Other competitor and/or scoring metrics or statistics may also be provided here for display.  In this instance, the top scores are averaged, from which average a final score is presented, and from this average the competitor's\nrelative standing is presented, such as in the order of the standing.  Other mathematical and/or statistical information may also be provided and displayed, such as the average, the mean, one or more standard deviations.  As indicated, the statistical\ninformation may be accessed by tapping the screen in a manner to bring up a further, e.g., tagged, screen that presents an alternative view of the relevant information.  Particularly, the user engages the user interface to pull up competitor information,\nto pull up scoring criteria, to evaluate or score the competitor, e.g., based on the scoring event, and to see the relative scoring status of the other competitors, judges, and other scorers, and the like.  Once the user has scored a participant, the\nuser may choose to view a leaderboard of all competitors for an event.\n FIG. 7L shows a user interface, which may be accessed by toggling between screens, e.g., by tapping relevant prompts on the user interface, and with respect to FIG. 7L, the user may view more information about an event and share the information\non a social platform.  For instance, the user may select to take a picture and/or make a recording of the event, and may then, using the APP, upload the photo and/or video or audio recording, such as to their account or to the system home page or to\nother social media platform, for viewing by other participants, friends, followers, and the like.  Particularly, in the user interface in FIG. 7L, the user of the client application has the option to view more information about the events including the\ntop competitors or past results, score the event, sign in/out of the application, and/or share event scores with anyone in the user's social network or other participants in the even.\n For example, given a previously scored event, the user may choose to view and/or share the results of the event with other participants and/or the user's social network such as by drafting a message, taking a picture, recording a video or audio\nfile, accessing the storage thereof, and pressing a share button.  The user may also choose to view a leaderboard and/or a direct or indirect livestream of the current event from the same screen.  A past event may also be viewed, in some instances.  The\nuser may also choose to sign in/out of the client application from the same screen by pressing a sign in/out button.  A menu of other screens may also be accessed by the present screen whereby the user may toggle between various others of the screens of\nthe system, such as by selecting them, e.g., pressing them, on the capacitive touch screen of the display of the device.  The user may also choose to score an event from the same screen if there is an event available to score by pressing a score button.\n FIG. 7M shows a user interface in which the user may view more information about the event as well as a real-time countdown displaying the remaining amount of time to score a competitor before the entered score is given a lower weight or\neliminated.  In FIG. 7M, a portion of the user interface may display information about the event including the top competitors and past results.  The user may also choose to score the event from this screen.  Another portion of the screen is configured\nto display in real-time the amount of time remaining to score the current competitor without algorithmic deductions or elimination.  If the competitor is scored outside of this window, the A/I \"weight\" and/or \"bias\" module may give the user's score less\nweight or discard the entered score.  Particularly, the user may choose to view the results of an event by pressing a results button.  Likewise, the user may choose to view a leaderboard of the current event from the same or a different screen, such as\nby toggling through a menu of options.  The user may also choose to sign in/out of the client application from the same screen by pressing a sign in/out button.  The user may also choose to score or comment on an event from the same screen given a\nreal-time countdown of time remaining to score a participant in another portion of the same screen without algorithmic deductions.\n FIG. 7N shows a user interface in which the user may view more information about the event as well as score the event, such as using a voice recognition protocol.  In FIG. 7N, a portion of the user interface may display information about the\nevent including the top competitors and past results.  Another portion of the screen is configured to accept scores from the user either via a numeric keypad or through voice recognition.  Particularly, the user may choose to view the results of an event\nby pressing a results button.  The results may be shown in full-screen or partial screen mode.  If in partial screen mode, the user may choose to view a leaderboard and/or a live-streaming of the current event, and from the same screen, the user may\naccess a scoring platform and may choose to score an event, e.g., from the same screen, via voice recording or numeric keypad.\n FIG. 7O shows a user interface in which the user may view more information about an event.  The user may choose to view the results of an event by pressing a results button.  The user may also choose to view a leaderboard of the current event. \nAnd the user may also choose to evaluate or score an event, e.g., by pressing a score button.  As can be seen with respect to FIG. 7P, a user of the application can select a competitor to record and can enter that recording into the system, which file\ncan be viewed and downloaded by other users of the system, and if desired, can be directly uploaded to the user's social medias accounts from the application.\n Accordingly, in view of the above, in various embodiments, an interactive, crowd-source communication platform for judging an activity by one or more participants of an event having the activity may be provided.  The platform may include a\nplurality of client application programs that are distributed to a corresponding plurality of mobile devices having an interactive electronic display.  In such an instance, each client application program may be configured with a mobile device identifier\n(ID) of a mobile device executing a corresponding client application program, a user ID of a user associated with the mobile device, user information, and/or location data representing a location of the mobile device.  In such an instance, each client\napplication program may be further configured to display, on the interactive electronic display of a corresponding mobile device, a plurality of input prompts, such as where each of the plurality of input prompts represent a qualitative and/or\nquantitative evaluation or judgment of the activity.  The plurality of input prompts may be configured to be individually selectable by a user of each of the plurality of mobile devices so as to allow them to input a user judgment of the activity.  In\nsuch an instance, the user judgment may include a time stamp as well as judging data representing the qualitative and/or quantitative judgment of the activity by the user, which may be according to a predetermined judgment criteria, such as a criteria\nconfigured in the corresponding client application program.  The client application program may further be configured to enable the user to submit the judging data to a communication network.\n A server system may also be included, such as where the server is connected with the plurality of client application programs via the communication network, and may be synchronized with a performance of the activity.  For instance, the server\nsystem may be configured to receive one or more of the judging data, the time stamp, the mobile device ID, the user ID, the user information, and the location data from each submitting user of one of the plurality of client programs, and may further be\nconfigured to filter the scores according to the time stamp, the user or device ID, and/or a determined bias of one or more of the authorized users.  Accordingly, the server system may be configured to generate an aggregated score from the filtered\nscores, such as for transmission to each of the plurality of client application programs for display in the interactive electronic display with the one or more events of the activity.\n As can be seen with respect to FIGS. 8A-8D, another useful feature of the system is the timeout window.  Particularly, in various instances, it may be desirable to limit the evaluations and/or scores to be considered in judging and/or scoring an\nevent, entered by a user of the system, based on the timing in which those comments and/or scores are entered.  For instance, during an athletic competition, such as a surfing, boxing, jujitsu, MMA, or a gymnastic event, e.g., where the competition\nincludes a number of actions or heats performed over time, the system may generate a time period or window within which scores will be accepted, and beyond which scores will not be accepted.  In certain instances, the timeout window may be based on an\nabsolute time period, such as a 3, 5, 10, 20, 30 minutes or more, such as up to 1 hour, 2, hours, 5, hours, or more, even up to 1 or 2 or 3 or more days or more.\n Alternatively, the timeout window may be dynamic, such as a timeout window that is based on the number and/or timing of score entries being entered into the system and when.  For instance, in particular instances, the system may be configured\nfor determining a velocity of scores being entered into the system and thereby determining when to open and when to close the timeout window.  Specifically, during and immediately after an action or event, a certain number of user inputs, e.g., scores\nwill be entered into the system, such as at a certain rate.  This may therefore define a scoring window, which scoring window may be configured to remain open for a time period during which scores may be entered into the system and be counted, such as\nwhere the time period for counting such scores is determined by that point of maximum velocity.  More specifically, the system may first determine a peak score number (e.g., based on the number of entries) and/or peak score velocity (e.g., based on the\nnumber and timing of entries), and based on that determination the length or range of the timeout window period, during which entered scores may be counted or otherwise considered, may then be determined.\n In certain instances, the timeout window may be configured for opening based on when a certain number or level of velocity of scores are being entered, and may further be configured for staying open until that number or level (or other\ndesignated number or level) is no longer being maintained, e.g., until it falls below a designated number or velocity of entries.  For instance, the timeout window may be configured for staying open while there is a certain number of scores being entered\nover a certain period of time, but when the rate of scoring drops to below a certain number and/or rate, the system may close the scoring window.  In various instances, this may be useful because the number and/or rapidity by which one or more scores is\nentered into the system, e.g., by a mobile scoring device, may indicate the extent of the vested interest of the scorer, namely those scores entered during or immediately after the event may indicate a greater interest in the actual event or performance,\nwhile those trailing behind may indicate a lagging interest.\n Hence, for a particular competition, within an event, a given rate of scores per time (e.g., seconds, minutes, hours, etc.) may be determined by which to keep a generated scoring window open.  This may occur in a situation where one competitor\nbegins to compete, a scoring window may be opened, scores come in with increasing numbers, a maximal score number or velocity is reached, and based on that scoring velocity a scoring rate may be determined, and then based on that rate, the size of the\ntimeout window may be determined.  In such an instance, the timeout window may define the period, e.g., length of time, within which entered scores will be counted, such that scores entered before or after the time period will not be counted, and those\nentered shortly after the window opens and shortly before the window closes may be discounted.  Particularly, such scores may form a cluster within which cluster the scores will be considered countable, e.g., valid, and those outside of the cluster will\nnot be considered, or considered invalid, e.g., ranging from a hot to a warm to a cooling and to a cold period, where the cold period represents scores that will not be considered, and the warmer the scores the more weight they are given, where the hot\nscores are the most immediate to the activity and within the largest cluster.\n In various instances, a first cluster may demarcate the beginning of a first competitor's performance in the event, such as where a first timeout window is opened to allow for evaluation of the first performer; and a second cluster of scoring\nmay demarcate the beginning a second competitor's performance in the event, such as where the second timeout window is opened to allow for evaluation of the second performer.  This may be repeated for third, fourth, fifth, competitors, etc. For instance,\nthe clusters of scoring number and/or velocity may be used by the system to not only determine the periods of time during which scores may be counted, but may also be used by the system to determine and demarcate when one competitors performance begins\nand ends as well as when another competitor's performance begins and then subsequently ends.  In other words, the system may be configured for distinguishing between different competitor's performances, and for allocating scores entered to the\nappropriate competitor, such as based on population density within the cluster, cluster size, and/or with regard to other statistically relevant factors.\n Accordingly, in various embodiments, the system may be configured for determining when one competitor begins a performance, such as within a heat of an event, and when they subsequently end, and for allocating evaluations and/or scores entered\nto the final count for that competitor, and likewise, the system can then determine when a second competitor's performance begins and ends and for allocating evaluations and/or scores entered in judgement of the second competitor's performance. \nConsequently, based on the system's determination of when the performance begins and ends, the system may then open and appropriately size the timeout window during which period evaluations and/or scores entered will be allocated to the final evaluation\nand/or score count for that competitor.  The opening and/or closing of the time out window and/or the determining of the score count period may be determined manually or autonomously by the system.\n For instance, in one instance, the opening and the closing of the timeout window may be performed manually.  For example, a system administrator may be tasked with viewing each of the heats of the event, and thereby determining when each given\ncompetitor begins their performance, and therefore, opening or otherwise initiating the beginning of a timeout window, and also determining when the competitor ends their performance, and therefore, closing the timeout window.  This may be done for each\ncompetitor in each heat, and for each heat in each event, e.g., manually.\n In other instances, the timeout window may be initiated and opened and then closed automatically by the system.  As indicated above, the opening and closing of the timeout window may be determined autonomously by the system, such as based on\ncluster factoring, such as where the system receives a number of entries, such as evaluations and/or scores, which entries may be collected and/or analyzed by the bias module for bias.  The entries may then be aggregated into a cluster, and/or factored,\nor otherwise subjected to a statistical analysis, such as to determine one or more of a score number, a score mean or average, a median score, or other statistically relevant factor.  Based on the result of this determination, a timeout window may be\ngenerated and applied to the entered data, and the length of the time-out period may also be set, such as where the period length determines at what point in time relevant to the cluster scores will be counted, discounted, and/or discarded all together,\nsuch as being entered into the system too soon or too late.  In this manner the characteristics of the timeout window may be dynamic.  In various instances, the system may determine the beginning, the ending, and/or the length of the timeout window,\nand/or which scores to count and how to weight them, based on an analysis of prior heats for prior events, such as for the same competitors.  Particularly, this data may be transmitted to the A/I module and/or a deep learning protocol implemented thereby\nso as to adequately and dynamically determining the opening, closing, and period for the timeout window, as well as for the ranking, weighting, counting, discounting, and/or discarding of entries, e.g., scores.\n Additionally, another method for automatically initiating the opening and/or closing and/or the period of a timeout window is through tracking the motions of the various participants in the competition or performance.  For instance, as discussed\nabove, the system may include a smart watch, or phone, or other smart device, that is configured for being worn, or carried, by a performer in the event.  In such an instance, the smart device may include a processor, a communications module, configured\nfor connecting to a wireless network, a GPS, and/or one or more sensors, such as an accelerometer and/or gyroscope.  Accordingly, the smart device may then be in wireless communication with the server such that a change in activity of the wearer in\ncompetition means that a performance has begun, which may be detected by the one or more sensors of the device, such as the accelerometer, gyroscope, and the like, and then that data may be communicated to the server, over the wireless network.  Once\nreceived by the server, the data may be analyzed, a timeout window may be automatically opened, and may stay open until after the sensors in the smart device communicate a change of activity that is indicative of the end of the performance.  Likewise,\nonce the end of the performance has been determined by the server, the timeout window may be closed, and the entries evaluated.\n One or more other characteristics may be employed to determine the length of time during which the time-out window is to remain open, during which time period evaluations, scores, and/or comments may be entered into the system and when. \nLikewise, in various instance, the time period may be increased or decreased, such as determined by the one or more characteristics.  This may be exemplified in a surfing or other competition, like an Olympic competition, where there is a beginning and\nend to one performance, and then a brief window of time before the next competitor competes.  In such an instance, during the competition, the hot window will be open, and after the competition, and before the next competitor begins, the window will grow\nincreasingly colder, and will be closed prior to or at the beginning of the next performance, when a new hot window will be open for judging the new competitor.\n For instance, in a dynamic environment, such as when a surfer in a competition catches a wave, a judgement window will be open, scores will be entered, evaluated, weighted, and/or tallied for the surfer currently on the wave, but once he or she\nis no longer on the wave, a brief time period will be open for the continued entry of scores judging his or her performance on the wave, which will close at about the time the next surfer catches a wave.  Alternatively, the scoring window may be open\nduring the period for the heat and scores can be entered at any time for competitors competing in that heat, designating to whom the scores belong, and when the heat is over, scores can no longer be entered for the competitors of that heat.  In either\ninstance, no scores will be counted after the window has closed, as these scores are less likely to be as valid as warmer scores, and thus, could contaminate the results, and as indicated the window may be open for longer or shorter periods of time, such\nas from hours to minutes to mere seconds, depending on how the system and/or competition is organized.\n Likewise, scores that are entered at the beginning of the performance or heat will be weighted heavier than those entered towards the end.  This will prevent those who have not really seen the performance from having undue influence on the\nscoring, such as by entering high scores after the event has concluded, which could be an indication of bias.  Additionally, as previously described, scores that are outliers, such as too high, e.g., all \"10s\", or too low, e.g., all \"1s\", or scores\nentered for categories that should be emptied, may be weighted lower and/or may be discarded.\n FIGS. 8A-8D shows a user interface in which the user may utilize screen swiping to score an event and/or to view a previously entered score for a competitor in the event.  For instance, FIG. 8A shows a user interface enabling the user to\nnavigate a series of menus, such as by swiping in the indicated direction.  Particularly, by swiping right on the capacitive sensing touch screen device the user may be brought to a scoring screen, whereby the user may be able to look up the statistics,\ne.g., scores, pertaining to competitors who have previously competed and/or are presently competing in the event.  In certain instances, the user may score the event by performing a swiping motion on the screen.  For example, given that a user has begun\nto engage with a spectator event, the user may swipe, e.g., right, on a smart phone to score a participant.  Likewise, in various instances, the user may swipe to a menu that will allow them to see the current or past results of a heat and/or event or a\ncompetitor thereof.  FIG. 8B shows a user interface menu enabling the user to swipe, e.g., left, on a touch screen device to view the results of the event.  FIG. 8C shows a user interface menu enabling the user to swipe, e.g., up, on a touch screen\ndevice to view the previous competitor to the current competitor in an event.  FIG. 8D shows a menu user interface enabling the user to swipe down on a touch screen device to view the next competitor in the event.\n Regardless of the interface, once scores have been entered into the system, the system may analyze those scores for a variety of different purposes, such as for bias.  Bias can be determined in many ways such as can be seen with respect to FIG.\n8E.  Specifically, in the graph in FIG. 8E, the left-hand, y-axis, represents a score range, for scoring a competitor in a given performance, e.g., heat, in a given event.  Likewise, the horizontal, x-axis, demarcates the length of time during which\nscores may be entered.  Accordingly, as can be seen by the graph, scores may be entered by participants of the event, e.g., observers, evaluators, and scorers, such as by interacting with a client interface on their mobile device to enter the scores into\nthe system, during an open window of time.  As can be seen, the scores may form clusters, showing that most observers score the action similarly, around the same time, with their being fewer outliers.\n Further, as seen by the shaded regions of the graph, moving through time from left, at time 0, to the right, as more time passes, as less and less scores are entered, the less the entered scores will be ranked and/or weighted.  In this instance,\nscores may be entered until the timeout portion of the window is achieved, which at such point scores may still be entered into the system, however, they may be given a lesser weight than those entered prior to the timeout window phase being entered. \nFurthermore, once the timeout window has been exceeded, any scores entered thereafter may be discarded, as they have been entered too late to be counted, and may be presumed to be biased.  It is further noted that there are outliers toward the left-hand\nside of the graph as well, which scores represent one or more entries that have been entered too early to be counted, such as before the competitors performance has begun.  These scores may also be discarded.\n Additionally, as can be seen with respect to the graph of FIG. 8E, the various scores entered into the system appear to cluster around one or more numbers.  These scores can be aggregated, tabulated, analyzed, and used to calculate a mean and an\naverage that may be used to determine, via the methods disclosed herein, one or more statistically important standard deviations from which outliers may be determined, scores ranked, weighted, and/or discarded.  For instance, as can be seen, various sets\nof outliers have been determined, and are demarcated in the encircled regions at the edges of the graph.  As these outliers appear to be at the extremes of both high and low scores, while the majority of scores cluster around the average and/or mean,\nthese outliers may be subjected to further processing, and if bias is determined, they may then be subjected to a correction process or excluded altogether.\n An exemplary instance for determining bias may involve the collecting, aggregating, pooling, ranking, and/or tallying of the numbers representing the scores entered by the users of the system, and once entered the scores may be subjected to a\nstatistical and/or factoring analyses, which in some instances may be for the purposes of determining a mean, median, or average score or evaluation, such as a representative score from which other scores may be measured for bias, and/or as indicated\nabove may be used for determining one or more characteristics of a timeout window.  Hence, in various instances, the system may be configured for allocating weight to the received scores and evaluations, such as by increasing the relative value of some\nentered scores, and decreasing the relative value, e.g., weight of others.  Particularly, throughout the scoring process, the results entered into the system may be processed through the artificial intelligence engine, which may be configured to\ncontinuously sample and/or correct the data flowing through the system.\n For example, as discussed, the system may be configured so as to determine when an event, such as the beginning of a heat, or a performance within a heat, is initiated, e.g., a surfer catches a wave, gymnast begins a routine, etc., and based on\nthe number and/or rate of entry of scores, e.g., of the surfer's maneuvers on the wave, and other such particulars, may determine a dynamic window during which scores may be entered into the system, e.g., when a timeout period begins, what weight is to\nbe given to the entered scores, if any, and of course, when the timeout period is to close, such as where scores entered at a time that is too late to be fully weighted and/or included at all.\n Specifically, as indicated, along with the evaluation, communication, score, and/or other user entered data, other various data may be entered into the system and considered such as with respect to ranking and/or weighting, or otherwise\nevaluating, the entered data.  In various embodiments, this data may include one or more of event identification data, event geolocation data, competitor identification data, competitor geolocation data, competitor movement data, competitor profile data,\ncompetitor social factor data, previous competitor performance data, competitor characterization data, competitor image data, user identification data, user geolocation data, user image data, user profile data, user social factor data, previous user\nperformance data, user characterization data, relevant outside characterization data, which in this instance, may include environmental factors such as wave identification and/or prediction data, interval data and mathematical index value data, and the\nlike.  As indicated, this data may be entered or otherwise collected by the system, e.g., retrieved by a database of the system, and may then be entered into a relevancy queue, such as for consideration by an analysis, e.g., A/I module of the system,\nsuch as for determining one or more factors relevant to analyzing the entered data.\n For instance, entered and/or collected data may be subjected to analyses by an artificial intelligence module, as disclosed herein below, which may include subjecting the data to a deep learning protocol and/or one or more predictive models\nand/or one or more statistical and/or factor analyses and/or a ranking and weighting regime.  For example, with respect to ranking and/or weighting, the A/I module may determine the relative importance of entered collected data, may rank the data, e.g.,\nwith regard to its importance for determining accuracy in weighting and scoring, and may factor the data along one more lines for determining bias.  If bias is determined then the entered scores and evaluations may be corrected, their weighting may be\nchanged, thereby evoking a change to the index.  This may be done for individual scores or for clusters of scores.  The weighted and/or corrected scores may then be inserted into a results table, e.g., a table of scores or other evaluations, which when\nassembled may be posted or otherwise broadcast to the client devices of the various different users.\n In this manner, the system can increase accuracy of the judging of performances, as well as enforce timely and reasonable user inputs, e.g., scores, evaluations, votes, opinion, results, and other such data, are entered into the system and\nconsidered by the judging module during a consistent time period.  In particular embodiments, the configuration of the timeout window may be dynamic and its characteristics may be arranged so as to slide in a manner that may change from performance to\nperformance, heat to heat, event to event, based on the actions taking place throughout the system and the actions of the performers in the underlying event.  Hence, the system may be configured to identify when an action or event has begun, when it is\nin the middle, and when it has concluded, such as when enough time has been given to collect the inputs of the crowd.\n For instance, the system can determine when a performer begins a performance, e.g., catches a wave, and an event begins, and likewise, when a performer, e.g., a surfer, completes his performance.  The system may also be configured determining\nwhen the performer is preparing for their next, e.g., subsequent, performance, such as when a surfer is paddling back out to get back in to a position to catch another wave, which during that period, scores can be discounted, before the timeout window\ncloses, such as when the surfer catches a second wave, and results for the second wave may then be entered into the system.  Accordingly, the timeout period may be dynamic and may be made longer or shorter to accommodate real-time adjustments to\nperformer actions currently being taken or are predicted to be taken.\n As can be seen with respect to FIG. 8E, the timeout window presented in this instance, is configured in three different portions.  For instance, a first portion is demarcated in white, and includes a majority of scores that are considered to be\ntimely entered, and presumably to be counted at full weight.  A second window is demarcated in light shading and includes those scores that were entered too late to be given full weight, but not so late as to be discarded.  Likewise, a third window is\npresented where the third window includes those scores that were entered too late to be counted.  As such, in this embodiment, the scores are presumed to be biased solely based on a preconfigured timing consideration.\n Further, as can be seen with respect to FIG. 8F, a scoring window is presented.  However, in this instance, not only is the horizontal, timing, axis being considered, such as for timeliness of scores being entered, but the vertical, y-axis,\ncomponent of the scores is also being considered, such as for bias.  The main section of this graphical representation of scores is demarcated in white.  This section sets forth a mean or average score demarcated by the horizontal hashed line.  This\nhashed line represents the average of all entered scores, which may include or may not include identified outliers.  In this instance, a first standard deviation is determined and is represented by the hashed line above and the hashed line below the\naverage score line.  Three other standard differentiations are determined and are demarcated by the three lines above and below the first standard deviation lines.  For the sake of simplicity, these lines also represent whole number scores.\n As indicated, even though the scores in the central section were timely entered, with reference to FIG. 8E, with reference to FIG. 8F, not all scores are given equal weight.  Specifically, in the instance of FIG. 8E, the more time that elapsed\nwith respect to the timing of the entry of scores, the less weight they are given, as the more time elapses from the event to score entry, the greater the chance of bias is presumed.  However, in the instance of FIG. 8F, the more the scores are outside\nof an identified range, in this instance, a first determined standard differentiation, the more they are presumed to be biased, and as such, the less weight they will be given by the system.  Specifically, with respect to FIG. 8F, the graph represents\nthe scores entered by the participants viewing the event.  The X axis is measured in elapsed time.  The Y axis is measured in the range of scores by which the competitors and/or the performers are judged, in this instance, the scores range from 0-10. \nThere are several clusters of scores.  For the Y axis, there are outlying scores clustered on the top portion of the graph, e.g., near perfect scores, then there are mainstream scores clustered about the middle, and outlying scores clustered toward the\nbottom of the graph, e.g., lower than average scores.\n Accordingly, as discussed herein, these outlying scores, e.g., in the upper and lower shaded regions of the graph, may be one or more standard deviations from the mean, and as such may be potentially indicators of bias, and thus may be discarded\nby the system and therefore not included in determining the average or mean.  Particularly, in determining the potential bias for these scores, the scores may be discarded if bias is found.  However, if bias is not found, the scores may be counted but\nmay be given a lesser weight and/or may be lowered to draw them more into conformity with the mean.  This situation may result where the scorer has a tendency of over-scoring or under-scoring generally.  In such an instance, the system may generate a\ntraining screen whereby the scorer may be given lessons as to how to better score the performers and/or event.  For the scores in the non-shaded section, these scores represent those entries falling within a certain range from a determined present or\nhistoric or predicted mean or average of the distribution of scores.  These scores would normally be counted and fully weighted (if bias is not determined).  However, as can be seen with respect to the X axis, scores clustering around the mean, but which\nwere entered in a non-timely manner, may be discounted, e.g., scores in the lighter grey section, or may be completely discarded, e.g., scores in the darker gray section, such as where the scores are entered too late to fairly be considered.\n More particularly, those scores in the shaded portions above and below the non-shaded portions, represent those scores that fall outside of a preconfigured range, and are therefore, may undergo determination for potential bias, or may simply be\npresumed to be biased.  In such an instance, those scores falling outside of the predetermined range may be given less weight than those falling closer to the mean, median and/or average.  The mean, median or average may be determined in many different\nmanners, and likewise, the range within which scores will be counted and equally weighted versus those outside of which will not be equally weighted and/or may be discarded.  For instance, the scores in the shaded portions may be one or more, e.g., 1, 2,\n3, or 4 standard deviations outside of the determined mean or average.  The variance between the scores and various determined regions of scoring may also be determined, and used in appropriately determining boundaries.\n More specifically, as can be seen with respect to FIG. 8F, there are four specific cluster points, demarcated by the darker shaded circles.  These clusters represent possible indicators of bias, which would flag the scores and/or scorers for\nfurther examination of bias.  In fact, these clusters can be used to set the boundaries of what is considered to be good and/or questionable data in the first instance.  For instance, the two clusters located above the mean (middle dotted) line may be\nused to determine the upper outer boundary demarcating scores that may be too high for consideration, and likewise the two lower clusters may be used to determine the lower outer boundary demarcating scores that may be too low for consideration.\n Scores that appear to be biased toward either upper or lower boundaries can be, but need not be discarded.  Rather, the scores can simply be discounted, if the system determines that the perceived bias does not reach to a level sufficient enough\nto discard the entire score input.  In such an instance, a mean regression analysis may be run on the data, and a corrective model may be employed to discount the entered score to a more appropriate level, or vice versa, where the entered score may be\nincreased to a more appropriate level.  In either instance, when the system determines that an entered score may be the subject of bias, the system may perform an analysis on all associated data to better determine if bias exist, and if so, to what\nextent, and if determined the system may then correct the identified bias, e.g., in a mean regression analysis, if it appears that the data can be corrected.  As can be seen with respect to FIG. 8F, the two upper clusters are of a nature that they can be\nincluded, but discounted, whereas only one of the lower two clusters may be included, and discounted, while the lower of the two clusters is completely discarded.  Further, as noted, the upper and lower clusters, in the shaded portions, may be\ndetermined, analyzed, and used to form the range and/or outer limits of scores to be discarded.\n For instance, FIGS. 8G and 8H illustrate examples where bias has occurred in the judging of an event.  Specifically, a table setting forth a scoring matrix is presented.  Along the top-row are listed the column designators, where the columns\nrepresent designated judges, competitors, scores entered by the judges, the system determined average and mean, along with the variance.  Likewise, the last column represents an action taken by the system to adjust the weighting of a score due to\ndetermined bias.  Specifically, as illustrated in FIG. 8G, a single judge (designated as 11111) is set forth, as well as the scores entered by that judge for scoring three different performances by the same competitor (22222).  The scores entered by the\njudge are 5, 6, and 8.  These scores are then compared by the system to a determined average, mean, and variance that were determined for each judge with respect to the scores entered for all of the performances of this competitor by all judges.\n As can be seen, this judge has consistently entered scores above the average, as indicated by the last column of the table.  If the variance is within a prescribed limitation, the scores may be counted but may be decreased, weighted lower,\nand/or otherwise corrected.  More specifically, once a variation from the norm is determined, the source of potential bias may be explored.  In this instance, the social network of the judge may be mined along with that of the competitor for\ncorrelations.  For instance, the user profile of the judge and competitor can be compared, as well as their social media platforms, and any correlations between the two, such as in this instance, common friends between them and living in the same city,\nmay be identified by the system.  As indicated, in this instance, the judge and competitors have been identified through their user profile and social network as being friends, and these factors may be considered by the writhing, e.g., A/I module in\ndetermining bias.  Any bias from this connection observed by the system may then be accounted for, and/or if necessary the scores may be adjusted.\n Likewise, the opposite scenario is presented in FIG. 8H, where through the processes described herein, the judge and competitor are determined to be correlated and related as non-friends.  The system, therefore, has flagged this interaction for\na determination of bias.  This is exemplified by the fact that the judge consistently underscores the performance of the competitor, and as such the system may adjust the weight to be given to the scores, such as by increasing them by a determined\ncorrection factor, or they may be discarded altogether.  In both instances, the scores entered where not beyond a level of correction, and therefore the scores were able to be saved by being corrected upward, but in other instances, the scores may be\noutside of the determined range, which in such an instance the scores may be discarded.  However, it is better to include as many data points as possible so as to make the final scores stronger, more relevant, and the system more accurate.  Hence, as can\nbe seen with respect to FIGS. 8G and 8H, the system may be configured for mining available data to determine relevant relationships, and to then build a table or graph of the relationships, such as a knowledge graph, described herein, which graph may be\nconfigured for determining relationships between any and all various data points between the competitors and evaluators, e.g., judges.\n Accordingly, a unique aspect of the system is its usefulness for determining and calling out bias, and in view thereof eliminating it.  Bias may come in many forms, such as by one or more evaluators and judges entering a score of a competitor's\nperformance in an event or activity that appears, on its face or upon further analysis, to not be completely reasonable and/or objective.  Such bias, for instance, may be founded on any of a number of different factors, such as nationalism, e.g., country\nof origin, particular demographics, such as age, sex, ethnicity, cultural background, bias due to positive or negative relationships, such as friends, enemies, or competitors, and the like.  Bias may be identified from many different sources, such as via\naccessing private or publically available information of a participant, competitor, and/or judge of the event.  For instance, accessible information, e.g., via a suitably configured API, may include various modes of social media, including FACEBOOK.RTM.,\nTWITTER.RTM., INSTAGRAM.RTM., FLICKR.RTM., PINTEREST.RTM., SNAPCHAT.RTM., WHATSAPP.RTM., and the like.  Such media databases may be searched and mined for data that may then be fed into a separate database, such as a database associated with the system,\nwhich database may be populated with data from each of the participants, competitors, and judges of the event, as described herein.\n The collected data may then be structured into a table or graph, or other relational infrastructure, such as a hash table or data tree or knowledge graph, which may then be used to identify correlations and/or relationships between the data. \nSuch relationships may then be weighted and mined to determine correlations between those involved in an event, which data may then be fed into an artificial intelligence engine to determine and/or predict the presence of bias.  Hence, the present system\nmay be configured for receiving and incorporating event related personal data, as well as additional information, of those associated with an event, to better determine accuracy of the evaluating, judging, and/or scoring of the event.  Additional\ninformation may include an analysis of various social media, e.g., FACEBOOK.RTM., postings of photos, comments, and/or likes or dislikes, as well as who they friend or de-friend, who they like, what and/or who a person follows, e.g., on INSTAGRAM.RTM.,\nand/or what they post on the internet.\n Such an analysis allows for a great quantity of data to be collected and analyzed so as to derive one or more conclusions, such as a conclusion as related to bias in judging an event.  These conclusions can be directed solely to the event in\nprogress, past events, and/or future events, e.g., regarding the quantity and/or quality of bias at a judging and/or competing and/or scoring level.  In other embodiments, as described herein below, the conclusions may pertain to auxiliary factors\nrelated to the likes and dislikes of the observers and/or scorers of the events, which may be useful for determining preferences of the observer and/or scorer, such as for directed marketing research and/or advertising purposes.  Likewise, the collected\ndata may be analyzed so as to make one or more determinations about the representation in the event and/or of a competitor with regard to one or more sponsors and/or advertisers.  In a manner such as this, the system can make a determination as to the\nreturn the sponsor or advertiser is or could receive on its investment, and may make suggestions as how to increase that return.\n In view of the above, an important aspect of the disclosure is a system configured for providing an interactive, crowd-sourced commentating, judging, and/or scoring platform that is adapted for both enabling and encouraging audience engagement\nwhen participating in an athletic event, theatrical performance, and/or any other form of competitive interaction.  Specifically, in one aspect, as can be seen with respect to FIG. 9A, a server system is provided, wherein the server is connectable to one\nor more associated client devices, such as one or more mobile communication devices, such as a cellular phone, tablet computer, laptop computer, smart-watch, and the like.  The one or more client devices may be configured so as to include a client\napplication program, such as a downloadable application or \"APP,\" as described herein.\n In particular instances, the server may be connectable to the client device over a suitably configured network, such as via a wireless communications protocol, such as via a cellular, RF, Wi-Fi interface, or over the Internet.  Particularly, in\nvarious instances, the server system may include a cloud accessible server.  Each of the client-apparatuses may include at least one processor, a transceiver to communicate with a communications network, and a display.  In particular instances, the\napparatus may be communicably coupled to the server system, such as over the communications network.  The connection may be such that it synchronizes the mobile device(s) with the server, and at a time during which one or more activities of the event or\nperformance are taking place.  In such an instance, the server system may be configured for receiving one or more of the mobile device ID(s), the user ID(s), the user(s) information, and/or the location data for each user of the one more client programs.\n Further, the server system may also be configured to authorize one or more users, so as to allow them to participate in, e.g., score, the event; receive the scores inputted from the authorized users; and for adjusting the scores to produce a\nfileted score, as described above.  Particularly, the system may be configured such that a multiplicity of such apparatuses are capable of observing, displaying, and/or evaluating, voting, scoring, commenting on, and/or judging an event or competition,\nsuch as in conjunction with one or more other apparatus possessed by other observers.  More particularly, in certain instances, the server may be configured for collecting, collating, and/or generating an aggregated score, such as from the collective\nand/or filtered scores of all, or a number of users, and for transmission to the plurality of client application devices, and/or for display thereby with respect to the one or more activities of the event(s).  In various instances, the filtering may\noccur so as to take account of the timing and/or location by which the score was entered, e.g., as described above, and/or to take into account any determined bias of a score by an associated, authorized user.  Accordingly, the server system may be\nconfigured for receiving user data, input or collected data or otherwise, and processing that data so as to determine the presence and/or extent of bias in the scoring activities of various of the users of the system.\n Consequently, in various instances, the server may be configured for receiving and processing user and/or device data, and especially for receiving and using user entered evaluation, score, and/or bias data.  For instance, as indicated, the\nsystem may include a mobile scoring device that includes a client application together which may be configured for directing data transmission, e.g., through the endogenous communications module of the scoring device, back and forth e.g., between the\ndevice and the host server, e.g., via the application.  Particularly, in various embodiments, the system may be configured for receiving and transmitting data to and from a plurality of client devices, such as a multiplicity of communication devices,\ne.g., desktop computers and/or handheld cellular phones and/or smart watches, running the same or similar programming.  Hence, in such embodiments, one or more, e.g., each, of the software implementations, e.g., client application programs, which may be\nrun on a handheld communication device, may be configured with a device identifier (ID), for providing a unique device identifier for the device.  Additionally, as indicated, the client application program of the mobile device may further include one or\nmore of a user ID of a user associated with the mobile device, information about the user, and/or location data, e.g., geolocation data, representing a location of the user and/or mobile device, which may also be communicated to the server system, such\nas for authenticating the user and/or the user's location.\n Accordingly, in certain instances, the entered evaluation, score, and/or determined bias data may be pooled, aggregated, a median, mean, average, and/or variance of the aggregated scores may be determined to produce results, and from these\nresults data, a table or graph of evaluations and/or scores to be included and excluded from the results pool, as set forth in FIGS. 8G and 8H, may be determined.  From this data a user and/or device rating may be used to rate the scores and/or\nindividuals entering the scores, which rating may then be used to correct the present and future scores entered by that user and employed by the system with respect thereto.  Specifically, in various use models, each client application program may be\nconfigured to generate an interactive user interface that may be configured for being displayed on the interactive display of the mobile device.  In certain instances, the interactive user interface may display one or more activities of one or more\nevents to be or being observed.  More specifically, the client user interface may be a graphical display for presenting a broadcast of the event and/or a scoring matrix to the user through which one or more scores may be entered, via the user interacting\nwith the score matrix graphic, and thereby being input into the system.\n Hence, in a manner such as this, users of the devices of the system may evaluate or score the activities of one or more competitors or performers engaging in the event(s).  In such an instance, the evaluation or score may include a time and/or\nlocation stamp and/or judging data, e.g. scores or commentary, representing the scoring and/or judging of the activity by the user.  In certain instances, the scoring may be in accordance with one or more predetermined judging categories and/or scales,\nsuch as a matrix configured in the corresponding client application program.  The system may also be configured for receiving third party inputs.\n For instance, a management application programming interface (API) may be employed so as to allow an event organizer to generate an event, configure the system, to set up and/or adjust weighting criteria to be employed by the system, as well as\nto determine and/or modify system parameters to better identify bias and more accurately resolve evaluation and/or score discrepancies.  Additionally, the API may allow third parties and/or third party applications to access the system so as to allow\nauxiliary applications and/or functionality to be joined to and/or added on to the system.  For example, the event organizer may access the system server remotely to set up the event, to enter event data, such as time, date, and place data, as well as\ndata inviting and/or authorizing various users to be scorers, judges, competitors, and the like.  Likewise, the organizer may set up the rules for how the event will take place, e.g., event relevant data, how the scoring is to take place, how the bias\nfilter is configured, and/or what the proximity characteristics will be.  In various embodiments, the system may be configured for broadcasting the event, or portions thereof, to the associated client devices for viewing, commenting, and scoring thereby.\n For example, as described above with reference to FIGS. 3A-3F, an event organizer may configure the event so as to only allow users within a given area to participate, e.g., as an evaluator or a scorer of the event.  Particularly, the client\ndevice of a prospective user of the system may include an identifier, such as a radio frequency identifier, RFID, or other suitable identifier by which the device may be identified.  Additionally, the device may also include a GPS, by which the location\nof the device may be identified and/or confirmed.  Accordingly, the client app running on the device may access the device ID and/or location information, and then send that data to the server, where the server will analyze the device ID, e.g., so as to\nidentify or confirm the identity of the user, and/or the device location and/or GPS data, e.g., to identify the position of the user, so as to then verify if the user is an authorized user, and/or within the prescribed proximity parameters, so as to be\nenabled to enter scores of the event and/or its participants within the system, e.g., via the client interface, as described above with reference to FIGS. 7A-7D.  Hence, each event may be set up to include a proximity parameter, such that depending on\nthe identity of the device and where it is located, scores may be accepted or not be accepted.  Specifically, if a prospective user is outside a determined radius, then device input will not be accepted.\n As indicated, one or more additional 3.sup.rd party APIs may be included and adapted for allowing other users to make changes to and/or receive data from the system as necessary, or otherwise determined as useful.  In particular instances, the\nsystem may be configured for running one or more analytics on received data, which data may then be used by the system or other third parties for a variety of commercially relevant purposes, such as for targeted advertising set forth herein.  For\nexample, in one use model, the system may include an advertisement generator, where data collected by the system may be used to generate an advertisement, which when generated may be delivered to one or more of the client applications of the client\ndevices being employed by the various users of the system.\n Accordingly, in one embodiment, the system may include an advertisement generator.  For instance, the system may include one or more servers, such as where the server is configured to include a set of processing engines or modules, which\nprocessing modules may incorporate a content collector, media repository, and/or advertisement builder, a competitive analysis engine may also be included.  Specifically, in various embodiments, the server may be a cloud based server having a network\nand/or wireless internet connection so as to communicate with one or more recipient computing devices, which computing devices may be a client computer, a recipient computer, a desktop computer, laptop computer, a tablet computing device, or other mobile\ncomputing device such as a cellular phone having online or other computing functionalities.\n In particular embodiments, the server may be associated with, either directly or indirectly, a content collector, evaluator, and/or generator.  For instance, the system may be configured for monitoring online usage, behaviors, and/or other\nactivities so as to determine their relevancy to a given user of the system, such as an evaluator and/or scorer.  Particularly, the system may include a suitably configured data-collector, such as a suitably configured application programming interface\n(API), web-crawler, skimmer, or other internet content collection mechanism, which is configured for identifying and/or collecting content of interest to a sponsor, advertiser, event organizer, and/or user of the system, such as by keyword, text, and/or\nimage recognition data, address data, such as physical address, virtual address, and/or web-address or URL data, metadata, other data of interest to a user, and the like.  Data pertinent to a sponsor, advertiser, or other user may be identified by the\nsystem, evaluated in accordance with one or more user selectable, or automatically determined, parameters, in accordance with the methods disclosed above, and once identified and determined to be relevant may be collected and stored, such as in a content\nrepository communicably connected to the communications server, from which repository one or more further communications, such as an advertisement may be generated.  Once collected, the data may then be formatted and/or modified, and/or otherwise be made\navailable as communication content, such as a media or advertisement asset, which may be selected for inclusion into a communication template for use in the generation of a advertisement, such as by a suitably configured advertisement builder of the\nsystem.\n Such advertisements may be generated automatically by an advertisement generator of the system, or individually by a user, e.g., system administrator, in which case the system may include an advertisement builder along with one or more of an\nadvertisement viewer, content procurer and/or evaluator, content repository, a mass distribution module, a communications monitor and/or responder, and/or an analytics and reporting module.  Once the advertisement has been generated, the system\ncontroller may broadcast or otherwise distribute the advertisement via a suitably configured distribution engine so as to direct the distribution of the advertisement to the relevant client device(s).  Additionally, the system may include an evaluation\nengine so as to evaluate the advertisement content and/or the effectiveness of its messaging.  The system may also include a customer care monitor, so as to monitor user response to system advertisements and communications and/or to monitor the\nnarrative(s) being propagated by the various users of the system.\n Accordingly, an advertisement engine may also be included.  For instance, in particular embodiments, a content and/or communications, e.g., advertisement, generator is provided.  Particularly, in one embodiment, a content and/or communication\ngeneration system is provided, such as for generating content for incorporation into an advertisement.  In various instances, the advertisement may be configured for being posted and/or published, such as on or at a social media platform, such as\ntargeted to the various users of the system.  In certain embodiments, the communication may be an advertisement, such as an advertisement that may be generated automatically, at real-time, and on the fly, such as to promote the commercial interests of\nthe sponsors and/or advertisers of the event, or the event organizers themselves.  In various instances, the content and/or communication generation system may include a communications and/or advertisement generation server.\n As indicated, the server may include or otherwise be in communication with an advertisement media repository, which repository may receive and store content, such as media related content, such as content entered into the system by an event\nsponsor or advertiser, and/or may be content, such as images, collected by the system itself, which when collected can be stored in the media repository, such as for incorporation into an advertisement template of the disclosure.  Accordingly, the\ncollected content may be used for building advertisement related communications.  Hence, the repository may further receive and store advertisement templates, which template may be in any form useful for generating an advertisement, as well as\ncommunications directed to users of the system, market influencers, general consumers, and/or the general public.  Thus, the communication may be in the form of an advertisement, memorandum, a letter, an email, a post, a comment, a TWEET.RTM., a review,\na response to a review, a sentiment, a like, a dislike, an upvote, or other form by which a communication may be sent via a social media platform, and the like.\n The server may also include a communications and/or advertisement building engine, which advertisement builder may be configured for accessing the media content repository and/or for generating an advertisement viewer, when manually building an\nadvertisement, which viewer may be configured for allowing a user, e.g., admin, of the system to build an advertisement, such as by providing a dashboard by which one or more of the advertisement template and/or the advertisement asset may be viewed,\nselected, configured for integration one into the other, and may include field indicators prompting the user for entering text or images so as to generate the advertisement.  The advertisement viewer may not only be used for creating advertisements, such\nas from scratch, e.g., using a template, but it may also be used to edit such communications.  Accordingly, the advertisement viewer may include a graphical user interface, e.g., dashboard, which includes controls for effectuating the building of the\nadvertisement, e.g., for the viewing and selecting of the template, assets, adding in text to text fields, editing one or more of these fields, and the like.  Although the project builder may be configured for allowing the manual input of directives for\ngenerating an advertisement, such as via the advertisement viewer, in alternative embodiments, the project builder may be configured for autonomously and/or automatically generating a communication, such as via direction from the A/I module of the\nsystem, whereby the system itself may select a template of interest and one or more advertisement assets for incorporation thereby, and/or may generate requisite text to be entered into various text fields of the template.\n In particular embodiments, the auto generation configuration of the advertisement builder may be configured for generating real-time communications, such as advertisements, on the fly, such as upon activation of one or more pre-defined triggers,\nsuch as upon receipt by the system of a keyword or key text or key image, interest or disinterest sentiment, a geographic indicator, or other online interactor, such as in response by the system in view of data collected about the user and/or in regard\nto their use of the system.  For instance, the advertisement building system of the disclosure may include a compiler, such as for effectuating the integration of the communications assets, images, videos, texts, and other data into the respective fields\nof the template, so as to build the final form of the advertisement.  As such, the compiler may be communicably associated with one or more of the advertisement builder, advertisement viewer, formatter, and/or distributor.\n Hence, the advertisement building system may also include a formatter such as for selecting and formatting the advertisement, e.g., the online communication, in an electronic format that can be automatically formatted, distributed, rendered,\nand/or viewed, such as in a distribution format based on one or more of data pertaining to one or more identifying features of the user, keywords, addresses, and/or other data received by the system, e.g., in response to user input, such as directly\ninput from an evaluator or scorer and/or based on data skimmed off of that (or other) persons social media postings.  Likewise, the advertisement building system may also include a distribution program for performing the delivery, such as for the\ntargeted broadcasting, of the advertisement over the Internet to the recipient, or to the social media platform of the recipient such as for posting thereby.  For instance, the distribution may be configured for delivery to one or more specific users,\nsuch as within the social circle of the user, or for general distribution to the public at large, such as by posting on to the event's or user's social media platform.  Further, the distribution may be autonomous and/or automatic, for delivery determined\nso as to be optimized by the system, or it may be determined and/or effectuated by a user of the system.\n Accordingly, as can be seen with respect to FIG. 9A, an important aspect of the system is an Artificial Intelligence (A/I) module that includes one or more of a learning or training platform or engine and an analytics or inference platform or\nengine.  In one instance, the learning platform includes a processing engine that is configured for taking known data, running a learning or training protocol on the data, and developing one or more organizing rules therefrom, such as rules for weighing\ndata and/or determining bias and/or making one or more predictions.  Likewise, the inference processing platform includes a processing, e.g., inference, engine, that is configured for applying the rules developed by or for the learning platform and\napplying them to newly or previously acquired data to generate one or more outcomes thereby, such as where the outcome may be a known or inferred relationship, a known or predicted result, and/or a probability of one or more outcomes, and the like.  In\nvarious instances, the inference engine is configured for continuously running sometime prior to, during, and/or sometime after an event, and functions with the purpose of improving the accuracy of the event results, such as for the event itself and/or\nfor one or more participants thereof, such as by improving the accuracy of the evaluating, scoring, and/or judging, for instance, by correcting for and/or eliminating bias in the use of the system.\n As indicated above, in one particular embodiment, the A/I module is configured for determining correlations and/or relationships between the various data collected by the system.  For instance, in various instances, the A/I module may be\nconfigured for generating a data structure, e.g., a decision tree, table, and/or knowledge graph, wherein the various data collected by the system are uploaded into the tree, table, or graph as a constellation of data points.  In such an instance, the\nlearning engine may be configured for taking known rules to determine known relationships between the known data points, and from these known data, the learning engine may be configured for inferring unknown relationships between data points to determine\nheretofore unknown relationships between the data points, which in turn may be used to determine new rules by which to determine other unknown data points, relationships between the two, and/or to make one or more predictive outcomes based on the known\nand/or learned data, such as in response to one or more queries.\n For instance, the data, the relationships between the data, and the determined and/or inferred rules may be employed to generate a data structure, such as a knowledge tree, table, and/or graph, and to mine the various data within the system to\ngenerate an answer to a query, such as a query related to weighting evaluations and/or determining bias.  Accordingly, a unique feature of the A/I module is its predictive functionality, which functionality may be implemented by a predictive analytics\nplatform that is configured for performing one or more predictive analyses on the obtained and/or generated data, such as by generating one or more predictive outcomes.  For example, a predictive outcome may be in response to the query as to whether\nthere is bias in the system and/or with respect to one or more inter-relations thereof.\n Accordingly, as can be seen with respect to FIG. 9A, in one aspect of the disclosure, an artificial intelligence module (A/I) may be included in the system.  For instance, in various instances, the various devices and systems, as well as their\nmethods of use, as disclosed herein may be employed so as to evaluate the authenticity of a user's interactions with the system.  Particularly, the user of the system may be an event organizer, an official judge of the event, a fan engaged in the event,\ne.g., a non-official judge, a performer in the event, and the like.  In such instances, it may be useful for the overall system to keep records, or stores, of data with respect to how the user engages with the system, e.g., with the system itself, a\ncomponent of the system, another user of the system, or a determined 3.sup.rd party factor, so as to ensure the authenticity of the usage.\n A third-party factor may be a 3.sup.rd party that may or may not be using the system, but whom the system has determined is relevant to how one or more other persons are engaging with or otherwise using the system.  For instance, a third-party\nfactor may be an agent, such as a person, who a judge of an event and a performer in that event may have in common.\n Accordingly, the third-party factor may be a person, a business, a contractual relationship, a geographical region, or anything by which two entities may be determined to be related, with respect to an organized event being hosted by the system. Particularly, the system may be configured for determining patterns in the behaviors of the various people using the system, from which patterns various relationships may be determined, and one or more actions may be taken by the system in view of the\nidentified relationships and/or determined patterns.  For example, once a relationship between the various agents acting upon the system is identified, such as with respect to how certain judges or scorers score particular participants in an event, and a\npattern with respect to how the judge is behaving with respect to their relationship to the participant is determined, the system may take one or more actions, e.g., corrective measures, to account for that relationship and/or error correct the\nactivities of that judge or scorer, such as subjecting the scores to one or more regression analyses.\n For instance, where an identified judge e.g., or other user, has a pattern of scoring certain participants too high and/or scoring other participants too low, then the judge's scores can be discarded altogether, or may be discounted, such as by\na bias factor, e.g., determined by the system, e.g., the A/I module.  Additionally, the system may store the judge's scoring patterns, as well as other data derived from how the judge interacts with the system, which information may be applied to the\nlater activities of the judge, such as when weighting their future scores or other actions they take in interacting with the system.  Such information may then be employed in evaluating the judge for fairness, bias, and the like, as described herein,\nand/or to predict how the judge will act in the future, given their past actions.\n Specifically, the judge or user's engagement with the system may form regular interactions and/or patterns that may be recorded and tracked within the system, from which patterns the machine learning and/or inference module of the system may be\nemployed to learn each user's particular pattern(s) of behavior, and determine a range of freedom behind those actions and/or predict future courses of action and outcomes.  This is useful when the user's pattern of engagement with the system appears to\ncoincide or conflict with the patterns of usage of other users of the system.  Particularly, determining patterns of usage of the system is useful when determining the degree of freedom, e.g., freedom from bias, by which one or more users is engaging\nwith the system.  By range or degree of freedom is meant a scale, such as from 0.0-1.0, or the like, which scale represents a degree to which a user's actions are more completely free from the influence of other factors, with respect to their engagement\nin the system, e.g., at the low end of the scale, toward the higher end of the scale which represents a higher degree of influence shaping their behavior(s).  Accordingly, in various embodiments, any score entered into the system may be analyzed by the\nsystem and may include a further sub-score, such as a quality score, whereby the entered score is given a confidence score itself, which confidence score can be an estimate by the system as to how confident the system is that the entered score is free\nfrom bias.  This confidence score may then be used to appropriately weight the entered scores.\n More specifically, the system may be configured for not only determining the presence of various factors influencing behavior, e.g., the actions taken by the judges, performers, and/or participants in the event, such as factors that influence\nthe presence of bias, but as well for determining which factors may be leading to that influencing, and to what degree.  Further, once determined, the system, e.g., via the suitably configured learning platform, may then be adapted to correct for such\ninfluences, such as by the inference module increasing or decreasing a weighting scale used to weight the connections between various influencing factors and user actions and/or outcomes of those actions.  For instance, in such instances, when various\npatterns are formed, the system may learn these patterns, determine the presence of bias, or other factors of import, and/or predict a likely manner in which the user will behave, and the level of confidence may be given to the predicted outcome, such as\nfrom 0.0, not very likely to 1.0 almost completely certain.  In certain instances, the collected data may be subjected to one or more regressive analyses, including but not limited to a necessary condition analysis, a probability distribution, a Gaussian\nor Markov (including Hidden Markov) Model, and/or a Burrows-Wheeler Transform, and the like.\n Accordingly, when the system makes a correct prediction, the connection between the initiating action and the presence of the condition, e.g., the condition of bias, in that action, as well as the connection between the action and a predicted\noutcome of that action, may be strengthened, such as by giving an estimation of bias and/or a predicted outcome in the future, for the same or substantially similar circumstances, more weight.  Likewise, when a pattern is broken, less weight may be given\nto the connection between the initiating action and the presence of bias and/or predicted outcome, until the old pattern is re-established and/or until a new pattern is formed.  These changes in patterns can also be aggregated along various dimensions to\ngroup a plurality of users together, and/or in the contrary, a group of people interacting with the system together, such as in a coalition, may be used to more precisely define and weight patterns of engagement.  In a manner such as this, the system may\nbe configured to keep track of the various users of the system as well as their individual and/or group patterns of behavior, so that the various identified factors that may be influencing the emergence and/or maintenance of such patterns may be\nidentified, predicted, and employed for a plurality of different uses, such as for determining the presence and/or degree of bias behind one or more user's engagement with the system, and/or for taking corrective actions, such as to correct for such\nbias.\n Specifically, the system may generate and employ one or more data structures that may be queried so as to predict the answer to one or more questions.  For instance, as described in detail herein, the system may be configured for receiving\ninformation with regard to a plurality of users, which information may include identifying information, social circle information, as well as social media engagement information.  Likewise, the system may present one or more users to a series of\nquestions, such as via an automated interview process, the responses to which may be used to characterize the user and/or generate a user profile thereof.  Additionally, the system may track how the user engages with the system, as well as the attendant\ndata pertaining thereto, such as time, place, number of times per day, length of time engaged, actions take or not taken with respect thereto, who he or she messages or otherwise interacts with through the system platform, who they follow, what events\nthey engage in, and the like.\n All of this information may form data points that characterize any given user.  These data points may then be employed as branches or nodes within a data structure, which data structure may take any suitable form, such as a data tree and/or a\nknowledge or De Bruijn graph.  From these various data points relationships between users of the system may be identified, and the connections between them may be weighted based on the number and form of the interactions between them.  Hence, the more\nusers interact with one another in a positive manner, the greater the weighting will be between the various nodes that may be employed to define their interaction.  Likewise, the more negatively the users interact with one another, the less (or more\nnegative) weight will be given to define their interactions.\n In similar manner, the more the user's interactions with the system comport with one or more groups of the system, e.g., model groups, the more weight those various connections will be given, and the more the user's interactions do not comport\nwith one or more model groups, the less weight those connections will be given.  Likewise, in various instances, a user may make a prediction as to an outcome that actually occurs, and in other instances, the predicted outcome does not occur, in such\ninstances, more or less weight will be given to the user when predicting outcomes for future events, based on his successful prediction of outcomes of past events.  Hence, where it is determined that external factors are in some way influencing a user's\ninteraction with the system, such as bias influencing a judge's inputting of scores, the system may be configured for identifying and correcting for the effects of such external factors, such as bias.\n Where outside factors are determined to be affecting a plurality of users' engagement with the system, such users may be grouped together by the system, such as based on the type of outside factors by which their use of the system is being\ninfluenced, and in such instances, their engagement in the system may be treated as a block and/or corrected for as a block, such as by giving the block's engagement with the system less weight.  Likewise, those whose use of the system are determined to\nbe free of external factors, such as bias, e.g., model users, may also be grouped by the system, and their engagement of the system may be given more weight.\n Accordingly, in a manner such as this, data points between the various categories, branches, or nodes in the data structure of the system may be used to generate correlations between the nodes and to weight those correlations so as to build a\ndata structure thereby, such as a knowledge tree or graph, which may then be queried to determine other relationships not previously known and/or to predict the influence of external factors affecting the usage of the system, and/or to predict and weight\npotential outcomes based on a collective of usage patterns of how users are engaging with the system.  For instance, a data structure, such as a relational or hierarchical or knowledge or De Bruijn graph structure, may be generated by the system\nreceiving known data about the various users of the system, e.g., event organizers or sponsors, participants in events, formal judges of events, informal fan based judges, or other users of the system, and, via a suitably configured data management\nsystem, building a structure, e.g., a tree or constellation, of data points and drawing connections between the data points.\n As indicated above, this data may be collected in a variety of ways, such as by system generated interviews of the users and/or system instigated accessing and review of user online or social media usage, e.g., collected via a suitably\nconfigured API, web-crawler and/or mining tool for mining social media, and/or other online usage data.  The data to be collected may also include data pertaining to user usage of the internet, such as through postings of texts, photos, blogs, comments,\nsearches performed, time spent on web-pages, and other associated metadata through which a user produces a presence on the global internet, all of which data may then form various branches or nodes of the data structure, e.g., graph.  For example, the\ndata to be entered into the database, may be used to structure and populate an inference engine, e.g., based on the graph, which engine may be employed for searching and/or otherwise performing queries, and may further be utilized by an artificial\nintelligence engine, as explained below, for predicting outcomes.  Consequently, user and/or performer data may be obtained and entered into the system in a variety of different manners, and may include the storing of information in hierarchical or\nrelational models, as well as in a resource description framework (RDF) file or graph, and the like, as described below.  Such a procedure may be performed for a number of different users.\n Once the system users have been identified, defined, characterized, and entered into the system, the users, as discussed above, may be grouped in accordance with one or more system usage parameters, and known relationships may be determined\nbetween the various users in the group.  In particular, once the database is set up, the data collected and entered into the system, the database may then be structured, such as for being queried or otherwise searched, such as with respect to the\nexistence of various relationships between data points, and/or with respect to one or more known determined or undetermined variables.  For instance, the number of relationships between the various users as well as the strength of those correlations may\nthen be determined and used to weight the known or fact based relationships.  Likewise, from these known fact based relationships, previously unknown, inferred relationships may be determined, and weighted.  In a manner such as this, a data structure,\nsuch as a knowledge graph, or other data structure may be generated so as to include both known and unknown, inferred, determined, and undetermined relationships.\n Accordingly, once generated, the data structure, e.g., knowledge graph, may then be queried along a number of lines so as to make one or more determinations with respect to the various relationships between the various branches or nodes of the\ngraph.  For instance, the system may be configured to automatically be queried to determine if there is a pattern by which one or more users are interacting with the system.  For example, if one or more otherwise unrelated users appear to be engaging\nwith the system in a like manner, e.g., their scoring appears to be relatively the same, the system may flag their behavior for further analysis, a deeper dive into any possible relationship between the two users may be initiated, a source of their\npossible correlation may be determined, and from the strength of that relationship between the two users, a score may be given so as to weight the possibility the users may be acting in a collective manner.\n Such an interaction could evidence bias in their engagement, which may then be identified by the system, flagged for review by the event organizers, and/or the system can initiate corrective measures by which such bias may be corrected for, such\nas by lowering the weight of their collective interaction, e.g., lowering the weight of their scoring, or discarding it all together, in such a manner as to eliminate the effects of such collective usage and/or bias.  In various instances, the system may\nperform such activities automatically based on its own programming, and/or in some instances another user of the system, e.g., a system operator, event organizer, sponsor, or the like, may initiate such queries, and/or a deeper dive as to how various\nusers are using the system.  Accordingly, in various embodiments, the system may be configured so as to be queried along a number of different parameters to determine and weight a number of different answers, and thereby make a variety of different\npredictions.  These predictions may then be given a weighted score, a confidence value, such as to the probability of being correct, and based on that score, the system can self-correct so as to properly account and/or correct for the predicted behavior\nof its users, such as with respect to bias.\n In a typical architecture for performing such functions, such as for performing a structured search query of a database of the system with respect to determining bias.  For instance, the system may include a database of events, users\nparticipating in those events, characteristic data pertaining to the user themselves, relational data pertaining to that user in relation to other users (or non-users) of the system, e.g., with respect to that or other events, and characteristic data\npertaining to how the user has interacted with the system, etc. in the past, e.g., including any pattern data, as well as predictive outcome data of the past, present, and/or future, and may include other characteristic data the system determines is\nrelevant to the particular question being queried.  In such an instance, the relevant data points may be identified and pulled from the general database, and a localized data structure may be built.  As indicated, the data to be stored may be stored in\nany sufficient manner, but in some instances, the data to be stored may be encoded and/or hashed prior to storage, and then later may be looked up in accordance with a hash key.\n However, any data structure may be employed for performing the search in question, in various instances, however, the data structure may be a relational data structure, such as a Structured Query Language (SQL) database, which may be implemented\nvia a relational database management system, or the data structure may be a hierarchical, or graph based data structure.  For instance, in one implementation, a SQL database is presented, which database may be a table based data structure, such as where\none or more tables form the base structure wherein data may be organized and stored, such as in a variety of columns and rows, searched, relations determined, and queries answered in a structured manner.  Particularly, in such an instance, SQL statements\nmay be used to structure, update, and search the database.  In various embodiments, a table-based database may be presented, e.g., a relational database structure, which data structure may be searched, and used to determine relationships from which\nanswers to one or more queries may be determined.  Typically, in such a data structure, identifiers, such as keys, are used to relate data in one table to that in another table.  Accordingly, provided herein is a database that may be built and structured\nas a structured query language (SQL) database that has a relational architecture, and may be managed by a data management system, such as a relational database management system (RDBMS).  In particular instances, a series of tables, for instance, may be\nemployed by which correlations may be made in an iterative fashion.\n Specifically, with respect to whether a certain user of the system is engaging with the system, such as with respect to a particular event and/or performer in the event, in a biased manner, a first correlation may be made with respect to the\nsubject's normal interactions with the system, as determined over a series of events, such as how well the user's interactions correspond with the median, mean or average of users overall, and a first table may be formed to record this data as a first\nuse model sample set.  Then, a second table may be built whereby the user's current use of the system with respect to a current present event and/or participant in the event is tracked and compared against the collective of current users, and the two\ntables can be compared with one another so as to determine if the user's present interactions comport with their past use of the system, such as with respect to how their present use comports against the collective.\n Where it is determined that a user's present use is outside of what would be one or more parameters of their historical or predicted usage average, the system could flag the interaction as worthy of a deeper dive, e.g., subjected to a\ndeep-learning protocol, and if necessary can begin to look for other correlations between this user and this event and/or participant therein so as to determine possible explanations as to why this user's present interactions our outside of their\npredicted behavior.  Specifically, where the data structure is a series of tables, the user's identifier or key may be searched and compared through a number of tables for a wide variety of correlations that may be determinative in explaining their\npresent, aberrant interactions.  Where a source of bias is determined to be present, the system can implement a corrective regime to account for that bias.\n Accordingly, a key may be used, in this instance, to correlate the tables, which key may be accessed in response to a question, prompt, or command, such as why the user's present use does not comport with their past use of the system, or the use\nof the system by the average user.  The key may be any common identifier, such as a name, a number, e.g., a RFID number, cellular identification number, a phone number, a license number, a social security number, a chosen password, and the like, by which\none or more of the data structures, e.g., tables, may be accessed, correlated, and/or a question answered.  Hence, without the key it becomes more difficult to access data and/or build correlations between the information in one data structure, e.g.,\ntable, with that of another.  In certain instances, the table may be a hash table and a hash function may be employed in search the table for correlations with other data structures.\n As indicated, a further architecture that may be used to structure a database is a hierarchical data structure.  For instance, in various instances, the database may be structured as a data tree, e.g., a suffix or prefix tree, where various data\nelements may be stored in a compressed, but in correlated fashion, where the various roots and branches form divergent data points with respect to potential correlations.  Specifically, in such an instance, the data may be stored within the data\nstructure in such a manner that the stored records are connected with one another through relational links, such as where the various records are a collection of fields that store data files in a chain of superior and subordinate levels of organization,\nsuch as in a pyramidal or other hierarchical configuration.\n In other instances, a graph-based architecture may be structured and used to determine the results for one or more queries.  Particularly, a knowledge graph architecture may be employed to structure the database; so as to enhance the performance\nof computational analyses executed using that database.  Such analyses may be employed so as to determine whether a given user's present use of the system comports with their past use and/or comports with how other users in general, e.g. the average\nuser, have or are presently interacting with the system, such as with respect to the present user's evaluating and/or scoring of a given event and/or performer in the event, and/or with respect to their regular pattern of usage.  Accordingly, the\nsophisticated algorithms employed herein, are adapted for structuring the infrastructure of a relational database so as to enable more efficient and accurate searching of that database such as via performing graph based analyses, as well as for\nperforming table or tree based analyses.\n For instance, where a user of the system is evaluating and/or scoring another user's performance in an event, a data structure may be generated where the first user's characteristic data are used as a first collection of data points, and the\ncharacteristic data of a second user, e.g. the performer in the event, may be used as a second collection of data points, and relationships between the first and second users may be determined through use of one of more data structures, e.g., knowledge\ngraphs, tables, e.g., hash tables, pre-/suffix-tress, etc. as described herein.  Additionally, a third collection of data points may include how the collective of present or past users are engaging with the system, such as with respect to the second\nuser's performance in the event.\n Where the first user's engagement with the system, e.g., the scores they enter in judging the second user's performance in the event, comport with the collective of users currently scoring the event, then the system may not need to take a deeper\ndive into determining how the first user may be biased for or against the second user.  However, where the user's engagement is outside of the norm, as determined by the collective's current engagement (or the user's past engagement) with the system,\nsuch as by scoring the second user, e.g., performer, higher or lower than average, then the various correlations between the first and second user may be explored to determine if there is bias.\n Where bias is determined, an even deeper dive may be performed so as to determine the source of that bias.  Such deeper dives may be performed, as indicated by generating and evaluating one or more of the searchable data architectures disclosed\nherein.  Sources of such bias may include any form of relationship between the first and second users, such as growing up in the same neighborhood, attending the same schools or churches or other social clubs, having the same friends in common, being\nknown friends with or competitors against one another, having business affiliations with one another, and the like.  Past interactions of either of the users may also be explored to determine previous patterns of behavior and/or to explain the nature of\nany presumed or determined bias.\n For instance, where the first user has previously diverged from the collective and been found to have been more accurate than the collective, this may be taken into account when determining if there is the presence of bias in this present\ninstance, or if the first user is once again being more informed and more accurate than the collective.  Specifically, various determined patterns of behavior may also be used as data points in the searchable data structure, and where an identified\npattern of behavior is maintained, the corresponding relationships making up that pattern may be strengthened by giving them more weight, but where those patterns are broken, less weight may be given to those relationships forming that pattern, and a\nquery as to why the pattern was broken in this instance may then be explored by the system.  Once it has been identified as to why a given pattern has been broken, the system may implement a corrective measure to account for the effects of that broken\npattern on the overall functioning of the system.  In a manner such as this, the system is configured for becoming increasingly more accurate over time.\n Consequently, in one aspect, a device, system, and method of using the same to build a searchable, relational data structure, such as described herein, is provided.  Particularly, in one instance, the device, systems, and methods disclosed\nherein may be employed so as to generate and/or otherwise collect data, such as data pertaining to the users of the system, including identification data, characterization data, and usage data.  Accordingly, in one embodiment, methods for building and\nstructuring a searchable database are provided.  For example, in a first step, data may be collected, cleaned, and then be prepared for analysis.  In various embodiments, the data may be labeled and/or categorized, and may then be structured into a\nsearchable data architecture, such as a knowledge graph, table, or tree-like structure.  And once the database is structured, it may then be populated with data, in accordance with determined or inferred relationships.  Such relationships may be\nnotional, fact, or effect based.\n More particularly, in certain instances, a machine learning protocol, as disclosed herein, may be employed so as to determine relationships between data points entered into the database.  Such relationships may be determined based on known\nfacts, and as such the learning may be supervised learning, e.g., such as where known factors may be used to label, categorize, and store data, such as location, interaction, social engagement, relationship, and/or usage and other related data.  In other\ninstances, the learning may be inferred, such as in an unsupervised learning.\n For instance, in certain instances, the data to be stored may not be known, relationships between the data may not have been pre-determined, and the query to be answered may also not have been otherwise identified.  In such instances, the data\nto be stored is unsupervised, and as such, patterns in data to be stored and their relationships, such as commonalities between data points, may be determined notionally, and once determined such patterns may then be used in forming the architecture that\nstructures the searchable data architecture.  Particularly, where a user's interactions with the system, e.g., scoring of a performer, breaks a pattern, the system may explore relational characteristics of the user and/or his or her use of the system so\nas to determine what the pattern was broken and/or to correct for its effects, or to simply determine a new pattern of behavior is emerging, in which instance, a deeper exploration may not be warranted.  For example, a known sequence of patterns may be\nused to infer that if events A and B in a known sequence may be followed by event C such that if event C does not happen as predicted, a flag is set off for initiating a deeper exploration of the nature of the causes of the flagged event.  However, where\nupon a first round of exploration, it is discovered a new pattern of behavior is being established, the flag may be removed and a deeper exploration as to the causes of the new pattern formation can be but need not be explored.\n As described above, in certain instances, at the heart of the platform may be a generated data structure, e.g., a SQL, hierarchical, or graph based database architecture, which may be generated on the fly by, e.g., an API, skimmer, or crawler of\nthe system, retrieving data points from a plurality of sources, and populating those data points into a suitable data structure from which relationships and/or correlations between the data points may be made.  First, when populating the data structure\nknown facts may be populated, then known relationships may be determined, and from these known facts and known relationships, otherwise unknown facts and/or unknown relationships may then be determined.  Such data points may include any user pertinent\ninformation, such as: user entered information, user determined information, such as with respect to how the user interacts with the system, in particular, or how they interact with the internet generally, information derived from the user's social\nmedia, user posted information, such as texts they send, commentary they post, photos they upload, comments they respond to, web-pages they visit and for how long, likes they make, up or down votes they make, purchases they make, video's or blogs they\nview, searches they perform, who they follow or are friends with on social media, and the like.\n Additionally, user location data may be determined and used to determine how close or far the user is from an event they are participating in, viewing, or otherwise engaging with.  The user may be tracked by their user ID, their cellular ID,\nRFID, GPS, Cellular tower triangulation, their Internet Protocol ID, etc. In various embodiments, the system may track the user's online interactions, travel, locations visited, whether engaging in the application or not, while being online or offline,\nand the like.  Further, friends, associates, and acquaintances of the user may be identified and their use of the system or internet generally, whether engaging with the system or not, may be identified and tracked and used as data points in determining\na user's pattern of usage, trends, and sources of possible correlations, relationships, bias, and the like.  Such persons may be identified directly by the user, by the user's online interaction with them, via the application or social media, via\ntagging, and/or via facial or voice recognition based on being in a posted image or voice recording in association with the user.\n In such a manner as this, the user's internet presence and/or social network may be leveraged and used as data points in the construction of a data structure, such as a knowledge graph, from which correlations and relationships may be\ndetermined, for instance, between various users of the system, and/or third parties, for example, by determining how these various entities interact with one another, within and outside of the application.  The type, quality, and/or quantity of these\nrelationships may then be determined by the system, the results of which may be employed so as to determine a predicted outcome, such as in response to a given query, such as for determining the potential presence of bias of one user of the system with\nrespect to another.\n Once the data structure is built, it may be populated with known and inferred facts and relationships determined and/or weighted, the data structure may then be queried.  Specifically, the system may be directed, such as by a system\nadministrator or event organizer as to what the query is or should be, such as from a list of known query types, so as to perform a supervised search query, or the system itself may generate a query automatically when it identifies certain patterns that\nare worthy of greater explanation, and as such an unsupervised query may also be instigated.  More specifically, the various data points entered into the data structure may be labeled and categorized, e.g., based on known patterns, and a given search\nquery may be performed with respect to the identified labels and categories, that have previously been determined to be important to the functioning of the system.  This is useful when the system has been primed in such a manner that it knows what it is\nlooking for.  In other instances, the A/I module, e.g., inference module, may itself identify patterns, commonalities, and/or other elements that form a relationship from which one or more labels and/or categories may be generated automatically by the\nsystem itself, and a query can be performed based on system generated prompting with respect to these unsupervised factors.  This is useful when it is not necessarily known what exactly is being looked for.\n In particular, in various instances, the machine learning or inference module, as described herein, may be adapted to recognize how an output was achieved based on the type and characteristics of the inputs received.  Specifically, in various\ninstances, the present system may be configured to learn from the inputs it receives, the relationships it determines, and the results it outputs, so as to learn to draw correlations more rapidly and accurately based on the initial input of data received\nand/or the types, quality, and quantitates of the relationships it is able to correlate.  Likewise, once the A/I machine learns the behavior, e.g., of one or more users of the system, or one or more third parties with respect thereto, the learned\nbehavior may then be applied to a second type of data, such as an inference engine, that is used to infer other various relationships and/or to predict the answer to one or more unknown variables, or heretofore unknown relationships.\n There are several different types of relationships that can be determined.  For instance, relationships may be determined based on what is known, e.g., they are fact based, and/or they may be determined based on the known effects of those facts,\ne.g., they are effect based, e.g., logic based; or they may be determined based on inferences, e.g., relationships that are unknown but determinable.  Specifically, a relationship between two subjects, locations, interactions, and/or other relevant\nconditions of one or more users of the system, or third parties with respect thereto, may be inferred based on various common facts and/or effects observed between them.  As described in great detail herein, these previously unknown but inferred facts\nand/or relationships may be determined and/or used to build and structure a database that may be searched and used in predictive models, such as by generating a data structure, e.g., a structured database, as disclosed herein.  For example, known, e.g.,\nfact, or effect based, or inferred data points may also generated, or otherwise entered into the system, and may be used to generate one or more nodes, e.g., a constellation of nodes, which may be used to build a data structure that may be used in the\ndetermination and/or weighting of relationships, which in turn may be employed in answering one or more queries through searching the data structure.\n Particularly, various data points may be used to compile and build a database that may then be structured in a number of different ways, such as in a hierarchical, e.g., XML, relational, e.g., Structured Query Language, SQL, and/or a resource\ndescription framework (RDF).  The data points to be stored in the database structure may be characterized in a plurality of different manners, such as, in one embodiment, with respect to being a subject, a predicate, and an object.  Specifically, with\nrespect to an RDF, a particular form of a knowledge graph may be generated such as where the data points to be fit within the data structure may be organized to form nodes within a graph, where each node and the relationship between the various nodes\nwill have properties by which they can be placed into one of these three categories, e.g., as a subject, a predicate, or an object, based on a given query to be answered.  Such structures, therefore, may be composed of triplets containing a subject,\nobject, and the relationship between them termed: a predicate, where the subject always points via the predicate to the object.\n More particularly, each node within this proposed data structure, e.g., data graph, may represent a subject or an object, which may be related to one another by a predicate.  Each object and subject may further include and/or may otherwise be\ncharacterized via one or more properties, which may be defined as predicates that characterize a relationship between the two.  Hence, each triplet represents a statement of a relationship between the things denoted by the nodes that it links.  For\nexample, the assertion of an RDF triplet indicates that some relationship, indicated by the predicate, holds between the nodes denoted by the subject and the object.  In this manner, data points may be populated into the database and may be structured as\nnodes in the graph in a manner such that the data points may be populated with one or more characteristic properties that more fully define and/or classify that node.\n Accordingly, when building a data structure between data elements or nodes, known facts, as well as their known properties, are first employed by the machine learning platform (ML) to determine known outcomes, during which process the ML\nplatform, e.g., learning or deep-learning engine, thereby learns the patterns of behavior between the nodes and their relationships to one another, such as in a training process.  Hence, first data is collected, from a variety of sources, as disclosed\nherein, and structured, such as in a relational, hierarchical, and/or graph based database, or the like, such as where each subject and object are relatable through properties, such as predicates.  Then the system may be trained to search the database,\ne.g., based on the nodes and/or their relationships, and to make a prediction to receive a result, where the answer to the query is previously known, and the result obtained is compared to the actual result.  Where the actual answer is the same as the\npredicted answer the relationships between the various nodes implemented in determining the result may then be strengthened.\n This training may take place over a wide range of sample sets, until an acceptable accuracy level has been achieved.  For instance, a training protocol may be implemented, such as where various known subjects and objects are related by known\nproperties, where the system is trained to identify the properties between them, so as to learn the relationships, and once learned are then given a selection of data points where the relationships, although already known, must be determined by the\nsystem, and the accuracy is measured, until a specified accuracy level has been determined.  Particularly, the system may be subjected to a deep learning protocol.  Once appropriately trained, e.g., via a deep learning protocol, then the ML platform, may\nbe given data points from which unknown relationships need to be determined, e.g., inferred, and unknown outcomes predicted.\n Specifically, once the ML platform has learned the expected patterns of relationships, e.g., behaviors, with respect to known data points and relationships, it may then develop \"inferred\" rules by which it may classify and label new or unknown\ndata points so as to determine and account for otherwise unknown relationships, so as to thereby classify and label and/or otherwise define the heretofore unknown data points, their properties, and relationships, which may then be classified and labeled. In such an instance, when the expected results are achieved, such as with respect to the user engagement with the system, the system status quo may be maintained, but when these new data points evoke a breakdown in patterns of relationships and/or\nexpected outcomes, e.g., a user acts in an unexpected way or an unexpected result occurs, then a system alert may be triggered and a deeper exploration may be initiated.\n Additionally, once the knowledge graph architecture, or other data structure, has been constructed, the inference engine may employ that knowledge graph to answer one or more queries of the system, and/or to make one or more predictions with\nrespect thereto.  Because the data to be considered is so prolific, such inferences could not otherwise be made except by such a suitably trained system.  For instance, the A/I module may configure the data structure, and implement one or more functions\nwith respect thereto, such as via one or more known or previously unknown facts and/or relationships, e.g., via the machine learning protocols disclosed herein, and thereby predict various consequences with respect thereto.  Further, once the data\nstructure is generated, e.g., by a suitably configured data management system, it can continually be updated and grown by adding more and more pertinent data into the knowledge structure, such as data received from any relevant source of information\nprovider pertaining to the subject(s) under examination, and building more and more potential nodes and/or relationships.\n In various embodiments, the system may be configured for being accessible by system administrators, event organizers, sponsors, users, performers, and/or other third parties having the appropriate access permissions.  In such an instance, the\nuser may access the A/I module, e.g., via a suitably configured user interface, upload pertinent information into the system and/or determine the relevant nodes by which to answer an inquiry, e.g., such as is there bias with respect to how a given user\nis engaging with the system and/or does their behavior with respect thereto fit within an established and/or otherwise expected pattern of behavior.\n The ML and inference engines of the system have many potential uses.  In certain embodiments, the system may be configured for promoting and measuring fan engagement, such as fan engagement when watching or otherwise participating in a\ncompetition or performance.  As such, the system may be configured for providing a platform by which a spectator or other observer of an event, such as a sporting event, or a singing or dancing competition, and/or the like may be enabled to more\nintimately be involved in the event, such as through use, as described herein, of their desktop or mobile computing device.  Specifically, in one embodiment, a downloadable application is provided, which downloadable application provides a graphical user\ninterface (GUI) through which interface the user may more intimately involve themselves in the event.  More specifically, the GUI may be configured to present a dashboard to the display of the computing device, through which display the user may be\nenabled to interact in the event environment in a more meaningful way.\n For instance, in one instance, the dashboard may present a display of the activities taking place in the event, e.g., real-time, such as in streaming fashion.  The real-time display of the event and its activities may be the same or similar to\nwhat is being broadcast, such as from a network or media content communications channel, or it may be from image and/or sound content that is captured by another device, such as from another fan who is engaging with the system in viewing and recording\nthe event, or the content may be streamed from a point of view image/sound capturing device, e.g., digital camera, worn by the event participant themselves.  Such media content may be transmitted to a server of the system, from one or more of these\nvarious sources, the data may be cleaned, edited, and broadcast, streamed, downloaded, or otherwise provided back to the various users of the system substantially real-time for their viewing.  In various instances, the user may select from which content\nsource they wish to view the event, such as by toggling back and forth between viewing options, such as point of view, e.g., of the competitor, streaming from an Official feed from a camera of the event, or from a mobile computing device of an observer\nof the event that has been configured for recording and transmitting the event, such as to other users of the system, e.g., via the APP.\n In addition to a viewing platform, where the event is some form of a competition, such as among competitors, the dashboard may present a scoring platform, as described herein above, whereby the user may score the individual competitors in the\nevent and/or the event itself.  In such a manner as this, the user is invited to participate more closely in the event.  Likewise, the dashboard may present the running-time scores being received by the other various users of the system so as to allow\neach user of the system to see how well each competitor is doing and/or how their scoring compares with a group of other users of the system.\n The dashboard may also provide a platform through which users may message other users of the system, such as through substantially instant messaging, SMS, text messaging, i-messaging, sending of sounds, photos, videos, and/or may allow for the\nuser to instantly send messages, texts, sounds, videos, etc. to one or more, e.g., all of their social media platforms, such as for posting thereby.  Likewise, the dashboard may allow users to interact with or otherwise respond to the messaging of others\nusing the system, such as through likes or dislikes, up or down voting, or otherwise replying to messages posted across the system.  Such messaging may be sent system wide or to one or more subgroups of the system, such as where the user has selected and\nformed a sub-group of system users with whom to share messaging and/or media content back and forth with each other.\n As indicated, in one aspect, the system may be configured for detecting and/or correcting for bias amongst the various users of the system, such as with respect to the evaluating and/or scoring function of the system.  For instance, participants\nof the system may include the performers of an event or competition, official judges, who are responsible for officially judging and/or refereeing the event, and general users of the system, such as fans, who have elected to participate in the event as\nan \"un-official\" judge, evaluator, or scorer of the event.  In various instances, the system may be configured for receiving the judging and/or scoring content from both official and non-official judges, aggregating and compiling the content, as well as\nrunning various statistical analyses thereon so as to determine a mean score as well as one or more degrees of variance therefrom, and to determine the degree to which any scorer's entered score or evaluation varies from the collective average score or\nevaluation.  This data may then be transmitted back to the users of the system in raw or cleaned up form.\n For example, as discussed in detail herein, if a given scorer enters scores that fall outside of a selected acceptable range of variance, the scores and/or scorer themselves may be flagged for further, e.g., deeper analyses.  Particularly, the\nscorer and/or their entered scores may be analyzed for bias.  Bias may be determined in a variety of ways, however, in particular instances, as described in great detail above, bias may be determined by subjecting the scorer and/or their scores for\nfurther analyses by the ML and/or Inference modules for further analyses, whereby with respect to the ML module, the scorer's past and present engagements with the system as well as their past and/or present engagement with other users of the system,\neither on the system or through other determined context, e.g., other social media engagements, may be examined so as to determine possible patterns of behavior that could be affecting the degree of freedom in their scoring behaviors.\n Once one or more patterns of behavior have been determined, the Inference engine may then run a predictive analysis on the pattern data so as to make one or more predictions as to what and/or how the user's score and/or behavior should be, such\nas if it were free from bias, and if the system determines the behavior is not free from bias, one or more corrective measures may be implemented.  A variety of corrective measures may be implemented so as to correct for possible bias and to ensure\nbetter accuracy in scoring the performers and/or their performances.  For instance, the score may be reduced or increased by a determined amount, such as by a corrective factor, the score could be grouped together with other similar, e.g., outlying,\nscores and averaged, the score may be zeroed out, or even just discarded, and the like.  The score may also be used to weight the other scores of the user, such as based on the user's ability to match with the collective, or with other judges, such that\nif a scorer is particularly more accurate in scoring performers, then their score may be given increased weight even when it does not presently comport with a present collective score.\n In part, as described above, the function of the software and/or hardware of the A/I module is to generate and/or modulate the timeout window, such as to determine when an event and/or performance is beginning, when it is ending, and/or when the\ntimeout window is to be closing and/or when it is to be closed.  As such, the A/I module may be configured for determining what scores will be counted, which scores will be weight adjusted, and which scores will be completely discounted, such as with\nrespect to determined or predicted bias and/or due to the formulation and functioning of the timeout window parameters.  For instance, the start of an event or performance may be manually or automatically determined by the system, such as when a\ncompetitor, e.g., an athlete, begins a routine or takes an action initiating a competition, such as catching a wave, stepping on to a performance platform, a diver climbing on to a diving platform, and the end of an event may be determined manually or\nautomatically determined by the system such as when the competitive athlete ends a routine or takes an action ending the competition, such as kicking out of a wave, stepping off of the platform, or entering the water, and the like.  In a manner such as\nthis the system, e.g., with the assistance of the A/I module, may be configured for determining and enforcing the timeout window parameters, and/or for determining the scoring criteria, such as with respect to the evaluations, votes, opinion, results,\nand associated data of the system.\n For example, there are many challenges faced when providing a crowd source scoring apparatus and system to organizers and to observers of an event.  One of the key challenges is ensuring that when a user scores a particular performance in a\ncompetition including several performances by several different competitors, that score is properly attributed to the correct performer and to the correct performance of that performer.  Overcoming this challenge ensures that the user's score is\naccurately counted, and that the performer's score is accurately calculated.  A good example of a situation where these challenges arise is in a surfing competition, where the event includes a plurality of heats, and each heat includes a plurality of\ncompetitors performing a variety a maneuvers in alternating performances in each heat, all of which is being evaluated and/or scored by the judges and observers or other users of the system with regard to the event.\n More particularly, where a first Surfer in a heat catches a First Wave in that heat and begins to perform various maneuvers, users of the system will begin scoring the performance.  In such an instance, the scores being entered are, therefore,\ndirected to the appropriate scoring slot for that surfer, in that heat, and for that particular performance within that heat.  This is challenging in its own right, but when tens or hundreds or thousands or even tens or hundreds of thousands or even\nmillions of observers are all entering scores for that performer for that particular performance, it becomes exponentially more difficult.  Making things even more difficult is when Surfer 2 catches their First Wave, where scores by the observers are\nstill being entered into the system with regard to Surfer 1, but now scores are also being entered in judgement of Surfer 2's performance.\n The present system, therefore, is configured for determining which scores belong to which performance being performed by which performer in which heat, so as to ensure the integrity of the system and the event overall.  More particularly, the\nsystem is configured to ensure that a majority, e.g., all, relevant scores get attributed to the right slots so as to be appropriately counted.  In this instance, the system is configured to ensure that during the transition period between the end of the\nperformance of Surfer 1 and the beginning of the performance of Surfer, scores being entered in evaluation of Surfer 1 are accurately attributed to Surfer 1, and those being entered in evaluation of Surfer 2 are accurately attributed to Surfer 2.  Such\nattribution can also be broken down more discretely such that scores entered to evaluate each maneuver of a performer in a performance get correctly attributed to that sub-part of the performance, e.g., that maneuver, in a series of maneuvers being\nperformed by that performer.  The system is further configured for adjusting and accounting for situations where an observer mis-attributes their entry into the wrong category, such as where they did not see the first wave caught by the performer, or the\nfirst maneuver performed, such as when they began evaluation the performance late, or where there is a technical glitch, or for whatever reason the scores are being delayed in their entry.\n The system has several configurations that are adapted for overcoming these challenges.  As explained above, one such set of mechanisms is to have a system administrator manually determine set-points, creating windows whereby scores during each\nwindow period get slotted to the manually determined scoring slot that has been set for that performer in that heat for that performance and with respect to the maneuver performed.  Alternatively, this may be performed automatically by the system, such\nas in accordance with preset timing and/or scoring regimes.  Additionally, this may also be performed in a fluid and/or dynamic manner, such as by the artificial intelligence module of the system.\n For instance, in a manual method, a physical human, or interface simulating a human like action, may open and close \"windows\" during which observers could provide subjective input, in this example scores, into the system.  In such an instance, a\nperson or process/device simulating a person, e.g., a smart camera or video stream using Artificial Intelligence) will set start/stop markers for which an observer or device may enter in input.  This would ensure observer or devices are only submitting\ninput for those waves or maneuvers that are currently active and insures accuracy and correct counting.  In particular, for example, a person or device may be interacting or watching an event, a surfer stands up on the wave and the person and\\or device\nsignals an action (\"SURFER UP:A\").  This action then triggers a window to be open for user or device input.  When the surfer has completed the wave or fallen, the person\\device signals n second action (\"SURFER DOWN:A\").  This action would signal or\nindicate that the surfer had completed the action and scores may or may not be allowed after this period (or in another case a timeout window may be utilized to allow input following a \"SURFER DOWN:A\" event of 30 seconds.  At that point the counter\nrecord would be increased by +1 for \"SURFER A\" and once SURFER A catches a second wave, the person or device would repeat the process; however, all inputs would now be counted towards SURFER A--WAVE 2.\n Additionally, an automatic method for overcoming these challenges may also be implemented.  In one implementation, the automatic method may produce a result that is similar to result achieved by the manual method.  However, the index or counter\nis increased leveraging software based interfaces that may or may not be hardcoded or otherwise embedded into the physical device/system or scoring devices.  These software interfaces can utilize REST calls, API or similar technology or standard, so as\nto send messages to the software that maintains an index or counter record to accurately place a score or user input into the correct \"slot\" as outlined in the manual method.  This configuration may be implemented in any device with or without a motor\nthat simulates or creates an event that could be used to score or engage fans.\n For example, a machine may be used in a manner so as to produce or simulate a condition that can then be transmitted to the system and used at set-points for evaluation and/or score entry and attribution.  Particularly, a good example of this is\nin the context of an artificial wave generating machine, which machine functions for the purpose of creating a wave for surfers in a competition.  In such an instance, in order to generate a wave, the wave machine will need to be started, demarcating the\nbeginning of a potential performance, and will then need to be stopped, thereby demarcating the end of the performance.  Accordingly, in such instances where automated mechanisms are implemented in a manner that initiates the beginning of an event, and\nfurther implemented in a manner that demarcates the ending of an event, the machine or a sensor associated therewith may communicate the beginning or the end of the event, such as via a wired or wireless communications network to the scoring system\nherein, which may then be employed in the evaluation and/or scoring regime in the manners described herein.  Particularly, in one instance, the machine, in this instance a wave generating machine, when running may be configured for continuously sending\nsoftware messages (RESTful APIs) to the remote scoring system (as disclosed herein) that keeps track of an index or counter that is used to properly slot users scores, such as based on the input generated by the machine, such as with respect to starting\nand stopping wave production.\n Specifically, when the wave generation machine starts it opens a window for scores to start being opened, and when it stops it closes a window for scores and increases index or counter as a result.  This action may then trigger a window to be\nopen for user or device input.  Likewise, when the machine has stopped, or a surfer (or other competitor in a different competition) has completed the maneuver, performance, and/or or has fallen, the machine signals a second action (\"stop\").  This action\nthen signals or indicates that the competitor has completed the action, at which point the counter record would be increased by +1 for \"SURFER A\" and once SURFER A catches a second wave, the person or device would repeat the process however all inputs\nwould not be counted towards SURFER A--WAVE 2.\n Likewise, these challenges may be overcome via a suitably configured artificial intelligence module of the system.  For instance, the A/I method may be implemented in a similar manner as the above to achieve the same or similar results, and in\nmany instances better, than those of both the manual and automatic methods disclosed above.  However, the A/I module uses software and A/I algorithms to maintain the index and counters of the waves.  In such an instance, the A/I module may be configured\nto open and close the event window for which scores or input are desired.  In one such instance, once the evaluation, e.g., scoring, window is open, users may then enter scores into their client devices, which scores may be sent to the computer system\nthat receives inputs like a messaging system (often referred to as a \"Message Bus\") and the system properly slots the users scores into the correct slot.  For example, the system itself or an event organizer may open an event for scoring.  User's may\nthen score or submit their opinions and the scores for the duration of the event.  All scores or inputs are then processed through the A/I system and automatically placed into the right place.  Alternatively, as described above in detail, the A/I module\nmay be configured for opening and closing a timeout window based on the velocity of scores, e.g., where scores may be entered prior to the establishment of the scoring window, where the mean or average or median score determines which scores will\neventually be counted, or the timeout window may be opened or closed based on system detectable user movement and/or by facial recognition.  Of course, a plurality of these mechanisms may be employed for the purpose of increasing accuracy.\n In various instances, the timeout window may be configured to be dynamic, such as where its parameters may change as new data is received and processed by the system over the course of an event and/or performance.  Hence, the timeout window may\nbe configured initially, e.g., by the event organizer, per event, but may change during the course of the event, or a sub-portion thereof.  In certain instances, a facial recognition (FR) module may be included, where the FR module is configured for\nassisting the system in determining the identification of participants, e.g., competitors, judges, and/or scorers, and/or other individuals within their social network, such as for determining bias, and in certain instances, the FR module may be employed\nto ascertain the beginning and/or ending of an event and/or one or more stages of the event.\n Particularly, the FR module may be configured so as to facilitate the identification of when an action or event has begun and/or concluded and/or enough time has been given to collect the inputs of the crowd so as to close the timeout window,\ne.g., when a surfer catches a wave, the window may be opened, and when he kicks out it may be closed, and may remain closed for the time period it takes him to paddle back out, or to when a second surfer catches the next wave, in which instance, only\nscores entered with respect to that second wave will be entered into the system.\n The A/I module and/or its predictive analytics may further be configured for determining the appropriate weighting for the scores entered into the system for an event and/or performance.  In certain instances, the weight given to the scoring may\nbe increased or decreased based on the history of the judge or scorer, such as where a scorer or judge consistently enters scores that fall within the mean, their scores may be given extra weight, e.g., the better a scorer performs the more they are\nrewarded and/or the more their future scores are weighted.  However, where a scorer or judge consistently enters scores that fall outside of the mean, their scores may be given less weight.  Accordingly, in various instances, a scoring and/or weighting\nmatrix may be generated, such as in the form of a lookup table whereby the system may automatically determine present and/or future weights based off of past weights employed by the system.  These weighting settings may be adjusted automatically by the\nsystem itself or may be adjusted by a user having administrative authorization, such as by an event organizer, e.g., on an event-by-event basis or globally.\n In another aspect of the disclosure, the application may be employed so as to generate an advertisement, such as an advertisement relevant to the fan and/or based on their engagement with the system and/or their location.  For instance, as\ndescribed above, the A/I module may be employed so as to generate a profile of a user.  The profile may be a list of properties, qualities, and/or characteristics that describe the user and/or their engagement with the system.  As such the profile may be\ngenerated by a plurality of different methods, such as by providing an interview to the user and saving their responses, further characteristics may be determined based on their engagement with the system, specifically, or social media, generally, such\nas by what they post, how and when they comment, the images they upload, and/or the activities surrounding the images they post, and the like.  Further characteristics may be determined based on how the user uses the system particularly, or the internet\ngenerally, such as by what searches they perform, who they follow, what pages they visit, the time spent on such pages, purchases they make and the like.  Additional characteristics may be defined by where the user is located, the places they visit, such\nas on a routine basis, and/or the places they or their friends have visited.\n In various instances, once these characteristics have been determined, the A/I module may determine various correlations between these characteristics, such as between the things or products the user likes, the location where the user is\nlocated, and the various different sponsors of the system, so as to generate a real-time advertisement that is generated in a manner to be specifically pertinent to the user, while at the same time generating interest in the event sponsors.\n Once the score has been entered and appropriately weighted by the A/I module, the scores may be presented for display at the user interface.  Particularly, FIG. 9B shows a user interface in which the user may view the competitors in relation to\neach other by weighted score in a mobile web browser.  As seen in FIG. 9B, the user has the option to view a window displaying a competitive scoreboard on a mobile web browser.  A portion of the screen displays information about the competitive event\nincluding but not limited to the name of the event, the event sponsor, the event category (e.g., GROM, junior men's', senior men's', junior women's', senior women's), and the heat.  For each competitor listed on the scoreboard, the scoreboard displays\nthe competitor's representation, e.g., jersey color, the competitor's individual scores per event, and the competitor's overall score.  This screen may be interactive providing the user a range of screens that may be selected from by interacting, e.g.,\npressing or touching, the screen in an interactive area of the screen.  In this instance, score data may be displayed in a number of different formats, such as averages, means, ranges, standard deviations, rates, curves, graphs, weights, biases,\nindividual or collective scores, averages between members selected to be in a group, and the like.  Hence, data from both individual's and teams may be collected, collated, and displayed individually or collectively.\n Likewise, FIG. 9C shows a user interface to be used by official judges of a competition to score competitors.  For example, official judges of an event can utilize the client application to score competitors of the event, such as on a touch\nscreen tablet, as seen in FIG. 9C, or desktop computer, and the like.  The screen shows a user interface by which judges can enter official scores of each competitor, e.g., demarcated by a selective representation like figure, emoji, animation, jersey,\ncolor, shape, and the like.  This screen may display the current time, a scoring legend, fields in which a judge can enter individual scores for each competitor, an option to view the scoring screen in full screen mode, and an option to print a score\nsheet in paper form.  Once the individual scores are entered, the average of the highest number, e.g., two, scores for each competitor may tabulated by the application and displayed in the right-most score field for each competitor.  Particularly, given\nthat the user, in this instance a judge, of the client application is an official judge and the device to be used is a smart tablet, the user may score competitors of an event by selecting portions of the screen for which to enter a score.  The user may\nalso choose to print a paper version of the scoreboard via a print button.  The user may also choose to view the scoreboard, or other screen, in full or partial screen mode via a full/partial screen button.  The user may also view the current time,\nand/or time left in the heat and/or event.\n FIG. 10A shows a user interface in which the user can configure engagement with the application by setting up a user account and user profile with the client application.  As seen in FIG. 10A, the user can configure engagement with the client\napplication by setting up a user profile with the client application.  The user may enter personal information including name, e-mail address, phone number, and gender.  The user may also have the option to upload profile photos associated with the\naccount.  Given that the user of the client application has created an account, the user may choose to create a user profile associated with the account.  The user may enter personal information including, but not limited to: name, email address, phone\nnumber, gender, social media contacts, and the like.  The user may then select a profile photo associated with the account.\n FIG. 10B shows a user interface in which the user can configure engagement with the client application by activating and deactivating various application settings.  The user of the client application may choose to configure settings of the\napplication as seen in FIG. 10B.  The user may activate and deactivate display settings related to the ability to view live events or past events.  The user may also choose to activate or deactivate voice scoring.  The user may also choose to score the\nevent anonymously.  Additionally, the user may choose to score an event as a competitor, a judge, or both.  Particularly, once the user of the client application has created an account, the user may choose to configure user engagement by customizing\napplication settings.  The user may choose, e.g., by touching the touch screen, to enable or disable the ability to view spectator events within a particular proximity via a toggle switch.  The user may also choose to enable or disable the ability to\nview previously completed events, e.g., via the toggle or switch.  The user may further choose to enable or disable the ability to view a video or audio streaming of an event, such as by engaging the relevant toggle switch.  Where set up, the user may\nchoose to enable or disable the ability to score an event by voice recording via a toggle switch.  Further, the user may choose to enable or disable the ability to be an anonymous user to other client application users.  Additionally, the user may choose\nto be identified in the client application as a competitor, a judge, or both by pressing a radio button.\n Likewise, FIG. 10C shows another user interface whereby the user can configure engagement with the client application by activating and deactivating various settings.  For instance, the user of the client application may configure engagement\nwith the client application by activating or deactivating settings, such as through a touch screen or voice directed interface, and may include the activations of location services, push notification settings, sound notification settings, and vibration\nnotification settings, which is seen in FIG. 10C.  The user may also choose to link their user account with other applications including PANDORA.RTM., FACEBOOK.RTM., and INSTAGRAM.RTM..  Particularly, the user may choose to configure user engagement by\ncustomizing application settings, may choose to enable or disable the ability to for the client application to use the user's location, to transmit push notification alerts to the user, to transmit breaking news alerts, to generate video and/or sound\nnotifications, and/or to generate vibration or other tactile notifications.  In various instances, the user may choose to allow the client application the ability to interact and sync with other applications to increase user engagement including\nPANDORA.RTM., FACEBOOK.RTM., INSTAGRAM.RTM., and/or other online social media platforms.\n FIG. 10D shows a user interface in which the user can view information about the events the user has signed up to score.  Specifically, once the user of the client application has decided on one or more events to score, the user may view a\nwindow displaying various information about the events including the name and location of the event, which is shown in FIG. 10D.  On this screen, the user may also select the days in which the user will score competitors of the event.  FIG. 10E shows a\nuser interface in which the user may choose to engage with international events.  The user of the client application may choose to engage in events all over the world, as seen in FIG. 10E.  The user may use the client application to score events\nincluding world champions, international events, European Football matches, such a Premier league, World Cup soccer, the Olympics, and other world-class events.  Particularly, the user of the client application has the option to view international\nevents, and may choose to view a plurality of scoreboards.  For instance, to view a scoreboard for all participants representing a particular country or region, the user may press on the country or region of interest to filter competitors by country or\nregion sorted by score in ascending order.  To view a scoreboard of all participants, the user may press on an \"ALL\" button to view all competitors sorted by score in ascending order to view all competitors in an event irrespective of country.\n FIG. 10F shows a user interface in which the user may view more information about an event.  In the user interface shown in FIG. 10F, a portion of the screen may be used to display information about a current competitor including name, user\nscore, crowd score, and score given by official judges.  Another portion of the screen may be configured to display the competitor's social media contacts, such as an INSTAGRAM.RTM.  username, live video stream of the event, or competitor photo.  The\nparticular social media to be selected can be chosen from a rolling tab of social media instances.  FIG. 10G shows a user interface in which the user may view more information about the event as well as a real-time countdown displaying the remaining\namount of time to score a competitor before the entered score is given a lower weight or eliminated.\n In FIG. 10G, a portion of the user interface may display information about the current competitor including the competitor's name, the competitor's social media, e.g., INSTAGRAM.RTM., username, and the country the competitor is representing if\nthe event is international.  Another portion of the screen is configured to display in real-time the amount of time remaining to score the current competitor without algorithmic deductions or elimination.  If the competitor is scored outside of this\nwindow, the A/I \"weight\" module may give the user's score less weight or discard the entered score.  A third portion of the screen may be configured to accept scores from the user either via a numeric keypad or through voice recognition.  See FIG. 10H.\n FIG. 10H shows a user interface in which the user may view more information about the current competitor as well as score the event using voice recognition.  For instance, once the user has selected a participant to score, the user will be led\nto a screen for which to enter a score via a numeric keypad or voice recording.  Once the user has either numerically entered or recorded the score for a participant, the user may view the entered score on the screen.  The user may also choose to remove\nthe entered score from the screen by pressing a button (e.g., an X button to close out the score, perhaps to re-enter a new score).  FIG. 10I shows a user interface in which the user may view more information about an event including competitor scores\nand competitor information.  In the user interface shown in FIG. 10I, a first portion of the screen may be configured to display advertisements during event intermissions.  A second portion of the screen may be configured to display information about the\nnext competitor in the event.  A third portion of the screen may be configured to display a scoreboard showing the event or wave number, the score(s) entered by the user, the crowd score(s), and the score(s) given by the official judges.  The user may\nalso choose to view on one or more of the screen portions the competitor's INSTAGRAM.RTM.  username, live video stream of the event, the next competitor in the event, or competitor photo.\n FIG. 10J shows a user interface in which the user has shared scores of a particular competitor using social media, such as on INSTAGRAM.RTM..  In the user interface shown in FIG. 10J, a user has shared an INSTAGRAM.RTM.  post of the scores of a\ncompetitor scored by the user.  The information in the post may include the name of the client application, the location of the event, the scores entered by the user, the average crowd score, and the scores entered by the official judges of the event. \nParticularly, once the user has shared the scores of a competitor on social media, e.g., FACEBOOK.RTM., INSTAGRAM.RTM., or the like, the client may view and customize the post generated by the client application on the social media application.\n FIG. 11A displays a page navigation layout of the client application.  FIG. 11A displays a page navigation layout of the client application in which the user may swipe in various directions to get to various pages of the client application. \nFrom various screens of the application, the user may choose to navigate to other screens either by pressing buttons or swiping up, down, left, or right on a smart phone.\n In various instances, as can be seen with respect to FIG. 11B, the system may include a Representational State Transfer platform (REST).  For instance, the server maybe configured implementing a stateless, client server, cacheable communications\nprotocol, such as in conjunction with a secured Hypertext Transfer Protocol (HTTP) protocol.  For instance, an event to be entered into the system may be configured as a REST event, which may be uploaded into the system via comma separated values file,\nthat is configured to allow data, e.g., event organization data, to be uploaded and/or saved within a database of the system in a table structured format, such as an Excel spreadsheet, e.g., but with a .csv extension.  Accordingly, as discussed with\nrespect to FIG. 9A, in various instances, an API may include a REST interface, such as for the generation of an event, such as by an organizer, as described above.\n In an additional aspect, in various embodiments, the system may include a training module, which training module may be configured for teaching a user to use the system, and may further be employed on training the system to receive and analyze\nuser results, which result may be informed by the training process.\n The contents of the articles, patents, and patent applications, and all other documents and electronically available information mentioned or cited herein, are hereby incorporated by reference in their entirety to the same extent as if each\nindividual publication was specifically and individually indicated to be incorporated by reference.  Applicants reserve the right to physically incorporate into this application any and all materials and information from any such articles, patents,\npatent applications, or other physical and electronic documents.\n The methods illustratively described herein may suitably be practiced in the absence of any element or elements, limitation or limitations, not specifically disclosed herein.  Thus, for example, the terms \"comprising\", \"including,\" containing\",\netc. shall be read expansively and without limitation.  Additionally, the terms and expressions employed herein have been used as terms of description and not of limitation, and there is no intention in the use of such terms and expressions of excluding\nany equivalents of the features shown and described or portions thereof.  It is recognized that various modifications are possible within the scope of the invention claimed.  Thus, it should be understood that although the present invention has been\nspecifically disclosed by preferred embodiments and optional features, modification and variation of the invention embodied therein herein disclosed may be resorted to by those skilled in the art, and that such modifications and variations are considered\nto be within the scope of this invention.\n The invention has been described broadly and generically herein.  Each of the narrower species and sub generic groupings falling within the generic disclosure also forms part of the methods.  This includes the generic description of the methods\nwith a proviso or negative limitation removing any subject matter from the genus, regardless of whether or not the excised material is specifically recited herein.\n Other embodiments are within the following claims.  In addition, where features or aspects of the methods are described in terms of Markush groups, those skilled in the art will recognize that the invention is also thereby described in terms of\nany individual member or subgroup of members of the Markush group.", "application_number": "16115559", "abstract": " Presented herein is an interactive platform for judging an activity by a\n     participant in an event. The platform includes a client application\n     program downloadable to a mobile device. The program may include a\n     database storing a mobile device identifier (ID), a user ID, user\n     information, and location data of the device. The application may further\n     be configured to display one or more events of the activity as well as an\n     input for receiving a score of the activity from the user. The platform\n     may additionally include a server system connected with the client\n     application programs via a communication network. The server system may\n     be configured for receiving the mobile device ID, the user ID, the user\n     information, and the location data for the client program, and may\n     further be configured to receive the scores from the users, and to adjust\n     the scores according to determined bias of the associated user.\n", "citations": ["7587214", "8442424", "8678899", "9009194", "9033781", "9044183", "9066144", "9165073", "9462030", "9751018", "20020165630", "20040171381", "20080154625", "20080242271", "20120022918", "20120179557", "20130185802", "20130203499", "20140089960", "20140100007", "20140156752", "20140164075", "20140164954", "20140278834", "20140364981", "20140365573", "20150302850", "20150350733", "20160004724", "20160105782", "20160171514", "20160180282", "20160224565", "20170087468", "20170223415"], "related": ["62667505", "62620452", "62567362"]}]