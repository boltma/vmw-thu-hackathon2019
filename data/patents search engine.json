[{"id": "20110313845", "patent_code": "10373193", "patent_name": "Learning display parameters to maximize advertising revenue", "year": "2019", "inventor_and_country_data": " Inventors: \nRichardson; Matthew (Seattle, WA), Bal; Hrishikesh (Bellevue, WA), Bapna; Deepak (Redmond, WA), Bilenko; Mikhail (Bellevue, WA), Crispo; Anthony (Issaquah, WA), Dominowska; Ewa (Kirkland, WA), Gupta; Arunesh (Seattle, WA), Kauhanen; Marty (Seattle, WA), Schult; Scott (Buffalo, NY)  ", "description": "<BR><BR>BACKGROUND\nMaintaining a website on the internet may entail a great deal of cost, based on server fees, bandwidth fees, and other expenditures.  A website is any source for digital data maintained by a server on the internet.  One revenue generating option\nis to provide advertising space on the website to an advertising platform.  The advertising platform may then project an online advertisement on the website.  While the content of the online advertisement may generally be provided by a client, the\nadvertisement characteristics may be controlled, for the most part, by an advertising platform.  The client is the entity paying for the advertising, usually for a brand, product or service sold by the client, while the advertising platform is the entity\nproviding the advertising.  The content of the advertisement is the information provided by the client contained in the advertisement.  The advertisement characteristics are the stylistic elements of the advertisement used to present the content of the\nadvertisement to the user.\nSome examples of advertisement characteristics may include the advertising background, the advertising title text properties, the advertising description text properties, the advertising display uniform resource locator (URL), a sponsored link\nbar, the sponsored link text properties, special effects, and border descriptions.  The advertising background may be described by parameters that include color, gradient, transparency, and patterns.  The advertising title text properties may be\ndescribed by parameters that include font face, color, size, and style.  The advertising description text properties may be described by parameters that include font face, color, size, and style.  The advertising display URL may be described by\nparameters that include font face, color, size, and style.  The sponsored link may be described by parameters that include color, location, and string.  The sponsored link text properties may be described by parameters that include font face, color,\nsize, and style.  The special effects may be described by parameters that include a drop shadow or a semi-transparent shadow.  The border descriptions may be described by parameters that include thickness, corner shape, or colors.\n<BR><BR>SUMMARY\nThis Summary is provided to introduce a selection of concepts in a simplified form that is further described below in the Detailed Description.  This Summary is not intended to identify key features or essential features of the claimed subject\nmatter, nor is it intended to be used to limit the scope of the claimed subject matter.\nEmbodiments discussed below relate to optimizing online advertising.  A data storage unit stores an evolving advertisement unit with an advertisement characteristic according to an initial configuration parameter.  A communication interface\ntransmits the evolving advertisement unit as part of a primary website.  A processor alters the evolving advertisement unit automatically upon a trigger event by changing the advertisement characteristic to follow an automatically generated configuration\nparameter to optimize an advertisement performance metric. <BR><BR>DRAWINGS\nIn order to describe the manner in which the above-recited and other advantages and features can be obtained, a more particular description is set forth and will be rendered by reference to specific embodiments thereof which are illustrated in\nthe appended drawings.  Understanding that these drawings depict only typical embodiments and are not therefore to be considered to be limiting of its scope, implementations will be described and explained with additional specificity and detail through\nthe use of the accompanying drawings.\nFIG. 1 illustrates, in a block diagram, one embodiment of a network.\nFIG. 2 illustrates, in a block diagram, one embodiment of a computing device.\nFIG. 3 illustrates, in a block diagram, one embodiment of an advertising supported website.\nFIG. 4 illustrates, in a block diagram, one embodiment of an advertisement profile.\nFIG. 5 illustrates, in a block diagram, one embodiment of a site profile\nFIG. 6 illustrates, in a block diagram, one embodiment of a user profile.\nFIG. 7 illustrates, in a flowchart, one embodiment of a method for creating an evolving advertisement unit with an initial configuration.\nFIG. 8 illustrates, in a flowchart, one embodiment of a method for altering an evolving advertisement unit.\nFIG. 9 illustrates, in a flowchart, one embodiment of a method for creating a test performance result.\n<BR><BR>DETAILED DESCRIPTION\nEmbodiments are discussed in detail below.  While specific implementations are discussed, it should be understood that this is done for illustration purposes only.  A person skilled in the relevant art will recognize that other components and\nconfigurations may be used without parting from the spirit and scope of the subject matter of this disclosure.  The implementations may be a machine-implemented method, a tangible machine-readable medium having a set of instructions detailing a method\nstored thereon for at least one processor, or an internet advertising system for a computing device.\nOnline advertising has become a useful revenue stream for most websites.  An advertising platform wants to make sure that the online advertising is effective.  The effectiveness of the online advertising may rely on both the brand, product, or\nservice being sold, referred to herein as the content, and the presentation selling the brand, product, or service, referred to herein as the advertising characteristics.  The content may generally be left to the client purchasing the advertising.  The\nadvertising characteristics may be manipulated by the advertising platform to select an optimum advertising characteristic.\nDetermining an optimum advertising characteristic may be a time intensive process.  Calibrating the optimum advertising characteristics may be further complicated by the fact that an effective advertising characteristic may lose effectiveness\nover time as a user is continually exposed to that advertising characteristic.  Additionally, an effective advertising characteristic during the summer may no longer be effective during the holiday season.  For example, a green and red banner\nadvertisement may stand out over the summer but not during Christmas time.\nAn advertising platform may improve the effectiveness of the online advertising using machine learning.  In machine learning, the advertising platform may test a large number of similar advertisements for a single product.  For online machine\nlearning, the test set of advertisements may be placed on the internet, with the results collected from the internet placements.  The advertising platform may make changes to a key advertising characteristic on subsets of those advertisements, and then\nmeasure how well the advertisement performs.  Those advertising characteristic adjustments that provide a statistically significant improvement in the advertisements performance may be used as the basis for making a similar change to other similar\nadvertisements.  The advertising characteristic adjustments may be performed and monitored using traditional statistical methods, such as A/B testing, banded convex optimization, multi-armed bandit solutions, or other statistical methods.\nThe test set of advertisements may be sorted so that similar advertisements are grouped together, as well as advertisements on similar web sites or advertisements viewed by demographically similar users.  Additionally, the advertisement\ncharacteristic adjustment may be determined based on the demographic or viewing profile of the user viewing the advertisement.  For example, an advertising platform may determine through online machine learning that middle-aged males that make over\n$100,000 a year and like auto-racing tend to favor advertisements with blue Times New Roman descriptive text.  Further, the advertisement characteristic adjustment may be determined based on the website displaying the advertisement.  For example, the\nadvertising platform may determine that banner advertisements with a solid red border tend to be more effective on celebrity gossip websites.\nThus, in one embodiment, an evolving advertising system automatically optimizes internet advertising.  In this instance, the term \"automatically\" refers to optimization not initiated by an administrator.  A data storage unit may store an\nevolving advertisement unit with an advertisement characteristic according to an initial configuration parameter.  A communication interface may transmit the evolving advertisement unit as part of a primary website.  A database interface may connect to a\ndatabase storing a test performance result from a parallel test run on a test set of advertisement units.  A processor may alter the evolving advertisement unit automatically upon a trigger event by changing the advertisement characteristic to follow an\nautomatically generated configuration parameter based on a test performance result to optimize an advertisement performance metric of the evolving advertisement unit.  The trigger event is an external event not initiated by an administrator that alerts\nthe advertisement server to alter an advertising characteristic.\nFIG. 1 illustrates one embodiment of a communication system 100, such as the internet.  The communication system 100 may include a core network 102 that may be accessed by a user terminal 104.  The user terminal 104 may also be referred to as\nsubscriber units, desktops, laptops, work stations, mobiles, mobile stations, user, wireless communication devices, user devices, or by other terminology used in the art.  Various communication devices may exchange data or information through the core\nnetwork 102.  The core network 102 may be a local area network (LAN), a wide area network (WAN), the internet, a WiMAX network, a universal terrestrial radio access network (UTRAN) cellular network, an evolved UTRAN (E-UTRAN) cellular network, or other\ntype of telecommunication network.  A server or a series of servers controlled by a site operator, referred to herein as a website server 106, may administer and provide a website to the user terminal 104.  A website server 106 is any server that\nprovides data content to the user terminal 104.  A website server 106 may have an arrangement with an advertisement server 108 to allow the advertisement server 108 to attach online advertising content to the website.\nThe online advertising content may be web page banner advertisements, search engine text advertisements, web page text advertisements, or other forms of online advertising.  The advertisement server 108 may be associated with a learning module\n110 that accesses a historical performance database 112 to determine the configuration parameters for an advertisement characteristic of an online advertisement to produce a result.  The configuration parameters may have produced statistically\nsignificant results in other online advertisements or may be suggestions for experimental configuration parameters.  The result may be quantified by an advertisement performance metric.  The advertisement performance metric may be the number of users\nthat have selected the advertisement and been led to a web page of a client, the number of viewers of the website that have performed a commercial transaction at the web page of the client, the number of positive survey responses, and other metrics.  The\nadvertising performance metric may be chosen by the advertising platform or by the client.\nThe historical performance database 112 may contain a test performance result collected from a test set of advertisement units shown to one or more test terminals 114 in a parallel test run.  The test performance result may indicate\nconfiguration parameters that produce the optimum result for a given metric in the test set of advertisement units as compared to a control set.  The test set of advertisement units may have a similar advertisement profile as the advertisement unit. \nSimilarly, the test set of websites displaying the test set of advertisement units may have a similar site profile as the website.  The user terminal 104 may have a similar user profile as the test terminal 114.  The user terminal 104 may act as a test\nterminal 114 in some instances.\nFIG. 2 illustrates a block diagram of an exemplary computing device 200 which may act as a user terminal 104, a website server 106, or an advertisement server 108.  The computing device 200 may include a bus 210, a processor 220, a memory 230, a\nread only memory (ROM) 240, a storage device 250, an input device 260, an output device 270, a communication interface 280, and a database interface 290.  The bus 210 may permit communication among the components of the computing device 200.\nThe processor 220 may include at least one conventional processor or microprocessor that interprets and executes a set of instructions.  The memory 230 may be a random access memory (RAM) or another type of dynamic storage device that stores\ninformation and instructions for execution by the processor 220.  The ROM 240 may include a conventional ROM device or another type of static storage device that stores static information and instructions for the processor 220.  The storage device 250\nmay include any type of tangible machine-readable medium, such as, for example, magnetic or optical recording media and its corresponding drive.  The storage device 250 may store a set of instructions detailing a method that when executed by one or more\nprocessors cause the one or more processors to perform the method.\nThe input device 260 may include one or more conventional mechanisms that permit a user to input information to the computing device 200, such as a keyboard, a mouse, a voice recognition device, a microphone, a headset, etc. The output device\n270 may include one or more conventional mechanisms that output information to the user, including a display, a printer, one or more speakers, a headset, or a medium, such as a memory, or a magnetic or optical disk and a corresponding disk drive.  The\ncommunication interface 280 may include any transceiver-like mechanism that enables processing device 200 to communicate with other devices or networks.  The communication interface 280 may include a network interface or a mobile transceiver interface. \nThe communication interface 280 may be a wireless, wired, or optical interface.  In one embodiment, the communication interface 280 may include a universal serial bus (USB) interface, a Bluetooth.RTM.  interface, or other such interface that may be used\nto attach peripheral devices or pair other communication devices.  A separate database interface 290 may interact with the historical performance database 112.\nThe computing device 200 may perform such functions in response to processor 220 executing sequences of instructions contained in a computer-readable medium, such as, for example, the memory 230, a magnetic disk, or an optical disk.  Such\ninstructions may be read into the memory 230 from another computer-readable medium, such as the storage device 250, or from a separate device via the communication interface 280.\nFIG. 3 illustrates, in a block diagram, one embodiment of a website 300.  The website 300 may have a website 310 containing the content data provided by the website server 106.  The advertisement server 108 may provide an evolving advertisement\nunit 320 to be attached to the website 310 when displayed to a user.  The evolving advertisement unit 320 has advertisement characteristics that change over time.  The advertisement characteristics are stylistic features of the evolving advertisement\nunit 320 unrelated to the content of the advertisement.  The advertisement characteristics of the evolving advertisement unit may include the advertising background, the advertising title text properties, the advertising description text properties, the\nadvertising display uniform resource locator (URL), the sponsored link bar, the sponsored link text properties, special effects, and border description.  The advertising characteristics may be initialized from configuration parameters, such as color,\ngradient, transparency, pattern, font, size, style, location, thickness, or other specific parameters.\nThe evolving advertisement unit 320 may act as a link to a vendor site 330 owned by the client purchasing the evolving advertisement unit 320.  The vendor site 330 may contain information leading to a physical location of the client or may allow\nthe client to sell goods or services online.  The vendor site 330 may have a user feedback input 340 to allow a user to critique or grade the effectiveness of the online advertisement.  The user feedback input 340 may also be present directly on the\nevolving advertisement unit 320.\nThe historical performance database 112 may maintain an advertisement profile of each of the evolving advertisement units 320 distributed by the advertising platform.  The historical performance database 112 may also maintain a site profile for\neach of the websites 310 supported by the advertising platform to tailor the advertisement characteristics of the evolving advertisement unit 320 to the website 310 displaying it.  For even greater accuracy, the advertising platform may maintain a user\nprofile for any of the user terminals 104 that view the evolving advertisement units 320.  Such records may allow the advertisement server 108 to tailor the advertising characteristics of the evolving advertisement unit 320 to the individual user.\nFIG. 4 illustrates, in a block diagram, one embodiment of an advertisement profile 400.  The advertisement profile 400 may have an advertisement identifier (ID) 402 to associate the advertisement profile 400 with the evolving advertisement unit\n320.  The advertisement profile 400 may have an advertisement content profile 404 describing the content being shown in the evolving advertisement unit 320.  The advertisement profile 400 may have an age field 406 describing the date that the evolving\nadvertisement unit 320 was first attached to the website 310.  The age field 406 may be used to determine when to alter the evolving advertisement unit 320, keeping an evolving advertisement unit fresh.  The advertisement profile 400 may have one or more\nadvertisement characteristic fields 408 describing stylistic features of the evolving advertisement unit 320.  The advertisement characteristic field 408 may be associated with one or more configuration constraint fields 410 describing any configuration\nconstraints placed on the configuration parameters that may be used by the advertisement characteristics.  These configuration constraints may be determined by the website administrator, the advertising platform, the client, or even the user terminal\n104.  The advertisement characteristic field 408 may be associated with one or more initial configuration parameter (ICP) fields 412 indicating the configuration parameters that may be used in creating an initial form for the associated advertisement\ncharacteristic.  An ICP field 412 may indicate a configuration parameter for an advertisement characteristic the first time the evolving advertisement unit 320 is displayed on the website 310.  The ICP field 412 may be based on the performance of that\nconfiguration parameter on other advertisement units 320 on that website 310, the same advertisement unit on other websites 310, or different advertisement units 320 on other websites 310.  The advertisement characteristic field 408 may be associated\nwith an automatically generated configuration parameter (AGCP) 414 to indicate a new configuration parameter that the evolving advertisement unit may use to alter an associated advertisement characteristic field 408.  The advertisement characteristic\nfield 408 may be altered multiple times by a new AGCP as the evolving advertisement unit 320 ages.  The advertisement characteristic field 408 may be associated with a season field 416 to indicate if the advertisement characteristic has a seasonal\ncharacteristic, with certain configuration parameters to be used based upon the time of year.  Seasonal characteristics are not limited to the traditional summer, fall, winter, spring seasons, but refers to any annual period that may affect an\nadvertisement characteristic, such as the Christmas shopping season or baseball season.  The AGCP field 414 may be associated with one or more advertisement performance metric fields 418 indicating a projected score for that advertisement performance\nmetric using the AGCP based upon a test performance result by a test set of advertisement units.  The advertisement performance metric field 418 may be associated with a weighting (WT) field 420, allowing multiple performance metrics to be taken into\naccount, while assigning a priority to each performance metric.\nFIG. 5 illustrates, in a block diagram, one embodiment of a site profile 500.  The site profile 500 may have a site ID 502 to associate the site profile 500 with the website 310.  The site profile 500 may have a site content profile 504\ndescribing the content being shown in the website 310.  The site profile 500 may have one or more advertisement characteristic fields 506 describing stylistic features of any advertisement units 320 found on the website 310.  An advertisement\ncharacteristic field 506 may be associated with one or more site configuration parameter (SCP) fields 508 indicating configuration parameters for the advertisement characteristics that have historically worked for that website.  The SCP field 508 may be\nassociated with one or more advertisement performance metric fields 510 indicating a projected score for that advertisement performance metric using the SCP based upon previous advertisements on the website 310.  The advertisement performance metric\nfield 510 may be associated with a WT field 512, allowing multiple performance metrics to be taken into account, while assigning the priority of each performance metric.\nFIG. 6 illustrates, in a block diagram, one embodiment of a user profile 600.  The user profile 600 may have a user ID 602 to associate the user profile 600 with the website 310.  The user profile 600 may have a user content profile 604\ndescribing the content historically selected by the user, as well as any obtainable demographic information describing the user.  The user profile 600 may have one or more advertisement characteristic fields 606 describing stylistic features of any\nadvertisement units 320 historically favored by the user.  An advertisement characteristic field 606 may be associated with one or more user configuration parameter (UCP) fields 608 indicating configuration parameters for the advertisement\ncharacteristics that have historically been favored by that user.  The UCP field 608 may be associated with one or more advertisement performance metric fields 610 indicating a projected score for that advertisement performance metric using the UCP based\nupon previous advertisements viewed by that user.  The advertisement performance metric field 610 may be associated with a WT field 612, allowing multiple performance metrics to be taken into account, while assigning the priority of each performance\nmetric.\nFIG. 7 illustrates, in a flowchart, one embodiment of a method 700 for creating an evolving advertisement unit 320 with an initial configuration.  The advertisement server 108 may assign a site profile 500 to a primary website 310 (Block 702). \nThe advertisement server 108 may assign a user profile 600 to a user viewing the primary website 310 (Block 704).  If the advertisement server 108 has received configuration constraints from an administrator (Block 706), the advertisement server 108 may\napply the configuration constraint to the appropriate advertisement characteristics of an evolving advertisement unit 320 (Block 708).  The advertisement server 108 may check an initial test performance result by a test set of advertisement units (Block\n710).  The advertisement server 108 may determine ICPs based on the initial test performance results (Block 712).  The advertisement server 108 may create the evolving advertisement unit 320 with advertisement characteristics according to the ICPs (Block\n714).  The advertisement server 108 may display the evolving advertisement unit 320 on a primary website 310 (Block 716).  The advertisement server 108 may initiate an advertisement characteristic age counter to measure the age of an advertisement\ncharacteristic (Block 718).  The age of the advertisement characteristic may be based on period of time since the advertisement characteristic was implemented or the number of views of the advertisement characteristic.\nFIG. 8 illustrates, in a flowchart, one embodiment of a method 800 for altering an evolving advertisement unit 320.  The advertisement server 108 may display the evolving advertisement unit 320 on a primary website 310 (Block 802).  The\nadvertisement server 108 may use a trigger event to initiate a change to the evolving advertisement unit 320.  The trigger event may be an advertisement performance metric falling below a threshold rate, an advertisement characteristic reaching a\nthreshold number of views, the expiration of a viewing period, or a seasonal change, such as the beginning of the Christmas shopping season.  If the advertisement server 108 identifies a trigger event (Block 804), the advertisement server 108 may read a\nuser profile 600 of the user viewing the website 302 (Block 806).  The advertisement server 108 may read a site profile 500 for the website 302 (Block 808).  The advertisement server 108 may check test performance results by a test set of advertisement\nunits that were displayed on a website 310 similar to the site profile 500 to a user terminal similar to the user profile 600 (Block 810).  If the advertisement server 108 has received configuration constraints from an administrator (Block 812), the\nadvertisement server 108 may apply the configuration constraints to the appropriate advertisement characteristics of an evolving advertisement unit 320 (Block 814).  The advertisement server 108 may check if any seasonal characteristics apply to the\nevolving advertisement unit 320 (Block 816).  The advertisement server 108 may create AGCPs based on the test performance results using machine learning, as well as the user profile 600, the site profile 500, any configuration constraints, and any\nseasonal characteristics (Block 818).  The advertisement server 108 may alter the evolving advertisement unit 320 automatically upon the trigger event by changing the advertisement characteristics to follow the AGCPs to optimize one or more advertisement\nperformance metrics (Block 820).  The advertisement server 108 may display the evolving advertisement unit 320 on a primary website 310 (Block 822).  The advertisement server 108 may reset the advertisement characteristic age counter (Block 824).\nFIG. 9 illustrates, in a flowchart, one embodiment of a method 900 for creating a test performance result, described here as online machine learning.  The test performance result may be an initial test performance result.  The advertisement\nserver 108 may display the test set of advertisement units 320 on multiple websites 310 over a similar period, executing a parallel test run (Block 902).  The advertisement server 108 may sort the test set of advertisement units by user profiles 600 for\nthe users viewing the advertisement unit 320 (Block 904).  The advertisement server 108 may sort the test runs by the site profile 500 of the website 310 hosting the advertisement unit 320 (Block 906).  The advertisement server 108 may alter one or more\nconfiguration parameters for one or more advertisement characteristics for a subset of the test set, leaving the rest as a control group (Block 908).  The advertisement server 108 may collect explicit advertisement performance metrics for the test set of\nadvertisement units (Block 910).  An explicit advertisement performance metric is based on actively seeking the affirmative opinion of a user viewing the advertisement unit 320.  The advertisement server 108 may determine implicit advertisement\nperformance metrics for the test set of advertisement units (Block 912).  An implicit advertisement performance metric is a response to the advertisement unit 320 inferred from the actions of a user viewing the advertisement unit 320.  If the explicit\nadvertisement performance metric or the implicit advertisement performance metric is above a pre-set threshold (Block 914), then the change to the configuration parameters may be added to the test performance result (Block 916).\nAlthough the subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that the subject matter in the appended claims is not necessarily limited to the specific features or\nacts described above.  Rather, the specific features and acts described above are disclosed as example forms for implementing the claims.\nEmbodiments within the scope of the present invention may also include non-transitory computer-readable storage media for carrying or having computer-executable instructions or data structures stored thereon.  Such non-transitory\ncomputer-readable storage media may be any available media that can be accessed by a general purpose or special purpose computer.  By way of example, and not limitation, such non-transitory computer-readable storage media can comprise RAM, ROM, EEPROM,\nCD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to carry or store desired program code means in the form of computer-executable instructions or data structures. \nCombinations of the above should also be included within the scope of the non-transitory computer-readable storage media.\nEmbodiments may also be practiced in distributed computing environments where tasks are performed by local and remote processing devices that are linked (either by hardwired links, wireless links, or by a combination thereof) through a\ncommunications network.\nComputer-executable instructions include, for example, instructions and data which cause a general purpose computer, special purpose computer, or special purpose processing device to perform a certain function or group of functions. \nComputer-executable instructions also include program modules that are executed by computers in stand-alone or network environments.  Generally, program modules include routines, programs, objects, components, and data structures, etc. that perform\nparticular tasks or implement particular abstract data types.  Computer-executable instructions, associated data structures, and program modules represent examples of the program code means for executing steps of the methods disclosed herein.  The\nparticular sequence of such executable instructions or associated data structures represents examples of corresponding acts for implementing the functions described in such steps.\nAlthough the above description may contain specific details, they should not be construed as limiting the claims in any way.  Other configurations of the described embodiments are part of the scope of the disclosure.  For example, the principles\nof the disclosure may be applied to each individual user where each user may individually deploy such a system.  This enables each user to utilize the benefits of the disclosure even if any one of a large number of possible applications do not use the\nfunctionality described herein.  Multiple instances of electronic devices each may process the content in various possible ways.  Implementations are not necessarily in one system used by all end users.  Accordingly, the appended claims and their legal\nequivalents should only define the invention, rather than any specific examples given.", "application_number": "12818157", "abstract": " In one embodiment, an evolving advertising system automatically optimizes\n     internet advertising. A data storage unit 250 may store an evolving\n     advertisement unit 320 with an advertisement characteristic according to\n     an initial configuration parameter. A communication interface 280 may\n     transmit the evolving advertisement unit 320 as part of a primary website\n     310. A processor 220 may alter the evolving advertisement unit 320\n     automatically upon a trigger event by changing the advertisement\n     characteristic to follow an automatically generated configuration\n     parameter to optimize an advertisement performance metric.\n", "citations": ["20040123247", "20060212350", "20070027754", "20070260520", "20080114639", "20080262913", "20090024480", "20090119179", "20100042421", "20110054980"], "related": []}, {"id": "20150070585", "patent_code": "10375343", "patent_name": "System and method for managing streaming services", "year": "2019", "inventor_and_country_data": " Inventors: \nSharif-Ahmadi; Seyed M. (Richmond, CA), Armani; Sam (Richmond, CA)  ", "description": "This application claims the benefit of Canadian patent application numbers 2,773,342\nand 2,791,935, which are hereby incorporated by reference.\n<BR><BR>TECHNICAL FIELD\nThe present disclosure is directed at a system and method for managing content delivery services, and more particularly for managing such services amongst a plurality of user devices though a common gateway.\n<BR><BR>BACKGROUND\nThe growth in data transmitted over wireless and cable networks has been driving consolidation of broadband operators, and creating a highly competitive global broadband service provision market.  \"Operator\" refers to an operator of a high-speed\nnetwork that offers data transfer services over one or more types of networks (e.g.: mobile networks and other wireless networks, wired networks, including telephone, cable satellite, and mobile networks, or a combination of these).  \"Broadband service\"\nincludes content delivered over a network; the network may be wired, wireless, cable, DSL, or other digital communication systems, or any combination thereof.\nOperators are evolving and becoming broadband service providers (referred to herein as \"service providers\").  Triple or quad operators means operators that provide three or four services, such as residential Internet service, residential phone\nservice, residential television, and mobile data services, which may in turn, include voice, Internet, and television.  Data usage over both wireless, DSL and cable networks around the world is growing.  Service providers may grow revenue by offering\nusers a variety of services, and by reducing the cost of the network operation centers (\"NOCs\") and the cost of data links.  Today, service providers are offering more services and applications, requiring larger NOCs, which in turn translates to higher\noperating costs for the service providers and therefore lower average revenue per user (\"ARPU\").  Service providers are looking for solutions that allow them to run smaller NOCs, offer services suited to users' profiles and the type of device that the\nuser is using at the time (e.g.: television (\"TV\"), laptop computer, desktop computer, pads, eReaders, or smartphone).\nToday, service providers can often identify where users are located, but it is difficult to know what type of content users want based on their context.  \"Context\" includes information about the nature of the user and content, including:\ninformation that refers to the user's location; the type of device the user is using to access the network; the level of connectivity access the user has; the user state, which includes information such as whether the user is connected to the network\nusing a mobile or wired connection; whether the user is accessing the network from home or while traveling (e.g.: on vacation or on a business trip); to which networks the user is connected; and the user's interests and history.\nUser demand for accessing media-rich content in real time is growing; concurrently, the types of devices that are used to view such media-rich content are also growing.  Broadband networks resulted from the migration of multiple networks, each\nhaving different characteristics and operating as silo networks wherein devices, applications, and networks are tightly coupled.  Typical broadband networks are designed based on the assumption that user devices used to access content are dumb terminals\nwith little ability to run rich multimedia applications.  Typical broadband networks were also designed under the assumption that traffic growth will remain linear, which has been an underestimate.\nConventional solutions to satisfying user demands for delivery of rich multimedia content in real-time has centered on a \"core-centric\" approach in which a centralized server resides within a NOC for each different type of network; an exemplary\n\"core-centric\" network 100 is depicted in FIG. 1.  This means that if a service provider is operating broadband networks to deliver TV content, residential Internet content and cellular phone content to specific user devices, then such service providers\nwould use three different NOCs, TV NOC 250, Internet NOC 350 and cellular NOC 450, respectively.  Each is directed to a particular type of user device, which usually do not share networks or content, for example, the user's laptop computer cannot use the\ncellular phone's cellular network to access content.  Likewise the cellular phone cannot access the cable network.\nIn such a model, a service provider uses software and servers that offer functionality such as determining the automatic bit rate (\"ABR\"), providing content inspections, and providing personalization engines that are installed within each NOC. \nChallenges arise when employing these approaches and serving large numbers, e.g. millions, of users; shortcomings associated with these approaches include scalability issues, accuracy issues, and synchronization of collected information.  As depicted in\nFIG. 1 a triple and quad player operator is managing multiple NOCs 250, 350 and 450, and within each NOC is run the same set of services including video transcoding and transrating engines, content reformatting, caching and proxy services.\nThree changes are simultaneously occurring with respect to broadband networks.  The first relates to digital content.  The amount of content available on the Internet is overwhelming for end users, even very technically savvy users, and the\namount of content is still growing exponentially.  This includes on-demand digital video streaming, television services such as video on demand (\"VoD\"), subscription video on demand (\"SVoD\") and pay-per-view (\"PPV\").  Also included in this market is\non-line video advertising, Internet protocol television (\"IPTV\") and mobile TV, as well as television provided through cable, digital cable set top boxes, and satellite.  References to \"television\" or \"TV\" herein shall refer to any of the above listed\nstreaming video services.\nThe second relates to the effect of a new generation of users and their needs.  These new users (often referred to as \"Millennials\"), represent the most populous generation that has ever lived on this planet.  They tend to be technology-centric,\nand both dependent on and aware of technology.  On average, each spends over $100 per week on technology-oriented products and services and directly influences over 80% of the spending in the home.  This is the generation that wants the right information\nsuited to their needs and context, delivered in the least amount of time.  This group of users are socially connected through their mobile phone, laptop and desktop computers, and are the driving force behind enabling connectivity through TV.  This\ngeneration wants to have personalized content; namely, content that is available on their own terms rather than on the terms of service providers and operators.  This is the generation that does not want to be bound to a particular location or device to\naccess specific content.  They like to be able to watch TV content on any device and location within and outside of the home and not just on a TV display.  They also want to be able to access Internet content and/or social networking services such as\nTwitter on their TV display while watching TV.  They are not only content consumers but also content generators and distributors.\nThe third relates to advancements in technology, and particularly advancements in customer premises equipment (\"CPE\").  A household often no longer just has a single TV display and a PC but may have multiple laptops and PCs, along with TV\ndisplays and mobile devices, such as smart phones, cellular phones, video game devices, net books, electronic reading tools (\"eReaders\"), pads, and portable music and video players, that are used in or outside of their homes (collectively, referred to\nherein as \"user devices\").  Additionally, users often have access to other user devices such as home residential gateways, set-top-boxes, routers, Wi-Fi access points and other networking equipment, and the use and availability of such equipment is\ngrowing rapidly.  These changes mean that content is no longer created, controlled and distributed by a specific organization, such as service providers, but instead content can be produced by anyone within a network and either pushed to, or requested\nby, anyone within the network.  Such content includes place shifted video content, multimedia streams, and personal digital content.  Therefore, a centralized approach wherein content is always produced by a selected entity and then distributed to\nsubscribers will no longer be effective due to variation between user interests, and because many users wish to play a more active role in generating content.  Additionally, to centralize all content being created for distribution also will not be\neffective due to the heavy network traffic that would result from distributing such content.\nShortcomings associated with the prior art include: 1.  Pre-formatted content storage any types of device Users' content preferences are very different, when, for example, they are outside of home, than when they are in front of their big TV\nscreen.  Deciding on storing the type of content for possible access from outside of home is therefore difficult.  Also keeping multiple formats suitable for all potential user devices that may be used to experience the content is not efficient.  Typical\nuser devices renew and update data displays very quickly and therefore repeatedly mining data and archiving different formats for possible access by different user devices is cumbersome, costly and inefficient.  2.  Content Inspection The data traffic\npath for the user is not always the same.  The purpose of content inspection is to identify the user's interests.  However, the user's interest is best determined based on user habits over a period of time and not solely based on a snapshot of the user's\ncurrent content usage.  Distributing content inspection over multiple network nodes provides a snapshot of the user's current usage based only on the current traffic flow.  Storing all traffic flows, determining the user identity, synchronizing the\ninformation for a post content inspection process in order to inspect usage over a longer period of time requires large amounts of storage, and extensive processing time.  This is highly costly and inefficient.  Conventional means allow the type of\ncontent the user is looking at any given moment in time to be determined, rather than a long term view.  Consequently, the real preferences of the user under different conditions cannot be predicted accurately.\nIn the prior art, a CPE only facilitates providing CPE specific and vertical functions, e.g. a CPE that only provides Set-Top-Box functionality or a CPE that provides Internet connectivity functionality.  Furthermore, in conventional methods the\nCPE has no role in content distribution functionality among subscribers while they are at home or outside using different methods of connectivity and in delivering content concurrently to different devices and/or users in parallel.\nAccordingly, there exists a need for a method and system that improves on the deficiencies of the prior art.\n<BR><BR>SUMMARY OF THE INVENTION\nThe system and method according to the invention includes a gateway (also referred to herein as a \"serving node\" or \"debox\"), to which user devices are registered and which serves as an intermediary between such registered user devices and\ncontent.\nThe system and method according to the invention includes a gateway having a control manager and a streaming server.\nThe control manager is responsible for registration of user devices with the box, including information about the capabilities and limitations of such device, and the users with whom the device is associated.\nThe control manager maintains usage logs for each user device.  These logs can be used to improve Quality of Service (QoS) by determining when errors or poor performance occurs and taking steps to improve the quality (for example by increasing\nthe buffer for streaming video).\nThe control manager acts as a message broker between user devices when one is used to interact with or access content on the other.\nThe control manager also manages the TV tuners included as part of the gateway.  Such tuners, and associated EPG information, are set to the appropriate channel and output to the appropriate user device by sending a request to the streaming\nserver.\nThe streaming server handles several duties.  It opens the port to the TV tuner, and handles the Electronic Program Guide (EPG), and modifies the format of content to the type of user device requesting the signal.\nThe streaming server also sets and monitors the quality of signal based on the size of the display.  The streaming server controls the size of the buffer needed based on the limitations of the device, its processor, the size of the display and\nthe signal.  Past user experiences with the device may be taken into account.\nThe streaming server thus can adapt to learned limitations of a user device to improve experience and can handle resolution changes and needs\nWhen a user device is receiving Internet content, the streaming server can send a link to the user device to the content and play no other role.  Alternatively, if the user device is local to the streaming server, the gateway can receive the\ncontent and adapt it for the user device.\nThe streaming server uses a transcoder to convert content from one format to another.  The transcoder provides both a coding and decoding process.  It receives input, decodes it, and codes it again as needed by the user device.\nThe gateway is able to authenticate users and user devices.  Limits on use may be in place (for example limits on a user's ability to access certain web sites or channels).  A single sign on (SSO) and backend gateway system may be used.\nThe authentication process can be used for local filtering so as not to display certain scenes displaying objectionable material (such as violence/sex) based on ratings of content (which may be scene by scene or a larger piece of content, like a\nshow or movie).  This can also be used to handle parental controls such as limit use to certain hours of the day or an overall limit on usage per day.\nOn use of the system is to take advantage of the connection between the user device and the gateway, which occurs even if the user device is distant from the gateway (e.g. outside of Wi-Fi range).  This allows a user device to receive adds based\non past users experiences, like browsing history, not available when logging in directly to a distant network.\nA method of determining a closest node for a user device is provided, including: providing a gateway having a server for receiving a request for content from a mobile user device at a remote location; authenticating the user and user device as\nassociated with the gateway; requesting the mobile device type and IP address from the mobile device; determining a geographic location of the mobile device; determining a proximity routing table for the mobile device; and determining a proximity\nneighbour information table for the mobile device.  The gateway further accesses a database associated with the user device to determine patterns of the user thereof.\nThe method of claim 2 wherein the patterns are provided to an advertisement server, and advertisements are transmitted to the mobile device based on the patterns.\nA method of displaying content on a television display is provided, including: determining a plurality of zones on said television display; displaying a video stream on a first zone; displaying means for user input on a second zone; and\ndisplaying content from the Internet on a third zone.  The content from the Internet may be from a social networking web site.\nA system for controlling a television with a mobile phone is provided, including: a gateway in communication with the television and the mobile phone; an application on the mobile phone displaying the functions of a remote control; on input from\na user of the mobile phone, the input is communicated to the gateway, the gateway communicating the input to the television.  The display on the mobile phone may be a d-pad.\nA method of controlling a television with a mobile phone is provided, including: providing a gateway in communication with the television and the mobile phone; providing an application on the mobile phone, the application displaying the\nfunctions of a remote control on a touchscreen of the phone; on receipt of input on the mobile phone, communicating the input to the gateway; on receipt of the input from the gateway communicating the input to the television.\nA method of determining usage patterns is provided, including; providing a gateway through which a plurality of users with a plurality of user devices can process content; when one of said users process content on one of said user devices,\ndetermining a particular user and a particular user device; the gateway gathering information about the content; analyzing the pattern of content processed by the particular user.\nA method of obtaining feedback about a video stream is provided, including: the video stream provided to a user device, through a gateway, said user device and an associated user registered with said gateway, including biographic information\nabout said user; at the conclusion of said video stream, providing a questionnaire to said user, answerable on said user device; Said gateway retrieving answers to said questionnaire, and returning said answers and biographic information to a server. \nThe video stream may be a pilot episode of television.\nA method of remotely managing a gateway in a residential home is provided, including: a service provider providing the gateway to the home; the gateway generating functionality reports to the service provider; the service provider providing\ntechnical support to the gateway based on the functionality reports.\nAn earlier embodiment of the gateway is described in PCT application no. PCT/CA2010/001536, which is hereby incorporated by reference. <BR><BR>BRIEF DESCRIPTION OF THE FIGURES\nFIG. 1 is a block diagram illustrating a prior art network services a number of user devices with both wireless and wired (landline) connections.\nFIG. 2 is a block diagram showing the gateway in communication with a variety of user devices at a variety of homes.\nFIG. 3 is a block diagram showing the gateway acting as a hub in communication with a number of user devices.\nFIG. 4 is a block diagram showing selected components of a gateway according to the invention.\nFIG. 5 is a workflow drawing showing the proximity detection process according to the invention.\nFIG. 6 is a drawing showing the split screen function according to the invention.\nFIG. 7 is a drawing showing an alternate split screen function according to the invention.\nFIG. 8 is a drawing showing a split screen function according to the invention for use on a mobile device.\nFIGS. 9a and 9b is a drawing showing embodiments of a remote control app according to the invention.\nFIG. 10 is a drawing showing the embodiment of FIG. 9a, detailing the area of functionality.\nFIGS. 11a to 11d are drawings showing different states and features of an embodiment of the remote control app according to the invention\nFIG. 12 shows the data analysis steps used in user characterization.\nFIG. 13 illustrates the filter application applied to a preprocessor.\nFIG. 14 is a block diagram showing the control manager of an embodiment of the gateway according to the invention.\nFIG. 15 shows a high level view of the communication within gateway 150 and the service provider data center.\nFIG. 16 shows the high-level software architecture of the gateway.\nFIG. 17 shows the basic workflow of the Gateway.\nFIG. 18 shows an overview of an embodiment of Gateway services, and the interaction with other applications.\nFIG. 19 shows a block diagram of an embodiment of an rStreamer service.\nFIG. 20 is a flow chart showing the content inspection process.\n<BR><BR>DETAILED DESCRIPTION\n<BR><BR>1.  Introduction\nThe embodiments described herein are directed at creating a network (or \"micro-cloud\") of service nodes, referred to as gateways 150, with each gateway being a configured CPE for servicing a home or other residential unit (such as a dorm room or\napartment), as seen in FIG. 2.  Each gateway acts as one node within a content distribution network (\"CDN\" 15), and is used to forward content to one or more user devices 180 that are registered to use that particular gateway 150.  The CDN 15 formed by\nthe gateways 150 and by the user devices 180 registered to each gateway 150 constitute a local area network.  Gateways 150 are able to communicate with each other throughout the network, either directly, or through server 325.\nAs seen in FIG. 3, gateway 150 may have the base functionality of a conventional set top box and is also the service provider's controlled node that can run applications for users and user devices 180 registered at the premises served by gateway\n150.  For example, as shown in FIG. 3, gateway 150 is connected to TV 180t, mobile phone 180p (which may be a smart phone), laptop 1801 and personal computer 180c.  Gateway 150 may be in communication with a set top box 180s, itself connected to a user\ndevice 180, such as TV 180t, that functions as a slave unit.  Gateway 150, depending on the user device's connectivity (for example, if the user device can connect to the CDN 15 through a wireless connection that is faster than the user's current wired\nconnection, gateway 150 will transmit data through the wireless connection), retrieves content for the user devices 180 and provides usage information to the operators.  In FIG. 3, wired connections are shown in solid lines and wireless in broken lines. \nThis architecture eliminates the need to run costly NOCs, and allows users to access data of many types, such as multimedia messaging, wireless VoIP, streaming video, video telephony, corporate applications, email, and wireless gaming.\n<BR><BR>2.  Gateway Components\nAs shown in FIG. 4, each gateway 150 includes processor 155 that executes software, including a variety of software modules, some of which are described below.  The processor 155 is coupled to memory 156 in the form of both permanent (flash\nmemory or hard disk storage) and volatile stores (random access memory).  Database 165 is typically stored in memory 156.  The operating system of the gateway 150 and the various modules described below are stored in the permanent memory storage such\nthat gateway 150 can be powered on and off without having its software erased.  During execution, parts or all of the software stored in the permanent memory store of the gateway 150 are copied into the volatile store where it is executed by the\nprocessor.  Gateway 150 also includes network communication modules 157, such as Wi-Fi port 157w, a DLNA port 157dl, Bluetooth port 157b, USB port 157u (there may be several USB ports 157u), cable modem port 157c, femtocell port 157f, and Ethernet port\n157e (and may include several Ethernet ports), DSL modem port 157d, speaker 158, and control interface 159 (with an IR interface, and optionally, selectable buttons for use by users), that are in communication with processor 155 and that are used to send\nand receive content.  A WiMax port may also be included.  Gateway 150 also has video and audio inputs and outputs 152, and may have several types of video and audio inputs and outputs (e.g. Svideo, composite, component, HDMI, optical, etc.).  Gateway 150\nalso has a power supply 153, and is typically plugged into a nearby outlet to receive power, and a plurality of TV-tuners 154.\nTV-tuners 154 accept RF band cable or antenna TV analog or digital input.  Preferably four or more TV-tuners 154 are present and they support VOD and EPG (meaning QPSK demodulation is required).  TV-tuners 154 may support satellite TV.\nEthernet port 157e enables a standard interface connection to the home IP LAN for media content sharing between gateway 150 and user devices 180.  Ethernet port 157e operates as a router and preferably at least four (10 M/100 M) ports are\navailable.  The Ethernet ports 157e providing routing, NAT, DHCP and DNS forwarding and support all IP device connections, such as network printers and VoIP phones.\nVideo formats supported by gateway 150 include Svideo, composite, component, HDMI, MPEG-1, MPEG-2, MPEG-4 (H264, DivX, Xvid and Nero Digital), Sorenson (used by fly and quicktime), WMV (Windows Media Video), VC1 (WMV 9 based, Blue Ray, HD-DVD),\nRealVideo, DivX, Xvid, FFmpeg and 3ivx (a different implement of mpeg4 part 2).\nAudio formats supported by gateway 150 should include 7.1 Dolby Digital (A/52, AC3); stereo, DTS Coherent Acoustics (DTS, Digital Theatre System Coherent Acoustics); MP1, MP2, MP3; AAC (MPRG-2 Part 7 and MPEG-4 Part 3) and Linear Pulse Code\nModulation (LPCM, generally only described as PCM)\nWi-Fi port 157w should support 802.11b/802.11g and other features such as security (WEP, WPA, and WPA2).  Gateway 150 will serve as a Wi-Fi access point.  A web interface should be provided to configure the Wi-Fi including set-up, maintenance\nand trouble shooting.\nBluetooth port 157b may support Bluetooth 2.0 and keyboard and mouse features.  Human Interface Device (HID) may also be supported as could be a mobile phone connection or headsets.\nUSB ports 157u enables gateway 150 to host a connection to user devices 180 or peripheral devices.  USB 2.0 may be supported and at least two USB ports 157u should be available.  USB ports can be used to support devices such as digital cameras,\ndigital camcorders, USB Hard drives, USB Flash memory, portable Blue Ray or DVD drives, GPS systems and the like.\nKeyboard and mouse connections may be supported via USB ports or Bluetooth.\nGateway 150 may be provided in a form fitting case (not shown) to fit on a typical TV shelf, and may be sized comparably to other and video components such as a Blu-ray player or amplifier.  The front panel of the case should include a reset\nbutton (optionally reset button may also be, or alternatively be, a software function).  The front panel should also include switches such as On/Off status; Recording/Replay status; Current Channel Number; any pre-set alerts, the current time, and other\ndisplays (as digits or icons) and an IR window for receiving input from remote controls.  The ports and connectors noted previously may be placed on the back panel of the case.\nIn operation gateway 150 will typically have a power input of about 100-240 V AC 50/60 Hz, and a power consumption of about 20 W. Gateway 150 should be operable in temperatures from 32 to 105.degree.  F. (0 to 40.degree.  C.) during normal\noperation.\n<BR><BR>3.  The Software\nThe software within gateway 150 should have the capability of receiving TV input and regular Internet content in all supporting protocol levels, provide regular VoIP services, and support connectivity functions including Ethernet (including when\nthe connectivity is initiated from cellular access), WLAN, DLNA and FemtoCell.\nBasic software services refer to a set of software services that are fundamental building blocks for the rest of the software services.  The basic software services are a set of software engines which would either provide a service and/or the\ninformation necessary for enabling delivery of other applications.  For example, streaming TV content and providing transcoding to a user mobile device is a service, whilst profiling a customer in terms of their interest in content, and usage patterns,\nis a function enabling a personalized advertisement insertion service.  The basic software services are described as software \"engines\", which are described below.\nThe Activation Engine is responsible for activate certain services that gateway 150 is authorized to serve to users.  Such services include: a. TV.fwdarw.Enabled/Disabled Channels and bundles that users are authorized to view b.\nInternet.fwdarw.Enabled/Disabled c. VoIP.fwdarw.Enabled/Disabled d. Network connectivity.fwdarw.Enabled/Disabled (means for a remote reset, configuration and troubleshooting)\nThe activation engine 200 is responsible for registering users, devices and services to which users have subscribed.  The input parameters for activating gateway 150 may be through a set of web interfaces.  This interface should follow the\nexisting operator's activation flow.  A master interface defines the activation fields and account setup.  The activation engine runs on the gateway while the activation field operates and runs by first the customer service and then by user device 180\nafter the activation is enabled, for example for activities such as device registration and/or even activities such as parental control for a user device 180.\nUsers may have more than one gateway 150 per household, for example to take advantage of more hard disk space or TV tuners.  In such a case, the gateways will function in a master-slave arrangement.\nAn Authentication, Authorization and Accounting Engine (AAA-E) is responsible for providing the authentication and authorizations of the gateway 150 registered users based on their subscription.  The authentication process must follow the\nbroadband operator's authorization process.  This typically includes combining device_ID (an identifier associated with a user device), username, password, IMEI number or other information.  The user must be distinguished from the user device 180 that is\nbeing used.  For example, a registered wireless data card may be used by two different users at the gateway 150 site.  While the card is the same, or even the laptop is the same, the users may be different.  Therefore a combination of wireless network\ncard, device id, and username and password is required to identify and distinguish the users and therefore their preference and their subscription to a service.  The accounting engine is also responsible for some of the billing and audit services.  It\nalso keeps track of service usage based on the subscription.  The subscription could be based on any combination; examples include: accessing TV only; Internet only; limited number of user device access; limited access means; Ethernet, WI-Fi, FemtoCell,\nand/or cellular along with other service subscriptions.  Any service provider accounting function may be included in Authentication, Authorization and Accounting Engine 210.\nA User Characteristics Catalog Engine (UCC-E) tracks information about registered users of user devices 180.  Personalization is a process by which the content pushed to one user could completely be different from that pushed to another user. \nIn order to be able to push personalized content to a user, gateway 150 must be able to characterize users' behavior.  Characterization of users could start from user segmentation, which means that the user population is subdivided, into more or less\nhomogeneous, mutually exclusive subsets of users who share common user profile characteristics enabling the possibility of providing them with a more personalized content.  The ultimate objective of the User Characteristics Catalog engine is to be able\nto characterize the user in terms of static and dynamic attributes of the user.  Static attributes are those that do not change rapidly, for example gender, age, marital status and income status.  Dynamic attributes are those which change more rapidly\nand which relate to the sensitivity of each user toward their surrounding social events such as politics, fashion, news and the user's location.  The latter is a characteristic with a substantial impact on users' preference, as for example the content a\nuser is interested in when the user is mobile using a small mobile user device is very different than the content that a user is interested in when they are using a larger screen user device.\nThe UCC-E should work using a definition dictionary which could be updated, extended and modified as needed.  For example, if a user was being characterized based on two parameters, a third parameter could be added or replaced by one or both\nparameters and the characterization should be continued based on the new definition dictionary.  The UCC-E should incorporate an artificial intelligent model whereby it continues learning about users' behavior and usage patterns over time, and therefore\nbe able to produce much more accurate characterization of the user.\nThe User Tracking Engine (UT-E) contains the information about the active user session's user device 180, identity, time, date, activity and duration, context, location, and the network connectivity and the proximity to gateway 150, which is\nprovided by routing measurement functionality in the UT-E to determine the optimum path for data transfer and closest content access.  This function may be performed with a client agent on the user device 180 or other means of measurements using lower\nprotocols to avoid client software on the device.  The UT-E also maintains information about accessed content, including TV channels, PVR use, VoD, internet sites, and even the type of content (e.g. video, text and/or mixed).  The UCC-E uses this\ninformation as a set of parameters in order to characterize the users.  The history of the user is used by the UCC-E. User tracking may be configurable, for example, user device ON/OFF; TV activities only; Internet activities only; or video activities\nonly.\nThe Content Personalization Engine (CP-E) uses the UCC-E and the UT-E. The CP-E uses an automatic intelligent search engine that finds matched content for the user.  The CP-E may be a software engine within gateway 150 or may be an external\nservice engine provided by the service provider.\nAn internal CP-E resides within gateway 150 and interfaces with both the UCC-E and the UT-E. The CP-E automatically searches the web and content producers and distributers on behalf of each registered user of gateway 150.  The CP-E indexes the\ncontent and has the content prepared for the user.\nAn external CP-E may be a large personalization server on the service provider network for content distribution, for example from advertising companies with relationship with the operator.  This external CP-E receives information from the UCC-E\nand UT-E (through common and single interfaces).  The CP-E then matches the content, in this case advertisements, and pushes it to gateway 150.\nIn a combined model, the internal CP-E could be considered as an agent of an external CP-E. In this case the internal CP-E supports the external CP-E interface.  The internal CP-E receives information from the UCC-E and UT-E and communicates\nwith the external CP-E for receiving the matched content.  The external CP-E may locate matching content from the other gateway 150 users in different households that share the same interest and are within the same social community authorized by the\nusers.  The external CP-E could be a global service that interfaces with many content distributers for finding matched content.\nThe decision making rule of the internal CP-E should be based on a configurable set of rules which could be updated and modified remotely.\nAn Intelligent Search Engine (IS-E) is an automatic search engine which fetches the users requested information and searches and compiles the information on behalf of the user.  For example, if a user is traveling to Las Vegas and inputs its\ntrip itinerary to the IS-E, the IS-E will find the location of restaurants, car rental agencies, and places to visit based on the proximity, interest and time line of the user.  The IS-E collaborates with the ICT-E (described below) to transcode and\narrange the content to meet the user's profile.\nA TV Transcoding Engine (TVT-E) is responsible for transcoding a stream to the user based on characteristics of the user collected by the UT-E. The TVT-E is therefore one of the users of the UT-E and also interfaces with the TV receiver.  The\nTVT-E must transcode and rate shape the stream to meet the user's particular user device 180, access network type, speed and proximity to gateway 150 (both in distance and/or number of hubs).  The TVT-E is able to stream the live received TV feed per\nchannel to each user device 180, and both a PVR stream and VoD stream.  The latter may require the appropriate subscriber package.  As an example if a user has purchased a VoD service, the user may only be allowed to watch the service on any of his/her\nuser devices 180 for a period of 24 hrs, or on just on a single device at a time.  If the user may watch a portion of the movie on one user device 180 and another portion on a second user device 180 then the TVT-E tracks the used content and the content\nusage policy and subscription.  A similar process may be used for PVR content.\nFor the purpose of digital right management, there may be client software on mobile phone and computer user devices which remove content proportionally as the user plays the content in case of PVR-d and VoD content which is being viewed.  The\npolicy of DRM is an additional layer over the top of all engines which decides on the sharing mechanism of digitally protected content.  Note that for the DRM purposes it may be useful to have software installed on the client side.\nThe Internet Content Transcoding Engine (ICT-E) is a service provider as is the TVT-E, but the ICT-E transcodes internet traffic.  The ICT-E also uses the UT-E and provides the content transcoding to the user based on their current user device,\nsoftware running on the user device, network type, speed, location and proximity.\nThe TV/Internet Content Mix Engine (TVIC-E) is responsible for mixing Internet based content with TV content.  The TVIC-E fetches and receives the content which needs to be displayed on TV as the user watches the TV.  This engine has a GUI\nengine that communicates with the ICT-E and the TVT-E and the TVIC-E has the task to select the format of the content for display based on the user device and preferences.\nThe Content Sharing Engine (CS-E) is responsible for sharing user content amongst other users.  The CS-E uses the UT-E to determine the current user's profile and if necessary uses the ICT-E for transcoding the content.\nThe Master Security Engine (MS-E) is responsible for security related to gateway 150, including intrusion detection, authenticating users, authenticating any access and transactions with outside sources, and encryption/decryption of streams.\nAs shown in FIG. 5, Proximity Engine (P-E) 320 is responsible for measuring the proximity of a mobile user device 180 (designated as \"mobile client\" 185) to gateway 150.  This includes the number of hubs to the user device.  P-E 320 is also\nresponsible for identifying the best and shortest path to the mobile user device.  P-E 320 will also use the geographic location of the user device which is sent to gateway 150.  Optionally, client software may be installed on the user device to\ncommunicate with P-E 320.\nAs shown in FIG. 5, the user device and user credentials are first authenticated by Global Access Portal server 325.  GAP server 325 is responsible for authentication of any gateway 150 local or remote users, and determines the user's registered\nlocation.  Once authenticated P-E 325 requests the mobile client 185's IP address and type of user device 180.  With that information P-E 320 determines the geographic information, a proximity routing table and a proximity neighbour information table. \nThe proximity neighbor information is a table of information related to a specific gateway 150 and any associated user.  This table determine the closest node (or source of information) to provide the content for the requesting mobile client 185 (or\ngateway 150) with the best possible quality of services.\nThe transcoding of TV/Video content varies depending on the particular user device 180, taking into account of the user devices' physical and performance constraints such as screen size, CPU, memory size, and audio/video codecs.  Furthermore,\nvideo stream content is transrated and rate shaped in light of the user device 180, access network type, speed and proximity to Gateway 150 (how far and/or how many hubs to the user).\nTranscoding is used when gateway 150 reformats content during TV broadcasting, TV Streaming, TV peer to peer, TV tuner sharing to other user devices via Ethernet, Wi-Fi or Bluetooth.  It is also used when content is being transferred from\ngateway 150 to user devices.  For example, if video content is transferred from gateway 150 to mobile phone, gateway 150 first transcodes and then transfers.  The transcoding and transrating process should be invisible to a user.\nGateway 150 provides an interface that allows users to transcode and save their contents in different formats; making it cross-device transferable.  The interface will be called from each user account.  It can be started and stopped at any time\nbased on availability of Gateway 150 resources.  It can convert the content and save it on Gateway 150 hard drive for further transfer.\nGateway 150 also reformats Internet content for the user device, by using content inspection, understanding the HTML content, downloading the content, and converting it to the appropriate format.  Gateway 150 can combine content for the TV\ndisplay and Internet, by fetching and receiving internet content, transcoding the content to the most suitable format for TV display, and mixing up with TV content; and displaying both on TV.  A split screen may be used for multiple purposes e.g.\nwidgets, advertising, menus, keyboard, game, or Internet browsing.\nUser owned content may be shared between user devices.  There are web portal service providers and social networking service providers which allows users to create accounts and share their own contents with other users within their network. \nThis service should support the interface to the operator's selected provider of such service for the content access.  Also user tracking information to determine the current user's access profile should be considered and transcoding completed if\nnecessary.\nGateway 150 allows content sharing and saving amongst registered user devices 180, such as TV displays, laptops, PCs, mobile phone devices and gaming consoles.  Gateway 150 can act as a storage device to save content.  Gateway 150 also allows\nfile transfer between user devices via Ethernet.  WI-Fi or Bluetooth connections.  For example, files may be transferred from a PC to gateway 150 or from gateway 150 to a PC; or to or from a mobile phone to gateway 150, etc. Files may be uploaded or\ndownloaded directly to gateway 150 from the Internet.\nContent files may be transferred between gateway 150 and USB devices.  For example, gateway 150 can be used to store personal digital media contents from digital cameras or iPods (image or MP3 files) onto gateway 150's hard drive.  Gateway 150\ncan then be used to organize the content, for example, by managing images, creating playlists and modifying properties, such as the format, of the saved content.\nGateway 150 may use auto detection with certain user devices.  In this case, the user will be prompted to provide certain parameters such as user account, location and content management parameters, and then the transfer begins.\nAlternatively, the user can specify when files are to be transferred.  In this case, the user selects files to import and is shown a set of icons appears representing different types of content.  The user selects the appropriate icon, is then\nasked to connect the user device.  The user connects the device, sets the parameters for the transfer and the transfer begins.\nA user is able to view, modify and play saved media content on the hard drive of gateway 150.  Options could include creating a slideshow, creating a playlist, managing mages, playing a music library of a user device connected to gateway 150,\nmodifying properties of the content, or organizing the content.\nGateway 150 has several ports, such as USB port 157u, that allows connection of external hard drive and an on-screen interface facilitating the file transfer.  The hard drive can be connected and called by any user.  It can be used at any time\nand is user based.  The interface allows the user to store allowed contents.  The external hard drive may be auto partitioned based on number of users of gateway 150.  Users can also modify properties of the external drive; organize the contents thereof;\nand use applications to create photo albums, music libraries, and video libraries.\nGateway 150 may also allow connection of an external home theatre audio system or an external DVD player.\nGateway 150 also may allow connection of user devices 180 that are digital media sources, such as cameras, camcorders, iPods etc. An on-screen interface should be available to facilitate managing the connected user device 180.  The user can\ntransfer allowed content, play a music library of the connected user device, modify properties of the content on the connected user device, and organize content on the connected user device.\nGateway 150 has an internal Wi-Fi LAN router and port 157w, and provides an on-screen interface for basic and advance configuration.  It can be configured by gateway 150 administrators and can be used at any time.\nGateway 150 has a pre-set interface that allows insertion of account information and instant connection to satellite radio providers such as XM or Sirius Radio.  The user inputs an account setting and gateway 150 connects to radio account\ncenter, allows channel changing and can transfer the signal to other registered and capable user devices 180 connected to gateway 150.\nGateway 150 can display an on-screen full keyboard on a user device.  It can be called by any user and used at any time.  It allows users to imitate typing action and type text commands for gateway 150.  It can be used for searching, and can be\ntoggled between a full keyboard and a phone pad keyboard.\nAdapted versions of the user interface for a PC or a mobile phone can be used through gateway 150 and display on different user devices 180.  The interface can be called by any user and used at any time.\nGateway 150 may be able to allow users to store content on extended online storage provided by the operator.  Such storage access can be connected and called by any user and used at any time.  It requires an Internet connection and can use a FTP\naccount or HTML or Flash GUI.\nGateway 150 user interface engine (UI-E), which partially feeds UCC-E, determines the proper zoning of a screen 600 by type of request, type of content, availability of information associated to the request/content and the eligibility of user\nregarding to view the different level of the information.  CP-E also is involved in this process as is a media object database containing information related to the object, object owner and also the type of request which is initiated by the user.  Based\non this information, UI-E determines how to split screen 600 into several zones.\nAs shown in FIGS. 6, 7 and 8, gateway 150 may displays into several zones to view TV content, Internet browser and/or some widgets on display 600 at the same time.  UI-E can split display 600 into 2, 3, 4 or more zones.  Each zone may be\nassigned to different user.  Each zone may individually access a menu and receive commands.  UI-E can automatically fetch info regarding the in progress content from Internet and preview it in the browser inside the assigned zone and can enable users to\naccess other viewers' feedback and ratings regarding any targeted or in progress content in an assigned zone.\nFIG. 6 shows a TV display 600 split into four zones.  Zone A displays information generated by gateway 150.  Zone B shows a live TV stream.  Zone C shows a keyboard for allowing the user to enter data.  Zone D shows information about the user. \nZone E shows some advertisements to the user provided by gateway 150.\nFIG. 7 shows a TV display 600 split into three zones.  The current content display (which may be television or another source) is displayed as background using the entire display 600.  A second zone displays advertising and information about the\ncurrent content.  A third zone provides a menu and system service information.\nFIG. 8 shows a mobile user device, such as a smart phone, with display 600 split unto several zones.  In this case Zone A displays a video stream, zone B user information, zone C system services menu, zone D, an EPG display and zone E system\nservices within the EPG zone D display.\nGateway 150 may be used for accessing personal messages, including emails, text messages, and allows users to twit and stream access to Twitter accounts.  Similarly, gateway 150 can be used for accessing social networking profiles and allows\nusers to access their preferred social networking accounts and display them on registered user devices 180.\nGateway 150 can also be used for sharing content, as users can share their digital content, such as digital images, videos or recorded TV, with other users, including users of different gateway 150 devices.\nGateway 150 also has several other software services available to users, including email services (serving registered user accounts); virus protection (servicing registered user devices); Internet favorite synchronization; and mobile keyboard\nmapping to TV which allows users to use their mobile devices (iPhone soft key or windows mobile hard key) in same fashion as if they had a full keyboard in front of TV, i.e. the mobile user device is used as the keyboard for accessing Internet on the TV\ndisplay.\nGateway 150 can interact with a mobile user device 180 that uses a remote control app to control a television or other display.  The use of the remote control app 900 allows uses to take advantage of the touch screen functionality of most smart\nphones.\nThe remote control app 900 allows users to use a gesture action (in addition to using the more traditional Left, Right, Up, and Down buttons (known as a \"D-Pad\") 910 to navigate TV menus and screens as shown in FIGS. 9a and 9b.  The display of\nthe smart phone appears similar to that of a conventional remote control.  The input area for a touchscreen display 920 using the remote control app is shown in FIG. 10 which allows for gesture functionality.  Similar to any regular physical remote\ncontrol, the remote control app 900 has control buttons 930 for audio and video content, and in addition user can type on any certain data entry fields 940 on the TV screen or mobile screen.\nFIGS. 11a through 11d show different displays presentable on the smart phone using mobile app 900 for different purposes.  FIG. 11 a shows the D-Pad; FIG. 11b a traditional audio/video control layout; FIG. 11c a numerical keypad; and FIG. 11d a\nkeyboard.  Remote control app 900 can use the native capability of the mobile devices with a touch screen as a data entry device.  The user can use the native keyboard of the mobile device to type text or any other type of data entry purpose (e.g. chat,\nsharing comments or etc.).\nRemote control app 900 can recognize its locations and determine if the user is sitting in front of TV, or is at home close to TV or not.  It will disable when user leaves the home is far from the TV.  This functionality prevents remote user app\n900 from interfering with a user who seats in front of TV and interacts with the TV using its regular remote control.\nGateway 150 can also provide Peer-to-Peer services, and interface with a Webcam to allow users to use the webcam for video capture, monitoring and video chatting and remote control services, and can allow users to use their mobile phone as a\nremote control for gateway 150 through a browser interface.\nGateway 150 includes software to perform a number of maintenance tasks, including: Auto Defragmentation, for periodic automatic disk check and defragmentation; Backup and Resource Center, available online or included with gateway 150 to\nauto-backup vital user settings; Interactive Help Center, which is an interactive online library of resources to help users find their way around features and issues; and Remote Assistance, which allows users to contact a customer care representative via\non-screen interface.\nUser Content Characterization\nThe UCC-E operating in gateway 150 provides the advantage of characterizing users on the edge of the network for all users at single premise.  Users at a single premise often have overlapping interests and that similarity assists in analyzing\neach user more accurately.  This means that more accurate information can be provided to the operators about the users which allow the operators to take ownership of users most valuable information from an advertising perspective; their characteristics,\nwhich would generate more revenue from the advertising industry.\nUser characterization is the analysis of a user's activities on Gateway 150 and determination of a user pattern so as to identify the content that he/she is interested in. It characterizes a user's interest and preference in his/her daily life\nthrough his/her activities using Gateway 150.  Characterization is based on current condition and also historical data.  The characterization model must continue to learn about users' behavior and usage patterns over time and therefore be able to produce\nmore accurate characterization for the users.  Characterization will be done on per user account basis\nThe UCC-E is one of the software engines within the middleware running on gateway 150.  Gateway 150 handles the entire user's traffic flow through whether a user's mobile phone is connected through a cellular access point, or the user is at\nhome, or is on an Internet cloud.  Therefore gateway 150 can characterize a user over time and \"learn\" about each registered user at a premise gradually.  To accomplish this characterization, gateway 150 must be aware of the context of the user.  Context\nincludes any information that can be used to characterize the situation of a user.  As an example the UCC-E may characterize a user based on his/her location, the time, type of device, access network and a typical activities within that context (state). \nThe context (state) of a user defines the type of activities.\nThe UCC-E builds a tree, which describes the user's context.  The CP-E uses these characteristics to find the content, which is most usable and is of interest to the user.  To match the content the CP-E must distinguish the management, updates\nand matching of the content to reusable contents such as news, movie and advertisements.  Both the UCC-E and CP-E are highly configurable in terms of algorithms, thresholds and weights.\nThe UCC-E determines the characteristics based on the current condition, and it also has to take into consideration the history of the user.  Therefore both current condition and history are input to the UCC-E. Some social variables also impact\nthe user characteristics.  For example, a holiday event for two users within the same neighborhood could have different importance and effects on them.  At the same time while someone may normally not be interested in politics, but an event may\ntemporarily change this, for example the 2008 US election for Canadian residents.  This means that the periodic social events, such as holidays, and instant social events, such as election, are also inputs to the UCC-E.\nCharacterizing a user includes defining the content description which users use, such as title, keyword, category, time and location; the user description, such as user preferences and history; the user context description, such as time,\nlocation, activity, device profile, active network access profile; and the user description extension, such as gender, age and activities.\nOver time, the UCC-E can detect a typical user session from an atypical user session.  One way to detect the atypical session is to use Mahalanobis distance statistics in the user session space.  Detecting the outliers (atypical) is important\nand valuable for cleaning the noisy user session history and avoids characterization based on random or false information.  The user history is taken into account since if a session is typical it is happening in a regular and periodic model and an\natypical session is no longer atypical but is a typical session should it have this pattern within a certain time, which is then characterized as such by the UCC-E. The UCC-E should also detect a page request as a user action or a system or automatically\ngenerated web action, which categorizes content description as that which a user has viewed (pushed, or pulled).\nThe UCC-E includes a Characterizer Module which tracks a user's browsing behavior down to individual mouse clicks such that advertisers, through gateway 150, can personalize their advertised content/products.  This module should distinguish and\nanalyze user data and the usage of such data.\nThe UCC-E also includes a Data Analyzer module which distinguishes the types of data that user is accessing.  As an example the data could be categorized in the following categories: Content: This is the real data that the site was designed to\ndeliver and convey to the users.  This type of data is consists (usually) of the text and graphics.  Structure: This is the data that describes the arrangement of the information within the page.  Intra-page structure information includes the arrangement\nof various HTML or XML tags with a given page.  This can be represented as a tree structure, where the \"html\" tag becomes the root of the tree.  Inter-page structure information is hyper-links connecting one page to another.  State: This is the data that\ndefines the frequency at which the data changes, e.g. a breaking news headline vs.  the full story of the headline Usage: This is the data that describes the pattern of usage of content, such as IP addresses, Page references, and Date and time of access\nUser location User device User access network\nSince the data is being analyzed on gateway 150, the performance of the information transfer and content value could be collected at the same time, such as the time which user spend on each page or even viewing the content.  Note that \"D-Box\" in\nthis acts like a proxy since it is the point of contact for all users within the premise.  Higher performance could be realized by ability to predict the future page requests correctly and accurately.\nUCC-E 220 uses a data processor for preprocessing, pattern discovery and pattern analysis.  As shown in FIG. 12, in order to discover patterns, the data retrieved needs to be processed.  Preprocessor 410 converts the content, structure, state\nand usage information into a data abstraction that is used for pattern discovery 415.\nTracked activity for the UCC-E can include the time, the location (home, office, Wi-Fi spot, etc.), the type of user devices used, the network connectivity type (Wi-Fi, Bluetooth, Cellular, etc.), the activities and duration (TV, Internet,\nVideo, content sharing, etc.), and the context, which includes TV content, such as TV channels, PPV, VOD, program category, or Internet, such as website visited and keyword monitoring.  Gateway 150 can detect if a TV user device is on or off and can\ndistinguish multiple users using the same device, for example if several users are watching different portions of a split TV screen.\nUsage preprocessing is used to distinguish user sessions from server sessions.  Therefore first the user is distinguished from: another user that uses the same device; the same user that is using a different browser; and the same user that is\nusing a different device and network.\nThe next step is then to identify the user's usage by obtaining a request that was generated from the user, or automatically generated, as a link within a page.  Table 1 is an example of such usage.\nTABLE-US-00001 TABLE 1 Rec Device Net Method/URL/ Size Action Server Browser # Dev_IP Time Type Type Protocol Statu (e) by IP Type 1 127.0.1.26 [dd/mm/YY: MotoQ 3G \"GET A.html 200 4000 -- 239.150.24.67 Java/3/2(Moto I) T:M:S:ms] M230 HTTP/10\" 2\n127.0.1.26 [dd/mm/YY: MotoQ 3G \"GET B.html 200 3050 A.html 239.0.24.67 Java/3/2(Moto I) T:M:S:ms] M230 HTTP/10\" 3 127.0.1.26 [dd/mm/YY: MotoQ 3G \"GET C.html 200 5020 A.html 239.1.24.66 Java/3/2(Moto I) T:M:S:ms] M230 HTTP/10\" 4 127.0.1.26 [dd/mm/YY:\nMotoQ 3G \"GET D.html 200 3080 -- 239.150.24.67 Java/3/2(Moto I) T:M:S:ms] M230 HTTP/10\"\nContent preprocessing classifies the content to different categories.  The result of the content classifications is used to filter the input or the output to the pattern discovery 415.  The page view could also be used to filter the sessions\nbefore or after pattern discovery.  As an example once a pattern is discovered, then the classified material of the page view could be used to focus the result to a certain subject or class of product.  FIG. 12 illustrates the filter application applied\nto preprocessor 410.\nThe preprocessor 410 thus classifies page views.  The page views could be classified based on topics and/or intended use, such as blogs, news, social networking, academia, corporate, personal, shopping, non-profit and others.  Page views convey\nthis information through text, graphic, and multimedia.  The information on the page views must first be converted into a quantifiable format.  Some version of a vector space model could be utilized to accomplish this.  Keywords or text descriptions can\nbe substituted for graphics or multimedia.\nFor the static page views, HTML/XML is parsed and reformatted based on an algorithm to break page content to categories suitable for preprocessing.  When breaking down dynamic page views, first the dynamic sections are distinguished from the\nstatic sections.  If the page is highly dynamic then it is broken down into sections based on the state of dynamic sections, e.g. a section could by highly dynamic while another one is semi-dynamic.  An example of such content is when viewing a news\ndescription page the reading pane is static while the area around it is dynamically changing.  Within the dynamic section some sections are highly dynamic and are automatically changing frequently while some sections are changing only if the page is\nrefreshed by the reader.  In fact these sections changing on refresh are the sections that are relying on user characteristics in order to push the personalized information.  A given set of server sessions may only access a fraction of the page views\npossible for a large dynamic site.  Also the content may be revised on a regular basis.  The content of each page view to be preprocessed should be assembled, either by an HTTP request from a crawler, or a combination of template, script, and database\naccess.  If only the portions of page views that are accessed are preprocessed, the output of any classification or clustering algorithms may be skewed.\nThe structure of a site is created by the hypertext links between page views.  The structure of a site and its preprocessing could follow the same model as content preprocessing described above.  Once again there should be a different model on\ndynamic and static pages.\nThere are main parts in pattern discovery.  The first part is based on the algorithms and methods used for the discovery.  These algorithms could be based on a single or a combination of statistical analytics, pattern recognition, data mining,\nand machine learning.\nStatistical techniques are used to gather information about the user visiting a site.  This includes the frequency at which the user visits the site, the mean value for the time the user spends on the site, the average length of navigation path\n(from where user follow the content linked from one to another page) and others.  These tools include statistical information such as the most frequently accessed pages, average view time of a page or average length of a path through other links or\nidentifying invalid URLs.\nAssociation rule generation can be used to relate pages that are most often referenced together in a single server session.  For example, if a user first visits a sporting equipment site and later visits an electronic equipment site then the\nassociation rule may reveal that the user should be interested in electronic sporting equipment.\nRelation of the pages that user browses may reveal the user's interest in content which assist in clustering and associating and grouping the content based on user interest.  Based on this information, dynamic links could be created to request\non behalf of the user, content based on the user interest.\nAs discussed previously, once the content is discovered and classified in different categories, the result can be used to categorize the user.  The classifier module will learn over time how to classify the user using AI algorithms such as\ndecision tree classifiers, naive Bayesian classifiers, k-nearest neighbor classifiers, Support Vector Machine or others.\nThe last step is Pattern analysis, which filters the irrelevant rules or patterns that was discovered through the discovery phase by using the raw content as above figure shows.  The filters and rules could also be through configuration\nparameters or rule and/or policy engine.  The most common form of pattern analysis consists of a knowledge query mechanism such as SQL.  Another method is to load usage data into a data cube in order to perform OLAP operations.  The content and structure\ninformation can be used to filter out patterns containing pages of a certain usage type, content type, or pages that match a certain hyperlink structure.\nGateway Features\nGateway 150 has the capability of receiving TV from IP, Cable and Satellite and is integrated with DSL modem capabilities, Wi-Fi and optional wireless 3G network connectivity.  The integration of TV, DSL and Wi-Fi reduces the customer premise\ninstallation time and prevents the need for multiple trips to the customer premise for different installations.\nThe middleware of gateway 150 is based on a component software architecture that provides a series of gateway functionalities, additional to standard STB and Modem features, and allows gateway 150 to function as an ASG (Application Serving\nGateway).  With this solution, users of gateway 150 become subscribers of the ASG.  User devices 180; such as TVs, mobile phones, laptops and PCs will all be registered to the ASG through an online portal during the gateway 150 activation process in a\nvery simple process.  To the end user, gateway 150 appears as an integrated Set Top Box that reduces their need for extra equipment and wiring requirements while delivering additional features and benefits that are part of the service differentiation\nofferings.\nThe system does not require installation of a client application on any of the user devices 180 (although in an alternative embodiment it may use such a client application).  All traffic is handled by Gateway 150 through an AI (Artificial\nIntelligent) engine that can distinguish and manage each user's traffic, under varying connectivity situations (device, network type, location, etc.).  The approach distributes the CPU and memory requirements for millions of users out to the subscribers'\npremises, similar to a subnet, enabling faster and more accurate data processing.\nGateway 150 is able to receive live TV broadcast from (cable and IPTV) service provider including free channels and encrypted channels based on subscriber package.  Gateway 150 understands the encryption algorithm used by service providers for\nsuch encrypted channels.  Gateway 150 can also support PPV/VOD services from the service provider and can follow the subscription package to support the services and can differentiate PPV/VOD from a regular TV broadcast.\nGateway 150 can provide TV streaming to other user devices e.g. laptops, mobile phones.  Gateway 150 is able to stream a live received TV feed per channel based on permission level to other users.  Gateway 150 can transcode TV streams\ndifferently and automatically based on the user device receiving the stream, taking into account the user devices' physical and performance constraints such as screen size, CPU, memory size, and supported audio/video codecs.  Gateway 150 has a\ntransrating dynamic speed control to suit the network bandwidth during streaming.\nGateway 150 allows user watching a TV stream to resume playing from a previous stop point while streaming to user devices.  A user can thus pause, play and go in reverse for a live TV stream.  For mobile phones and similar user devices, a hot\nkey can be implemented.\nUsers can view live TV program and recorded TV/video contents on a TV display simultaneously.  Gateway 150 supports pause, replay, rewind, and fast-forward functions for both live TV programs and recorded TV/Video content while watching on TV or\non PC or mobile or handheld user device.\nA user can play recorded content on a user device while available tuners are allocated for recording.  Gateway 150 should have a buffer, for example a 60 minute buffer for time shifting (recording and playback) a TV program.  This buffer should\nalways contain the past 60 minutes of live TV content being watched.  This buffer can be emptied when gateway 150 is turned off.\nA user can also switch from a watched program from live TV (with time shifting buffering in progress), to a recorded TV or video content while the time shifting buffering continues, or to programs being recorded.\nFor TV streams using PPV and VOD services, the availability of the content is subject on the subscriber package.  For example, if a user has purchased a VOD, the user may be allowed to watch the stream on any of his/her user devices for a period\nof 24 hrs, or perhaps just on a single device at a time.  The user may also be able to watch a portion of the content on a first user device and another portion on another user device.  If this is the case, Gateway 150 must follow the user's device and\ntrack the used content and the content usage policy and subscription.  Therefore gateway 150 is able to inspect the content and differentiate PPV/VOD streams from regular TV streams, allow conditional access and obtain information about the subscriber\npackage from the service provider.\nA users' service subscription is resident on servers controlled and managed by the service provider, Gateway 150 can communicate with such servers and obtain information about the user's subscription to determine the services for Gateway 150\nusers.\nUsers can also use gateway 150 to watch TV streams wherever they are located with a registered user device, including PPV and VOD TV streams.  Thus the local receiver capacity (number of TV tuners) is no longer the limitation in receiving live\nTV contents.  Gateway 150 is required to share the live TV/PPV/VOD contents with Gateway 150 users and the process is managed by service provider.  A user may be able to watch TV/PPV/VOD contents through other Gateway 150s in any geographic location. \nFor example, assume User A only two TV tuners in his Gateway 150.  Both tuners are already engaged in recording activities and user A would like to watch a live hockey game on channel X. User A subscribes this live hockey game program thru a PPV service. Service provider will search all Gateway 150 users and locate the closest Gateway 150 user who is watching the live hockey game on channel X, User B. The service provider will have user B's Gateway 150 share the content with user A's Gateway 150.  In the\ncase where user B switches to different channel, the service provider will locate another Gateway 150 to share contents with user A.\nIf gateway 150 has two tuners (gateway 150 may have more than two), then users could record TV programs through local TV tuners, and record a maximum of 2 TV programs including live TV/PPV/VOD at the same time while watching only one of the two\nprograms in a given time.  The users could play recorded content while both tuners are allocated to record by using Recording and Playback at the same time.  Users could record one program and navigate through any other in-progress TV program.  They\ncould switch a watched program from live TV, to pre-recorded programs or to the programs being recorded.\nThe user has two ways to start recording a TV stream, \"Quick\" or \"Programmed\".  For the Quick method, on any in progress content, the user presses Remote Control's Record Button or on any listed content in the EPG, the user highlights the\nprogram and presses the Remote Control's Record Button.  Gateway 150 then starts recording the stream or for a future program sets the program in schedule to be recorded.  When programming gateway 150 to record, the user selects record from an onscreen\nmenu, and then follows the onscreen instructions including the option of selecting a program from the EPG.  Recording options include time, extended time, auto deletion setting (per recording), user restriction for viewing, format/codec for saving\nrecording, and whether it is a recurring recording.\nThe user is able cancel a recording before the recording start or during recording.  In the case of a schedule conflict of recording, priority will be given to user with higher priority set by admin, and for users with the same priority setting;\nthe first to set the recording has priority.  Gateway 150 has an LED or message to indicate a show recording start or in progress.  A warning will be provided if a user would like to switch channels during recording when both tuners are engaged.  In this\ncase, the recording should be stopped first before the channel switch is permitted.  Gateway 150 maintains a personal library of recorded programming, accessed through content grid guide.  Users can access right protection on recorded content, e.g. user\nA can normally view/play user B's recorded contents unless User B restricts it.  Users can also record programs using a gateway 150 to which they are not registered.  The service provider can have a second gateway 150 record a TV stream and send it to\nthe first gateway 150.\nGateway 150 allows the recording of digital content from sources such as digital camcorders to be saved in original HD or standard formats on its hard drive.  These viewing interfaces will be called from each user account.  The recording and\nviewing of such files can be started and stopped at any time based on availability of Gateway 150 resources.\nThe user interface could be a direct interface with gateway 150 or use an Internet browser.  In the case of a direct interface, the user can connect a digital camcorder to gateway 150, an interface appears indicating that a device is connected\nto the gateway 150, the user navigates through and sets parameters and starts recording and gateway 150 software automatically converts the content to a format suitable for replay on gateway 150 if needed.  If using an Internet Browser, the user calls\nthe gateway 150 interface and navigates to an \"Other Recording\" feature, an interface opens asking to connect the camcorder, the user connects the digital camcorder, PC, or Laptop and selects to start recording on interface, and the gateway 150 starts\ntranscoding and transferring the content.  Users should be prompted if hard drive is becoming full.\nGateway 150 can perform video playback, and can play recorded content at the same time both tuners are allocated to record other streams.  Any user can replay saved video contents, subject to permissions regarding the content.  The playback can\nbe started and stopped at any time.\nThe video playback option allows users to access a saved contents list; browse the list and sort the list based on: original recording time; category of content; time of recording and channel.  The user can replay from the last second watched or\nreplay partially watched saved video contents from the moment stopped.\nThe replay can be tagged at certain points by a user.  The replay can be started directly from such tagged points.  The user can access the saved tagged points, browse the points and preview screenshots.  Gateway 150 also allows volume balance\nfor each channel.  The sound volume may be automatically adjusted based on pre-set specifications which may be based on the particular user separately.  Gateway 150 can detect the audio quality and adjust the volume accordingly.\nGateway 150 also allows video streaming to user devices other than a TV, such as laptops, pads and smartphones.  The user of the user device must have permission to view the content.  The video stream may need to be transcoded by gateway 150, in\na manner invisible to the user, based on the user device, taking into account the user devices' physical and performance constraints, such as screen size, CPU, memory size, and supported audio/video codecs.  Gateway 150 transrates the content using a\ndynamic speed control to suit the network bandwidth during streaming.  The user is able to resume playing from a previous stop point.\nGateway 150 saves the data of a TV stream in a cache in its original HD or standard format on hard drive.  It can be accessed by a user at any time.  The user can tag any point of the stream for future reference.  The tagged point will\nautomatically be saved and given a name, and a user may later modify the tags name, start and end point.\nGateway 150 includes multiple interfaces that allow saving digital contents from auxiliary user devices, such as digital camera or iPod (image or MP3 files) in a specific format suitable for replay on gateway 150.  These interfaces will be\ncalled from each user account.  They can be started and stopped at any time based on availability of gateway 150 resources.  They allow users to record content and saving it on the gateway 150 hard drive, create a slideshow or playlist and manage images. Users may also delete saved content from the gateway 150 hard drive dependent on their permission to access the content.\nVideo playback on gateway 150 can be controlled using a TV remote, including adjustments for channel and volume.  Gateway 150 will also be provided with a remote control.\nGateway 150 will provide users with a \"content grid guide\" which is an interface for previewing a TV guide (through the EPG) and the stored content, such as music, video files, and recordings on gateway 150.  The content grid guide will serve as\na TV preview using the EPG provided by the service provider.  It may have various display modes including full screen, quarter, double, and a miniaturized version, such as a strip at the bottom of the screen.  It may be accessed by different user devices\nincluding, computers, TVs, smart phones and pads.  The user may scroll through in a variety of ways, including a fast scroll option (e.g. by day or page).  The user may change preferences, such as color and font, and permissions using parental control\nfeatures.\nThe content grid guide allows users to sort channels by certain criteria, such as channel number, program category and time.  Users can access record and search (by name or category) functions.  Users can also access information about a program\n(or for any file or content being played).  Such information may be available from the EPG, and may also include recording status, remainder (time left in program or file), the rating and an image representing the content.\nGateway 150 may also be used to exert parental control over the ability of children to watch certain content, or to limit the time that they may view certain user devices (for example TV viewing hours may be limited from 7 pm to 9 pm), or the\nmaximum amount of use per day.  Users access a parental control interface and can select permissions based on parameters such as ratings, channels, programs or combinations of these.  Likewise, controls can be established based on time per week or\ncertain time periods.  The parameters can also be based on particular user devices.\nGateway 150 is able to connect to the Internet through Ethernet or Wi-Fi.  User devices can connect to the Internet through Gateway 150 via Wi-Fi.  Gateway 150 thus functions as a web server to provide content (such as streaming videos).\nGateway 150 also allows files to be transferred between registered user devices.  Gateway 150 can receive a file through the Internet, a user device, or another source, such as a USB device.  Once on gateway 150, the content can be transferred,\nused to create a playlist, modified and organized.\nGateway 150 can auto detect devices connected to it, and allow the user to set parameters such as user account, location and content management parameters.  Alternatively, the user can select to import files, and respond to prompts regarding\nconnecting the device, and setting the parameters.  Gateway 150 allows user to access saved content through user devices or directly through gateway 150 interface.\nGateway 150 can also be used for VoIP, either directly or through a user device.  One use of gateway 150 is to allow VoIP to be used though a user device such as a smart phone, through the Internet access via gateway 150 or a PC.\nMobile devices can connect to gateway 150 when they are distant by using a web interface.  Gateway 150 provides the appropriate transcoding and transrating.\nGateway 150 provides several basic services.  For example, gateway 150 is configured to assist users through activation.  Activation may proceed through a web interface, and should follow the service provider's activation flow.  Customer service\nrepresentatives from service provider may have an admin ability to overwrite configurations completed by users.  Each gateway 150 may have an associated \"family\" account with all administrative privileges.\nEach user should create their own account (which may or may not be password protected) using the \"family\" account.  The service provider may set a limit on the number of accounts.  Each user account may be given different privileges, for example\nwhether the account has admin privileges, password change rights, name change rights, or theme change rights.  Each user may have their own profile, e.g. color, desktop, theme, font, font size.  Typically, only an admin account will be able to do device\nregistration.  The \"family\" account can also be used to reset passwords, such as forgotten passwords, possibly including the \"family\" account password.  Restrictions can be placed on accounts using the parental control options.\nUser devices 180 are also registered with gateway 150.  Multiple user devices 180 may be assigned to a user account, and likewise, multiple user accounts may be associated with the same user device 180.  Gateway 150 provides an interface to\nallow a user to connect a user device 180 based on the capabilities and parameters of the device.  The service provider may wish to limit the type or number of user devices 180 that can connect to gateway 150.  If a device 180 is connecting to gateway\n150 via a 3G network, gateway 150 is able to identify the device through the device's EG cookie.\nGateway 150 should authenticate and authorize users and their user devices 180 when connecting to gateway 150.  The user's service subscription must be used to determine the privileges of the user.  The gateway 150 thus uses the user account and\npassword, the user device 180, and the user's subscription.\nGateway 150 performs a TV and video content transcoding role.  The transcoding is done automatically, taking into account the content, the user device 180, the bit rate and location of the user device 180, and other constraints such as memory\navailable, processing power, screen size and audio and video codec available.  The bit rate and transrating shape the video stream.  Transcoding occurs whenever video content is shared or transmitted to user devices 180 via the Ethernet or Bluetooth\nports.  Gateway 150 includes a user interface whereby a user can transcode and save content in different formats; making the content cross-device transferable.  This interface will be called from each user account and can be started and stopped at any\ntime based on availability of Gateway 150 resources.  Once converted the content can be saved on a user device 180 storage or on Gateway 150's hard drive.  The interface generates a list of compatible registered user devices 180 based on the content\nformat selected.\nGateway 150 can thus reformat content, such as that found on the Internet, for different smart phones.  Gateway 150 does this by inspecting the content, downloading it, and if necessary for the user device, converting it to a different format. \nIn an embodiment of gateway 150, when a user downloads content, for example, a video, that the user device cannot play, the user is prompted with a note the video is unplayable on that device, and asking the user if they would like to convert. \nAlternatively, any content reformatting could be done seamlessly, invisible to the user.\nGateway 150 can display Internet content on a TV display and TV content on an Internet display, like a smart phone or laptop screen.  In doing so, gateway 150 receives the content, transcodes it into the most suitable format for the receiving\ndisplay.  In such a manner, gateway 150 can provide a split screen TV display including both TV video and other content.  The other content can include widgets, advertising, a menu, a keyboard, a game or Internet browsing.  A video call, audio call, or\ntext chat interface may be included.  Gateway 150 should support popular chat functions, such as MSN, Google Talk and Skype Chat.  The display options may be limited and determined by the service provider, or by the user.  Gateway 150 provides an\ninterface whereby a user may split the TV display screen, for example by half, quarters (with each quarter displaying different content), or other combinations.\nA user can use the user device 180 displaying the TV content, for example a laptop, iPad or smartphone, to control the TV, for example change channels, pause the program, etc.\nGateway 150 may be configured to have interfaces with popular social networking or file sharing sites, such as Facebook and LinkedIn.  Transcoding may be used to make these sites usable on user devices such as TV displays.  User particulars,\nsuch as user names and passwords may be stored on Gateway 150.\nGateway 150 should be able to assist content provider in digital rights management.  In an embodiment of the invention in which client software is on the user device 180, the user device 180, such as a smart phone can, for example, remove\ncontent as it is played in the case of PVRed or VOD content.  The policy of DRM is an additional layer over the top of all engines which decides on the sharing mechanism of digitally protected content.\nGateway 150 should offer several security features for box access, communication of content amongst users, user access to data, intrusion detection, authenticating users and user devices and encryption/decryption of video streams.\nGateway 150 includes a graphical user interface (GUI) to interact with users and perform certain functions.  Users may use the GUI to record TV content.  The GUI allows scheduling future recording of TV content as single program or a TV series. \nIt can be set by any user can be called at any time.  A user may use the GUI to access a TV listing, and modify record settings such as start time, stop time, new or repeating episodes on different channels, time of broadcasting, and deletion conditions.\nThe GUI can be used to review the scheduled recording list.  Users can access the schedule and make modifications.  The list can be sorted based on user, date and time, category of program and channel.  The GUI can also be used to access and\ninput items into a calendar with a reminder system (which may be sync to calendar systems on user's smart phones or PCs).  Users can use the GUI to review the scheduled reminder list, access the schedule, and modify, browse and sort the schedule based on\nuser, date and time, category and channel.\nThe GUI can also be used to search content stored on Gateway 150 or user devices.  Such a search can be done using different filters.  An on screen keyboard may be used for this purpose.  The results of the search may be sorted by user, type of\ncontent, category of content, channel, recorded or future status, PPV or source.  Trailers or previews of the content may also be provided.\nEach user may have certain favourite channels associated, which can be based on particular channels or categories of channels.  This can be sued to assign characteristics to the user.  The channels may be sorted by user, type of content,\ncategory, channel, PPV status, on demand or downloadable status.\nRemote Management\nGateway 150 may be managed remotely by the service provider.  Such management may include monitoring and diagnostic functions, trouble management, performance management, and configuration and customer support.\nThe remote monitoring and diagnostics module is responsible for monitoring the health of gateway 150.  It includes low level debugging tools that monitor errors and warnings reported by the various modules and the resource usage of each module\naccording to its upper bound limit as set by configuration.  The module operates in active mode, in which it automatically reports on errors; or a request/response mode, in which it responds to command generated by a remote technical support team.  The\nrequest/response could be a request for gateway 150 to run a test script.  The configuration file for this module could be updated remotely, as could the module and its parameters.\nThe remote trouble management module can provide full control of gateway 150 to the service provider's technical support team.  The module enables technical support to control all levels, allowing the support team to run procedures, look up\nregistries, communicate directly with any hardware and software components, measure throughput, signal quality, etc., check the communication between hardware components and software, isolate the cause of problems, apply updates and software patches, and\nenable or disable components.\nThe remote performance management module tests the performance of the connectivity to gateway 150 and within gateway 150, which includes timing: per transport layer protocol; per application layer protocol; per transaction; per session and per\nnumber of concurrent sessions and transactions.  The module also tests packet loss, throughput, error rate and delay, and connectivity to user devices registered to Gateway 150.\nThe remote customer support module provides high level customer support, including allowing users to contact customer support.  It may include demos and some remote management functions, such as activation, authorization, setup, configuration,\nand software updates.\nGateway 150 can support multiplexing streaming video to add layers and images to video content.  For example gateway 150 can add logos to TV content.  Gateway 150 also supports closed captioning, including the language, font size, and colours\nused with captions.\nApplication Manager\nGateway 150 has an application manager to manage the state transition among the applications operating within gateway 150.  The application manager processes a user's request and delivers the request quickly to a desired application service or\nmodule.  The application manager receives user request from a native key event manager, and dispatches the request to application services.  The application manager resides between application modules and middleware inside Gateway 150 system.\nThe application manager, when the power is on, leads the system into a \"main\" service in the factory setting mode of gateway 150.  The main service waits the user's choice, and notifies the application manager to launch the selected application\nservice.  The application manager, when the power begins as \"off\", leads the system to enter the previous application service which is \"remembered\" in the last power off state.  The application manager, when the power is turned \"off\", notifies the\napplication service shut down and brings the time update automatically in front panel.\nArchitecture\nA representative architecture for gateway 150 is now described.  Gateway 150, as previously described includes hardware, middleware and services.  The software in gateway 150 is based on modules and engines characterized by their main\nfunctionalities, and therefore the modules in each group or category have the same basic functionalities and can be distinguished from each other by specific functions.  Modules are grouped into the following categories:\nEssentials: The main container will not be loaded without the essential modules.\nActors: The service engines that need to be running and providing an automatic series of functionalities.\nReactors: The modules that act according to reflexes (invoked and/or behave differently as per request and types of others).\nAn important aspect of the design is that all modules support protocol communication interface through a message structure.  This isolates the dependency of the modules from each other to the extent that each module could even be developed in a\ndifferent language and each module is a sub system of a larger system.  The output of one module could be the input of another module and the output of the system is the result of the collaborations among all modules.\nThe Main Container is a small module that reads a configuration file and then, based on the information presented in the file, loads the modules.  The path to the configuration file is an input parameter to the main container.  The path could be\nan address to a file and/or a database which has either resides locally on gateway 150 and/or remotely within a data centre.  Therefore the Main Container must have the communication capability of connecting to a database or file, and receive command\nlines as input for mandatory functionalities and as well as the ability to read a path for loading modules.\nActors are the modules that actively and automatically act on a task.  Actors will load and run in the background at all the time.  For example, a watcher is an actor the loads at runtime and is invoked periodically to check the status and\nhealth of the system.  Actors have the same behavior without any dependency in change of state.  For example, a deep content inspection module is always scanning the content of a stream no matter what type of network, device or content is involved.\nReactors are the modules that change their behaviour based on the state change.  Reactors can be considered as intelligent modules where the module learns through the time and behaves differently based on the state.  For example, the\nPersonalization Engine (PE) module could be considered as a Reactor.  The PE Module learns from the user's behavior under different conditions and based on the collected knowledge, provides the service differently i.e. looking for different content to\npresent and push to the user based on knowing the network, device, and state the user is.  The collection of the affirmative information forms a state that causes the PE Module to move or behave differently and produce a different result.  The video rate\nshaping module is another example of such a reactor, wherein summation of users' state information, including type of device, connectivity type and quality, state in terms of mobile or stationary, at home or distant location, and type of content and the\nviewing capabilities under the state condition, would result in a rate shaping algorithm which could be completely different under different conditions (for example when only one parameter is different).  The design of reactors is based on separation of\nlogical layer from data layer.\nActor or Reactor defines the main framework of each module, whether the framework is an algorithm that changes based on input event and parameters, or is a static framework.  The modules also differ on basic functionalities for each module of\nthe defined type.  Therefore, the modules can be further grouped into the following categories:\n1.  Content Inspectors, including all client portals: the entry point of the client user device, such as game consoles, OS-based mobile devices, non OS mobile devices, laptops, TVs, remote controls.  This also includes deep content inspection\nmodules for both textual content and non-textual content (graphic and video).\n2.  Content Consumers: are the modules that process and analyze the content/data collected and consume it for other purpose and tasks.  These include the characterization module which receives the data collected from the inspectors and analyzes\nit in order to characterize the user.  Note that this module is of Reactor framework type.  It also includes the personalization module wherein different menu options will be loaded based on user preference, context (state) and the output from\ncharacterization module.  This module could also be the module that draws the page on the display based on user characterization output.  For example while one user may like to have the screen layout and information organization different from one user\nto another, or even from one user's device (e.g. Mobile) to another (e.g. laptop).  All storage, backup modules are also consumers; they are also of type Reactors, e.g. based on type of content, and user's status where to backup and how often to backup\nvary from one condition to another.\n3.  Content Producers: are the modules that produce the content for the user.  These include the intelligent search engine which searches for content on behalf of the user based on the output of characterization module.  Note that this module is\nof type Actor since its behavior/task (searching) doesn't change while the input (type of content to search for) is changing.  It also includes the multiplexer which is also a content producer, since it receives, for example, a video (TV stream) and the\nadvertisement and produces a new mixed content.  Note that this architecture allow gateway 150 to have a dedicated multiplexer for each content format; e.g. 3GPP or MPEG4, etc. which makes it of type Actor, or alternatively one multiplexer handles all\ntypes and changes its algorithm based on the request which makes it of type Reactor.\n4.  Data Communicators: are modules that are responsible for networking related activities and include determining the fastest/shortest path to available content; the fastest/shortest path to the user device; video rate shaping and others.\nAs shown in FIG. 14, all user devices, when accessing gateway 150, pass through an access portal 1400, which act as a proxy or gateway.  These access portals pass the identity of the device (and user if it is shared) to the AAA module 1410,\ncollect user data to send to the data collection module, access the resource through the resource management modules.  Examples of these access portals 1400 include the Web Portal 1420, the WAP Portal 1430, the remote control portal 1440, the SMS portal\n1450, and the game console portal 1460.\nThe Remote control portal 1440 has two modes, basic and advanced.  The basic mode allows turning on gateway 150 with a remote control and ignores the password notification.  A default account is selected for the basic mode for the AAA module\n1410.  In the advanced mode, the user inputs 2-4 digit number as a password when prompted, allowing the gateway 150 to identify the user.  Alternatively, gateway 150 waits until the user selects a certain option, for example favourite channels or web\nsites, at which point it prompts the user to provide the password.\nThe Access Control layer includes the AAA modules in charge of user activation, identification and authorization.  Each user device needs activation through AAA module 1410 to start to use the gateway 150, and during this process, the AAA module\n1410 will record (and update if possible) the configuration of the user device.  For shared devices, the module will activate the user account.\nAfter activation, each time a user device or a user account starts using gateway 150 via an access portal 1400, the portal will pass the user device (or user account) to the AAA module 1410 for authorization.  The AAA module 1410 will identify\neach user device and user, and through a content filter system, search the local content container for suitable content, and pass it to content blender 1470.\nThus the access control layer 1400 is responsible for: keeping all user profiles and box defaults and updated data centrally; communicating with the data center database to get, update, backup, and restore user device data; providing\nauthentication interface for the Portal layer; providing user profile interface for other modules; and providing an interface for User UI for any parameter that can be updated by the user, such as parental control; providing interface to Configuration\nand Manage net program; and providing interface to data center to activate gateway 150.\nThe Data Inspection/Collection modules 1480 act as the interface between data collection points (the portals 1400 and the AAA modules 1410) and Data storage (files and local database).\nThe content blender module 1470 has two major functions, to blend the video content for TV streams, recorded TV and other resources, and to transcode Internet content (typically web pages) for adapting to user device screen size and webpage\nreformatting to add more content.\nAn example of the work flow process from a user device connecting to the Internet via gateway 150 and is communication with gateway 150 via a Wi-Fi or LAN interface follows.  First, the gateway 150 WEP portal 1430 intercepts the Internet\nrequest, abstracts the user device's IP/MAC information, and interacts with the AAA module 1410 for authorization.  The AAA modules 1410 passes the user device ID (and account ID if available) to the content blender module 1470, and checks if available\n(matched) content is in local Add-on container 1490.\nIf the content blender 1470 finds the matched add-on in Add-on container 1490, it links the add-on for later blundering usage when it receives the feedback from interface agents.  The data inspection/collection module 1480 starts to collect the\nuser data along with the device ID (and account ID) passed by the AAA module 1470.  The content blender module 1470 transcodes the feedback from the resource/request with the add-on together, sends it back to portal module.  The data collection module\nalso gathers the information from feedback data.\nThe SMS portal 1450 may be a set of pre-defined SMS commands, and also uses the AAA module 1410 to authenticate the sender authority, and to decide to allow or deny the commands send from the user device.  After a command passes the AAA\nverification, the control and management module 1500 parses the command and processes it.  Depending on the command type, the control/management module 1500 may determine if it will send the feedback/confirm back to SMS sending user device.\nFIG. 15 shows an embodiment of a high level explanation about data communication inside gateway 150 ND between Gateway 150 and the service provider Data Center 1510.\nSoftware Architecture\nAn alternative embodiment of the software architecture for gateway 150 is now disclosed.  The Gateway software architecture includes the following components: Application layer; Access control layer; Services Layer; Hardware abstraction layer;\nManagement interfaces; and Database interface.\nFIG. 16 shows an embodiment of the high-level software architecture of Gateway 150.  As shown, function modules of Gateway 150 include transcoding 1610, multiplexing 1620, proximity based content inspection 1630, presentation and streaming\nmodules 1640.\nAn embodiment of the basic workflow of gateway 150 is shown in FIG. 17.  As shown, the first step taken is user authentication through permission control 1710, and the following depends on the use cases.  The user can call an application to send\nout a request to certain services, which includes answering \"Who I am\" (user and device identity), \"Where I am\" (geographic location) and \"What I want\" (service information).  The access control activates a portal guard and queries the database for\naccount information.  If the request is allowed, the request is passed on to the related service.  Once the service processes the request, it also logs the activity and the result to the database.\nThe application layer 1720 includes application catalogued into four different groups (based on the access portal used).  The first group are web-based applications, which refer to the Webpages which run on top of the Gateway 150 web server. \nAny web enabled user device could open these Webpages, and thereby access the Gateway 150.  The webpages are of two types; management pages which are used to configure gateway 150 and users thereof; and application webpages with provide web access for\nfunctions such as live streaming TV, EPG information, recording programs, and personal content management, such as image and music files.\nThe second group are local applications, which include management applications, which as described above, can be used to configure gateway 150 or the users thereof, and function applications, such as media players, media guides, and photo\nalbums.  The GUI interface with users is dedicated to providing use of such modules.\nThe third group are carrier applications, which is the group of services, web pages and applications which favour the service provider requirements.  Gateway 150's carrier applications provide control and information collection interfaces for\nthe service provider.  Control interfaces in this group include gateway 150 activation; gateway 150 configuration; services configuration; and advertisement content pushing.  Information collection interfaces in this group include gateway 150 activity\nreporting; user characterization reporting; and content inspection reporting.  These reports include proximity information.\nThe fourth group is third party applications which are the applications not developed and controlled by the gateway service provider or manufacturer but follow the same interface as above application.  These applications could be developed by\nany third party company or developer following the Gateway 150 SDK.  Examples include Skype, and Google Widgets.\nThe access control layer acts as a portal for all the service requests.  This layer defines a service framework and is implemented by two libraries, libdsService and libdsLog.  libdsService defines the service framework.  The framework\nimplements a skeleton of CLI and the access control interface.  libdsLog defines a post instrumentation interface for all the services.\nThe Service layer 1730 includes all the core services Gateway 150 provides.  Each service is a slave of the control manager.  In other words, the service can only be accessed by an application and by other services through the control manager\nusing the access control interface.  The reason behind this approach is to hide and centralize all the complex service-interaction logic in the control manager.\nFor example, an application could dispatch a request to the control manager to start recording a live TV channel.  Behind the scenes, the control manager would communicates with the user manager service to see if the application has the needed\npermissions, and then setup the recording session with the recording service.  As soon as the schedule arrives, the recording service could inform the control manager.  The control manager then can inform the replication service to start delivering live\nstreaming to the recording service.  FIG. 18 shows an overview of gateway 150 services, and the interaction with other applications.\nThe gateway 150 Control Manager 1800 exposes a list of important Gateway 150 functions, and delegates to one or more Gateway 150 services to complete a request.  It also serializes and prioritizes requests, and provides event subscription\nmechanisms to other Gateway 150 services.  Control manager 1800's duty is to manage complex interactions of services to carry out each gateway 150 function request.  Therefore, the Gateway 150 control manager provides the following functions: a facade of\nGateway 150 services; serialization and prioritization of requests, and event subscriptions.\nThe UI Service 1810 acts as the presentation interface of gateway 150.  This service contains an application audio control 1830.  The application audio control 1830 is a volume mixer that can perform volume control on all the applications used\nby the UI Service 1810.  The UI Service 1810 also interacts and manages layouts with the following major applications: the Media Guide; 1820 the Setup and Activation Manager; the Personal Content Manager; the Photo Album; Recorded Content; the Media\nPlayer 1840; Personalized Advertisements; Channel-based Chat; Skype 1850; Google Widgets 1860; and the IP Cam Viewer.\nThe iAccess service 1870 is a web service that exposes a list of UI features to remote user devices such as iPhone, laptop, etc. This service is similar to a PHP web service.  The service features include: serving over HTTP using PHP; converting\nweb requests into equivalent DS messages to invoke action supported by the Gateway 150 control manager 1800; dynamically creating thumbnails for personal images; and automatically providing different layout based on the end-user device.\nThe RF receiver module 1880 allows a user to use the remote control.  To be more specific, this service receives RF signals, then translates the signals into Gateway 150 remote control command, and notifies the control manager 1800.  The service\nlistens for RF command; and converts the RF command into equivalent DS message to invoke action supported by the Gateway 150 control manager 1800.\nThe Stream Replication Service 1890 retrieves remote media streams and then replicates them to the recording service 1815, rStreamer 1825, media player, and/or EPG guide service 1835.  An IP cam module 1845 is also available to retrieve IP\ncamera streams.  The service demuxes and muxes MPEGTS containers; demuxes DVB Electronic Program Guide from MPEGTS stream; receives multicast, and RTSP; and replicates MPEGTS packet via RTP.\nThe rStreamer service 1825 mainly runs in the background as a streaming server 1900 to output media content into the required format.  As shown in FIG. 19, the rStreamer includes two entities: the trans-coding/muxing module 1910, and a streaming\nserver 1900.  The trans-coding component 1910 is able to trans-code a video stream into H.264 baseline profile and mux logos and advertisements; trans-code an audio stream into AAC LC; mux video streams and audio into a MPEGTS container; and deliver to\nstreaming server 1900 using RTP.\nThe streaming server component 1900 supports the streaming protocol, including Flash RTMP (to support all the desktop/laptop computers); Apple HTTP Live Streaming, (to support all iOS user devices); Microsoft Smooth Streaming (to support all\nWindows 7 Phones); and RTSP/RTP interleaved (to support all the RIM (Blackberry) devices) and, TV tuner sharing.  The server 1900 services all the required streaming protocol over 1 single port.  The streaming server 1900 receives the MPEGTS RTP stream. \nThe video coder may be H.264 and the audio coder may be AAC.  The streaming server interacts with the User Manager 1920 to provide access control.\nThe Recording service 1815 works with the replication service 1890 to record live TV programs to the gateway 150 hard disk.  The Recording service 1815 also has a scheduler that tracks all recording sessions.  Therefore the recording service\nreceives MPEGTS RTP (the video may be encoded with H.264 and audio with AAC); tracks all recording schedules; and updates and reflects changes in the database.\nGateway 150 Content inspection function provides the \"be aware\" ability to Gateway 150, and allows Gateway 150 be a \"smart box\" and understand what is going on inside.  It detects information and uses it to further characterization data to\ncreate value-add service for the service provider.  The structure flow chart of content inspection is shown in FIG. 20.\nTwo types of data are generally handled by gateway 150, video and internet.  A different interface is used for each type to implement basic capture and store functions.  The result analyzing and audit function can be implemented in offline mode\non either client side or server side.  The content inspection provides the data for the multiplex to achieve accurate advertisement push.\nThe EPG service 1835 receives a DVB electronic guide from the Stream Replication Service 1890 via RTP.  Then it updates the local cached EPG list, and notifies if there is a change in the guide.  The EPG service 1835 thus receives MPEGTS RTP\nstream with DVB electronic program guide; processes DVB electronic program guide; and updates the locally cached EPG list.\nThe Media Share Service 1855 is responsible for searching and accessing multimedia contents shared in a home network.  The Media Share Service thus supports DLNA; supports CIFS; and mounts all supported network file sharing protocols.\nThe User Manager service 1865 provides product feature control, account management, and parental control as well as gateway 150 user management.  The product feature control ensures certain functionalities of gateway 150 are disabled or enabled\nbased on the accounting information of the customer.  Account management lets the customer review the services currently subscribed to, and order or cancel services.\nThe Advertisement Service 1875 provides logos and advertisements by communicating with an external advertisement server.  The actual delivery protocol and specification of the format of the advertisement contents are defined in advance.\nThe DS Monitor 1885 enables post instrumentation and is based on three modules--monitoring, reporting and updating.\nThe Domain Service 1895 helps exposing gateway 150 to the public internet from a home network.\nA Wi-Fi AP Service 1896 controls the built-in Gateway 150 wireless access point.  Once enabled, this service will be responsible for providing DHCP service; detecting new user devices; and managing firewall and application port-forwarding.\nA Channel-based Chat service manages and services channel based chats.  A channel-based chat is a service that allows the end user to automatically join a different social chat room based on the current channel selected.\nThe Proximity P4P service 1897, like content inspection, will mainly be used by the service provider to provide network-optimized content delivery.\nAn Application Loader Service allows third parties to develop applications that make use of Gateway 150 services.  This service thus can load and unload a signed Java application package JAR, and enforce rules; and the service contains a Java\nAPI that exposes Gateway 150 services.\nGateway 150 can use previously determined location information to optimize content delivery.  If more than more node trying to access the same content, the content deliver path could be optimized for the best usage of network.  The services\ninvolved include the content management and location management services, and each gateway 150 acts as a network node of this matrix with a content management server on the service provider side.\nThe workflow includes the gateway 150 sending out the content request with the location information to the content server in the service provider data center.  The content management services will check the current mapping table, select the most\n\"near\" (network topology) node (which already contain the content) and sends a redirect for the request to the select node.  The proximity based P4P service could become a cloud computing platform.\nThe hardware abstraction layer (HAL) allows for hardware independent software.  This layer requires the whole software to be aware of the porting capacity.  Software modules that interact with HAL include: USB Keyboard Driver; Audio Mixer; Video\ndemux; Video decoder/encoder; Audio decoder/encoder; and TV tuner interface.\nThe Database Layer includes two database interfaces, one inside the Gateway 150 locally and the other in the service provider data center.  The local database stores user account information, content information, inspection data and audit\ninformation as well as providing data access interface through an API.  The service provider manages the Gateway 150 and collects data related to Gateway 150, such as default data, characterization data, user backup data and so on.  The Gateway 150\nmanagement client also communicates with third party Management systems to retrieve customer account information.\nProduct Description\nGateway 150 does not require installation of a client application on any of the user devices.  All traffic is handled by Gateway 150 through an AI (Artificial Intelligent) engine that can distinguish and manage each user's traffic, under varying\nconnectivity situations (device, network type, location, etc.).  The approach distributes the CPU and memory requirements for millions of users out to the subscribers' premises, similar to a subnet, enabling faster and more accurate data processing. \nThis unique capability enables Gateway 150 to provide the following features.\nTherefore, there is no need for large advertisement insertion servers; Gateway 150 directly connects to the advertisement content and brings the content to the content being watched.  It can simply be an unmanaged content overlaid from any\nsource the user was actually browsing when s/he was on the Internet.  An advertisement for one user within a premise could be completely different from another user within the same premise, each based on the Internet content they are viewing.\nAnother feature is that all types of usage traffic can be reported to the operator for billing purposes.  Operators will be able to bill users and content vendors.  This includes generating advertisement revenue from managed or unmanaged sources\nor generally speaking from any content vendor that the user is subscribed to.\nSharing content among Gateway 150(s) can be either a known feature for the users or a transparent feature focusing on delivering benefits to the operators.\nUser's Content Sharing:\nGateway 150 can provide user owned storage and allow users to share their content between users, either within or outside the premise and between friends.  It must be noted that either are possible and configurable based on a service provider's\npolicy model.\nOperator's Content Sharing:\nGateway 150 enables a service provider to reduce the traffic over their core-network and enables them to use gateway 150 as distributed CPU and storage.  An example of such functionality is when two users are watching the same VoD.  One of the\nusers could get the VoD from the other gateway 150 instead of accessing the service provider server in a regional office or deep core-network.  Another example of such functionality could turn gateway 150 into distributed nodes with a massive number of\nTV tuners for recording different programs that run simultaneously.  Two users, each with a single TV tuner gateway 150 that have access to same channels, can share the program that each has recorded (be peer-to-peer content streaming).  Note that the\npolicy of access, digital right management and billing for all features always taken into consideration.\nControl Manager\nThe control manager is responsible for registration of user devices 180 with gateway 150, including information about the capabilities and limitations of such user device 180 and the users with whom the user device 180 is associated.\nThe control manager maintains usage logs for each user device.  These logs can be used to improve Quality of Service (QoS) by determining when errors or poor performance occurs and taking steps to improve the quality (for example by increasing\nthe buffer for streaming video).\nThe control manager acts as a message broker between user devices when one is used to interact with or access content on the other.\nThe control manager also manages the TV tuners included as part of the box.  Such tuners, and associated EPG information, are set to the appropriate channel and output to the appropriate user device by sending a request to the streaming server.\nAll traffic is being handled by Gateway 150 through an AI (Artificial Intelligent) engine that can distinguish and manage each user's traffic, under varying connectivity situations (device, network type, location, etc.).  The approach\ndistributes the CPU and memory requirements for millions of users out to the subscribers' premises, similar to a subnet, enabling faster and more accurate data processing.\nStreaming Server\nThe streaming server handles several duties.  It opens the port to the TV tuner, and handles the Electronic Program Guide (EPG), and modifying the format to the type of user device requesting the signal.\nThe streaming server also sets and monitors the quality of signal based on the size of the display.  The streaming serve controls the size of the buffer needed based on the limitations of the device, its processor, the size of the display and\nthe signal.  Past user experiences with the device may be taken into account.\nThe streaming server thus can adapt to learned limitations of a user device to improve experience and can handle resolution changes and needs\nWhen a user device is receiving Internet content, the streaming server can send a link to the user device to the content and play no other role.  Alternatively, if the user device is local to the streaming server, the gateway can receive the\ncontent and adapt it for the user device.\nThe streaming server uses a transcoder to convert content from one format to another.  The transcoder provides both a coding and decoding process.  It receives input, decodes it, and codes it again as needed by the user device.\nDistant User Devices\nOne use of gateway 150 is to take advantage of the connection between the user device and gateway 150, which occurs even if the user device is distant from the gateway (e.g. outside of Wi-Fi range).  This allows a user device to receive\nadvertisements based on the user's past experiences, like browsing history on other user devices and personal interests, not available when logging in directly to a distant network.\nIn this use of gateway 150, a user at a remote location uses a user device, such as a smart phone, to access the Internet from a location far from the gateway 150.  By accessing gateway 150, advertisements presented to the user on the user\ndevice may be directly target based on the UCC-E and CP-E.\nBit Rate\nThe gateway 150 streaming server approach uses a single transcoder to provide multi-bitrate adaptive streaming for a single user, as opposed to a separate transcoder for each bitrate that is provided to the user.\nOur streaming server advertises multiple bitrates to a client device, and then begins transcoding at the first requested bitrate.  As the client device requests different bitrates (based on network conditions and the client's hardware\ncapability), the server reads these requests and seamlessly adapts its transcoder parameters to accommodate the client's request.  A single transcoder has its parameters tuned for a single client's requests, so a single transcoder can only service a\nsingle client.\nThis allows gateway 150 to scales well with the number of bitrates provided (as only one transcoder is ever required for a single user).  This approach does not scale well with the number of users requesting streams (each user requires one\ntranscoder), so this approach is likely not preferable if the number of users with access to the server is larger than the number of transcoders available.  However, as the number of expected users of gateway 150 is relatively low (family sized), the\nexpected range of bitrates provided will have a larger impact on gateway 150 then the number of users.\nMulti-Tasking on User Devices\nGateway 150 allows users to use their TV display for a many functions simultaneously.  For example, a user can view a movie, make an Internet call (Skype) with a friend, and share a video or audio file all at the same time on the same device.\nGateway 150 accomplishes this by splitting the TV display into different zones, and displaying different content, transcoded appropriately, in each zone.  Each zone can individually access a menu and receive commands, and can display content\nfetched, including content from Internet, and can enable users to access other viewers' feedback and ratings regarding any targeted or in progress content in an assigned zone.\nUser Information & Communication with Service Provider Server\nGateway 150 frequently communicates with service provider servers for a number of reasons.  For examples when a new gateway 150 is purchased and configured, the gateway 150 registers with the service provider server and becomes associated with a\ncustomer, as well as the registered users and user devices.  When changes are made, for examples, new users or user devices registered, the service provider server is updated accordingly.\nGateway 150 functions as a dynamic DNS server by handling changes to the IP addresses associated with gateway 150, as assigned by the ISP (which may be the service provider).\nGateway 150 can function as a feedback device for the service provider.  Questions and surveys can be directed to registered users, and provided by gateway 150 to the service provider with appropriate demographic information about the responding\nuser.\nThis feedback could be used to instantly obtain user opinions on content, such as TV pilot episodes, scheduling, or services.  For example, a TV pilot could be aired, and followed immediately with a few questions asking user who viewed the pilot\nwhether they would follow the series, perhaps even with questions related to time slot, plots, and characters.\nThe feedback could also be used to determine TV ratings.  Gateway 150 already knows at least one registered user is watching a particular program at a particular time.  A question as to identifying any other users watching is all that is\nrequired to allow gateway 150 to provide information for inclusion in TV ratings, such as those provided by Nielsen.  If a sufficient number of gateway 150s are in use, it can be a simple function to determine very accurate viewership of a particular\nprogram, along with information about PVRing of the program.\nOther feedback could be used to inquire about the success of advertisements (e.g. is a trailer more likely to draw someone to a movie; or is a political advertisement likely to sway a voter).\nGateway 150 can also be used for policy management, particularly as the files are stored on gateway 150, and the content can be policed by the service provider, for example by deleting PPV files, once watched.\nThe parental control system is quite flexible.  If the video or audio content, either from Internet or the TV, is encoded to include rating information by scene, then gateway 150 can modify the content accordingly.  In this embodiment, for\nexample, if the parental control indicates that certain language is not to be output, then gateway 150 can simply leave out such audio according to the audio stream information.  Similar systems could be used for sexual or violent content, as gateway 150\ncould simply excise portions of the video stream.\nAlternatively if a database about the times within a video or audio stream when potentially objectionable content takes place is available at service provider servers, the content could be excised based on the time played of the video stream.\nService provider can also monitor gateway 150 by logging into gateway 150 remotely, accessing logs and hard drives, etc. The service provider can even take control of gateway 150 if necessary.\nGateway 150 provides users with means for social networking.  Users will have access to popular social networking services, such as Facebook and LinkedIn, but will also have access to a community of gateway 150 users.  Users of gateway 50 can\nmark content stored on gateway 150 as public or private.  Public content may be made available to users of other gateway 150s, possibly limited to \"friends\" of the user with the content.  Private content will not be so accessible.  Public content can be\nstreamed to other gateway 150s, even when the user is performing a different task, for example watching TV.  Both the sending user and receiving user may be watching the same TV program simultaneously while engaging in VoIP discussion about same and\nexchanging a file.\nGateway 150 can also be used to target advertising specifically to user activities.  For example, when a user is watching TV content, for example a music video, if an Internet viewing screen is enabled; they could be presented with an\nadvertisement for purchase of that same, or very similar, music content.  Likewise, when a user is watching a TV program, they could be presented with an advertisement for the DVD of the previous season of that same program.  When selecting appropriate\nadvertising for users, the service provider will also have the information from gateway 150 about the user's interests, demographics, and habits.\nFor the sake of convenience, the embodiments above are described as various interconnected functional blocks or distinct software modules.  This is not necessary, however, and there may be cases where these functional blocks or modules are\nequivalently aggregated into a single logic device, program or operation with unclear boundaries.  In any event, the functional blocks and software modules or features of the flexible interface can be implemented by themselves, or in combination with\nother operations in either hardware or software.\nWhile particular embodiments have been described in the foregoing, it is to be understood that other embodiments are possible and are intended to be included herein.  It will be clear to any person skilled in the art that modifications of and\nadjustments to the foregoing embodiments, not shown, are possible.", "application_number": "14389601", "abstract": " A gateway for location at a user premises is provided. Bother users and\n     user devices are registered with the gateway which is provided by a\n     service provider. Users access services, such as video streaming, on\n     their user devices via the gateway. The gateway allows users to share\n     content. The gateway collects information about the user's patterns of\n     behavior for the system provider, and can be used to directly obtain\n     feedback from the users.\n", "citations": ["4566030", "8291453", "8858313", "20020184629", "20040070593", "20050021387", "20050157217", "20060074708", "20060253330", "20070088553", "20070244752", "20080235746", "20080279216", "20100031162", "20100138868", "20110106750", "20110197240", "20110282749", "20120011529", "20120023518", "20130024293", "20130073387", "20130073400", "20130073473", "20130159499", "20140236732", "20140237053", "20150180747", "20160029155", "20170201779"], "related": []}, {"id": "20150113098", "patent_code": "10375133", "patent_name": "Content distribution and data aggregation for scalability of observation\n     platforms", "year": "2019", "inventor_and_country_data": " Inventors: \nVanBuskirk; Guy R. (Spicewood, TX), Kumar; Ravi Shankar (Richardson, TX), Cheedella; Shiva (Dallas, TX), Lucy; Steve (Dallas, TX)  ", "description": "<BR><BR>BACKGROUND\nRetailers are under constant pressure to cut costs, improve margins, and increase floor traffic and customer satisfaction.  This has always been so, but the rise of the internet, available at home and while mobile, has increased the pressure\ngreatly.  Loyalty programs and per-customer pricing, such as special discounts, are one set of tools used in the past, and used more.  Moreover, there is an increased demand to manage and train associates and provide an increased measure of customer\nsatisfaction in a retail environment.  Such concerns also extend to situations and environments besides retail settings.  Modern communication devices provide for many communication and business analytics opportunities in retail and other settings.\n<BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nFIG. 1A illustrates a block diagram of an example environment for an observation platform for structuring a communication in accordance with embodiments of the present technology.\nFIG. 1B illustrates a block diagram of an example environment for an observation platform for structuring a communication in accordance with other embodiments of the present technology.\nFIG. 1C illustrates a block diagram of an example environment for an observation platform for structuring a communication in accordance with other embodiments of the present technology.\nFIG. 2 illustrates a block diagram of an example environment for structuring communication in an observation platform in accordance with embodiments of the present technology.\nFIG. 3 illustrates a flowchart of an example method for structuring communication in an observation platform in accordance with embodiments of the present technology.\nFIG. 4 illustrates a flowchart of an example method for disciplining communications in accordance with embodiments of the present technology.\nFIG. 5 illustrates a flowchart of an example method for observing and recording users of communication devices in accordance with embodiments of the present technology.\nFIG. 6 illustrates a flowchart of an example method for characterizing communications in a group of users in accordance with embodiments of the present technology.\nFIG. 7 illustrates a flowchart of an example method for structuring communication in a plurality of observation platforms in accordance with embodiments of the present technology.\nFIG. 8 illustrates a flowchart of an example method for performing communications in an of observation platforms in accordance with embodiments of the present technology.\nFIG. 9 illustrates a flowchart of an example method for performing communications in an of observation platforms in accordance with embodiments of the present technology.\nFIG. 10 illustrates a flowchart of an example method for performing communications in an of observation platforms in accordance with embodiments of the present technology.\nFIG. 11 illustrates a flowchart of an example method for performing communications in an of observation platforms in accordance with embodiments of the present technology.\nFIG. 12 illustrates a flowchart of an example method for performing communications in an of observation platforms in accordance with embodiments of the present technology.\nFIG. 13 illustrates a block diagram of an example environment for managing and distributing content in a plurality of observation platforms in accordance with embodiments of the present technology.\nFIG. 14 illustrates a flowchart of an example method for managing and distributing content and aggregation of data in a plurality of observation platforms in accordance with embodiments of the present technology.\nFIG. 15 illustrates a flowchart of an example method for managing a plurality of observation platforms via a manager application in accordance with embodiments of the present technology.\nThe drawings referred to in this description of embodiments should be understood as not being drawn to scale except if specifically noted.\n<BR><BR>DESCRIPTION OF EMBODIMENTS\nReference will now be made in detail to embodiments of the present technology, examples of which are illustrated in the accompanying drawings.  While the technology will be described in conjunction with various embodiment(s), it will be\nunderstood that they are not intended to limit the present technology to these embodiments.  On the contrary, the present technology is intended to cover alternatives, modifications and equivalents, which may be included within the spirit and scope of\nthe various embodiments as defined by the appended claims.\nFurthermore, in the following description of embodiments, numerous specific details are set forth in order to provide a thorough understanding of the present technology.  However, the present technology may be practiced without these specific\ndetails.  In other instances, well known methods, procedures, components, and circuits have not been described in detail as not to unnecessarily obscure aspects of the present embodiments.\nUnless specifically stated otherwise as apparent from the following discussions, it is appreciated that throughout the present description of embodiments, discussions utilizing terms such as \"receiving,\" \"recognizing,\" \"deriving,\" \"storing,\"\n\"relaying,\" \"executing,\" \"generating,\" \"determining,\" \"tracking,\" \"recording,\" \"identifying,\" \"making,\" \"delivering,\" \"scheduling,\" \"specifying,\" or the like, refer to the actions and processes of a computer system, or similar electronic computing\ndevice.  The computer system or similar electronic computing device, such as a telephone, smart phone, tablet computer, or handheld mobile device, manipulates and transforms data represented as physical (electronic) quantities within the computer\nsystem's registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage, transmission, or display devices.  Embodiments of the present technology are\nalso well suited to the use of other computer systems such as, for example, optical and mechanical computers.\n<BR><BR>Overview of Content Distribution and Data Aggregation for Scalability of Observation Platforms\nEmbodiments of the present technology are for a content distribution manager for scalability of observation platforms.  An observation platform may involve a number of users or people and provides structure and discipline communications for the\nusers and captures data regarding the communications such as user context and performance metrics.  The present technology may be employed in various environments such as retail settings, public-stage floors, outdoor venues, concerts, police scenarios,\ndisaster areas, schools, sporting events, hospitality operations, security operations, military operations, a prison organization, customer service, manufacturing organization, a factory, and other environments where humans work together and where\ncommunications occur between users.\nThe content distribution and data aggregation system, hereinafter called \"the content distribution manager,\" allows for simple operation of a plurality of observation platforms by providing a graphical and audio interface for choosing\nindividuals or groups of individuals for which to send or receive messages and collect, group, aggregate and statistically analyze connections, motions or behaviors of users within and between a set of observation platforms.\nThe present technology employs an architecture with cloud based services that allows for the management of a plurality of observation platforms.  For example, a manager may be responsible for any number of retail and/or hospitality environments\nsuch as a hundreds or even thousands of retail environment each with their own physical building and each with an observation platform.  Each environment may comprise any number of users associated with the observation platform including managers, store\nassociates, employees, hourly workers, salespeople, customers, guests, etc. The manager may employ the present technology to observe the observation platforms or may use the present technology to hear, see or deliver messages to the devices associated\nwith the observation platforms.  The messages may be a voice recording or other type of audio file possibly generated by text-to-speech (TTS) engines attached to or contained within the present technology.  The manager or user may employ a content\ndistribution manager to generate or access the message.  The content distribution manager is then used to schedule when the message is to be delivered to the devices.  The content distribution manager may also be used to determine or specify which\ndevices in the observation platforms are to receive the message.  For example, the content distribution manager may be used to specify that the message is only to be delivered to devices that are associated with the store manager(s) or may specify that\nall devices associated with a particular department are to receive the message.  The specification of devices to receive the information may also be based on the context of the devices as determined by one or more of: the identity of the user (if\nidentified) associated with the device, the characteristics of the signals being received by the device, the history of the signals that have been received by the device or associated user and the analysis of the historical metrics of the device or user\nassociated with the device.  In one embodiment, different devices may be scheduled to receive the message at different times.\nIn one embodiment, the content distribution manager may be accessed by any computing device via a uniform resource locator (URL) or web address.  The content distribution manager has a graphical interface for the user or manager to observe the\nobservation platforms and to control the content and messages sent by the content distribution manager to the devices associated with the observation platforms.  Portions or components of the content distribution manager may be executed or associated\nwith cloud services or cloud computing.  The cloud services are then used to deliver the messages with content to the observation platforms.  The cloud services may also be used to provide other applications or services associated with observation\nplatforms.  For example, the other services or applications may be a web portal or service portal, a manager app, a log-on tool, a support manager, data analysis tools, and third party apps and computer systems.\nUsing structuring communications in an observation platform, as referred to herein, may refer to the following actions regarding communications between two or more users: disciplining, structuring, controlling, participating, discouraging,\nencouraging, influencing, nudging, making an example of, permitting, managing, managing to be in compliance with policies, measuring what goes on as a communication occurs, characterizing, enabling, observing, recording, correcting, directing, etc.\nThe structuring or disciplining process envisioned herein involves using a communications and computer system as a platform to listen to commands from users, interpret those commands, establish two-party and multiparty communications links, pass\non messages, and store messages and commands, thereby permitting an owner or manager of a group of people to observe and analyze the effectiveness the group's interactions.  In a way, it is like the central nervous system of an organism, wherein the\norganism is a group of people.\nOne purpose of structuring or disciplining a communication is for associates to become better customer service associates or sales people in a retail setting.  The present technology may accomplish this goal by monitoring communications of the\nusers that occur via communication devices.  The communications may be monitored to derive context information from the communication such as the name of the user, geographic location of a user, the state or status of the user (e.g., busy, available,\nengaged, conversing, listening, out-of-range, not logged on, etc.), business metrics regarding the user's interaction with others, and commands from the user.  The communications may be monitored by a computer system associated with a radio base station\nthat acts as a central hub for the user communications.  The computer system may convert audible, voice or speech communications to a text or machine-compatible format using standard and well-known techniques.  The text may be used to derive the context\ninformation from the communication.  The computer system may also store some or all of the communication including the time and geographic location of the device, the audible portion of the communication and the text format of the communication.  The\nstructured communications may extend beyond a single venue to multiple venues or storage locations without regard to geographic location.  Customers or users may refer to customers who are purchasing items in an environment, past customers, potential\ncustomers, perspective customers, shoppers, browsers, or others who enter the environment and do not represent the environment in an official capacity such as an employee does.\nIn one embodiment, the computer system uses the derived context information to determine a destination of the communication and forwards or relays the communication to the destination.  For example, a first user may attempt to contact a second\nuser via communication devices.  The first user sends the communication to the computer system associated with the radio base station.  The computer system recognizes the first user and is able to derive context information regarding the communication\nand determine that the communication's destination is a third user.  The computer system then relays the communication, via the radio base station, to a communication device associated with the third user.  The computer system may also convert the\ncommunication to text and derive contextual or performance metrics regarding the first or third user.  For example, the first user may be an associate in a retail setting and the third user is a customer.  The first user may be responding to a query made\nby the third user.  The performance metric may be the length of time it took for the first user to respond to the query or may be whether or not the query was satisfied or may be a different metric entirely.  The computer system may derive and store more\nthan one performance metric.  The computer system may also access more than one communication regarding a user to determine some metrics.\nIn one embodiment, the computer system is able to determine the state of the user based on either direct user action such as a button press or voice command; or based on inference from words being spoken, motions, locations or other contextual\ninformation.  In one embodiment, the third user may be out of range of the radio base station and sends communications via a network associated with the computer system.  In one embodiment, the third user may be part of a similar operation to that in\nFIG. 1A, i.e., another retail outlet or corporate headquarters for the same company in a different location as shown in FIG. 1C.\nIn one embodiment, the computer system is able to determine geographic locations of users based on information received from communication devices associated with the users.  The geographic location data may be stored as data associated with a\nuser's communications device at a particular time, or as a performance metric, or may be combined with other information to generate a performance metric.  The geographic information may also be used by managers to manage or train associates or to\noptimize customer service.\nA user, as referred to herein, may be a person or people such as, associates, employees, managers, trainees, trainers, customers, emergency responders, personnel, etc. In one embodiment, the user interfaces with a device for communications with\nother users.  Such a device may be a handheld device, a headset, a Smartphone, an earpiece, a radio, a computer system, or other device capable of providing communications between users.  Such users may be external to the operating entity and desire\naccess via smart devices or applications.\nA performance metric may also be a metric, a key performance metric or a business metric.  A metric or performance metric as referred to herein may be any type of data associated with or derived from a communication between users, including the\nlocation of the communications device, or the words spoken and the contextual state at the time of a particular communication event.  In one embodiment, the computer system is able to generate a visual representation of metrics.  For example, the visual\nrepresentation may be a map of the geographic location of the users in an environment or may visual demonstrate the availability status of a user.  In another example, the visual representation may be textual information such as the number of\ncommunications sent by a user or the length of time it took for a user to respond to a communication.  The performance metrics may be sent or displayed to a manager or other user for use in making decisions.  The performance metrics may be used by the\nmanager to optimize customer service in a retail setting by taking actions such as reprimanding or rewarding an associate or noticing that no associates are located near a group of customers.  Performance metrics may also generate real-time alarms or\nnotifications that action or coordination is needed.\nThe present technology provides for many examples of how structuring communications may be used in various environments for a variety of purposes.  The following discussion will demonstrate various hardware, software, and firmware components\nthat are used with and in computer systems and other user devices for structuring communications using various embodiments of the present technology.  Furthermore, the systems, platforms, and methods may include some, all, or none of the hardware,\nsoftware, and firmware components discussed below.\n<BR><BR>Content Distribution and Data Aggregation for Scalability of Observation Platforms\nWith reference now to FIG. 1A, a block diagram of an environment 100 for structuring communications in an observation platform.  Environment 100 includes devices 105, 110 and 130, radio base station 115, computer 120, database 125 and network\n135.  Environment 100 comprises components that may or may not be used with different embodiments of the present technology and should not be construed to limit the present technology.  Some or all of the components of environment 100 may be described as\nan observation platform for structuring a communication.\nThe present technology makes use of communication devices.  Radio base station 115 and devices 105, 110 and 130 may also be described as communication devices.  Devices 105, 110 and 130 may be user devices that are mobile and employed by a user\nto communicate with other users via other devices.  Communications between the devices may be described as signals.  The devices 105, 110 and 130 may be a smart phone, a personal digital assistant, a fob, a handheld device, a headset device or other\nsmall electronic device.  In one embodiment, devices 105, 110 and 130 employ speakers and microphones with control buttons for audible communications.  The control buttons may be press to signal buttons, push to talk buttons, volume control buttons, and\npower on/off buttons or other standard buttons and may be options on a touch screen.  Devices 105, 110 and 130 may be handheld, may be worn around the neck, and may be a headset worn on the head or behind the ear or otherwise interface with the human\nbody.  Devices 105, 110 and 130 may or may not comprise a screen or display such as a liquid crystal display (LCD).  In one embodiment, devices 105, 110 and 130 do not comprise a display such that a user is not inundated with too many options or too much\ninformation from the device.  A user device without a display may simplify communications and thus allow heads-up awareness and presence in the environment.  Another user, such as a customer, may be more likely to employ the device for its intended\npurpose if the human interface is simplified.\nDevices 105, 110 and 130 and other devices in environment 100 may be dispensed to a user upon entering environment 100 or may be brought by the user into environment 100.  For example, in a retail setting associates may be issued devices by the\nemployer or owner of the retailer setting.  Customers in the retail setting may also be issued devices as they enter the retail setting.  Customers may choose whether or not to accept the device or whether or not to use the device after accepting it. \nThe associate devices and the customer devices may or may not be the same type or model of devices.  Alternatively, the customer may bring a device into the retail setting such as a smart phone.  The customer may download an app to the smart phone that\nwill allow the customer to use the device for communications in the store with associates or others in accordance with present technology.  The customer may remain anonymous or may elect to identify themselves.  In one embodiment, recognition of the\ncustomer's identity is not required for additional services or offers.\nDevices 105, 110 and 130 may be low power devices.  The devices may use batteries or solar power including either ambient or battery solar power in a low duty-cycle manner to save power.  In one embodiment, the devices have an automatic sleep\nfunction when location of the device does not change and no communications are sent or received after a period of time.\nRadio base station 115 may be a communication device that is capable of communicating with devices 105, 110 and 130.  Radio base station may simply be a component of computer 120 or may be a standalone device that is coupled with, connect to, or\notherwise associated with computer 120.  Radio base station 115 and computer 120 may be physically adjacent to one another or may be separated by a distance (e.g., cloud services).  Computer 120 is able to instantaneously receive communications from\nradio base station 115 and to send communications to radio base station 115 for radio base station 115 to transmit the communication to its destination.  Computer 120 is a computer system with a process and memory and is capable of executing commands,\nsoftware and firmware.  Computer 120 may be a desktop computer, a server computer, a cloud-based computer or other standard computing system or may be custom built for the present technology.\nRadio base station 115 and devices 105, 110 and 130 employ standard techniques for communicating wirelessly.  The communications may be performed using radio techniques such as near field communications, short wave radio, infrared, Bluetooth,\nWi-Fi, standard wireless computer network protocols, etc. Devices 105, 110 and 130 may be able to communicate with each other directly or through radio base station 115.  Devices 105, 110 and 130 communicate with each other via the computer system 120. \nIn one embodiment, all communications in environment 100 are relayed through radio base station 115 which acts as a central hub.  For example, device 105 may communicate with device 110 by device 105 sending a communication to radio base station 115,\ncomputer 120 derives that device 110 is the destination for the communication and relays the communication to device 110.  This may occur automatically and quickly enough such that the users will not experience any undue lag in communications.  In one\nembodiment, devices 105, 110 and 130 may communicate directly with computer 120.  For example, a user may issue a command to computer 120 via device 105 or computer 120 may send information to device 105.  Information send from computer 120 to device 105\nmay be an audible voice signal or may be textual, contextual, geographical or graphical data to be displayed at device 105 if it is properly equipped to do so.\nIn one embodiment, devices 105, 110 and 130 may communicate with one another directly, and their signals may be monitored and processed by computer system 120 via a monitoring system associated with the radio base station 115.  Instructions or\ncommands may still be directed towards the computer system 120.\nIn one embodiment, computer 120 is able to recognize a user sending a communication.  The user may be recognized based on the device used to send the communication to computer 120 and radio base station 115.  For example, device 105 may have a\nunique signature associated with its transmissions such that computer 120 can identify differentiate the device from another user.  Such recognition of a user may then be employed by computer 120 for future communications with other devices.  In one\nembodiment, the signal or communications between devices are encrypted.  The signal may be encoded such that it is unique to a specific device.  The encryption or encoding may be employed by computer 120 to recognize the user of the device.  In one\nembodiment, the user may identify himself to the computer system 120 and the computer system 120 makes the association between user identification and device 105's internal electronic identification.\nComputer 120 may determine that the destination of a communication is a single device or a plurality of devices.  Thus computer 120 may relay a communication from device 105 only to device 110 or may relay it to both device 110 and device 130. \nComputer 120 may determine that another user device is the destination of a communication originated by device 105 but may also directly respond to the communication by executing a command or sending a communication back to device 105.  In one\nembodiment, a communication from device 105 has more than one characteristic or aspect.  For example, the communication may have a first characteristic that corresponds to an audible source such the words spoken by a user employing device 105.  The\ncommunication may also contain contextual information such as engaged, available, listening to information, returning to coverage zones, or other behavioral/contextual information.  The communication may also have a third characteristic that comprises\ngeographical position information of device 105 or may have information indicative of a geographic position of device 105.  Computer 120 is able to determine a geographic position and direction of motion of a device from the information indicative of a\ngeographic position of device.  The motion may also be described as path of travel.  A characteristic of the communication may be a portion of the communication, data associated with the communication, attributes of the communication, or metadata\nregarding the communication.\nIn one embodiment, computer 120 comprises a storage medium for storing some or all of a communication.  Computer 120 may store all communications between devices in environment 100.  Computer 120 may store communications for a pre-determined\namount of time.  Different characteristics of the communication may be stored including portions of the communication itself.  Additionally, the computer may request and store all audible information regardless if the user presses a push to talk button\nor otherwise signals the need to begin a communication.  For example, the communication may comprise an audible portion, a text portion, information indicative of a geographical position, and a geographical data portion.  The audible portion may also be\nconverted to text.  Computer 120 may store all or some of the different portions including the portion converted to text.  Computer 120 may store geographic position information regarding a device over a period of time such that a path of travel of the\nuser may be inferred.  Thus the position and context of a user may be mapped, tracked or predicted through a physical environment or area.\nIn one embodiment, computer 120 receives a communication from a device with a portion of the communication that corresponds to a voice of the user of the device.  Computer 120 is able to convert the audible portion to information used by\ncomputer 120 to derive context information from the communication to determine performance metrics regarding the communication or the user of the device.  The resulting information may also be interpreted as a command for computer 120 to execute.  The\nresulting information may also be employed to determine a destination for the communication.\nIn one embodiment, each speaker is identified with a unique identifier with each voice file so that a speech-to-text engine can train on the speaker's voice and more accurately choose words from the dictionaries and individual user grammars. \nIndividually customized dictionaries and grammars may be used for the sequential context of the spoken words.  For example, saying, \"urgent Bob\" is interpreted by looking up the first word in a command dictionary and the second word in a names or places\ndictionary.  In one embodiment, a frequency table is built for each user defining how frequently they call a name or place to improve the probability of selecting the correct word.  In one embodiment, if a command, name, or place is not understood, the\nsystem may default to the most likely destination group.  The user can easily opt out of the default destination and start again.  Alternatively, if the command, name or place is not recognized, the computer system 120 may be programmed to default to a\nsimple reply such as \"command not recognized\" or \"person not found.\"\nIn one embodiment, computer 120 executes a command received from device 105.  The command may be directly received from device 105 or may be received in an audible voice signal with is converted to text and then interpreted to be a command for\ncomputer 120.  The command may be to initiate a virtual voice connection between device 105 and device 110.  The command may be to initiate a connection to a telephony system such that a user of device 105 may communicate with another user who is\nemploying a telephone for communication.  The command may be for computer 120 to store information into or extract information out of database 125.\nIn one embodiment, computer 120 is able to access database 125 over network 135.  Network 135 may be a local area network, a wireless network, the Internet or another computer network.  In one embodiment, database 125 is a component part of\ncomputer 120 and network 135 is not required for computer 120 to access database 125.  Database 125 may comprise an inventory of product or any other type of information.  For example, in a retail setting a customer may use a device to communicate with\nan associate regarding whether the retail setting has a particular product in stock.  The associate may use key terms to query computer 120 regarding whether the product is in stock.  Computer 120 may convert the associate's voice to text and recognize\nthe command regarding whether the product is in stock.  Computer 120 then queries database 125 and sends a response back to the associate and/or customer.  The response may be sent back using an audible signal or a signal to be displayed on a screen at\nthe user device.  Similar examples may be constructed around product location databases, workforce scheduling systems, on-floor zone assignments, time clock systems or other information systems used for operations and reporting.  Alternatively, computer\n120 may recognize a command based on the converted text without a user saying key terms.\nDatabase 125 may be a local inventory or a larger inventory.  In one embodiment, database 125 is not an inventory but comprises different data.  For example, a user may employ the device to communicate with and command computer 120 to perform a\nkey word search of the Internet using a search engine such as a website search engine.\nWith reference now to FIG. 1B, a block diagram of an environment 140 for structuring communications in an observation platform.  Environment 140 includes devices 105, 110 and 130, radio base station 115, computer 120, transceivers 145, 150, and\n155, and regions 160, 165, and 170.  Environment 140 comprises components that may or may not be used with different embodiments of the present technology and should not be construed to limit the present technology.  Some or all of the components of\nenvironment 140 may be described as an observation platform for structuring a communication.\nTransceivers 145, 150, and 155 are capable of sending and receiving signals to and from radio base station 115 and devices 105, 110 and 130.  Transceivers 145, 150, and 155 may or may not be networked to one another and to either radio base\nstation 115, computer 120 or both.  Transceivers 145, 150, and 155 may be transceivers such as wireless routers in a computing network.  The transceivers may relay a communication from a user device to computer 120.  A communication or signal may be\nrouted through a plurality of transceivers before reaching computer 120.\nIn one embodiment, the transceivers may be uniquely identifiable such that a communication may comprise a characteristic that identifies the communication as being routed through a given transceiver.  This identification of the transceiver may\nbe employed by computer 120 to determine a geographic location of a device or user.  Thus, a characteristic of the communication may be an identity of a transceiver and comprises information that is indicative of a geographic position.  Computer 120 may\ndetermine that a device is in a geographic region that is associated with a transceiver such as region 160 associated with transceiver 145.  Computer 120 may also use geographic information and user motion characteristics to predict and pre-set\nassociation to the next likely transceiver.\nIn one embodiment, computer 120 determines the geographic location of a device based on a transceiver signal strength received at the device from one or more transceivers.  For example, device 130 may receive signals from both transceivers 150\nand 155 each with a corresponding signal strength.  The signal strength data is sent from device 130 to computer 120 as a characteristic of a signal or communication sent to computer 120.  The signal strength data is then used by computer 120 to\ndetermine the geographic position of device 130.\nTransceivers 145, 150, and 155 each have an associated region such as regions 160, 165, and 170.  The regions may define the transmission range of the transceiver or may be defined based on some other criteria.  In one embodiment, the regions\nmay be described as wireless hotspots.  Regions 160, 165 and 170 may be well defined geographical regions either indoors or outdoors and me be known to computer 120.  Regions 160, 165 and 170 are depicted as not overlapping one another.  However, the\nregions may or may not overlap one another.  In one embodiment, computer 120 may determine the geographic location of a device based on its location in one or more regions.  For example, device 105 may be located in region 160.  In another example,\nregions 160 and 165 may be overlapping and computer 120 determines that device 110 is in the overlapping portions of region 160 and 165 because a characteristic of a communication from device 110 indicates that device 110 is receiving signals from both\ntransceiver 145 and 150.  Thus a characteristic of signal sent from a user device to computer 120 may be contents of a communication, a portion of a communication corresponding to an audible source, signal strength data of a transceiver, an identity of a\ntransceiver, geographic position data, or other information.\nIn one embodiment, computer 120 determines the geographic motion, movement, or path of travel of a user based on transceiver signal strengths received at the device from one or more transceivers.  Movement of the communications device 130 may be\nderived from data regarding signal strength measurements made at one or more of the transceivers, where the signal strength is measured and sampled at successive time intervals, via well-known methods.  For example, as a user moves about the region in\nenvironment 140, the signal strength will increase at one transceiver device and decrease at another.  Movement of the communications device 130 may also be derived from internal components in the device such as accelerometers, again via successive time\nsamples of acceleration data.  This data may be used to detect a smaller range of movement.\nWith reference now to FIG. 1C, a block diagram of an environment 180 for structuring communications in an observation platform.  Environment 180 includes devices 105, 110, 111 and 131, radio base stations 115 and 116, computers 120 and 121,\nnetwork 135 and regions 175 and 176.  Environment 180 comprises components that may or may not be used with different embodiments of the present technology and should not be construed to limit the present technology.  Some or all of the components of\nenvironment 180 may be described as an observation platform for structuring a communication.\nIn one embodiment, device 105 and 110 are located within region 175.  The components depicted within region 175 may be described as an observation platform.  Region 175 may be described as having a radio range, or span of operating distance. \nFor example, radio base station 115 may have a physical limit regarding the distance which it may transmit radio signals.  Therefore, a device outside of the radio range, such as devices 131 or 111 will not be able to communicate with computer 120 via a\nradio signal transmitted from radio base station 115.  Additionally, devices 105, 110, 111 and 131 may also have a limited radio range.\nThese limitations may be overcome by computer 120 relaying the communication to either device 131 or a second observation platform within region 176 via network 135.  Therefore, devices 105 and 110 may communicate with either device 111 or 131\nwhere the communications are relayed by computer 120 and network 135.  Region 176 may be described as a second observation platform with components that are duplicates of or similar to components of region 175.  The regions 175 and 176 may comprises any\nnumber of communication devices or other components such computers, routers, and transceivers.  Thus, the present technology provides for structured or disciplined communications between at least two user devices that may or may not be within radio range\nof one another.\nIn one embodiment, the communications between computer 120 and devices 105 and 110 are accomplished via radio signals and the communications between device 131 and computer 120 are accomplished via network 135.  In one embodiment, the connected\nbetween network 135 and device 131 is telephony call such that device 105, which may not be a telephone, places a phone call to device 131, which is a telephone, via the observation platform.  In such an embodiment, network 135 may comprise both a\ncomputer network and a phone network or cloud.\nIn one embodiment, device 131 and/or region 176 may be physically remote relative to radio base station 115.  For example, all the components shown within region 175 may be located within radio range of one another at a first location, but\ndevice 131 and region 176 are located at a second and third location outside of region 175.  These first, second and third locations may be separated by any length of distance.  The second or third location may be hundreds or even thousands of miles away\nfrom the first location or may be less than a mile away but still outside of region 175.  In one embodiment, computer 120 and radio base station 115 are located at a first physical address such as a street address for a building or other physical\nlocation, device 131 is located at a second physical address, and computer 121 and radio base station 116 are located at a third physical address.\nIn one embodiment, computer 120 and radio base station 115 are associated with a retail environment and region 175 includes the retail floor as well as an office or other area designated for associates, managers, or employees of the retail\nenvironment.  However, computer 121 and radio base station 116 are located in region 176 are located at a second retail environment.  The first and second retail environments may be related to one another such as both being a franchise of the same\nbusiness or enterprise.  Thus, a customer or associate may be located in region 175 associated with a first franchise, e.g. a first observation platform, and speak with an associate using device 111 in a second franchise, e.g., a second observation\nplatform.  The customer or associate may ask questions regarding the inventory of an item at the second franchise or speak with an associate at the second franchise that has knowledge not known by associates at the first franchise.\nIn one embodiment, where region 175 and region 176 each comprise separate observation platforms, the present technology is employed to manage and observe the observation platforms.  For example, applications 1314 of FIG. 13 may be employed with\nthe observation platforms.  These applications allow for the scalability of observation platforms such that a single user may have access to the data from a plurality of observation platforms and the ability to send messages to all or some of the devices\nassociated with the observation platforms simultaneously in a scheduled transmission.\nWith reference now to FIG. 2, a block diagram of an environment 200 for structuring communications in an environment.  Environment 200 includes radio base station 115, computer 120, users 205, 210, 215, 220, 225, 230, and 235, structure 240,\narea 245, area 250, radio devices 255 and 260 and user 265.  Environment 200 comprises components that may or may not be used with different embodiments of the present technology and should not be construed to limit the present technology.\nEnvironment 200 depicts a setting in which the present technology may be employed.  Environment 200 may be, but is not limited to, retail settings, public-stage floors, outdoor venues, concerts, police scenarios, disaster areas, and other\nenvironments where communications occur between users.  Areas 245 and 250 are depicted as being enclosed.  However, the present technology may be implemented in an outdoor or indoor environment or a combination of the two.  Users 205, 210, 215, 220, 225,\n230, and 235 are depicted as each holding a device such as device 105 of FIG. 1.  The devices do not necessarily need to be handheld.  Users 205, 210, 215, 220, 225, 230, and 235 may be a variety of different types of users.  For example, the users may\nbe associates and customers intermingled in a retail setting.  Area 245 may be the retail floor while area 250 is a back office or other area designated for associates, managers, or employees of the retail environment.\nStructure 240 may be a display, shelves, aisle divider, or other structure that physically separates spaces in area 245.  For example, users 205, 210, and 215 are depicted as being in separate space of area 245 than users 220, 225, 230, and 235. Computer 120 may be able to interact with users 205, 210, 215, 220, 225, 230, and 235 and determine the user's geographic locations as well as act as a central hub for all communications between the users.  In one embodiment, computer 120 recognizes a\ngroup of users associated with communication devices.  The group may be based on a classification or type of user or may be based on a location of said users.  In one example, computer 120 recognizes that users 205, 215, 230, and 235 are associates and\nusers 210, 220, and 225 are customers in a retail setting.  The associates may be considered a first group and the customers a second group.  In a second example, computer 120 recognizes that users 205, 210, and 215 are a first group in a separate space\nof area 245 than the second group of users 220, 225, 230, and 235.  Computer 120 may then employ the recognition of groups to generate visual representations of features of the group and its communications.  It should be appreciated that groups can\nsimultaneously exist in many locations and are not constrained by building walls or geography.\nIn one embodiment, environment 200 comprises radio devices 255 and 260 used for communication with user devices and radio base station 115.  Radio devices 255 and 260 may or may not be networked with radio base station 115 to provide additional\ncoverage or range for radio base station 115.  For example, radio devices 255 and 260 may be antennas or radio repeaters for radio base station 115.  In one embodiment, radio devices 255 and 260 are wireless routers for computer networking.  Computer 120\nmay employ radio devices 255 and 260 to determine a geographic location of a user.  Radio devices 255 and 260 and transceivers 145, 150 and 155 may each have the same capabilities and features as one another.\nThe geographic location or position of a user may be determined by computer 120 receiving periodic clues or evidence of the geographic location of the user device and then computer 120 infers or deduces the geographic location based on the\nevidence or clues.  For example, the user device associated with user 205 may receive a plurality of signals from radio base station 115 and radio devices 255 and 260.  Each signal has a unique signature at the current position of user 205.  The\nsignatures of each source are periodically sent to computer 120 or as a component characteristic of any communication.  Computer 120 may then determine the geographic position of user 205 based on the signatures of each source and the known location of\nthe sources e.g., radio base station 115 and radio devices 255 and 260.  In one embodiment, the user device knows its geographic position based on geographic position component which is part of the user device.  The geographic position component may be a\ncomponent device or chip that employs the global positing system, other satellite navigation system, inferred signals, radio signals or RFID signals for determining a geographic location or position.  A user device with a geographic position component\nmay transmit the determined geographic position to computer 120 periodically or as part of a communication.  Thus computer 120 may know the location of a user at a given time based on the geographic position of the device associated with the user.\nIn one embodiment, user 265 interfaces with computer 120 to use the present technology to optimize communications.  Computer 120 may determine and display performance metrics or visual representations regarding communications to user 265.  User\n265 may then use the performance metrics and visual representations to make decisions.  For example, user 265 may be a manager of associates who can identify that a customer has asked for assistance at a given location but no associates have responded. \nThe manager may then use the present technology to request an associated to assist the customer.  In one embodiment, user 265 is able to directly use computer 120 and radio base station 115 to communicate with other users by individual identification,\nlocation groupings or contextual groupings.\nIn one embodiment, user 265 interfaces with computer 120 to use the present technology to optimize geographic location.  User 265 may be a customer and requests help from computer 120.  Computer 120 determines the associate nearest the location\nof user 265 and provides the current and updated location of user 265 until intercepted by the associate.  In one embodiment, user 265 may request help verbally, not engaging computer 120, and that request is heard by all nearby associates whose context\nis \"not engaged with shoppers.\"\nIn one embodiment, computer 120 derives performance metrics, business metric or metric from the communications between users.  The metrics may be used to generate visual representations.  The metrics and/or visual representations may be employed\nto make decisions.  The metrics and visual representations may be sent to another computer system or device.  A metric may be based on the behavior of a user, the context of the user, information carried by the tone and quality of voice, and the user's\nspoken or signaled communications.\nA sales performance metric may be determined by linking sales with users, measuring busy (or \"engaged with shopper\") times of users, and ascertaining busy status of user.  The busy status of a user may indicate that the user is engaged in a\ncommunication, a task, assisting a customer or otherwise occupied.  A response time metric may also be determined by measuring the time it takes to answer a user's question, or how long it takes to receive assistance after asking for it.  A customer\nsatisfaction metric may also be derived based on the text of the customer's communication.  A task performance metric may be determined by measuring the length of time an associate is currently engaged in performing said task, including noting pending\nand completed tasks.  Metrics may be used by a manager to reward good behavior or correct undesired behavior.  Additionally, because the communications and other audio information have been recorded, the communications may be used in training as\nexamples.\nVisual representations may be described as communication traffic intensity maps between users and/or groups such as who talks to whom, how frequently and at what time of day; who asks questions and who responds; who responds to tasks, when and\nhow long it took to respond; and who has listened to which training podcasts, where they listened and when.  Visual representations may also be described as location maps such as, a status of when users indicate that they are engaged, busy or available,\nwhen users ask questions; quiet areas where no communications or engagements are occurring; where users are not located; where selling tips were left and by whom; location-based-tasks and the times it takes to complete them; a path of where users have\ntraveled geographically; and a map of the environment.  With this observation platform for structuring communications, a more complete observation of many of the events in the interaction between and among all users can be observed, cataloged, and\nanalyzed, providing a great deal of useful information to any manager of the overall process.\nWith reference now to FIG. 13, a block diagram of an environment 1300 for a content distribution manager for scalability of observation platforms.  Environment 1300 depicts multiple components and as a whole may be described as a content\ndistribution manager.  Specifically, content distribution manager browser 1304 depicts a browser that may be an interface that controls the tools of the content distribution manager including tools such as message scheduler 1322 and message broker 1324. \nEnvironment 1300 comprises components that may or may not be used with different embodiments of the present technology and should not be construed to limit the present technology.\nThe right column of environment 1300 refers to observation platforms 1330 which specifically comprises observation platforms 1332, 1334, and 1336.  It should be appreciated that observation platforms 1332, 1334, and 1336 have the same\ncapabilities and features of the observation platforms described in FIGS. 1A-1C and 2.  Moreover, observation platforms 1330 may refer to the three depicted observation platforms but may also describe any number of observation platforms such as hundreds\nor thousands of observation platforms.  In other words, the present technology may be used to manage, observe, measure, control, and otherwise operate with any number of observation platforms thus associating a large group of observation platforms with\none another and providing for scalability in the management and use of a large number of observation platforms.\nEach of observation platforms 1332, 1334, and 1336 are depicted with three devices, devices 1338, 1340, 1342, 1344, 1346, 1348, 1350, 1352, and 1354 respectively.  It should be appreciated that and observation platforms 1332, 1334, and 1336 may\nhave any number of devices associated with them and will likely have more than three devices associated with them.  Devices 1338, 1340, 1342, 1344, 1346, 1348, 1350, 1352, and 1354 have all of the same features and capabilities of devices 105, 110, and\n130 of FIG. 1C.\nThe left column of environment 1300 depicts applications 1314.  The components of applications 1314 may refer to applications executing on a computer system or systems but may also refer to specific purpose devices built for the present\ntechnology.  In an embodiment where one of applications 1314 refers specifically to an application, the application is carried out on a hardware computing device or devices and may make use of cloud computing techniques.\nIn one embodiment, web portal 1302 refers to a web portal or service portal that is employed by a user to configure a local observation platform and the policies therein.  Web portal 1302 may be used to change network passwords and add remove,\nor reassign employees and their devices within the observation platform.  Web portal 1302 may also be used to create groups within the observation platform.  For example, all of the devices that are used by a department such as the plumbing department in\na hardware store may be placed in a group via the web portal 1302.\nIn one embodiment, groupings may be determined by any of the characteristics determined by the context information of the users by each observation platform(s) and aggregated in the content distribution managers, such as: location, motion,\ncommunications patters or current user activity; or by pre-assigned roles, responsibilities or functions, such as: plumbing experts, register trained employees, housekeeping, or marketing executives.  For example, a change in return policy may be\ncommunicated to all register trained employees as they arrive to the register area and must be acknowledged so that the web portal shows who has heard the message, where and when they heard the message, and where and when they verbally acknowledged the\nreceipt.\nIn one embodiment, web portal 1302 is a software application that is accessed via a uniform resource locator (URL) by any computing device that employs a web browser.  Web portal 1302 may comprise an application program interface (API) or\ngraphical interface that is employed by a user of web portal 1302.  A user of web portal 1302 may be required to provide authentication to access web portal 1302.  In one embodiment, web portal 1302 has different levels of authentication allowing\ndifferent users different levels of access each with differing levels of access or abilities to configure the observation platform.  For example, local policies of observation platform may be configured by web portal 1302.  In one embodiment, web portal\n1302 is employed by users who are locally associated with the specific observation platform as the web portal 1302 provides access and control that may only be of interest to a local user.  For example, the local user may be a technical support\nspecialist or store manager located within the physical environment associated with observation platform.  In one embodiment, the user of web portal 1302 may be a technical support specialist associated with a plurality of observation platforms and is\ncontacted by a local user of the specific observation platform and asked for assistance with configuring the specific observation platform.\nIn one embodiment, web portal 1302 may be hosted or executed on computer systems local to the specific observation platform in which web portal 1302 is used to configure.  In one embodiment, web portal 1302 is hosted or executed on computer\nsystem physically remote to an observation platform and is located in cloud services 1316.  Web portal 1302 may be located in cloud services 1316 and designed to configure any number of different observation platforms.\nIn one embodiment, content distribution manager browser 1304 is a software application that is accessed via a uniform resource locator (URL) by any computing device that employs a web browser.  Content distribution manager browser 1304 may\ncomprise an application program interface (API) or graphical interface that is employed by a user of content distribution manager browser 1304.  A user of content distribution manager browser 1304 may be required to provide authentication to access\ncontent distribution manager browser 1304.\nContent distribution manager browser 1304 is employed by a user to manage and control messages that are sent to a plurality of observation platform and the devices therein.  In one embodiment, content distribution manager browser 1304 can\nretrieve content for a message or can be employed to generate new and original content for a message.  In one embodiment, the content is an audio file such as a WAV file that is the recording of an audible voice such that when the message is delivered to\nand accessed by a destination device, the message will playback the audible voice.  The content distribution manager browser 1304 may be employed by a manager to record a voice message which is then delivered to a plurality of devices.\nIn one embodiment, a message controlled by content distribution manager browser 1304 is delivered to a plurality of devices simultaneously.  This may be accomplished by content distribution manager browser 1304 sending out the message to the\nvarious devices at the same time, or content distribution manager browser 1304 may deliver the message to a plurality of observation platforms with commands or instructions to deliver the message to specified devices within the observation platform at a\ndesignated time.  Delivering the messages to the devices may also be described as pushing the message.  The manager using the content distribution manager browser 1304 may designate the time a message should be available for the users and how long that\nmessage should be available to hear (end time).  Alternatively, the content distribution manager browser 1304 may be employed to deliver the same message to different devices at different times.  For example, the message may be delivered to store\nmanagers within observation platforms at a designated time before the message is delivered to other employees within the same observation platforms.  The user of content distribution manager browser 1304 may also specify that additional content or\nmessages are sent to different devices.  For example, additional content may be sent to store managers or additional content may be sent to devices associated with a specific department in a retail setting such as the painting department.\nIn one embodiment, content distribution manager browser 1304 is employed to specify who or what devices are to receive the message with its content.  For example, the user of content distribution manager browser 1304 may have authority over\nseveral different environments each with its own observation platform.  The user may wish that the message only be sent to specified observation platforms within the plurality of observation platforms.  Alternatively, the user may specify that all of the\ndevices within all of the observation platforms receive the message, or only devices located within the physical boundaries of the observation platform at the designated time receive the message, or only devices associated with a specific department\nreceive the message or only devices associated with store employees and not customers receive the message.  The possible options for specifying which devices receive a message and when are limitless.  A message may also be generated and sent to a\nspecific individual.  In one embodiment, content distribution manager browser 1304 employs the groups created by web portal 1302 to determine which devices a message may be sent to.  It should be appreciated that the content of the message may be a voice\nrecording but may also be other content such as text, images, or video.  In one embodiment, the message is sent to a given device with a command to notify the user of the device that there is a message received.  The notification may be a light, a\nblinking light, a specific color of light, a sound, a textual notification, or any other type of notification that the device is capable of providing.\nContent distribution manager browser 1304 may be employed by a user that has high level access to the plurality of observation platforms.  For example, a corporation may have hundreds or thousands of hospitality locations or store fronts that\neach makes use of an observation platform.  The corporation may have a headquarters or central office with employees who have access to content distribution manager browser 1304 with the ability and authority to send a message to anyone and everyone\nassociated with the corporation.\nIn one embodiment, a device that receives a message from content distribution manager browser 1304 automatically sends a confirmation back to content distribution manager browser 1304 that the message has been received.  Additionally, once the\nmessage has been accessed or heard by the user of the device, the device may send a message back to content distribution manager browser 1304 that the messaged has been heard or otherwise accessed.  In one embodiment, the message may be a mandatory\nmessage that the user of the device is required to access and listen to.  For example, process 1200 herein describes various embodiments of mandatory messages and consequences, rules or policies associated with mandatory messages.\nIn one embodiment, manager application 1306 a software application or app that is accessed via a mobile computer system such as a smart phone or tablet.  In one embodiment, the mobile computer system executes an Android operating system.  In one\nembodiment, the mobile computer system executes an iOS operating system.  Other operating systems may also be employed.  Manager application 1306 may be an app available for download and installation on the mobile computer system.  The manager\napplication 1306 is designed with an API or graphical interface specific to a mobile computer system such as a smart phone and to be used in the field by a user or manager associated with at least one observation platform.  The user of manager\napplication 1306 may be a regional manager that has access to a plurality of observation platforms.  The regional manager may regularly travel between the physical locations of the plurality of observation platforms and needs to have access to the\nobservation platforms while physically remote and in the field on the go.\nIn one embodiment, manager application 1306 allows the user of manager application 1306 to communicate with or monitor any device or plurality of devices within any of the observation platforms associated with the user.  Manager application 1306\nalso is able to report statistics or observe or monitor communications with any of the observation platforms associated with the user.  For example, the manager application 1306 may be able to listen in to communications happening in real time within an\nobservation platform or may be able to play back past recorded communications.  In one embodiment, the manager application may operate in a manner identical to the mobile devices in the observation platform as a peer-like device.  In this mode the\nmanager application may broadcast or direct communications to specific devices, receive alerts and provide both a primary signal for communication and a secondary signal for determining geographic location.  In one embodiment, the peer-like device may be\nable to operate and interact with devices within an observation platform without directly communicating with a central computer system.  In other words, the central computer system may or may not be required for receiving and relaying messages from the\nmanager application.  The manager application 1306 may also be employed to send announcements or messages similar to content distribution manager browser 1304.  The manager application 1306 may communicate directly through a network with a given\nobservation platform or may use cloud services 1316 and gateway 1326 to communicate with a given observation platform.\nIn one embodiment, log-on tool 1308 is a software application that is accessed via a uniform resource locator (URL) by any computing device that employs a web browser.  Log-on tool 1308 may comprise an application program interface (API) or\ngraphical interface that is employed by a user of log-on tool 1308.  In one embodiment, log-on tool 1308 is employed by a user of a device in an observation platform that is having trouble logging onto the observation platform verbally.\nIn one embodiment, support manager 1310 is a software application that is accessed via a uniform resource locator (URL) by any computing device that employs a web browser.  Support manager 1310 may comprise an application program interface (API)\nor graphical interface that is employed by a user of support manager 1310.  Support manager 1310 is a tool for a user to see network and system performance metrics, observe who was talking to whom, hear all or selected communications, respond in\nreal-time with spoken responses, collect and play messages, and/or make and summarize real-time performance/user activities to determine trends or generate alarms Support manager 1310 also keeps track of all devices within the observation platform as\nwell as technical details such as the connection point a device is using, where the device is located within the physical boundaries of the observation platform, battery state of the device and if charged, where user has volume set for a device,\nenvironmental noise and sounds, and tracks communications and messages.  In one embodiment, support manager 1310 has an alarming feature that mines relational database 1318 in the cloud and creates alarms or manipulates relational database to create\nalarms after detecting an anomaly.  It should be appreciated that support manager 1310 is designed to be used by a store help desk so they know what is going on in a technical way, a support center supporting a plurality of observation platforms, or an\nescalation center capable of troubleshooting and correcting more complex issues.\nIn one embodiment, third party apps and computer systems 1312 refers to software applications and computer system created by third parties to be implemented within the observation platform(s).  For example, a third party may create an\napplication that makes use of the data generated by the observation platform.  Such software may then sends results, reports, alarms, or other data back to the users within the observation platform or to the web portal for decision support, action\nplanning or coordinating immediate actions within the observation platform(s).\nIn one embodiment, third party apps and computer systems 1312 refers to other software applications and computer system created by third parties to be implemented within the observation platform(s).  This third party application may make use of\nexternally derived data that is relevant to the operation of the enterprise which needs to be communicated to groups of observation platforms or groups of uses in near-real-time per the policies of the enterprise.  For example, door counter or video\nderived traffic information may be used to alert a group of uses within an observation platform that action is necessary to improve coordination on the floor.  Additionally, this third-party application information may be aggregated within the content\ndistribution manager so that managers, district managers, regional managers or enterprise executives may overview patterns of operations between a plurality of observation platforms.\nIn one embodiment, cloud services 1316 refers to cloud computing techniques and hardware that may be employed in cloud configuration 1328 to assist observation platforms and embodiments of the present technology.  Relational database 1318 may be\nlocated in cloud services 1316 and comprises data generated and related to one or more observation platforms.  Platform configurations 1320 refers to configurations that may be default configurations and policies to be implemented and used by a specific\nobservation platform.  Message scheduler 1322 and message broker 1324 may be software applications located in cloud services 1316 and employed by content distribution manager browser 1304 and manager application 1306 as well as other applications in\napplications 1314 to send and/or receive messages to/from devices located in observation platforms 1330.  Gateway 1326 is a security gateway located in cloud services 1316 and is employed to ensure that only authorized programs and users have access to\nthe observation platforms.\n<BR><BR>Operations of Using Structured Communications in an Observation Platform and Content Distribution and Data Aggregation for Scalability of Observation Platforms\nFIG. 3 is a flowchart illustrating process 300 for using structured communication in an observation platform in accordance with one embodiment of the present technology.  Process 300 may also be described as disciplining communications in an\nobservation platform.  In one embodiment, process 300 is a computer implemented method that is carried out by processors and electrical components under the control of computer usable and computer executable instructions.  The computer usable and\ncomputer executable instructions reside, for example, in data storage features such as computer usable volatile and non-volatile memory.  However, the computer usable and computer executable instructions may reside in any type of non-transitory computer\nusable storage medium.  In one embodiment, process 300 is performed by the components of FIG. 1A, 1B, 1C or 2.  In one embodiment, the methods may reside in a computer usable storage medium having instructions embodied therein that when executed cause a\ncomputer system to perform the method.\nAt 302, a signal from a first communication device is received at a second communication device associated with a computer system, wherein a first characteristic of the signal corresponds to an audible source and a second characteristic of the\nsignal corresponds to information indicative of a geographic position of the first communication device.  Additional characteristics of the signal may include contextual information and environmental information.  For example, the audible source may be\nthe voice of a user, the signal characteristics may include signal signature information and contextual/environmental information may include user status (e.g., engaged or on-break) and/or background noise levels.\nAt 304, a first user associated with the first communication device is recognized at the computer system.\nAt 306, context information for the signal is derived at the computer system associated with the second communication device.  The context information may be geographic information, data regarding length or time of communication, or text of the\ncommunication.  In one embodiment, speech to text recognition techniques are employed to covert an audible communication to text.  In one embodiment, the context information is a command for the computer system to perform.  In one embodiment, the signal\nis encrypted or encoded uniquely with respect to the first communication device.  The context information may be a command to the computer system.  For example the computer system may be commanded to access a database in response to a query or may be\ngiven information to store for future reference.\nIn one embodiment, the information is a command and the command may be issued verbally by a user in a communication.  For example, a user may speak into a communication device the phrase \"hello everybody\" thus the communication is the spoken\nphrase and the computer system may derive that the communication is to be sent to everybody.  The computer system then relays the signal to everybody associated with the communication observation platform.  In another example, the communication may be\nthe phrase \"hello Bob.\" The computer system derives that the destination of the communication is Bob; the communication is then relayed only to Bob.\nThe Table below shows examples of Communication Phrases and Derived Context information.  Specific examples using sample vocabulary are given as well as more general cases indicated by the brackets [ ].\nTABLE-US-00001 Communication Phrase Derived Context Information \"Hello Everybody\" The communication is to be relayed to a Hello [Group] group defined as \"everybody\" and anyone may respond.  Context information such as \"engaged\" may limit those\nwho hear and may respond to the \"Hello\" phrase.  \"Hello Bob\" The communication is to be relayed to an Hello [Person] individual identified as \"Bob\" and only \"Bob\" hears the message and is able to respond.  Context information such as \"engaged\" may result\nin the computer providing additional information to the caller such as the state of the user (e.g., \"engaged\") and other factors such as location.  \"Hello Workshop\" The communication is to be relayed to Hello [Location] everyone associated with the\n\"Workshop\" location.  Context information such as \"engaged\" may limit those who hear and may respond to the \"Hello\" phrase.  \"Hello Process Experts\" The communication is relayed to all identified Hello [Group] as the group, \"Process Experts.\" These\npeople or machines may be physically located in any region or environment.  Context information such as \"engaged\" may limit those who hear and may respond to the \"Hello\" phrase.  \"Urgent Bob\" or The communication is an urgent \"Interrupt Bob\"\ncommunication to be relayed to \"Bob.\" Such Interrupt [Person] a command may interrupt \"Bob\" if he is Interrupt [Group] \"engaged\" or communicating with others or Interrupt [Location] the system as defined by the operator of the environment.  Once\ninterrupted, communication is between the caller and original user (i.e., Bob) and may or may not include others who may have been talking with Bob at the time.  \"Message Bob\" Leaves a message that persists for a pre- Message [Person] determined\ninterval.  Messages for groups Message [Group] are heard as persons become available.  Message [Location] Messages for locations are heard as persons become available or enter the location area.  Special cases for `messages\" include delivering audio\ninformation to groups such as Marketing Departments, Buyers, Help Desks, Websites, Technical Support or Product improvement requests.  \"Announcement The communication is to be relayed to Everybody\" \"everyone\" as a bulletin.  Those users who Announcement\n[Group] are engaged or not yet on the system will hear the bulletin when they become available.  \"Selling tip for the side The communication is to be relayed to those hallway\" who are within or enter the side hallway as Announcement an announcement.  No\nresponse is [Location] anticipated.  \"Absolute The communication is delivered to all who Announcement for are available and in the proper context.  A Maintenance Team\" response is mandatory.  The system records Absolute Announcement the time, location,\nuser and spoken response [Group] or [Location] or for later analysis or storage.  [Person] \"Where is Steve\" The communication is a command to Where is [Person] determine a geographic location of Steve Where is [Group] and to send a message back to the\ncommunication device from the computer system that speaks the response.  The response may also include contextual information such as \"Steve is available\" or Steve is engaged\" or other information from other sources such as \"Steve is on break.\" Steve\ndoes not need to hear that his status was being probed, although it is possible to alert him.  \"Who is near the central The communication is a command to hallway\" determine who is geographically located near Who is near [Location] the central hallway\nregion and to send a message back to the communication device from the computer system that speaks the response.  The response may include additional contextual information for the persons in that location.  \"Go to simple menu\" The communication is a\ncommand for the Command [profile] computer system to go to the simple menu profile and to send a message back that speaks the phrase \"you will now go to simple menu.\" This feature allows individual users to move into different command, control and skill\nlevel profiles within the system.  \"Does anyone know if Some formats of commands are natural to we have .  . .?\" the users, but not is a structured speech Spoken String pattern.  In this case, the words, \"Does anyone know .  . .\" may trigger the computer\nto send this message to group of people who know where things are.  Additional contextual information may limit that group to a department or location.\nThe phrase \"Go to simple menu\" may be a command to enter a different menu structure for such activities as new-user learning, learning about products or business, listening to communications, or set-up functions such as group participation and\ndefault settings for the individual.\nAt 308, a geographic location of the first communication device is determined based on the second characteristic of the signal and at least one other source of information.  For example, the at least one other source of information may be a\nrouter that the signal is routed through, a signal strength of the signal, information from the second communication device, etc.\nAt 310, a copy of at least one characteristic of the signal is stored in a storage medium and is made available for performance metric analysis.  In one embodiment, the performance metrics are key performance metrics.  At least one\ncharacteristic may be, but is not limited to, a time stamp, engaged, available status, a message, a voice file, a location, a signal signature, a type of message, text corresponding to a message, commands used to initiate the message, other contextual\ninformation about the user and an identity of the path the signal was routed through.\nAt 312, instructions are received at the computer system comprising rules for the relaying the signal to the destination derived from the context information.  The rules may instruct to whom and to how the communication is to be relayed.  For\nexample, information derived from a communication may command that the communication be sent to everyone associated with the geographic location of \"Workshop.\" However, the rules may instruct that the communication is only relayed to those associated\nwith the \"Workshop\" who are designated as available or not busy.  The rules may also comprise a predetermined time or a lifetime in which a response may be relayed to an available communication device.\nAt 314, the signal is relayed to a destination derived from the context information.  The destination may be another user or a plurality of user or the computer system itself.  The destination may be located outside of a radio range associated\nwith the second communication device or be otherwise physically remote relative to the second communication device.\nAt 316, a data entry and visual representation is generated indicating the geographic position of the first communication device with respect to a geographic environment in which the first communication device is located.  For example, the\nvisual representation may be a map depicting the location of users or where users have been.  The data entry and visual representation may include a status indicator of the user such as whether the user is busy or available.\nFIG. 4 is a flowchart illustrating process 400 for using a structured communication in an observation platform in accordance with one embodiment of the present technology.  In one embodiment, process 400 is a computer implemented method that is\ncarried out by processors and electrical components under the control of computer usable and computer executable instructions.  The computer usable and computer executable instructions reside, for example, in data storage features such as computer usable\nvolatile and non-volatile memory.  However, the computer usable and computer executable instructions may reside in any type of non-transitory computer usable storage medium.  In one embodiment, process 400 is performed by the components of FIG. 1A, 1B,\n1C or 2.  In one embodiment, the methods may reside in a computer usable storage medium having instructions embodied therein that when executed cause a computer system to perform the method.\nAt 402, a signal from a first communication device is received at a second communication device, wherein a first characteristic of the signal corresponds to a voice of a first user and a second characteristic of the signal corresponds to\ninformation indicative of a geographic position of the first communication device.  Additional characteristics of the signal may include contextual information and environmental information.  For example, the audible source may be the voice of a user,\nthe signal characteristics may include signal signature information and contextual/environmental information may include user status (e.g., engaged or on-break) and/or background noise levels.\nAt 404, the first user associated with the first communication device is recognized.\nAt 406, text or machine code related to the voice of the first user is recognized.\nAt 408, context information from the text or machine code is derived at a computer system associated with the second communication device, wherein the context information corresponds to a command related to the text or machine code.\nAt 410, the text or machine code is stored in a storage medium for developing performance metrics.\nAt 412, the signal is relayed to a destination derived from the context information.  The destination may be located outside of a radio range associated with the second communication device or be otherwise physically remote relative to the\nsecond communication device.\nFIG. 5 is a flowchart illustrating process 500 for observing and recording users of communication devices in accordance with one embodiment of the present technology.  In one embodiment, process 500 is a computer implemented method that is\ncarried out by processors and electrical components under the control of computer usable and computer executable instructions.  The computer usable and computer executable instructions reside, for example, in data storage features such as computer usable\nvolatile and non-volatile memory.  However, the computer usable and computer executable instructions may reside in any type of non-transitory computer usable storage medium.  In one embodiment, process 500 is performed by the components of FIG. 1A, 1B,\n1C or 2.  In one embodiment, the methods may reside in a computer usable storage medium having instructions embodied therein that when executed cause a computer system to perform the method.\nIn one embodiment, process 500 is a management observation tool for keeping track of mobile human resources and collecting data on their activities.\nAt 502, a first user associated with a first communication device and a second user associated with a second communication device are recognized at a central computer system.\nAt 504, geographic locations of the first communication device and the second communication device are tracked at the central computer system.  In one embodiment, tracking means storing data about location and any spoken information.\nAt 506, a communication between the first communication device and the second communication device are tracked and recorded at the central computer system, wherein at least a portion of the communication is an audible communication.\nAt 508, features of the communication are identified at the central computer system.  Features may be described as characteristics or data regarding the communication itself.  The features may be user status such as engaged/available, location\nof a user, communication history of the user, context of the communication, keywords used in the communication, a classification of the communication, and time stamps.\nAt 510, the features are made available to a manager, operations staff or operations machines for making decisions or informing the users that new actions are requested.\nFIG. 6 is a flowchart illustrating process 600 for characterizing communications in a group of users in accordance with one embodiment of the present technology.  In one embodiment, process 600 is a computer implemented method that is carried\nout by processors and electrical components under the control of computer usable and computer executable instructions.  The computer usable and computer executable instructions reside, for example, in data storage features such as computer usable\nvolatile and non-volatile memory.  However, the computer usable and computer executable instructions may reside in any type of non-transitory computer usable storage medium.  In one embodiment, process 600 is performed by the components of FIG. 1A, 1B,\n1C or 2.  In one embodiment, the methods may reside in a computer usable storage medium having instructions embodied therein that when executed cause a computer system to perform the method.\nAt 602, a group of users is recognized, at a computer system, wherein each user of the group of users are associated with communication devices.  The group of users may be recognized based on a classification of the user or a geographic location\nof the users.  For example, a classification of the users may be whether the user is an associate or a customer in a retail setting.\nAt 604, a communication between the communication devices is recorded at the computer system, wherein at least a portion of the communication is an audible communication.  In one embodiment, at least a portion of the communication is a\npre-recorded audible communication.\nAt 606, geographic locations of the communication devices are recorded at the computer system.  The location may be determined based on signal signatures or other \"clues\" from other devices sent periodically or with the communication indicative\nof the location.\nAt 608, features are identified based upon the communication.  Features may be described as characteristic or data regarding the communication itself.  The features may be a user status such as engaged/available, location of a user,\ncommunication history of the user, context of the communication, a classification of the communication, a frequency of communications between two users, a length of a communication, keywords used in the communication, a response time to a communication\nand time stamps.\nAt 610, a visual representation of the features is generated at the computer system.  The visual representation may depict when a user of said group of users is engaged in said communication, when a user of said group of users asks a question in\nsaid communication, who responds to the question, where each user of said group of users are located, and where said group of users are not located.  Alerts, either visual or verbal, may be generated depending on the rules established by the system\noperators.\nAt 612, the visual representation is made available to a manager, operations staff or operations machines for making decisions or informing the users that new actions are requested.\nFIG. 7 is a flowchart illustrating process 700 for using structured communication in a plurality of observation platforms in accordance with one embodiment of the present technology.  Process 700 may also be described as disciplining\ncommunications in an observation platform.  In one embodiment, process 700 is a computer implemented method that is carried out by processors and electrical components under the control of computer usable and computer executable instructions.  The\ncomputer usable and computer executable instructions reside, for example, in data storage features such as computer usable volatile and non-volatile memory.  However, the computer usable and computer executable instructions may reside in any type of\nnon-transitory computer usable storage medium.  In one embodiment, process 700 is performed by the components of FIG. 1A, 1B, 1C or 2.  In one embodiment, the methods may reside in a computer usable storage medium having instructions embodied therein\nthat when executed cause a computer system to perform the method.\nAt 702, a signal in a first observation platform is received from a first communication device at a second communication device associated with a computer system wherein a first characteristic of the signal corresponds to an audible source and a\nsecond characteristic of the signal corresponds to information indicative of a geographic position of the first communication device, and wherein the second observation platform is associated with a radio range.  Additional characteristics of the signal\nmay include contextual information and environmental information.  For example, the audible source may be the voice of a user, the signal characteristics may include signal signature information and contextual/environmental information may include user\nstatus (e.g., engaged or on-break) and/or background noise levels.\nAt 704, a first user associated with the first communication device is recognized at the computer system.\nAt 706, context information for the signal is derived at the computer system associated with the second communication device.  The context information may be geographic information, data regarding length or time of communication, or text of the\ncommunication.  In one embodiment, speech to text recognition techniques are employed to covert an audible communication to text.  In one embodiment, the context information is a command for the computer system to perform.  In one embodiment, the signal\nis encrypted or encoded uniquely with respect to the first communication device.  The context information may be a command to the computer system.  For example the computer system may be commanded to access a database in response to a query.\nAt 708, the signal is relayed from the computer system to a second computer system associated with a second observation platform via a computer network\nAt 710, the signal is relayed to a destination in the second observation platform via the second computer system derived from said context information.\nFIG. 8 is a flowchart illustrating process 800 for performing structured communications in an observation platform in accordance with one embodiment of the present technology.  Process 800 may also be described as disciplining communications in\nan observation platform.  In one embodiment, process 800 is a computer implemented method that is carried out by processors and electrical components under the control of computer usable and computer executable instructions.  The computer usable and\ncomputer executable instructions reside, for example, in data storage features such as computer usable volatile and non-volatile memory.  However, the computer usable and computer executable instructions may reside in any type of non-transitory computer\nusable storage medium.  In one embodiment, process 800 is performed by the components of FIG. 1A, 1B, 1C or 2.  In one embodiment, the methods may reside in a computer usable storage medium having instructions embodied therein that when executed cause a\ncomputer system to perform the method.\nAt 802, a signal is received from a first communication device at a second communication device associated with a computer system, wherein the computer system is associated with an organization, wherein a first characteristic of the signal\ncorresponds to an audible source and a second characteristic of the signal corresponds to information indicative of a geographic position of the first communication device.  Additional characteristics of the signal may include contextual information and\nenvironmental information.  For example, the audible source may be the voice of a user, the signal characteristics may include signal signature information and contextual or environmental information may include user status (e.g., engaged or on-break)\nand/or background noise levels.  The organization may be a retail environment, a school, an event, a military organization, a prison organization, customer service, manufacturing organization, a factory, a disaster response team, or any environment where\nhumans interact with one another to accomplish a purpose.  The first communication device may be a handheld device that is capable of sending and receiving signals and may comprise a display, a microphone and a speaker.  The first communication device\nmay be owned by the organization and issued to the user or may be the user's personal property such as a smart phone executing an application.  The second communication device may be a radio base station as described herein.\nAt 804, a user is identified as associated with the first communication device at the computer system.  In one embodiment, 804 only identifies that there is a user employing the communication device.  The actual identity of the user may remain\nanonymous to the computer system or the user may be identified.  The user may be identified using one or a combination of several different techniques.  The user may be identified via a unique signature of the communication device associated with the\nuser.  For example, the user's communication device may be a smart phone running an application.  The smart phone may be the user's personal property and is always associated with the user.  In one embodiment, the user may be authenticated upon\nactivation of the communication device or the application.  For example, a user may enter an environment, activate a communication device and then give user credentials that identify the user.  This may accomplished via voice commands or text inputs.  In\none embodiment, the user credentials are associated with a user profile, but the actual identity of the user remains anonymous.  In one embodiment, the user may activate a communication device and self-identify.  Identifying a user may be automatic\ntaking place without the user's knowledge, or may require the user to acknowledge or give permission for the computer system to identify the user.\nAt 806, the audible source of the signal is converted to text or machine understandable language at the computer system.  This may occur using speech-to-text techniques or other techniques employed by computer systems.\nAt 808, a query related to the organization is derived based on the text or understanding at the computer system.  The query may be any number of queries from the user.  The user may ask for general assistance or may ask a more specific question\nsuch as whether an item is in stock, where an item is located, what sales are taking place, technical details or features regarding an item.\nAt 810, a response to the query is compiled at the computer system, wherein the response represents the organization.  For example, the response relates to the purpose of the organization.  In one embodiment, the response is regarding a location\nor status of a person or an item within the organization.  The computer system may access a database to complete the response.  The database maybe a local database such as an inventory of a local store, or may access a database in part of a larger\nnetwork associated with the organization, or may access a database associated with the Internet.  In one embodiment, the computer system performs a key word search of the Internet using a search engine to complete the response.\nAt 812, the response is sent to the first communication device, wherein the response is audible at the first communication device.  In one embodiment, the response is initially a text response that is converted from text to speech.  The\nconversion may occur at the computer system such that a signal with an audible portion is sent to the first communication device, or a text message may be sent to the first communication device where it is converted to speech.  The response may be\nrecorded by the organization in a computer system and may also be sent to a person associated with the organization such as a manager or associated.  Thus, a person associated with the business may monitor the responses of the computer system and may be\naware of the needs or requirements of the user associated with the first communication device.\nAt 814, a prior user history of the user is associated with the first communication device.  The user history may be a user profile that may or may not identify the user.  The history may have a list of all the transactions of this user\nassociated with the organization.  The history may also comprise information provided by the user such as likes and dislikes or preferences regarding which person the user wishes to be served by while in the organization.\nAt 816, the signal and the response are relayed to a third communication device associated with a person representing the organization.  The person associated with the organization may be a consultant, an employee, an associate, a sales\nassociate, a civil servant, a volunteer or a manager.  The third communication device may be a handheld device and may or may not be the same type of device as the first communication device.\nAt 818, a second response is received at the second communication device from the third communication device.  For example, the person representing the organization may respond using a signal that may have an audible voice portion a text portion\nor both.\nAt 820, the second response is relayed to the first communication device.  The computer system may initiate a virtual voice connection between the first communication device and the second communication device.\nFIG. 9 is a flowchart illustrating process 900 for performing structured communications in an observation platform in accordance with one embodiment of the present technology.  Process 900 may also be described as disciplining communications in\nan observation platform.  In one embodiment, process 900 is a computer implemented method that is carried out by processors and electrical components under the control of computer usable and computer executable instructions.  The computer usable and\ncomputer executable instructions reside, for example, in data storage features such as computer usable volatile and non-volatile memory.  However, the computer usable and computer executable instructions may reside in any type of non-transitory computer\nusable storage medium.  In one embodiment, process 900 is performed by the components of FIG. 1A, 1B, 1C or 2.  In one embodiment, the methods may reside in a computer usable storage medium having instructions embodied therein that when executed cause a\ncomputer system to perform the method.\nAt 902, a signal is received from a first communication device at a second communication device associated with a computer system, wherein the computer system is associated with an organization, wherein a first characteristic of the signal\ncorresponds to an audible source and a second characteristic of the signal corresponds to information indicative of a geographic position of the first communication device.  Additional characteristics of the signal may include contextual information and\nenvironmental information.  For example, the audible source may the voice of a user, the signal characteristics may include signal signature information and contextual or environmental information may include user status (e.g., engaged or on-break)\nand/or background noise levels.  The organization may be a retail environment, a school, an event, a military organization, a prison organization, customer service, manufacturing organization, a factory, a disaster response team, or any environment where\nhumans interact with one another to accomplish a purpose.  The first communication device may be a handheld device that is capable of sending and receiving signals and may comprise a display, a microphone and a speaker.  The first communication device\nmay be owned by the organization and issued to the user or may be the user's personal property such as a smart phone executing an application.  The second communication device may be a radio base station as described herein.\nAt 904, a user is identified as associated with the first communication device at the computer system.  The actual identity of the user may remain anonymous to the computer system or the user may be identified.  The user may be identified using\none or a combination of several different techniques.\nAt 906, a query is derived from the signal of the first device related to the organization, at the computer system.  The query may be any number of queries from the user.  The user may ask for general assistance or may ask a more specific\nquestion such as whether an item is in stock, where an item is located, what sales are taking place, technical details or features regarding an item or requesting general assistance.\nAt 908, a person representing the organization is determined to respond to the query, wherein the determining is based on a factor related to the person representing the organization.  The factor may also be described as a characteristic.  The\nfactor may be related to the query from the user.  For example, the user may ask a question regarding an item in a given department.  The determining may be based on who is associated with the given department.  The factor may also be based on the status\nof the person, the availability of the person, the proximity of the person to the user, geographic location of the person, knowledge level of the person, authority level of the person, ability of the person, or a combination of factors.  The determining\nmay determine that a plurality of persons qualify to respond.  The signal may then be forwarded to one of the plurality, a subset of the plurality, or all of the plurality of persons.\nAt 910, the signal is forwarded to a third communication device associated with the person representing the organization.\nAt 912, a determination that no response has been received at the second communication device from the third communication device is made.  912 may occur after 910 in an embodiment where 916 and 918 do not occur.  However, 912, 914, 916 and 918\nmay all occur in one embodiment.  Such determination may occur after a pre-determined time period has passed with no response from the third communication device.  Such a determination may or may not preclude the third communications device from later\nresponding.\nAt 914, the signal is forwarded to a fourth communication device associated with the person representing the organization.  912 and 914 may be repeated forwarding the signal to additional communication devices until it is determined that a\nperson representing the organization has responded via a communication device.  Alternatively, 910 and 914 may forward the signal to a plurality of communication devices associated with a plurality of persons representing the organization.  Once any one\nof the plurality of persons responds, the person and the user may be placed into a communications channel via their communications devices.  The communications channel may be private in the sense that the other members of the plurality of persons\nrepresenting the organization do not hear subsequent communications over the communications channel.  This may be accomplished via the computer system associated with the second communications device.  The subsequent communications may all be relayed or\nforwarded between the user and the person representing the organization via the second communication device and the associated computer system.  In one embodiment, the communication channel is open to all members of the plurality of persons representing\nthe organization.  In one embodiment, the communication channel is open to a subset group of the plurality of persons representing the organization.  For example, the subset group may be only persons who are determined by the computer system to have\nknowledge regarding the query made by the user or may only be persons who are determined to be available, or persons who have interest in learning more about the subject, or some combination of these characteristics.\nBy forwarding the signal to a fourth communication device or a plurality of other devices, the circle or group of those required or enlisted to help the user is enlarged.  In other words, the user may send a communication or query indicating\nthat the user is in need of assistance.  The computer system determines a first person is to assist the user, but if the first person doesn't respond, the computer system then determines a second person or a plurality of persons to assist the user.  Thus\nthe group of those responding to the assistance need increases.  In one embodiment, the initial communication from the first user may go to a designated plurality and the first person to respond becomes established in a private one-on-one conversation\nwith the first (originating) user.\nAt 916, a response is received from the third communication device at the computer system.  916 may occur after 910 in an embodiment where 912 and 914 do not occur.\nAt 918, the response is forwarded to the first communication device.  918 may occur after 916 in an embodiment where 912 and 914 do not occur.  Process 900 may initiate a virtual voice connection between two communication devices where the\ncommunication is relayed or forwarded via the computer system and the second communication device.  Thus the computer system and the second communication device may be described as mediating the communications.\nFIG. 10 is a flowchart illustrating process 1000 for performing structured communications in an observation platform in accordance with one embodiment of the present technology.  Process 1000 may also be described as disciplining communications\nin an observation platform.  In one embodiment, process 1000 is a computer implemented method that is carried out by processors and electrical components under the control of computer usable and computer executable instructions.  The computer usable and\ncomputer executable instructions reside, for example, in data storage features such as computer usable volatile and non-volatile memory.  However, the computer usable and computer executable instructions may reside in any type of non-transitory computer\nusable storage medium.  In one embodiment, process 1000 is performed by the components of FIG. 1A, 1B, 1C or 2.  In one embodiment, the methods may reside in a computer usable storage medium having instructions embodied therein that when executed cause a\ncomputer system to perform the method.\nAt 1002, a signal is received from a first communication device at a second communication device associated with a computer system, wherein the computer system is associated with an organization, wherein a first characteristic of the signal\ncorresponds to an audible source and a second characteristic of the signal corresponds to information indicative of a geographic position of the first communication device.  Additional characteristics of the signal may include contextual information and\nenvironmental information.  For example, the audible source may the voice of a user, the signal characteristics may include signal signature information and contextual or environmental information may include user status (e.g., engaged or on-break)\nand/or background noise levels.  The organization may be a retail environment, a school, an event, a military organization, a prison organization, customer service, manufacturing organization, a factory, a disaster response team, or any environment where\nhumans interact with one another to accomplish a purpose.  The first communication device may be a handheld device that is capable of sending and receiving signals and may comprise a display, a microphone and a speaker.  The first communication device\nmay be owned by the organization and issued to the user or may be the user's personal property such as a smart phone executing an application.  The second communication device may be a radio base station as described herein.\nAt 1004, a user is identified as associated with the first communication device at the computer system.  The actual identity of the user may remain anonymous to the computer system or the user may be identified.  The user may be identified using\none or a combination of several different techniques.\nAt 1006, a query is derived from the signal related to the organization, at the computer system.  The query may be any number of queries from the user.  The user may ask for general assistance or may ask a more specific question such as whether\nan item is in stock, where an item is located, what sales are taking place, technical details or features regarding an item.\nAt 1008, a plurality of persons representing the organization are determined to respond to the query, wherein the determining is based on a factor related to the plurality of persons representing the organization.\nAt 1010, the signal is forwarded to a plurality of communication devices associated with the plurality of persons representing the organization.  Such a series of communications may be described as a one-to-many communication.  The \"many\" group\nmay be default or predefined group such as all those associated with a given department or all those who are associated with a given area of expertise.  Groups may also be created based on names, locations, availability or status.\nAt 1012, a response is received from a communication device associated with one of the plurality of persons representing the organization at the second communication device.\nAt 1014, the response is forwarded from the second communication device to the first communication device.  Thus the communication may go from a one-to-many to a one-to-one communication.\nAt 1016, a communication channel is opened between the first communication device and the communication device associated with one of the plurality of persons.  In other words, the communication from the first (originating) user is sent to\nmultiple persons.  The first person to respond enters into a communication channel between the first communication device and the communication device associated the person.  Others who respond within a pre-determined timeframe are also included in the\n\"channel.\" The communication channel may be mediated by the computer system and once all users have entered, may not be overheard by the other persons from the plurality of persons.  The usefulness of this structure is that it allows ad-hoc group\nconstruction by simply announcing the intent of the group, and only those responding are tied into the private group \"channel\".\nIn one embodiment, the communication may go from a one-to-many to a one-to-few communication.  The persons in the few of the one-to-few communication may be a subset of the many persons from the one-to-many.  For example, the initial\ncommunication may be sent to all those persons holding communication devices.  The computer system may then open a communication channel between the first person to respond where the channel is also opened to others person representing the store who are\nassociated with a specific role or department.  Thus only one person may be actively communicating with the user, but other persons may hear the communications and may join at any time.  Thus the communication may not disrupt those who are otherwise not\ninterested.\nFIG. 11 is a flowchart illustrating process 1100 for sending notifications in an observation platform in accordance with one embodiment of the present technology.  Process 1100 may also be described as disciplining communications in an\nobservation platform.  In one embodiment, process 1100 is a computer implemented method that is carried out by processors and electrical components under the control of computer usable and computer executable instructions.  The computer usable and\ncomputer executable instructions reside, for example, in data storage features such as computer usable volatile and non-volatile memory.  However, the computer usable and computer executable instructions may reside in any type of non-transitory computer\nusable storage medium.  In one embodiment, process 1100 is performed by the components of FIG. 1A, 1B, 1C or 2.  In one embodiment, the methods may reside in a computer usable storage medium having instructions embodied therein that when executed cause a\ncomputer system to perform the method.\nAt 1102, a signal is received from a first communication device at a second communication device associated with a computer system, wherein the computer system is associated with an organization, wherein a first characteristic of the signal\ncorresponds to information indicative of a geographic position of the first communication device.  The organization may be a retail environment, a school, an event, a military organization, a prison organization, customer service, manufacturing\norganization, a factory, a disaster response team, or any environment where humans interact with one another to accomplish a purpose.  The first communication device may be a handheld device that is capable of sending and receiving signals and may\ncomprise a display, a microphone and a speaker.  The first communication device may be owned by the organization and issued to the user or may be the user's personal property such as a smart phone executing an application.  The second communication\ndevice may be a radio base station as described herein.\nAt 1104, a user is identified as associated with the first communication device at the computer system.  The actual identity of the user may remain anonymous to the computer system or the user may be identified.  The user may be identified using\none or a combination of several different techniques.\nAt 1106, a history of activities of the user associated with the organization is accessed.  The history of activities may be a user history or user profile that may or may not identify the user.  The history may have a list of all the\ntransactions of this user associated with the organization.  The history may also comprise information provided by the user such as likes and dislikes or preferences regarding which person the user wishes to be served by while in the organization.  The\ncomputer may attempt to find the preferential associate(s) and notify them that the shopper is in the store and where they are located.  The associates contacted may hear prior conversations with that shopper to refresh their memory and aid in making the\nshopper experience seamless.\nAt 1108, a geographic location of the first communication device in the organization is derived at the computer system.  For example, the computer system may determine that the user is on a given aisle such as the cereal aisle in a grocery store\nor in a zone that may correlate to a department such as the lumber department in a hardware store.\nAt 1110, a notification is sent to the first communication device wherein the notification is based on the history of activity and the geographic location of the first communication device.  For example, the notification may alert the user of a\ncoupon or special on a given item in the organization that is for sale.  The coupon or special may be for an item that the user previously purchased which knowledge was obtained by the computer system based on the history of user activity.  The\nnotification maybe any number of notifications including a text message or an audible message and the notification may be accompanied by an alert such as a vibration or an audible sound.  The history of activity may be utilized to automatically connect\ncommunications from the user to a person with whom the user has prior interactions.\nAt 1112, at least a portion of the history of activities is delivered to the first communication device.  Such information may be used the user to determine what items the user previously purchased.  For example, the user may wish to purchase\nthe same item again but does not remember the exact details of the item or the user may wish to avoid purchasing the same item.  The user may also use the information to identify a person representing the organization with whom the user wishes to\ninteract with again.  For example, the user may have had a pleasant experience with a given sales associate and know that sales associate can meet the user's needs.  In one embodiment, step 1112 is not performed as part of process 1100.\nProcess 1100 may be used in conjunction with a loyalty program involving lotteries or coupons that may be in existence before the communications platform is implemented in the organization or may be created based on the communications platform\nor a combination of the two.\nFIG. 12 is a flowchart illustrating process 1200 for performing structured communications in an observation platform in accordance with one embodiment of the present technology.  Process 1200 may also be described as disciplining communications\nin an observation platform.  In one embodiment, process 1200 is a computer implemented method that is carried out by processors and electrical components under the control of computer usable and computer executable instructions.  The computer usable and\ncomputer executable instructions reside, for example, in data storage features such as computer usable volatile and non-volatile memory.  However, the computer usable and computer executable instructions may reside in any type of non-transitory computer\nusable storage medium.  In one embodiment, process 1200 is performed by the components of FIG. 1A, 1B, 1C or 2.  In one embodiment, the methods may reside in a computer usable storage medium having instructions embodied therein that when executed cause a\ncomputer system to perform the method.\nAt 1202, a signal is received from a first communication device at a second communication device associated with a computer system, wherein the signal comprises a mandatory message for a third communication device.  For example, the mandatory\nmessage may be a message that is required to be delivered to each employ associated with an organization and an acknowledgement received and recorded.  The requirement may be a legal requirement to notify employees of certain information or may be\nrequirement implemented by the organization.  The mandatory message may be delivered as an audible message or a text message.  The mandatory message may also direct a user to a location where more information may be found.\nAt 1204, the signal is forwarded with the mandatory message to the third communication device associated with a user such that a receipt of the mandatory message at the third communication device will lock features of the third communication\ndevice until the mandatory message has been acknowledged by the user.  For example, the third communication device may be a handheld device and may have features such as the ability to communicate with other devices or the ability to connect to other\ndevices such as a computer system and may be used to access information from a database.  Upon receipt of the mandatory message, some or all of the features of the communication device may be locked meaning that the user not able to access the features. \nFor example, upon receipt of the mandatory message the communication device may lock or disable the ability to communicate with other devices.\nAt 1206, an acknowledgement of the mandatory message is received from the third communication device at the second communication device.  The acknowledgement may be generated manually by the user of the third communication device or may be\nautomatically generated.  For example, upon receipt of the mandatory message, the third communication device may display an option to access the mandatory message.  Once the user accesses the message, the acknowledgement may be sent automatically, or an\noption may be presented to the user to send the message.  In one embodiment, the user is required to create an acknowledgement message to send back.  The acknowledgement message may be a text or audible message created by the user.\nAt 1208, the acknowledgement of the mandatory message is forwarded from the second communication device to the first communication device.  In one embodiment, the locked features of the third communication device may be unlocked in response to\nthe user accessing the mandatory message.  In one embodiment, the locked features of the third communication device may be unlocked in response the computer system receiving the acknowledgement.  In one embodiment, the locked features of the third\ncommunication device may be unlocked in response to the user of the first communication device receiving the acknowledgement.\nAt 1210, the signal with the mandatory message is forwarded to a plurality of communication devices associated with a plurality of users such that a receipt of the mandatory message at each of the plurality of communication devices will lock\nfeatures of each of the plurality of communication devices until the mandatory message has been acknowledged by each of the plurality of users.\nAt 1212, a characteristic of the forwarding the signal with the mandatory message is tracked.  In one embodiment, the system tracks the time the message was sent, when it was heard by the user, and when and where the user was located when they\nacknowledged.  Associated with the statistical information is a speech file of what the user said.  This feature is ideal for communicating policy or liability information and assuring that that information was received and understood.  It should be\nappreciated that there is more than one type or class of mandatory messages.  Each type or class may have different requirements for the delivery and/or acknowledgement.\nIt should be appreciated that processes 300, 400, 500, 600, 700, 800, 900, 1000, 1100 and 1200 need not carry out each of the described steps to complete its operation.  Nor do the steps need to be carried out in the order described.  It should\nbe appreciated that processes 300, 400, 500, 600, 700, 800, 900, 1000, 1100 and 1200, or portions thereof, may be combined with one another using any number of combination.  For example, the response from the computer system in process 800 may take place\nin 900, 1000, 1100 and 1200.\nFIG. 14 is a flowchart illustrating process 1400 for managing and distributing content in a plurality of observation platforms in accordance with one embodiment of the present technology.  In one embodiment, process 1400 is a computer\nimplemented method that is carried out by processors and electrical components under the control of computer usable and computer executable instructions.  The computer usable and computer executable instructions reside, for example, in data storage\nfeatures such as computer usable volatile and non-volatile memory.  However, the computer usable and computer executable instructions may reside in any type of non-transitory computer usable storage medium.  In one embodiment, process 1400 is performed\nby the components of FIG. 1A, 1B, 1C, 2, or 13.  In one embodiment, the methods may reside in a non-transitory computer usable storage medium having instructions embodied therein that when executed cause a computer system to perform the method.\nAt 1402, a message is generated with content at a content distribution manager.  The content may be pre-recorded or generated and accessed by the content distribution manager or may be generated as new original content.  In one embodiment, the\ncontent is an audible voice recording stored in an electronic format such as a WAV file.  The content distribution manager may be accessed by a computer system via a URL and comprise an API.\nAt 1404, a delivery of the message is scheduled and a plurality of specified devices are specified for delivery in a plurality of observation platforms at a designated time via the content distribution manager, wherein the plurality of\nobservation platforms are capable of receiving a signal from a first mobile device with a first characteristic that corresponds to an audible source and a second characteristic that corresponds to information indicative of a geographic position of the\nfirst mobile device and relaying the signal to a destination based on the first characteristic and/or the second characteristic.  The message may be scheduled for immediate delivery such the message is delivered immediately after it is received or\nimmediately after it is scheduled for delivery.\nThe user of the content distribution manager may designate a time to deliver the message, how long the message should be available, and which devices in which observation platforms to deliver the message to.  The message may be delivered to\ndifferent devices simultaneously or at different designated times.\nThe user of the content distribution managers may specify which devices receive the message based on groupings defined by context information (Process 700), rules or policy information (Process 300), historical information (Process 1100) or\nmanual groupings of individuals who are identified by the system.\nAt 1406, the message is delivered to the plurality of observation platforms which relays the message to the plurality of specified devices at the designated time.  Cloud services 1316 of FIG. 13 may employ message scheduler 1322 and message\nbroker 1324 for such delivery.  Once the device has received the message, the device may notify the user that a message has been received using a light or sound or other notification technique.  In one embodiment, the message is immediately delivered.\nAt 1408, a confirmation is received at the content distribution manager that the message has been delivered to one of the plurality of specified devices.\nAt 1410, a confirmation is received at the content distribution manager that the message has been played at one of the plurality of specified devices.  1408 and 1410 may be associated with mandatory messages as described herein.\nFIG. 15 is a flowchart illustrating process 1500 for managing a plurality of observation platforms via a manager application in accordance with one embodiment of the present technology.  In one embodiment, process 1500 is a computer implemented\nmethod that is carried out by processors and electrical components under the control of computer usable and computer executable instructions.  The computer usable and computer executable instructions reside, for example, in data storage features such as\ncomputer usable volatile and non-volatile memory.  However, the computer usable and computer executable instructions may reside in any type of non-transitory computer usable storage medium.  In one embodiment, process 1500 is performed by the components\nof FIG. 1A, 1B, 1C, 2, or 13.  In one embodiment, the methods may reside in a non-transitory computer usable storage medium having instructions embodied therein that when executed cause a computer system to perform the method.\nAt 1502, one observation platform of a plurality of observation platforms is accessed via a manager application at a mobile device comprising a processor, memory, and a network interface card, wherein the plurality of observation platforms are\ncapable of receiving a signal from a first mobile device with a first characteristic that corresponds to an audible source and a second characteristic that corresponds to information indicative of a geographic position of the first mobile device and\nrelaying the signal to a destination based on the first characteristic and/or the second characteristic.  The mobile device may be a mobile computer system such as a smart phone or a tablet computer.  The manager application may be an app designed\nspecifically for the type of device.  The device may execute a specific operating system such as an Android operating system or an iOS operating system.\nAt 1504, monitoring statistics are displayed of the one observation platform corresponding to devices associated with the observation platform.  The manager application may be able to access monitoring statistics for a plurality of observation\nplatforms.\nAt 1506, a message is received at the manager application from a user of the mobile device.\nAt 1508, the message is relayed to specified devices in the one observation platform from the manager application, wherein the specified devices are specified by the user via the mobile device and the manager application.  1506 and 1508 may be\nmanaged using techniques similar to what is described for messages in regards to content distribution manager browser 1304 of FIG. 13 and/or the context based techniques described in Processes 700 and 900.\nAt 1510, a confirmation is received at the manager application that the message has been delivered to at least one of the specified devices.\nIn one embodiment, process 1500 is used only to send and receive messages and does not include monitoring steps such as 1504.  In one embodiment, process 1500 is used only for monitoring steps such as 1504 and does not send and receive messages\nsuch as steps 1506, 1508, and 1510.\n<BR><BR>Example Computer System Environment\nPortions of the present technology are composed of computer-readable and computer-executable instructions that reside, for example, in computer-usable media of a computer system or other user device.  Described below is an example computer\nsystem or components that may be used for or in conjunction with aspects of the present technology.\nIt is appreciated that that the present technology can operate on or within a number of different computer systems including general purpose networked computer systems, embedded computer systems, cloud-based computers, routers, switches, server\ndevices, user devices, various intermediate devices/artifacts, stand-alone computer systems, mobile phones, personal data assistants, televisions and the like.  The computer system is well adapted to having peripheral computer readable media such as, for\nexample, a floppy disk, a compact disc, and the like coupled thereto.\nThe computer system includes an address/data bus for communicating information, and a processor coupled to bus for processing information and instructions.  The computer system is also well suited to a multi-processor or single processor\nenvironment and also includes data storage features such as a computer usable volatile memory, e.g. random access memory (RAM), coupled to bus for storing information and instructions for processor(s).\nThe computer system may also include computer usable non-volatile memory, e.g. read only memory (ROM), as well as input devices such as an alpha-numeric input device, a mouse, or other commonly used input devices.  The computer system may also\ninclude a display such as liquid crystal device, cathode ray tube, plasma display, and other output components such as a printer or other common output devices.\nThe computer system may also include one or more signal generating and receiving device(s) coupled with a bus for enabling the system to interface with other electronic devices and computer systems.  Signal generating and receiving device(s) of\nthe present embodiment may include wired serial adaptors, modems, and network adaptors, wireless modems, and wireless network adaptors, and other such communication technology.  The signal generating and receiving device(s) may work in conjunction with\none or more communication interface(s) for coupling information to and/or from the computer system.  A communication interface may include a serial port, parallel port, Universal Serial Bus (USB), Ethernet port, antenna, or other input/output interface. \nA communication interface may physically, electrically, optically, or wirelessly (e.g. via radio frequency) couple the computer system with another device, such as a cellular telephone, radio, a handheld device, a smartphone, or computer system.\nAlthough the subject matter is described in a language specific to structural features and/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features\nor acts described above.  Rather, the specific features and acts described above are disclosed as example forms of implementing the claims.", "application_number": "14586797", "abstract": " Methods and systems for managing and distributing content and aggregation\n     of data in a plurality of observation platforms are disclosed. A message\n     is generated with content at a content distribution manager. A delivery\n     of the message is scheduled and a plurality of specified devices are\n     specified for delivery in a plurality of observation platforms at a\n     designated time and/or location(s) via the content distribution manager,\n     wherein the plurality of observation platforms are capable of receiving a\n     signal from a first mobile device with a first characteristic that\n     corresponds to an audible source and a second characteristic that\n     corresponds to information indicative of a geographic position of the\n     first mobile device and relaying the signal to a destination based on the\n     first characteristic and/or the second characteristic. The message is\n     delivered to the plurality of observation platforms which relays the\n     message to the plurality of specified devices at the designated time.\n", "citations": ["5712899", "6301573", "6377927", "6928343", "6937988", "6937998", "6996531", "7058208", "7248881", "7397368", "7656295", "7748618", "7925777", "8055296", "8060412", "8140340", "8174359", "8179872", "8180377", "8183996", "8200480", "8271188", "8352260", "8369505", "8473289", "8630851", "8699694", "8798036", "8948730", "9042921", "9053449", "9271118", "9311466", "9349128", "9407543", "9414195", "9445232", "9501951", "9514656", "9542695", "9602625", "9686732", "9691047", "9928529", "9971983", "9971984", "10069781", "10134001", "10204524", "10257085", "10304094", "20020055866", "20020136383", "20020143548", "20030065805", "20030130893", "20040203989", "20050021838", "20050190772", "20050213518", "20050221264", "20060071775", "20060095317", "20060248011", "20070046458", "20070064913", "20070129061", "20070207789", "20080041937", "20080154612", "20080159271", "20080240384", "20080242319", "20080270249", "20080279133", "20090003309", "20090005972", "20090176510", "20090234655", "20090249432", "20090254667", "20100003659", "20100009698", "20100054526", "20100070268", "20100088749", "20100094707", "20100113062", "20110022642", "20110055207", "20110072154", "20110077989", "20110093818", "20110171935", "20110179180", "20110201356", "20110202466", "20110205053", "20110255680", "20120034590", "20120089617", "20120123890", "20120151380", "20120226757", "20120310980", "20130040600", "20130060568", "20130073388", "20130117824", "20130130207", "20130196706", "20130196906", "20130204972", "20130204998", "20130317944", "20140052676", "20140143354", "20140148210", "20140316779", "20140316898", "20150065149", "20150100433", "20150105061", "20150106167", "20150113098", "20150213382", "20150269869", "20160012471", "20160171432", "20160225045", "20160321595", "20160321596", "20160321611", "20160323181", "20160364790", "20170011449", "20170024804", "20170039512", "20170091837", "20170093952", "20170187826", "20170213178", "20180189844", "20180260758", "20180375810", "20190130336", "20190156688"], "related": ["13665527", "13401146", "61445504", "61487432"]}, {"id": "20160034966", "patent_code": "10373209", "patent_name": "Driving behaviors, opinions, and perspectives based on consumer data", "year": "2019", "inventor_and_country_data": " Inventors: \nMaycotte; Higinio O. (Austin, TX), Baird; Michael (Austin, TX), Shah; Rishi (Austin, TX), Turner; Travis (Austin, TX), Lanier; Troy (Austin, TX)  ", "description": "Influencing the behavior, opinions, or perspectives of an individual may be difficult.  Media producers around the world\nmay spend hundreds of billions of dollars every year to influence people in certain ways.  For example, media producers may desire to encourage people to consume more of a specific product.  Other media producers may want to affect political leanings or\npeople's opinions on a topic (e.g., politics, environmentalism, etc.).  As the examples described above demonstrate, a goal of certain kinds of communication may be to drive a particular kind of response.  For centuries, the primary method of enticing\npeople to take a certain action was through print media.  Then, the method shifted to radio advertising.  Now, there is a brand new digital medium that offers more promise than previous mediums.  The internet and various associated digital media may have\ninfluence in influencing the actions of individual users, and may enable media producers to communicate with many people nearly instantly.  Despite the strength of this new medium, if the media producers deliver messages to \"incorrect\" audiences, the\nmessages may reduce the likelihood that those audiences will take a desired response (e.g., make a purchase, form a particular opinion, etc.).\n<BR><BR>SUMMARY\nAs internet usage increases, a larger volume of data regarding the interactions of individual users may become available.  However, such data may be difficult to access and analyze, which may present a significant hurdle to harnessing the data\nand developing an understanding of an audience.  Advertisers and other enterprises that are able to effectively use such data to understand their audience may have a greater chance to influence the opinions and behaviors of their audience.  The present\ndisclosure presents systems and methods of driving and influencing actions, behaviors, opinions, and/or perspectives based on consumer data.  For example, a reach extension module may receive an identification of a target audience segment.  The target\naudience segment may correspond to a segment of a population that an advertiser wishes to reach with precise messaging.  The messaging could be delivered via an advertisement, an e-mail communication, or via another digital communication.  As an\nillustrative, non-limiting example, if an advertiser is selling tickets to a football game, the target audience segment may be \"audience members that are likely to buy tickets to football games.\" Examples of actions that an advertiser may wish to\ndrive/increase/influence may include, but are not limited to, purchasing tickets, watching videos, purchasing retail items, navigating to a particular website, voting on a particular issue or for a particular candidate, etc. The reach extension module\nmay identify first attributes associated with members of the target audience segment.  For example, the reach extension module may identify first attributes \"male\" and \"age 21-50\" in response to determining that previous buyers of football tickets were\nlargely male and between the ages of 21-50.  The reach extension module may further identify second attributes that are correlated to the first attributes.  For example, the reach extension module may identify a second attribute \"likes beverage A\" in\nresponse to determining that, in a population for which data is available, a large percentage of the men between the ages of 21-50 have an affinity for beverage A. The first attributes and/or the second attributes may be \"available\" attributes for which\ntargeted communication is available.  As an illustrative, non-limiting example, targeted communication could include an e-mail, an advertising message delivered via a social network, or a push notification on a mobile device.  Alternatively, or in\naddition, the reach extension module may map the first attributes and/or the second attribute to additional available attributes.  For example, one or more advertising networks may offer advertising targeted to users with an attribute \"man\" and/or an\nattribute \"likes beverage A website.\" The reach extension module may map the first attribute \"male\" to the available attribute \"man\" and may map the second attribute \"likes beverage A\" to the available attribute \"likes beverage A website.\"\nPrior to initiating targeted communication aimed at driving behavior, the reach extension module may determine estimated cost and reach of one or more of the available attributes.  \"Reach\" may be estimated as a number of unique views for an\nadvertisement, a number of people who interact with the advertisement, a number of people who take a desired action as a result of the advertisement, a number of people expected to open an e-mail, number of people that have installed a mobile\napplication, number of people wearing a device connected to the internet, or any combination thereof.  For example, the reach extension module may query one or more channels for prices associated with the one or more available attributes.  The prices may\nbe represented as a cost per thousand impressions, a cost per click, a cost per e-mail, or a cost per notification.  The type of pricing may depend on the network the targeted messaging is delivered on.  The reach extension module may compare the\nreceived price information to historical data regarding previous targeted communication and audiences to estimate cost and reach.  Based on the estimated cost and reach of the one or more available attributes, the reach extension module may reference a\nlibrary of strategies and execute targeted communication at the one or more networks according to the strategy.  The reach extension module may monitor results of the strategies, the number of people completing the desired behavior (e.g., reach of\ntargeted communication, conversions due to targeted communication, cost of communication, etc.), adjust the strategies according to the results, and store data regarding the strategies for subsequent use.  The described system and method may thus provide\nan automatic method to target communication aimed at driving behaviors.  The targeted communication may exist on paid advertising networks, e-mail delivery systems, or other ways to digitally intercept a user's attention and promote a particular action.\n<BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nFIG. 1 is a diagram to illustrate a particular embodiment of a system that is operable to initiate a messaging action based on audience attributes;\nFIG. 2 is a diagram to illustrate automatic targeted messaging actions;\nFIG. 3 is a diagram to illustrate a particular embodiment of an attribute mapping used by the system of FIG. 1;\nFIG. 4 is a diagram to illustrate a particular embodiment of a graphical user interface (GUI) associated with initiating messaging actions based on audience attributes;\nFIG. 5 is a diagram of another embodiment of a system that is operable to initiate targeted messaging actions based on audience attributes;\nFIG. 6 is a flowchart to illustrate a particular embodiment of a method of initiating targeted messaging actions;\nFIG. 7 is a flowchart to illustrate another particular embodiment of a method of initiating targeted messaging actions;\nFIG. 8 is a diagram to illustrate a particular embodiment of an audience measurement system; and\nFIGS. 9A, 9B, 9C, and 9D are diagrams to illustrate another particular embodiment of an audience measurement system.\n<BR><BR>DETAILED DESCRIPTION\nFIG. 1 illustrates a particular embodiment of a system 100 that is operable to initiate a messaging action (e.g., an advertisement, an e-mail, a text message, a push notification to a computing device, a social networking message, etc.) based on\naudience attributes.  The system 100 includes a measurement system 120 that may be implemented using one or more computing devices (e.g., servers).  For example, such computing devices may include one or more processors or processing logic, memories, and\nnetwork interfaces.  The memories may include instructions executable by the processors to perform various functions described herein.  The network interfaces may include wired and/or wireless interfaces operable to enable communication to local area\nnetworks (LANs) and/or wide area networks (WANs), such as the Internet.  In the illustrated example, the measurement system 120 is communicably coupled to a network 130.\nThe measurement system 120 may include a reach extension module 124, which may be implemented using instructions executable by one or more processors at the measurement system 120.  In alternative embodiments, the reach extension module 124 may\nbe external to and in communication with the measurement system 120 (e.g., functions described herein with reference to the reach extension module 124 may be executed on a separate computing device in communication with the measurement system 120).  The\nreach extension module 124 is executable to initiate messaging actions based on audience attributes, as further described herein.  For example, a media producer may initiate a messaging action in an attempt to cause an individual to perform a desired\naction (e.g., buy a product, form a particular opinion, join a cause, enroll in a program, watch a video, read an article, etc.) The reach extension module 124 may initiate messaging actions based on attributes of an audience made of people who have\nperformed the desired action.\nIn a particular non-limiting example, initiating a messaging action corresponds to purchasing targeted advertising.  In other non-limiting examples, initiating a messaging action corresponds to a different type of communication.  In the context\nof targeted internet advertising, \"purchasing\" targeted advertising directed to a particular attribute refers to placing a bid with an advertising network (e.g., a social network that advertises to users, a search engine that inserts advertisements in\nsearch results webpages, etc.) for the opportunity to advertise to members/users of the advertising network that exhibit the particular attribute.  Bids may be placed in terms of cost per mille (CPM), cost per click (CPC), or cost per action (CPA), as\nillustrative non-limiting examples.  For example, an advertiser may place a CPC bid of $1.25 targeting unmarried users of a social network.  If the advertiser's bid is accepted, the advertiser's advertisement(s) are presented to unmarried users of the\nsocial network and the advertiser is charged $1.25 each time a user of the social network clicks on the advertisement(s).\nThe measurement system 120 may be coupled via the network 130 to one or more segment databases 140 that store data (e.g., consumer data).  Although the segment databases 140 are illustrated in FIG. 1 as being external to the measurement system\n120, in alternative embodiments the segment databases 140 may be part of (e.g., internal to) the measurement system 120.  As used herein, a \"segment\" is based on (or corresponds to) a group of people (e.g., an audience or a subset thereof).  As further\ndescribed herein, a set of traits may be determined for each segment.  In an illustrative embodiment, the set of traits for a segment corresponds to a Digital Genome.RTM.  of the segment (Digital Genome is a registered trademark of Umbel Corporation of\nAustin, Tex.).  Examples of segments include, but are not limited to, brand affinity segments (also called brand segments), demographic segments, geographic segments, social activity segments, employer segments, educational institution segments,\nprofessional group segments, industry category of employer segments, brand affinity category segments, professional skills segments, job title segments, and behavioral segments.  In a particular embodiment, behavioral segments are defined by a client\n(e.g., property owner or publisher) or by the measurement system 120, and represent actions taken on a client's property, such as \"watched a video,\" \"read an article,\" \"made a purchase,\" etc. In this context, \"property\" refers to a media property, such\nas media content, a website, etc.\nAdditional examples of segments include segments based on an advertisement, an advertisement campaign, an advertisement placement, an advertisement context, a content item, a content context, content placement, etc. As another example, a segment\nmay be generated based on a platform (e.g., desktop/laptop computer vs.  mobile phone vs.  tablet computer).  For example, a \"tablet segment\" may include users that viewed a media property using a tablet computing device.  Segments may be used to\nevaluate characteristics of an audience, craft a content strategy, generate advertising leads, create advertising pitches, and respond to inbound advertising requests.\nThe measurement system 120 collects and analyzes data that describes attributes of audiences of one or more media properties and stores the data and analysis results in the segment databases 140.  In a particular embodiment, audience data is\nstored in the segment databases 140 according to media property (e.g., website), and audience segments are generated and analyzed on demand by the measurement system 120.  Each segment is defined by a relation between members of a media property's\naudience (e.g., users) and one or more attributes.  The relations may include a HAS relation, an AND relation, an OR relation, a NOT relation, or a combination thereof.  For example, a \"male\" segment may be defined by a relation \"users who have the male\nattribute\" (male={users: has male attribute}), and a \"males who watch video\" segment may be defined by a relation \"users who have the male attribute and have the watch video attribute\" (male AND watches video={users: has male attribute; and has watches\nvideo attribute}).  Similarly, a \"males who do not watch video\" segment may be defined by a relation \"users who have the male attribute and do not have the watches video attribute\" (male AND watches video={users: has male attribute; and does not have\nwatches video attribute}).  In addition, a \"male or watches video\" segment may be defined by a relation \"users who have the male attribute or users that have the watches video attribute\" (male OR watches video={users: has male attribute; or has watches\nvideo attribute}).  In a particular embodiment, the measurement system 120 collects and organizes data regarding audience members (e.g., users) of various media properties based on event signals received from user devices (e.g., mobile phones, tablet\ncomputers, laptop/desktop computers, radio-frequency identification (RFID) tags, etc.), media property web servers, network logs, and/or third-party signal sources, as further described with reference to FIGS. 8-9.\nAn audience of a media property may be segmented by demographic attributes, brand affinities, behavioral attributes, or a combination thereof.  For example, as explained above, the \"Male\" segment may include all members of an audience of a media\nproperty who are male.  As another example, a \"Brand X\" audience segment may include all members of the audience who are determined (e.g., based on statements made in social networks, actions performed on the media property, etc.) to have an affinity for\n\"Brand X.\" As a third example, an audience segment may include all members of the audience who have performed a particular action related to the media property (e.g., users who have purchased an item at a website, watched a video, clicked on an\nadvertisement, etc.).  Further, audience segments may be defined by an AND or an OR relation between attributes (and associated segments).  For example, a particular audience segment may include all members of an audience who are female AND who are\nbetween the ages of 23 and 27.  As another example, a particular audience segment may include all members of an audience who are female OR who are between the ages of 23 and 27.  In addition, audience segments may be defined in part by a NOT relation. \nFor example, a segment may include all members of an audience who are NOT female.\nThe measurement system 120 may be in communication with one or more digital networks 150 (e.g., advertising networks, social networks, etc.) via the network 130.  In alternative embodiments, the measurement system 120 may communicate with the\ndigital networks 150 via a second network (e.g., a local area network (LAN), a wide area network (WAN) such as the Internet, etc.).  The digital networks 150, or at least a subset thereof, may correspond to Internet-based networks and may offer the\nability to purchase advertising targeted to specific users 160 and/or specific user attributes.  For example, a social network may enable media producers (e.g., advertisers, political campaigns, etc.) to initiate messaging actions (e.g., advertisements,\ne-mails, text messages, push notifications, social network messages, etc.) targeted to users of the social network who have certain attributes (e.g., affinities, demographic attributes, etc.) using an auction-based system, as further described herein. \nThe users 160 of the digital networks 150 may include users of the media properties (e.g., websites) for which data is collected by the measurement system 120, although this is not necessary.  For example, one of the digital networks 150 may correspond\nto a social network.  The social network may include a user \"John Smith,\" who is also a registered user of a particular blog that is tracked by the measurement system 120.  Thus, in this example, John Smith is both a user of a tracked media property as\nwell as one of the users 160 of the digital networks 150.\nIn a particular example, a digital network, such as a social network, may solicit bids for messaging actions directed to specific attributes of members of the social network.  To illustrate, an advertiser may bid to initiate a messaging action\nto members of the social network who, according to their social networking profiles, match the attributes \"female,\" \"unmarried,\" and \"age 21-30.\" As further described herein, during initiation of messaging actions, the measurement system 120 may target\nindividual users (e.g., John Smith) of a social network if permitted to do so by the social network.\nSome or all of the attributes tracked by the measurement system 120 (also referred to herein as \"first attributes\") may correspond to some or all of the attributes for which messaging actions are available at the digital networks 150 (also\nreferred to herein as \"second attributes\").  For example, the measurement system 120 may collect data indicating which users of a media property like a particular television show.  One or more of the digital networks 150 may offer messaging actions\ntargeted to a subset of the users 160 who are members of a fan group dedicated to the particular television show.  One non-limiting example of a messaging action is purchasing targeted advertising.  Members of a fan group dedicated to the particular\ntelevision show may have similar attributes to users of the media property who like the particular television show (e.g., may share other affinities, demographic attributes, etc.).  As further described herein, by identifying and initiating messaging\nactions directed to the \"related\" second attributes instead of or in addition to purchasing advertising directed to the first attributes, the reach extension module 124 may improve results of a messaging action (e.g., by initiating messaging actions\ntargeted to users who may share interests, demographic attributes etc., with a target segment and may therefore be more likely to respond to the messaging action), decrease advertising cost, and/or increase advertising reach.\nIn a particular embodiment, the measurement system 120 generates, maintains, stores, and/or accesses one or more mapping databases 126 that store mappings between the first attributes and the second attributes.  For example, such attribute\nmappings may be one-to-one, one-to-many, many-to-one, and/or many-to-many relations that are stored in tables or other data structures.  In alternative embodiments, the mapping databases 126 may be stored externally to the measurement system 120 or as\npart of the reach extension module 124.  The mappings database 126 may maintain a mapping between attributes tracked by the measurement system 120 and attributes associated with each of the digital networks 150 for which messaging actions are available\n(e.g., targeted advertising, e-mail, text message, social networking message, push notification, etc.).  The mappings may be user-defined (e.g., by an administrator of the measurement system 120) and/or automatically determined (e.g., based on a fuzzy\nlogic algorithm, a pattern matching algorithm, an advertising attribute ontology, etc.).  An illustrative example of attribute mappings is further described with reference to FIG. 3.\nThe measurement system 120 may further include or have access to a messaging action history database 128.  The messaging action history database 128 may be configured to store data indicating reach, price, yield, or a combination thereof of\nmessaging actions initiated at the digital networks 150.  For example, the messaging action history database 128 may store data indicating a number of impressions, a number of unique impressions, a number of engagements, a number of conversions, or a\ncombination thereof.  As used herein, a number of impressions indicates how many devices displayed a message as a result of a messaging action, a number of engagements indicates how many devices interacted with the message (e.g., a \"click\" on a hyperlink\nin the message), and a number of conversions indicates a number of times a desired action was taken by devices (or users thereof) after displaying the message (e.g., a number of purchased a products, a number of new enrollments in a program, a number of\npledges to an organization, etc.).  \"Reach\" may represent a number of unique views for the message, a number of people who interact with the message, a number of people who take the desired action as a result of the message, or any combination thereof. \nYield may represent a total number of times the desired action is taken in response to the message.  The messaging action history database 128 may store cost data such as total cost of a messaging action campaign, cost per conversion, cost per\nimpression, cost per unique impression, or a combination thereof.  While reach, cost, and yield are described to vary for messaging actions depending on the attribute targeted, it should be noted that reach, cost, and yield may also vary by type of\nmessaging action.  For example, a text message messaging action may have a different cost/reach/yield than an e-mail.\nIn operation, the measurement system 120 may receive data identifying a target segment 110.  In particular embodiments, the data identifying the target segment 110 may be received from a web application or other application that provides a user\ninterface (UI) that is operable to configure the measurement system 120.  For example, the target segment 110 may correspond to users who have performed a desired action in the past (e.g., watched a movie, read a book, indicated a certain opinion, liked\na particular product, etc.) Alternatively, the data identifying the target segment may be programmatically (e.g., automatically) selected based on a probability of audience conversion or previous advertising campaign success, as further described herein. The target segment 110 may correspond to a segment of one or more media properties tracked by the measurement system 120.  The one or more media properties may or may not be commonly owned.  A client of the measurement system 120 (e.g., an advertiser or\nother entity) may be interested in initiating messaging actions to people similar to the target segment 110 (e.g., to \"amplify\" or \"drive\" the action or behavior corresponding to the target segment 110).  For example, the target segment 110 may\ncorrespond to users of one or more media properties who watched a video, and the client of the measurement system 120 may be interested in initiating messaging actions to people likely to watch the video or a similar video.  As another example, the\nadvertiser may be a coffee vendor that is determining how to present a newly created video advertisement for a coffee product.  In this example, the target segment 110 may be a \"likes coffee\" segment or a \"likes particular coffee brand\" segment.\nThe reach extension module 124 may determine a first list of attributes associated with the target segment 110.  For example, the first list of attributes may include attribute(s) tracked by the measurement system 120 that are associated with at\nleast one member of the target segment 110.  In addition, the reach extension module 124 may determine one or more attributes that are correlated to the attributes of the first list.  For example, the \"likes coffee\" target segment 110 may include a large\nnumber of users with an attribute \"job starts before 7 AM.\" The reach extension module 124 may identify that an attribute \"is a police officer\" has a high degree of correlation with the attribute \"job starts before 7 AM.\" Thus, the \"police officer\"\nattribute may be determined by the reach extension module 124 to be correlated to the attribute \"job starts before 7 AM.\" Thus, the reach extension module 124 may determine that the coffee-related messaging actions (e.g., advertisements) may be\neffectively targeted to users in the digital networks 150 that have attributes matching or similar to the \"likes coffee,\" \"job starts before 7 AM,\" and/or \"police officer\" attributes tracked by the measurement system 120.\nTo identify attributes in the digital networks 150 that match or are similar to \"likes coffee,\" \"job starts before 7 AM,\" and \"police officer,\" the reach extension module 124 may access the mapping databases 126.  To illustrate, the mapping\ndatabases 126 may store data indicating that the first list of attributes \"likes coffee,\" \"job starts before 7 AM,\" and \"police officer\" maps to a second list of attributes (e.g., \"likes early morning radio,\" \"caffeine lovers,\" \"police union,\" etc.) for\nwhich a messaging action is available at a particular digital network.  The reach extension module 124 may determine metrics, such as an estimated cost (e.g., cost per conversion), an estimated reach (e.g., a number of unique views), and an estimated\nyield (e.g., a number of desired actions, such as purchases, that are taken) for messaging actions targeted to different combinations of attributes in the second list.  For example, a particular second list corresponding to a particular digital network\nmay include the attributes \"police union\" (e.g., indicating that a user is a member of a police union social networking group) and \"likes coffee brand M\" (e.g., indicating that a user is a \"fan\" of coffee brand M in a social network).  The reach\nextension module 124 may determine metric(s) (e.g., estimated cost, estimated reach, estimated yield, or a combination thereof) for messaging actions in the particular digital network targeted to the \"police union\" attribute, the \"likes coffee brand M\"\nattribute, and a combination of both attributes.  It should be noted that the estimated cost may include lost opportunity costs associated with users who react negatively to messaging actions.  For example, a particular user who may have bought a product\nif not for receiving a targeted messaging action may account for a portion of the estimated cost of the messaging action.  Further, the estimated cost may not be stored as a monetary value.  For example, estimated cost may correspond to votes lost in a\npolitical campaign as a result of initiating targeting messaging actions.\nIn a particular embodiment, the estimated cost, the estimated reach, the estimated yield, or a combination thereof of messaging actions directed to a particular attribute (as used herein, messaging actions directed to a particular attribute\nrefers to messaging actions directed to users who have, exhibit, or are associated with the particular attribute) may be calculated based on data stored in the segment databases 140.  Thus, the data in the segment databases 140 may be used to estimate\nmessaging action efficacy in the external digital networks 150.  For example, the measurement system 120 may determine a \"degree of correlation\" between the target segment 110 (e.g., the \"likes coffee\" segment) and each attribute or combination of\nattributes identified by the reach extension module 124 (e.g., the \"likes coffee\" attribute, the \"job starts before 7 AM\" attribute, and the \"police officer\" attribute).  A correlation of the \"likes coffee\" attribute to the \"likes coffee\" segment may be\n1, because each user in the \"likes coffee\" segment has the attribute \"likes coffee\"=1 (or true).  A correlation of a \"police officer\" attribute to the \"likes coffee\" segment may be 0.7 based on data in the segment databases 140 indicating that 70% of\nusers in the \"police officer\" segment are also in the \"likes coffee\" segment (e.g., have the attribute \"likes coffee\"=1).  Correlation data may also be calculated for custom segments defined using set unions and/or intersections (e.g., a \"police officers\nwho like coffee\" segment defined by performing a set intersection on the \"police officer\" segment and the \"likes coffee\" segment).\nThe reach extension module 124 may use the correlation data to estimate reach and cost for each combination of mapped attributes in the digital networks 150.  To illustrate, in the above example in which an advertiser is presenting a\ncoffee-related advertisement, a digital network may have 3,000 users that are members of a police union.  To determine an estimated reach of a coffee-related advertisement directed to the \"police union\" attribute, the reach extension module 124 may apply\nthe previously determined correlation between \"likes coffee\" and \"police officer\" of 0.7 to the number of members of the police union.  Thus, the reach extension module 124 may determine the estimated reach as 3,000*0.7=2,100.\nIn a particular embodiment, the segment databases 140 may include information indicating a frequency of actions carried out by users.  For example, the segment databases 140 may include information indicating that, on average, coffee drinkers\n(e.g., user in a \"likes coffee\" segment) drink 1.5 cups of coffee per day.  Thus, the reach extension module may determine that an estimated yield of initiating a messaging action (e.g., an advertisement) designed to encourage coffee drinking directed to\nmembers of the police union is 2,100*1.5=3,150 cups of coffee per day.\nIn addition or in the alternative, the reach extension module 124 may determine estimated cost, estimated reach, and estimated yield based at least in part on historical data received from the messaging action history database 128.  For example,\nthe messaging action history database 128 may include information indicating reach, cost, and yield of a prior advertising campaign for coffee directed to police officers.  The reach extension module 124 may use the advertisement history information as\nan estimated cost, estimated reach, and estimated yield or may weight/adjust the estimated cost, the estimated reach, and the estimated yield based on the advertisement history information.  For example, the reach extension module 124 may average the\nestimated cost, the estimated reach, and the estimated yield with the advertisement history data to produce a weighted cost, a weighted reach, and a weighted yield.\nOne or more of the attributes identified by the reach extension module 124 may be used to initiate messaging actions.  For example, the reach extension module 124 may initiate (e.g., initiate purchase of, initiate transmission of, etc.)\nmessaging actions based on an attribute of a target segment, a first attribute associated with at least one member of the target segment, a second attribute determined to be related to the first attribute, or a combination thereof.  In the coffee-related\nadvertisement example, the target segment 110 is \"likes coffee,\" the first attribute may be \"job starts before 7 AM\" and the second attribute may be \"is a police officer.\" Thus, one or more of the attributes \"likes coffee,\" \"job starts before 7 AM,\" and\n\"is a police officer\" may be used.  At a first digital network, targeted messaging actions directed to users who like coffee (e.g., directed to the \"likes coffee brand M\" attribute at the digital network) may have a particular estimated cost (e.g., $4\nper user reached), a particular estimated reach (e.g., 1,000 users), and a particular estimated yield (e.g., 1,500 coffee purchases per day).  Similarly, targeted advertising directed to police officers (e.g., directed to the \"police union\" attribute at\nthe digital network) may have an estimated cost of $1 per user, an estimated reach of 2,100 users, and an estimated yield of 3,150 coffee purchases per day.  Targeted messaging actions directed to people who start work before 7 AM (e.g., a \"likes early\nmorning radio\" attribute at the digital network) may have an estimated cost of $2 per user reached, an estimated reach of 800 users, and an estimated yield of 1,200 coffee purchases per day.\nIn a particular embodiment, the reach extension module 124 may determine that one or more messaging actions targeted to one attribute at a digital network is likely to be more effective (e.g., cost effective) than one or more messaging actions\ntargeted to another attribute.  For example, as described above, a target segment of coffee drinkers may include people who start work before 7 AM.  Being a police officer may be correlated with starting work before 7 AM.  In a particular digital\nnetwork, messaging actions directed to police officers may have an estimated cost, an estimated reach, an estimated yield or a combination thereof, that is more cost effective (e.g., lower cost, a larger reach, a larger yield, or a combination thereof)\nthan messaging actions directed to people who start work before 7 AM.  In the above example, the cost per user reached for messaging actions directed to people who start work at 7 AM (e.g., the \"likes early morning radio\" attribute) is $2 and the cost\nper user reached directed to police officers (e.g., the \"police union\" attribute) is $1.  Therefore, the reach extension module 124 may automatically initiate messaging actions targeted to users with attributes corresponding to police officers (e.g., the\n\"police union\" attribute of the digital network) instead of messaging actions directed to users who start work before 7 AM.\nIn a particular embodiment, the reach extension module 124 may initiate messaging actions based on an estimated yield.  For example, the estimated cost per additional cup of coffee sold per day may be 75 cents for messaging actions directed to\npeople who start work at 7 AM (e.g., the \"likes early morning radio\" attribute) and the estimated cost per additional cup of coffee sold per day may be 68 cents for messaging actions directed to people who are police officers (e.g., the \"police union\"\nattribute).  Therefore, the reach extension module 124 may automatically initiate messaging actions targeted to more users with attributes corresponding to police officers (e.g., the \"police union\" attribute of the digital network) than users who start\nwork before 7 AM.\nAlternately, the reach extension module 124 may utilize a first portion of a budget to initiate targeted messaging actions directed to coffee drinkers, a second portion of the budget to initiate targeted messaging actions directed to users who\nstart work before 7 AM, and/or a third portion of the budget to initiate targeted messaging actions directed to police officers.  The ratios of the first portion, the second portion, and/or the third portion of the budget may be based on the estimated\ncosts, the estimated reaches, the estimated yields, or a combination thereof of the corresponding targeted messaging action.  The ratios may also be determined based on historical success of previous advertising campaigns.  To illustrate, lower-cost\nand/or higher-reach/yield attributes may be purchased in larger proportions than higher-cost and/or lower-reach/yield attributes.\nIn a particular embodiment, the reach extension module 124 may make messaging decisions by determining that a messaging action satisfies message criteria 112.  The message criteria 112 may be received from a client application (e.g., a web\napplication, a mobile phone application, or any other application).  The message criteria 112 may indicate that targeted messaging actions directed to an attribute should be initiated at a digital network when a particular number of members of the\ndigital network exhibit the attribute.  In addition or in the alternative, the message criteria 112 may indicate that the targeted messaging action should be initiated when an estimated cost (e.g., monetary cost, negative reactions, loss of good will,\netc.) of the targeted messaging action satisfies (e.g., is less than) a cost threshold.  Further, the message criteria 112 may indicate that the targeted messaging action should be initiated when an estimated reach of the targeted advertising satisfies\n(e.g., is greater than) a reach threshold.  In addition or in the alternative, the message criteria 112 may indicate that the targeted messaging action should be initiated when an estimated yield of the targeted messaging action satisfies (e.g., is\ngreater than) a yield threshold).  In a particular embodiment, the message criteria 112 indicates that when a combination of estimated reach, estimated cost, and estimated yield (e.g., cost per audience member reached or cost per action caused) satisfies\na threshold, the targeted messaging action should be initiated.  In a particular embodiment, targeted messaging actions directed to more than one attribute may satisfy the message criteria.  The reach extension module 124 may evaluate each potential\ntargeted messaging action that satisfies the message criteria 112 and select a combination of the messaging actions based on estimated cost, estimated reach, estimated yield or a combination thereof.  For example, the reach extension module 124 may\ninitiate targeted messaging actions to achieve a highest estimated reach at a cost that is less than or equal to a budget.  Further, the reach extension module 124 may initiate targeted messaging actions to achieve a highest estimate yield at a cost that\nis less than or equal to the budget.  The estimated yield may be a function of the estimated reach.\nIn a particular embodiment, the reach extension module 124 develops a messaging strategy based on the estimated cost, the estimated reach, the estimated yield, or a combination thereof of targeted messaging actions available for one or more\nattributes and initiates targeted messaging actions based on the messaging strategy (e.g., transmits bids for targeted advertising to one or more advertising networks, initiates transmission of e-mails, initiates transmission of text messages, etc.). \nThe messaging strategy may be further based on the message criteria 112.  The message criteria 112 may indicate a number of members (e.g., users) of a digital network that exhibit a particular attribute (e.g., only purchase advertising directed to the\nattribute \"likes television show x\" when at least 1,000 members are part of a social networking fan club for television show x).  The messaging strategy may improve effectiveness of targeted messaging actions across the digital networks 150.  The\nmessaging strategy may be designed to achieve a highest estimated reach and/or estimated yield within the budget.  The reach extension module 124 may continue initiating targeted messaging actions according to the messaging strategy until a budget is\nspent.\nAs targeted messaging actions are initiated and messages are transmitted to the users 160 via the digital networks 150, the reach extension module 124 may monitor actual cost, actual reach, and actual yield of the initiated targeted messaging\nactions, and may update the messaging action history database 128 based on the monitoring.  For example, the messaging action history database 128 may include data regarding some or all previous messaging campaigns, a top 10 indexing strategies, a top 10\nbrand strategies, etc. When a client (e.g., advertiser) wishes to influence a particular behavior, the described techniques may include identifying target segment(s) corresponding to the behavior, checking the messaging action history database 128 to see\nif any previous strategies regarding the same or similar target segments exist, and then deploying a strategy/data combination that is predicted to have at least a desired cost/reach/yield.  The reach extension module 124 may update an in-progress or\nstored messaging strategy (e.g., a previously used messaging strategy for a coffee-related messaging action campaign) based on the updated messaging action history database 128.  For example, the updated messaging action history database 128 may indicate\nthat messaging actions targeted to a particular attribute have reached fewer users than estimated, have generated fewer desired actions than estimated (i.e., have a lower yield than estimated), or have cost more than anticipated.  The reach extension\nmodule 124 may issue an alert (e.g., an e-mail, a text message, a pop-up notification, etc. to a client or administrator of the measurement system 120) and/or adjust the messaging strategy to stop or slow initiation of messaging actions targeted to the\nparticular attribute in response to the updated messaging action history database 128.  Alternatively, the updated messaging action history database 128 may indicate that messaging actions directed to a certain attribute cost less, have reached more\nusers than anticipated, or have generated more desired actions than estimated (i.e., have a higher yield than estimated).  The reach extension module 124 may issue an alert to a client or administrator and/or adjust the messaging strategy to accelerate\ninitiation of messaging actions targeted to the particular attribute.  Examples of alerts and modification of messaging strategy are further described with reference to FIG. 4.\nIn a particular embodiment, the reach extension module 124 may recursively or iteratively refine and/or expand a messaging strategy.  For example, the reach extension module 124 may generate a messaging strategy for a coffee-related messaging\naction campaign.  Thus, in this example, the initial target segment is coffee drinkers.  The messaging strategy may include initiating messaging actions directed to one or more attributes at one or more digital networks, where the one or more attributes\nare determined to be correlated to the segment of coffee drinkers.  For example, the messaging strategy may include initiating targeted messaging actions directed to police officers, people who start work before 7 AM, light house operators, people who\nsubscribe to a newspaper, and people who attend a yoga studio.  These attributes can be considered \"first-degree\" attributes of the coffee drinkers target segment.  The reach extension module 124 may receive information (e.g., from the one or more\ndigital networks, such as a social network) related to performance of the targeted messaging actions directed to each of the one or more attributes and may expand and refine the messaging strategy based on the performance.  For example, messaging actions\ntargeted to police officers may be performing \"well\" (e.g., meeting or exceeding a particular cost, yield, or reach threshold or performing better than targeted messaging actions directed to the other attributes).  In response, the reach extension module\n124 may identify attributes exhibited by members of the police officers segment (e.g., subscribes to a particular magazine, has a high stress job, likes comfortable shoes, etc.) These attributes can be considered \"second-degree\" attributes for the\noriginal coffee drinkers target segment.  The reach extension module 124 may expand and refine the advertising strategy by purchasing advertising targeted to one or more of the second-degree attributes.  The reach extension module 124 may continue\nexpanding and refining the advertising strategy in this fashion (e.g., by determining third-degree attributes, fourth-degree attributes, etc.) to extend reach for the coffee drinkers target segment.  Thus, the described techniques may be used to find\npeople with non-obvious characteristics while maintaining quality (e.g., a user acquisition cost).\nThus, the reach extension module 124 may enable automatic initiation of targeted messaging actions at multiple digital networks 150, where the initiated targeted messaging actions are directed to attributes that are highly correlated to the\ntarget segment 110.  Further, the reach extension module 124 may monitor the initiated targeted messaging actions and adjust messaging strategies based on performance of the targeted messaging actions.  Moreover, the reach extension module 124 may\nmaintain mappings between first attributes tracked by the measurement system 120 (e.g., \"likes coffee,\" \"job starts before 7 AM,\" and \"police officer\") and second attributes available for purchase at each of multiple digital networks (e.g., \"likes coffee\nbrand M,\" \"likes early morning radio,\" and \"police union\") so that targeted messaging actions may be initiated at multiple digital networks based on a common list of attributes (e.g., the first attributes).\nIn some embodiments, data regarding users (e.g., audience members) may be used internally within the measurement system 120 instead of, or in addition to, being used externally with respect to an advertising network, push notification system,\ne-mail system, etc. For example, based on data regarding users that have performed a particular behavior, similar users may be identified by internal data or an internal system 129, such as a consumer relationship management (CRM) system, an e-mail list,\na transaction log, etc. Thus, the reach extension module 124 may be used to identify \"new\" users that an enterprise does not have a relationship with as well as \"known\" users that the enterprise has an existing relationship with (e.g., users that the\nmeasurement system 120 is aware of).  As an illustrative non-limiting example, the reach extension module 124 may determine that a large percentage of coffee drinkers age 44-51 have performed a behavior of interest (e.g., read an article).  Based on such\ninformation, the reach extension module 124 may identify other users from an internal e-mail list that are also coffee drinkers age 44-51, and are therefore likely to perform the behavior (e.g., read the article or a different article).  The reach\nextension module 124 may use such data (e.g., \"coffee drinkers\" and \"age 44-51\") to initiate an internal messaging action (e.g., send a targeted e-mail) to drive the desired behavior.  For example, the targeted e-mail may be sent by the measurement\nsystem 120 (or a component thereof) to other users tracked by the measurement system 120 that have not performed the target behavior.\nFIG. 2 is a diagram 200 illustrating automatic targeted messaging actions.  In an illustrative embodiment, the automatic initiation of targeted messaging actions may be performed by a reach extension module, such as the reach extension module\n124 of FIG. 1.\nIn operation, a reach extension module may receive data identifying a target segment (or identification thereof).  The target segment may be associated with a desired behavior that a content producer wishes to drive.  In the illustrated example,\nthe target segment includes users who like a product D (e.g., a content producer may wish to drive purchases of product D).  The reach extension module may generate a first list 202 of attributes associated with at least one member of the target segment. For example, in the segment databases 140, users x, y, and z may be members of a likes product D segment.  The first list 202 may include attributes of the users x, y, and/or z (e.g., if one or more of the users x, y, and z are male, then the first list\n202 may include an attribute \"male\").  The attributes of the first list 202 may include demographic attributes (e.g., gender, age, income, etc.), brand affinities (e.g., brands \"liked\" by the users of the target segment), behavioral attributes (e.g.,\nactions performed by the users of the target segment), or any combination thereof.\nIn particular embodiments, the reach extension module may also identify one or more attributes that are related to the attributes in the first list 202.  For example, a related attribute 203 \"likes TV show A\" is related to an attribute \"likes TV\nnetwork C\" of the first list 202.  In FIG. 2, the relation between the attribute \"likes TV show A\" and the attribute \"likes TV network C\" is illustrated by a dashed line.  The related attribute 203 may be identified based on a correlation between users\nliking TV network C and users liking TV show A, as described with reference to FIG. 1.  Further, although one related attribute 203 is shown in FIG. 2, it will be understood that in alternate embodiments multiple related attributes may be identified.\nThe reach extension module may map the first list 202 and the related attribute 203 to attributes 204 for which targeted messaging actions are available at a particular digital network to generate a second list 206 of attributes.  For example,\nthe reach extension module 124 of FIG. 1 may receive a map from the mapping databases 126.  The map may identify relationships between attributes of the first list 202 and the attributes 204.  The second list 206 of attributes may thus include attributes\nfor which targeted messaging actions are available at the digital network and are predicted to be successful in reaching the target segment.\nFor each attribute and each combination of attributes of the second list 206, the reach extension module may calculate an estimated cost, an estimated reach, and an estimated yield.  The reach extension module may store the estimated cost and\nthe estimated reach of each attribute and each combination (e.g., in a data structure, such as illustrative table 208).  In a particular embodiment, the estimated cost is an estimated cost per user reached and the estimated reach indicates an estimated\nnumber of unique views for an advertisement.  The estimated yield may correspond to an estimated number of desired actions taken (e.g., purchases of product D) in response to a messaging action.  In alternate embodiments, different cost, reach, and yield\nmetrics may be used.\nIn a particular embodiment, cost and reach may be estimated by determining a correlation between each attribute of the first list 202 with the target segment.  The correlations may correspond to correlations between the attributes and the target\nsegment in a segment database, such as the segment databases 140.  The correlations may be applied to data regarding the attributes of the second list 206 received from the digital network.  For example, data regarding estimated audience size and\nestimated cost per view for each attribute of the second list 206 may be received from the digital network.  The correlations may be used to predict user behavior and may be combined with the data received from the digital network to generate estimated\ncost and estimated reach for each combination of attributes of the second list 206.  For example, an attribute A of the first list 202 may have a 0.5 correlation to a target segment X. An attribute B of the second list 206 may be mapped to (i.e.,\ncorrespond to) the attribute A and may also have a 0.5 correlation to the target segment X. The reach extension module 124 may receive data indicating that a digital network has 20 users with attribute B and that a cost per unique view of advertising\ndirected to users with attribute B is $2.  Based on the 0.5 correlation, the reach extension module 124 may predict that 10 of the 20 users with attribute B would, if tracked by the measurement system 120, be classified into target segment X. Therefore,\nthe reach extension module 124 may determine that 10 target users may be reached for $40 if the reach extension module initiates targeted messaging actions directed to attribute B.\nThe segment database may further include frequency information indicating how often a particular action is taken.  For example, the segment database may indicate that users who like product D buy product D, on average, 1.5 times per year.  The\nfrequency information may be used to determine an estimated yield of targeting messaging action directed to each attribute of the first list 202.  Thus, the estimated yield for targeted messaging actions directed to attribute B may be 15 purchases per\nyear.\nIn addition or in the alternative, the estimated cost, estimated reach, and the estimated yield may be based on messaging action history data such as messaging action history data stored in the messaging action history database 128 of FIG. 1. \nFor example, the messaging action history data may include information related to actual reach, actual cost, and actual yield of past targeted messaging actions.\nThe reach extension module may initiate targeted messaging actions based on the table 208.  For example, the reach extension module may compare estimated costs, estimated reaches, and estimated yields to decrease an estimated cost per\nconversion, increase an estimated reach, increase an estimated yield, or a combination thereof.  For example, the first list 202 may include a \"likes product D\" attribute with a corresponding \"likes product D\" attribute available for targeted messaging\nactions at the digital network.  The \"likes product D\" attribute of the first list 202 may be shared by 100% of the \"likes product D\" target segment.  Therefore, the table 208 may include a relatively high estimated reach for messaging actions targeted\nto users who like product D. However, the reach extension module may further identify that a likes TV network C attribute of the first list 202 is shared by a large percentage of the likes product D target segment.  In addition, a corresponding TV\nnetwork C fan group member attribute available for targeted messaging actions at the digital network may have a relatively lower estimated cost.  Based on comparisons between the estimated costs, estimated reaches, and estimated yields of messaging\nactions directed to users who like product D and messaging actions directed to TV network C fan group members, the reach extension module may determine that initiating messaging actions targeted to TV network C fan group members is predicted to be the\nmore cost/reach/yield-effective option.  The reach extension module may thus initiate messaging actions targeted to TV network C fan group members at the digital network.  Similarly, the reach extension module may determine to initiate targeted messaging\nactions based on the related attribute 203 \"likes TV show A\" rather than or in addition to messaging actions based on the first list 202 of attributes based on the comparisons.\nIn a particular embodiment, the reach extension module may compare estimated costs/reaches/yields to threshold costs/reaches/yields.  When the estimated cost, reach, yield or a combination thereof for a particular targeted messaging action meets\na threshold, the reach extension module may automatically initiate the particular targeted messaging action.  For example, the reach extension module may automatically initiate targeted messaging actions if an associated estimated reach or yield is above\na particular threshold or if an estimated cost is below a particular threshold.  Further, the reach extension module may receive updates from the digital network or from servers indicating user interactions with the initiated messaging actions as well as\nactual cost data.  Additionally, the reach extension module may receive updates from a segment database indicating that a correlation between an attribute tracked by the measurement system and the target segment has changed.  The reach extension module\nmay update the table 208 based on the received updates and modify messaging strategies based on the updated table 208.  For example, the reach extension module may issue an alert or may stop or slow initiation of messaging actions targeted to TV network\nC fan group members when the updated table 208 indicates an increased price, a decreased reach, a decreased yield or a combination thereof.  In another example, the reach extension module may initiate messaging actions (or additional messaging actions)\ntargeted to TV network C fan group members at an increased rate when the updated table 208 indicates a decreased price, an increased reach, an increased yield, or a combination thereof.\nThus, as illustrated in FIG. 2, a reach extension module (e.g., the reach extension module 124 of FIG. 1) may automatically initiate messaging actions targeted to users having attributes that are similar to a target segment.  The reach extension\nmodule may further monitor the success of the initiated messaging actions and adjust future messaging actions accordingly (e.g., by increasing messaging actions or decreasing messaging actions based on the success).\nReferring to FIG. 3, a diagram 300 illustrating mappings between attributes tracked by a measurement system and attributes for which targeted messaging actions are available at particular digital networks is shown.  In an illustrative\nembodiment, the measurement system may be the measurement system 120 of FIG. 1 and the digital networks may be the digital networks 150 of FIG. 1.  The mappings illustrated in FIG. 3 may be stored in one or more mapping databases, such as the mapping\ndatabases 126 of FIG. 1.  As illustrated in FIG. 3, attribute mappings may be one-to-one, one-to-many, and/or many-to-one.  Attribute mappings may be automatically generated and/or manually created by a user/administrator.  As described with reference to\nFIGS. 1-2, a reach extension module (e.g., the reach extension module 124 of FIG. 1) may identify a first list of attributes (e.g., \"likes TV network C\") that are tracked by the measurement system 120 and that are associated with a target segment.  The\nreach extension module may use the attribute mappings of FIG. 3 to identify corresponding attributes (e.g., \"member of TV network C fan group\", \"works at TV Network C,\" etc.) available for targeted messaging actions at various digital networks.  In a\nparticular embodiment, one or more attributes may be mapped to a \"Free Text Search\" option at a digital network.  In the example of FIG. 3, the attributes \"Income [110-120 k]\" and \"Likes Coffee Shop A\" in the measurement system do not have corresponding\nattributes at Digital Network B. Instead, the attributes are mapped to a \"Free Text Search,\" indicating that when initiating messaging actions directed to such attributes, a reach extension module would initiate a text-based search of user profiles in\nDigital Network B to identify users of interest.\nReferring to FIG. 4, a messaging action campaign graphical user interface (GUI) 400 is illustrated.  In an illustrative embodiment, the GUI 400 may be displayed by a web application in communication with a measurement system, such as the\nmeasurement system 120 of FIG. 1.  For example, the GUI 400 may be displayed to an advertiser, a media property owner, an administrator of the measurement system 120 of FIG. 1, etc. to track the performance of messaging action campaigns.  The GUI 400 may\ndisplay statistics for a plurality of messaging action campaigns (e.g., \"Product D,\" \"Restaurant Z,\" and \"Bookstore Y\").  In FIG. 4, statistics 402 are displayed for the selected \"Product D\" campaign.  The statistics 402 may include aggregated\ninformation regarding a plurality of messaging actions targeted to a plurality of attributes initiated by a reach extension module, such as the reach extension module 124 of FIG. 1.  In FIG. 4, the statistics 402 for the \"Product D\" campaign are\ndisplayed using a line graph 404 that illustrates conversions (e.g., yield) over time and includes information regarding actions taken by the reach extension module.  In the example of FIG. 4, an alert was issued at a first time 406 in response to a\ndeclining number of conversions.  Initiation of targeted messaging actions was stopped at a second time 408 and at a fourth time 412 (e.g., when a number of conversions met a threshold number for a particular period of time).  Initiation of targeted\nmessaging actions was increased at a third time 410 in response to a rising number of conversions.\nAs shown in FIG. 4, the statistics 402 for a messaging action campaign may also include a number of conversions (e.g., 100), a cost per acquisition (CPA) (e.g., $5.60), a reach (e.g., 500), an amount of money earned from the conversions (e.g.,\n$12,000), an amount of campaign budget spent/remaining (e.g., $560 of $900 spent), demographic information about an advertising audience (e.g., 51% male, 49% female, average age of 24), and/or device information (e.g., 57% mobile phones/tablet computers\nand 43% desktop/laptop computers).  The statistics may also display a number of impressions (e.g., 1,000) and a number of engagements (e.g., 300) associated with the messaging action campaign.  In alternate embodiments, a messaging action campaign GUI\nmay illustrate, more, less, and/or different types of messaging action statistics.\nFIG. 5 illustrates another embodiment of a system 500 operable to initiate targeted messaging actions based on audience attributes.  The system 500 includes a measurement system 520, which may, in an illustrative embodiment, correspond to the\nmeasurement system 120 of FIG. 1.  The system 500 further includes a reach extension module 524 and a monitoring module 550, which may partially or collectively correspond to the reach extension module 124 of FIG. 1.\nThe measurement system 520 may track data regarding audience segments for one or more media properties (e.g., a website) and may store the data in a data store 538.  In a particular embodiment, the data store 538 corresponds to the segment\ndatabases 140 of FIG. 1.  Each audience segment may have a relation to one or more demographic attributes, brand affinities, behaviors, or any combination thereof (e.g., each member of a segment HAS an attribute, each member of the segment does NOT have\nthe attribute, each member of the segment has a first attribute AND a second attribute, each member of the segment has the first attribute OR the second attribute, etc.).\nIn operation, the measurement system 520 may receive data indicating selection of a segment 532 from a client application 510 (e.g., a web browser executing at a computing device associated with an advertiser).  The segment 532 may be associated\nwith one or more media properties.  The selection of the segment 532 may indicate that a client (e.g., an advertiser) wishes to initiate messaging actions targeted to users similar to members of the segment 532 (e.g., to drive a behavior associated with\nthe members of the segment 532).  For example, the segment 532 may correspond to users who watched a previous coffee-related advertisement.  Based on a low cost, a high yield, a high reach, or a combination thereof of the previous advertisement, an\nadvertiser may select the segment 532 to target the same (or similar) users with a new advertisement for a new coffee-related product.  In response to the selection, the measurement system 520 may identify first attributes 534 (e.g., demographic\nattributes and interests) for each member of the segment 532.  Data indicating the first attributes 534 may be sent to the reach extension module 524 to create a messaging action campaign 526.  In a particular embodiment, data identifying second\nattributes that are correlated to the first attributes 534 is also sent to the reach extension module 524 to create the messaging action campaign 526, as described with reference to FIGS. 1-2.\nThe messaging action campaign 526 includes message groups 528.  The message groups 528 may include a message group for each combination of the first and second attributes for each of the digital networks 530.  Each message group of the message\ngroups 528 may include one or more third attributes for which targeted messaging actions are available.\nThe reach extension module 524 may analyze each message group of the message groups 528 to determine an estimated cost, an estimated reach, and an estimated yield of messaging actions based on the message group.  The estimated cost, the\nestimated reach, and the estimated yield may be estimated based on data stored in the data store 538 (e.g., past cost, past reach data, past yield data, correlation data, etc.) and messaging strategies stored in a relational database 536.  When the\nestimated cost, the estimated reach, the estimated yield, or a combination thereof, of a particular message group meets a threshold, the reach extension module 524 may create messaging strategy to automatically initiate targeted messaging actions.  The\ntargeted messaging actions may correspond to the particular message group at a particular digital network of the digital networks 530.  The reach extension module 524 may store information regarding the message strategy in the relational database 536 of\nthe measurement system 520.  The information may identify the segment 532, an estimated cost, an estimated reach, an estimated yield, specific targeted messaging actions to be initiated, or a combination thereof.\nIn the illustrated example, the initiated targeted messaging actions include an advertisement 542 to be presented to a user (e.g., an individual viewing a website) via a user browser 540.  In other examples, the messaging actions may include\nother types of messaging actions in addition to or instead of the advertisement 542.  The particular digital network may count impressions (e.g., a number of times the advertisement 542 has been seen by users) resulting from the targeted advertising. \nWhen the user clicks on the advertisement 542, the user browser 540 may display a conversion page 544.  The conversion page 544 may correspond to a web page of the client.  The conversion page 544 may also be displayed responsive to an action within a\nmobile application (e.g., a purchase) or an action at a specific venue (e.g., a RFID \"check-in\").  In particular embodiments, the particular digital network counts a number of clickthroughs (e.g., a number of times users click on the advertisement 542).\nFrom the conversion page 544, the user may initiate a conversion event (e.g., buying an advertised product or service from the client, watching a video, etc.).  The conversion event may result in a conversion \"pixel\" (e.g., message or data)\nbeing sent to the particular digital network notifying the particular digital network that the conversion event occurred.  In addition, an event capture module 531 may capture the conversion event and update the first attributes 534 accordingly (examples\nof an event capture module capturing events are described in reference to FIG. 8).  Thus, the conversion event can be associated with users of certain demographics and interests, which may enable a system (e.g., the measurement system 120 of FIG. 1) to\nfind additional users that are similar to users that are converting.  The updated first attributes 534 may in turn be used to update the message groups 528.\nIn a particular embodiment, the user browser 540 may receive a retargeting \"pixel\" 546 from the conversion page 544.  The retargeting pixel 546 may be observed by one or more of the digital networks 530 and used to send particular advertising to\nthe user browser 540.  To illustrate, one of the benefits of attributing conversion(s) to a specific audience is an ability to value the audience for retargeting.  For example, different bids and budgets may be used for different retargeting segments.\nA digital network monitor 556 of the monitoring module 550 may receive updates from the digital networks 530 (e.g., the digital networks 530 may send updates periodically or when a particular message is viewed or clicked on).  The updates may\ninclude performance data related to particular targeted messaging actions, such as cost per message, a number of impressions, a number of conversions, total money spent, or a combination thereof.  The digital network monitor 556 may update the data store\n538 based on the performance data.\nA campaign monitor 552 of the monitoring module 550 may monitor the performance data stored in the data store 538.  Based on the performance data, alert logic 554 of the campaign monitor 552 may issue commands to the reach extension module 524\nto adjust the message strategy.  For example, the alert logic 554 may issue a command to increase targeted messaging actions based on a particular message group of the message groups 528 in response to detecting that a reach, a number of conversions\n(e.g., a yield), a cost, or a combination thereof, associated with the messaging actions based on the particular message group meets a first threshold.  Alternatively, the alert logic 554 may issue a command to decrease messaging actions, stop messaging\nactions, issue a warning, or a combination thereof in response to detecting that the reach, number of conversions, cost, or a combination thereof, associated with the messaging actions based on the particular message group meets a second threshold.  In\nresponse to the command from the alert logic 554, the reach extension module 524 may alter the message strategy and update (e.g., by changing the message strategy to increase, decrease, or stop initiation of messaging actions) the advertising strategy\nstored in the relational database 536.  The reach extension module 524 may continue to initiate messaging actions according to the updated message strategy.  When the alert logic 554 of the campaign monitor 552 issues a stop command, the reach extension\nmodule 524 may stop initiating messaging actions.  In a particular embodiment, the reach extension module 524 may stop initiating messaging actions for a period of time before automatically resuming initiating messaging actions in response to the period\nof time elapsing.  In an alternate embodiment, the reach extension module 524 may stop initiating messaging actions until a command is received to continue initiating messaging actions (e.g., from the alert logic 554 or from a content producer).\nThus, the system 500 may automatically initiate targeted messaging actions that are more likely to reach users with certain attributes.  The targeted messaging actions may thus be sent to users more likely to perform a desired action than a\nrandom sampling of people.  The targeted messaging actions may be based on estimated costs and effectiveness data (e.g., how well an audience performs a desired behavior), which may be updated as messaging actions are initiated.  The updated cost and\neffectiveness data may be used to update a message strategy.\nReferring to FIG. 6, a method 600 of initiating targeted messaging actions is shown.  In an illustrative embodiment, the method 600 may be performed by a measurement system, such as the measurement system 120, the measurement system 520 of FIG.\n5, or a measurement system as further described with reference to FIGS. 8-9.\nThe method 600 includes receiving input identifying a target audience segment, at 602.  For example, the measurement system 120 of FIG. 1 may receive input (e.g., from a client application, such as a web client, in response to user selection)\nidentifying the target segment 110.  As another example, the measurement system 520 may receive input identifying the segment 532 from the client application 510 (e.g., in response to a user selection).  The target segment may be associated with a\nparticular behavior, perspective, or opinion.  To illustrate, referring to FIG. 2, the target segment is a segment of users who like product D.\nThe method 600 further includes identifying a first attribute measured by a measurement system (e.g., the measurement system 120 or the measurement system 520), at 604.  The first attribute is determined to correlate to users tracked by the\nmeasurement system and that belong to the target audience segment.  For example, the measurement system 120 may identify the target segment 110 in the segment databases 140 and determine a first list of attributes associated with at least one member of\nthe target segment 110.  In the example illustrated in FIG. 2, the first list 202 includes attributes associated with at least one member of the target segment of users who like product D. As a further example, the measurement system 520 may identify the\nfirst attributes 534 (e.g., demographics and interests) of users of the segment 532.\nThe method 600 further includes identifying a second attribute that corresponds to the first attribute, where a messaging action directed to the first attribute and/or the second attribute is available for purchase at a digital network, at 606. \nFor example, the reach extension module 124 may use the mapping databases 126 to map the first list of attributes associated with the target segment 110 to a second list of attributes for which messaging actions are available at the digital networks 150. In the example shown in FIG. 2, attributes of the first list 202 are mapped to the attributes 204 for which messaging actions are available to generate the second list 206.  As another example, the first attributes 534 of FIG. 5 (e.g., demographics and\ninterests) may be mapped to attributes for which messaging actions are available at the digital networks 530 to form the message groups 528.  The method 600 further includes initiating the messaging action at the digital network directed to the first\nattribute and/or the second attribute, at 608.\nIn some embodiments, instead of or in addition to identifying the second attribute (at 606) and initiating the messaging action at the digital network (at 608), the method 600 may include initiating an internal messaging action, at 610.  The\ninternal messaging action may be directed to other users tracked by the measurement system that do not belong to the target segment (e.g., have not performed a target behavior) but that may have the first attribute and/or a related attribute, as\ndescribed with reference to the internal data/system(s) 129 of FIG. 1.\nReferring to FIG. 7, another method 700 of initiating targeted messaging actions is shown.  In an illustrative embodiment, the method 700 may be performed by a measurement system, such as the measurement system 120, the measurement system 520 of\nFIG. 5, or a measurement system as further described with reference to FIGS. 8-9.\nThe method 700 includes receiving input identifying a target audience segment that corresponds to a particular behavior, at 702.  For example, the measurement system 120 of FIG. 1 may receive input identifying the target segment 110.  As another\nexample, the measurement system 520 may receive input identifying the segment 532 from the client application 510.  To illustrate, referring to FIG. 2, the target segment is a segment of users who like product D (e.g., buy or use product D).\nThe method 700 further includes identifying a first attribute measured by a measurement system, where the first attribute is determined to correlate to users tracked by the measurement system and that belong to the target audience segment, at\n704.  For example, the measurement system 120 may identify the target segment 110 in the segment databases 140 and determine a first list of attributes associated with at least one member of the target segment 110.  In the example illustrated in FIG. 2,\nthe first list 202 includes attributes associated with at least one member of the target segment of users who like product D (e.g., buy or use product D).  As a further example, the measurement system 520 may identify the first attributes 534 (e.g.,\ndemographics and interests) of users of the segment 532.\nThe method 700 further includes identifying a second attribute measured by the measurement system, where the second attribute is determined to correlate to users indicated by the measurement system as having the first attribute, at 706.  For\nexample, the measurement system 120 may identify attributes related to attributes of the first list.  As shown in FIG. 2, the related attribute 203 may be determined to be related to an attribute of the first list 202.  The relation may be based on\ncorrelation between the related attribute and an attribute on the first list.\nThe method 700 further includes comparing estimated yields, estimated costs, estimated reaches, or a combination thereof of a first messaging action available for purchase at one or more digital networks and estimated yields, estimated costs,\nestimated reaches, or a combination thereof of a second messaging action available at the one or more digital networks, at 708.  A target of the first messaging action is based on the first attribute and a target of the second messaging action is based\non the second attribute.  For example, the table 208 of FIG. 2 may be used to compare estimated cost/reach of messaging actions based on the related attribute 203 \"Likes TV Show A\" (e.g., a second attribute) to estimated cost/reach of messaging actions\nbased on an attribute \"Likes Product D\" (e.g., a first attribute) from the first list 202.  The method 700 further includes initiating the second messaging action at based on the comparisons, at 710.\nAs described herein, to perform \"intelligent\" evaluation of targeted messaging actions to improve cost/reach/yield-effectiveness, a measurement system may rely on data that has been collected about audiences of media properties.  FIGS. 8-9\nillustrate examples such measurement systems.\nFIG. 8 illustrates an embodiment of a measurement system 840, and is generally designated 800.  For example, the measurement system 840 may include, correspond to, or be included within the measurement system 120 of FIG. 1 or the measurement\nsystem 520 of FIG. 5.  The measurement system 840 may be communicatively coupled to one or more user devices (e.g., illustrative user devices 812, 814, and 816), to one or more content delivery networks (CDNs) (e.g., illustrative CDN 822), and to media\nproperties (e.g., websites) 832 and 834.  In FIG. 8, the media properties 832 and 834 are illustrated by corresponding servers (e.g., web servers).  The measurement system 840 may be implemented using one or more computing devices (e.g., servers).  For\nexample, such computing devices may include one or more processors or processing logic, memories, and network interfaces.  The memories may include instructions executable by the processors to perform various functions described herein.  The network\ninterfaces may include wired and/or wireless interfaces operable to enable communication to local area networks and/or wide area networks (e.g., the Internet).\nThe user devices 812-816 may be associated with various users.  For example, the desktop computing device 812 and the tablet computing device 814 may be associated with a first user 802, and the mobile telephone device (e.g., smartphone) 816 may\nbe associated with a second user 804.  It should be noted that the user devices 812-816 are shown for example only and are not to be considered limiting.  In alternate embodiments, fewer, additional, and/or different types of user devices may be present\nin the system 800.  For example, a radio-frequency identification (RFID)-enabled device may be carried by a user and may transmit a signal in response to detecting that the user is visiting a particular physical location.  In a particular embodiment, the\nuser devices 812-816 may execute applications that are operable to access the media properties 832 and 834.  For example, the user devices 812-816 may include applications developed using a mobile software development kit (SDK) that includes support for\naudience measurement functions.  To illustrate, when the SDK-based applications interact with the media properties 832 and 834, the applications may generate first event signals 810 that are transmitted by the user devices 812-816 to the measurement\nsystem 840.\nThe first event signals 810 may include information identifying specific interactions by the users 802-804 via the user devices 812-816 (e.g., what action was taken at a media property, when the action was taken, for how long the action was\ntaken, etc.).  The user interactions may include interactions with advertisements presented by the media property and/or interactions with content presented by the media property.  The event signals 810 may also include an identifier, such as a browser\nidentifier (browser ID) generated by the SDK.  In a particular embodiment, browser identifiers are unique across software installations and devices.  For example, a first installation of a SDK-based application at the desktop computing device 812 and a\nsecond installation of the same SDK-based application at the tablet computing device 814 may use different browser IDs, even though both installations are associated with the same user 802.\nIn another particular embodiment, Browser IDs may remain consistent until applications or web browsers are \"reset\" (e.g., caches/cookies are cleared).  In some embodiments, the user devices 812-816 may execute applications other than browser\napplications, such as downloadable mobile applications, that generate the event signals 810 based on user interactions with advertisements and/or content presented by the applications.\nThe user devices 812-816 may access content provided by the media properties 832 and 834 directly or via the CDN 822.  The CDN 822 may provide distributed, load-balanced access to audio, video, graphics, and web pages associated with the media\nproperties 832 and 834.  For example, the CDN 822 may include geographically distributed web servers and media servers that serve Internet content in a load-balanced fashion.  The CDN 822 may send second event signals 820 to the measurement system 840. \nThe second event signals 820 may include information identifying interactions with media properties and browser IDs provided to the CDN 822 by the user devices 812-816 and/or the media properties 832 and 834.  For example, the second event signals 820\nmay include CDN logs or data from CDN logs.\nThe media properties 832 and 834 may be controlled by the same entity (e.g., may be part of a federated property) or by different entities.  The media properties 832 and 834 may send third event signals 830 to the measurement system 840.  The\nthird event signals 830 may include information identifying interactions with the media properties and browser IDs provided by the user devices 812-816 during communication with the media properties 832 and 834 (e.g., communication via hypertext transfer\nprotocol (HTTP), transport control protocol/internet protocol (TCP/IP), or other network protocols).\nIn a particular embodiment, the third event signals 830 may include server logs or data from server logs.  Alternately, or in addition, the third event signals 830 may be generated by SDK-based (e.g., web SDK-based) applications executing at the\nmedia properties 832 and 834, such as scripts embedded into web pages hosted by the media properties 832 and 834.\nIn a particular embodiment, the media properties 832 and 834 may send data to the measurement system 840 and receive data from the measurement system 840 regarding advertisements and/or content presented by the media properties 832 and 834. \nSuch communication is illustrated in FIG. 8 as advertisement/content communication 860.  For example, an advertisement (or software associated with the advertisement that is executing on a client device, such as web server, a computer, a mobile phone, a\ntablet device, etc.) may collect and transmit data on a per-advertisement, per-user basis.  The data may include or identify a profile of a user, a duration that the user viewed the advertisement, action(s) performed by the user with respect to the\nadvertisement, etc. As another example, a content item or software associated therewith may collect and transmit data regarding user interactions with the content item.\nIn a particular embodiment, the measurement system 840 includes a data filtering module 842, a data processing module 844, a data reporting module 846, and a reach extension module 847 (e.g., the reach extension module 124 of FIG. 1 or the reach\nextension module 524 of FIG. 5).  In a particular embodiment, each of the modules 842-847 is implemented using instructions executable by one or more processors at the measurement system 840.\nThe data filtering module 842 may receive the event signals 810, 820, and 830.  The data filtering module 842 may check the event signals 810, 820, and 830 for errors and may perform data cleanup operations when errors are found.  The data\nfiltering module 842 may also receive and perform cleanup operations on advertisement measurement data and content measurement data received from the media properties 832 and 834 and from applications executing on the user devices 812-816.  In a\nparticular embodiment, the data filtering module 842 may implement various application programming interfaces (APIs) for event signal collection and inspection.  The data filtering module 842 may store authenticated/verified event signals in a database,\nevent cache or archive, such as in data storage 848 and/or cloud storage 852.  In a particular embodiment, the measurement system 840 includes or has access to a brand database that tracks brands.  For example, \"raw\" data corresponding to the brand\ndatabase and other collected data may be stored in the cloud storage 852.  Signals received from the media properties 832 and 834 and from applications executing the user devices 812-816 may identify a brand that matches one of the brands in the brand\ndatabase.  The measurement system 840 may thus track advertisements/content for various brands across multiple media properties.\nThe data processing module 844 may associate received event signals (and interactions represented thereby) with user profiles of users.  For example, when an event signal having a particular browser ID is a social networking registration event\n(e.g., when a user logs into a website using a Facebook.RTM.  account, a Twitter.RTM.  account, a LinkedIn.RTM.  account, or some other social networking account), the data processing module 844 may retrieve a corresponding social networking profile or\nother user profile data from third party data sources 850.  Facebook is a registered trademark of Facebook, Inc.  of Menlo Park, Calif.  Twitter is a registered trademark of Twitter, Inc.  of San Francisco, Calif.  LinkedIn is a registered trademark of\nLinkedIn Corp.  of Mountain View, Calif.  In a particular embodiment, the social networking profile or other user profile data is received after an authentication process.  For example, the measurement system 840 may receive a user token.  The user token\nmay enable the measurement system 840 to request a social network for information associated with a corresponding user.\nIt will be appreciated that interactions that were previously associated only with the particular browser ID (i.e., \"impersonal\" alphanumeric data) may be associated with an actual person (e.g., John Smith) after retrieval of the social\nnetworking profile or user profile.  Associating interactions with individuals may enable qualitative analysis of the audiences of media properties.  For example, if John Smith is a fan of a particular sports team, the measurement system 840 may indicate\nthat at least one member of the audience of the first media property 832 or the second property 834 is a fan of the particular sports team.  When a large percentage of a media property's audience shares a particular characteristic or interest, the media\nproperty may use such information in selecting and/or generating advertising or content.  User profiles (e.g., a profile of the user John Smith) and audience profiles (e.g., profiles for the media properties associated with the media properties 832 and\n834) may be stored in the data storage 848, the cloud storage 852, and/or in another database, as further described with reference to FIG. 9.  An audience profile for a particular media property may be generated by aggregating the user profiles of the\nindividual users (e.g., including John Smith) that interacted with the particular media property.\nAudience profiles may be generated using as few as one or two user profiles, although any number of user profiles may be aggregated.  In a particular embodiment, audience profiles may be updated periodically (e.g., nightly, weekly, monthly,\netc.), in response to receiving updated data for one or more users in the audience, in response to receiving a request for audience profile data, or any combination thereof.  Audience profiles may similarly be generated for audiences of a particular\nmobile application based on signals generated by installations of the mobile application on various user devices.\nThe data reporting module 846 may generate various interfaces, such as the GUI 400 of FIG. 4.  The data reporting module 846 may also support an application programming interface (API) that enables external devices to view and analyze data\ncollected and stored by the measurement system 840.  In a particular embodiment, the data reporting module 846 is configured to segment the data.  In a particular embodiment, the measurement system 840 may be operable to define \"new\" segments based on\nperforming logical operations (e.g., logical OR operations and logical AND operations).\nThe data processing module 844 may also be configured to, upon receiving an event signal, parse the event signal to identify what user and media property the event signal corresponds to.  The data processing module 844 may store data\ncorresponding to the event signal in one or more databases (e.g., the cloud storage 852, the data storage 848, a user profile database, etc.).  If the user is a new audience member for the media property, the data processing module 844 may assign a new\nID to the user.\nDuring operation, the users 802-804 may interact with the media properties 832 and 834 and with applications executing on the user devices 812-816.  In response to the interactions, the measurement system 840 may receive the event signals 810,\n820, 830, and/or 860.  Each event signal may include a unique identifier, such as a browser ID and/or an audience member ID.  If the user is a \"new\" audience member, the data processing module 844 may create a user profile.  Data for the user profile may\nbe stored in the cloud storage 852 and/or the data storage 848.  In a particular embodiment, data for the user profile may be retrieved from the third party data sources 850.\nFor example, the data processing module 844 may retrieve and store data from one or more social network profiles of the user.  The data may include demographic information associated with the user (e.g., a name, an age, a geographic location, a\nmarital/family status, a homeowner status, etc.), social information associated with the user (e.g., social networking activity of the user, social networking friends/likes/interests of the user, etc.), and other types of data.  The data processing\nmodule 844 may also collect and store data associated with advertisements and content served by the media properties 832 and 834 and by applications executing on the user devices 812-816.  In a particular embodiment, the measurement system 840 is further\nconfigured to receive offline data from external data sources.  For example, the measurement system 840 may receive data regarding transactions (e.g., purchases) made by an audience and may use the transaction data to generate additional signals that\ncontribute to a set of traits of an audience, brand, property, etc. Another example of offline data may be a \"data dump\" of data collected by an RFID-enabled device or an RFID detector.  Offline data may be stored in one or more computer-readable files\nthat are provided to the measurement system 840.  In a particular embodiment, offline data can include previously collected data regarding users or audience members (e.g., names, addresses, etc.).\nThe data reporting module 846 may report data collected by the measurement system 840.  For example, the data reporting module 846 may generate reports based on an audience profile of a media property (or application), where the audience profile\nis based on aggregating user profiles of users that interacted with the media property (or application).  To illustrate, the data reporting module 846 may generate an interface indicating demographic attributes of the audience as a whole (e.g., a\npercentage of audience members that are male or female, percentages of audience members in various age brackets, percentages of audience members in various income bracket, most common audience member cities/states of residence, etc.).  The interface may\nalso indicate social attributes of the audience as a whole (e.g., the most popular movies, sports teams, etc. amongst members of the audience).  Audience profiles may also be segmented and/or aggregated with other audience profiles.  Audience profiles\nmay further be segmented based on advertisement, advertisement campaign, brand, content item, etc. Audience profiles may also be constructed by combining segments.\nIn a particular embodiment, the system 800 may also receive event signals based on measurements (e.g., hardware measurements) made at a device.  For example, an event signal from the tablet computing device 814 or the mobile telephone device 816\nmay include data associated with a hardware measurement at the tablet computing device 814 or the mobile telephone device 816, such as an accelerometer or gyroscope measurement indicating an orientation, a tilt, a movement direction, and/or a movement\nvelocity of the tablet computing device 814 or the mobile telephone device 816.  As another example, the system 800 may receive a signal in response to an RFID device detecting that a user is visiting a particular physical location.  The system 800 of\nFIG. 8 may also link interactions with user profiles of users.  This may provide information of \"how many\" viewers and \"how long\" the viewers watched a particular video (e.g., as in direct response measurement systems), and also \"who\" watched the\nparticular video (e.g., demographic, social, and behavioral attributes of the viewers).\nFIG. 9A illustrates a particular embodiment of a system 900 in accordance with the present disclosure.  The system 900 includes a data collection tier (e.g., subsystem) 910, an event processing tier 950, a monitoring tier 970, and a reach\nextension module 990 (e.g., the reach extension module 124 of FIG. 1, the reach extension module 524 of FIG. 5, or the reach extension module 847 of FIG. 8).  Components of the data collection tier 910 are illustrated in further detail in FIG. 9B. \nComponents of the event processing tier 950 are illustrated in further detail in FIG. 9C.  Components of the monitoring tier are illustrated in further detail in FIG. 9D.  As further described with reference to FIG. 9D, the monitoring tier includes a\npenetration monitor 974 that is illustrated using horizontal and vertical hatching, a system monitor 978 that is shown using diagonal hatching, and a ping monitor 984 that is shown using horizontal-only hatching.  Various other components in FIG. 9\ninclude indicators with hatching corresponding to their respective monitor(s).  For example, capture servers 926 include indicators to illustrate that the capture servers are monitored by both the penetration monitor 974 and the system monitor 978.\nThe system 900 includes (or has access to) an authentication provider 932, third party data sources 934, an audience web application 946, a first framework 944, a second framework 942, a database 948, an interrogator 938, a data store 936, and\nan index 940.  In an illustrative embodiment, the third party data sources 934 are the third party data sources 850 of FIG. 8, and the event processing tier 950 and the interrogator 938 correspond to the data processing module 844 of FIG. 8.  In a\nparticular embodiment, information from the third party data sources 934 is mapped to information collected by the system 900 by using personally identifiable information as a key to the third party data sources 934.  For example, personally identifiable\ninformation may include an e-mail address, first/last name, a mailing or residential address, etc. To illustrate, when the system 900 has an e-mail address for a user, the system 900 may request the third party data sources 934 for additional information\nassociated with the e-mail address.\nThe data collection tier 910 includes a content management system (CMS) 912, cloud storage 916, content delivery networks 918, client browsers 920, and client servers 922.  The data collection tier 910 may further include an application\nprogramming interface (API) 921.  The API 921 includes a load balancer 924, the capture servers 926, and cloud storage 930.\nThe event processing tier 950 includes a job queues module 951, an anonymous buffer 960, and an event bundle buffer 962.  The job queues module 951 includes an authentication token handler 952, an event dispatch 956, and an event bundle handler\n958.  In alternate embodiments, the job queues module 951 may include more, fewer, and/or different handlers than illustrated in FIG. 9.\nThe monitoring tier 970 includes an internal monitoring module 972, the ping monitor 984, and a notifications module 982.  The internal monitoring module 972 includes the penetration monitor 974, a performance analysis module 976, the system\nmonitor 978, and an alert rules module 980.\nDuring operation, the content management system 912 may be used to generate a client specific script (e.g., webscript) 914 for various clients (e.g., media properties).  The client specific script 914 may be stored in the cloud storage 916 and\nreplicated to the content delivery networks 918.  As audience members register and interact with a media property, the content delivery networks 918 may deliver the client specific script 914, along with property content, to the client browsers 920. \nBased on the client specific script 914, the client browsers 920 may generate tags (e.g., a tag corresponding to a particular user activity, such as watching a video) or tokens (e.g., a social networking registration token).  The tags or tokens may be\nsent to the load balancer 924.  The client servers 922 may also generate tags or tokens to send to the load balancer 924 based on user registrations and user activity at media properties.  The tags or tokens from the client servers 922 may be\nauthenticated by the authentication provider 932.\nThe load balancer 924 may send the tags or tokens to the capture servers 926 based on a load balancing algorithm.  The capture servers 926 may generate event data (e.g., event signals) based on the tags or tokens.  The capture servers 926 may\nstore the event data in event logs 928 in the cloud storage 930 and send the event data to the job queues module 951.\nThe job queues module 951 may distribute the event data to different event handler(s) based on the type of the event data.  For example, event data including an authentication token may be sent to the authentication token handler 952.  In\naddition, event data requiring additional information from social media sources may be sent to the authentication token handler 952.  The handler 952 may perform asynchronous event collection operations based on the received event data.  For example,\nwhen a new user registers with a media property using a social networking profile, a token may be provided by the data collection tier to the authentication token handler 952.  The handler 952 may use the token to retrieve demographic and brand affinity\ndata for the user from the user's social networking profile.\nEvent signals may also be sent to the event dispatch 956, which determines whether the event signals correspond to known or unknown users.  When event data corresponds to an unknown user, the event dispatch 956 buffers the event data in the\nanonymous buffer 960.  After a period of time (e.g., three days), event data from the anonymous buffer 960 may be sent to the job queues module 951 to be processed again.\nWhen event data corresponds to a \"known\" user (e.g., a user that has already been assigned a user ID), the event dispatch 956 may send the event data to the event bundles buffer 962.  The event bundle handler 958 may retrieve event data stored\nin the event bundles buffer 962 every bundling period (e.g., one hour).  The event bundles processor 958 may bundle event data received each bundling period into an event bundle that is sent to the interrogator 938.\nThe interrogator 938 may parse the event bundle and update the data store 936, the database 948 (e.g., a relational database), and/or the index 940.  In a particular embodiment, the database 948 corresponds to a profiles database that is\naccessible the first framework 944 to the audience web application 946.  For example, the first framework 944 may be a database-driven framework that is operable to dynamically generate webpages based on data in the database 948.  The audience web\napplication may be operable to generate various graphical user interfaces (e.g., the GUI 400 of FIG. 4) to analyze the data collected by the system 900.  The index 940 may be accessible to the audience web application 946 via the second framework 942. \nIn one example, the second framework 942 supports representational state transfer (REST)-based data access and webpage navigation.  Although not shown, in particular embodiments, the data store 936 may also be accessible to the audience web application\n946.\nThe monitoring tier 970 may monitor the various components of the system 900 during operation to detect errors, bottlenecks, network intrusions, and other issues.  For example, the penetration monitor 974 may collect data indicating unauthorized\naccess to or from the capture servers 926 and the first framework 944.  The penetration monitor 974 may provide the data to the alert rules module 980.  Similarly, the system monitor 978 may collect performance data from the capture servers 926, from the\nsecond framework 942, and from the data store 936.  The system monitor 978 may provide the performance data to the performance analysis module 976, which may analyze the data and send the analyzed data to the alert rules module 980.  The alert rules\nmodule 980 may compare received data to alert rules and, based on the comparison, send an alert to the notifications module 982.  For example, the alert rules module 980 may determine that an intruder has accessed components of the system 900 or that the\nsystem 900 is not operating at a desired level of efficiency, and may send an alert to the notifications module 982.\nThe notifications module 982 may also receive alerts from the ping monitor 984.  The ping monitor 984 may monitor the load balancer 924 and the audience web application 946 and collect data regarding uptime, downtime, and performance, and\nprovide alerts to the notification module 982.\nThe notification module 982 may send notifications (e.g., via short message service (SMS), e-mail, instant messaging, paging, etc.) to one or more technical support staff members 964 to enable timely response in the event of errors, performance\nbottlenecks, network intrusion, etc.\nIn accordance with various embodiments of the present disclosure, the methods, functions, and modules described herein may be implemented by hardware, software programs executable by a computer system, or a combination thereof.  Further, in an\nexemplary embodiment, implementations can include distributed processing, component/object distributed processing, and parallel processing.  Alternatively, virtual computer system processing can be constructed to implement one or more of the methods or\nfunctionality as described herein.\nParticular embodiments can be implemented using a computer system executing a set of instructions that cause the computer system to perform any one or more of the methods or computer-based functions disclosed herein.  A computer system may\ninclude a laptop computer, a desktop computer, a mobile phone, a tablet computer, a set-top box, a media player, or any combination thereof.  The computer system may be connected, e.g., using a network, to other computer systems or peripheral devices. \nFor example, the computer system or components thereof can include or be included within any one or more devices, modules, and/or components illustrated in FIGS. 1-9.  In a networked deployment, the computer system may operate in the capacity of a server\nor as a client user computer in a server-client user network environment, or as a peer computer system in a peer-to-peer (or distributed) network environment.  The term \"system\" can include any collection of systems or sub-systems that individually or\njointly execute a set, or multiple sets, of instructions to perform one or more computer functions.\nIn a particular embodiment, the instructions can be embodied in a computer-readable or a processor-readable device.  The terms \"computer-readable device\" and \"processor-readable device\" include a single storage device or multiple storage\ndevices, such as a centralized or distributed database, and/or associated caches and servers that store one or more sets of instructions.  The terms \"computer-readable device\" and \"processor-readable device\" also include any device that is capable of\nstoring a set of instructions for execution by a processor or that cause a computer system to perform any one or more of the methods or operations disclosed herein.  For example, a computer-readable or processor-readable device or storage device may\ninclude random access memory (RAM), flash memory, read-only memory (ROM), programmable read-only memory (PROM), erasable programmable read-only memory (EPROM), electrically erasable programmable read-only memory (EEPROM), registers, a hard disk, a\nremovable disk, a disc-based memory (e.g., compact disc read-only memory (CD-ROM)), or any other form of storage device.  A computer-readable or processor-readable device is not a signal.\nIn accordance with at last one described embodiment, a method includes receiving an input identifying a target audience segment.  The method further includes identifying a first attribute measured by a measurement system, where the first\nattribute is determined to correlate to users tracked by the measurement system and that belong to the target audience segment.  The method further includes identifying a second attribute that corresponds to the first attribute.  A messaging action\ndirected to the first attribute, the second attribute, or a combination thereof is available at a digital network.  The method further includes initiating the messaging action at the digital network directed to the first attribute, the second attribute,\nor a combination thereof.\nIn another particular embodiment, a method includes receiving an input identifying a target audience segment that corresponds to a particular behavior.  The method further includes identifying a first attribute measured by a measurement system,\nwhere the first attribute is determined to correlate to users tracked by the measurement system that have exhibited the particular behavior and that belong to the target audience segment.  The method further includes identifying a second attribute\nmeasured by the measurement system, where the second attribute is determined to correlate to users indicated by the measurement system as having the first attribute.  The method further includes determining a first metric associated with a first\nmessaging action that is available at one or more digital networks, where the first messaging action is based on the first attribute, and where the first metric includes a first estimated yield, a first estimated cost, a first estimated reach, or a\ncombination thereof.  The method includes determining a second metric associated with a second messaging action that is available at the one or more digital networks and that is directed to the second attribute, where a target of the second messaging\naction is based on the second attribute, and where the second metric includes a second estimated yield, a second estimated cost, a second estimated reach, or a combination thereof.  The method also includes initiating the second messaging action based on\na comparison of the first metric to the second metric.\nIn another particular embodiment, a computer readable storage device stores instructions that when executed by a processor cause the processor to perform operations.  The operations include identifying, based on received input identifying a\ntarget audience behavior, a first attribute measured by a measurement system, where the first attribute is correlated to users identified by the measurement system as having performed the target behavior.  The operations further include initiating a\nmessaging action directed to a second attribute at a digital network, where the second attribute corresponds to the first attribute.\nThe illustrations of the embodiments described herein are intended to provide a general understanding of the structure of the various embodiments.  The illustrations are not intended to serve as a complete description of all of the elements and\nfeatures of apparatus and systems that utilize the structures or methods described herein.  Many other embodiments may be apparent to those of skill in the art upon reviewing the disclosure.  Other embodiments may be utilized and derived from the\ndisclosure, such that structural and logical substitutions and changes may be made without departing from the scope of the disclosure.  Accordingly, the disclosure and the figures are to be regarded as illustrative rather than restrictive.\nAlthough specific embodiments have been illustrated and described herein, it should be appreciated that any subsequent arrangement designed to achieve the same or similar purpose may be substituted for the specific embodiments shown.  This\ndisclosure is intended to cover any and all subsequent adaptations or variations of various embodiments.  Combinations of the above embodiments, and other embodiments not specifically described herein, will be apparent to those of skill in the art upon\nreviewing the description.\nThe Abstract of the Disclosure is submitted with the understanding that it will not be used to interpret or limit the scope or meaning of the claims.  In addition, in the foregoing Detailed Description, various features may be grouped together\nor described in a single embodiment for the purpose of streamlining the disclosure.  This disclosure is not to be interpreted as reflecting an intention that the claimed embodiments require more features than are expressly recited in each claim.  Rather,\nas the following claims reflect, inventive subject matter may be directed to less than all of the features of any of the disclosed embodiments.\nThe above-disclosed subject matter is to be considered illustrative, and not restrictive, and the appended claims are intended to cover all such modifications, enhancements, and other embodiments, which fall within the true scope of the present\ndisclosure.  Thus, to the maximum extent allowed by law, the scope of the present disclosure is to be determined by the broadest permissible interpretation of the following claims and their equivalents, and shall not be restricted or limited by the\nforegoing detailed description.", "application_number": "14448672", "abstract": " A method includes receiving an input identifying a target audience\n     segment (e.g., reflecting a behavior, perspective, or opinion an\n     advertiser would like to influence/increase). The method further includes\n     identifying a first attribute measured by a measurement system, where the\n     first attribute is determined to correlate to users tracked by the\n     measurement system and that belong to the target audience segment, which\n     reflects the behavior that the advertiser wants to influence/increase.\n     The method further includes identifying a second attribute that\n     corresponds to the first attribute, where a messaging action directed to\n     the first attribute, the second attribute, or a combination thereof is\n     available at one or more digital networks. The digital networks may\n     require payment for message delivery or may allow such messaging for\n     free. The method further includes initiating the messaging action aimed\n     at driving behavior directed to the first attribute, the second\n     attribute, or a combination thereof.\n", "citations": ["5724521", "5848396", "6134532", "6286005", "6470079", "6654725", "6772129", "7103592", "7197459", "7254547", "7356547", "7383243", "7424439", "7519562", "7523087", "7689456", "7729940", "7835940", "7958000", "8000993", "8060398", "8112301", "8464290", "8620748", "8634814", "8762201", "9117227", "20020144262", "20030154212", "20050093866", "20050125289", "20060041480", "20060047725", "20060116930", "20070100805", "20070124290", "20070208728", "20070214097", "20070260635", "20080091508", "20080103898", "20080109491", "20080140508", "20080288310", "20090106070", "20090150224", "20090150919", "20090248513", "20090259518", "20090307073", "20100011059", "20100023408", "20100042421", "20100049584", "20100145791", "20100146531", "20100161492", "20100169175", "20100169792", "20100306043", "20110106611", "20110110515", "20110125573", "20110208586", "20110225038", "20110288907", "20120022937", "20120041792", "20120041817", "20120041969", "20120042253", "20120042262", "20120047022", "20120054020", "20120079519", "20120089455", "20120203594", "20120215903", "20120245978", "20120323674", "20130006706", "20130084882", "20130151332", "20130166648", "20130204710", "20130227394", "20130268351", "20130297543", "20130305271", "20140012659", "20140033317", "20140040171", "20140046777", "20140114745", "20140120864", "20140143018", "20140173641", "20140173643", "20140189541", "20140236715", "20140278921", "20140278959", "20150019639", "20150025948", "20150046256", "20150149267", "20150161670", "20150235261"], "related": []}, {"id": "20160050167", "patent_code": "10373192", "patent_name": "Matching conversions from applications to selected content items", "year": "2019", "inventor_and_country_data": " Inventors: \nRamachandran; Vinod Kumar (Sunnyvale, CA)  ", "description": "<BR><BR>BACKGROUND\nIn a networked environment, such as the Internet or other networks, first-party content providers can provide information for public presentation on resources, for instance webpages, documents, applications, and/or other resources.  The\nfirst-party content can include text, video, and/or audio information provided by the first-party content providers via, for instance, a resource server for presentation on a client device over the Internet.  The first-party content may be a webpage\nrequested by the client device or a stand-alone application (e.g., a video game, a chat program, etc.) running on the client device.  Additional third-party content can also be provided by third-party content providers for presentation on the client\ndevice together with the first-party content provided by the first-party content providers.  For instance, the third-party content may be a public service announcement or advertisement that appears in conjunction with a requested resource, such as a\nwebpage (e.g., a search result webpage from a search engine, a webpage that includes an online article, a webpage of a social networking service, etc.) or with an application (e.g., an advertisement within a game).  Thus, a person viewing a resource can\naccess the first-party content that is the subject of the resource as well as the third-party content that may or may not be related to the subject matter of the resource.\n<BR><BR>SUMMARY\nImplementations described herein relate to matching conversions performed in an application of a third-party to interactions with content items presented with a resource via a web browser of a client device.  For instance, a content item, such\nas an advertisement, for a company may be presented with a resource, such as a webpage, via a web browser of a client device.  Responsive to an interaction with the content item (e.g., clicking on the content item), first data may be transmitted to a\nsystem for matching conversions performed in applications to interactions with content items presented with resources.  The first data may include an account identifier and an application identifier for an application associated with the third-party of\nthe content item.  For instance, the company of the advertisement may also have a mobile application that may be installed and executed on the client device for interacting with the user of client device and/or completing transactions with the\nthird-party.\nIn some instances, a user of the client device may not perform an action that constitutes a conversion using the webpage presented via the web browser of the client device responsive to the interaction with the content item.  Instead, the user\nmay choose to perform the action constituting a conversion using the application of the third-party, such as completing a purchase using a company's application, booking travel arrangements using the company's application, etc. When the action\nconstituting a conversion is performed using the application of the third-party, second data is transmitted to the system for matching conversions performed in applications to interactions with content items presented with resources.  The second data may\ninclude the application identifier for the application associated with the third-party of the content item and a device identifier for the client device.\nThird data of the account identifier and the device identifier may also be transmitted responsive to or substantially concurrent with the conversion action.  In other instances, the account identifier and the device identifier may be transmitted\nas third data at a later point in time relative to the transmission of the second data.  The third data may be transmitted when a user of the client device access or logins into another application associated with the account identifier.  The system for\nmatching conversions performed in applications to interactions with content items presented with resources may match the second data with the first data based on the third data.  Accordingly, conversions that result from usage of an application of the\nthird-party can be matched with interactions with content items presented with resources displayed on a display of the client device, such as advertisements presented with webpages via a web browser of the client device.\nOne implementation relates to a method that includes receiving first data from a client device associated with an interaction with a content item displayed with a resource via a web browser of the client device.  The first data includes an\napplication identifier for an application associated with the content item and an account identifier.  The method also includes receiving second data from the client device associated with a conversion using the application.  The second data includes the\napplication identifier and a device identifier for the client device.  The method further includes receiving third data from the client device that includes the account identifier and the device identifier.  The method still further includes determining\nthe conversion is associated with the interaction with the content item displayed with the resource based on matching the second data with the first data based on the third data.\nAnother implementation relates to a system having one or more processors and one or more storage devices storing instructions that, when executed by the one or more processors, cause the one or more processors to perform several operations.  The\noperations include receiving first data from a client device responsive to an interaction with a content item displayed with a resource displayed on a display of the client device.  The first data includes an application identifier for an application\nassociated with the content item, an account identifier, and a first timestamp.  The operations also include receiving second data from the client device associated with a conversion using the application.  The second data includes the application\nidentifier, a device identifier for the client device, and a second timestamp.  The operations further include receiving third data from the client device including the account identifier and the device identifier.  The operations still further include\ndetermining the conversion is associated with the interaction with the content item displayed with the resource based on matching the second data with the first data based on the third data and a difference between the first timestamp and the second\ntimestamp occurring within a predetermined period of time.\nYet a further implementation relates to a non-transitory computer readable storage device storing instructions that, when executed by one or more processors, cause the one or more processors to perform several operations.  The operations include\nreceiving first data from a mobile device associated with an interaction with a content item displayed with a resource displayed on a display of the mobile device.  The first data includes an application identifier for a mobile application associated\nwith the content item and an account identifier.  The operations also include receiving second data from the mobile device associated with a conversion using the mobile application.  The second data includes the application identifier and a device\nidentifier for the mobile device.  The operations further include receiving third data from the mobile device including the account identifier and the device identifier.  The operations still further include determining the conversion is associated with\nthe interaction with the content item displayed with the resource based on matching the second data with the first data based on the third data.  The operations also include storing data for the determined conversion associated with the interaction with\nthe content item in a database. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe details of one or more implementations are set forth in the accompanying drawings and the description below.  Other features, aspects, and advantages of the disclosure will become apparent from the description, the drawings, and the claims,\nin which:\nFIG. 1 is an overview depicting an implementation of a system for providing information via a computer network;\nFIG. 2A is an illustration of an implementation of a first-party resource displayed on a display of a mobile client device and having third-party content;\nFIG. 2B is an illustration of the mobile client device of FIG. 2A displaying an application of a third-party on the display of the mobile client device;\nFIG. 3 is a block diagram of an implementation of a system for matching conversions from applications with interactions with content items presented with resources;\nFIG. 4 is a flow diagram of an implementation of a process for matching conversions from applications with interactions with content items presented with resources;\nFIG. 5 is a flow diagram of another implementation of a process for matching conversions from applications with interactions with content items presented with resources and calculating an estimated total conversion amount;\nFIG. 6 is a flow diagram of still another implementation of a process for matching conversions from applications with interactions with content items presented with resources and aggregating monetary values for conversions for the content item;\nand\nFIG. 7 is a block diagram depicting a general architecture for a computer system that may be employed to implement various elements of the systems and methods described and illustrated herein.\nIt will be recognized that some or all of the figures are schematic representations for purposes of illustration.  The figures are provided for the purpose of illustrating one or more embodiments with the explicit understanding that they will\nnot be used to limit the scope or the meaning of the claims.\n<BR><BR>DETAILED DESCRIPTION\nFollowing below are more detailed descriptions of various concepts related to and implementations of, methods, apparatuses, and systems for matching conversions from applications on a client device to interactions with content items presented\nwith resources displayed on a display of the client device.  The various concepts introduced above and discussed in greater detail below may be implemented in any of numerous ways as the described concepts are not limited to any particular manner of\nimplementation.  Examples of specific implementations and applications are provided primarily for illustrative purposes.\nI. Overview\nA computing device (e.g., a client device) can view a resource, such as a webpage, a document, an application, etc. In some implementations, the computing device may access the resource via the Internet by communicating with a server, such as a\nwebpage server, corresponding to that resource.  The resource includes first-party content that is the subject of the resource from a first-party content provider and may also include additional third-party provided content, such as advertisements or\nother content.  In one implementation, responsive to receiving a request to access a webpage, a webpage server and/or a client device can communicate with a data processing system, such as a content item selection system, to request a content item to be\npresented with the requested webpage, such as through the execution of code of the resource to request a third-party content item to be presented with the resource.  The content item selection system can select a third-party content item and provide data\nto display the content item with the requested webpage on a display of the client device.  In some instances, the content item is selected and served with a resource associated with a search query response.  For instance, a search engine may return\nsearch results on a search results webpage and may include third-party content items related to the search query in one or more content item slots of the search results webpage.\nThe third-party content item may include a link that, when the third-party content item is interacted with, such as clicking on the content item, directs the computing device to retrieve and/or transmit data to the address indicated by the link. For instance, some content items may have a link to a webpage of the third-party of the third-party content item, a link to an application store or website to install an application of the third-party, and/or a link to the content item selection system\nto transmit data to the content item selection system, which then redirects the computing device to a subsequent destination, such as the webpage of the third-party of the third-party content item.\nThe computing device (e.g., a client device) may also be used to view or execute an application, such as a mobile application.  The application may be an application for the third-party associated with the third-party content item.  For\ninstance, a travel company may have content items, such as advertisements, that may be presented with resources and may also have an application that can be installed and executed on the client device.  Thus, users of the computing device may interact\nwith the travel company via a landing webpage of the travel company via interaction with the content item of the travel company and/or via the application of the travel company.\nIn some instances, a user may initially interact with the content item, but does not perform an action that constitutes a conversion, such as completing a transaction, registering for a service, etc. Instead, the user may elect to utilize the\napplication of the third-party to perform the action that constitutes a conversion.  For instance, the third-party may not have a website that is optimized for mobile devices or completing transactions, so a user may prefer the mobile application to\nperform the action that constitutes a conversion.  In other instances, the retrieved webpage may include code to detect whether the device has the application of the third-party installed and/or is capable of installing and executing the application on\nthe device.  Thus, prior to utilizing the webpage, a user of the device may elect to install and/or execute the application on the client device and may then perform the action that constitutes a conversion via the application.  In still other\nimplementations, a user of the device may initially browse the retrieved webpage and then elect to complete a transaction via the application.\nIn some instances, such as when content items are presented via a web browser, the method for linking a subsequent conversion involves setting a cookie or other identifier stored in a data structure in a storage device of the device when the\ncontent item is presented and/or interacted with such that, when a conversion event occurs via the web browser, the cookie or other identifier may be identified and used to link the conversion event to the presentation and/or interaction with the content\nitem.  When a conversion event occurs via an application, another device identifier may be used to link the conversion to the client device.  However, in some instances, the cookie or other identifier for content items presented via a web browser is\ndifferent from the device identifier used with conversion via the application.  For instance, some operating systems of devices prevent or restrict access between the cookies or other identifier of a web browser and the device identifiers of\napplications.  Accordingly, it may be useful to link the interaction with a content item presented with a resource on a device via a web browser to the conversion occurring via the application executing on the device.\nIn some instances, a device may be associated with a device identifier.  In situations in which the systems discussed here collect personal information about users, or may make use of personal information, the users may be provided with an\nopportunity to control whether programs or features collect user information (e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current location), or to control whether and/or how\nto receive content from the content server that may be more relevant to the user.  In addition, certain data may be treated in one or more ways before it is stored or used, so that personally identifiable information is removed.  For instance, a user's\nidentity may be treated so that no personally identifiable information can be determined for the user, or a user's geographic location may be generalized where location information is obtained (such as to a city, ZIP code, or state level), so that a\nparticular location of a user cannot be determined.  Thus, the user may have control over how information is collected about the user and used by a content server.\nIn some implementations, the device identifier may be a universally unique device identifier (UDID), such as an identifier for advertisers (IDFA) or an AdID.  In other implementations, the device identifier may be an identifier associated with\nhardware and/or software characteristics of the device, such as a mobile equipment identifier (MEID), international mobile equipment identifier (IMEI), integrated circuit card identifier (ICCID), device serial number, an identifier generated based on\nhardware and/or software characteristics of the device (e.g., an identifier based, at least in part, on an operating system version, device model, firmware version(s), installed application(s), memory capacity, and/or storage space), etc. Such a device\nidentifier may be used as the cookie or other identifier when interacting with content items via a web browser of the device.\nTo link the interaction with the content item via the web browser to a conversion action performed using an application associated with a third-party of the content item, an application identifier for the application may be utilized to link the\ndevice identifier to an account identifier.  The account identifier may be a login name, an obfuscated account identifier, a pseudorandom account identifier, an account number, etc. For instance, when a content item is interacted with by a user of the\ndevice, first data including the account identifier and the application identifier for the third-party's application may be transmitted to a system for matching conversions using applications to interactions with content items presented with resources. \nIn some implementations, the account identifier is transmitted with the application identifier for devices in which the user is logged into the account associated with the account identifier via the web browser at the time of the interaction with the\ncontent item.  In some implementations, the first data may further include a first timestamp indicative of a time when the interaction with the content item occurred.  The first data may be stored in a database, such as a content item interaction\ndatabase.\nWhen an action constituting a conversion using the application occurs, such as completing a transaction, registering for a service, etc., the application may be configured to transmit second data to the system for matching conversions using\napplications to interactions with content items presented with resources.  For instance, a portion of code may be provided to be included with the source code for the application that is configured to transmit the second data to the system for matching\nconversions using applications to interactions with content items presented with resources responsive to an action constituting a conversion occurring for the application.  The second data may include the application identifier and a device identifier. \nIn some implementations, the second data may further include a second timestamp indicative of a time when the conversion occurred.  In some further implementations, the second data may include a monetary value and/or other metadata (e.g., a product SKU,\na number of products purchased, etc.).  The second data may be stored in a database, such as an application conversion database.\nSubstantially concurrent with or subsequent to the conversion via the application, third data including the account identifier and the device identifier may be transmitted to the system for matching conversions using applications to interactions\nwith content items presented with resources.  In some implementations, the account identifier and the device identifier may be transmitted substantially concurrent with the second data if the device is currently logged into the account associated with\nthe account identifier via another application of the device.  In other instances, the account identifier and the device identifier may be transmitted subsequent to the second data when the device next logs into the account associated with the account\nidentifier or accesses an application associated with the account from the device.\nThe system for matching conversions using applications to interactions with content items presented with resources may utilize the third data to match the second data for the application conversion with the first data for the content item\ninteraction.  That is, the system for matching conversions using applications to interactions with content items presented with resources may match the account identifier of the third data with the account identifier of the first data and the device\nidentifier of the third data with the device identifier of the second data.  Information for the matched conversion with the interaction with the presented content item may be stored in a database, such as a matched conversion database.  In some\nimplementations, the information may simply include data indicative of the third-party, the content item, and that a conversion occurred.  In some implementations, the information may further include a monetary value for the conversion and/or the other\nmetadata (e.g., a product SKU, a number of products purchased, etc.).  In some implementations, the first, second, and third data may be deleted once the information for the matched conversion with the interaction with the presented content item is\nstored in the database.  Thus, any information about the client device may be deleted and only information pertinent to the third-party, the content item, and the conversion may be stored.\nIn some instances, a number of application conversions may be calculated for a third-party by aggregating the matched conversions with the interactions with presented third-party content items.  In some implementations, an estimated total\nconversion amount may be calculated based, at least in part, on the number of application conversions and a login rate.  The login rate may be based on a login in rate of devices when the content item is interacted with (i.e., the number of devices\nlogged into an account when a content item is interacted with via the web browser divided by the total number of content item interactions) and a percentage of devices with applications associated with the account (i.e., a percentage of devices for a\nparticular operating system that have one or more applications associated with the account).  Thus, the estimated total conversion amount (ETC) may be defined as:\n.times..times..times..times..times..times.  ##EQU00001##\nwhere Login Rate=(% Devices.sub.Interaction)(% Devices.sub.Application) where % Devices.sub.Interaction is the fractional amount of devices that are logged into an account for the account identifier via the web browser at the time of the\ninteraction with a content item (e.g., 0.3 for 30% of devices are logged to an account for the account identifier) and % Devices.sub.Application is the fractional amount of devices that are known or estimated to have applications installed that are\nassociated with an account for the account identifier.  In some implementations, the estimated total conversion amount may be separated by operating system.\nIn some implementations, an aggregate or an average value may be calculated based on the matched conversions to interactions with content items.  For instance, the monetary values of conversions may be aggregated and an aggregate value for all\nthe application conversions may be determined.  In some instances, an estimated total conversion value may be determined using the aggregated application conversion values and the login rate.  In some implementations, the estimated total conversion value\nmay be separated by operating system.  In still further implementations, an average application conversion value may be calculated by taking an average of the monetary values for all the application conversions.\nWhile the foregoing has provided an overview of matching conversions to presentation of content items with resources, more specific implementations and systems to implement such a system will now be described.\nII.  Overview of an Implementation of a System for Providing Information Via a Computer Network\nFIG. 1 is a block diagram of an implementation of a system 100 for providing information via at least one computer network such as the network 106.  The network 106 may include a local area network (LAN), wide area network (WAN), a telephone\nnetwork, such as the Public Switched Telephone Network (PSTN), a wireless link, an intranet, the Internet, or combinations thereof.  The system 100 can also include at least one data processing system or processing module, such as a content item\nselection system 108.  The content item selection system 108 can include at least one logic device, such as a computing device having a data processor, to communicate via the network 106, for instance with a resource server 104, a client device 110,\nand/or a third-party content server 102.  The content item selection system 108 can include one or more data processors, such as a content placement processor, configured to execute instructions stored in a memory device to perform one or more operations\ndescribed herein.  In other words, the one or more data processors and the memory device of the content item selection system 108 may form a processing module.  The processor may include a microprocessor, an application-specific integrated circuit\n(ASIC), a field-programmable gate array (FPGA), etc., or combinations thereof.  The memory may include, but is not limited to, electronic, optical, magnetic, or any other storage or transmission device capable of providing processor with program\ninstructions.  The memory may include a floppy disk, compact disc read-only memory (CD-ROM), digital versatile disc (DVD), magnetic disk, memory chip, read-only memory (ROM), random-access memory (RAM), Electrically Erasable Programmable Read-Only Memory\n(EEPROM), erasable programmable read only memory (EPROM), flash memory, optical media, or any other suitable memory from which processor can read instructions.  The instructions may include code from any suitable computer programming language such as,\nbut not limited to, ActionScript.RTM., C, C++, C#, Java.RTM., ActionScript.RTM., JavaScript.RTM., JSON, Perl.RTM., HTML, HTML5, XML, Python.RTM., and Visual Basic.RTM..  The processing module may process instructions and provide data to display one or\nmore content items to the resource server 104 and/or the client device 110.  In addition to the processing circuit, the content item selection system 108 may include one or more databases configured to store data.  The content item selection system 108\nmay also include an interface configured to receive data via the network 106 and to provide data from the content item selection system 108 to any of the other devices on the network 106.  The content item selection system 108 can include a server, such\nas an advertisement server or otherwise.\nThe client device 110 can include one or more devices such as a computer, laptop, desktop, smart phone, tablet, personal digital assistant, set-top box for a television set, a smart television, or server device configured to communicate with\nother devices via the network 106.  The device may be any form of portable electronic device that includes a data processor and a memory, i.e., a processing module.  The memory may store machine instructions that, when executed by a processor, cause the\nprocessor to perform one or more of the operations described herein.  The memory may also store data to display one or more resources, content items, etc. on the computing device.  The processor may include a microprocessor, an application-specific\nintegrated circuit (ASIC), a field-programmable gate array (FPGA), etc., or combinations thereof.  The memory may include, but is not limited to, electronic, optical, magnetic, or any other storage or transmission device capable of providing processor\nwith program instructions.  The memory may include a floppy disk, compact disc read-only memory (CD-ROM), digital versatile disc (DVD), magnetic disk, memory chip, read-only memory (ROM), random-access memory (RAM), Electrically Erasable Programmable\nRead-Only Memory (EEPROM), erasable programmable read only memory (EPROM), flash memory, optical media, or any other suitable memory from which processor can read instructions.  The instructions may include code from any suitable computer programming\nlanguage.\nThe client device 110 can execute a software application (e.g., a web browser or other application) to retrieve content from other computing devices over network 106.  Such an application may be configured to retrieve first-party content from a\nresource server 104.  In some cases, an application running on the client device 110 may itself be first-party content (e.g., a game, a media player, a business application, etc.).  In one implementation, the client device 110 may execute a web browser\napplication which provides a browser window on a display of the client device.  The web browser application that provides the browser window may operate by receiving input of a uniform resource locator (URL), such as a web address, from an input device\n(e.g., a pointing device, a keyboard, a touch screen, or another form of input device).  In response, one or more processors of the client device 110 executing the instructions from the web browser application may request data from another device\nconnected to the network 106 referred to by the URL address (e.g., a resource server 104).  The other device may then provide webpage data and/or other data to the client device 110, which causes visual indicia to be displayed by the display of the\nclient device 110.  Accordingly, the browser window displays the retrieved first-party content, such as webpages from various websites, to facilitate interaction with the first-party content.\nIn some implementations, the client device 110 may also be used to view or execute an application other than a web browser.  The application may be an application for a third-party associated with a third-party content item.  For instance, a\ntravel company may have content items, such as advertisements, that may be presented with resources and may also have an application that can be installed and/or executed on the client device 110.  Thus, users of the client device 110 may interact with\nthe travel company via a landing webpage of the travel company via the web browser and/or via the application of the travel company.\nThe resource server 104 can include a computing device, such as a server, configured to host a resource, such as a webpage or other resource (e.g., articles, comment threads, music, video, graphics, search results, information feeds, etc.).  The\nresource server 104 may be a computer server (e.g., a file transfer protocol (FTP) server, file sharing server, web server, etc.) or a combination of servers (e.g., a data center, a cloud computing platform, etc.).  The resource server 104 can provide\nresource data or other content (e.g., text documents, PDF files, and other forms of electronic documents) to the client device 110.  In one implementation, the client device 110 can access the resource server 104 via the network 106 to request data to\ndisplay a resource of the resource server 104.\nOne or more third-party content providers may have third-party content servers 102 to directly or indirectly provide data for third-party content items to the content item selection system 108 and/or to other computing devices via network 106. \nThe content items may be in any format that may be presented on a display of a client device 110, for instance, graphical, text, image, audio, video, etc. The content items may also be a combination (hybrid) of the formats.  The content items may be\nbanner content items, interstitial content items, pop-up content items, rich media content items, hybrid content items, etc. The content items may also include embedded information such as hyperlinks, metadata, links, machine-executable instructions,\nannotations, etc. In some instances, the third-party content servers 102 may be integrated into the content item selection system 108 and/or the data for the third-party content items may be stored in a database of the content item selection system 108.\nIn one implementation, the content item selection system 108 can receive, via the network 106, a request for a content item to present with a resource.  The received request may be received from a resource server 104, a client device 110, and/or\nany other computing device.  The resource server 104 may be owned or ran by a first-party content provider that may include instructions for the content item selection system 108 to provide third-party content items with one or more resources of the\nfirst-party content provider on the resource server 104.  In one implementation, the resource may include a webpage.  The client device 110 may be a computing device operated by a user, which, when accessing a resource of the resource server 104, can\nmake a request to the content item selection system 108 for content items to be presented with the resource, for instance.\nThe content item request can include requesting device information (e.g., a web browser type, an operating system type, one or more previous resource requests from the requesting device, one or more previous content items received by the\nrequesting device, a language setting for the requesting device, a geographical location of the requesting device, a time of a day at the requesting device, a day of a week at the requesting device, a day of a month at the requesting device, a day of a\nyear at the requesting device, etc.) and resource information (e.g., URL of the requested resource, one or more keywords of the content of the requested resource, text of the content of the resource, a title of the resource, a category of the resource, a\ntype of the resource, a property of the resource, an interactivity level of the resource, a ranking of the resource, a popularity of the resource, a category of a publisher associated with the resource, a type of a publisher associated with the resource,\na property of a publisher associated with the resource, etc.).  The information or parameters that the content item selection system 108 receives can include a HyperText Transfer Protocol (HTTP) cookie can contain a form of device identifier (e.g., a\nrandom number) that represents the client device 110.  In some implementations, the device and/or the resource information or parameters may be appended to a content item request URL (e.g., /page/contentitem?devid=abc123&devnfo=A34r0).  In some\nimplementations, the device and/or the resource information or parameters may be encoded prior to being appended to the content item request URL.  The requesting device and/or the resource information or parameters may be utilized by the content item\nselection system 108 to select third-party content items to be served with the requested resource and presented on a display of a client device 110.\nIn some instances, a resource of a resource server 104 may include a search engine feature.  The search engine feature may receive a search query (e.g., a string of text) via an input feature (an input text box, etc.).  The search engine may\nsearch an index of documents (e.g., other resources, such as webpages, etc.) for relevant search results based on the search query.  The search results may be transmitted as a second resource to present the relevant search results, such as a search\nresult webpage, on a display of a client device 110.  The search results may include webpage titles, hyperlinks, etc. One or more third-party content items may also be presented with the search results in a content item slot of the search result webpage. Accordingly, the resource server 104 and/or the client device 110 may request one or more content items from the content item selection system 108 to be presented in the content item slot of the search result webpage.  The content item request may\ninclude additional information, such as the client device information, the resource information, a quantity of content items, a format for the content items, the search query string, keywords of the search query string, information related to the query\n(e.g., geographic location information and/or temporal information), etc. In some implementations, a delineation may be made between the search results and the third-party content items to avert confusion.\nIn some implementations, the third-party content provider may manage the selection and serving of content items by content item selection system 108.  For instance, the third-party content provider may set bid values and/or selection criteria\nvia a user interface that may include one or more content item conditions or constraints regarding the serving of content items.  A third-party content provider may specify that a content item and/or a set of content items should be selected and served\nfor client devices 110 associated with a certain geographic location or region, a certain language, a certain operating system, a certain web browser, etc. In another implementation, the third-party content provider may specify that a content item or set\nof content items should be selected and served when the resource, such as a webpage, document, etc., contains content that matches or is related to certain keywords, phrases, etc. The third-party content provider may set a single bid value for several\ncontent items, set bid values for subsets of content items, and/or set bid values for each content item.  The third-party content provider may also set the types of bid values, such as bids based on whether a user clicks on the third-party content item,\nwhether a user performs a specific action based on the presentation of the third-party content item (i.e., a conversion), whether the third-party content item is selected and served, and/or other types of bids.\nIII.  Implementations of Interaction with Content Items and In-Application Conversions\nWhile the foregoing has provided an overview of a system 100 for selecting and serving content items to client devices 110, implementations of displaying selected and served content items on a client device and in-application conversions will\nnow be described in reference to FIGS. 2A-2B.\nFIG. 2A depicts a mobile client device 200, such as a smartphone or tablet, on which a resource 210 may be displayed by a display 202 of the client device 200.  In the implementation depicted in FIG. 2A, the resource 210 is a webpage displayed\nvia a mobile web browser 204 executing on the client device 200.  In some implementations, the client device 200 may execute code for the resource 210 to display first-party content 212 (e.g., a webpage or website) on the display 202 of the client device\n200 via the mobile web browser 204.  In some implementations, the resource 210 may also include code to request one or more third-party content items 214 to be presented with the first-party content 212.  In response, one or more processors of the client\ndevice 200 executing the instructions may request data from another device (e.g., a content item selection system 108) connected to a network, such as network 106.  The other device may then provide data to display the third-party content item 214 to the\nclient device 200, which causes visual indicia to be displayed by the display 202 of the client device 200.\nIn some instances, a user of the client device 200 may interact with the third-party content item 214.  The interaction with the third-party content item 214 may include a link associated with the third-party content item 214.  The link may be a\ndirect link to a third-party resource, such as a webpage associated with the third-party content item 214 and/or a main webpage for the third-party of the third-party content item 214.  Upon interaction with the third-party content item 214, the mobile\nweb browser 204 may retrieve data for the webpage of the link.  In some instances, the link associated with the third-party content item 214 may initially cause the web browser 204 to transmit data to the content selection system 108 responsive to an\ninteraction with the third-party content item 214.  For instance, the link may link to the content selection system 108 or a system associated with the content selection system 108 and may append or include data to the link.  Thus, when the mobile web\nbrowser 204 sends a request to the link to the content selection system 108 or a system associated with the content selection system 108, the content selection system 108 or a system associated with the content selection system 108 may store the data\nincluded with the requested link.  The content selection system 108 or a system associated with the content selection system 108 may then transmit data to the client device 200 to redirect the mobile web browser 204 to the webpage associated with the\nthird-party content item 214.  The mobile web browser 204 may then be redirected to retrieve data for the webpage after transmitting the data to the content selection system 108.\nThe transmitted data may include first data that includes an account identifier associated with the client device 200.  For instance, when the web browser is executing on the client device 200, a user of the client device 200 may be logged into\nan account for one or more services via the web browser (e.g., an e-mail service, a media service, etc.).  The account may be associated with an account identifier, such as a login name, an obfuscated account identifier, a pseudorandom account\nidentifier, an account number, etc. The first data may also include an application identifier associated with an application of the third-party of the third-party content item 214.  That is, in some instances, a third-party of the third-party content\nitem 214 may also have an application that may be installed and/or executed by the client device 200 (e.g., a mobile application).  The application identifier may be an identifier to identify the application in an application store or other application\nmanagement system.  In some implementations, the first data may further include a first timestamp with the account identifier and/or application identifier.  The first timestamp may be a coordinated universal time (UTC) timestamp.\nAlthough the interaction with the third-party content item 214 may direct the mobile web browser 214 to retrieve data for a webpage associated with the third-party content item 214, in some instances, a user of the client device 200 may elect to\nutilize the application associated with the application identifier instead of the retrieved webpage.  For instance, a third-party content item 214 for a travel website may direct a client device to the travel company's website.  However, the travel\ncompany may also provide an application that may be installed and/or executed on the client device 200 separately from the web browser.  FIG. 2B depicts the client device 200 of FIG. 2A executing an application 206 of the third-party.  In some instances,\nthe application 206 may be utilized instead of the webpage as a result of the interaction with the third-party content item 214 of FIG. 2A.  For instance, the retrieved webpage may not be optimized for viewing or completing transactions on a mobile\nclient device 200, so the application 206 may be preferred.  In other instances, the retrieved webpage may include code to detect whether the client device 200 has the application 206 installed and/or is capable of installing and executing the\napplication 206.  Thus, prior to utilizing the webpage, a user of the client device 200 may elect to install and/or execute the application 206 on the client device 200.  In still other implementations, a user of the client device 200 may initially\nbrowse the retrieved webpage and then elect to complete a transaction via the application 206.\nWhen an action that constitutes a conversion occurs via the application 206, the application may include code configured to cause the client device 200 to transmit second data to the content selection system 108 or a system associated with the\ncontent selection system 108.  For instance, when a conversion, such as a purchase transaction, a booking transaction, a service sign-up, etc., occurs via the application 206 executing on the client device 200 may transmit the second data to the content\nselection system 108 or a system associated with the content selection system 108.  The second data may include the application identifier for the application 206 and a device identifier.  In some implementations, the second data may further include a\nsecond timestamp indicative of a time when the conversion occurred.  The second timestamp may also be a coordinated universal time (UTC) timestamp.  In some further implementations, the second data may include a monetary value and/or other metadata\n(e.g., a product SKU, a number of products purchased, etc.).\nIn some instances, the client device 200 may also transmit third data to the content selection system 108 or a system associated with the content selection system 108 substantially concurrent with or subsequent to the conversion via the\napplication 206.  The third data may include the account identifier and the device identifier.  In some implementations, the account identifier and device identifier may be substantially concurrently transmitted with the second data if the client device\n200 is currently logged into the account associated with the account identifier via another application executing on the client device 200 (e.g., an e-mail application, a search application, a media application, etc.).  In other instances, the account\nidentifier and the device identifier may be transmitted subsequent to the second data when the client device 200 next logs into the account associated with the account identifier or executes the other application associated with the account from the\nclient device 200.\nIV.  Overview of an Implementation of a System for Matching Conversions from Applications to Interactions with Content Items\nFIG. 3 is a block diagram of an implementation of a portion of the content item selection system 108 of FIG. 1 that includes a content item selection module 120 and an application conversion matching module 130.  The content item selection\nsystem 108 also includes one or more databases, such as a content item interaction database 140, an application conversion database 150, and/or a matched conversion database 160.\nThe databases 140, 150, 160 may store data for and/or provide data to the application conversion matching module 130.  The databases 140, 150, 160 may include a static storage device, such as ROM, solid state drive (SSD), flash memory (e.g.,\nEEPROM, EPROM, etc.), magnetic disc, optical disc, etc., a plurality of static storage devices, a cloud storage system, a server, and/or any other electronic device capable of storing and providing data.  While the implementation shown in FIG. 3 depicts\nthe databases 140, 150, 160 as separate databases, it should be understood that the databases 140, 150, 160 may be combined into a single database or sets of databases.\nThe content item selection module 120 is configured to receive a content item request 302 via the network 106.  A client device, such as client device 110 of FIG. 1 or client device 200 of FIGS. 2A-2B, or a resource server, such as resource\nserver 104, may send the content item request 302 to the content item selection system 108 via the network 106.  The content item request 302 may include one or more parameters representative of characteristics of the client device (e.g., a unique\nidentifier associated with the client device, a type of client device, a display type of a client device, dimensions of the display, etc.) and/or characteristics of a resource with which the content item is to be presented (e.g., a URL of the resource,\none or more keywords of the content of the resource, text of the content of the resource, a title of the resource, a category of the resource, a type of the resource, a property of the resource, an interactivity level of the resource, a ranking of the\nresource, a popularity of the resource, a category of a publisher associated with the resource, a type of a publisher associated with the resource, a property of a publisher associated with the resource, etc.).  In some implementations, the foregoing\nparameters may be appended to or included in a content item request URL (e.g., /page/contentitem?devid=abc123&devnfo=A34r0).\nResponsive to the content item request 302, the content item selection module 120 is configured to select and serve a content item 304.  In some implementations, the content item selection module 120 is configured to perform an auction.  That\nis, the content item selection module 120 may generate one or more values, such as scores, for one or more content items based, at least in part, on the content item request 302, and select one or more content items to be served.  In some instances, the\ncontent item selection module 120 ranks the values (e.g., highest to lowest) and selects the content item associated with a value based on the ranking (e.g., selecting the content item associated with the highest ranked value or score).\nData to display the selected content item 304 may be transmitted or served by the content item selection module 120 to the client device and/or the resource server via the network 106.  The data can include graphical data, textual data, image\ndata, audio data, video data, etc. that may be accessed from a database.\nAs discussed above in reference to FIG. 2A, a user of the client device may interact with the content item.  The interaction with the content item may include a link associated with the third-party content item that initially causes a web\nbrowser of the client device to transmit data to the content selection system 108 responsive to an interaction with the content item.  Thus, when a user of the client devices interacts with the content item, such as clicking on the content item, the web\nbrowser of the client device may send a request to the content item selection system 108 based on the link associated with the third-party content item.  The link may include first data that can be appended to the link as parameters (e.g.,\n/page/contentitem?firstdata=abc123).  Thus, the content selection system 108 can receive outputted first data 306 from the client device responsive to the interaction with the content item.  The content selection system 108 can store the outputted first\ndata 306 in a content item interaction database 140.  In some implementations, the content selection system 108 can then transmit data to the client device to redirect the web browser of the client device to the webpage associated with the content item. \nThe web browser of the client device may then be redirected to retrieve data for the webpage after transmitting the first data to the content selection system 108.\nThe outputted first data 306 includes an identifier associated with the client device, such as an account identifier.  For instance, when the web browser is executing on the client device the user of the client device may be logged into an\naccount for one or more services via the web browser (e.g., an e-mail service, a media service, etc.).  Thus, when the user interacts with the content item, an account identifier for the account may be included in the first data transmitted to the\ncontent item selection system 108 The account identifier may be a login name, an obfuscated account identifier, a pseudorandom account identifier, an account number, etc. The first data may also include an application identifier associated with an\napplication of the third-party of the content item.  That is, in some instances, a third-party of the content item may also have an application that may be installed and/or executed by the client device (e.g., a mobile application).  The application\nidentifier may be an identifier to identify the application in an application store or other application management system.  In some implementations, the first data may further include a first timestamp with the account identifier and/or application\nidentifier.  The first timestamp may be a coordinated universal time (UTC) timestamp.\nThe content item selection system 108 may also receive outputted second data 308 via network 106 from a client device.  When an action that constitutes a conversion occurs via the application of the third-party of the content item, the\napplication may include code configured to cause the client device to transmit second data to the content selection system 108.  A conversion can be any defined action, such as a purchase transaction, a booking transaction, a service sign-up, etc. When\nthe conversion occurs via the application executing on the client device, then the client device may transmit the second data to the content selection system 108.  The second data may include the application identifier for the application and a device\nidentifier for the client device that is different from the account identifier.  In some implementations, the second data may further include a second timestamp indicative of a time when the conversion occurred.  The second timestamp may be a coordinated\nuniversal time (UTC) timestamp.  In some further implementations, the second data may include a monetary value and/or other metadata (e.g., a product SKU, a number of products purchased, etc.).  The content selection system 108 can store the outputted\nsecond data 308 in an application conversion database 150.\nThe content selection system 108 can also receive outputted third data 310 via the network 106 from the client device.  The outputted third data 310 may be received by the content item selection system 108 substantially concurrent with or\nsubsequent to the conversion via the application executing on the client device.  The third data may include the account identifier and the device identifier.  In some implementations, the account identifier and device identifier may be substantially\nconcurrently transmitted with the second data if the client device is currently logged into the account associated with the account identifier via another application executing on the client device (e.g., an e-mail application, a search application, a\nmedia application, etc.).  In other instances, the account identifier and device identifier may be transmitted subsequent to the second data when the client device next logs into the account associated with the account identifier or executes the other\napplication associated with the account from the client device.  In some implementations, the outputted third data 310 may be stored in a database or may be utilized by the application conversion matching module 130 without storing the outputted third\ndata 310 in a database.\nResponsive to receiving the outputted third data 310, the application conversion matching module 130 is configured to query the application conversion database 150 and the content item interaction database 140.  The application conversion\nmatching module 130 may initially query the content item interaction database 140 using the account identifier of the outputted third data 310 to retrieve stored first data in the content item interaction database 140 that includes that same account\nidentifier.  In some implementations, the stored first data in the content item interaction database 140 may be stored in as data in a data table.  The data for any of the matching entries of first data, such as the account identifier, the application\nidentifier, and/or the timestamp, stored in the content item interaction database 140 may be returned to the application conversion matching module 130 responsive to the query.\nThe application conversion matching module 130 may then use the application identifier of each matching entry of first data to query the application conversion database 150.  For instance, the application conversion matching module 130 queries\nthe application conversion database 150 using the device identifier of the outputted third data 310 and the application identifier of each matching entry of first data to retrieve any stored second data in the application conversion database 150 that\nincludes the same device identifier and the same application identifier.  In some implementations, the stored second data in the application conversion database 150 may be stored in as data in a data table.  The data for any of the matching entries of\nsecond data, such as the device identifier, the application identifier, the timestamp, the monetary value, and/or other metadata, stored in the application conversion database 150 may be returned to the application conversion matching module 130\nresponsive to the query.\nIn some implementations, the application conversion matching module 130 may compare the timestamp from the first data (i.e., the first timestamp) to the timestamp from the second data (i.e., the second timestamp) to determine a difference\nbetween the timestamps.  In some instances, the difference between the timestamps may be compared to a predetermined period of time, such as one minute, five minutes, ten minutes, thirty minutes, an hour, two hours, three hours, four hours, five hours,\nsix hours, twelve hours, one day, two days, three days, one week, etc. Thus, the application conversion matching module 130 may determine whether the application conversion of the second data occurred within the predetermined period of time relative to\nthe interaction with the content item of the first data.  If the application conversion does not occur within the predetermined period of time relative to the interaction with the content item, then the application conversion matching module 130 can\nfilter the application conversion out as occurring independent of or not influenced by the interaction with the content item.\nIn some implementations, the application conversion matching module 130 may generate or store data indicative of the matched application conversion associated with the interaction with a content item.  For instance, the application conversion\nmatching module 130 may store the application identifier, the device identifier, the timestamps, the monetary value, and/or the other metadata in the matched conversion database 160.  The application conversion matching module 130 may aggregate the\nmatched application conversions for the third-party in the matched conversion database 160 or may store the matched conversion separately in the matched conversion database 160.\nIn some implementations, the matched conversion data stored in the matched conversion database 160 can be used to generate data for a report, such as calculating a number of application conversions, calculating an estimated total conversion\namount, calculating an aggregate monetary value for the application conversions, calculating an average monetary value for the application conversions, calculating an average time to conversion using the timestamps for the application conversions, etc.\nV. Implementations of Processes for Matching In-Application Conversions with Interactions with Content Items\nFIGS. 4-6 depict processes 400, 500, 600 that may be implemented by the content item selection system 108 and/or the application conversion matching module 130.  FIG. 4 depicts an implementation of a process 400 that may be used by the\napplication conversion matching module 130 for matching conversions from applications on a client device to interactions with content items presented with resources displayed on a display of the client device.  In brief overview, as shown in FIG. 4, such\na process 400 includes receiving first data from a client device associated with an interaction with a content item (block 402), receiving second data from the client device associated with a conversion using an application (block 404), receiving third\ndata from the client device (block 406), determining the conversion is associated with the interaction with the content item (block 408), and storing data for the determined conversion associated with the interaction with the content item in a database\n(block 410).\nThe process 400 includes receiving first data from a client device associated with an interaction with a content item (block 402).  When a user of the client devices interacts with a content item, such as clicking on the content item, the web\nbrowser of the client device may send a request to the content item selection system based on the link associated with the content item.  The link may include the first data that can be appended to the link as parameters (e.g.,\n/page/contentitem?firstdata=abc123).  Thus, the content selection system can receive the first data from the client device responsive to the interaction with the content item.  The content selection system can store the first data in a content item\ninteraction database.  The first data includes an identifier associated with the client device, such as an account identifier.  For instance, when the web browser is executing on the client device the user of the client device may be logged into an\naccount for one or more services via the web browser (e.g., an e-mail service, a media service, etc.).  Thus, when the user interacts with the content item, an account identifier for the account may be included in the first data transmitted to the\ncontent item selection system.  The account identifier may be a login name, an obfuscated account identifier, a pseudorandom account identifier, an account number, etc. The first data may also include an application identifier associated with an\napplication of the third-party of the selected content item.  That is, in some instances, a third-party of the content item may also have an application that may be installed and/or executed by the client device (e.g., a mobile application).  The\napplication identifier may be an identifier to identify the application in an application store or other application management system.  In some implementations, the first data may further include a first timestamp with the account identifier and/or\napplication identifier.  The first timestamp may be a coordinated universal time (UTC) timestamp.\nThe process 400 also includes receiving second data from the client device associated with a conversion using an application (block 404).  For instance, the content item selection system may receive second data via a network from the client\ndevice.  When an action that constitutes a conversion occurs via the application of the third-party of the content item, the application may include code configured to cause the client device to transmit second data to the content selection system.  A\nconversion can be any defined action, such as a purchase transaction, a booking transaction, a service sign-up, etc. When the conversion occurs via the application executing on the client device, then the client device may transmit the second data to the\ncontent selection system.  The second data may include the application identifier for the application and a device identifier for the client device that is different from the account identifier.  In some implementations, the second data may further\ninclude a second timestamp indicative of a time when the conversion occurred.  The second timestamp may be a coordinated universal time (UTC) timestamp.  In some further implementations, the second data may include a monetary value and/or other metadata\n(e.g., a product SKU, a number of products purchased, etc.).  The content selection system can store the second data in an application conversion database.\nThe process 400 further includes receiving third data from the client device (block 406).  The content selection system can also receive outputted third data via the network from the client device.  The outputted third data may be received by\nthe content item selection system substantially concurrent with or subsequent to the conversion via the application.  The third data may include the account identifier and the device identifier.  In some implementations, the account identifier and device\nidentifier may be substantially concurrently transmitted with the second data if the client device is currently logged into the account associated with the account identifier via another application executing on the client device (e.g., an e-mail\napplication, a search application, a media application, etc.).  In other instances, the account identifier and the device identifier may be transmitted subsequent to the second data when the client device next logs into the account associated with the\naccount identifier or executes the other application associated with the account from the client device.  In some implementations, the third data may be stored in a database or may be utilized by the application conversion matching module without storing\nthe outputted third data in a database.\nThe process 400 still further includes determining the conversion is associated with the interaction with the content item (block 408).  For instance, the application conversion matching module of the content item selection system may be\nconfigured to query the application conversion database and the content item interaction database.  The application conversion matching module may query the content item interaction database using the account identifier of the third data to retrieve any\nstored first data in the content item interaction database that includes that same account identifier.  The data for any of the matching entries of first data, such as the account identifier, the application identifier, and/or the timestamp, stored in\nthe content item interaction database may be returned to the application conversion matching module responsive to the query.\nThe application conversion matching module may use the application identifier of each matching entry of first data and the device identifier of the third data to query the application conversion database.  For instance, the application\nconversion matching module queries the application conversion database using the device identifier of the third data and the application identifier of each matching entry of first data to retrieve any stored second data in the application conversion\ndatabase that includes the same device identifier and the same application identifier.  The data for any of the matching entries of second data, such as the device identifier, the application identifier, the timestamp, the monetary value, and/or other\nmetadata, stored in the application conversion database may be returned to the application conversion matching module responsive to the query.\nIn some implementations, the application conversion matching module may compare the timestamp from the first data (i.e., the first timestamp) to the timestamp from the second data (i.e., the second timestamp) to determine a difference between\nthe timestamps.  In some instances, the difference between the timestamps may be compared to a predetermined period of time, such as one minute, five minutes, ten minutes, thirty minutes, an hour, two hours, three hours, four hours, five hours, six\nhours, twelve hours, one day, two days, three days, one week, etc. Thus, the application conversion matching module may determine whether the application conversion of the second data occurred within the predetermined period of time relative to the\ninteraction with the content item of the first data.  If the application conversion does not occur within the predetermined period of time relative to the interaction with the content item, then the application conversion matching module can filter the\napplication conversion out as occurring independent of or not influenced by the interaction with the content item.\nIn some implementations, process 400 may also include storing data for the determined conversion associated with the interaction with the content item in a database (block 410).  For instance, the application conversion matching module may store\ndata indicative of the matched application conversion associated with the interaction with a content item.  The application conversion matching module may also store the application identifier, the device identifier, the timestamps, the monetary value,\nand/or the other metadata in a matched conversion database.  The application conversion matching module may aggregate the matched application conversions for the third-party in the matched conversion database or may store the matched conversion\nseparately in the matched conversion database.\nFIG. 5 depicts an implementation of a process 500 that may be used by the application conversion matching module 130 for matching conversions from applications on a client device to interactions with content items presented with resources\ndisplayed on a display of the client device and to calculate an estimated total conversion amount.  In brief overview, as shown in FIG. 5, such a process 500 includes receiving first data from a client device associated with an interaction with a content\nitem (block 502), receiving second data from the client device associated with a conversion using an application (block 504), receiving third data from the client device (block 506), determining the conversion is associated with the interaction with the\ncontent item (block 508), calculating a number of application conversions (block 510), and calculating an estimated total conversion amount (block 512).\nThe receiving of first data from a client device associated with an interaction with a content item (block 502), receiving of second data from the client device associated with a conversion using an application (block 504), receiving of third\ndata from the client device (block 506), and determining the conversion is associated with the interaction with the content item (block 508) may be performed substantially in accordance with the receiving of first data from a client device associated\nwith an interaction with a content item (block 402), receiving of second data from the client device associated with a conversion using an application (block 404), receiving of third data from the client device (block 406), and determining the conversion\nis associated with the interaction with the content item (block 408) of the process 400 of FIG. 4.\nThe process 500 further includes calculating a number of application conversions (block 510).  The calculation of the number of application conversions may include aggregating the matched application conversions for the third-party in the\nmatched conversion database to calculate the number of application conversions for the third-party.\nThe process 500 further includes calculating an estimated total conversion amount (block 512).  The estimated total conversion amount may be calculated based, at least in part, on the number of application conversions and a login rate.  The\nlogin rate may be based on a login in rate of devices when the content item is selected (i.e., the number of devices logged into an account when a content item is interacted with via the web browser divided by the total number of content item\ninteractions) and a percentage of devices with applications associated with the account (i.e., a percentage of devices for a particular operating system that have one or more applications associated with the account).  Thus, the estimated total\nconversion amount (ETC) may be defined as:\n.times..times..times..times..times..times.  ##EQU00002##\nwhere Login Rate=(% Devices.sub.Interaction)(% Devices.sub.Application) where % Devices.sub.Interaction is the fractional amount of devices that are logged into an account for the account identifier via the web browser at the time of the\ninteraction with a content item (e.g., 0.3 for 30% of devices that are logged to an account for the account identifier) and % Devices.sub.Application is the fractional amount of devices that are known or estimated to have applications installed that are\nassociated with an account for the account identifier.  In some implementations, the estimated total conversion amount may be separately calculated for differing operating systems, by differing geographic regions (e.g., city, state, country, continent,\netc.), by device type, etc.\nIn some implementations, the estimated total conversion amount may be used to calculate an aggregated application conversion value using an average of the monetary values for the matched conversions to interactions with content items.  The\naggregate application conversion value may be separately calculated for differing operating systems, by differing geographic regions (e.g., city, state, country, continent, etc.), by device type, etc.\nIn some implementations, the calculating of a number of application conversions (block 510) and calculating of an estimated total conversion amount (block 512) may be included as part of process 400 and/or process 600.\nFIG. 6 depicts an implementation of a process 600 that may be used by the application conversion matching module 130 for matching conversions from applications on a client device to interactions with content items presented with resources\ndisplayed on a display of the client device and to aggregate monetary values for conversions for the content item.  In brief overview, as shown in FIG. 6, such a process 600 includes receiving first data from a client device associated with an\ninteraction with a content item (block 602), receiving second data from the client device associated with a conversion using an application (block 604), receiving third data from the client device (block 606), determining the conversion is associated\nwith the interaction with the content item (block 608), and aggregate a monetary value of the conversion with other monetary values associated with other conversions for the content item (block 610).\nThe receiving of first data from a client device associated with an interaction with a content item (block 602), receiving of second data from the client device associated with a conversion using an application (block 604), receiving of third\ndata from the client device (block 606), and determining the conversion is associated with the interaction with the content item (block 608) may be performed substantially in accordance with the receiving of first data from a client device associated\nwith an interaction with a content item (block 402), receiving of second data from the client device associated with a conversion using an application (block 404), receiving of third data from the client device (block 406), and determining the conversion\nis associated with the interaction with the content item (block 408) of the process 400 of FIG. 4.\nThe process 600 further includes aggregating a monetary value of the conversion with other monetary values associated with other conversions for the content item (block 610).  That is, once the application conversion matching module matches the\napplication conversion as associated with an interaction with a content item, the application conversion matching module may aggregate a monetary value, such as that included in the second data, with other monetary values or an aggregate monetary value\nassociated with other conversions for the content item for the third-party, such as other monetary values or an aggregate monetary value stored in a matched conversion database or other monetary values of other databases.  Thus, the application\nconversion matching module may generate a total monetary value for the application conversions for the content item and/or a total monetary value for all conversions for the content item using the monetary value for the matched application conversion.\nIn some implementations, the aggregating of a monetary value of the conversion with other monetary values associated with other conversions for the content item (block 610) may be included as part of process 400 and/or process 500.\nIn some implementations, the second data may include the account identifier with the application identifier when an action constituting a conversion occurs.  Thus, the application conversion matching module may simply match the application\nidentifier and account identifier from the second data to the application identifier and account identifier of the first data without utilizing a device identifier that is different from the account identifier.\nVI.  Implementation of a Computing System\nFIG. 7 is a block diagram of a computing system 700 that can be used to implement the client device 110, content item selection system 108, third-party content server 102, resource server 104, client device 200, etc. The computing system 700\nincludes a bus 705 or other communication component for communicating information and a processor 710 coupled to the bus 705 for processing information.  The computing system 700 can also include one or more processors 710 coupled to the bus for\nprocessing information.  The computing system 700 also includes main memory 715, such as a RAM or other dynamic storage device, coupled to the bus 705 for storing information, and instructions to be executed by the processor 710.  Main memory 715 can\nalso be used for storing position information, temporary variables, or other intermediate information during execution of instructions by the processor 710.  The computing system 700 may further include a ROM 720 or other static storage device coupled to\nthe bus 705 for storing static information and instructions for the processor 710.  A storage device 725, such as a solid state device, magnetic disk or optical disk, is coupled to the bus 705 for persistently storing information and instructions. \nComputing device 700 may include, but is not limited to, digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, cellular telephones, smart phones, mobile computing devices (e.g., a\nnotepad, e-reader, etc.) etc.\nThe computing system 700 may be coupled via the bus 705 to a display 735, such as a Liquid Crystal Display (LCD), Thin-Film-Transistor LCD (TFT), an Organic Light Emitting Diode (OLED) display, LED display, Electronic Paper display, Plasma\nDisplay Panel (PDP), and/or other display, etc., for displaying information to a user.  An input device 730, such as a keyboard including alphanumeric and other keys, may be coupled to the bus 705 for communicating information and command selections to\nthe processor 710.  In another implementation, the input device 730 may be integrated with the display 735, such as in a touch screen display.  The input device 730 can include a cursor control, such as a mouse, a trackball, or cursor direction keys, for\ncommunicating direction information and command selections to the processor 710 and for controlling cursor movement on the display 735.\nAccording to various implementations, the processes and/or methods described herein can be implemented by the computing system 700 in response to the processor 710 executing an arrangement of instructions contained in main memory 715.  Such\ninstructions can be read into main memory 715 from another computer-readable medium, such as the storage device 725.  Execution of the arrangement of instructions contained in main memory 715 causes the computing system 700 to perform the illustrative\nprocesses and/or method steps described herein.  One or more processors in a multi-processing arrangement may also be employed to execute the instructions contained in main memory 715.  In alternative implementations, hard-wired circuitry may be used in\nplace of or in combination with software instructions to effect illustrative implementations.  Thus, implementations are not limited to any specific combination of hardware circuitry and software.\nThe computing system 700 also includes a communications module 740 that may be coupled to the bus 705 for providing a communication link between the system 700 and the network 106.  As such, the communications module 740 enables the processor\n710 to communicate, wired or wirelessly, with other electronic systems coupled to the network 106.  For instance, the communications module 740 may be coupled to an Ethernet line that connects the system 700 to the Internet or another network 106.  In\nother implementations, the communications module 740 may be coupled to an antenna (not shown) and provides functionality to transmit and receive information over a wireless communication interface with the network 106.\nIn various implementations, the communications module 740 may include one or more transceivers configured to perform data communications in accordance with one or more communications protocols such as, but not limited to, WLAN protocols (e.g.,\nIEEE 802.11 a/b/g/n/ac/ad, IEEE 802.16, IEEE 802.20, etc.), PAN protocols, Low-Rate Wireless PAN protocols (e.g., ZigBee, IEEE 802.15.4-2003), Infrared protocols, Bluetooth protocols, EMI protocols including passive or active RFID protocols, and/or the\nlike.\nThe communications module 740 may include one or more transceivers configured to communicate using different types of protocols, communication ranges, operating power requirements, RF sub-bands, information types (e.g., voice or data), use\nscenarios, applications, and/or the like.  In various implementations, the communications module 740 may comprise one or more transceivers configured to support communication with local devices using any number or combination of communication standards.\nIn various implementations, the communications module 740 can also exchange voice and data signals with devices using any number or combination of communication standards (e.g., GSM, CDMA, TDNM, WCDMA, OFDM, GPRS, EV-DO, WiFi, WiMAX, S02.xx,\nUWB, LTE, satellite, etc).  The techniques described herein can be used for various wireless communication networks 106 such as Code Division Multiple Access (CDMA) networks, Time Division Multiple Access (TDMA) networks, Frequency Division Multiple\nAccess (FDMA) networks, Orthogonal FDMA (OFDMA) networks, Single-Carrier FDMA (SC-FDMA) networks, etc. A CDMA network can implement a radio technology such as Universal Terrestrial Radio Access (UTRA), cdma2000, etc. UTRA includes Wideband-CDMA (W-CDMA)\nand Low Chip Rate (LCR).  CDMA2000 covers IS-2000, IS-95, and IS-856 standards.  A TDMA network can implement a radio technology such as Global System for Mobile Communications (GSM).  An OFDMA network can implement a radio technology such as Evolved\nUTRA (E-UTRA), IEEE 802.11, IEEE 802.16, IEEE 802.20, Flash-OFDM, etc. UTRA, E-UTRA, and GSM are part of Universal Mobile Telecommunication System (UMTS).  Long Term Evolution (LTE) is an upcoming release of UMTS that uses E-UTRA.  UTRA, E-UTRA, GSM,\nUMTS, and LTE are described in documents from an organization named \"3rd Generation Partnership Project\" (3GPP).  CDMA2000 is described in documents from an organization named \"3rd Generation Partnership Project 2\" (3GPP2).\nAlthough an implementation of a computing system 700 has been described in FIG. 7, implementations of the subject matter and the functional operations described in this specification can be implemented in other types of digital electronic\ncircuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.\nImplementations of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software embodied on a tangible medium, firmware, or hardware, including the structures\ndisclosed in this specification and their structural equivalents, or in combinations of one or more of them.  The subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer\nprogram instructions, encoded on one or more computer storage media for execution by, or to control the operation of, data processing apparatus.  Alternatively or in addition, the program instructions can be encoded on an artificially generated\npropagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.  A computer storage medium\ncan be, or be included in, a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them.  Moreover, while a computer storage medium is not a\npropagated signal, a computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal.  The computer storage medium can also be, or be included in, one or more separate\ncomponents or media (e.g., multiple CDs, disks, or other storage devices).  Accordingly, the computer storage medium is both tangible and non-transitory.\nThe operations described in this specification can be performed by a data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.\nThe terms \"data processing system,\" \"computing system,\" or \"system\" encompass all kinds of apparatus, devices, and machines for processing data, including without limitation a programmable processor, a computer, a system on a chip, or multiple\nones, a portion of a programmed processor, or combinations of the foregoing.  The apparatus can include special purpose logic circuitry, e.g., an FPGA or an ASIC.  The apparatus can also include, in addition to hardware, code that creates an execution\nenvironment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or\nmore of them.  The apparatus and execution environment can realize various different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.\nA computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be\ndeployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment.  A computer program may, but need not, correspond to a file in a file system.  A program\ncan be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one\nor more modules, sub programs, or portions of code).  A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication\nnetwork.\nProcessors suitable for the execution of a computer program include, without limitation, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.  Generally, a processor will receive\ninstructions and data from a read only memory or a random access memory or both.  The essential elements of a computer are a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and\ndata.  Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.  However, a computer\nneed not have such devices.  Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable\nstorage device (e.g., a universal serial bus (USB) flash drive), to name just a few.  Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including without limitation\nsemiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD disks.  The processor and the memory can be supplemented by, or\nincorporated in, special purpose logic circuitry.\nTo provide for interaction with a user, implementations of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD monitor, for displaying information to\nthe user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.  Other kinds of devices can be used to provide for interaction with a user as well; feedback provided to the user can be any\nform of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.\nWhile this specification contains many specific implementation details, these should not be construed as limitations on the scope of what may be claimed, but rather as descriptions of features specific to particular implementations.  Certain\nfeatures described in this specification in the context of separate implementations can also be implemented in combination in a single implementation.  Conversely, various features described in the context of a single implementation can also be\nimplemented in multiple implementations separately or in any suitable subcombination.  Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed\ncombination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.\nSimilarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations\nbe performed, to achieve desirable results.  In certain circumstances, multitasking and parallel processing may be advantageous.  Moreover, the separation of various system components in the implementations described above should not be understood as\nrequiring such separation in all implementations, and it should be understood that the described program components and systems can generally be integrated in a single software product or packaged into multiple software products embodied on tangible\nmedia.\nReferences to \"or\" may be construed as inclusive so that any terms described using \"or\" may indicate any of a single, more than one, and all of the described terms.\nThus, particular implementations of the subject matter have been described.  Other implementations are within the scope of the following claims.  In some cases, the actions recited in the claims can be performed in a different order and still\nachieve desirable results.  In addition, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results.  In certain implementations, multitasking and parallel\nprocessing may be advantageous.\nThe claims should not be read as limited to the described order or elements unless stated to that effect.  It should be understood that various changes in form and detail may be made by one of ordinary skill in the art without departing from the\nspirit and scope of the appended claims.  All implementations that come within the spirit and scope of the following claims and equivalents thereto are claimed.", "application_number": "14470201", "abstract": " Matching conversions from within applications on a client device to\n     interactions with content items presented with resources may include\n     receiving first data associated with an interaction with a content item\n     displayed with a resource. The first data includes an application\n     identifier for an application associated with the content item and an\n     account identifier. The matching may also include receiving second data\n     from the client device associated with a conversion using the\n     application. The second data includes the application identifier and a\n     device identifier for the client device. The matching may further include\n     receiving third data from the client device that includes the account\n     identifier and the device identifier. The conversion can be determined as\n     associated with the interaction with the content item displayed with the\n     resource based on matching the second data with the first data based on\n     the third data.\n", "citations": ["6332126", "7124092", "7742946", "7945573", "8396822", "8606696", "8843827", "8887095", "9639858", "20030061132", "20030216960", "20050176407", "20070073585", "20070208618", "20080235243", "20080262928", "20090216579", "20090240538", "20090327151", "20100042495", "20100082629", "20100145926", "20100198772", "20100198826", "20110040655", "20110087916", "20110191714", "20110238476", "20110246267", "20120046995", "20120166272", "20120221411", "20120265599", "20120290389", "20130085841", "20130124309", "20130166376", "20140095297", "20140200991", "20140207567", "20140365296", "20150235258"], "related": ["62038671"]}, {"id": "20160094963", "patent_code": "10375004", "patent_name": "Facilitating social network service connections based on mobile device\n     validated calendar data", "year": "2019", "inventor_and_country_data": " Inventors: \nGupta; Akhilesh (Sunnyvale, CA)  ", "description": "<BR><BR>TECHNICAL FIELD\nThe present application relates generally to the processing of data, and, in various example embodiments, to systems, methods, and computer program products for facilitating social networking service connections based on calendar data validated\nby a mobile device.\n<BR><BR>BACKGROUND\nUsually, when business people meet for the first time they exchange physical or electronic business cards.  In addition, a business meeting attendee who is a member of a social networking service (e.g., LinkedIn.TM.) may wish to connect with\nother attendees who are members of the social networking service via the social networking service.  Sometimes, instead of connecting via the social networking service during the meeting, a member of the social networking service may create a reminder to\nestablish social graph connections with the new people met at the meeting after the meeting.  Examples of such reminders may be a note written on a piece of paper, a note in an electronic document, or a note in an electronic calendar.  Additionally or\nalternatively, an electronic calendar application may issue a reminder to the member of the social networking service to establish social graph connections with the other scheduled attendees via the social networking service.\nHowever, these reminders may be deficient in a number of ways.  For example, a reminder created in a rushed manner and that lacks sufficient information with respect to the event or the attendees may fail to be useful to the member of the social\nnetworking service.  In another example, an automatic reminder based on an electronic calendar of the member of the social networking service may not account for the possibility that the scheduled event did not take place, or the member or the other\nscheduled attendees did not actually attend the scheduled event.  As such, it is not uncommon, for such reminders to be ineffective in assisting the member of the social networking service to remember to connect with other people via the social\nnetworking service. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nSome embodiments are illustrated by way of example and not limitation in the figures of the accompanying drawings, in which:\nFIG. 1 is a diagram that illustrates an ad hoc peer-to-peer network of mobile devices, according to some example embodiments;\nFIG. 2 is a network diagram illustrating a client-server system, according to some example embodiments;\nFIG. 3 is a diagram that illustrates a communication presented in a user interface of a mobile device, according to some example embodiments;\nFIG. 4 is a block diagram illustrating components of a mobile device, according to some example embodiments;\nFIG. 5 is a flowchart that illustrates a method for facilitating social networking service connections based on calendar data validated by a mobile device, according to some example embodiments;\nFIG. 6 is a flowchart that illustrates a method for facilitating social networking service connections based on calendar data validated by a mobile device and represents an additional step of the method illustrated in FIG. 5, according to some\nexample embodiments;\nFIG. 7 is a flowchart that illustrates a method for facilitating social networking service connections based on calendar data validated by a mobile device and represents the step 530 of the method illustrated in FIG. 5 in more detail, according\nto some example embodiments;\nFIG. 8 is a flowchart that illustrates a method for facilitating social networking service connections based on calendar data validated by a mobile device and represents the step 530 of the method illustrated in FIG. 5 in more detail, according\nto some example embodiments;\nFIG. 9 is a flowchart that illustrates a method for facilitating social networking service connections based on calendar data validated by a mobile device and represents the step 530 of the method illustrated in FIG. 5 in more detail, according\nto some example embodiments;\nFIG. 10 is a flowchart that illustrates a method for facilitating social networking service connections based on calendar data validated by a mobile device and represents an additional step of the method illustrated in FIG. 5, according to some\nexample embodiments;\nFIG. 11 is a flowchart that illustrates a method for facilitating social networking service connections based on calendar data validated by a mobile device and represents an additional step of the method illustrated in FIG. 5, according to some\nexample embodiments;\nFIG. 12 is a flowchart that illustrates a method for facilitating social networking service connections based on calendar data validated by a mobile device and represents the steps 530 and 540 of the method illustrated in FIG. 5 in more detail,\naccording to some example embodiments;\nFIG. 13 is a flowchart that illustrates a method for facilitating social networking service connections based on calendar data validated by a mobile device and represents additional steps of the method illustrated in FIG. 5, according to some\nexample embodiments;\nFIG. 14 is a flowchart that illustrates a method for facilitating social networking service connections based on calendar data validated by a mobile device and represents additional steps of the method illustrated in FIG. 5, according to some\nexample embodiments;\nFIG. 15 is a flowchart that illustrates a method for facilitating social networking service connections based on calendar data validated by a mobile device and represents additional steps of the method illustrated in FIG. 5, according to some\nexample embodiments;\nFIG. 16 is a block diagram illustrating a mobile device, according to some example embodiments; and\nFIG. 17 is a block diagram illustrating components of a machine, according to some example embodiments, able to read instructions from a machine-readable medium and perform any one or more of the methodologies discussed herein.\n<BR><BR>DETAILED DESCRIPTION\nExample methods and systems for facilitating social networking service connections based on calendar data validated by a mobile device are described.  In the following description, for purposes of explanation, numerous specific details are set\nforth to provide a thorough understanding of example embodiments.  It will be evident to one skilled in the art, however, that the present subject matter may be practiced without these specific details.  Furthermore, unless explicitly stated otherwise,\ncomponents and functions are optional and may be combined or subdivided, and operations may vary in sequence or be combined or subdivided.\nIn some example embodiments, a user of a mobile device may meet other users of other mobile devices at a scheduled event (e.g., a business meeting, a conference, etc.) or serendipitously (e.g., at a sports game, in an elevator, on a trip, etc.). If the user and the people the user met are members of a social networking service (also \"SNS\"), such as LinkedIn.TM., their mobile devices may be enabled to discover the presence of the other members' mobile devices in a local area and automatically\nestablish a wireless ad hoc peer-to-peer network that includes the members' mobile devices.  In some instances, the establishing of the peer-to-peer network of mobile devices is based on the users of the mobile devices being members of the SNS.  In some\nexample embodiments, a particular mobile device of a particular member of the SNS may not join or participate in the establishing of the peer-to-peer network of mobile devices but may record the discovery of other mobile devices of other members of the\nSNS within the local area (e.g., within a distance or proximity from the mobile device).\nIn some example embodiments, the particular mobile device may transmit a communication to a server associated with the SNS, based on the discovery of other mobile devices of other members of the SNS within the local area.  The communication may,\nin some instances, indicate that the particular member met other members of the SNS at a particular time.  For example, the communication may include an identifier of the particular mobile device of the particular member, one or more identifiers of the\nother mobile devices of the other members of the SNS, and a timestamp associated with a time when the other mobile devices were discovered by the particular mobile device to be in proximity to the particular mobile device (e.g., during a scheduled\nevent).\nIn some instances, the server may confirm that a scheduled event took place and that one or more scheduled attendees of the schedules event were present at the event, based on the communication received from the particular mobile device.  The\nserver may also generate a prompt (e.g., a reminder, a recommendation, a suggestion, etc.) for the particular member to connect with another member of the one or more other members via the SNS based on the determination (or confirmation) that the\nparticular member and the other member met at the scheduled event.  The determination that the particular member and the other member met at the scheduled event may be based on, for example, the calendar data associated with the particular member and the\ncommunication received by the server from the particular mobile device.\nIn some example embodiments, a server associated with a social networking service may receive a first communication from a first mobile device associated with a first member of the social networking service.  The communication may indicate that\nthe first member was in proximity to one or more other members of the social networking service at a particular time.  The server may access calendar data pertaining to the first member.  The calendar data may, for example, be stored in one or more\nrecords of a database associated with the social networking service.  The server may identify a scheduled event associated with the particular time, based on the calendar data.  The server may also determine that the one or more other members were\nscheduled to attend the scheduled event, based on the calendar data.  The server may confirm, based on the first communication and the calendar data, that the scheduled event occurred during a window of time associated with the particular time, and that\nthe first member and the one or more other members attended the scheduled event.\nIn some example embodiments, the server may generate a second communication that includes a prompt for the first member to connect with a second member of the one or more other members via the social networking service.  Similarly, the server\nmay generate a third communication that includes a prompt for a second member of the one or more other members determined to be in proximity of the first member at the particular time to connect with the first member via the social networking service. \nThe prompt (for the first member or for the second member) may be based on the confirmation that the first and second members met at the scheduled event.\nAccording to some example embodiments, the server may cause the second communication to be presented to the first member.  The second communication may include a textual message, audio message, visual message, haptic message, etc. In some\ninstances, the causing of the second communication to be presented to the first member includes displaying the prompt to connect, in a user interface of the first mobile device.\nIn certain example embodiments, the first communication includes a member identifier of the first member, one or more member identifiers of the one or more other members, and a timestamp associated by the first mobile device with a time\nindicating a presence of the first member and the one or more other members in a local area.  In some example embodiments, the confirming that the scheduled event occurred during the window of time associated with the particular time includes matching\nthe timestamp included in the first communication with a time associated with the scheduled event (e.g., a time included in a time range or a time window).  The confirming that the first member attended the scheduled event may include matching the member\nidentifier of the first member with a first identifier of a first scheduled attendee of the scheduled event.  The confirming that the one or more other members attended the scheduled event may include matching the one or more identifiers of the one or\nmore other members with one or more identifiers of one or more other scheduled attendees of the scheduled event.\nIn various example embodiments, the determining that the one or more other members were scheduled to attend the scheduled event is based on one or more identifiers of the one or more other members.  The one or more identifiers of the one or more\nother members may be included in the calendar data pertaining to the first member.  In some example embodiments, the one or more identifiers of the one or more other members include one or more email addresses of the one or more other members.\nThe members of the SNS who met serendipitously or while attending a scheduled event may find it beneficial to be able to view, in user interfaces of their mobile devices, information regarding the newly made acquaintances (e.g., the other event\nattendees).  Such information may include the attendees' names and pictures, electronic business cards, profiles (e.g., titles, seniority, employer's name, decision maker, colleges attended, etc.) and other relevant information pertaining to the\nattendees of the meeting.  Further, the newly acquainted members of the social networking service may wish to establish social graph connections among themselves via the social networking service.  In some instances, the newly acquainted members may\nestablish SNS connections when they first meet.\nIn some example embodiments, the members of the SNS who met serendipitously or while attending a scheduled event may not establish SNS connections when they first meet.  The mobile devices of the members of the SNS may be configured to confirm\nthat certain members have met and may facilitate the establishing of the SNS connections between the members who met, at a later time.  In some instances, the confirming that the members have met may be based on a physical proximity of the mobile devices\nof the members, calendar data, Global Positioning System (GPS) data pertaining to one or more of the mobile devices, one or more time stamps, or a suitable combination thereof.\nFor example, two members of the SNS meet for the first time during a scheduled event (e.g., a business meeting).  It may not be practical for the two members to spend time sending and accepting invitations to connect via the SNS during the\nbusiness meeting.  A first mobile device associated with a first member of the social networking service may be configured to determine that the first member and a second member of the social networking service attended the scheduled event based on a\nproximity between a first mobile device associated with the first member and a second mobile device associated with the second member.  The proximity may be identified by the first mobile device.  The first and second mobile devices of the two members of\nthe social networking service may be configured to facilitate the establishing of a social networking service connection between the two members of the social networking service based on the determining that the first member and the second member of the\nsocial networking service attended the scheduled event.\nFor instance, the first mobile device may generate a first communication for the first member that includes a prompt (e.g., a reminder or a suggestion) to connect with the second member via the social networking service.  In some example\nembodiments, the first mobile device prompts the first member to connect with the second member based on the first mobile device determining that a physical distance value between the first and second mobile devices identified when the first user and the\nsecond user met does not exceed a proximity threshold value.  For example, the first mobile device may issue a prompt to the first user to connect with the second user via the SNS based on determining that the first mobile device and the second mobile\ndevice were within an immediate proximity to each other.  This may indicate that the first member and the second member were in close proximity to each other when they met (e.g., sat next to each other during a scheduled event).  In some example\nembodiments, the first mobile device prompts the first member to connect with the second member based on the first mobile device determining that a duration value identifying the period of time the first mobile device and the second mobile device were in\nproximity to each other exceeds a minimum time threshold value.  For example, the first mobile device may issue a prompt to the first user to connect with the second user via the SNS based on determining that the first and second mobile devices were near\neach other (e.g., in the same room) for at least five minutes.\nThe first mobile device may display the first communication for the first member of the social networking service in a user interface of the first mobile device associated with the first member.  In response to the displaying of the first\ncommunication, the first mobile device may receive an input from the first member via the user interface of the first mobile device.  The input may indicate a request to establish the connection between the first member and the second member via the\nsocial networking service.  Based on receiving the input from the first member, the first mobile device may generate an invitation for the second member to connect with the first member via the social networking service.  The first mobile device may\ntransmit a second communication to the second mobile device associated with the second member.  The second communication may include the invitation for the second member to connect with the first member via the social networking service.\nIn response to the second communication, the first mobile device may receive a third communication from the second device.  The third communication may include an acceptance by the second member of the invitation for the second member to connect\nwith the first member via the social networking service.  Based on the acceptance by the second member of the invitation, a new edge may be added (e.g., by the first mobile device or by a server associated with the SNS) between a first node representing\nthe first member and a second node representing the second member in a first social graph (e.g., a graph data structure in a graph database) associated with the first member.  The new edge may indicate that the second member is a new connection in the\nfirst social graph associated with the first member.\nIn some example embodiments, a particular mobile device is configured to transmit a communication to a server associated with the SNS to indicate an identified change with respect to the presence of other mobile devices within the local area. \nThe communication may also include a timestamp associated with the change.  Examples of identifying a change with respect to the presence of other mobile devices within the local area include determining that a new mobile device has entered the local\narea or determining that a mobile device has left the local area.  In some instances, the identifying of the change is based on a distance value (e.g., the proximity) between the particular mobile device and the other mobile devices in the local area. \nThe determining of one or more changes and of the associated timestamps identifying the time(s) when the change(s) occurred may allow the particular mobile device (or the server receiving the reports of changes and timestamps) to determine that a\nparticular event occurred, to determine the duration of the event, and to identify the attendees of the event, as well as the times that the various attendees spent at the event.  Further, this data pertaining to changes with respect to the presence of\nmobile devices within the local area may facilitate the validating of calendar data using mobile devices.  For example, the change data captured by one or more mobile devices may be utilized to confirm that a scheduled event took place.  In another\nexample, the change data may be utilized to determine which of the scheduled attendees were actually present at the scheduled event and how long.\nIn some example embodiments, when two members meet serendipitously (e.g., not at a scheduled event), there is no scheduled event that may be validated.  In such a case, at least one of the mobile devices of the two members generates a record of\nthe meeting of the two members to memorialize the fact that the two members have met.  In some instances, the at least one of the mobile devices may transmit a communication (e.g., the record of the meeting) to the server associated with the SNS to\nindicate that the two members of the SNS have met.  The server may store a record of the meeting of the two members in a database as, for example, member activity and behavior data.\nThe at least one mobile device may generate a prompt for the respective member and display it in a user interface of the mobile device associated with the respective member to encourage or remind the respective member to connect with the other\nmember via the SNS.  The prompt may be generated based on the record of the serendipitous meeting between the two members.  In some instances, the mobile device may display the prompt to the respective member sometime after the two members meet (e.g.,\nthe day after the members met).\nIn some instances, the server may generate a prompt for the respective member to connect with the other member.  The server may display the prompt to the respective member in a browser based on determining that the respective member has logged\nin and is browsing a particular web site pertaining to the social networking service.\nIn some example embodiments, a prompt may be generated when a first member meets a second and a third member but only connects with the second member via the SNS.  The prompt may be generated (e.g., by the mobile device of the first member or by\nthe server) to remind the first member that the first member also met the third member and that the first member might be interested in connecting with the third member via the SNS.\nIn some example embodiments, a plurality of mobile devices associated with a plurality of members of the SNS may be configured to establish an ad hoc wireless peer-to-peer network (e.g., a session or a connection) that includes the plurality of\nmobile devices, and to transmit data between the connected mobile devices without the involvement of a server.  A mobile device associated with a member of the social network and connected via the peer-to-peer network may exchange (e.g., transmit or\nreceive) electronic business cards with the other mobile devices in the peer-to-peer network, transmit its user's member profile on the social networking service to the other mobile devices or receive other member profiles from the other mobile devices,\ntransmit or receive invitations for members to chat via the ad hoc wireless peer-to-peer network, transmit or receive invitations for members to connect via the social networking service, establish social networking connections on the social networking\nservice between members of the social networking service via the ad hoc wireless peer-to-peer network, display communications (e.g., communications that are about to be transmitted to other mobile devices via the peer-to-peer network or communications\nreceived from other mobile device via the peer-to-peer network), etc.\nThe establishing of an ad hoc wireless one-to-one connection or of a multi-peer connection among mobile devices without the use of a server may enhance the experiences of the users of the mobile devices who are attending the meeting, for\nexample, by facilitating speedy interactions among the users, by providing information directly to the mobile devices included in the an ad hoc wireless peer-to-peer connection, or by providing prompts to establish social networking connections among the\nusers.  In some example embodiments, a mobile device of a meeting attendee may determine the identities of the other meeting attendees based on the identifiers (e.g., member identifiers) advertised to a local area by the mobile devices of the other\nmeeting attendees.  Also, the mobile device of a particular member may identify which of the other attending members are existing social networking service connections of the particular member, and may display the names of the existing connections in a\nuser interface of the mobile device of the particular member.\nFurther, the mobile device may identify the members who are not connected via the social networking service and may prompt the respective members to connect via the social networking service.  In addition, the mobile device may obtain\ninformation about the other attendees' backgrounds and display that information in the user interfaces of the mobile devices of the other attendees.  The mobile device may also store the names and photographs of the meeting attendees, in some instances,\nin association with data that identifies certain attendees as being met, by the user of the mobile device, at the respective meeting for the first time.  These and other functionalities of a mobile device connected via an ad hoc multi-peer network may\nallow the user of the device to take notes with reference to the speaker, identify the decision maker in the group, have chat sessions with other attendees during the meeting, learn about new people attending who were not scheduled to attend, remember\nthe names of the attendees met at the respective meeting for the first time, match names to faces via photographs presented via the mobile device, etc.\nFurthermore, in some example embodiments, the mobile device(s) may memorialize (e.g., store a record of) the activities that involved the mobile device(s), the ad hoc wireless peer-to-peer network, or both, on the mobile device(s) and/or in one\nor more records of a database associated with the social networking service (e.g., via a server of an interaction support system).  For example, a record of the activity that pertains to the users of the mobile devices exchanging electronic business\ncards via the ad hoc wireless peer-to-peer network may be stored, by at least one of the involved mobile devices, as member activity and behavior data in a database associated with the social networking system.\nIn another example, the mobile device of the member Alice White and the mobile device of the member Bob Smith are connected via the peer-to-peer network.  Alice's mobile device may display an identifier (e.g., the name and/or photograph) of Bob\nin a user interface of Alice's mobile device.  Similarly, Bob's mobile device may display an identifier (e.g., the name and/or photograph) of Alice in a user interface of Bob's mobile device.  Alice may request to connect with Bob via the social\nnetworking service by, for example, clicking on a \"Connect\" button displayed in the user interface of Alice's mobile device in association with the identifier of Bob.  Alice's mobile device may transmit a communication including an invitation to connect\nwith Bob on the social networking service to Bob's mobile device via the ad hoc wireless peer-to-peer network.\nAlice's mobile device may also transmit a communication to a server associated with the social networking service.  The communication to the server may indicate that Alice invited Bob to connect on the social networking service.  The server may\ngenerate a database record that indicates that Alice has invited Bob to connect via the social networking service.  In some example embodiments, the generating of the database record may allow the maintaining of an up-to-date list of invitations received\nby Bob.  For example, if, upon receiving Alice's invitation to connect, Bob were to view a website associated with the social networking service, he may be able to see an indication of Alice's invitation to connect via the social networking service.\nBob may accept Alice's invitation to connect via the social networking service (e.g., by selecting an \"Accept\" button in the user interface of the Bob's mobile device).  In response to the communication including the invitation to connect on the\nsocial networking service received from Alice's mobile device, Bob's mobile device may transmit a further (e.g., a second) communication including an acceptance of Alice's invitation to connect on the social networking service via the ad hoc wireless\npeer-to-peer network.  Upon receiving the second communication from Bob's mobile device, Alice's mobile device may display an indication of Bob's acceptance of Alice's invitation to connect in the user interface of Alice's mobile device (e.g., \"Bob\naccepted your invitation to connect via the SNS.\").\nBob's mobile device may also transmit another communication to the server associated with the social networking service.  The other communication to the server may indicate that Bob accepted Alice's invitation to connect on the social networking\nservice.  Based on the other communication, the server may generate another database record that indicates that Alice and Bob have established a connection via the social networking service.  Further, the server may update the record(s) that pertain to\nthe two members (e.g., Alice and Bob) in a social graph database to indicate the establishing of the new relationship between the two members of the social networking service.  The new relationship between the two members may be represented by a new edge\nconnecting a first graph node representing one of the two members (e.g., Alice) and a second graph node representing the other of the two members (e.g., Bob).\nIn certain example embodiments, the context or the physical circumstances associated with one or more members of the social network, at a particular time, may be leveraged (e.g., by the mobile devices or by an interaction support system) to\nfacilitate context-aware, social-graph-based interactions between the members via the members' mobile devices.  According to one example, based on access to a member's calendar, geolocation (e.g., GPS) data, member profile data, or a suitable combination\nthereof, one or more mobile devices of the attending members or the interaction support system may identify the people that a particular member is likely to meet at a scheduled event.  The mobile devices or the interaction support system may provide the\nnames, email addresses, or other additional data determined (e.g., by the mobile devices or by the interaction support system) to be pertinent to the meeting or useful to a particular member.  During the meeting, the mobile devices of the attending\nmembers may automatically establish a multi-peer connection between the mobile devices and facilitate the transmission of data (e.g., electronic business cards) between the attending members.  A record of the attendees and the data transmitted may be\nstored on one or more of the mobile devices of the members attending the meeting.  In some instances, the record of the attendees and the data transmitted may be stored in a database associated with the interaction support system.\nIn some example embodiments, a plurality of mobile devices that have WiFi.TM.  or Bluetooth.RTM.  capabilities activated and that are located within a particular distance from each other may discover each other and may establish one or more ad\nhoc wireless peer-to-peer connections (e.g., one or more wireless peer-to-peer networks) among themselves, all without the involvement of an intermediary entity (e.g., a server).  Some or all of the functionalities associated with the establishing of the\none or more wireless peer-to-peer connections among the mobile devices may be performed by one or more modules associated with one or more copies of an application executed on the respective mobile devices.\nIn some example embodiments, a user of a mobile device that hosts a copy of such an application interacts with the copy of the application via a user interface of the mobile device when configuring one or more settings associated with the copy\nof the application.  The user may specify that the mobile device may automatically establish ad hoc wireless peer-to-peer networks with other mobile devices that host other copies of the application.  Based on the configuration provided by the user of\nthe mobile device, the mobile device may participate in the establishing of one or more wireless peer-to-peer connections with other mobile devices discovered in a local area.  The wireless peer-to-peer connections among a plurality of mobile devices may\nbe established ad hoc (e.g., without planning) based on the plurality of mobile devices being within a distance from each other and discovering the presence of each other.\nFor example, a particular mobile device may broadcast (or advertise) its availability to establish wireless peer-to-peer connections with one or more other mobile devices located within a particular distance from the particular mobile device. \nThe one or more other mobile devices, may receive the broadcast of the particular mobile devices, may respond to the broadcast, and may participate in the establishing of the wireless peer-to-peer connections among the mobile devices.  In some instances,\nthe mobile devices may facilitate the establishing of the wireless peer-to-peer connections with the other advertising mobile devices based on determining that the users associated with the other mobile devices are members of the social networking\nservice.\nIn some example embodiments, the iBeacon.TM.  technology provided by Apple Inc.  may be used by a plurality of mobile devices to facilitate the establishment of an ad hoc wireless peer-to-peer connection among the plurality of mobile devices\nusing Bluetooth.RTM.  low energy proximity sensing.  The plurality of mobile devices may be both transmitting (e.g., broadcasting, advertising, etc.) communications to other mobile devices in a local area and receiving communications from the other\nmobile devices in the local area.  As such, the advertising mobile device and the listening mobile device may have both advertising and listening capabilities.\nAccording to certain example embodiments, one or more of the mobile devices host copies of an application that performs one or more of the functionalities associated with establishing the wireless ad hoc peer-to-peer connection.  The copies of\nthe application may associate the one or more mobile devices with a universally unique identifier (UUID) for purposes of identifying other mobile devices in the local area that have the same UUID and facilitating the establishing of an ad hoc wireless\npeer-to-peer network among a plurality of mobile device in the local area.\nFor example, a mobile device broadcasts a communication including a UUID to a local area.  The mobile device may also broadcast a major number and a minor number.  The major and minor numbers may be mapped to a member identifier associated with\nthe advertising device by a listening mobile device.  The member identifier may be generated, in some instances, upon the user of the advertising mobile device being authenticated by the advertising mobile device (e.g., based on login data provided by\nthe user).  According to some example embodiments, without determining that the user's login data maps to a valid member identifier, the advertising mobile device may not advertise its availability to participate in establishing (or joining) the ad hoc\nwireless peer-to-peer network.\nThe transmitted UUID may be picked up by another mobile device that is listening for UUIDs.  In some example embodiments, the other mobile device (e.g., the listening mobile device) determines that the UUID received from the transmitting mobile\ndevice is the same as its own UUID.  Upon matching the received UUID and its own UUID, the listening mobile device may access and map a combination of the major and minor numbers received from the transmitting mobile device to a member identifier of a\nparticular member of a social networking service.  In some example embodiments, the major and minor numbers are encrypted at the advertising mobile device, and are decrypted at a listening mobile device.  In some instances, if the listening mobile device\ndoes not match the received UUID with its own UUID, the listening mobile device may not access (or decrypt) the major and minor numbers.\nThe member identifier of the particular member may be associated with the advertising mobile device.  The listening mobile device may determine that the user associated with the advertising mobile device is a member of the social networking\nservice.  In some example embodiments, a plurality of mobile devices may participate in the establishing of a wireless peer-to-peer connection among themselves based on determining that the users associated with the respective mobile devices are members\nof the social networking service.  In some example embodiments, a listening mobile device participates in establishing (or joining) an ad hoc wireless peer-to-peer network only with transmitting mobile devices that are associated with the same UUID as\nthe listening mobile device.\nThe iBeacon.TM.  technology may allow the determination of the proximity of certain mobile device.  For example, a particular mobile device may determine, based on the advertising transmitted by another mobile device, whether the other device is\nin an immediate proximity to the particular mobile device, is near the particular mobile device, or is far from the mobile device.  For example, the particular mobile device may determine a received signal strength indicator (RSSI) value associated with\na radio signal received from the other mobile device.  The particular mobile device may identify a distance value between the particular mobile device and the other mobile device based on the RSSI value.\nIn some example embodiments, if the distance between two mobile devices is determined by a mobile device to not exceed a threshold value (e.g., a proximity threshold value), the two mobile devices automatically establish the wireless\npeer-to-peer connection between the two mobile devices.  In certain example embodiments, if the distance between two mobile devices is determined by a mobile device to exceed the threshold value, the two mobile devices issue queries displayed to the\nusers associated with the respective mobile devices.  The queries may, for example, be in the form of a question regarding the connecting to the other mobile device (e.g., \"Would you like to connect to the mobile device of John Doe?\" or \"Would you like\nto connect to John Doe?\").\nIn various example embodiments, a plurality of mobile devices may utilize the multi-peer connectivity technology to facilitate the establishment of a wireless peer-to-peer connection among the plurality of mobile devices.  The multi-peer\nconnectivity technology may allow mobile devices to send connection requests, accept connection requests, facilitate chatting among the users of the connected mobile devices, send files, etc.\nDuring the initial, discovery phase of the establishing of a multi-peer session, a plurality of mobile devices advertise their peer identifier (ID) and receive the peer IDs of other mobile devices from other mobile devices located at a\nparticular distance from each other.  In some instances, the peer ID is a member identifier (ID).  In other instances, the peer ID identifies a particular mobile device, and may be mapped to the member ID of the user associated with the particular mobile\ndevice.\nUpon discovering each other, the plurality of mobile devices may automatically establish connections among all the discovered mobile devices that advertised their peer IDs.  The connections are established locally, peer-to-peer (e.g., mobile\ndevice to mobile device), without the involvement of the server.  The mobile devices may display a number of identifiers of the users associated with the connected peers (e.g., the mobile devices) in user interfaces of the mobile devices.  The peers may\njoin the session and may leave the session.  When a peer leaves the session, the peer is dropped out of the pool of connections, and the number of connected users that may be seen on the screen of each remaining mobile device is decreased to indicate\nthat a particular peer has left the session.\nFIG. 1 is a diagram that illustrates an ad hoc peer-to-peer network of mobile devices, according to some example embodiments.  The ad hoc peer-to-peer network of mobile devices may include two or more mobile devices of various types.  As shown\nin FIG. 1, mobile devices 110 and 120 are smartphones.  In certain example embodiments, one or more of the mobile devices that may facilitate a wireless peer-to-peer connection among mobile devices may be wearable devices, tablets, or other mobile\ndevices.  In some instances, a wearable device is a stand-alone or independent device.  In some instances, the wearable device is an accessory-like device that works in conjunction with another mobile device (e.g., a smart phone).  For example, the\nwearable device may be an Input/Output (I/O) device for an application executed on a smart phone.\nAccording to certain example embodiments, the peer-to-peer connection among a plurality of mobile devices is established automatically based on the users associated with the respective mobile devices previously configuring copies of a particular\napplication (e.g., a social networking application) hosted on their mobile devices to automatically establish the wireless peer-to-peer connections.  In some example embodiments, the mobile device 110 advertises (e.g., broadcasts) a first identifier to a\nlocal area.  The first identifier, in some instances, is a device identifier of the mobile device 110.  In other instances, the identifier is a member identifier of the member of the social network, associated with the mobile device 110.  The mobile\ndevice 110 may broadcast the first identifier via a transceiver of the mobile device 110.\nSimilarly, the mobile device 120 may advertise (e.g., broadcast) a second identifier of the mobile device 120 to the local area.  The second identifier, in some instances, is a device identifier of the mobile device 120.  In other instances, the\nsecond identifier is a member identifier of the member of the social network associated with the mobile device 120.  In some example embodiments, the advertised identifiers may be referenced in the communications broadcast (e.g., transmitted) by the\nmobile devices 110 or 120, respectively.\nIn addition to broadcasting communications, the mobile device 110 listens to communications broadcast by other mobile devices (e.g., the mobile device 120, etc.) located within the proximity (e.g., a local area or a particular distance) of the\nmobile device 110.  Similarly, the mobile device 120 listens to communications broadcast by other mobile devices located within the proximity of the mobile devices 120, for example, the mobile device 110.\nWhen the mobile device 110 discovers an advertising device (e.g., the mobile device 120), the mobile device 110 may send a request to the advertising device to establish a wireless peer-to-peer connection, which the advertising device may accept\nor decline.  In some example embodiments, a mobile device makes the determination whether to accept or decline the request to establish the peer-to-peer connection based on the configuration settings associated with the application executed on the mobile\ndevice.\nIn some instances, when the configuration settings (e.g., provided by the user of the mobile device) specify a permission to automatically establish the peer-to-peer connections, the mobile device may automatically establish the wireless\npeer-to-peer connection with the requesting mobile device in response to the received request to establish the wireless peer-to-peer connection.  In other instances, when the configuration settings do not specify a permission to automatically establish\nthe peer-to-peer connections, the mobile device generates and presents a communication to the user of the mobile device via a user interface of the mobile device.  The communication may, for example, be in the form of a question regarding the connecting\nto the other mobile device (e.g., \"John, would you like to connect to the mobile device of Dave Austin?\" or \"Would you like to connect to Dave Austin?\")\nBy accepting the invitation to connect, a session is established and the mobile devices may start communicating.  The mobile devices may communicate by transmitting data, such as images, video, texts, etc.\nThe session (or peer-to-peer connection, or peer-to-peer network) may be established between two or more peers (e.g., mobile devices).  For example, as shown in FIG. 1, the mobile devices 110 and 120 are included in the multi-peer connection\nestablished by the mobile devices 110 and 120.  Other mobile devices not shown in FIG. 1 may be included in the peer-to-peer network of mobile devices.  If one of the mobile devices leaves the session, the session may be maintained as long as at least\ntwo mobile devices are still connected.\nAs shown in FIG. 1, the mobile device 110 displays a user interface 112 to a first user associated with the mobile device 110.  Similarly, the mobile device 120 displays a user interface 122 to a second user associated with the mobile device\n120.  The mobile devices 110 and 120 may display a variety of information in the user interfaces 112 or 122, respectively.\nAs shown in FIG. 1, the user interface 112 of the mobile device 110 may inform the user associated with the mobile device 110 about the members of the social networking service who are discovered (e.g., by the mobile device 110) to be in a\nproximity to the mobile device 110 by displaying information pertaining to the discovered members.  For example, the mobile device 110 displays in the user interface 112 presented to Dave Austin, the member associated with the mobile device 110, profile\ndata pertaining to the member John Smith.  The profile data of John Smith may include his name, image (e.g., a photograph or video), title, employer's name, and location.  The profile data of John Smith may also include data pertaining to his work\nexperience and education.  In some instances, the user interface 112 may also indicate that one or more of the discovered nearby members are existing connections of Dave Austin via the social networking service.\nSimilarly, the user interface 122 of the mobile device 120 may inform the user associated with the mobile device 120 about the members of the social networking service who are discovered (e.g., by the mobile device 120) to be in a proximity to\nthe mobile device 120 by displaying information pertaining to the discovered members.  For example, the mobile device 120 displays in the user interface 122 presented to John Smith, the member associated with the mobile device 120, profile data\npertaining to the member Dave Austin.  The profile data of Dave Austin may include his name, image (e.g., a photograph or video), title, employer's name, and location.  The profile data of Dave Austin may also include data pertaining to his work\nexperience and education.  In some instances, the user interface 112 may also indicate that one or more of the discovered nearby members are existing connections of John Smith via the social networking service.\nAccording to certain example embodiments, the mobile devices 110 and 120 may facilitate interactions by the members associated with the mobile devices 110 and 120.  The mobile devices 110 and 120 may provide John Smith and Dave Austin with\ninformation regarding the wireless peer-to-peer session established between the mobile devices 110 and 120 (and, possibly, other mobile devices), such as the names of the members whose mobile devices are still connected via the ad hoc peer-to-peer\nnetwork, the name of the scheduled attendees who are not present at the scheduled event, the names of the people who are not scheduled to attend but are present at the scheduled event, the actual duration of the scheduled event, etc.\nIn some example embodiments, a mobile device may generate a message for the user of the mobile device to trigger an action by the user (e.g., the transmitting of a text message to another user associated with another mobile device, the\ntransmitting of an invitation to connect via the social networking service, etc.).  In certain example embodiments, a first mobile device may respond to a query generated by a second mobile device and transmitted to the first mobile device via the\npeer-to-peer network.  The query may pertain to the user associated with the first mobile device.  These and other items of content generated by the mobile devices may be displayed by the mobile devices in one or more user interfaces of the mobile\ndevices connected wirelessly via the peer-to-peer session.\nIn certain example embodiments one or more of the mobile devices illustrated in FIG. 1 may be clients in a client-server system.  One or more of the mobile devices 110 and 120 may transmit and receive communications to and from a server\nassociated with the social networking service.  For example, the mobile device 110 may send a request for information about one of the users associated with the other mobile devices (e.g., the SNS member associated with the mobile device 120, John\nSmith).\nAn example method and system for facilitating social networking service connections based on calendar data validated by a mobile device may be implemented in the context of the client-server system illustrated in FIG. 2.  As shown in FIG. 2, the\nsocial networking system 220 is generally based on a three-tiered architecture, consisting of a front-end layer, application logic layer, and data layer.  As is understood by skilled artisans in the relevant computer and Internet-related arts, each\nmodule or engine shown in FIG. 2 represents a set of executable software instructions and the corresponding hardware (e.g., memory and processor) for executing the instructions.  To avoid obscuring the inventive subject matter with unnecessary detail,\nvarious functional modules and engines that are not germane to conveying an understanding of the inventive subject matter have been omitted from FIG. 2.  However, a skilled artisan will readily recognize that various additional functional modules and\nengines may be used with a social networking system, such as that illustrated in FIG. 2, to facilitate additional functionality that is not specifically described herein.  Furthermore, the various functional modules and engines depicted in FIG. 2 may\nreside on a single server computer, or may be distributed across several server computers in various arrangements.  Moreover, although depicted in FIG. 2 as a three-tiered architecture, the inventive subject matter is by no means limited to such\narchitecture.\nAs shown in FIG. 2, the front end layer consists of a user interface module(s) (e.g., a web server) 222, which receives requests from various client-computing devices including one or more client device(s) 250, and communicates appropriate\nresponses to the requesting device.  For example, the user interface module(s) 222 may receive requests in the form of Hypertext Transport Protocol (HTTP) requests, or other web-based, application programming interface (API) requests.  The client\ndevice(s) 250 may be executing conventional web browser applications and/or applications (also referred to as \"apps\") that have been developed for a specific platform to include any of a wide variety of mobile computing devices and mobile-specific\noperating systems (e.g., iOS.TM., Android.TM., Windows.RTM.  Phone).\nFor example, client device(s) 250 may be executing client application(s) 252.  The client application(s) 252 may provide functionality to present information to a user and to participate in establishing wireless peer-to-peer networks with other\nclient application(s) 252 executed on other client device(s) 250.  The client application(s) 252 may also provide functionality to communicate via the network 240 to exchange information with the social networking system 220 or with other client\napplication(s) 252 executed on other client device(s) 250.  Each of the client devices 250 may comprise a computing device that includes at least a display and communication capabilities with the network 240 to access the social networking system 220 or\nother client device(s) 250.  The client devices 250 may comprise, but are not limited to, remote devices, work stations, computers, general purpose computers, Internet appliances, hand-held devices, wireless devices, mobile, devices, portable devices,\nwearable computers, cellular or mobile phones, personal digital assistants (PDAs), smart phones, tablets, ultrabooks, netbooks, laptops, desktops, multi-processor systems, microprocessor-based or programmable consumer electronics, game consoles, set-top\nboxes, network PCs, mini-computers, wearable devices (e.g., head-mounted displays), smart watches, and the like.  One or more users 260 may be a person, a machine, or other means of interacting with the client device(s) 250.  The user(s) 260 may interact\nwith the social networking system 220 or with other user(s) 260 via the client device(s) 250.  The user(s) 260 may not be part of the networked environment, but may be associated with client device(s) 250.\nAs shown in FIG. 2, the data layer includes several databases, including a database 228 for storing data for various entities of a social graph.  In some example embodiments, a \"social graph\" is a mechanism used by an online social network\nservice (e.g., provided by the social networking system 220) for defining and memorializing, in a digital format, relationships between different entities (e.g., people, employers, educational institutions, organizations, groups, etc.).  Frequently, a\nsocial graph is a digital representation of real-world relationships.  Social graphs may be digital representations of online communities to which a user belongs, often including the members of such communities (e.g., a family, a group of friends, alums\nof a university, employees of a company, members of a professional association, etc.).  The data for various entities of the social graph may include member profiles, company profiles, educational institution profiles, as well as information concerning\nvarious online or offline groups.  Of course, with various alternative embodiments, any number of other entities may be included in the social graph, and as such, various other databases may be used to store data corresponding to other entities.\nConsistent with some embodiments, when a person initially registers to become a member of the social networking service, the person is prompted to provide some personal information, such as the person's name, age (e.g., birth date), gender,\ninterests, contact information, home town, address, the names of the member's spouse and/or family members, educational background (e.g., schools, majors, etc.), current job title, job description, industry, employment history, skills, professional\norganizations, interests, and so on.  This information is stored, for example, as profile data in the database 228.\nOnce registered, a member may invite other members, or be invited by other members, to connect via the social networking service.  A \"connection\" may specify a bi-lateral agreement by the members, such that both members acknowledge the\nestablishment of the connection.  Similarly, with some embodiments, a member may elect to \"follow\" another member.  In contrast to establishing a connection, the concept of \"following\" another member typically is a unilateral operation, and at least with\nsome embodiments, does not require acknowledgement or approval by the member that is being followed.  When one member connects with or follows another member, the member who is connected to or following the other member may receive messages or updates\n(e.g., content items) in his or her personalized content stream about various activities undertaken by the other member.  More specifically, the messages or updates presented in the content stream may be authored and/or published or shared by the other\nmember, or may be automatically generated based on some activity or event involving the other member.  In addition to following another member, a member may elect to follow a company, a topic, a conversation, a web page, or some other entity or object,\nwhich may or may not be included in the social graph maintained by the social networking system.  With some embodiments, because the content selection algorithm selects content relating to or associated with the particular entities that a member is\nconnected with or is following, as a member connects with and/or follows other entities, the universe of available content items for presentation to the member in his or her content stream increases.  As members interact with various applications, items\nof content, and user interfaces of the social networking system 220, information relating to the member's activity and behavior (e.g., data pertaining to the member selecting or clicking on an online ad) may be stored in a database, such as database 232.\nThe social networking system 220 may provide a broad range of other applications and services that allow members the opportunity to share and receive information, often customized to the interests of the member.  For example, with some\nembodiments, the social networking system 220 may include a photo sharing application that allows members to upload and share photos with other members.  With some embodiments, members of the social networking system 220 may be able to self-organize into\ngroups, or interest groups, organized around a subject matter or topic of interest.  With some embodiments, members may subscribe to or join groups affiliated with one or more companies.  For instance, with some embodiments, members of the social network\nservice may indicate an affiliation with a company at which they are employed, such that news and events pertaining to the company are automatically communicated to the members in their personalized activity or content streams.  With some embodiments,\nmembers may be allowed to subscribe to receive information concerning companies other than the company with which they are employed.  Membership in a group, a subscription or following relationship with a company or group, as well as an employment\nrelationship with a company, are all examples of different types of relationships that may exist between different entities, as defined by the social graph and modeled with social graph data of database 230.  The social graph data may be stored in the\ndatabase 230 in one or more graph data structures.  The graph data structures may utilize nodes (e.g., for entities), edges (e.g., for connections or relationships between entities), and properties to represent and store the data pertaining to different\ntypes of relationships that may exist between different entities.\nThe application logic layer includes various application server module(s) 224, which, in conjunction with the user interface module(s) 222, generates various user interfaces with data retrieved from various data sources or data services in the\ndata layer.  With some embodiments, individual application server modules 224 are used to implement the functionality associated with various applications, services, and features of the social networking system 220.  For instance, a messaging\napplication, such as an email application, an instant messaging application, or some hybrid or variation of the two, may be implemented with one or more application server modules 224.  A photo sharing application may be implemented with one or more\napplication server modules 224.  A search engine that searches for items of content pertaining to users of the client device(s) 250 based on queries received from other client device(s) 250 may be implemented with one or more application server modules\n224.  Similarly, the search engine may enable users to search for and browse member profiles.  Of course, other applications and services may be separately embodied in their own application server modules 224.  As illustrated in FIG. 2, social networking\nsystem 220 may include the interaction support system 226, which is described in more detail below.\nAdditionally, a third party application(s) 248, executing on a third party server(s) 246, is shown as being communicatively coupled to the social networking system 220 and the client device(s) 250.  The third party server(s) 246 may support one\nor more features or functions on a website hosted by the third party.\nFIG. 3 is a diagram that illustrates a communication presented in a user interface of a mobile device, according to some example embodiments.  As shown in FIG. 3, the mobile device 110 may display a prompt for the member associated with the\nmobile device 110, Dave Austin, the user interface 112 to remind him to connect with John Smith via the SNS.  The user interface 112 may also include an indication as to when and where Dave Austin met John Smith (e.g., \"You met John Smith at the ABC\nEvent you attended yesterday.\").  If Dave selects the \"Yes\" button included in the user interface 112, the mobile device 110 may transmit an invitation to connect on the SNS to the mobile device 122 of the member John Smith.  If Dave selects the \"No\"\nbutton in the user interface 112, the mobile device 110, in some instances, may not display the prompt to connect with John Smith again.\nSimilarly, the user interface 122 of the mobile device 120 may inform the second user about the members of the social networking service who are discovered (e.g., by the mobile device 120) to be in a proximity to the mobile device 120 by\ndisplaying a statement such as \"Social Networking Service (SNC) Members Nearby: Alice White and Catherine Jones.\" The member Alice White may be the first user associated with the mobile device 110, and the member Catherine Jones may be the third user\nassociated with the mobile device 130.  In some instances, the user interface 122 may also indicate that one or more of the discovered nearby members are existing connections of the first user via the social networking service (e.g., \"Alice is an\nexisting connection via the SNC.\").  The mobile device 120 may generate and display a further message such as \"Would you like to connect to Catherine via the SNC?\" via the user interface 122.  The mobile device 120 may receive input from the second user\nvia the user interface 122 in response to the further message displayed to the second user.  For example, the second user (e.g., the member Bob Smith) may provide a command to send an invitation to connect to the member Catherine Jones (e.g., the third\nuser) by selecting (e.g., clicking on) the further message displayed in the user interface 122.\nFIG. 4 is a block diagram illustrating components of the mobile device 110, according to some example embodiments.  As shown in FIG. 4, the mobile device 110 may include a calendar module 410, a proximity module 420, a member connecting module\n430, a user interface module 440, a mobile device communication module 450, a social networking module 460, a mobile device connecting module 470, a server communication module 480, and a security module 490, all configured to communicate with each other\n(e.g., via a bus, shared memory, or a switch).\nAny one or more of the modules described herein may be implemented using hardware (e.g., one or more processors of a machine) or a combination of hardware and software.  For example, any module described herein may configure a processor (e.g.,\namong one or more processors of a machine) to perform the operations described herein for that module.  In some example embodiments, any one or more of the modules described herein may comprise one or more hardware processors and may be configured to\nperform the operations described herein.  In certain example embodiments, one or more hardware processors are configured to include any one or more of the modules described herein.\nMoreover, any two or more of these modules may be combined into a single module, and the functions described herein for a single module may be subdivided among multiple modules.  Furthermore, according to various example embodiments, modules\ndescribed herein as being implemented within a single machine, database, or device may be distributed across multiple machines, databases, or devices.  The multiple machines, databases, or devices are communicatively coupled to enable communications\nbetween the multiple machines, databases, or devices.  The modules themselves are communicatively coupled (e.g., via appropriate interfaces) to each other and to various data sources, so as to allow information to be passed between the applications so as\nto allow the applications to share and access common data.  Furthermore, the modules may access one or more databases 492 (e.g., the database 228, the database 230, or the database 232).\nFIGS. 5-15 are flowcharts of a method for facilitating social networking service connections based on calendar data validated by a mobile device, according to some example embodiments.  Operations in the method 500 may be performed using modules\ndescribed above with respect to FIG. 4.  As shown in FIG. 5, the method 500 may include one or more of operations 510, 520, 530, and 540.\nAt method operation 510, the calendar module 410 accesses (e.g., receives, obtains, etc.) calendar data that pertains to an electronic calendar of a first member of a social networking service.  In some example embodiments, the accessing of the\ncalendar data that pertains to the electronic calendar of the first member is performed at the first mobile device associated with the first member of the social networking service.  In certain example embodiments, the accessing of the calendar data that\npertains to the electronic calendar of the first member is performed at a server that is associated with the social networking service and that is in electronic communication with the first mobile device.\nAt method operation 520, the calendar module 410 identifies a scheduled event.  The scheduled event may occur at a future time.  The identifying of the scheduled event may be based on the calendar data of the first member of the social\nnetworking service.  In some example embodiments, the identifying of the scheduled event is performed at the first mobile device associated with the first member of the social networking service.  In certain example embodiments, the identifying of the\nscheduled event is performed at a server that is associated with the social networking service and that is in electronic communication with the first mobile device.\nAt method operation 530, the proximity module 420 determines that the first member and a second member of the social networking service attended the scheduled event.  The determining may be based on a proximity between a first mobile device\nassociated with the first member and a second mobile device associated with the second member.  The proximity may be identified by the first mobile device.\nAt method operation 540, the member connecting module 430 generates a communication for the first member.  The communication may include a prompt to connect with the second member via the social networking service.  The generating of the\ncommunication may be based on the determining that the first member and the second member of the social networking service attended the scheduled event.  Further details with respect to the method operations of the method 500 are described below with\nrespect to FIGS. 6-15.\nAs shown in FIG. 6, the method 500 may include method operation 601, according to some example embodiments.  Method operation 601 may be performed before or after method operation 540, in which the member connecting module 430 generates a\ncommunication for the first member.  At method operation 601, the proximity module 420 determines a period of time that the scheduled event lasted based on the proximity between the first mobile device and the second mobile device.  For example, the\nproximity module 420 may identify a start time of the scheduled event based on the calendar data, the proximate location of (or a distance between) the first mobile device and the second mobile device, or both.  The proximity module 420 may also identify\nan end time of the scheduled event based on the calendar data, a change in the proximate location of (or an increase in distance between) the first mobile device and the second mobile device, or both.  The difference between the end time of the scheduled\nevent and the start time of the scheduled time may identify the actual duration of the scheduled event.\nAs shown in FIG. 7, the method 500 may include method operation 701, according to some example embodiments.  Method operation 701 may be performed as part (e.g., a precursor task, a subroutine, or a portion) of method operation 530, in which the\nproximity module 420 determines that the first member and the second member of the social networking service attended the scheduled event.\nAt method operation 701, the proximity module 420 determines a period of time that the first mobile device and the second mobile device where in proximity to each other based on a plurality of received signal strength indicator (RSSI) values\nassociated with radio signals received from the second mobile device.  The plurality of the RSSI values may be determined by the first mobile device.\nAs shown in FIG. 8, the method 500 may include one or more of method operations 801, 802, and 803, according to some example embodiments.  Method operation 801 may be performed as part (e.g., a precursor task, a subroutine, or a portion) of\nmethod operation 530, in which the proximity module 420 determines that the first member and the second member of the social networking service attended the scheduled event.  At method operation 801, the proximity module 420 determines (e.g., identifies,\ncomputes, measures, etc.) a received signal strength indicator (RSSI) value associated with a radio signal received from the second mobile device.\nMethod operation 802 may be performed after method operation 801.  At method operation 802, the proximity module 420 identifies a distance value (e.g., immediate, near, far, etc.) between the first and second mobile device based on the RSSI\nvalue.\nMethod operation 803 may be performed after method operation 802.  At method operation 803, the proximity module 420 determines that the distance value between the first mobile device and the second mobile device does not exceed a proximity\nthreshold value.\nAs shown in FIG. 9, the method 500 may include one or more of method operations 901, 902, and 903, according to some example embodiments.  Method operation 901 may be performed as part (e.g., a precursor task, a subroutine, or a portion) of\nmethod operation 530, in which the proximity module 420 determines that the first member and the second member of the social networking service attended the scheduled event.  In some example embodiments, the method operation 901 is performed after method\noperation 803, in which the proximity module 420 determines that the distance value between the first mobile device and the second mobile device does not exceed a proximity threshold value.\nAt method operation 901, the proximity module 420 identifies a start time that corresponds to a first time that the distance value (e.g., a first distance value) between the first mobile device and the second mobile device did not exceed the\nproximity threshold value.  In some instances, the start time may correspond to a time when the first mobile device and second mobile device are determined (e.g., by at least one of the first mobile device or the second mobile device) to come within a\nparticular close distance to each other.  For example, the proximity module 420 may determine that the second mobile device is located in close proximity to the first mobile device, and may identify the time when the distance value between the first and\nsecond mobile devices became less than a particular proximity threshold value.\nIn some example embodiments, the proximity module 420 may determine, based on location (e.g., Global Positioning System (GPS)) data pertaining to the first mobile device and/or the calendar data, the time when the first mobile device has arrived\nat the location of the scheduled event.  If the location of the first mobile device is determined to be the same as the location of the scheduled event and the first distance value is determined to not exceed the proximity threshold value, the identified\nstart time associated with the first distance value between the first mobile device and the second mobile device may indicate when the second member associated with the second mobile device has joined (e.g., has arrived at the location of) the scheduled\nevent.\nMethod operation 902 may be performed after the method operation 901.  At method operation 902, the proximity module 420 identifies an end time that corresponds to a second time that the distance value (e.g., a second distance value) between the\nfirst mobile device and the second mobile device exceeded the proximity threshold value.  For example, the end time may correspond to a time when the distance value between the first mobile device and second mobile device is determined by the proximity\nmodule 420 to have become greater than the proximity threshold value such as when at least one of the first member or the second member has left location of the scheduled event.\nIn some example embodiments, the proximity module 420 may determine, based on GPS data pertaining to the first mobile device, the time when the first mobile device has left the location of the scheduled event.  If the location of the first\nmobile device is determined to be the same as the location of the scheduled event but the second distance value is determined to have exceeded the proximity threshold value, the identified end time associated with the second distance value between the\nfirst mobile device and the second mobile device may indicate when the second member associated with the second mobile device has left the location of the scheduled event.\nMethod operation 903 may be performed after the method operation 902.  At method operation 903, the proximity module 420 determines a period of time that the distance value between the first mobile device and the second mobile device does not\nexceed the proximity threshold value, the determining of the period of time being based on the start time and the end time.  The period of time between the start time and the end time may indicate the time that the first mobile device and the second\nmobile device were located within a distance value from each other that did not exceed the proximity threshold value.\nAs shown in FIG. 10, the method 500 may include method operation 1001, according to some example embodiments.  Method operation 1001 may be performed after method operation 540, in which the member connecting module 430 generates a communication\nfor the first member.\nAt method operation 1001, the user interface module 440 displays the communication for the first member of the social networking service in a user interface of the first mobile device associated with the first member.  In certain example\nembodiments, the communication for the first member is presented to the first member by the user interface module 440 in audio form.  In certain example embodiments, the communication is presented to the first member during the scheduled event or\nimmediately after the scheduled event has ended.  In other example embodiment, the communication is presented to the first member a period of time after the scheduled event has ended (e.g., the day after the scheduled event, a week after the scheduled\nevent, etc.).\nAs shown in FIG. 11, the method 500 may include method operation 1101, according to some example embodiments.  Method operation 1101 may be performed after method operation 1001, in which the user interface module 440 displays the communication\nfor the first member of the social networking service in the user interface of the first mobile device associated with the first member.\nAt method operation 1101, the member connecting module 430 facilitates an establishing of a social graph connection between the first member and the second member via the social networking service.  In some example embodiments, before the member\nconnecting module 430 facilitates the establishing of the social graph connection, the social networking module 460 determines that the second member is not a first-degree connection of the first member within the first social graph associated with the\nfirst member.  The determining that the second member is not a first-degree connection of the first member may be based on the first social graph associated with the first member.\nTo facilitate the establishing of the social graph connection between the first member and the second member via the social networking service, the member connecting module 430 generates a message for the first member.  The generating of the\nmessage for the first member may be based on the determining that the second member is not a first-degree connection of the first member.  The message may pertain to establishing a connection between the first member and the second member via the social\nnetworking service.  The user interface module 440 may present (e.g., display) the message for the first member via (e.g., in) a user interface of the first mobile device associated with the first member.\nIn some example embodiments, the user interface module 440 receives an input from the first member.  The input, in some instances, may be received via a user interface of the first mobile device.  The input may be audio, visual, textual,\ntactile, etc. The input may indicate a request to establish the connection between the first member and the second member via the social networking service.  In some example embodiments, the input may be received from the first user in response to the\npresentation (e.g., displaying) of a prompt (e.g., a message) for the first member to connect with the second member via the social networking service.\nThe member connecting module 430, in response to receiving the input from the first member, may generate an invitation for the second member to connect with the first member via the social networking service.  The mobile device communication\nmodule 450 may transmit a second communication to the second mobile device associated with the second member.  The second communication may include the invitation for the second member to connect with the first member via the social networking service.\nIn response to the second communication transmitted to the second mobile device, the mobile device communication module 450 may receive a third communication from the second mobile device.  The third communication may include an acceptance by\nthe second member of the invitation for the second member to connect with the first member via the social networking service.\nThe member connecting module 430 may add a new edge between a first node representing the first member in the first social graph associated with the first member and a second node representing the second member in the first social graph\nassociated with the first member.  The adding of the new edge in the first social graph associated with the first member may be based on the acceptance by the second member of the invitation to connect with the first member via the social networking\nservice.  The new edge may indicate that the second member is a new connection (of the first member) in the first social graph associated with the first member.\nSimilarly, at the second mobile device, one or more modules associated with the second mobile device may add another new edge between a third node representing the second member in the second social graph associated with the second member and a\nfourth node representing the first member in the second social graph associated with the second member.  The adding of the other new edge in the second social graph associated with the second member may be based on the acceptance by the second member of\nthe invitation to connect with the first member via the social networking service.  The other new edge may indicate that the first member is a new connection (of the second member) in the second social graph associated with the second member.\nIn some example embodiments, the new social graph connection between the first member and the second member is memorialized in one or more records of one or more databases 492.  For example, data that pertains to the creation of a relationship\nbetween the first member and the second member via the social networking service is stored in one or more records as, for example, social graph data, member activity and behavior data, or profile data.  The data that pertains to the creation of a\nrelationship between the first member and the second member may include a variety of information such as when and where the first and second members met, whether the first and second members met at a scheduled event, the duration of their meeting or of\nthe event, the type of information exchanged (e.g., electronic business cards, chat messages, presentations, etc.) by the first and second members using their mobile devices, whether information pertaining to (e.g., about) the other member was presented\nto a particular member before, during, or after the event, etc.\nAs shown in FIG. 12, the method 500 may include one or more of method operations 1201, 1202, 1203, 1204, and 1205, according to some example embodiments.  Method operation 1201 may be performed as part (e.g., a precursor task, a subroutine, or a\nportion) of method operation 530, in which the proximity module 420 determines that the first member and the second member of the social networking service attended the scheduled event.\nAt method operation 1201, the proximity module 420 determines a first distance value between the first mobile device and the second mobile device based on a first wireless communication received from the second mobile device at a first time.\nMethod operation 1202 may be performed after method operation 1201.  At method operation 1202, the proximity module 420 identifies the first time as a time representing a beginning of the scheduled event based on determining that the first\ndistance value does not exceed a proximity threshold value and based on the calendar data.\nMethod operation 1203 may be performed after method operation 1202.  At method operation 1203, the proximity module 420 determines a second distance value between the first mobile device and the second mobile device based on a second wireless\ncommunication received from the second mobile device at a second time.\nMethod operation 1204 may be performed after method operation 1203.  At method operation 1204, the proximity module 420 identifies the second time as a time representing an end of the scheduled event based on the second distance value exceeding\nthe proximity threshold value and based on the calendar data.\nMethod operation 1205 may be performed as part (e.g., a precursor task, a subroutine, or a portion) of method operation 1001, in which the user interface module 440 displays the communication for the first member of the social networking service\nin the user interface of the first mobile device associated with the first member.  At method operation 1205, the user interface module 440 displays the communication for the first member of the social networking service in the user interface of the\nfirst mobile device based on (e.g., upon) the identifying of the second time as the time representing the end of the scheduled event.\nAs shown in FIG. 13, the method 500 may include one or more of method operations 1301, 1302, and 1303, according to some example embodiments.  Method operation 1301 may be performed after method operation 520, in which the calendar module 410\nidentifies a scheduled event.  At method operation 1301, the calendar module 410 identifies the second member of the social networking service scheduled to attend the event.\nIn some example embodiments, the identifying of the second member of the social networking service scheduled to attend the event is based on the calendar data that pertains to the electronic calendar of the first member.  In various example\nembodiments, the identifying of the second member is based on an electronic communication broadcast by the second mobile device associated with the second member to a local area where the first mobile device is located.  The electronic communication\nbroadcast by the second mobile device may be received at the first mobile device and may serve as basis for identifying the second member of the social networking service.  For example, an identifier of the second member of the social networking service\nincluded in the electronic communication may be used to identify the second member.\nFurthermore, the identifier of the second member may be used to determine that the second member is scheduled to attend the scheduled event.  In some example embodiments, the first and second mobile devices may establish an ad hoc wireless\npeer-to-peer network, and may transmit and receive data (e.g., calendar data that pertains to one or more members of the social networking service) to and from the first and second mobile devices.  In some instances, the calendar module 410 identifies\nthe second member of the social networking service scheduled to attend the event based on the calendar data that pertains to the electronic calendar of the second member.\nMethod operation 1302 may be performed after method operation 1301.  At method operation 1302, the user interface module 440 accesses a digital item of content pertaining to the second member of the social networking service.\nIn some example embodiments, the digital item of content pertaining to the second member is received (e.g., obtained, accessed, etc.) by the mobile device communication module 450 from the second mobile device via the ad hoc wireless\npeer-to-peer network established by the first mobile device and the second mobile device.  Such an ad hoc wireless peer-to-peer network may be established by the first mobile device and the second mobile device during the scheduled event attended by the\nfirst member and the second member.\nIn certain example embodiments, the digital item of content pertaining to the second member is received (e.g., obtained, accessed, etc.) by the server communication module 480 from a server associated with the social networking system 220 (e.g.,\na server of the interaction support system 226).  In some instances, the server selects the item of content that pertains to the second member based on a request for the digital item of content received from the first mobile device.  In some example\nembodiments, the selecting of the item of content that pertains to the second member includes accessing a calendar of the second member, identifying a present context pertaining to the second member based on the calendar of the second member, identifying\ndata relevant to the present context pertaining to the second member, and generating an item of content based on the data relevant to the present context pertaining to the second member.\nMethod operation 1303 may be performed after method operation 1302.  At method operation 1303, the user interface module 440 presents the digital item of content pertaining to the second member via the user interface of the first mobile device.\nAs shown in FIG. 14, the method 500 may include one or more of method operations 1401, 1402, and 1403, according to some example embodiments.  Method operation 1401 may be performed after method operation 520, in which the calendar module 410\nidentifies a scheduled event.  At method operation 1401, the calendar module 410 determines that the second member is scheduled to attend the scheduled event.  The determining that the second member is scheduled to attend the scheduled event may be based\non the calendar data.\nMethod operation 1402 may be performed after method operation 540, in which the member connecting module 430 generates a communication for the first member.  The communication may include a prompt to connect with the second member via the social\nnetworking service.  At method operation 1402, the calendar module 410 generates a data record that confirms that the scheduled event took place, and that the first member and the second member attended the scheduled event.\nMethod operation 1403 may be performed after method operation 1402.  At method operation 1403, the calendar module 410 stores the data record in a database (e.g., the database 492) associated with the first mobile device.\nAs shown in FIG. 15, the method 500 may include one or more of method operations 1501, 1502, and 1503, according to some example embodiments.  In some example embodiments, the communication for the first member is a first communication.  Method\noperation 1501 may be performed after method operation 520, in which the calendar module 410 identifies a scheduled event.  At method operation 1501, the mobile device communication module 450 receives a second communication from a second mobile device\nvia a transceiver of the first mobile device.  The second communication may include a second member identifier of the second member.  The second member identifier may be associated with the second mobile device.  The second communication may be broadcast\nby the second mobile device and may indicate that the second mobile device is available to establish one or more wireless peer-to-peer connections between the second mobile device and one or more other mobile devices.\nMethod operation 1502 may be performed after method operation 1501.  At method operation 1502, the social networking module 460, in response to the receiving of the second communication, determines that the second member is one of a plurality of\nmembers of the social networking service.  The determining that the second member is one of the plurality of members of the social networking service may be based on the second member identifier.\nMethod operation 1503 may be performed after method operation 1502.  At method operation 1503, the mobile device connecting module 470 facilitates an establishing of a wireless peer-to-peer connection with the second mobile device.  The\nfacilitating of the wireless peer-to-peer connection with the second mobile device may be based on the determining that that the second member is one of the plurality of members of the social networking service.  In some example embodiments, the\nfacilitating of the establishing of the wireless peer-to-peer connection is further based on the proximity between the first mobile device and the second mobile device.\nIn some example embodiments, the user interface module 440 displays, in a user interface of the first mobile device, one or more member identifiers to indicate one or more established wireless peer-to-peer connections between the first mobile\ndevice and other mobile devices.  The one or more established wireless peer-to-peer connections may include the wireless peer-to-peer connection with the second mobile device.  The displaying of the second member identifier may represent the wireless\npeer-to-peer connection with the second mobile device.\nIn various example embodiments, the social networking module 460 determines, based on the second member identifier, that the second member is not a first-degree connection of the first member within the social graph associated with the first\nmember.  The social networking module 460 may also determine that a third member identifier associated with a third member of the social networking service and referenced in a third communication received from a third mobile device is a first-degree\nconnection of the first member within the social graph associated with the first member.  The user interface module 440 may indicate, in the user interface of the first mobile device, that the second member of the social networking service is not a\nfirst-degree connection within the social graph associated with the first member.  The user interface module 440 may also indicate, in the user interface of the first mobile device, that the third member of the social networking service is a first-degree\nconnection within the social graph associated with the first member.\nConsistent with various example embodiments, the mobile device communication module 450 receives a third communication from a third mobile device via the transceiver of the first mobile device.  The third communication may include a third member\nidentifier of a third member of the social networking service.  The third member identifier may be associated with the third mobile device.  The third communication may be broadcast by the third mobile device and may indicate that the third mobile device\nis available to establish one or more wireless peer-to-peer connections between the third mobile device and one or more other mobile devices.\nIn response to the receiving of the third communication, the social networking module 460 may determine, based on the third member identifier, that the third member is one of the plurality of members of the social networking service.  The mobile\ndevice connecting module 470 may, in some instances, facilitate the third mobile device joining the wireless peer-to-peer connection established between the first and second mobile devices based on the determining that that the third member is one of the\nplurality of members of the social networking service.  In other instances, the mobile device connecting module 470 may facilitate an establishing of a further wireless peer-to-peer connection with the third mobile device based on the determining that\nthat the third member is one of the plurality of members of the social networking service.\nIn some example embodiments, the user interface module 440 displays a number of member identifiers including the second member identifier and the third member identifier in a user interface of the first mobile device.  The second member\nidentifier and the third member identifier may identify the second mobile device and the third mobile device as being wirelessly connected to the first mobile device via one or more wireless peer-to-peer connections.  The mobile device connecting module\n470 may determine that the second mobile device has interrupted the wireless peer-to-peer connection with the first mobile device.  The user interface module 440 may update the member identifiers displayed in the user interface of the first mobile device\nbased on the determining that the second mobile device has interrupted the wireless peer-to-peer connection with the first mobile device.\nExample Mobile Device\nFIG. 16 is a block diagram illustrating a mobile device 1600, according to an example embodiment.  The mobile device 1600 may include a processor 1602.  The processor 1602 may be any of a variety of different types of commercially available\nprocessors 1602 suitable for mobile devices 1600 (for example, an XScale architecture microprocessor, a microprocessor without interlocked pipeline stages (MIPS) architecture processor, or another type of processor 1602).  A memory 1604, such as a random\naccess memory (RAM), a flash memory, or other type of memory, is typically accessible to the processor 1602.  The memory 1604 may be adapted to store an operating system (OS) 1606, as well as application programs 1608, such as a mobile location enabled\napplication that may provide LBSs to a user.  The processor 1602 may be coupled, either directly or via appropriate intermediary hardware, to a display 1610 and to one or more input/output (I/O) devices 1612, such as a keypad, a touch panel sensor, a\nmicrophone, and the like.  Similarly, in some embodiments, the processor 1602 may be coupled to a transceiver 1614 that interfaces with an antenna 1616.  The transceiver 1614 may be configured to both transmit and receive cellular network signals,\nwireless data signals, or other types of signals via the antenna 1616, depending on the nature of the mobile device 1600.  Further, in some configurations, a GPS receiver 1618 may also make use of the antenna 1616 to receive GPS signals.\nModules, Components and Logic\nCertain embodiments are described herein as including logic or a number of components, modules, or mechanisms.  Modules may constitute either software modules (e.g., code embodied (1) on a non-transitory machine-readable medium or (2) in a\ntransmission signal) or hardware-implemented modules.  A hardware-implemented module is a tangible unit capable of performing certain operations and may be configured or arranged in a certain manner.  In example embodiments, one or more computer systems\n(e.g., a standalone, client or server computer system) or one or more processors may be configured by software (e.g., an application or application portion) as a hardware-implemented module that operates to perform certain operations as described herein.\nIn various embodiments, a hardware-implemented module may be implemented mechanically or electronically.  For example, a hardware-implemented module may comprise dedicated circuitry or logic that is permanently configured (e.g., as a\nspecial-purpose processor, such as a field programmable gate array (FPGA) or an application-specific integrated circuit (ASIC)) to perform certain operations.  A hardware-implemented module may also comprise programmable logic or circuitry (e.g., as\nencompassed within a general-purpose processor or other programmable processor) that is temporarily configured by software to perform certain operations.  It will be appreciated that the decision to implement a hardware-implemented module mechanically,\nin dedicated and permanently configured circuitry, or in temporarily configured circuitry (e.g., configured by software) may be driven by cost and time considerations.\nAccordingly, the term \"hardware-implemented module\" should be understood to encompass a tangible entity, be that an entity that is physically constructed, permanently configured (e.g., hardwired) or temporarily or transitorily configured (e.g.,\nprogrammed) to operate in a certain manner and/or to perform certain operations described herein.  Considering embodiments in which hardware-implemented modules are temporarily configured (e.g., programmed), each of the hardware-implemented modules need\nnot be configured or instantiated at any one instance in time.  For example, where the hardware-implemented modules comprise a general-purpose processor configured using software, the general-purpose processor may be configured as respective different\nhardware-implemented modules at different times.  Software may accordingly configure a processor, for example, to constitute a particular hardware-implemented module at one instance of time and to constitute a different hardware-implemented module at a\ndifferent instance of time.\nHardware-implemented modules can provide information to, and receive information from, other hardware-implemented modules.  Accordingly, the described hardware-implemented modules may be regarded as being communicatively coupled.  Where multiple\nof such hardware-implemented modules exist contemporaneously, communications may be achieved through signal transmission (e.g., over appropriate circuits and buses that connect the hardware-implemented modules).  In embodiments in which multiple\nhardware-implemented modules are configured or instantiated at different times, communications between such hardware-implemented modules may be achieved, for example, through the storage and retrieval of information in memory structures to which the\nmultiple hardware-implemented modules have access.  For example, one hardware-implemented module may perform an operation, and store the output of that operation in a memory device to which it is communicatively coupled.  A further hardware-implemented\nmodule may then, at a later time, access the memory device to retrieve and process the stored output.  Hardware-implemented modules may also initiate communications with input or output devices, and can operate on a resource (e.g., a collection of\ninformation).\nThe various operations of example methods described herein may be performed, at least partially, by one or more processors that are temporarily configured (e.g., by software) or permanently configured to perform the relevant operations.  Whether\ntemporarily or permanently configured, such processors may constitute processor-implemented modules that operate to perform one or more operations or functions.  The modules referred to herein may, in some example embodiments, comprise\nprocessor-implemented modules.\nSimilarly, the methods described herein may be at least partially processor-implemented.  For example, at least some of the operations of a method may be performed by one or more processors or processor-implemented modules.  The performance of\ncertain of the operations may be distributed among the one or more processors or processor-implemented modules, not only residing within a single machine, but deployed across a number of machines.  In some example embodiments, the one or more processors\nor processor-implemented modules may be located in a single location (e.g., within a home environment, an office environment or as a server farm), while in other embodiments the one or more processors or processor-implemented modules may be distributed\nacross a number of locations.\nThe one or more processors may also operate to support performance of the relevant operations in a \"cloud computing\" environment or as a \"software as a service\" (SaaS).  For example, at least some of the operations may be performed by a group of\ncomputers (as examples of machines including processors), these operations being accessible via a network (e.g., the Internet) and via one or more appropriate interfaces (e.g., application program interfaces (APIs).)\nElectronic Apparatus and System\nExample embodiments may be implemented in digital electronic circuitry, or in computer hardware, firmware, software, or in combinations of them.  Example embodiments may be implemented using a computer program product, e.g., a computer program\ntangibly embodied in an information carrier, e.g., in a machine-readable medium for execution by, or to control the operation of, data processing apparatus, e.g., a programmable processor, a computer, or multiple computers.\nA computer program can be written in any form of programming language, including compiled or interpreted languages, and it can be deployed in any form, including as a stand-alone program or as a module, subroutine, or other unit suitable for use\nin a computing environment.  A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.\nIn example embodiments, operations may be performed by one or more programmable processors executing a computer program to perform functions by operating on input data and generating output.  Method operations can also be performed by, and\napparatus of example embodiments may be implemented as, special purpose logic circuitry, e.g., a field programmable gate array (FPGA) or an application-specific integrated circuit (ASIC).\nThe computing system can include clients and servers.  A client and server are generally remote from each other and typically interact through a communication network.  The relationship of client and server arises by virtue of computer programs\nrunning on the respective computers and having a client-server relationship to each other.  In embodiments deploying a programmable computing system, it will be appreciated that that both hardware and software architectures require consideration. \nSpecifically, it will be appreciated that the choice of whether to implement certain functionality in permanently configured hardware (e.g., an ASIC), in temporarily configured hardware (e.g., a combination of software and a programmable processor), or a\ncombination of permanently and temporarily configured hardware may be a design choice.  Below are set out hardware (e.g., machine) and software architectures that may be deployed, in various example embodiments.\nExample Machine Architecture and Machine-Readable Medium\nFIG. 17 is a block diagram illustrating components of a machine 1700, according to some example embodiments, able to read instructions 1724 from a machine-readable medium 1722 (e.g., a non-transitory machine-readable medium, a machine-readable\nstorage medium, a computer-readable storage medium, or any suitable combination thereof) and perform any one or more of the methodologies discussed herein, in whole or in part.  Specifically, FIG. 17 shows the machine 1700 in the example form of a\ncomputer system (e.g., a computer) within which the instructions 1724 (e.g., software, a program, an application, an applet, an app, or other executable code) for causing the machine 1700 to perform any one or more of the methodologies discussed herein\nmay be executed, in whole or in part.\nIn alternative embodiments, the machine 1700 operates as a standalone device or may be connected (e.g., networked) to other machines.  In a networked deployment, the machine 1700 may operate in the capacity of a server machine or a client\nmachine in a server-client network environment, or as a peer machine in a distributed (e.g., peer-to-peer) network environment.  The machine 1700 may be a server computer, a client computer, a personal computer (PC), a tablet computer, a laptop computer,\na netbook, a cellular telephone, a smartphone, a set-top box (STB), a personal digital assistant (PDA), a web appliance, a network router, a network switch, a network bridge, or any machine capable of executing the instructions 1724, sequentially or\notherwise, that specify actions to be taken by that machine.  Further, while only a single machine is illustrated, the term \"machine\" shall also be taken to include any collection of machines that individually or jointly execute the instructions 1724 to\nperform all or part of any one or more of the methodologies discussed herein.\nThe machine 1700 includes a processor 1702 (e.g., a central processing unit (CPU), a graphics processing unit (GPU), a digital signal processor (DSP), an application specific integrated circuit (ASIC), a radio-frequency integrated circuit\n(RFIC), or any suitable combination thereof), a main memory 1704, and a static memory 1706, which are configured to communicate with each other via a bus 1708.  The processor 1702 may contain microcircuits that are configurable, temporarily or\npermanently, by some or all of the instructions 1724 such that the processor 1702 is configurable to perform any one or more of the methodologies described herein, in whole or in part.  For example, a set of one or more microcircuits of the processor\n1702 may be configurable to execute one or more modules (e.g., software modules) described herein.\nThe machine 1700 may further include a graphics display 1710 (e.g., a plasma display panel (PDP), a light emitting diode (LED) display, a liquid crystal display (LCD), a projector, a cathode ray tube (CRT), or any other display capable of\ndisplaying graphics or video).  The machine 1700 may also include an alphanumeric input device 1712 (e.g., a keyboard or keypad), a cursor control device 1714 (e.g., a mouse, a touchpad, a trackball, a joystick, a motion sensor, an eye tracking device,\nor other pointing instrument), a storage unit 1716, an audio generation device 1718 (e.g., a sound card, an amplifier, a speaker, a headphone jack, or any suitable combination thereof), and a network interface device 1720.\nThe storage unit 1716 includes the machine-readable medium 1722 (e.g., a tangible and non-transitory machine-readable storage medium) on which are stored the instructions 1724 embodying any one or more of the methodologies or functions described\nherein.  The instructions 1724 may also reside, completely or at least partially, within the main memory 1704, within the processor 1702 (e.g., within the processor's cache memory), or both, before or during execution thereof by the machine 1700. \nAccordingly, the main memory 1704 and the processor 1702 may be considered machine-readable media (e.g., tangible and non-transitory machine-readable media).  The instructions 1724 may be transmitted or received over the network 1726 via the network\ninterface device 1720.  For example, the network interface device 1720 may communicate the instructions 1724 using any one or more transfer protocols (e.g., hypertext transfer protocol (HTTP)).\nIn some example embodiments, the machine 1700 may be a portable computing device, such as a smart phone or tablet computer, and have one or more additional input components 1730 (e.g., sensors or gauges).  Examples of such input components 1730\ninclude an image input component (e.g., one or more cameras), an audio input component (e.g., a microphone), a direction input component (e.g., a compass), a location input component (e.g., a global positioning system (GPS) receiver), an orientation\ncomponent (e.g., a gyroscope), a motion detection component (e.g., one or more accelerometers), an altitude detection component (e.g., an altimeter), and a gas detection component (e.g., a gas sensor).  Inputs harvested by any one or more of these input\ncomponents may be accessible and available for use by any of the modules described herein.\nAs used herein, the term \"memory\" refers to a machine-readable medium able to store data temporarily or permanently and may be taken to include, but not be limited to, random-access memory (RAM), read-only memory (ROM), buffer memory, flash\nmemory, and cache memory.  While the machine-readable medium 1722 is shown in an example embodiment to be a single medium, the term \"machine-readable medium\" should be taken to include a single medium or multiple media (e.g., a centralized or distributed\ndatabase, or associated caches and servers) able to store instructions.  The term \"machine-readable medium\" shall also be taken to include any medium, or combination of multiple media, that is capable of storing the instructions 1724 for execution by the\nmachine 1700, such that the instructions 1724, when executed by one or more processors of the machine 1700 (e.g., processor 1702), cause the machine 1700 to perform any one or more of the methodologies described herein, in whole or in part.  Accordingly,\na \"machine-readable medium\" refers to a single storage apparatus or device, as well as cloud-based storage systems or storage networks that include multiple storage apparatus or devices.  The term \"machine-readable medium\" shall accordingly be taken to\ninclude, but not be limited to, one or more tangible (e.g., non-transitory) data repositories in the form of a solid-state memory, an optical medium, a magnetic medium, or any suitable combination thereof.\nThroughout this specification, plural instances may implement components, operations, or structures described as a single instance.  Although individual operations of one or more methods are illustrated and described as separate operations, one\nor more of the individual operations may be performed concurrently, and nothing requires that the operations be performed in the order illustrated.  Structures and functionality presented as separate components in example configurations may be\nimplemented as a combined structure or component.  Similarly, structures and functionality presented as a single component may be implemented as separate components.  These and other variations, modifications, additions, and improvements fall within the\nscope of the subject matter herein.\nCertain embodiments are described herein as including logic or a number of components, modules, or mechanisms.  Modules may constitute software modules (e.g., code stored or otherwise embodied on a machine-readable medium or in a transmission\nmedium), hardware modules, or any suitable combination thereof.  A \"hardware module\" is a tangible (e.g., non-transitory) unit capable of performing certain operations and may be configured or arranged in a certain physical manner.  In various example\nembodiments, one or more computer systems (e.g., a standalone computer system, a client computer system, or a server computer system) or one or more hardware modules of a computer system (e.g., a processor or a group of processors) may be configured by\nsoftware (e.g., an application or application portion) as a hardware module that operates to perform certain operations as described herein.\nIn some embodiments, a hardware module may be implemented mechanically, electronically, or any suitable combination thereof.  For example, a hardware module may include dedicated circuitry or logic that is permanently configured to perform\ncertain operations.  For example, a hardware module may be a special-purpose processor, such as a field programmable gate array (FPGA) or an ASIC.  A hardware module may also include programmable logic or circuitry that is temporarily configured by\nsoftware to perform certain operations.  For example, a hardware module may include software encompassed within a general-purpose processor or other programmable processor.  It will be appreciated that the decision to implement a hardware module\nmechanically, in dedicated and permanently configured circuitry, or in temporarily configured circuitry (e.g., configured by software) may be driven by cost and time considerations.\nAccordingly, the phrase \"hardware module\" should be understood to encompass a tangible entity, and such a tangible entity may be physically constructed, permanently configured (e.g., hardwired), or temporarily configured (e.g., programmed) to\noperate in a certain manner or to perform certain operations described herein.  As used herein, \"hardware-implemented module\" refers to a hardware module.  Considering embodiments in which hardware modules are temporarily configured (e.g., programmed),\neach of the hardware modules need not be configured or instantiated at any one instance in time.  For example, where a hardware module comprises a general-purpose processor configured by software to become a special-purpose processor, the general-purpose\nprocessor may be configured as respectively different special-purpose processors (e.g., comprising different hardware modules) at different times.  Software (e.g., a software module) may accordingly configure one or more processors, for example, to\nconstitute a particular hardware module at one instance of time and to constitute a different hardware module at a different instance of time.\nHardware modules can provide information to, and receive information from, other hardware modules.  Accordingly, the described hardware modules may be regarded as being communicatively coupled.  Where multiple hardware modules exist\ncontemporaneously, communications may be achieved through signal transmission (e.g., over appropriate circuits and buses) between or among two or more of the hardware modules.  In embodiments in which multiple hardware modules are configured or\ninstantiated at different times, communications between such hardware modules may be achieved, for example, through the storage and retrieval of information in memory structures to which the multiple hardware modules have access.  For example, one\nhardware module may perform an operation and store the output of that operation in a memory device to which it is communicatively coupled.  A further hardware module may then, at a later time, access the memory device to retrieve and process the stored\noutput.  Hardware modules may also initiate communications with input or output devices, and can operate on a resource (e.g., a collection of information).\nThe performance of certain operations may be distributed among the one or more processors, not only residing within a single machine, but deployed across a number of machines.  In some example embodiments, the one or more processors or\nprocessor-implemented modules may be located in a single geographic location (e.g., within a home environment, an office environment, or a server farm).  In other example embodiments, the one or more processors or processor-implemented modules may be\ndistributed across a number of geographic locations.\nSome portions of the subject matter discussed herein may be presented in terms of algorithms or symbolic representations of operations on data stored as bits or binary digital signals within a machine memory (e.g., a computer memory).  Such\nalgorithms or symbolic representations are examples of techniques used by those of ordinary skill in the data processing arts to convey the substance of their work to others skilled in the art.  As used herein, an \"algorithm\" is a self-consistent\nsequence of operations or similar processing leading to a desired result.  In this context, algorithms and operations involve physical manipulation of physical quantities.  Typically, but not necessarily, such quantities may take the form of electrical,\nmagnetic, or optical signals capable of being stored, accessed, transferred, combined, compared, or otherwise manipulated by a machine.  It is convenient at times, principally for reasons of common usage, to refer to such signals using words such as\n\"data,\" \"content,\" \"bits,\" \"values,\" \"elements,\" \"symbols,\" \"characters,\" \"terms,\" \"numbers,\" \"numerals,\" or the like.  These words, however, are merely convenient labels and are to be associated with appropriate physical quantities.\nUnless specifically stated otherwise, discussions herein using words such as \"processing,\" \"computing,\" \"calculating,\" \"determining,\" \"presenting,\" \"displaying,\" or the like may refer to actions or processes of a machine (e.g., a computer) that\nmanipulates or transforms data represented as physical (e.g., electronic, magnetic, or optical) quantities within one or more memories (e.g., volatile memory, non-volatile memory, or any suitable combination thereof), registers, or other machine\ncomponents that receive, store, transmit, or display information.  Furthermore, unless specifically stated otherwise, the terms \"a\" or \"an\" are herein used, as is common in patent documents, to include one or more than one instance.  Finally, as used\nherein, the conjunction \"or\" refers to a non-exclusive \"or,\" unless specifically stated otherwise.", "application_number": "14530490", "abstract": " A first mobile device associated with a first member of a social\n     networking service may be configured to facilitate social networking\n     service connections based on calendar data validated by the first mobile\n     device. For example, the first mobile device accesses calendar data that\n     pertains to an electronic calendar of the first member. The first mobile\n     device identifies a scheduled event based on the calendar data. The first\n     mobile device determines that the first member and a second member of the\n     social networking service attended the scheduled event based on a\n     proximity between the first mobile device and a second mobile device\n     associated with the second member. The first mobile device generates a\n     communication for the first member that includes a prompt to connect with\n     the second member via the social networking service, based on the\n     determining that the first member and the second member attended the\n     scheduled event.\n", "citations": ["9118724", "9572000", "20030050977", "20100138481", "20110125850", "20120238285", "20130166654", "20130185654", "20140189785", "20140310366", "20150178691", "20160095140"], "related": ["62057937"]}, {"id": "20160125491", "patent_code": "10373225", "patent_name": "Method and apparatus for facilitating purchase transactions associated\n     with a showroom", "year": "2019", "inventor_and_country_data": " Inventors: \nZellner; Sam (Atlanta, GA), Mikan; Jeffrey (Atlanta, GA), Pedro; Jeff (Alpharetta, GA)  ", "description": "<BR><BR>FIELD OF THE DISCLOSURE\nThe subject disclosure relates to a method and apparatus for facilitating purchase transactions associated with a showroom.\n<BR><BR>BACKGROUND\nPurchases can be made online or can be made in a store or showroom.  Often, users desire to see the product in person before making a purchase.  AN inability to see the product can create a lack of confidence in understanding the product during\nthe online purchasing experience.  Further, users often desire to review information regarding the product at a time of purchase which can add to the lack of confidence in understanding the product during the showroom purchasing experience. <BR><BR>BRIEF\nDESCRIPTION OF THE DRAWINGS\nReference will now be made to the accompanying drawings, which are not necessarily drawn to scale, and wherein:\nFIG. 1 depicts an illustrative embodiment of a system that guides purchases according to obtaining user data;\nFIG. 2 depicts an illustrative embodiment of data flow for the system of FIG. 3;\nFIG. 3 depicts another illustrative embodiment of a system that guides purchases according to obtaining user data;\nFIG. 4 depicts another illustrative embodiment of data flow for the system of FIG. 3;\nFIG. 5 depicts an illustrative embodiment of a method used in portions of the systems described in FIGS. 1 and 3;\nFIG. 6 depicts an illustrative embodiment of a communication system that provides media services and facilitates product and service purchases;\nFIG. 7 depicts an illustrative embodiment of a communication device; and\nFIG. 8 is a diagrammatic representation of a machine in the form of a computer system within which a set of instructions, when executed, may cause the machine to perform any one or more of the methods described herein.\n<BR><BR>DETAILED DESCRIPTION\nThe subject disclosure describes, among other things, illustrative embodiments for facilitating or otherwise guiding a purchase transaction.  The exemplary embodiments provide for accessing and synthesizing data associated with a user and then\nguiding interactions with that user according to the data synthesis to facilitate a purchase of a product.  The purchase can occur in a showroom or can occur elsewhere such as at a merchant's website.\nOne or more aspects of the subject disclosure include determining whether a user in a showroom is seeking to purchase a product (i.e., showrooming), obtaining user data if showrooming is occurring, and analyzing the user data to identify or\notherwise select interaction data that can be used to facilitate the purchase transaction.  In one or more embodiments, the interaction data can be provided to a sales agent (e.g., a human or a virtual agent) so that the sales agent can interact with the\nuser as to a price of a product, alternative products, accessories for the product, bundling of services (and/or other products) for the product.  In one or more embodiments, the interaction data can be provided to an online merchant to enable the online\nmerchant to complete a purchase transaction and share revenue with the showroom based on the purchase.  In one or more embodiments, monitoring of the user in the showroom can be performed, including location tracking, eye-tracking, and/or device activity\ntracking, to enable a determination as to whether the user is seeking to purchase a particular product.\nIn one or more embodiments, a showroom engine, which can be a network device or can be a local device at a showroom, can be used for obtaining user data and facilitating product purchases based on the user data.  The showroom engine can be\nconfigured into various functional areas such as inputs (e.g., data collection), processing (e.g., arriving and conclusions) and outputs (e.g., broadcasting conclusions).  In one embodiment, as inputs into the showroom engine, the engine can collect user\ndata relevant to the purchasing decision.  The showroom engine can leverage sensor technologies to discern the likelihood the user is showrooming, such as based on the user's geo-location within the showroom, eye tracking, idleness, and/or active-state\nof the user's smartphone device.  In one embodiment, once the showroom engine discerns the user satisfies a defined threshold as a purchase target (e.g., based on an amount of device activity, location in showroom, eye-tracking correlating to a\nparticular product, and so forth), additional data collection can be performed, such as attributes of the user, what the user is looking at, available stock of the product being considered, and/or other active functions on the smartphone.\nIn one or more embodiments, the showroom engine can process the data collected and arrive at one or more conclusions, which form or otherwise guide interactions with the user.  Business rules driven by combinations of data points can inform\nopportunities to: suggest similar products, suggest different products, suggest accessories to products, arrive at attractive pricing, and/or suggest bundling offers.  The showroom engine can communicate with devices within the showroom in order to\nprovide its conclusions regarding how to interact with the particular user to facilitate a product purchase in the showroom.  These interactions can manifest in various forms including: directing human resource support to the user by providing messages\nto the support staff that includes interaction guidance such as pricing, user background, alternative products, bundling offers, and so forth.  Multimedia and signage content adjustments can be tailored to the user.  On-demand product fabrication can be\nperformed.  The product sale can be completed with the assistance of data obtained by the showroom engine, such as user identification, billing information, delivery information, and so forth.\nIn one embodiment, if a user is reviewing a product in the showroom, but later purchases the product online, the showroom engine can communicate with the online merchant to facilitate the transaction.  For instance, metadata about the showroom\nexperience of the user can be provided to the online merchant so that a compensation for providing the tactile experience may be rendered.\nThe exemplary embodiments described herein can include one or more components and/or one or more steps described with respect to U.S.  application Ser.  No. 14/529,630 entitled \"Method and Apparatus for Managing Purchase Transactions\" with the\ndisclosure of which is hereby incorporated by reference.\nOne embodiment of the subject disclosure includes monitoring, by a system including a processor, a user in a showroom to generate shopping parameters of the user.  The system can determine that the user is seeking to purchase a product in the\nshowroom according to the shopping parameters.  The system can obtain user data of the user in response to the determining that the user is seeking to purchase the product.  The system can analyze the user data and can select interaction data from among\na group of interaction data according to the analyzing of the user data.  The system can provide the interaction data to a computing device of a sales agent to enable the sales agent to transact with the user for the purchase of the product.\nOne embodiment of the subject disclosure includes a device having a processor and a memory that stores executable instructions that, when executed by the processor, facilitate performance of operations, including determining that a user is\nseeking to purchase a product in a showroom according to monitoring of the user in the showroom.  The processor can obtain user data of the user in response to the determining that the user is seeking to purchase the product.  The processor can analyze\nthe user data and can select interaction data from among a group of interaction data according to the analyzing of the user data.  The processor can determine that the user has not purchased the product in the showroom and can provide the interaction\ndata to a computing device of a sales agent to enable the sales agent to transact with the user for the purchase of the product.  The sales agent can be an online entity that sells the product, and the providing of the interaction data to the computing\ndevice of the sales agent can be responsive to the determining that the user has not purchased the product in the showroom.\nOne embodiment of the subject disclosure includes a machine-readable storage device, comprising executable instructions that, when executed by a processor, facilitate performance of operations, including determining that a user is seeking to\npurchase a product in a showroom according to monitoring of the user in the showroom.  The processor can obtain user data of the user in response to the determining that the user is seeking to purchase the product.  The processor can analyze the user\ndata and can select interaction data from among a group of interaction data according to the analyzing of the user data.  The processor can provide the interaction data to a display device of the showroom to cause the display device to present an\nadvertisement associated with the interaction data.\nFIG. 1 depicts an illustrative embodiment of a system 100 that includes a showroom 102.  Showroom 102 can be any location that allows users (one of which is illustrated as user 105) to view or otherwise access products 125, such as a store, a\ncar dealership, and so forth.  Showroom 102 is illustrated as a physical show in FIG. 1, however, in one or more embodiments, the showroom can be located in a virtual environment, such as a virtual store located in a virtual world.\nThe type of product 125 can vary including electronic products (e.g., a mobile phone, a tablet, a television, software, a home appliance, and so forth) and non-electronic products (e.g., tools, furniture, consumer goods, perishable items\nincluding food, and so forth).  Products 125 can also include service(s) that can be purchased with or without purchasing a physical product.\nSystem 100 can include a server 130 associated with the showroom 102.  The server 130 can be located at the showroom 102 or can be located elsewhere, such as in a distributed hardware environment.  A single server 130 is illustrated but any\nnumber of servers can be utilized in association with any number of showrooms 102.  In one or more embodiments, the server 130 can perform a number of functions to facilitate purchasing of product(s) by the user 105.  For instance, the server 130 can\ndetermine whether a user is seeking (e.g., has an interest) to purchase a product 125 in the showroom 102.  For example, the server 130 can monitor the showroom 102, including one or more of identifying a location of the user 105 in the showroom such as\nin proximity to a particular product 125.  The monitoring can also include detecting an activity state of a communication device 116 of the user 105.  In one embodiment, the server 130 can determine particular activities occurring at the communication\ndevice 116 that is indicative of not seeking to purchase a product in the showroom 102, such as an active gaming session of the communication device 116 which would indicate that the user is not looking at products but is rather playing a video game.  In\nanother embodiment, other activities can be detected (e.g., via polling from the server 130 to the communication device 116) that are indicative of seeking to purchase a product 125, such as a communication session of the communication device 116 with a\ncontent source (e.g., a content download) that is related to the product, including viewing a consumer reports website for a particular product 125 that the user is in proximity to in the showroom 102.  For example, the server 130 can request and obtain\na list of recently accessed websites from the communication device 116.  More specific information can also be requested by the server 130 of the communication device 116, including recent searches via a search engine accessed by the communication\ndevice.\nIn another embodiment, the server 130 can perform eye-tracking of the user 105 to determine whether the user is seeking to purchase a particular product 125.  The eye-tracking can be performed using various techniques including capturing images\nof the eye of the user 105 and performing image pattern recognition on the images to determine a viewing angle of the user.  The server 130 can be coupled with or in communication with various other components for monitoring the user 105 in the showroom\n102, such as video cameras.\nIn one embodiment, the eye-tracking can be an optical method for measuring eye motion where light, such as infrared, is reflected from the eye and sensed by a video camera or another optical sensor.  Eye rotation can be determined from changes\nin the reflections.  For example, a video-based eye tracker can be used based on corneal reflection and the center of the pupil being tracked over time.  Other eye-trackers can also be utilized such as a dual-Purkinje eye tracker, which uses reflections\nfrom the front of the cornea and the back of the lens.\nIn one embodiment, the server 130 can obtain user data of the user 105 in response to determining that the user is seeking to purchase the product (e.g., based on the techniques described herein including showroom location, eye-tracking, device\nactivity, and so forth).  The obtaining of the user data can include identifying the user 105, such as based on communications with the communication device 116 of the user, image pattern recognition of images captured of the user and/or other\nidentification techniques.  The user data can be of various types including user preferences (e.g., according to user input such as in a user profile that is accessed by the server 130 via a request provided to the communication device 116), a\ntransaction history of the user (such as obtained from records of the showroom, records of other showrooms, or from other sales sources), a media consumption history of the user (such as obtained from a service provider of the user), demographics of the\nuser, or a combination thereof.  The obtaining of the user data can be performed in conjunction with authorization received from the user, such as a user that permits the server 130 to obtain this data whenever the user is seeking to purchase a product\n125 at the showroom 102.  For example, the communication device 116 can present a permission request according to a message sent from the server 130 when the server is seeking to access user data of the user.  In one embodiment, the user data can be\nobtained over a network 132 from various sources including a database 135.  The sources can be managed by the same entity managing the server 130 and/or can be managed by other entities that are different from the entity managing the server 130.\nIn one embodiment, the server 130 can analyze the user data and can select interaction data from among a group of interaction data according to the analysis.  The interaction data can be information that guides interaction with the user,\nincluding interaction between the user and human sales agents, interaction between the user and other computing devices, and/or interaction between the user and an online merchant website.  The group of interaction data can be stored locally by the\nserver 130 or remotely, such as at database 135.  The interaction data can be a customized guide for facilitating a sale to the user 105.\nIn one embodiment, the interaction data can be information that is provided to a sales agent at the showroom 102, such as transmitting the information from the server 130 to a communication device of the sales agent.  The information can be of\nvarious types, such as one or more of suggested pricing for the product 125, alternative products that are similar or dissimilar to the product 125, an accessory that can be used with the product, and other services (which may or may not be provided by\nthe manufacturer of the product) that can be bundled or otherwise utilized with the product.  As an example, the product 125 can be a television and the interaction data can include a suggested price for the television, as well as On-Demand Services that\ncan be purchased with the television.\nIn one embodiment, the product 125 can be adjusted according to the interaction data.  For example, the interaction data can indicate that the user has a preference for particular software on a mobile phone.  The product 125 can be adjusted\n(e.g., automatically by the server 130 without user intervention via adjustment signals sent from the server to a communications receiver of the product and/or by the sales agent or another individual) so that it includes the particular software to allow\nthe user 105 to see how the software functions on the product while in the showroom 102.  The adjustment of the product 125 is not limited to software and can include adjusting physical features of the product 125 according to the interaction data, such\nas a sales agent providing a heavy-duty case for an electronic product where the interaction data indicates that the user utilizes the electronic product in rough environments.\nIn one embodiment, signs or advertisements can be adjusted according to the interaction data.  For example, a display device 108 of the showroom 102 (such as a display in proximity to a detected location of the user 105) can present an\nadvertisement associated with the interaction data, including information about the product 125, information about alternative products, reviews of the product, and so forth.  This presentation by the display device can be based on the server 130\nproviding the interaction data to the display device 108.  In one embodiment, the showroom 102 can have multiple display devices 108 and the server 130 can customize presentations at each of the display devices according to users in proximity to the\nparticular display device.  In one embodiment, the server 130 can detect an activity state of the communication device 116 via polling of the communication device or other monitoring techniques whereby the user 105 is viewing a negative consumer review\nof the product 125.  The interaction data can cause the display device 108 to present a positive consumer review of the product 125 or provide other information that shows why the negative consumer review is inaccurate or incomplete.\nIn one embodiment, the server 130 can determine that the user 105 has not purchased the product 125 in the showroom 102.  The interaction data can be provided to a computing device 150 of an online entity (which may be a different entity than\nmanages the server 130 or can be the same entity) to enable or otherwise facilitate transacting with the user 105 for the purchase of the product 125.  In one embodiment, the server 130 can determine that the user 105 has purchased the product 125 from\nthe online entity and can engage in revenue sharing with the online entity according to a purchase transaction for the product.  In this example, the showroom 102 can be managed by a first entity that is different from the online entity.  In one\nembodiment, the server 130 can provide metadata to the online entity which describes an interaction of the user 105 at the showroom 102 associated with the product 125.  The described interaction can include a description of accessories that were viewed,\nalternative products that were viewed, service bundles that were viewed, suggested prices that were discussed, and so forth.\nReferring to FIGS. 2-4, data flows 200 and 400 are illustrated for a system 300.  System 300 can include one or more features of system 100.  A showroom engine 210 is illustrated that can provide knowledge, interaction to social networks, and/or\nan interface to bots.  The bots can be executed on one or more devices of the user or other users and can be used for providing the showroom engine 210 with user data that can be analyzed to facilitate the purchase transaction by the user of a product.\nSystem 300 provides for the showroom engine 210 detecting when a user in a showroom is seeking to purchase a product, obtaining user data in response to the detection, analyzing the user data, and selecting interaction data based on the analysis\nwhich can assist in completing a purchase transaction of a product.  The interaction data can be of various forms such as a selection of a particular advertisement to be presented by a display device in the showroom that is in proximity to the user to\nfurther encourage the user to purchase a product.  The interaction data can be a history of negotiations with the user which is then forwarded to an online merchant so that the online merchant can later complete a sale of the product with the user.  The\ninteraction data can be useful information concerning the product, which is helpful to the user in determining whether to complete the purchase transaction.\nThe showroom engine 210 can include numerous outputs which enable merchandising execution of products available for purchase based upon users in the store, solving for immediacy--executing the sale right then, reduction in investment,\ncapitalizing on opportunity, and so forth.  User hesitation can be addressed by the showroom engine 210 utilizing immediate access to product knowledge, cost of product, and/or approval from a party (e.g., social approval indicated by reviews or likes in\na social network website).\nThe showroom engine 210 can provide value to the retailer and to the user by taking inputs and generating relevant information to facilitate a product purchase.  The showroom engine 210 can identify that someone is showrooming in the store, can\ndetermine what product the user is looking for, and can perform a broadcast relevant to that product to provide more information and to encourage the purchase.  The showroom engine 210 can be portable to brick & mortar stores and online shopping\nexperiences.\nIn one embodiment, the showroom engine 210 can obtain attributes of a user, determine what the user is looking at (what and where), determine what the store has available to sell the user, determine idle time while walking aisle of products\n(e.g., utilized as a showrooming predictor), determine if user is active on a communication device (e.g., a smart phone), determine what activities are being executed on the communication device.\nThe showroom engine 210 can predict showrooming from inputs, profile the user, identify the needs or interests of the user, determine a sales strategy to meet or otherwise satisfy the needs/interests and user profile, determine products to\npitch, determine how to pitch the products (e.g., target devices such as mobile devices, static advertisement, utilize a human sales agent, provide user with pith via a Bluetooth.RTM.  or other short-range wireless headset).  Based on the various\nobtained information, the showroom engine 210 can determine product pricing to offer to the user.  The showroom engine 210 can also utilize the various obtained information to determine products which can be bundled, including services to be bundled with\nproducts.\nIn one or more embodiments, the showroom engine 210 can provide guidance or direction to sales support (e.g., human agents or virtual agents), including suggested prices, suggested accessories, alternative products, bundling or\nproducts/services, and so forth.  The showroom engine 210 can enable delivering a sales offer tailored to the user.\nFor example, media can be leveraged for customizing to the user, including updating signage (e.g., a display device or an electronic display) according to the sales offer.  In one embodiment, a buy decision on behalf of the user can be detected\nor otherwise determined.  If the purchase is from an online merchant, the showroom 102 can share in revenue of the purchase.  If the purchase is from the showroom 102, then the online merchant (which provided online user service) can share in the revenue\nof the purchase.  In one embodiment, the user can add the product to a wish list.  In another embodiment, information sharing between the showroom 102 and the online merchant can be performed, including sharing product data, social network data, ranking\ndata, and so forth.  The showroom engine 210 can enable on-demand product production or fabrication which is customized to the user, such as while the user is in the showroom 102.\nShowroom engine 210 can detect a user that is showrooming (e.g., seeking to purchase a product or interested in a product).  The showroom engine 210 can predict or otherwise determine which product or products the user is interested in. The\nshowroom engine 210 can make recommendations regarding the product, accessories for the product, alternative products, services associated with the product, bundles or products or services that are available, and so forth.  These recommendations can be\nprovided to a human sales agent and/or a virtual sales agent to guide interactions with the user.  The user can purchase the product at the showroom or can purchase the product elsewhere (e.g., from an online merchant) with the assistance of information\ncollected during the user's visit to the showroom.  Revenue sharing can be performed between the showroom engine 210 and another entity that facilitated the purchase, such as where the user initially looked up the product at a website but then came to\nthe showroom of a different entity to purchase the product.\nIn another embodiment, the showroom(s) 102, the online merchant(s) 150 and/or other merchant(s) can engage in an auction to sell the product to the user, such as a reverse auction.  For example, a selection of auction participants can be made\nfor an auction of a product that the user desires to purchase (e.g., a product that was being showroomed by the user).  The selection of the participants can be by the user, by the showroom engine 210, and/or by another entity.\nIn one embodiment, auction participants can submit offers to sell the product to the user and the lowest offer can be selected by the user.  In another embodiment, the auction can be of similar products offered by different sellers.  In this\nexample, the lowest price may not be the winning bid, such as where a particular model of the product is considered to be more valuable by the user.  In one embodiment, the showroom engine 210 can moderate or otherwise manage the auction, which can be an\nonline auction.  For example, the showroom engine 210 can establish a communication session with the user 105 (e.g., via the users device 116), as well as with other merchants (e.g., via online merchant servers 150).  The auction can be held and the user\n105 can accept a desired offer from one of the seller participants.\nThe auction can occur at various times, such as while the user 105 is in the showroom 102 or later when the user is elsewhere such as at home.  In one embodiment, the showroom engine 210 can provide user data or a portion thereof to the auction\nparticipants to assist them in determining offers to be made during the auction.  In another embodiment, the showroom engine 210 can receive revenue sharing from the winner of the auction.\nIn one or more embodiments, the showroom engine 210 can learn from the interaction with the user and use the learned data to facilitate subsequent purchases of products.  In one embodiment, the learned data can be images and/or audio captured\nduring the interaction.  In another embodiment, the learned data can be other products viewed by the user.  As an example, negotiation information such as a user's bid for a product, can be stored (e.g., in database 135) so that subsequent negotiations\nare adjusted based on historical bid data of the user.  In one or more embodiments, monitoring of the user and obtaining of user data can be utilized by an advertisement engine to generate or otherwise adjust advertisements that are presented to the\nuser.  For example, the advertisements can be presented by a display device or an electronic sign in proximity to the user.  In another embodiment, the advertisement can be presented on a product that the user is presently viewing.  For example, a user\ncan be looking to purchase a smart phone and can be testing its functions.  During the testing in the showroom, the showroom engine 210 can provide targeted advertising via the smart phone.  The targeted advertising can include positive reviews of the\nsmart phone, accessories for the smart phone, services that can be bundled with the smart phone, and so forth.\nIn one embodiment, the showroom engine 210 can detect an identity of a user and a location of the user in the showroom.  This information can be used for determining whether the user is seeking to purchaser a product and determining an identity\nof the product.  Other information can also be utilized in these determinations, such as an activity state of a communication device of the user, recent media consumption of the user (e.g., visiting a consumer reports website regarding a particular\nproduct or watching a movie where a particular product is prominent in the movie (e.g., a particular type of boat or car).  The showroom engine 210 can perform other types of monitoring of the user, such as beacon, location, idleness, eye tracking,\ndevice-in-hand, and so forth.  In one embodiment, the showroom engine 210 can access an inventory management system associated with the showroom to determine product availability.\nIn one embodiment, the showroom engine 210 can generate a virtual sales agent according to user data obtained for a user.  The virtual sales agent can be based on the user or can be based on a person for whom the user is seeking to purchase the\nproduct.  As an example, the showroom engine can detect that the user is seeking to purchase a television for the user's husband.  This detection can be based on an analysis of various user data including knowledge of family birthdays, recent website\nvisits by the user, and so forth.  The showroom engine 210 can generate a virtual agent with characteristics similar to the user's husband.  The characteristics can be determined based on obtained user data for the user's husband, such as monitored media\nconsumption, purchasing history, user preferences, and so forth.\nFIG. 5 depicts an illustrative embodiment of a method 500 used by one or both of systems 100 and 300.  Method 500 can begin at 502 where monitoring of a user in a showroom is performed to generate shopping parameters of the user.  At 504, a\ndetermination can be made as to whether the user is seeking to purchase a product (i.e., showrooming) in the showroom according to the shopping parameters.  If the user is showrooming then at 506, user data of the user can be obtained.  At 508, the user\ndata can be analyzed.  At 510, interaction data can be selected from among a group of interaction data according to the analyzing of the user data.\nAt 512, the interaction data can be provided to a computing device of a sales agent to enable the sales agent to transact with the user for the purchase of the product.  The sales agent can be of various types, including a human sales agent in\nthe showroom, a virtual sales agent in the showroom, an online sales agent at a website that sells the product, and so forth.  In one embodiment, the shopping parameters can include an activity state of a communication device of the user.  In one\nembodiment, the interaction data can include a price for the product, identification of an alternative product, identification of an accessory for the product, or a combination thereof.\nIn one embodiment, the interaction data can be provided to a display device of the showroom to cause the display device to present an advertisement associated with the interaction data.  In one embodiment, the shopping parameters can include a\nlocation of the user in the showroom, eye-tracking data of the user in the showroom, or a combination thereof.  In one embodiment, other preferences associated with other users can be determined, such as where the other users and the user are part of an\nonline social network, and where the selecting of the interaction data is based on the other preferences.  In one embodiment, a determination can be made that the user has not purchased the product in the showroom, where the sales agent is an online\nentity that sells the product, and where the providing of the interaction data to the computing device of the sales agent is responsive to the determining that the user has not purchased the product in the showroom.\nIn one embodiment, the providing of the interaction data to the computing device of the sales agent includes providing metadata that describes an interaction of the user at the showroom associated with the product.  In one embodiment, a\ndetermination can be made that the user has purchased the product from the online entity and engaging in revenue sharing with the online entity according to a purchase transaction for the product.  In one embodiment, the product can be adjusted to\ngenerate an adjusted product according to the interaction data.\nIn one embodiment, the user data can include user preferences according to user input, a transaction history of the user, a media consumption history of the user, demographics of the user, or a combination thereof.  In one embodiment, the\nproviding of the interaction data to the computing device of the sales agent can include providing bundling offers that describe services associated with the product, where the services are provided by different third parties.\nFIG. 6 depicts an illustrative embodiment of a communication system 600 for providing communication services that include facilitating purchases or products and services, as well as enabling voice, video, data and messaging services.  The\ncommunication system 600 can represent an Internet Protocol Television (IPTV) media system.  Communication system 600 can be overlaid or operably coupled with the devices and systems of FIGS. 1-4 as another representative embodiment of communication\nsystem 600.  For instance, one or more devices illustrated in the communication system 600 of FIG. 6 can enable monitoring of a user in a showroom to generate shopping parameters of the user (e.g., by location tracking, eye-tracking, and/or device\nactivity state tracking), determining that the user is seeking to purchase a product in the showroom according to the shopping parameters, obtaining user data of the user in response to the determining that the user is seeking to purchase the product,\nanalyzing the user data, selecting interaction data from among a group of interaction data according to the analyzing of the user data, and providing the interaction data to a computing device of a sales agent to enable the sales agent to transact with\nthe user for the purchase of the product.  The interaction data can enable various functions to be performed including adjusting information presented at display devices in proximity to the user, providing metadata describing user interaction to an\nonline service that sells the product, guiding human and/or virtual agents with respect to a sale of the product, such as pricing, alternative products, bundling of other products or services, accessories, and so forth.\nThe IPTV media system can include a super head-end office (SHO) 610 with at least one super headend office server (SHS) 611 which receives media content from satellite and/or terrestrial communication systems.  In the present context, media\ncontent can represent, for example, audio content, moving image content such as 2D or 3D videos, video games, virtual reality content, still image content, and combinations thereof.  The SHS server 611 can forward packets associated with the media\ncontent to one or more video head-end servers (VHS) 614 via a network of video head-end offices (VHO) 612 according to a multicast communication protocol.\nThe VHS 614 can distribute multimedia broadcast content via an access network 618 to commercial and/or residential buildings 602 housing a gateway 604 (such as a residential or commercial gateway).  The access network 618 can represent a group\nof digital subscriber line access multiplexers (DSLAMs) located in a central office or a service area interface that provide broadband services over fiber optical links or copper twisted pairs 619 to buildings 602.  The gateway 604 can use communication\ntechnology to distribute broadcast signals to media processors 606 such as Set-Top Boxes (STBs) which in turn present broadcast channels to media devices 608 such as computers or television sets managed in some instances by a media controller 607 (such\nas an infrared or RF remote controller).\nThe gateway 604, the media processors 606, and media devices 608 can utilize tethered communication technologies (such as coaxial, powerline or phone line wiring) or can operate over a wireless access protocol such as Wireless Fidelity (WiFi),\nBluetooth.RTM., Zigbee.RTM., or other present or next generation local or personal area wireless network technologies.  By way of these interfaces, unicast communications can also be invoked between the media processors 606 and subsystems of the IPTV\nmedia system for services such as video-on-demand (VoD), browsing an electronic programming guide (EPG), or other infrastructure services.\nA satellite broadcast television system 629 can be used in the media system of FIG. 6.  The satellite broadcast television system can be overlaid, operably coupled with, or replace the IPTV system as another representative embodiment of\ncommunication system 600.  In this embodiment, signals transmitted by a satellite 615 that include media content can be received by a satellite dish receiver 631 coupled to the building 602.  Modulated signals received by the satellite dish receiver 631\ncan be transferred to the media processors 606 for demodulating, decoding, encoding, and/or distributing broadcast channels to the media devices 608.  The media processors 606 can be equipped with a broadband port to an Internet Service Provider (ISP)\nnetwork 632 to enable interactive services such as VoD and EPG as described above.\nIn yet another embodiment, an analog or digital cable broadcast distribution system such as cable TV system 633 can be overlaid, operably coupled with, or replace the IPTV system and/or the satellite TV system as another representative\nembodiment of communication system 600.  In this embodiment, the cable TV system 633 can also provide Internet, telephony, and interactive media services.\nThe subject disclosure can apply to other present or next generation over-the-air and/or landline media content services system.\nSome of the network elements of the IPTV media system can be coupled to one or more computing devices 630, a portion of which can operate as a web server for providing web portal services over the ISP network 632 to wireline media devices 608 or\nwireless communication devices 616.\nCommunication system 600 can also provide for all or a portion of the computing devices 630 to function as a showroom engine (herein referred to as server 630).  The server 630 can perform one or more of the functions described with respect to\nshowroom engine 210 or server 130 of FIGS. 1-4.  The server 630 can use computing and communication technology to perform function 662, which can include among other things, determining that a user is seeking to purchase a product in a showroom according\nto monitoring of the user in the showroom, obtaining user data of the user in response to the determining that the user is seeking to purchase the product, analyzing the user data, selecting interaction data from among a group of interaction data\naccording to the analyzing of the user data, determining that the user has not purchased the product in the showroom, and providing the interaction data to a computing device of a sales agent to enable the sales agent to transact with the user for the\npurchase of the product.  In one embodiment, the sales agent can be an online entity that sells the product, and the providing of the interaction data to the computing device of the sales agent can be responsive to the determining that the user has not\npurchased the product in the showroom.\nIn another embodiment, the media processors 606 and wireless communication devices 616 can be provisioned with software functions 664 and 666, respectively, to utilize the services of server 630.  For instance, functions 664 and 666 of media\nprocessors 606 and wireless communication devices 616 enable user data to be collected and analyzed by the server 630.  For instance, a device activity state of the communication device 616 can be transmitted to the server 630 to facilitate a\ndetermination as to whether the user is showrooming in the showroom 102.  As another example, media consumption data can be monitored for a user (e.g., at the user's residence) and this data (or a portion thereof depending on permissions provided by the\nuser) can be provided to the server 630 for analysis to determine interaction data for the user when the user is in the showroom seeking to purchase a product.  In one embodiment, the showroom 102 can have one or more components that perform function 668\nfor monitoring of the user in the showroom.  For instance, function 668 can enable location tracking of the user to determine what product the user may be looking at. Function 668 can also enable device activity state tracking, as well as eye-tracking of\nthe user to see what product the user is looking at.\nMultiple forms of media services can be offered to media devices over landline technologies such as those described above.  Additionally, media services can be offered to media devices by way of a wireless access base station 617 operating\naccording to common wireless access protocols such as Global System for Mobile or GSM, Code Division Multiple Access or CDMA, Time Division Multiple Access or TDMA, Universal Mobile Telecommunications or UMTS, World interoperability for Microwave or\nWiMAX, Software Defined Radio or SDR, Long Term Evolution or LTE, and so on.  Other present and next generation wide area wireless access network technologies can be used in one or more embodiments of the subject disclosure.\nFIG. 7 depicts an illustrative embodiment of a communication device 700.  Communication device 700 can serve in whole or in part as an illustrative embodiment of the devices depicted in FIGS. 1-4 and 6 and can be configured to perform portions\nof method 500 of FIG. 5.  Communication device 700 can perform a number of operations including determining that a user is seeking to purchase a product in a showroom according to monitoring of the user in the showroom, obtaining user data of the user in\nresponse to the determining that the user is seeking to purchase the product, analyzing the user data, selecting interaction data from among a group of interaction data according to the analyzing of the user data, and providing the interaction data to a\ndisplay device of the showroom to cause the display device to present an advertisement associated with the interaction data.  In one embodiment, the communication device 700 can determine that the user has not purchased the product in the showroom and\ncan provide the interaction data to a computing device of a sales agent to enable the sales agent to transact with the user for the purchase of the product.  In this example, the sales agent is an online entity that sells the product, and the providing\nof the interaction data to the computing device of the sales agent can be responsive to the determining that the user has not purchased the product in the showroom.  In one embodiment, the user data can include user preferences according to user input, a\ntransaction history of the user, a media consumption history of the user, demographics of the user, or a combination thereof.\nCommunication device 700 can comprise a wireline and/or wireless transceiver 702 (herein transceiver 702), a user interface (UI) 704, a power supply 714, a location receiver 716, a motion sensor 718, an orientation sensor 720, and a controller\n706 for managing operations thereof.  The transceiver 702 can support short-range or long-range wireless access technologies such as Bluetooth.RTM., ZigBee.RTM., WiFi, DECT, or cellular communication technologies, just to mention a few (Bluetooth.RTM. \nand ZigBee.RTM.  are trademarks registered by the Bluetooth.RTM.  Special Interest Group and the ZigBee.RTM.  Alliance, respectively).  Cellular technologies can include, for example, CDMA-1X, UMTS/HSDPA, GSM/GPRS, TDMA/EDGE, EV/DO, WiMAX, SDR, LTE, as\nwell as other next generation wireless communication technologies as they arise.  The transceiver 702 can also be adapted to support circuit-switched wireline access technologies (such as PSTN), packet-switched wireline access technologies (such as\nTCP/IP, VoIP, etc.), and combinations thereof.\nThe UI 704 can include a depressible or touch-sensitive keypad 708 with a navigation mechanism such as a roller ball, a joystick, a mouse, or a navigation disk for manipulating operations of the communication device 700.  The keypad 708 can be\nan integral part of a housing assembly of the communication device 700 or an independent device operably coupled thereto by a tethered wireline interface (such as a USB cable) or a wireless interface supporting for example Bluetooth.RTM..  The keypad 708\ncan represent a numeric keypad commonly used by phones, and/or a QWERTY keypad with alphanumeric keys.  The UI 704 can further include a display 710 such as monochrome or color LCD (Liquid Crystal Display), OLED (Organic Light Emitting Diode) or other\nsuitable display technology for conveying images to an end user of the communication device 700.  In an embodiment where the display 710 is touch-sensitive, a portion or all of the keypad 708 can be presented by way of the display 710 with navigation\nfeatures.\nThe display 710 can use touch screen technology to also serve as a user interface for detecting user input.  As a touch screen display, the communication device 700 can be adapted to present a user interface with graphical user interface (GUI)\nelements that can be selected by a user with a touch of a finger.  The touch screen display 710 can be equipped with capacitive, resistive or other forms of sensing technology to detect how much surface area of a user's finger has been placed on a\nportion of the touch screen display.  This sensing information can be used to control the manipulation of the GUI elements or other functions of the user interface.  The display 710 can be an integral part of the housing assembly of the communication\ndevice 700 or an independent device communicatively coupled thereto by a tethered wireline interface (such as a cable) or a wireless interface.\nThe UI 704 can also include an audio system 712 that utilizes audio technology for conveying low volume audio (such as audio heard in proximity of a human ear) and high volume audio (such as speakerphone for hands free operation).  The audio\nsystem 712 can further include a microphone for receiving audible signals of an end user.  The audio system 712 can also be used for voice recognition applications.  The UI 704 can further include an image sensor 713 such as a charged coupled device\n(CCD) camera for capturing still or moving images.\nThe power supply 714 can utilize common power management technologies such as replaceable and rechargeable batteries, supply regulation technologies, and/or charging system technologies for supplying energy to the components of the communication\ndevice 700 to facilitate long-range or short-range portable applications.  Alternatively, or in combination, the charging system can utilize external power sources such as DC power supplied over a physical interface such as a USB port or other suitable\ntethering technologies.\nThe location receiver 716 can utilize location technology such as a global positioning system (GPS) receiver capable of assisted GPS for identifying a location of the communication device 700 based on signals generated by a constellation of GPS\nsatellites, which can be used for facilitating location services such as navigation.  The motion sensor 718 can utilize motion sensing technology such as an accelerometer, a gyroscope, or other suitable motion sensing technology to detect motion of the\ncommunication device 700 in three-dimensional space.  The orientation sensor 720 can utilize orientation sensing technology such as a magnetometer to detect the orientation of the communication device 700 (north, south, west, and east, as well as\ncombined orientations in degrees, minutes, or other suitable orientation metrics).\nThe communication device 700 can use the transceiver 702 to also determine a proximity to a cellular, WiFi, Bluetooth.RTM., or other wireless access points by sensing techniques such as utilizing a received signal strength indicator (RSSI)\nand/or signal time of arrival (TOA) or time of flight (TOF) measurements.  The controller 706 can utilize computing technologies such as a microprocessor, a digital signal processor (DSP), programmable gate arrays, application specific integrated\ncircuits, and/or a video processor with associated storage memory such as Flash, ROM, RAM, SRAM, DRAM or other storage technologies for executing computer instructions, controlling, and processing data supplied by the aforementioned components of the\ncommunication device 700.\nOther components not shown in FIG. 7 can be used in one or more embodiments of the subject disclosure.  For instance, the communication device 700 can include a reset button (not shown).  The reset button can be used to reset the controller 706\nof the communication device 700.  In yet another embodiment, the communication device 700 can also include a factory default setting button positioned, for example, below a small hole in a housing assembly of the communication device 700 to force the\ncommunication device 700 to re-establish factory settings.  In this embodiment, a user can use a protruding object such as a pen or paper clip tip to reach into the hole and depress the default setting button.  The communication device 700 can also\ninclude a slot for adding or removing an identity module such as a Subscriber Identity Module (SIM) card.  SIM cards can be used for identifying subscriber services, executing programs, storing subscriber data, and so forth.\nThe communication device 700 as described herein can operate with more or less of the circuit components shown in FIG. 7.  These variant embodiments can be used in one or more embodiments of the subject disclosure.\nThe communication device 700 can be adapted to perform the functions of the devices of FIGS. 1-4 and/or 6, such as the server 130, the database 135, the computing device 150, the showroom engine 210, and so forth.  In addition, the controller\n706 can be adapted in various embodiments to perform the functions 662-666, respectively.\nUpon reviewing the aforementioned embodiments, it would be evident to an artisan with ordinary skill in the art that said embodiments can be modified, reduced, or enhanced without departing from the scope of the claims described below.  For\nexample, the interaction data can provide a sales agent (e.g., human or virtual) with competitor pricing for the same product.  In one embodiment, access to user data can be limited to only after a determination is made that the user is seeking to\npurchase a particular product.  In another embodiment, access to the user data can be based responsive to identifying the presence of the user in the showroom.  In on embodiment, purchases by individuals in a social network of the user of similar\nproducts (at the showroom or elsewhere) can be detected so that the sales agent (e.g., human or virtual) can present this information to the user.  Other embodiments can be used in the subject disclosure.\nIt should be understood that devices described in the exemplary embodiments can be in communication with each other via various wireless and/or wired methodologies.  The methodologies can be links that are described as coupled, connected and so\nforth, which can include unidirectional and/or bidirectional communication over wireless paths and/or wired paths that utilize one or more of various protocols or methodologies, where the coupling and/or connection can be direct (e.g., no intervening\nprocessing device) and/or indirect (e.g., an intermediary processing device such as a router).\nFIG. 8 depicts an exemplary diagrammatic representation of a machine in the form of a computer system 800 within which a set of instructions, when executed, may cause the machine to perform any one or more of the methods described above.  One or\nmore instances of the machine can operate, for example, as the server 130 or the showroom engine 210 to facilitate a purchase of a product by a user.  In some embodiments, the machine may be connected (e.g., using a network 826) to other machines.  In a\nnetworked deployment, the machine may operate in the capacity of a server or a client user machine in a server-client user network environment, or as a peer machine in a peer-to-peer (or distributed) network environment.\nThe machine may comprise a server computer, a client user computer, a personal computer (PC), a tablet, a smart phone, a laptop computer, a desktop computer, a control system, a network router, switch or bridge, or any machine capable of\nexecuting a set of instructions (sequential or otherwise) that specify actions to be taken by that machine.  It will be understood that a communication device of the subject disclosure includes broadly any electronic device that provides voice, video or\ndata communication.  Further, while a single machine is illustrated, the term \"machine\" shall also be taken to include any collection of machines that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of\nthe methods discussed herein.\nThe computer system 800 may include a processor (or controller) 802 (e.g., a central processing unit (CPU)), a graphics processing unit (GPU, or both), a main memory 804 and a static memory 806, which communicate with each other via a bus 808. \nThe computer system 800 may further include a display unit 810 (e.g., a liquid crystal display (LCD), a flat panel, or a solid state display).  The computer system 800 may include an input device 812 (e.g., a keyboard), a cursor control device 814 (e.g.,\na mouse), a disk drive unit 816, a signal generation device 818 (e.g., a speaker or remote control) and a network interface device 820.  In distributed environments, the embodiments described in the subject disclosure can be adapted to utilize multiple\ndisplay units 810 controlled by two or more computer systems 800.  In this configuration, presentations described by the subject disclosure may in part be shown in a first of the display units 810, while the remaining portion is presented in a second of\nthe display units 810.\nThe disk drive unit 816 may include a tangible computer-readable storage medium 822 on which is stored one or more sets of instructions (e.g., software 824) embodying any one or more of the methods or functions described herein, including those\nmethods illustrated above.  The instructions 824 may also reside, completely or at least partially, within the main memory 804, the static memory 806, and/or within the processor 802 during execution thereof by the computer system 800.  The main memory\n804 and the processor 802 also may constitute tangible computer-readable storage media.\nDedicated hardware implementations including, but not limited to, application specific integrated circuits, programmable logic arrays and other hardware devices can likewise be constructed to implement the methods described herein.  Application\nspecific integrated circuits and programmable logic array can use downloadable instructions for executing state machines and/or circuit configurations to implement embodiments of the subject disclosure.  Applications that may include the apparatus and\nsystems of various embodiments broadly include a variety of electronic and computer systems.  Some embodiments implement functions in two or more specific interconnected hardware modules or devices with related control and data signals communicated\nbetween and through the modules, or as portions of an application-specific integrated circuit.  Thus, the example system is applicable to software, firmware, and hardware implementations.\nIn accordance with various embodiments of the subject disclosure, the operations or methods described herein are intended for operation as software programs or instructions running on or executed by a computer processor or other computing\ndevice, and which may include other forms of instructions manifested as a state machine implemented with logic components in an application specific integrated circuit or field programmable gate array.  Furthermore, software implementations (e.g.,\nsoftware programs, instructions, etc.) including, but not limited to, distributed processing or component/object distributed processing, parallel processing, or virtual machine processing can also be constructed to implement the methods described herein. It is further noted that a computing device such as a processor, a controller, a state machine or other suitable device for executing instructions to perform operations or methods may perform such operations directly or indirectly by way of one or more\nintermediate devices directed by the computing device.\nWhile the tangible computer-readable storage medium 822 is shown in an example embodiment to be a single medium, the term \"tangible computer-readable storage medium\" should be taken to include a single medium or multiple media (e.g., a\ncentralized or distributed database, and/or associated caches and servers) that store the one or more sets of instructions.  The term \"tangible computer-readable storage medium\" shall also be taken to include any non-transitory medium that is capable of\nstoring or encoding a set of instructions for execution by the machine and that cause the machine to perform any one or more of the methods of the subject disclosure.  The term \"non-transitory\" as in a non-transitory computer-readable storage includes\nwithout limitation memories, drives, devices and anything tangible but not a signal per se.\nThe term \"tangible computer-readable storage medium\" shall accordingly be taken to include, but not be limited to: solid-state memories such as a memory card or other package that houses one or more read-only (non-volatile) memories, random\naccess memories, or other re-writable (volatile) memories, a magneto-optical or optical medium such as a disk or tape, or other tangible media which can be used to store information.  Accordingly, the disclosure is considered to include any one or more\nof a tangible computer-readable storage medium, as listed herein and including art-recognized equivalents and successor media, in which the software implementations herein are stored.\nAlthough the present specification describes components and functions implemented in the embodiments with reference to particular standards and protocols, the disclosure is not limited to such standards and protocols.  Each of the standards for\nInternet and other packet switched network transmission (e.g., TCP/IP, UDP/IP, HTML, HTTP) represent examples of the state of the art.  Such standards are from time-to-time superseded by faster or more efficient equivalents having essentially the same\nfunctions.  Wireless standards for device detection (e.g., RFID), short-range communications (e.g., Bluetooth.RTM., WiFi, Zigbee.RTM.), and long-range communications (e.g., WiMAX, GSM, CDMA, LTE) can be used by computer system 800.\nThe illustrations of embodiments described herein are intended to provide a general understanding of the structure of various embodiments, and they are not intended to serve as a complete description of all the elements and features of apparatus\nand systems that might make use of the structures described herein.  Many other embodiments will be apparent to those of skill in the art upon reviewing the above description.  The exemplary embodiments can include combinations of features and/or steps\nfrom multiple embodiments.  Other embodiments may be utilized and derived therefrom, such that structural and logical substitutions and changes may be made without departing from the scope of this disclosure.  Figures are also merely representational and\nmay not be drawn to scale.  Certain proportions thereof may be exaggerated, while others may be minimized.  Accordingly, the specification and drawings are to be regarded in an illustrative rather than a restrictive sense.\nAlthough specific embodiments have been illustrated and described herein, it should be appreciated that any arrangement calculated to achieve the same purpose may be substituted for the specific embodiments shown.  This disclosure is intended to\ncover any and all adaptations or variations of various embodiments.  Combinations of the above embodiments, and other embodiments not specifically described herein, can be used in the subject disclosure.  In one or more embodiments, features that are\npositively recited can also be excluded from the embodiment with or without replacement by another component or step.  The steps or functions described with respect to the exemplary processes or methods can be performed in any order.  The steps or\nfunctions described with respect to the exemplary processes or methods can be performed alone or in combination with other steps or functions (from other embodiments or from other steps that have not been described).\nLess than all of the steps or functions described with respect to the exemplary processes or methods can also be performed in one or more of the exemplary embodiments.  Further, the use of numerical terms to describe a device, component, step or\nfunction, such as first, second, third, and so forth, is not intended to describe an order or function unless expressly stated so.  The use of the terms first, second, third and so forth, is generally to distinguish between devices, components, steps or\nfunctions unless expressly stated otherwise.  Additionally, one or more devices or components described with respect to the exemplary embodiments can facilitate one or more functions, where the facilitating (e.g., facilitating access or facilitating\nestablishing a connection) can include less than every step needed to perform the function or can include all of the steps needed to perform the function.\nIn one or more embodiments, a processor (which can include a controller or circuit) has been described that performs various functions.  It should be understood that the processor can be multiple processors, which can include distributed\nprocessors or parallel processors in a single machine or multiple machines.  The processor can be used in supporting a virtual processing environment.  The virtual processing environment may support one or more virtual machines representing computers,\nservers, or other computing devices.  In such virtual machines, components such as microprocessors and storage devices may be virtualized or logically represented.  The processor can include a state machine, application specific integrated circuit,\nand/or programmable gate array including a Field PGA.  In one or more embodiments, when a processor executes instructions to perform \"operations\", this can include the processor performing the operations directly and/or facilitating, directing, or\ncooperating with another device or component to perform the operations.\nThe Abstract of the Disclosure is provided with the understanding that it will not be used to interpret or limit the scope or meaning of the claims.  In addition, in the foregoing Detailed Description, it can be seen that various features are\ngrouped together in a single embodiment for the purpose of streamlining the disclosure.  This method of disclosure is not to be interpreted as reflecting an intention that the claimed embodiments require more features than are expressly recited in each\nclaim.  Rather, as the following claims reflect, inventive subject matter lies in less than all features of a single disclosed embodiment.  Thus the following claims are hereby incorporated into the Detailed Description, with each claim standing on its\nown as a separately claimed subject matter.", "application_number": "14529276", "abstract": " Aspects of the subject disclosure may include, for example, monitoring a\n     user in a showroom to generate shopping parameters of the user,\n     determining that the user is seeking to purchase a product in the\n     showroom according to the shopping parameters, obtaining user data of the\n     user in response to the determining that the user is seeking to purchase\n     the product, analyzing the user data, selecting interaction data from\n     among a group of interaction data according to the analyzing of the user\n     data, and providing the interaction data to a computing device of a sales\n     agent to enable the sales agent to transact with the user for the\n     purchase of the product. Other embodiments are disclosed.\n", "citations": ["7311605", "8219438", "9082143", "9483789", "20050102185", "20050175218", "20050177463", "20060200413", "20070090185", "20080004951", "20080294607", "20110093339", "20120095805", "20120158482", "20130080290", "20130124365", "20130290145", "20140132728", "20140143055", "20140208272", "20140358666", "20140363059", "20150142596", "20150262117"], "related": []}, {"id": "20160299975", "patent_code": "10373057", "patent_name": "Concept analysis operations utilizing accelerators", "year": "2019", "inventor_and_country_data": " Inventors: \nAcar; Emrah (Montvale, NJ), Bordawekar; Rajesh R. (Yorktown Heights, NY), Franceschini; Michele M. (White Plains, NY), Lastras-Montano; Luis A. (Cortlandt Manor, NY), Puri; Ruchir (Baldwin Place, NY), Qian; Haifeng (White Plains, NY), Soares; Livio B. (New York, NY)  ", "description": "<BR><BR>BACKGROUND\nThe present application relates generally to an improved data processing apparatus and method and more specifically to mechanisms for performing concept analysis operations utilizing accelerators.\nEveryday life is dominated by information technology and systems for obtaining information and knowledge from collections of data.  For example, search engines operate on large collections of data to obtain information related to a search query. Question and Answer (QA) systems, such as the IBM Watson.TM.  QA system available from International Business Machines (IBM) Corporation of Armonk, N.Y., operate on a corpus of documents or other portions of information to answer natural language\nquestions.  Moreover, many social networking services represent their users, communications, and the like, as large data sets.  Many times it is important to perform knowledge extraction, reasoning, and various other analytics on these large scale data\nsets so as to facilitate the operation of the systems, e.g., answer questions, return search results, or provide functionality within the social networking services.  For example, many social networking services help individuals identify other registered\nusers that they may know or have a connection with.  Such functionality requires analyzing a large set of data representing the users of the social networking service.\nIn facilitating searching of information in large sets of documents, such as searches of the web pages on the Internet (or the \"web\"), search engines are employed which rank results based on various factors.  One such search engine is the\nGoogle.TM.  search engine which uses a ranking algorithm referred to as \"PageRank.\" PageRank exploits the linkage structure of the web to compute global \"importance\" scores that can be used to influence the ranking of search results.\nRecently, an effort at Stanford University, as part of their Stanford Global Infobase Project, has developed an algorithm for allowing users to define their own notion of importance for each individual query.  This algorithm, referred to as\npersonalized PageRank, provides online personalized web searching with personalized variants of PageRank based on a private, personalized profile.\n<BR><BR>SUMMARY\nIn one illustrative embodiment, a method, in a system comprising a host system having a processor and a memory, and at least one accelerator device, for performing a concept analysis operation is provided.  The method comprises extracting, by\nthe host system, a set of one or more concepts from an information source and providing, by the host system, the set of one or more concepts to the accelerator device.  Moreover, the method comprises providing, by the host system, at least one matrix\nrepresentation data structure representing a graph of concepts and relationships between concepts in a corpus.  In addition, the method comprises executing, by the accelerator device, the concept analysis operation internal to the accelerator device to\ngenerate an output vector identifying concepts in the corpus, identified in the at least one matrix representation data structure, related to the set of one or more concepts extracted from the information source.  The method also comprises outputting, by\nthe accelerator device, the output vector to the host system, wherein the host system utilizes the output vector to respond to a request submitted to the host system associated with the information source.\nIn other illustrative embodiments, a computer program product comprising a computer useable or readable medium having a computer readable program is provided.  The computer readable program, when executed on a computing device, causes the\ncomputing device to perform various ones of, and combinations of, the operations outlined above with regard to the method illustrative embodiment.\nIn yet another illustrative embodiment, a system/apparatus is provided.  The system/apparatus may comprise a host system having one or more processors and a memory coupled to the one or more processors and an accelerator device.  The memory may\ncomprise instructions which, when executed by the one or more processors, cause the one or more processors to perform various ones of, and combinations of, the operations outlined above with regard to the method illustrative embodiment and attributed to\nthe host system.  Other operations attributed to the accelerator device are performed internal to the accelerator device with the accelerator device outputting an output vector to the host system.\nThese and other features and advantages of the present invention will be described in, or will become apparent to those of ordinary skill in the art in view of, the following detailed description of the example embodiments of the present\ninvention. <BR><BR>BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWINGS\nThe invention, as well as a preferred mode of use and further objectives and advantages thereof, will best be understood by reference to the following detailed description of illustrative embodiments when read in conjunction with the\naccompanying drawings, wherein:\nFIG. 1 depicts a schematic diagram of one illustrative embodiment of a question/answer creation (QA) system in a computer network;\nFIG. 2 is a block diagram of an example data processing system in which aspects of the illustrative embodiments are implemented;\nFIG. 3 illustrates a QA system pipeline for processing an input question in accordance with one illustrative embodiment;\nFIG. 4 is an example diagram illustrating one approach to performing concept analysis operations using a host system and host system storage;\nFIG. 5 is an example diagram illustrating concept analysis operations being performed using one or more accelerator devices in accordance with one illustrative embodiment;\nFIG. 6 is an example diagram illustrating an ordering of matrix entries obtained by using a clustering based matrix reordering operation in accordance with one illustrative embodiment;\nFIG. 7 is a flowchart outlining an example clustering based matrix reordering operation in accordance with one illustrative embodiment;\nFIG. 8A illustrates a matrix vector multiplication operation performed using a Compact Sparse Row (CSR) formatted data structure of a matrix along with sample pseudo-code for performing the partial matrix vector multiplication operations;\nFIG. 8B illustrates a matrix vector multiplication operation performed using a Compact Sparse Column (CSC) formatted data structure of a matrix along with sample pseudo-code for performing the partial matrix vector multiplication operations;\nFIG. 9 is a flowchart outlining an example hybrid representation matrix vector multiplication operation in accordance with one illustrative embodiment;\nFIG. 10 is a flowchart outlining an example operation for dynamically modifying the compressed matrix representation utilized for iterations of a matrix operation based on a determination of the sparsity/density of an input vector using a hybrid\nmatrix representation mechanism in accordance with one illustrative embodiment; and\nFIG. 11 is a flowchart outlining an example operation for performing a concept analysis operation utilizing one or more accelerator devices in accordance with one illustrative embodiment.\n<BR><BR>DETAILED DESCRIPTION\nAs discussed above, modern computing systems often are engaged in performing knowledge extraction, reasoning, and various other analytical operations on large scale data sets.  Search engines, Question and Answer systems, Natural Language\nProcessing (NLP) systems, relationship analytics engines, and the like, are only some examples of these types of modern computing systems which operate on large scale data sets to facilitate their operations.  Often times these systems operate on\nidentified concepts in portions of information, e.g., electronic documents, web pages, data files, or the like, referred to collectively as a corpus of information.\nThese concepts may be represented as networks or graphs comprising nodes and edges, where the nodes represent the concepts themselves and the edges represent relationships between the concepts identified in the corpus.  The edges may be\nuni-directional or bi-directional and may have associated weights or strengths which represent how strongly one concept (node) associated with the edge is determined to be related to another concept (node) associated with the edge.  In one illustrative\nembodiment, the sum of all weights on every outgoing edge stemming from a node is 1.0.  It should also be noted that with such graphs, there may be \"self-edges\" or \"self-links\", which are edges that point back to the node from which they originated.  It\ncan be appreciated that with a large corpus of information, such as the Wikipedia.TM.  website for example, the complexity and size of such networks/graphs becomes very large as well.\nIn order to perform knowledge extraction, reasoning, and other analytical operations on such large scale data sets (i.e. networks/graphs), these networks/graphs are often represented as matrices in which the indices of the matrix represent the\nnodes of the network/graph, and entries at the rows/columns of the matrix represent whether or not the nodes have an edge connecting them to each other, i.e. whether or not the nodes have a relationship with one another.  A non-zero value in the entry at\nthe intersection of a row/column indicates a relationship being present between the concepts corresponding to the indices while a zero value indicates that there is no relationship between the concepts.  The strength of a relationship between the\nconcepts is measured by the value of the entry, e.g., higher values are indicative of stronger relationships between the concepts represented y the intersecting row/column indices.  The weights or strengths of the edges may be constant during a matrix\noperation performed on the matrix but may change due to dynamic events or updates happening in real time between matrix operations.\nAs can be appreciated, with large scale data sets and corresponding large scale networks/graphs, the matrices representing these networks/graphs are large sparse matrices which may have millions or even billions of nodes and corresponding edges. These matrices are sparse in that the majority of the entries in the matrix have zero-values (dense matrices have a majority of values being non-zero).  In one illustrative embodiment, the properties of such a matrix are as follows: The nodes correspond\nto concepts, entities, information, search terms of interest, or the like.  The edges are unidirectional in the graph and an entry in column j, row i corresponds to the weight (or strength) of the edge from node j to node i. The sum of all out-going\nedges (including self-edges) is 1.0 and thus, the sum of each column in the corresponding matrix is 1.0.  The matrix is square and sparse.\nIt should be appreciated that these are properties of just one example large sparse matrix upon which the mechanisms of the illustrative embodiments may operate but is not intended to be limiting of the types of matrices upon which the\nillustrative embodiments may operate.  To the contrary, as will be apparent to those of ordinary skill in the art in view of the present description, the mechanisms of the illustrative embodiments may be implemented with, and may operate on, other types\nof matrices having different properties than those mentioned in the example set of properties above.\nMatrix operations are performed on these large scale matrices to extract relationships between the entries in the matrices so as to glean knowledge, perform reasoning operations, or the like.  For example, if a process wants to know what\nconcepts are related to concept A (e.g., a search term concept or concept in an information source, such as an online encyclopedia or the like), concept A may be represented as an index (column and/or row) in the matrix (and may be specified by way of an\ninput vector for example), and other concepts may be represented as other indices in the matrix, organized into rows and columns.  Intersections of rows and columns in the matrix have values that are set to non-zero values if column A is related to the\nother concept, e.g., if concept A is represented as an index in a row, indices along the columns may represent other concepts and the intersection of each column with concept A represents whether or not concept A is related to the other concept (non-zero\nif concept A is related to the other concept and zero if concept A is not related to the other concept).  Again, \"relations\" between concepts are represented in the graph by edges and associated weights/strengths of the edges such that the zero or\nnon-zero value in the matrix is the weight/strength of the edge between concept A and the other concept.\nMost matrix operations for knowledge extraction, information extraction, concept analysis, or other analysis operations directed to identifying relationships between nodes of a graph using matrix representations, involve a matrix vector\nmultiplication operation in which the matrix is multiplied by a vector which results in an output indicative of the intersection of the vector with the matrix, e.g., non-zero values in the vector multiplied with non-zero values in the matrix result in\nnon-zero values in the output indicative of a relationship between the corresponding vector element and the matrix index.  The sparsity of the matrix and the sparsity of the vector both influence the efficiency by which this matrix vector multiplication\noperation can be accomplished due to limited size of cache memories.\nBecause these large scale matrices tend to be very sparse, the processes executing on these sparse matrices often involve many runtime resource-intensive large sparse matrix operations each comprising matrix vector multiplication operations. \nWhen a matrix vector multiplication operation is performed, portions of the matrix are loaded speculatively so as to perform the matrix vector multiplication.  A portion of the matrix/vector is loaded into cache memory and used to perform a partial\nproduct multiplication of the matrix/vector.  When a particular entry or location in the matrix is loaded into the cache, other entries or locations in close proximity to the selected entry/location are also loaded to speculate that the next matrix\nvector multiplication will target an entry/location in close proximity to the selected entry/location.  However, in a sparse matrix, this speculative loading of the cache memory, more often than not, results in a cache miss, i.e. the non-zero entry or\nlocation of interest is not present in the cache memory and must be loaded from main memory or storage.  Hence, sparse matrices/vectors, along with limited size cache memories results in cache misses which affect performance.\nThus, it can be appreciated that a process to identify related concepts, or perform other knowledge or information extraction on a large scale data set, may be very resource intensive involving a large number of cache misses and thus, loadings\nfrom main memory or storage, as the size of the matrix and the sparsity of the matrix increases.  This is also the case when the vector is sparse since the matrix vector multiplication operation essentially is looking for non-zero entries in the matrix\nwith which the non-zero elements of the vector are multiplied and if the vector and matrix are sparse, the majority of entries/elements will be zero when loaded into the cache memory.  Therefore, the efficiency by which functionality of the system is\nprovided may be limited by the efficiency and speed of performing the matrix operations on these large scale data sets.\nIn addition, because of the large size of the matrix, it becomes difficult to maintain the entire matrix in memory for use with matrix operations.  Thus, various mechanisms have been devised for representing the matrix in a compressed format. \nFor example, formats for representing matrices based on the non-zero values in the matrix have been devised which significantly reduce the size of the memory required to maintain information about the matrix.  For example, the Compact Sparse Row (CSR)\nand Compact Sparse Column (CSC) storage formats provide examples of such.  However, each of these formats are more or less efficient for different types of input vector sparsities (or densities).\nInternational Business Machines (IBM) Corporation of Armonk, N.Y.  has developed mechanisms for addressing the issues with regard to cache efficiency during large sparse matrix operations and the utilization of different storage formats for\nlarge scale matrices.  For example, U.S.  patent application Ser.  No. 14/611,297, entitled \"Matrix Ordering for Cache Efficiency in Performing Large Sparse Matrix Operations\", filed Feb.  2, 2015, which is hereby incorporated by reference, provides a\nmechanism for re-ordering a matrix to concentrate non-zero values of the matrix along the diagonal of the matrix by use of a clustering approach.  Moreover, U.S.  patent application Ser.  No. 14/635,007, entitled \"Parallelized Hybrid Sparse Matrix\nrepresentations for Performing Personalized Content Ranking\", filed Mar.  2, 2015, which is hereby incorporated by reference, provides mechanisms for selecting different storage formats of a large scale matrix for use during different iterations of a\nmatrix operation based on the sparsity (or density) of a vector being used in the matrix operation during the particular iteration.\nThese mechanisms are directed to improving the way in which the matrix operation itself is performed by modifying the way in which the matrix is represented or used within the matrix operation.  These mechanisms may be used in conjunction with\nfurther mechanisms provided by the illustrative embodiments set forth herein to provide a more efficient concept analysis mechanism for analyzing relationships between concepts represented in large scale sparse matrices for purposes of performing\nknowledge extraction, reasoning operations, concept analysis operations, or other analytical operations.  That is, while the above mechanisms are focused on the way in which the matrix is represented to achieve greater efficiency in processing the matrix\noperation by reorganizing the non-zero values of the matrix to reduce cache misses and to utilized different compressed formats to represent the matrix for different iterations of the matrix operation, the illustrative embodiments set forth hereafter\nfurther improve the overall performance of the matrix operation by providing mechanisms to facilitate acceleration of the end-to-end process of concept analysis by utilizing one or more accelerator devices.  Mechanisms are provided for accelerating the\ncore computations of a matrix operation using massive data parallelism with a large number of parallel threads being executed, each thread performing operations on a portion of the large sparse matrix.  Due to memory limits of the accelerator devices,\nbatched execution is utilized that enables operations to be performed on batches of data, e.g., input vectors, corresponding to the fixed memory limits of the accelerator devices.  The vector data structures are initialized in the memory of the\naccelerator devices, rather than at a host system, so as to reduce the amount of data transfer required.  Results of operations within the accelerator devices are merged using a single function to thereby eliminate the need to store results to main\nmemory of the host system.  Thus, the matrix operation can be completely performed within the accelerator device without having to access host system resources.\nIn operation, the host system provides the accelerator device a set of concepts extracted from an information source, e.g., a document, an input natural language question, or any other source of concepts.  In one illustrative embodiment, the\ninformation source is an input document having one or more embedded concepts and the concept analysis operation seeks to identify concepts related to those embedded in the input document to thereby associate a vector of related concepts with the input\ndocument.  This resultant vector may be used to identify other documents having related concepts so as to provide a relative ranking of one document to another.  In some illustrative embodiments, the information source may comprise a user profile, either\nalone or in combination with a document, search query, natural language question, or other request for content by the user, which is then used to provide the input set of concepts upon which the concept analysis operation is performed.  For example, this\nmay be used to perform a personalized PageRank operation in which the request for content specifies the content the user wishes to access and the user profile specifies the personal preferences of the user which can be used to modify the ranking of the\nresultant content returned to the user.  In some illustrative embodiments, the user profile may be used as a way to modify the ranking of documents used to provide answers to a natural language question in a Question and Answer system.  In short, any\nconcept analysis operation that involves identifying related concepts based on a network/graph of a corpus of information that specifies related concepts may make use of the mechanisms of the illustrative embodiments.\nWith the mechanisms of the illustrative embodiments, the host system performs some initial processing of the information source to identify these concepts, with such processing being generally known in the art and hence, a more detailed\ndescription is not provided herein.  For example, in a natural language processing system, search engine, or Question and Answer (QA) system context, the concepts may be extracted from a natural language question input to the QA system, or from an\nelectronic document or search query, using natural language processing, search query parsing, or other textual analysis techniques.  The resulting set of concepts are input to the accelerator device (hereafter referred to simply as the \"accelerator\"),\nwhich may be provided as a special purpose processor, a service processor, or the like.  In one illustrative embodiment, the accelerator is a graphics processing unit (GPU) that is integrated in, or coupled to, the host system and whose main function is\nprocessing for rendering graphics, but which is repurposed dynamically to perform the concept analysis operations of the illustrative embodiments.  The GPU may be provided with a different GPU kernel, in addition to the standard GPU kernel for graphics\nrendering, for performing concept analysis operations in accordance with the illustrative embodiments and which may be dynamically switched when needed and instructed by way of submitting jobs to the GPU from the host system, an instruction sent from the\nhost system to the GPU, setting a predefined bit in a communication from the host system to the GPU, or any other mechanism that may be implemented for switching the operational mode of the GPU.\nIn addition to the concepts extracted from the information source, the host system provides the accelerator with a representation of a large sparse matrix for use in performing concept analysis operations.  The large sparse matrix itself\nrepresents the network/graph of concepts and their relationships as already identified through processing of a corpus of information.  For example, in a QA system environment, such as may be provided by the IBM Watson.TM.  QA system, for example, a\ncorpus of documents may be provided, e.g., Wikipedia.TM.  web pages identifying various concepts and having links between concepts which are identifiable, to the QA system for use in answering questions submitted to the QA system.  In a healthcare\napplication, such a QA system may ingest a corpus of documents including medical journals, medical trial documents, medical resources including texts directed to describing drugs and procedures, medical dictionaries, patient records, or any other\ndocuments deemed pertinent to the medical domain.  In other domains, similar collections of electronic documents may be provided as a corpus for ingestion by a QA system.  The corpus may be processed using known or later developed ingestion processes,\nwhich may include natural language processing, feature extraction, and the like, to identify concepts specified in the corpus and the relationships between the concepts, as well as the strengths of these relationships, as specified in the corpus.  The\nresult is a network or graph of concepts with nodes representing the concepts and edges representing relationships between the concepts with the edges having weights representing the strength of the relationship between the connected concepts.\nIn a search engine context, the corpus of information may be a large set of web pages of various domains, such as the Internet.  Thus, the network/graph may comprise many thousands of nodes and edges between nodes representing the concepts,\ntheir relationships, and the strengths of these relationships, as discussed above.\nThe network/graph may be represented as a large sparse matrix as discussed above.  In accordance with the IBM mechanisms mentioned above and described in commonly assigned and co-pending U.S.  patent application Ser.  Nos.  14/611,297 and\n14/635,007, the large sparse matrix may be re-organized using clustering and the resulting re-organized matrix may be represented using a plurality of compressed representations which may be used in a hybrid approach to performing matrix operations\nwithin the accelerator.  In one illustrative embodiment, the re-organizing of the matrix may be performed by the host system along with the generation of the plurality of compressed format representations of the re-organized matrix.  The resulting\ncompressed format representations of the re-organized matrix may then be provided to the accelerator for use with its internal concept analysis operations which involve the use of matrix operations, such as matrix vector multiplication operations.  The\naccelerator may comprise internal logic which implements the hybrid approach to performing matrix operations described in U.S.  patent application Ser.  No. 14/635,007 when performing iterations of the concept analysis operations.\nIt should be appreciated that the processing of the large sparse matrix to generate the re-organized matrix and compressed format representations of the re-organized matrix may be performed very infrequently.  That is, as long as the large\nsparse matrix does not change significantly, there is no need to re-compute the re-organized matrix and compressed format representations.  However, when the matrix changes significantly, such as due to updates to the corpus, a re-computation of the\nmatrix may be initiated and an updated re-organized matrix and compressed format representations may be generated.  This may be done on a periodic basis, in response to a system administrator or other authorized user request, or in response to occurrence\nof a defined event, e.g., an update to the corpus.  Thus, while computational resources of the host system may be engaged for generating the re-organized matrix and corresponding compressed format representations infrequently, for the majority of the\noperations of the accelerator, the host system resources are not utilized other than to extract concepts from the information source and provide them as input to the accelerator.\nUpon receiving the extracted concepts from the information source, as provided by the host system, the accelerator operates on the extracted concepts to generate one or more vectors for use with the concept analysis operations.  The one or more\nvectors are provided to concept analysis operation logic which performs concept analysis operations using the vector(s) generated by the accelerator and the compressed format matrix representations provided as input to the accelerator by the host system. The result of the concept analysis operation is then normalized to a normal distribution and provided to post-processing logic of the accelerator which calculates rankings, i.e. strengths of relationships, of the concept relationships in the matrix with\nthe concepts specified in the vector(s).  The result is an output vector that comprises non-zero vector elements where concepts of the vector(s) intersect with non-zero entries in the matrix.  The values of the output vector elements indicate the\nstrength of relationships between the concepts in the vector(s) and the concepts in the matrix.\nThus, the mechanisms of the illustrative embodiments provide for the accelerator to perform concept analysis operations, external to the host system, based on the extracted concepts and matrix representation provided by the host system.  The\naccelerator does not require that the results of the concept analysis operation be stored in main memory or external storage before calculating the final result and instead provides a single function that outputs the final result directly without\nintermediate storage to main memory or external storage.  This minimizes data exchange between the host system and the accelerator and between the accelerator and main memory or external storage.  As a result, the speed by which such concept analysis\noperations are performed is dramatically increased.\nBefore beginning the discussion of the various aspects of the illustrative embodiments in more detail, it should first be appreciated that throughout this description the term \"mechanism\" will be used to refer to elements of the present\ninvention that perform various operations, functions, and the like.  A \"mechanism,\" as the term is used herein, may be an implementation of the functions or aspects of the illustrative embodiments in the form of an apparatus, a procedure, or a computer\nprogram product.  In the case of a procedure, the procedure is implemented by one or more devices, apparatus, computers, data processing systems, or the like.  In the case of a computer program product, the logic represented by computer code or\ninstructions embodied in or on the computer program product is executed by one or more hardware devices in order to implement the functionality or perform the operations associated with the specific \"mechanism.\" Thus, the mechanisms described herein may\nbe implemented as specialized hardware, software executing on general purpose hardware, software instructions stored on a medium such that the instructions are readily executable by specialized or general purpose hardware, a procedure or method for\nexecuting the functions, or a combination of any of the above.\nThe present description and claims may make use of the terms \"a\", \"at least one of\", and \"one or more of\" with regard to particular features and elements of the illustrative embodiments.  It should be appreciated that these terms and phrases are\nintended to state that there is at least one of the particular feature or element present in the particular illustrative embodiment, but that more than one can also be present.  That is, these terms/phrases are not intended to limit the description or\nclaims to a single feature/element being present or require that a plurality of such features/elements be present.  To the contrary, these terms/phrases only require at least a single feature/element with the possibility of a plurality of such\nfeatures/elements being within the scope of the description and claims.\nIn addition, it should be appreciated that the following description uses a plurality of various examples for various elements of the illustrative embodiments to further illustrate example implementations of the illustrative embodiments and to\naid in the understanding of the mechanisms of the illustrative embodiments.  These examples intended to be non-limiting and are not exhaustive of the various possibilities for implementing the mechanisms of the illustrative embodiments.  It will be\napparent to those of ordinary skill in the art in view of the present description that there are many other alternative implementations for these various elements that may be utilized in addition to, or in replacement of, the examples provided herein\nwithout departing from the spirit and scope of the present invention.\nIt should be appreciated that the present invention may be a system, a method, and/or a computer program product.  The computer program product may include a computer readable storage medium (or media) having computer readable program\ninstructions thereon for causing a processor to carry out aspects of the present invention.\nThe computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device.  The computer readable storage medium may be, for example, but is not limited to, an electronic\nstorage device, a magnetic storage device, an optical storage device, an electromagnetic storage device, a semiconductor storage device, or any suitable combination of the foregoing.  A non-exhaustive list of more specific examples of the computer\nreadable storage medium includes the following: a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a static random access memory (SRAM), a\nportable compact disc read-only memory (CD-ROM), a digital versatile disk (DVD), a memory stick, a floppy disk, a mechanically encoded device such as punch-cards or raised structures in a groove having instructions recorded thereon, and any suitable\ncombination of the foregoing.  A computer readable storage medium, as used herein, is not to be construed as being transitory signals per se, such as radio waves or other freely propagating electromagnetic waves, electromagnetic waves propagating through\na waveguide or other transmission media (e.g., light pulses passing through a fiber-optic cable), or electrical signals transmitted through a wire.\nComputer readable program instructions described herein can be downloaded to respective computing/processing devices from a computer readable storage medium or to an external computer or external storage device via a network, for example, the\nInternet, a local area network, a wide area network and/or a wireless network.  The network may comprise copper transmission cables, optical transmission fibers, wireless transmission, routers, firewalls, switches, gateway computers and/or edge servers. \nA network adapter card or network interface in each computing/processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium\nwithin the respective computing/processing device.\nComputer readable program instructions for carrying out operations of the present invention may be assembler instructions, instruction-set-architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, firmware\ninstructions, state-setting data, or either source code or object code written in any combination of one or more programming languages, including an object oriented programming language such as Java, Smalltalk, C++ or the like, and conventional\nprocedural programming languages, such as the \"C\" programming language or similar programming languages.  The computer readable program instructions may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software\npackage, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server.  In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area\nnetwork (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).  In some embodiments, electronic circuitry including, for example, programmable\nlogic circuitry, field-programmable gate arrays (FPGA), or programmable logic arrays (PLA) may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to personalize the electronic\ncircuitry, in order to perform aspects of the present invention.\nAspects of the present invention are described herein with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems), and computer program products according to embodiments of the invention.  It will be\nunderstood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer readable program instructions.\nThese computer readable program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute\nvia the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.  These computer readable program instructions may also be\nstored in a computer readable storage medium that can direct a computer, a programmable data processing apparatus, and/or other devices to function in a particular manner, such that the computer readable storage medium having instructions stored therein\ncomprises an article of manufacture including instructions which implement aspects of the function/act specified in the flowchart and/or block diagram block or blocks.\nThe computer readable program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other device to cause a series of operational steps to be performed on the computer, other programmable apparatus or\nother device to produce a computer implemented process, such that the instructions which execute on the computer, other programmable apparatus, or other device implement the functions/acts specified in the flowchart and/or block diagram block or blocks.\nThe flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present invention. \nIn this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of instructions, which comprises one or more executable instructions for implementing the specified logical function(s).  In some alternative\nimplementations, the functions noted in the block may occur out of the order noted in the figures.  For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse\norder, depending upon the functionality involved.  It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special\npurpose hardware-based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.\nThe illustrative embodiments may be utilized in many different types of data processing environments including search engines, QA systems, natural language processing systems, and the like.  In order to provide a context for the description of\nthe specific elements and functionality of the illustrative embodiments, FIGS. 1-3 are provided hereafter as example environments in which aspects of the illustrative embodiments may be implemented.  It should be appreciated that FIGS. 1-3 are only\nexamples and are not intended to assert or imply any limitation with regard to the environments in which aspects or embodiments of the present invention may be implemented.  Many modifications to the depicted environments may be made without departing\nfrom the spirit and scope of the present invention.\nFIGS. 1-3 are directed to describing an example Question Answering (QA) system (also referred to as a Question/Answer system or Question and Answer system), methodology, and computer program product with which the mechanisms of the illustrative\nembodiments are implemented.  As will be discussed in greater detail hereafter, the illustrative embodiments may be integrated in, augment, and extend the functionality of these QA mechanisms with regard to performing concept analysis operations, which\nmay be used with regard to identifying portions of a corpus where concepts related to concepts specified in an input question are found or with regard to identify evidence passages within the corpus when calculating confidence values associated with\ncandidate answers to an input question, for example.  In addition, the ranking of the evidence passages may be modified based on the concept analysis operations performed using the mechanisms of the illustrative embodiments by taking into account\npersonal preferences of a user as specified by a user profile that may be input with the input question and which identifies concepts of interest to the user, thereby augmenting the input concepts used as a basis for identifying the related concepts.\nSince the mechanisms of the illustrative embodiments will be described with regard to their implementation in a QA system, it is important to first have an understanding of how question and answer processing in a QA system is implemented before\ndescribing how the mechanisms of the illustrative embodiments are integrated in and augment such QA systems.  It should be appreciated that the QA mechanisms described in FIGS. 1-3 are only examples and are not intended to state or imply any limitation\nwith regard to the type of QA mechanisms with which the illustrative embodiments are implemented.  Many modifications to the example QA system shown in FIGS. 1-3 may be implemented in various embodiments of the present invention without departing from\nthe spirit and scope of the present invention.\nAs an overview, a Question Answering system (QA system) is an artificial intelligence application executing on data processing hardware that answers questions pertaining to a given subject-matter domain presented in natural language.  The QA\nsystem receives inputs from various sources including input over a network, a corpus of electronic documents or other data, data from a content creator, information from one or more content users, and other such inputs from other possible sources of\ninput.  Data storage devices store the corpus of data.  A content creator creates content in a document for use as part of a corpus of data with the QA system.  The document may include any file, text, article, or source of data for use in the QA system. For example, a QA system accesses a body of knowledge about the domain, or subject matter area, e.g., financial domain, medical domain, legal domain, etc., where the body of knowledge (knowledgebase) can be organized in a variety of configurations, e.g.,\na structured repository of domain-specific information, such as ontologies, or unstructured data related to the domain, or a collection of natural language documents about the domain.\nContent users input questions to the QA system which then answers the input questions using the content in the corpus of data by evaluating documents, sections of documents, portions of data in the corpus, or the like.  When a process evaluates\na given section of a document for semantic content, the process can use a variety of conventions to query such document from the QA system, e.g., sending the query to the QA system as a well-formed question which are then interpreted by the QA system and\na response is provided containing one or more answers to the question.  Semantic content is content based on the relation between signifiers, such as words, phrases, signs, and symbols, and what they stand for, their denotation, or connotation.  In other\nwords, semantic content is content that interprets an expression, such as by using Natural Language Processing.\nAs will be described in greater detail hereafter, the QA system receives an input question, parses the question to extract the major features of the question, including identified concepts in the input question, uses the extracted features to\nformulate queries, and then applies those queries to the corpus of data.  Based on the application of the queries to the corpus of data, the QA system generates a set of hypotheses, or candidate answers to the input question, by looking across the corpus\nof data for portions of the corpus of data that have some potential for containing a valuable response to the input question.  The QA system then performs deep analysis, e.g., English Slot Grammar (ESG) and Predicate Argument Structure (PAS) builder, on\nthe language of the input question and the language used in each of the portions of the corpus of data found during the application of the queries using a variety of reasoning algorithms (see, for example, McCord et al., \"Deep Parsing in Watson,\" IBM J.\nRes.  & Dev., vol. 56, no. 3/4, May/July 2012 for more information on deep analysis in IBM Watson.TM.).  There may be hundreds or even thousands of reasoning algorithms applied, each of which performs different analysis, e.g., comparisons, natural\nlanguage analysis, lexical analysis, or the like, and generates a score.  For example, some reasoning algorithms may look at the matching of terms and synonyms within the language of the input question and the found portions of the corpus of data.  Other\nreasoning algorithms may look at temporal or spatial features in the language, while others may evaluate the source of the portion of the corpus of data and evaluate its veracity.\nThe scores obtained from the various reasoning algorithms indicate the extent to which the potential response is inferred by the input question based on the specific area of focus of that reasoning algorithm.  Each resulting score is then\nweighted against a statistical model.  The statistical model captures how well the reasoning algorithm performed at establishing the inference between two similar passages for a particular domain during the training period of the QA system.  The\nstatistical model is used to summarize a level of confidence that the QA system has regarding the evidence that the potential response, i.e. candidate answer, is inferred by the question.  This process is repeated for each of the candidate answers until\nthe QA system identifies candidate answers that surface as being significantly stronger than others and thus, generates a final answer, or ranked set of answers, for the input question.\nAs mentioned above, QA systems and mechanisms operate by accessing information from a corpus of data or information (also referred to as a corpus of content), analyzing it, and then generating answer results based on the analysis of this data. \nAccessing information from a corpus of data typically includes: a database query that answers questions about what is in a collection of structured records, and a search that delivers a collection of document links in response to a query against a\ncollection of unstructured data (text, markup language, etc.).  Conventional question answering systems are capable of generating answers based on the corpus of data and the input question, verifying answers to a collection of questions for the corpus of\ndata, correcting errors in digital text using a corpus of data, and selecting answers to questions from a pool of potential answers, i.e. candidate answers.\nContent creators, such as article authors, electronic document creators, web page authors, document database creators, and the like, determine use cases for products, solutions, and services described in such content before writing their\ncontent.  Consequently, the content creators know what questions the content is intended to answer in a particular topic addressed by the content.  Categorizing the questions, such as in terms of roles, type of information, tasks, or the like, associated\nwith the question, in each document of a corpus of data allows the QA system to more quickly and efficiently identify documents containing content related to a specific query.  The content may also answer other questions that the content creator did not\ncontemplate that may be useful to content users.  The questions and answers may be verified by the content creator to be contained in the content for a given document.  These capabilities contribute to improved accuracy, system performance, machine\nlearning, and confidence of the QA system.  Content creators, automated tools, or the like, annotate or otherwise generate metadata for providing information useable by the QA system to identify these question and answer attributes of the content.\nOperating on such content, the QA system generates answers for input questions using a plurality of intensive analysis mechanisms which evaluate the content to identify the most probable answers, i.e. candidate answers, for the input question. \nIn accordance with the mechanisms of the illustrative embodiments, these intensive analysis mechanisms may utilize a large sparse matrix that represents the concepts and their relationships within the documents of the corpus as a means by which to\nidentify concepts related to concepts specified in an input question, user profile (such as in a personalized ranking process), document being assessed for inclusion into the corpus, or any other concept analysis based operation.\nAs noted above, the concept analysis operation generally uses a large sparse matrix representing the network/graph of concepts and their relationships obtained from a knowledge source.  The \"knowledge source\" is any electronic system or data\nstructure that acts as a source of information and knowledge regarding known concepts and their relationships.  This \"knowledge source\" may be a corpus of documents provided in a natural language format or predefined structured format, portions of text\nfrom various text sources such as postings to web sites, electronic mail messages, or any other source of textual content, web pages, or the like.  In some cases, these \"knowledge sources\" may comprise databases of information provided in a structured\nformat.  Any source of information and knowledge that provides information upon which a network/graph of concepts and the relationships may be generated is intended to be within the spirit and scope of the term \"knowledge source.\"\nAn \"information source\" as the term is used herein refers to the source of concepts for which relationship information is sought and whose concepts are the source for generating at least one vector to be used in a matrix operation.  The\n\"information source\" may be an input document or set of input documents having natural language or structured text, an input natural language question, phrase, search query, user profile, and/or the like.  In one illustrative embodiment, the \"information\nsource\" is an input question to a QA system.  In another illustrative embodiment, the \"information source\" is a document to be added to a corpus of information upon which the QA system operates.  In other illustrative embodiments, the \"information\nsource\" is any other source of textual input, such as a search query.  Regardless of the particular implementation, the \"information source\" provides the text from which one or more concepts may be extracted for use in generating at least one vector to\nbe used in performing a matrix operation as part of a concept analysis operation executed within an accelerator.\nIn general, concept analysis operations involve two main stages: indexing and related concept identification.  With regard to the indexing stage, for example, assume that a knowledge source is an online encyclopedia comprising a large number of\nweb pages, e.g., Wikipedia.  From this knowledge source, a list of concepts N is extracted and a sparse relationship matrix N*N is computed where an entry (row i, column j) in the matrix is non-zero, e.g., \"1\", if concept i is related to concept j. For\nexample, the concept \"information theory\" is a branch of \"electrical engineering\" such that in the network/graph an edge would connect \"information theory\" to \"electrical engineering\" and correspondingly, a entry in (i,j) corresponding to \"information\ntheory\" and \"electrical engineering\" is set to 1.  Performing this process for each concept in the knowledge source results in an indexed knowledge source represented by an N*N matrix in which non-zero entries represent relationships between concepts. \nThe values in the entries may be set to values indicative of the strengths of the relationships between the concepts, with these strengths being calculated in many different ways based on various criteria.\nDuring the related concept identification stage, for each information source, concept analysis extracts a set of M concepts.  Using the knowledge source relationship matrix generated in the indexing stage, related concepts for each of the input\nconcepts M are computed.  This is typically done using a matrix vector multiplication operation, as discussed hereafter, or other sparse matrix (N*N)--dense matrix (N*M) multiplication operation (where the \"dense matrix\" is a set of vectors in matrix\nform and each vector may be processed separately to generate partial products which can later be combined).  In some embodiments, this information source may comprise a personal profile of a user which indicates concept preferences for the user such that\na personalized ranking process is performed as part of this related concept identification stage, i.e. concepts of interest to the user are identified by way of the concept analysis operation involving the matrix vector multiplication operation.  This\nconcept analysis may involve multiple iterations of these multiplication operations with the resultant matrix being post-processed to generate an N-element vector that represents how concepts from the information source relate to all other concepts in\nthe knowledge source.\nIt should be appreciated that the value of M can vary significantly, e.g., from tens to thousands of concepts.  The size of the intermediate data, i.e. the result of the matrix vector multiplications, depends on the value of M. To limit the\nintermediate data size, as discussed hereafter, the illustrative embodiments utilize batches, e.g., batches of size 32 (32 input vectors) in some embodiments, but may be of different batch sizes depending on the desired implementation.\nThe mechanisms of the illustrative embodiments accelerate the process of identifying related concepts, such as in the related concept identification stage of a concept analysis operation, by performing concept analysis operations using one or\nmore accelerators, as described hereafter.  The related concepts output vector generated by the one or more accelerators may be used to generate the candidate answers and rank these candidate answers in a QA system, or in other implementations, such as a\nsearch engine, rank the search results returned to a user's search query, for example.  In some illustrative embodiments, this process may be utilized to provide a personalized ranking operation as mentioned above, in which concepts of interest to a\nparticular user as specified by a user's profile (which may be provided as part of the information source) are identified.  The most probable answers are output as a ranked listing of candidate answers ranked according to their relative scores or\nconfidence measures calculated during evaluation of the candidate answers, as a single final answer having a highest ranking score or confidence measure, or which is a best match to the input question, or a combination of ranked listing and final answer.\nFIG. 1 depicts a schematic diagram of one illustrative embodiment of a question/answer creation (QA) system 100 in a computer network 102.  One example of a question/answer generation which may be used in conjunction with the principles\ndescribed herein is described in U.S.  Patent Application Publication No. 2011/0125734, which is herein incorporated by reference in its entirety.  The QA system 100 is implemented on one or more computing devices 104 (comprising one or more processors\nand one or more memories, and potentially any other computing device elements generally known in the art including buses, storage devices, communication interfaces, and the like) connected to the computer network 102.  The network 102 includes multiple\ncomputing devices 104 in communication with each other and with other devices or components via one or more wired and/or wireless data communication links, where each communication link comprises one or more of wires, routers, switches, transmitters,\nreceivers, or the like.  The QA system 100 and network 102 enables question/answer (QA) generation functionality for one or more QA system users via their respective computing devices 110-112.  Other embodiments of the QA system 100 may be used with\ncomponents, systems, sub-systems, and/or devices other than those that are depicted herein.\nThe QA system 100 is configured to implement a QA system pipeline 108 that receives inputs from various sources.  For example, the QA system 100 receives input from the network 102, a corpus of electronic documents 106, QA system users, and/or\nother data and other possible sources of input.  In one embodiment, some or all of the inputs to the QA system 100 are routed through the network 102.  The various computing devices 104 on the network 102 include access points for content creators and QA\nsystem users.  Some of the computing devices 104 include devices for a database storing the corpus of data 106 (which is shown as a separate entity in FIG. 1 for illustrative purposes only).  Portions of the corpus of data 106 may also be provided on one\nor more other network attached storage devices, in one or more databases, or other computing devices not explicitly shown in FIG. 1.  The network 102 includes local network connections and remote connections in various embodiments, such that the QA\nsystem 100 may operate in environments of any size, including local and global, e.g., the Internet.\nIn one embodiment, the content creator creates content in a document of the corpus of data 106 for use as part of a corpus of data with the QA system 100.  The document includes any file, text, article, or source of data for use in the QA system\n100.  QA system users access the QA system 100 via a network connection or an Internet connection to the network 102, and input questions to the QA system 100 that are answered by the content in the corpus of data 106.  In one embodiment, the questions\nare formed using natural language.  The QA system 100 parses and interprets the question, and provides a response to the QA system user, e.g., QA system user 110, containing one or more answers to the question.  In some embodiments, the QA system 100\nprovides a response to users in a ranked list of candidate answers while in other illustrative embodiments, the QA system 100 provides a single final answer or a combination of a final answer and ranked listing of other candidate answers.\nThe QA system 100 implements a QA system pipeline 108 which comprises a plurality of stages for processing an input question and the corpus of data 106.  The QA system pipeline 108 generates answers for the input question based on the processing\nof the input question and the corpus of data 106.  The QA system pipeline 108 will be described in greater detail hereafter with regard to FIG. 3.\nIn some illustrative embodiments, the QA system 100 may be the IBM Watson.TM.  QA system available from International Business Machines Corporation of Armonk, N.Y., which is augmented with the mechanisms of the illustrative embodiments described\nhereafter.  As outlined previously, the IBM Watson.TM.  QA system receives an input question which it then parses to extract the major features of the question, that in turn are then used to formulate queries that are applied to the corpus of data. \nBased on the application of the queries to the corpus of data, a set of hypotheses, or candidate answers to the input question, are generated by looking across the corpus of data for portions of the corpus of data that have some potential for containing\na valuable response to the input question.  The IBM Watson.TM.  QA system then performs deep analysis on the language of the input question and the language used in each of the portions of the corpus of data found during the application of the queries\nusing a variety of reasoning algorithms.  The scores obtained from the various reasoning algorithms are then weighted against a statistical model that summarizes a level of confidence that the IBM Watson.TM.  QA system has regarding the evidence that the\npotential response, i.e. candidate answer, is inferred by the question.  This process is be repeated for each of the candidate answers to generate ranked listing of candidate answers which may then be presented to the user that submitted the input\nquestion, or from which a final answer is selected and presented to the user.  More information about the IBM Watson.TM.  QA system may be obtained, for example, from the IBM Corporation website, IBM Redbooks, and the like.  For example, information\nabout the IBM Watson.TM.  QA system can be found in Yuan et al., \"Watson and Healthcare,\" IBM developerWorks, 2011 and \"The Era of Cognitive Systems: An Inside Look at IBM Watson and How it Works\" by Rob High, IBM Redbooks, 2012.\nAs shown in FIG. 1, the QA system 100 utilizes one or more accelerators 120 which accelerate concept analysis operations performed by the QA system 100.  The one or more accelerators 120 preferably are provided such that they operate in\nconjunction with, and in parallel with, the operations performed by the QA system's pipeline.  The accelerators 120 comprise processors and memory external to the primary QA system 100 processors and which operate as special purpose processors or service\nprocessors for the prior QA system 100 processors.  In some illustrative embodiments the accelerators 120 are graphics processing units (GPUs) that are configured with a special purpose kernel for performing concept analysis operations on behalf of the\nQA system 100.  As such, the GPUs may be dynamically switched between graphics processing and concept analysis operations by switching kernels in response to the QA system 100 submitting concept analysis operation tasks to the GPUs and thus, switching\nmodes of operation.  This switch may be facilitated by an instruction from the QA system 100, the setting of a bit that is checked by the GPU, or any other mechanism that may be used for switching the mode of operation of a GPU from graphics processing\nto another mode of operation.\nIn operation in a concept analysis mode of operation, the one or more accelerators 120 each receive one or more representations of a matrix 122 representing the known concepts and relationships between concepts previously extracted from a corpus\nthrough a corpus ingestion operation by the QA system 100.  This process of generating a network/graph of concepts and relationships between concepts from natural language content is generally known in the art and thus, a more detailed description is not\nprovided herein.  Moreover, the network/graph is then represented as a matrix in a manner generally known in the art.  Given the matrix, mechanisms are utilized to re-organize the matrix to concentrate the non-zero values of the matrix along the diagonal\nof the matrix.  This process may utilize well known matrix re-organizing algorithms, such as the Cuthill-McKee algorithm, to perform such re-organization.  In some illustrative embodiments, this matrix re-organization may be performed by implementing the\nmatrix processing mechanisms described in co-pending and commonly assigned U.S.  patent application Ser.  No. 14/611,297, which utilize clustering to concentrate the non-zero values into clusters and organize the clusters along the diagonal of the\nmatrix.\nThe matrix representation provided to the accelerators 120 may be previously generated and stored by the QA system 100 such that it is not re-calculated each time an input question is processed by the QA system 100.  The matrix representation\n122 is only re-calculated when there are changes to the corpus upon which the QA system 100 operates, in response to an authorized user's request, at periodic intervals, or upon occurrence of another designated event.  The accelerators 120 may receive\nmultiple representations of the matrix that are optimized for different sparsities (or densities) of the vector upon which the matrix operations are performed within the accelerators 120.  For example, as described in commonly assigned and co-pending\nU.S.  patent application Ser.  No. 14/635,007, a CSR and CSC representation of the matrix may be provided with different iterations of the matrix operation performed within the accelerators using different ones of these representations based on the\nsparsity (or density) of the vector with which the matrix is being multiplied as part of a matrix vector multiplication operation.  As iterations are executed, the vector's density may increase causing a need to switch from one representation to another\nto maximize efficiency of the operation.\nThe accelerators 120 also receive, from the QA system 100, a listing of concepts 124 extracted from the natural language input question, or other information source such as a document for inclusion in the corpus, a user profile as part of a\npersonalized ranking operation, or the like, that is input to the QA system 100.  The listing of concepts 124 are input to the accelerator 120 which operates on the listing of concepts to construct an input vector of extracted concepts for use in\nperforming the concept analysis operation.  In some illustrative embodiments, this concept analysis operation involves multiple iterations of a matrix vector multiplication in which initially, the input vector generated by the accelerator is multiplied\nby a representation of the matrix, e.g., the CSC representation, and the resulting vector output is used as the input vector for a next iteration.  Based on the sparsity (or density) of the vector being multiplied with the matrix, the representation of\nthe matrix may be selected so as to maximize efficiency of the operation, e.g., switching from using the CSC representation during an initial set of iterations to using the CSR representation during a latter subsequent set of iterations.  Iterations\ncontinue until it is determined that the output vector has converged, i.e. changes to the output vector are below a specified threshold amount of change.  Once converged, the resulting output vector represents the set of concepts related to the concepts\nextracted from the input question as well as the strength of the relationships (represented by the values in the vector elements of the output vector).\nThus, the work for generating an output vector of related concepts to the extracted concepts from an input question, or other information source, is offloaded to the accelerators 120 which operate in parallel to the host QA system 100.  The host\nQA system 100 need only extract the concepts from the input question or other information source (assumed to be an input question for purposes of illustration), which is an operation that the QA system 100 performs already, and provide those concepts to\nthe accelerators 120.  The host QA system 100 infrequently generates one or more concept matrix representations as part of an ingestion process or other pre-processor processing process, with the frequency being relatively seldom due to infrequent\nchanges to the corpus requiring updates of the concept matrix representations.  Thus, the host QA system 100 is offloaded such that the accelerators 120 accelerate the process of generating the output vector of related concepts.\nBecause the accelerators 120 have limited memory size, the processing of concept analysis operations may be performed in a batched manner, with batches of input vectors representing one or more of the extracted concepts from the input question,\nuser profile, or other information source, such that the accelerators utilize various threads for executing the concept analysis operation on batches representing portions of the input vector.  Each batch may have a size (number of input vectors) set\nbased on the size of the memory of the accelerator 120, for example.  In one illustrative embodiment, the accelerator 120 is a GPU having a memory size capable of handling a batch size of 32 input vectors.  In one illustrative embodiment, each input\nvector has one non-zero entry corresponding to an extracted concept from the information source.  In other illustrative embodiments, each input vector in the batch may have one or more non-zero entries.  The benefit of utilizing a single non-zero entry\nin the input vector is that it greatly simplifies the matrix-vector multiplication operation performed with regard to that vector such that it is simply a column or row lookup of the non-zero entries in the corresponding column/row in the matrix.\nThe results of the execution of the batches may be combined by the accelerator 120 internally to generate the resulting output vector of related concepts.  Combining of partial products of a matrix-vector multiplication is generally known in the\nart and thus, need not be explained in detail further.  However, it is important to note that the accelerator 120 does not store any intermediate results to the host QA system 100 memory or an external storage system but instead maintains the data\ninternal to the accelerator 120 until the output vector of related concepts is output back to the QA system 100 for use by the QA system pipeline.\nIt should be appreciated that in some illustrative embodiments, a plurality of accelerators 120 may be utilized with each accelerator operating on a portion of the concept matrix representation 122 and/or listing of concepts 124.  In such an\nembodiment, results from the various accelerators 120 may be combined by a combination stage of processing in one of the accelerators 120 (master accelerator), an external mechanisms to the accelerators (not shown), or in the host QA system 100.\nFIG. 2 is a block diagram of an example data processing system in which aspects of the illustrative embodiments are implemented.  Data processing system 200 is an example of a computer, such as server 104 or client 110 in FIG. 1, in which\ncomputer usable code or instructions implementing the processes for illustrative embodiments of the present invention are located.  In one illustrative embodiment, FIG. 2 represents a server computing device, such as a server 104, which, which implements\na QA system 100 and QA system pipeline 108 augmented to include the additional mechanisms of the illustrative embodiments described hereafter.\nIn the depicted example, data processing system 200 employs a hub architecture including north bridge and memory controller hub (NB/MCH) 202 and south bridge and input/output (I/O) controller hub (SB/ICH) 204.  Processing unit 206, main memory\n208, and graphics processor 210 are connected to NB/MCH 202.  Graphics processor 210 is connected to NB/MCH 202 through an accelerated graphics port (AGP).  The graphics processor 210 may be a GPU which may be used as an accelerator in the context of the\npresent description, for example.\nIn the depicted example, local area network (LAN) adapter 212 connects to SB/ICH 204.  Audio adapter 216, keyboard and mouse adapter 220, modem 222, read only memory (ROM) 224, hard disk drive (HDD) 226, CD-ROM drive 230, universal serial bus\n(USB) ports and other communication ports 232, and PCI/PCIe devices 234 connect to SB/ICH 204 through bus 238 and bus 240.  PCI/PCIe devices may include, for example, Ethernet adapters, add-in cards, and PC cards for notebook computers.  PCI uses a card\nbus controller, while PCIe does not.  ROM 224 may be, for example, a flash basic input/output system (BIOS).\nHDD 226 and CD-ROM drive 230 connect to SB/ICH 204 through bus 240.  HDD 226 and CD-ROM drive 230 may use, for example, an integrated drive electronics (IDE) or serial advanced technology attachment (SATA) interface.  Super I/O (SIO) device 236\nis connected to SB/ICH 204.\nAn operating system runs on processing unit 206.  The operating system coordinates and provides control of various components within the data processing system 200 in FIG. 2.  As a client, the operating system is a commercially available\noperating system such as Microsoft.RTM.  Windows 8.RTM..  An object-oriented programming system, such as the Java.TM.  programming system, may run in conjunction with the operating system and provides calls to the operating system from Java.TM.  programs\nor applications executing on data processing system 200.\nAs a server, data processing system 200 may be, for example, an IBM.RTM.  eServer.TM.  System P.RTM.  computer system, running the Advanced Interactive Executive (AIX.RTM.) operating system or the LINUX.RTM.  operating system.  Data processing\nsystem 200 may be a symmetric multiprocessor (SMP) system including a plurality of processors in processing unit 206.  Alternatively, a single processor system may be employed.\nInstructions for the operating system, the object-oriented programming system, and applications or programs are located on storage devices, such as HDD 226, and are loaded into main memory 208 for execution by processing unit 206.  The processes\nfor illustrative embodiments of the present invention are performed by processing unit 206 using computer usable program code, which is located in a memory such as, for example, main memory 208, ROM 224, or in one or more peripheral devices 226 and 230,\nfor example.\nA bus system, such as bus 238 or bus 240 as shown in FIG. 2, is comprised of one or more buses.  Of course, the bus system may be implemented using any type of communication fabric or architecture that provides for a transfer of data between\ndifferent components or devices attached to the fabric or architecture.  A communication unit, such as modem 222 or network adapter 212 of FIG. 2, includes one or more devices used to transmit and receive data.  A memory may be, for example, main memory\n208, ROM 224, or a cache such as found in NB/MCH 202 in FIG. 2.\nThose of ordinary skill in the art will appreciate that the hardware depicted in FIGS. 1 and 2 may vary depending on the implementation.  Other internal hardware or peripheral devices, such as flash memory, equivalent non-volatile memory, or\noptical disk drives and the like, may be used in addition to or in place of the hardware depicted in FIGS. 1 and 2.  Also, the processes of the illustrative embodiments may be applied to a multiprocessor data processing system, other than the SMP system\nmentioned previously, without departing from the spirit and scope of the present invention.\nMoreover, the data processing system 200 may take the form of any of a number of different data processing systems including client computing devices, server computing devices, a tablet computer, laptop computer, telephone or other communication\ndevice, a personal digital assistant (PDA), or the like.  In some illustrative examples, data processing system 200 may be a portable computing device that is configured with flash memory to provide non-volatile memory for storing operating system files\nand/or user-generated data, for example.  Essentially, data processing system 200 may be any known or later developed data processing system without architectural limitation.\nFIG. 3 illustrates a QA system pipeline for processing an input question in accordance with one illustrative embodiment.  The QA system pipeline of FIG. 3 may be implemented, for example, as QA system pipeline 108 of QA system 100 in FIG. 1.  It\nshould be appreciated that the stages of the QA system pipeline shown in FIG. 3 are implemented as one or more software engines, components, or the like, which are configured with logic for implementing the functionality attributed to the particular\nstage.  Each stage is implemented using one or more of such software engines, components or the like.  The software engines, components, etc. are executed on one or more processors of one or more data processing systems or devices and utilize or operate\non data stored in one or more data storage devices, memories, or the like, on one or more of the data processing systems.  The QA system pipeline of FIG. 3 is augmented, for example, in one or more of the stages to implement the improved mechanism of the\nillustrative embodiments described hereafter, additional stages may be provided to implement the improved mechanism, or separate logic from the pipeline 300 may be provided for interfacing with the pipeline 300 and implementing the improved functionality\nand operations of the illustrative embodiments.\nAs shown in FIG. 3, the QA system pipeline 300 comprises a plurality of stages 310-380 through which the QA system operates to analyze an input question and generate a final response.  In an initial question input stage 310, the QA system\nreceives an input question that is presented in a natural language format.  That is, a user inputs, via a user interface, an input question for which the user wishes to obtain an answer, e.g., \"Who are Washington's closest advisors?\" In response to\nreceiving the input question, the next stage of the QA system pipeline 300, i.e. the question and topic analysis stage 320, parses the input question using natural language processing (NLP) techniques to extract major features from the input question,\nand classify the major features according to types, e.g., names, dates, or any of a plethora of other defined topics.  For example, in the example question above, the term \"who\" may be associated with a topic for \"persons\" indicating that the identity of\na person is being sought, \"Washington\" may be identified as a proper name of a person with which the question is associated, \"closest\" may be identified as a word indicative of proximity or relationship, and \"advisors\" may be indicative of a noun or\nother language topic.\nIn addition, the extracted major features include key words and phrases classified into question characteristics, such as the focus of the question, the lexical answer type (LAT) of the question, and the like.  As referred to herein, a lexical\nanswer type (LAT) is a word in, or a word inferred from, the input question that indicates the type of the answer, independent of assigning semantics to that word.  For example, in the question \"What maneuver was invented in the 1500s to speed up the\ngame and involves two pieces of the same color?,\" the LAT is the string \"maneuver.\" The focus of a question is the part of the question that, if replaced by the answer, makes the question a standalone statement.  For example, in the question \"What drug\nhas been shown to relieve the symptoms of ADD with relatively few side effects?,\" the focus is \"drug\" since if this word were replaced with the answer, e.g., the answer \"Adderall\" can be used to replace the term \"drug\" to generate the sentence \"Adderall\nhas been shown to relieve the symptoms of ADD with relatively few side effects.\" The focus often, but not always, contains the LAT.  On the other hand, in many cases it is not possible to infer a meaningful LAT from the focus.\nReferring again to FIG. 3, the identified major features are then used during the question decomposition stage 330 to decompose the question into one or more queries that are applied to the corpora of data/information 345 in order to generate\none or more hypotheses.  The queries are generated in any known or later developed query language, such as the Structure Query Language (SQL), or the like.  The queries are applied to one or more databases storing information about the electronic texts,\ndocuments, articles, websites, and the like, that make up the corpora of data/information 345.  That is, these various sources themselves, different collections of sources, and the like, represent a different corpus 347 within the corpora 345.  There may\nbe different corpora 347 defined for different collections of documents based on various criteria depending upon the particular implementation.  For example, different corpora may be established for different topics, subject matter categories, sources of\ninformation, or the like.  As one example, a first corpus may be associated with healthcare documents while a second corpus may be associated with financial documents.  Alternatively, one corpus may be documents published by the U.S.  Department of\nEnergy while another corpus may be IBM Redbooks documents.  Any collection of content having some similar attribute may be considered to be a corpus 347 within the corpora 345.\nThe queries are applied to one or more databases storing information about the electronic texts, documents, articles, websites, and the like, that make up the corpus of data/information, e.g., the corpus of data 106 in FIG. 1.  The queries are\napplied to the corpus of data/information at the hypothesis generation stage 340 to generate results identifying potential hypotheses for answering the input question, which can then be evaluated.  That is, the application of the queries results in the\nextraction of portions of the corpus of data/information matching the criteria of the particular query.  These portions of the corpus are then analyzed and used, during the hypothesis generation stage 340, to generate hypotheses for answering the input\nquestion.  These hypotheses are also referred to herein as \"candidate answers\" for the input question.  For any input question, at this stage 340, there may be hundreds of hypotheses or candidate answers generated that may need to be evaluated.\nThe QA system pipeline 300, in stage 350, then performs a deep analysis and comparison of the language of the input question and the language of each hypothesis or \"candidate answer,\" as well as performs evidence scoring to evaluate the\nlikelihood that the particular hypothesis is a correct answer for the input question.  As mentioned above, this involves using a plurality of reasoning algorithms, each performing a separate type of analysis of the language of the input question and/or\ncontent of the corpus that provides evidence in support of, or not in support of, the hypothesis.  Each reasoning algorithm generates a score based on the analysis it performs which indicates a measure of relevance of the individual portions of the\ncorpus of data/information extracted by application of the queries as well as a measure of the correctness of the corresponding hypothesis, i.e. a measure of confidence in the hypothesis.  There are various ways of generating such scores depending upon\nthe particular analysis being performed.  In generally, however, these algorithms look for particular terms, phrases, or patterns of text that are indicative of terms, phrases, or patterns of interest and determine a degree of matching with higher\ndegrees of matching being given relatively higher scores than lower degrees of matching.\nThus, for example, an algorithm may be configured to look for the exact term from an input question or synonyms to that term in the input question, e.g., the exact term or synonyms for the term \"movie,\" and generate a score based on a frequency\nof use of these exact terms or synonyms.  In such a case, exact matches will be given the highest scores, while synonyms may be given lower scores based on a relative ranking of the synonyms as may be specified by a subject matter expert (person with\nknowledge of the particular domain and terminology used) or automatically determined from frequency of use of the synonym in the corpus corresponding to the domain.  Thus, for example, an exact match of the term \"movie\" in content of the corpus (also\nreferred to as evidence, or evidence passages) is given a highest score.  A synonym of movie, such as \"motion picture\" may be given a lower score but still higher than a synonym of the type \"film\" or \"moving picture show.\" Instances of the exact matches\nand synonyms for each evidence passage may be compiled and used in a quantitative function to generate a score for the degree of matching of the evidence passage to the input question.\nThus, for example, a hypothesis or candidate answer to the input question of \"What was the first movie?\" is \"The Horse in Motion.\" If the evidence passage contains the statements \"The first motion picture ever made was `The Horse in Motion` in\n1878 by Eadweard Muybridge.  It was a movie of a horse running,\" and the algorithm is looking for exact matches or synonyms to the focus of the input question, i.e. \"movie,\" then an exact match of \"movie\" is found in the second sentence of the evidence\npassage and a highly scored synonym to \"movie,\" i.e. \"motion picture,\" is found in the first sentence of the evidence passage.  This may be combined with further analysis of the evidence passage to identify that the text of the candidate answer is\npresent in the evidence passage as well, i.e. \"The Horse in Motion.\" These factors may be combined to give this evidence passage a relatively high score as supporting evidence for the candidate answer \"The Horse in Motion\" being a correct answer.\nIt should be appreciated that this is just one simple example of how scoring can be performed.  Many other algorithms of various complexity may be used to generate scores for candidate answers and evidence without departing from the spirit and\nscope of the present invention.\nIn the synthesis stage 360, the large number of scores generated by the various reasoning algorithms are synthesized into confidence scores or confidence measures for the various hypotheses.  This process involves applying weights to the various\nscores, where the weights have been determined through training of the statistical model employed by the QA system and/or dynamically updated.  For example, the weights for scores generated by algorithms that identify exactly matching terms and synonym\nmay be set relatively higher than other algorithms that are evaluating publication dates for evidence passages.  The weights themselves may be specified by subject matter experts or learned through machine learning processes that evaluate the\nsignificance of characteristics evidence passages and their relative importance to overall candidate answer generation.\nThe weighted scores are processed in accordance with a statistical model generated through training of the QA system that identifies a manner by which these scores may be combined to generate a confidence score or measure for the individual\nhypotheses or candidate answers.  This confidence score or measure summarizes the level of confidence that the QA system has about the evidence that the candidate answer is inferred by the input question, i.e. that the candidate answer is the correct\nanswer for the input question.\nThe resulting confidence scores or measures are processed by a final confidence merging and ranking stage 370 which compares the confidence scores and measures to each other, compares them against predetermined thresholds, or performs any other\nanalysis on the confidence scores to determine which hypotheses/candidate answers are the most likely to be the correct answer to the input question.  The hypotheses/candidate answers are ranked according to these comparisons to generate a ranked listing\nof hypotheses/candidate answers (hereafter simply referred to as \"candidate answers\").  From the ranked listing of candidate answers, at stage 380, a final answer and confidence score, or final set of candidate answers and confidence scores, are\ngenerated and output to the submitter of the original input question via a graphical user interface or other mechanism for outputting information.\nAs shown in FIG. 3, the QA system pipeline 300 may offload processing of the extracted concepts 392, which are part of the extracted features generated as part of stage 320, to one or more accelerators 390.  The one or more accelerators 390 also\nreceive corpus matrix representations 394 from the host QA system.  These corpus matrix representations 394 are representations of the concepts and relationships between concepts previously generated by the QA system as part of a corpus ingestion\noperation.  The corpus matrix representations 394, in one illustrative embodiment, comprise a CSC and a CSR representation.  While multiple representations are utilized in the illustrative embodiments, it should be appreciated that the optimizations of\nusing different representations for different densities of vectors as described in co-pending U.S.  patent application Ser.  No. 14/635,007 is not required and a single concept matrix representation may be utilized instead.\nThe accelerators 390 generate input vectors based on the received extracted concepts 392 from the input question 310 and perform concept analysis operations on the one or more corpus matrix representations 394 with the input vector.  The\naccelerators 390 output the output vector of related concepts and strengths of relationships to the hypothesis generation stage 340 which utilizes the output vector to generate candidate answers to the input question 310.  For example, the output vector\nspecifies what the related concepts are to the concepts in the input question and/or user profile (if a personalize ranking operation is being performed) and thus, similar vectors associated with documents within the corpus may be analyzed to identify if\nthey have non-zero values for the same concepts as the output vector for the current information source.  If so, these documents may be ranked according to the non-zero values such that the higher valued (and stronger related) documents for the related\nconcepts in the output vector for the information source are ranked higher when generating candidate answers.\nIt should be noted that while the above illustrative embodiments are described with regard to the use of the concept analysis operations, with acceleration provided by the accelerators of the illustrative embodiments, to identify concepts in the\nmatrix (network/graph of the corpus) that are related to concepts extracted from an input question, the present invention is not limited to such.  Rather, any operation where concept analysis and identification of related concepts using matrix operations\nmay implement the mechanisms of the illustrative embodiments without departing from the spirit and scope of the illustrative embodiments.  For example, rather than the input vector representing concepts extracted from an input question submitted to the\nQA system, the extracted concepts and resulting input vector may be obtained from other documents, web pages, portions of electronic natural language content, or the like.  As one example, when ingesting or adding documents to a corpus, the mechanisms of\nthe illustrative embodiments may be used to analyze and extract concepts from these documents, identify the related concepts already present in the matrix and expand the matrix and network/graph to include the addition of the new documents to the corpus,\ne.g., by adding nodes and edges, modifying edge weights, or the like.  Moreover, during the ingestion process, this identification of a vector of related concepts may be performed with regard to each document, web page, portion of natural language\ncontent, or the like, so as to associate a vector of related concepts with that portion of the knowledge source.  The vectors associated with the portions of the knowledge source may then be used to quickly identify the concepts with which those portions\nof the knowledge source correspond for purposes of selecting those portions for handling search queries, input questions, or other requests for information, such as personalize ranking operations, by identifying vectors having non-zero values\ncorresponding to the concepts specified in the search query, input question, or requests.  The vectors may be used to modifying the ranking of these portions of the knowledge source, or answers/results returned based of these portions of the knowledge\nsource, according to the strengths of the relations with the related concepts specified in the associated vectors.  Of course additional processing of the portions of the knowledge source may be performed once the portions are identified as being related\nto concepts related to those extracted from the information source, e.g., further natural language processing to evaluate the way in which the concepts are addressed within the portion of the knowledge source, etc.\nAs mentioned above, the accelerators 390 accelerate the performance of the concept analysis operation by offloading the host QA system and executing these concept analysis operations in parallel with other operations being performed by the host\nQA system.  It should be appreciated that in offloading the host QA system, the accelerators 390 minimize data communications between the host QA system and the accelerators 390.  To further illustrate the benefits and operational differences between the\nuser of accelerators in accordance with the mechanisms of the illustrative embodiments, and a host system based concept analysis operation, reference is now made to FIGS. 4 and 5.\nFIG. 4 is an example diagram illustrating one approach to performing concept analysis operations using a host system and host system storage.  As shown in FIG. 4, with a host based system for performing concept analysis operations, the host\nsystem performs all of the operations for performing the concept analysis operations and must store the intermediate results to main memory or a host system storage before performing post processing to generate the output vector.  That is, as shown in\nFIG. 4, the host system 400 performs the initial concept extraction from the information source 410 (e.g., input question, document, or the like) and generates an initial input vector 412 that is provided as input to a concept analysis operation engine\n420.  In addition, the host system 400 either generates of retrieves the matrix data structure 414 for the corpus and inputs it to the concept analysis operation engine 420 as well.  The concept analysis operation engine 420 comprises logic that operates\non the input vector 412 and matrix data structure 414 to perform concept analysis operations involving iterations of matrix vector multiplication operations to identify concepts in the matrix data structure 414 that are related to the concepts specified\nin the input vector 412.\nIt should be appreciated that the input vector 412 may comprise any number of identified concepts and in general can range from 20 to over 3000 identified concepts.  In some illustrative embodiments, the input vector 412 may be one of a\nplurality of input vectors 412 that together comprise an N*M input matrix.  Each input vector 412 of the N*M input matrix may be handled separately during concept analysis operations as separate matrix vector multiplication operations, for example.  The\nmatrix data structure 414 preferably represents an N*N square sparse matrix which may comprise millions of concepts and their relationships (edges).\nIt has been observed that the concept analysis operation engine 420 may require multiple iterations to perform the concept analysis operation.  In general, the concept analysis operation uses approximately 5 iterations of the matrix vector\nmultiplication operations to achieve a converged result, although more or less iterations may be used under certain circumstances.  With the matrix data structure 414 representing millions of concepts and their relationships, and the input vector(s) 412\nrepresenting potentially thousands of extracted concepts, the processing resources required to perform these multiple iterations is quite substantial.\nThe results generated by the concept analysis operation engine 420 comprise one or more output vectors specifying the concepts in the matrix that are related to the concepts in the input vector.  Each non-zero value in the one or more output\nvectors indicates a related concept.  The value itself is indicative of the strength of the relationship between the concepts.  These values in the output vector(s) are normalized by the normalization engine 430 with the result being stored in the file\nsystem of the host system storage 440.  The result is stored in the file system of the host system storage 440 because existing mechanisms are not designed for use with limited size memories and the intermediate data structures generated by the concept\nanalysis operation can be very large due to the large scale input matrix and vector(s).\nAs part of a post processing, the post processing engine 450 retrieves the normalized output vector results stored in the host system storage 440 and performs a ranking operation on the output vector results.  The ranking operation essentially\nranks the concepts according to their strength values in the output vector such that the highest ranked concepts are ranked higher than the other concepts.  The post processing engine 450 then outputs a final N-element output vector 460 representing a\nranked listing of the concepts related to the concepts extracted from the information source.  This N-element output vector 460 may be associated with the information source, such as in a metadata data structure associated with the information source,\nintegrated into the data structure of the information source, stored in a separate data structure linked with the information source, or the like.  For example, if the information source is a document that is being added to a corpus of documents, the\nN-element vector generated by the accelerator may be stored in association with this document for later use when searching the corpus for documents having certain related concepts.\nIt should be appreciated that in some illustrative embodiments, this concept analysis operation may be performed with regard to each portion of information in a corpus of information so that each portion may have an associated N-element vector\n(N being the number of concepts).  This facilitates different operations for using the N-element vector to identify related portions of information and performing searching of the corpus.  For example, calculations may be performed on N-element vectors\nof different portions of information, e.g., documents, in the corpus to determine which portions are highly similar to one another.  For example, a calculation of the degree of similarity of the non-zero strength concepts in the N-element vectors of two\ndocuments may be evaluated to determine whether these documents are directed to describing similar concepts.  A document regarding oncology patient treatment may have an N-element vector that has high strength values for oncology concepts while another\ndocument directed to a measles outbreak in California may not have high strength values for oncology concepts in its N-element vector and thus, by comparing the N-element vectors, one can determine whether these documents are similar (i.e. address\nsimilar concepts) or not.  Likewise, a document directed to cancer research at a leading university may have an N-element vector with many of the oncology concepts having non-zero values corresponding to those of the oncology concepts of the oncology\npatient treatment document and thus, will be determined to be similar based on a comparison of the N-element vectors.  Similarity in this manner can also be performed between the related concepts output vector for an input question, search query, user\nprofile, and/or the like and portions of information in a corpus for purposes of answering the input question, returning search results, or performing personalized ranking operations, for example.\nFIG. 5 is an example diagram illustrating concept analysis operations being performed using one or more accelerator devices in accordance with one illustrative embodiment.  As shown in FIG. 5, as opposed to the operation outlined in FIG. 4, the\nhost system 500 does not generate the input vector(s) and does not perform the concept analysis operations, normalization operations, or post-processing operations.  Moreover, there is no storage of intermediate results to a host system storage.  To the\ncontrary, these operations and the intermediate results are performed and maintained within the accelerator.\nAs shown in FIG. 5, the host system 500 is responsible for extracting a listing of concepts 512 from the information source 510 and providing the list of concepts 512 and one or more matrix representation data structures 514 to the accelerator\n520.  The accelerator 520 includes an initialization engine 522 which operates on the listing of concepts 512 to generate one or more input vectors 523, which again may be a set of input vectors 523 that together constitute and N*M matrix of concepts\nextracted from the information source 510.  The generated input vector(s) 523 are provided to the concept analysis operation engine 524.  The concept analysis operation engine 524 operates on the input vector(s) 523 and the matrix representation data\nstructures 514 to perform iterations of a matrix vector multiplication operation that identifies concepts in the matrix that are related to concepts in the input vector(s) 523.\nIt should be appreciated that the operations performed by the initialization engine 522, the concept analysis operation engine 524, and the normalization engine 526 are performed on batches of data from the input vector(s) 523 and/or matrix\nrepresentation data structures 514 with the batch size being determined by the memory limits of the accelerator 520 which comprises a memory 521 that is used to stored the data while it is being processed.  Thus, each batch may be handled by a separate\nthread with the sum of all of the batches equaling the total size of the matrix and the input vectors.  In one illustrative embodiment, the batch size is set to 32 vectors generated from the input vectors of extracted concepts from the information\nsource.  For example, the input vectors may specify tens to thousands of concepts extracted from the information source and these concepts may be broken down into individual vectors having one or a small subset of the extracted concepts that are handled\nby a thread of execution in the accelerator.  Thus, multiple threads may be utilized, each thread handling processing of a different vector within a batch of vectors generated from the input vectors.  Multiple batches may be executed in this manner with\nthe results of processing each vector in each batch being combined with the results of the other vectors in the batch and the results of each of the batches being combined to generate the final result.  For example, if the input vectors represent 1000\nextracted concepts, a first batch may have the first 32 extracted concepts (one non-zero value in each vector of the batch), while a second batch may comprise concepts 33-65, a third batch may comprise concepts 66-98, etc. Hence, for large size matrices\nand input vectors, the operations of elements 522-526 are performed using massive data parallelism and potentially thousands of threads operating on batches of vectors generated based on the input vectors of the extracted concepts.\nThe results of the concept analysis operation are output by concept analysis operation engine 524 to the normalization engine 526 which normalizes the results and directly outputs the normalized results to the post processing engine 528.  The\npost processing engine 528 compiles the normalized results and performs a ranking operation to rank the results and generate an N-element output vector 530 which is output to the host system 500.  The ranking of the results may rank the results according\nto the strengths specified by the non-zero values in the output vectors generated for each of the batches so as to generate a single ranked N-element output vector 530.\nIt should be appreciated that in the above operation, the matrix representation data structures 514 need only be loaded into the accelerator once and may be used as a basis for all of the threads batch executions.  Moreover, until the matrix\nrepresentation data structures 514 need to be updated or modified due to changes in the matrix (and thus, the network/graph representing the corpus), the loaded matrix representation data structures 514 may be used for subsequent processing of other\ninformation sources 510.  Hence, for the majority of operations performed by the accelerator, the only input required from the host system 500 is the listing of extracted concepts 512 from the information source 510.  Moreover, the output from the\naccelerator 520 is an N-element vector.  Thus, the data transfer between the host system 500 and the accelerator 520 is minimized.  Moreover, because all of the operates are performed internal to the accelerator without having to utilize host system\nresources, and these operations are performed in parallel with operations being performed by the host system 500, the speed by which the concept analysis operations is performed is increased.\nAs mentioned above, one of the optimizations that may be performed to improve performance of the concept analysis operations is to re-organize the large sparse matrix so that non-zero entries are concentrated near one another.  This minimizes\ncache misses due to speculative loading of portions of the matrix into memory.  Similar performance improvement is made possible by using such re-ordering of the matrix prior to generating the matrix representations in the illustrative embodiments since\nportions of the matrix are loaded into the memory of the accelerator for each thread to operate on.  The more non-zero entries present in the portion loaded into memory, the more efficient the operation.  One way to perform such concentration of non-zero\nentries is to utilize clustering of non-zero entries as described in co-pending and commonly assigned U.S.  patent application Ser.  No. 14/611,297.  FIGS. 6 and 7 illustrate this clustering based matrix reordering operation which may be used with the\nmechanisms of the illustrative embodiments to provide the matrix representation data structures 514 to the accelerator 520.\nFIG. 6 is an example diagram illustrating an ordering of matrix entries obtained by using a clustering based matrix reordering operation in accordance with one illustrative embodiment.  As shown in FIG. 6 the resulting reordered matrix generated\nas a result of the operations of the mechanisms of the illustrative embodiments has non-zero entries or elements concentrated into clusters 610 near the diagonal, e.g., clusters 612-616, and those non-zero entries that are far from the diagonal, e.g.,\nclusters 622-624, are closely collocated into off-diagonal clusters 620.  Each of the clusters 610, 620 (represented as shaded regions of the matrix in FIG. 6), has non-zero entries that are loosely tied to one another, meaning that the number of\nconnections of the nodes corresponding to these entries or elements have to other nodes within the same cluster 610, 620 is greater than the number of connections the nodes have to nodes outside of the cluster 610, 620, e.g., movie stars have more\nconnections to other movie stars than to general public and thus, movies stars would be a cluster.  The non-shaded regions 630 of the matrix are permitted to include a small-number of non-zero entries or elements (referred to as \"elements\" hereafter).\nThe ordering of the matrix shown in FIG. 6 is achieved even for large scale matrices that have near-scale-free graph topologies.  As shown in FIG. 6, there is no dominant chunk of non-zero elements that is generated as a result of the matrix\nreordering operations.  Thus, the limitations on the improvement in cache efficiency and runtime execution encountered with the known reordering algorithms, such as the Cuthill-McKee and dissection algorithms, are not an issue for the matrix reordering\nmechanisms of the illustrative embodiments.  To the contrary, the matrix reordering mechanisms achieve the ideal reordering sought by the Cuthill-McKee algorithm, for large scale near-scale-free graph topology matrices, with only some off diagonal\nclusters 620 being present but with these being concentrated into their own clusters 620.\nThe benefits of this organization of clusters 610, 620 of non-zero elements along the diagonal is that cache misses are reduced during the matrix operation.  That is, the matrix multiplication operation typically looks for non-zero elements in\nthe matrix when performing the matrix multiplication operation.  Since these non-zero elements are concentrated into clusters 610, 620, when a cluster is loaded into the cache memory, more cache hits occur with less cache misses.  This is especially true\nfor the clusters 610 positioned along the diagonal of the reordered matrix where even when additional clusters are loaded into the cache memory, the clusters are closely located to one another.  Even for those non-zero elements that are off the diagonal,\nthe majority of these non-zero elements are clustered within off-diagonal clusters 620 such that when these clusters are loaded into cache memory of the accelerator 520, there are less cache misses when accessing the corresponding entries in the\ninput/output vector.  While some non-zero elements are not in the clusters 610, 620, i.e. located in the non-shaded areas 630 of the reordered matrix, the efficiency increase obtained through the clustering outweighs the relative few non-zero elements\nthat are not located in clusters 610, 620.\nThe clustering methodology of the mechanisms of the illustrative embodiments concentrates the non-zero elements into closely tied clusters 610, 620 as opposed to the known Cuthill-McKee algorithm which is more concerned with graph distances, as\nrepresented by the various levels utilized in the Cuthill-McKee algorithm.  Because Cuthill-McKee is more concerned with graph distances, it is possible to have large chunks of non-zero elements that do not improve cache efficiency as discussed above. \nMoreover, with dissection algorithms, the concern is to look for disconnected sets of data such that if the set of data is taken out of the matrix, the remaining data will be connected.  Again, this can lead to large chunks of non-zero elements that do\nnot improve cache efficiency as discussed above.  Because the mechanisms of the illustrative embodiments utilize closely tied clusters 610, 620, the possibility of large chunks of non-zero elements is significantly reduced.  Moreover, because the\nmechanisms of the illustrative embodiments organize the clusters 610, 620 so as to reduce cross cluster connections and cross cluster connection lengths, the clusters 610, 620 are organized in a compact configuration within the reordered matrix making\nfor more efficient loading of the non-zero elements into cache memory and thereby reducing cache misses.\nTo illustrate the amount of efficiency increase that is achieved by implementation of the mechanisms of the illustrative embodiments, consider the following Table 1 which illustrates results of a sparse matrix to vector multiplication benchmark\nthat is used to evaluate the number of cache misses on an Intel Nehalem processor with and without the matrix reordering mechanisms of the illustrative embodiments.\nTABLE-US-00001 TABLE 1 Sparse Matrix to Vector Multiplication Benchmark Without Reordering With Reordering INST_RETIRED.ANY 527,852 Mil 552,980 Mil MEM_INST_RETIRED.LOADS 145,950 Mil 139,740 Mil L2_LINES_IN.ANY 10,326 Mil 7,524 Mil\nOFFCORE_RESPONSE_0.  4,158 Mil 2,365 Mil ANY_DATA.ANY_LLC_MISS\nEach row of the table represents a hardware counter.  The first row is the total number of instructions.  The second row is the number of memory load instructions.  The third row is the number of L2 cache misses.  The fourth row is the number of\nL3 cache misses, which are the most expensive and dominate runtime.  It should be noted that the L3 cache misses are reduced by almost half through use of the reordering mechanisms of the illustrative embodiments.  The measured runtime execution is also\nreduced by almost half by the reordering mechanisms of the illustrative embodiments, which is far superior than the 10% improvement achieved by the Cuthill-McKee or dissection algorithms.\nHaving illustrated the results obtained by the clustering based matrix reordering mechanisms of the illustrative embodiments, the following is a more detailed discussion of the mechanisms and operations performed as part of this clustering based\nmatrix reordering.  It should be appreciated that the following discussion is directed to one illustrative embodiment for implementing the clustering based matrix reordering and is not intended to be limiting but rather illustrative of one example\nmethodology and mechanisms used to perform this reordering.  Many modifications may be made to the illustrative embodiments as will become apparent to those of ordinary skill in the art in view of the present description, without departing from the\nspirit and scope of the present invention.\nFIG. 7 is a flowchart outlining an example clustering based matrix reordering operation in accordance with one illustrative embodiment.  The clustering based matrix reordering operation shown in FIG. 7 may be implemented in specially configured\nhardware configured to implement the operations described hereafter, software executed on hardware and configured to implement these operations, or any combination of specially configured hardware and software executed on hardware.  In one illustrative\nembodiment, the operations outlined in FIG. 7 are performed in a host system as part of a corpus ingestion or pre-processing operation so as to reorganize the matrix representing the concepts in the corpus and their relationships to achieve the\norganization shown in FIG. 6.  This re-organized matrix may then be used to generate one or more matrix representation data structures that are provided to the accelerators of the illustrative embodiments for accelerating concept analysis operations as\npreviously discussed above.\nTo illustrate the operation of the illustrative embodiments in association with the description of the operations set forth in FIG. 7, an example in which a matrix M is comprised of indices or nodes corresponding to users of a social networking\nservice will be utilized, where non-zero elements corresponding to the indices or nodes are indicative of a relationship between the users represented by the indices or nodes.  It should be appreciated that this is only an example and the mechanisms of\nthe illustrative embodiments may be utilized with any matrix operation performed on any large scale matrix having a near-scale-free graph topology.  For example, instead of users of a social networking service, the matrix M may have nodes representing\nconcepts found in a corpus of information and edges representing identified relationships between these concepts.\nAs shown in FIG. 7, the operation starts by analyzing an input matrix M to identify cliques within the matrix M (step 710).  The input matrix M may be a set of collected data representing connections or relationships between particular\ninformation objects, concepts, entities, or the like, which are specified as indices of the matrix M. For example, in a social networking environment, the indices of the matrix M may represent users of the social networking services, their attributes,\ncommunications exchanged between the users, or the like.  In a question answering environment or internet search environment, the indices of the matrix M may represent features (concepts) of documents in the corpus of information that is used as a basis\nfor performing the question answering or internet search.  Thus, the indices in the input matrix M represent any information, concepts, or entities that are suitable for the performance of a knowledge extraction, reasoning, or other analysis operations. \nThe entries at the intersection of two indices stores a value indicative of the existence or non-existence of a relationship between the information, concepts, or entities represented by the indices that intersect.\nA clique is defined as a set of row and column indices (or nodes) of the matrix such that their sub-matrix is composed of all non-zero entries.  For example, if a first user represented in matrix M is connected to a second user and vice versa,\nthe two users may constitute a clique.  The clique may be identified by using a starting node or index and identifying other nodes or indices within the matrix M that are connected to the starting node or index and vice versa, i.e. the intersections of\nthe indices identifies a non-zero value element in the matrix M. This process can be repeated for each of the nodes or indices that are connected to the starting node or index where some of the connected nodes or indices (hereafter referred to simply as\n\"nodes\") may be part of the clique while others are not.  That is, for example, if John Smith is the starting node and has a \"friend\" connection to Pete Johnson, and Pete Johnson has a \"friend\" connection to John Smith, then John Smith and Pete Johnson\nmay constitute a clique.  If Pete Johnson did not have a connection to John Smith, then Pete Johnson may not be included in the clique, although, as discussed hereafter, he may be included in the subsequently generated cluster.\nThe process of generating cliques may be repeated for each node in the input matrix M such that multiple cliques are generated.  A minimum size requirement may be specified in a configuration parameter that indicates a minimum number of nodes\nthat must be present in the clique for the clique to be maintained for further use as a starting point for the other operations in FIG. 7, e.g., the clique must have at least 20 nodes, where smaller size cliques having less than 20 nodes are discarded.\nHaving generated the cliques as initial clusters of connected nodes of the matrix, a cluster growth operation is performed to grow clusters from each of the cliques (step 720).  In growing the cliques into clusters, the nodes of the matrix that\nare not already part of a clique are assigned to a cluster.  The assignment of nodes to clusters is based on the number of connections that the corresponding row/column has to nodes already within the cluster.  That is, the node is added to a cluster to\nwhich it has the most connections, i.e. non-zero value elements associated with other nodes of the cluster.  This may leave some nodes without a cluster if the node does not have any connections to the other nodes or relatively few connections to other\nnodes.  As a node joins a cluster, the data structure representing the cluster is updated and the newly added node is used as a basis for adding additional nodes, e.g., if John Smith is added to a cluster of users that represents friends of Mary Monroe,\nthen friends of John Smith may further be used as a basis for determining if any of these friends should be added to the cluster of friends of Mary Monroe.\nIn this way, the cluster grows from an initial clique to a larger size cluster.  A maximum cluster size for clusters may be set in configuration parameters of the clustering-based matrix reordering mechanisms.  The maximum cluster size may be\nset as a function of the memory hierarchy parameters, e.g., cache sizes, in the computing architecture in which the mechanisms of the cluster-based matrix reordering are to be implemented.  For example, a cluster maximum size may be set to a size equal\nto a proportion of a particular cache size in the architecture, e.g., the X % of the L3 cache size, for example, or the size of the memory of the accelerators 520, as another example.\nOnce each of the non-zero elements of the matrix have been processed and corresponding nodes added to clusters in the manner described above, the clusters may be refined by allowing nodes to be reassigned to other clusters (step 730).  For\nexample, the nodes of each cluster may be analyzed to determine if they have more connections to nodes of other clusters than to nodes in their currently assigned cluster.  If a node in cluster A has more connections (non-zero elements) to nodes in\ncluster B, then the node may be reassigned to cluster B. The reassignment may be permitted by the cluster-based matrix reordering mechanisms in response to a determination that the reassignment will result in reduction in the total number of\ncross-cluster connections (or edges) without violating cluster maximum size limitations.  Thus, if cluster B is already at a maximum size, the reassignment may be denied.  Moreover, if the reassignment does not reduce the total number of cross-cluster\nconnections, the reassignment may be denied.  This refining may be performed with regard to each node of each cluster generated in step 720.\nHaving refined the clustering of the nodes in step 730, the resulting clusters are ordered so as to minimize the total length of cross-cluster connections (or edges), i.e. minimize how far away the node entries are from the diagonal of the\nmatrix (step 740).  Moreover, as part of this operation, cross-cluster connections are concentrated in terms of their two end nodes distribution, i.e. length of connections between the two nodes of the connection is minimized.  This operation may be\nimplemented as a dynamic programming algorithm which optimizes a partial solution at a time and the partial solution is incremented one cluster by one cluster until all are ordered.\nAlthough not required, and instead being an optional operation, nodes within the ordered clusters may themselves be ordered locally within the cluster (step 750).  This local ordering of the clusters moves the non-zero entries closer to the\ndiagonal within the cluster and moves nodes with cross-cluster connections closer to the boundaries of the cluster.  That is, when ordering within a cluster, if node of the matrix in the cluster has connections (non-zero elements) to nodes of other\nclusters that are ordered before the cluster, then the node is located at an early location (towards a \"front end\") within this cluster such that its cross-cluster non-zeroes are closer to the diagonal of the reordered matrix.  In the same manner, if a\nnode has connections (non-zero elements) to other clusters that are ordered after this cluster, the node is located in a later location (towards a \"back end\") within the cluster.  At the local level, such reordering within the clusters may be\naccomplished by performing the same operations as discussed above (steps 710-740) but on the local cluster, or other known algorithms may be used to reorder the non-zero elements, such as Cuthill-McKee or dissection.  It should be noted that the known\nalgorithms may be used on the individual clusters for intra-cluster reorganization because the cluster is a relatively smaller and more strongly connected sub-graph (not as sparsely populated) such that there is not a large difference in cache efficiency\nbetween reordering algorithms.\nIt should be appreciated that, either while this reordering process is being performed or after this reordering process is complete, vectors in a lookup table are updated to represent the new locations of the elements that have been moved.  For\nexample, the lookup table may comprise a first vector with locations of the indices of the original matrix M and a second vector having the new locations after the reordering is performed, with pointers from one vector entry to the other so as to allow\nmapping of the elements' original locations to their new locations in the reordered matrix.  This lookup table and the reordered matrix may be output for use in performing matrix operations and ultimately, the knowledge extraction operations, reasoning\noperations, or other analytical operations (step 760).\nDuring runtime operation, the output lookup table and reordered matrix may be used to generate matrix representations for use in performing matrix operations as part of a knowledge operation, i.e. knowledge extraction, reasoning, analysis, or\nthe like, which in the case of the above accelerator 520, may be a concept analysis operation performed within the accelerator 520 (step 770).  The results of this knowledge operation may then be output (step 780), e.g., the N-element vector may be\noutput by the accelerator.  Thus, the mechanisms of the illustrative embodiments specifically configured the computing devices upon which they are implemented for specifically performing this type of clustering-based matrix ordering operation, which is\nnot performed by generic computing devices, and which improves the operation and functionality of the computing devices.  Moreover, the mechanisms of the illustrative embodiments modify the functionality and performance of the computing devices upon\nwhich they are implemented by increasing the cache efficiency of the computing device during matrix operations.\nIt should be appreciated that steps 710-760 may be implemented prior to use of the reordered matrix and lookup table during runtime matrix operations.  That is, these steps 710-760 may be used in a pre-processing of the matrix with the resulting\nreordered matrix and lookup table being output to the runtime system components for use in performing their runtime operations.  In this way, the matrix need not be reordered each time a matrix operation is to be performed during runtime, which is\nrepresented as steps 770-780.  Applying this to the accelerator 520 in FIG. 5, steps 710-760 may be performed by the host system 500 as a pre-processing operation while steps 770-780 are performed within the accelerator 520 based on the matrix\nrepresentation input to the accelerator 520, this matrix representation being generated based on the re-ordered matrix.\nAs also noted above, the accelerators 520 may receive as input, one or more matrix representation data structures that represent the matrix in a compressed format.  The compressed format concentrates on representing the non-zero values in the\nmatrix while any entries in the matrix not represented in the compressed format are assumed to be zero entries in the matrix.  There are various types of formats that may be utilized including the Yale sparse matrix format, CSR, CSC, and others.  As\nrecognized in the co-pending and commonly assigned U.S.  patent application Ser.  No. 14/635,007, efficient execution of concept analysis operations may be achieved by dynamically modifying the representation used to perform iterations of the matrix\nvector multiplication operations that make up the concept analysis operation in accordance with the sparsity (or density) of the multiplicand vector.  Thus, in some illustrative embodiments, a single matrix representation may be utilized that implements\na single compressed format, e.g., either Yale, CSR, CSC, or another known or later developed compressed format for matrix representation.  In other illustrative embodiments a plurality of matrix representation data structures having different compressed\nformats may be provided to the accelerator 520 such that the accelerator dynamically selects a matrix representation data structure based on a determined sparsity (or density) of the multiplicand vector.\nFor example, it has been recognized that CSR formatted matrices are more suitable for parallel execution of matrix vector multiplication operations for dense vectors, i.e. vectors having more non-zero values than zero values.  This is because\nCSR orders the non-zero values of the matrix row by row and allows non-zero values of a row to be grouped together with the value of the vector with which they are being multiplied.  As a result, each multiplication of a row by a vector element can be\ndistributed to a different worker, e.g., a different thread within the accelerator 520.\nFor example, FIG. 8A illustrates a matrix vector multiplication operation performed using a Compact Sparse Row (CSR) formatted data structure of a matrix along with sample pseudo-code for performing the partial matrix vector multiplication\noperations.  As shown in FIG. 8A, the matrix A 810 is being multiplied by the vector X 820 such that the kernel of the matrix vector multiplication operation is y(i)=y(i)+(A(i,j)*x(j)) where again i is the row index, j is the column index, y is the\npartial result of the matrix vector multiplication operation, A(i,j) is the entry at i,j in matrix A, and x(j) is the value in the vector X corresponding to column index j.\nAs shown in FIG. 8A, the data structure representation 830 of the matrix A 810 comprises a value (val) array or vector 832, a column index (ind) array or vector 234, and a row pointer (ptr) array or vector 836.  The val array 832 stores the\nvalues of the non-zero entries in the matrix A (left-to-right, then top-to-bottom).  Thus, the non-zero values in row 0 of matrix A appear first (as depicted by the shading patterns), followed by the non-zero values in row 1, row 2, and so on.  The ind\narray 834 stores the column indices of the corresponding values in the val array 832.  The ptr array 836 stores the pointer to the where the row starts for the values in the ind array 834.\nAs shown in the pseudo-code for performing the partial matrix vector multiplication operations, for each row i, and for each pointer value k in the ptr array 836, a partial matrix vector multiplication operation result is generated as\ny[i]=y[i]+val[k]*x[ind[k]], essentially calculating the matrix vector multiply kernel noted above for each row of the matrix A. The result is a sum of weighted rows.  It should be noted that the calculations associated with each row can be performed in\nparallel at substantially a same time and thus, may be distributed to different workers.\nFIG. 8B illustrates a matrix vector multiplication operation performed using a Compact Sparse Column (CSC) formatted data structure of a matrix along with sample pseudo-code for performing the partial matrix vector multiplication operations.  As\nshown in FIG. 8B, the matrix A 840 is multiplied by the vector X 850 such that the kernel of the matrix vector multiplication operation is again y(i)=y(i)+(A(i,j)*x(j)) where again i is the row index, j is the column index, y is the partial result of the\nmatrix vector multiplication operation, A(i,j) is the entry at i,j in matrix A, and x(j) is the value in the vector X corresponding to column index j.\nAs shown in FIG. 8B, the data structure representation 860 of the matrix A 840 comprises a value (val) array or vector 862, a row index (ind) array or vector 864, and a column pointer (ptr) array or vector 864.  The val array 862 stores the\nvalues of the non-zero entries in the matrix A (left-to-right, then top-to-bottom).  Thus, the non-zero values in row 0 of matrix A appear first (as depicted by the shading patterns), followed by the non-zero values in row 1, row 2, and so on.  The ind\narray 864 stores the row indices of the corresponding values in the val array 862.  The ptr array 866 stores the pointer to the where the column starts for the values in the ind array 834.\nAs shown in the pseudo-code for performing the partial matrix vector multiplication operations, for each column i, and for each pointer value k in the ptr array 866, a partial matrix vector multiplication operation result is generated as\ny[ind[k]]=y[ind[k]]+val[k]*x[k], essentially calculating the matrix vector multiply kernel noted above for each column of the matrix A. This results in a sum of weighted columns.  It should be noted that the calculations associated with each vector value\nx[k] can be distributed for small numbers of non-zero x[k] values to exploit the superposition.  Since the x vector can be represented by the sum of many single-entry vectors, their corresponding outputs (y[ ]) can simply be added together for the final\noutput vector.\nThus, while CSR and CSC formatted data structures may be used to represent a large scale sparse matrix in a compact manner within memory, each of these formats provides different levels of efficiency for parallel execution in a data processing\nsystem based on the sparsity of the vector with which the matrix is being multiplied in a matrix vector multiplication operation.  The CSR representation of the matrix is suitable and more efficient for parallel execution for dense vectors X while the\nCSC representation of the matrix is suitable and more efficient for sparse vectors X. The illustrative embodiments may leverage this difference in format efficiency to provide a hybrid approach to performing matrix vector multiplication operations.  The\nmatrix representation that is utilized in the illustrative embodiments for a particular iteration of the concept analysis operation in the accelerator 520 may be selected dependent upon the sparsity (or density) of the multiplicand vector.\nSince knowledge extraction, information extraction, relationship analysis, and other complex processes for obtaining information from large scale networks or matrices utilize multiple iterations of matrix operations, which comprise matrix vector\nmultiplication operations, the density of the vectors by which the matrix is multiplied tends to increase with subsequent iterations.  Thus, a vector X, in an initial iteration of process may be rather sparse, while in later iterations the vector X may\nbecome denser.  For example, an initial iteration may determine \"what concepts are related to concept A\" which may be determined by multiplying the matrix M by a vector X where the entry in vector X that is a non-zero value is the entry corresponding to\nconcept A. This operation may output a result as an output vector of Y having a plurality of non-zero elements.  In order to determine what other concepts may be related to concept A, it is necessary to then multiply matrix M by the vector Y to determine\nwhat concepts are related to the concepts in vector Y. As a result, an output vector Z may be generated that includes an even larger set of non-zero elements.  This may continue until the difference in number of non-zero elements in the output vector\nfrom the previous output vector converges, i.e. does not exceed a predetermined threshold at which point the process is complete and the result is the combination of the vector outputs.  Thus, it can be seen that as the vectors X, Y, and Z, etc. become\nmore dense with each subsequent iteration of the process, and hence, different matrix representations may be more efficient for parallel execution of subsequent iterations.\nIn some illustrative embodiments, the illustrative embodiments dynamically modify the matrix representation used during iterations of the concept analysis operation that is operating on a large scale matrix by either providing a predetermined\nnumber of iterations in which a first matrix representation is utilized with subsequent switching to a second matrix representation during subsequent iterations, or providing a mechanism for evaluating the sparsity of the vector of the matrix vector\nmultiplication operations being performed during an iteration of the process with a threshold sparsity value to determine if switching of the matrix representation should be performed.  The selection of a matrix representation is made so as to maximize\nparallel execution of the partial matrix vector multiplication operations that are performed.  This dynamic selection is described in greater detail in commonly assigned and co-pending U.S.  patent application Ser.  No. 14/665,007, but is summarized in\nthe following flowcharts for illustration as to how this process may be implemented in the accelerators 520 of the illustrative embodiments.\nFIG. 9 is a flowchart outlining an example hybrid representation matrix vector multiplication operation in accordance with one illustrative embodiment.  It should be appreciated that with the process outlined in FIG. 9, the operations 910-920\nmay be performed by a host system 500, with the remainder of the operations being performed by one or more accelerators 520 on behalf of the host system.\nAs shown in FIG. 9, the operation starts with receiving a matrix that is the basis for the performance of the matrix vector multiplication operation (step 910).  The matrix is a representation of a large scale data set which may in turn\nrepresent many different types of relationships between entities, concepts, information, or the like, depending upon the particular system in which the mechanisms of the illustrative embodiments are implemented.  For example, the matrix may represent,\nconcepts and relationships between concepts, in one or more electronic documents of a corpus of documents upon which a Question and Answer (QA) system, such as the IBM Watson.TM.  QA system available from International Business Machines (IBM) Corporation\nof Armonk, N.Y., operates.  In such a case, the matrix may be generated as part of an ingestion operation in which the corpus is ingested by the QA system for use in performing question answering operations.  The network or graph of the concepts and\ntheir relationships may have nodes representing concepts and edges representing relationships between concepts with the strengths of these relationships being indicated by the specific values associated with the edges.  This network or graph may then be\ntranslated into a matrix representation in which the concepts (nodes) are indices of the matrix while edges are represented as values at locations within the matrix.\nHaving received the matrix as input, the mechanisms of the illustrative embodiments generate a plurality of compressed representation data structures of the matrix, each compressed representation data structure being for a different type of\ncompressed representation of the matrix (step 920).  The compressed representations represent the matrix in a compressed manner, preferably by concentrating the representation on specifying the non-zero values within the matrix and assuming that any\nvalues not represented by the compressed representation are zero values.  For example, the plurality of compressed representations, in one illustrative embodiment, comprises a CSR representation and a CSC representation.  Other representations may also\nbe utilized, including, but not limited to, the Yale sparse matrix representation, for example.  In some illustrative embodiments, the compressed representation represents the matrix as a plurality of arrays or vectors that focus on the non-zero values\npresent within the input matrix.\nA vector is generated based on an information source, such as a document, input question, or the like, with the vector specifying the entity, concept, information, or the like, of interest (step 930).  For example, the vector may have multiple\nentries for different concepts that can be the basis for the evaluation of the matrix with one of these entries being set to a non-zero value to indicate the particular concept of interest, e.g., concept i in FIG. 9.  For example, if the process is to\nidentify all of the concepts that may be related to concept i in the matrix, then the vector entry for concept i may be set to a non-zero value such that when the vector is multiplied by the matrix, only those non-zero values in the matrix associated\nwith concept i will generate non-zero outputs in the output vector, i.e. only those concepts directly related to concept i and thus, having an edge or relationship with concept i will result in a non-zero value being output.\nFor an initial set of iterations of the process, a first matrix representation data structure is selected for use in performing the partial matrix vector multiplication operations (step 940).  In one illustrative embodiment, this first matrix\nrepresentation may be the CSC matrix representation data structure which, as discussed above, is efficient for sparse vectors.  For example, with a vector input that has a single non-zero value in the vector, e.g., concept i, during a first iteration 942\nof the process a CSC matrix representation data structure may be selected and CSC based sparse matrix multiplication operations may be performed to generate a partial matrix vector multiplication output.  Alternatively, since the vector has only a single\nnon-zero value, a lookup in the CSC formulation data structure may be performed for the i'th vector which is then used as the output for the partial matrix vector multiplication operation of the first iteration.\nFor a second iteration 944 of the process, the CSC representation data structure may again be utilized to perform a partial matrix vector multiplication operation for this iteration using the vector output of the first iteration as the vector to\nmultiply with the matrix for this second iteration.  During this iteration, a weighted sum of columns of the CSC representation data structure based on the output vector of the first iteration is generated.  As noted above in the description of the CSC\nrepresentation with regard to FIG. 8B, the evaluations of x[k] can be distributed for small number of non-zeros in x[k] with the result being a sum of weighted columns.  Thus, the second iteration 344 may be parallelized using a plurality of workers.\nAs shown in FIG. 9, after an initial set of iterations in which the first matrix representation data structure is utilized to perform the partial matrix vector multiplication operations (step 940), matrix vector multiplication operations are\nperformed in a distributed/parallel manner using a second matrix representation data structure which is partitioned into portions for each of the various workers (step 950).  For example, in one illustrative embodiment, the second matrix representation\ndata structure may be a CSR representation of the matrix.  As discussed above with regard to FIG. 8A, the partial matrix vector multiplication operations for the various rows of the matrix may be distributed to a large number of workers such that the\ncalculations for multiple rows can be performed substantially at the same time.\nHence, in addition to the parallelization of step 940 above, step 950 may be performed in parallel using multiple workers (threads) as well.  Thus, parallelization of the matrix vector multiplication operation is maximized through the selection\nof compressed matrix representations that are suited to the particular sparsity of the vector involved in the iteration of the matrix vector multiplication operation and overall process.  The workers may be separate threads in the same accelerator 520 or\nmay be different threads in different accelerators 520 when a plurality of accelerators are utilized.\nThe parallel partial matrix vector multiplication operations 950 may be repeated until the iterations of the process converge (step 960).  Iterations typically converge (step 960) based on monitoring the change in the output vector.  If the\noutput vector change becomes very small in relative terms and in magnitude, the iterations are deemed to be converged, and the system generates the output vector (step 970).  Based on a benchmark set that typically represents the test cases, the\niteration convergence can be also be set as a fixed number of iterations.  For example, one could set the number of iterations to 5 based on the benchmark test, where the final output vector is generated upon execution of the fifth iteration.\nThe resulting vector output generated from the convergence of the iterations is then output as the final result of the process (step 970).  For example, if the process was attempting to find concepts related to concept A, then the resulting\nvector output would have non-zero values in each entry of the vector corresponding to a concept that is related either directly or indirectly with concept A, as determined from the multiple iterations of the matrix vector multiplication operation.  As\ndiscussed previously, with regard to the accelerators 520 of the illustrative embodiments, the final result output generated by this process may be provided to a normalization engine 526 for normalization and the resulting normalized output may be\nprovided to a post processing engine 528 for further processing before returning a N-element vector 530 result to the host system 500.\nWhile FIG. 9 shows an illustrative embodiment in which a fixed number of initial iterations utilize the first compressed matrix representation data structure while subsequent iterations utilize a second compressed matrix representation, the\nillustrative embodiments are not limited to such.  Rather, the switching from one compressed matrix representation to another may be performed dynamically based on an evaluation of the sparsity of the input vector.  It should be appreciated that in an\niterative matrix vector multiplication operation, the input vector is the output vector of the previous iteration.  Thus, as the sparsity of the input vector decreases and the input vector becomes more dense with each iteration, the compressed matrix\nrepresentation may be dynamically switched from one compressed matrix representation to another.  Looking at it from a vector density perspective, as the density of the input vector increases with each iteration, the compressed matrix representation may\nbe dynamically switched.\nFIG. 10 is a flowchart outlining an example operation for dynamically modifying the compressed matrix representation utilized for iterations of a matrix operation based on a determination of the sparsity/density of an input vector using a hybrid\nmatrix representation mechanism in accordance with one illustrative embodiment.  As shown in FIG. 10, the operation again starts with receiving a matrix that is the basis for the performance of the matrix vector multiplication operation (step 1010).  A\nplurality of compressed representation data structures of the matrix, each compressed representation data structure being for a different type of compressed representation of the matrix, are again generated and stored for use in performing the matrix\noperation (step 1020).  During an initial iteration, an input vector is generated (step 1030) in a manner similar to that of step 930 in FIG. 9 above.\nA next iteration of the matrix operation is then initiated (step 1040).  At the start of the matrix operation, the \"next iteration\" is the first iteration and utilized the vector that is input in step 1030.  In subsequent iterations, the input\nvector will be the output vector generated from the previous iteration of the matrix operation.\nThe sparsity (or alternatively the density) of the input vector is calculated and compared to one or more sparsity (or density) threshold values (step 1050).  It should be appreciated that sparsity and density are alternative sides of the same\ncharacteristics.  Both measure a relation between zero and non-zero values in the input vector.  When the number of zero values in the input vector is greater than the number of non-zero values, the input vector is more sparse, or less dense.  When the\nnumber of zero values in the input vector is less than the number of non-zero values in the input vector, then the input vector is less sparse, or more dense.  Thus, sparsity or density may be evaluated in this operation.  Hereafter, it will be assumed\nthat sparsity is utilized for purposes of illustration.\nBased on results of the comparison, a corresponding compressed matrix representation data structure is selected for use with the current iteration (step 1060).  For example, if the sparsity of the input vector is equal to or greater than a\nsparsity threshold value, i.e. the vector is sufficiently sparse, then a first compressed matrix representation data structure (e.g., CSC) is selected for use during the present iteration.  However, if the sparsity of the input vector is less than the\nsparsity threshold value, i.e. the input vector is dense, then a second compressed matrix representation data structure (e.g., CSR) is selected for use during the present iteration.  Of course this may be extended to additional types of compressed matrix\nrepresentations based on additional threshold values such that as the density continues to increase, other compressed matrix representations suitable for parallelized execution at higher density input vectors may be selected.\nThe iteration of the matrix operation is then executed in a parallel manner using the selected compressed matrix representation data structure (step 1070).  A determination is made as to whether the iterations have converged (step 1080) and, if\nnot, the operation returns to step 1040 with the input vector now being the output vector of the previous iteration.  Otherwise, if the iterations have converged, then the output vector is generated as the aggregate of the output vectors of the partial\nmatrix vector multiplication operations performed during the iterations (step 1090).  Thus, the illustrative embodiments may further utilize a hybrid compressed matrix representation based matrix vector multiplication operation mechanism which greatly\nincreases the possibility of parallel execution of the matrix vector multiplication operation and thus, the efficiency with which the overall matrix operation or process is performed.\nFIG. 11 is a flowchart outlining an example operation for performing a concept analysis operation utilizing one or more accelerator devices in accordance with one illustrative embodiment.  For purposes of the present description, it is assumed\nthat the matrix has already been pre-processed so as to generate one or more compressed format representations of the matrix, e.g., a CSC formatted representation and a CSR formatted representation.  As noted above, this needs to be done relatively\ninfrequently.\nAs shown in FIG. 11, the operation starts with receiving an information source and extracting a listing of concepts identified within the information source (step 1110).  The listing of concepts is sent to an accelerator along with one or more\nmatrix representation data structures if the matrix representation data structures have not already been loaded by the accelerator (step 1120).  The listing of concepts is processed by an initialization engine of the accelerator to generate one or more\ninput vectors (step 1130).  The one or more input vectors are used along with the one or more matrix representation data structures to perform a concept analysis operation (step 1140).  As discussed above, this concept analysis operation may involve\nmultiple iterations of a matrix vector multiplication operation in which an initial iteration may utilize the one or more input vectors and one of the matrix representation data structures and subsequent iterations may utilize the resulting vector from\nthe previous iteration and either the same or another matrix representation data structure depending on the density of the vector being utilized.  This concept analysis operation may be performed using batch processing with multiple threads in a parallel\nmanner.\nThe results generated by the concept analysis operation are normalized by a normalization engine of the accelerator (step 1150).  The normalized results are then processed by a post processing engine of the accelerator (step 1160) to generate\nand output an N-element vector (step 1170).  The N-element vector is output to the host system which utilizes the N-element vector to perform a knowledge extraction, reasoning, or other analytical operation in the host system (step 1180).  The operation\nthen terminates.\nThus, the illustrative embodiments provide mechanisms for improving the execution of concept analysis operations for use with natural language processing (NLP) systems, knowledge extraction systems, or the like.  In particular, in some\nillustrative embodiments, the mechanisms are utilized as part of a Question and Answer (QA) system, such as the IBM Watson.TM.  QA system, to assist with concept analysis operations performed when ingesting documents and/or answering input questions. \nThe mechanisms of the illustrative embodiments provide the ability to offload processing of the concept analysis operations, directed to identifying related concepts within a large scale sparse matrix, to one or more accelerators with minimized data\ntransfer between the host system and the accelerators.  Batch processing using massive data parallelism and a plurality of threads in each accelerator also increases the efficiency and speed by which the concept analysis operation is performed. \nMoreover, using cluster based matrix reordering and hybrid matrix storage formats further improves the efficiency of the concept analysis operation of the accelerators.\nAs noted above, it should be appreciated that the illustrative embodiments may take the form of an entirely hardware embodiment, an entirely software embodiment or an embodiment containing both hardware and software elements.  In one example\nembodiment, the mechanisms of the illustrative embodiments are implemented in software or program code, which includes but is not limited to firmware, resident software, microcode, etc.\nA data processing system suitable for storing and/or executing program code will include at least one processor coupled directly or indirectly to memory elements through a system bus.  The memory elements can include local memory employed during\nactual execution of the program code, bulk storage, and cache memories which provide temporary storage of at least some program code in order to reduce the number of times code must be retrieved from bulk storage during execution.\nInput/output or I/O devices (including but not limited to keyboards, displays, pointing devices, etc.) can be coupled to the system either directly or through intervening I/O controllers.  Network adapters may also be coupled to the system to\nenable the data processing system to become coupled to other data processing systems or remote printers or storage devices through intervening private or public networks.  Modems, cable modems and Ethernet cards are just a few of the currently available\ntypes of network adapters.\nThe description of the present invention has been presented for purposes of illustration and description, and is not intended to be exhaustive or limited to the invention in the form disclosed.  Many modifications and variations will be apparent\nto those of ordinary skill in the art without departing from the scope and spirit of the described embodiments.  The embodiment was chosen and described in order to best explain the principles of the invention, the practical application, and to enable\nothers of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated.  The terminology used herein was chosen to best explain the principles of the\nembodiments, the practical application or technical improvement over technologies found in the marketplace, or to enable others of ordinary skill in the art to understand the embodiments disclosed herein.", "application_number": "14682676", "abstract": " Mechanisms, in a system comprising a host system and at least one\n     accelerator device, for performing a concept analysis operation are\n     provided. The host system extracts a set of one or more concepts from an\n     information source and provides the set of one or more concepts to the\n     accelerator device. The host system also provides at least one matrix\n     representation data structure representing a graph of concepts and\n     relationships between concepts in a corpus. The accelerator device\n     executes the concept analysis operation internal to the accelerator\n     device to generate an output vector identifying concepts in the corpus,\n     identified in the at least one matrix representation data structure,\n     related to the set of one or more concepts extracted from the information\n     source. The accelerator device outputs the output vector to the host\n     system which utilizes the output vector to respond to a request submitted\n     to the host system associated with the information source.\n", "citations": ["6546120", "7688748", "8250069", "8751556", "8775495", "20090178042", "20090287678", "20110029756", "20110066587", "20110125734", "20120022841", "20120259843", "20130007055", "20130018652", "20130066886", "20130110861", "20130142289", "20140108481", "20140298351", "20150113031", "20150254312", "20150324125", "20160140084"], "related": []}, {"id": "20160307596", "patent_code": "10375452", "patent_name": "Apparatus and methods for thumbnail generation", "year": "2019", "inventor_and_country_data": " Inventors: \nHardin; Glen (Charlotte, NC), Manchester; Eric (Rockville, MD)  ", "description": "<BR><BR>RELATED APPLICATIONS\nThe present application is related to co-owned and co-pending U.S.  application Ser.  No. 14/220,021 filed on Mar.  19, 2014 and entitled \"APPARATUS AND METHODS FOR RECORDING A MEDIA STREAM\" which is incorporated herein by reference in its\nentirety.\n<BR><BR>COPYRIGHT\nA portion of the disclosure of this patent document contains material that is subject to copyright protection.  The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure, as it\nappears in the Patent and Trademark Office patent files or records, but otherwise reserves all copyright rights whatsoever.\n<BR><BR>BACKGROUND\n1.  Technological Field\nThe present disclosure relates generally to the field of delivery of digital media data (e.g., text, video, and/or audio) over data delivery networks, such as an Internet Protocol Television (IPTV) network, and/or the Internet; and specifically\nin one aspect to the generation of thumbnail information relating to the delivered data.\n2.  Description of Related Technology\nAdaptive bitrate (ABR) streaming is a technique to distribute program content over a large distributed network.  Multiple bitrates of a particular piece of content are available to stream to a viewer and the selection of the bit rate is based on\ncurrent network conditions.  This means that when there is greater bandwidth availability, a larger bitrate version of the content may be selected.  If available bandwidth narrows, a lower bitrate (i.e., smaller) version of the content may be selected to\nprovide a seamless user experience.\nDuring playback of content whether through traditional digital video recorder (DVR) or network based delivery, a user is additionally provided with an ability to perform various trick mode functions including e.g., pause, rewind, fast forward,\netc. These trick mode functions are available to streaming media and existing Video-on-Demand (VOD) solutions as well.  One or more thumbnails are displayed to the user upon selection of a fast forward, rewind, pause, random seek command, or other trick\nmode button.  The display of the thumbnails requires the creation of a large number of stand-alone image files which are referenced to particular positions within the content.  For example, if a thumbnail needs to be generated for every two seconds of\ncontent on a particular platform for a movie lasting two hours, 3600 thumbnails would need to be generated and managed.  This requires a large amount of storage and content management for the variety of image files required under the existing solution.\nBased on the foregoing, it is clear that while prior art solutions have generally recognized the benefits of adaptive bitrate streaming and trick mode functionality, technical limitations pose a storage and content management problem on the\nnetworks that store the content.  What is needed is methods and apparatus for enabling thumbnail generation during trick mode operation which does not tax the system storage and which are easily managed.\n<BR><BR>SUMMARY\nThe present disclosure addresses the foregoing needs by providing, inter alia, methods and apparatus for thumbnail generation for use in adaptive bitrate (ABR) streaming via extracting key frames from a thumbnail stream by a Just in Time\nPackager (JITP).\nIn a first aspect, an encoder apparatus is disclosed.  In one embodiment, the apparatus comprising includes: a first interface configured to communicate with a network, a storage apparatus, and a processor configured to execute at least one\ncomputer program, the at least one computer program comprising a plurality of instructions.  The instructions are configured to, when executed, cause the apparatus to encode a plurality of video files from a content source for use with adaptive bitrate\nstreaming.  The plurality of video files include a thumbnail stream comprising a first plurality of key frames separated by null frames, the thumbnail stream configured to have said first plurality of key frames extracted and be transmitted to a user\ndevice during a trick mode operation of the user device and at least one other encoded stream.  The at least one other encoded stream comprising a second plurality of key frames separated by delta frames.\nIn another embodiment, the first plurality of key frames of the thumbnail stream and the second plurality of key frames of the at least one other encoded stream are synchronized.  In another embodiment, the at least one other encoded stream\ncomprises a plurality of streams each associated with a different bit rate configured for use with adaptive bit rate streaming.  In a further embodiment, the thumbnail stream and each stream of the at least one other encoded stream are packaged in\nseparate transport stream files.  In an additional embodiment, he at least one other encoded stream comprise an audio track.\nIn a second aspect, a packager apparatus is disclosed.  In one embodiment, the packager apparatus comprises: a first interface configured to communicate with a content delivery network, a storage apparatus, and a processor configured to execute\nat least one computer program, the at least one computer program comprising a plurality of instructions.  The instructions are configured to, when executed, cause the apparatus to: based on a user request, extract a plurality of thumbnail images\nconfigured for use during trick mode operation of a user device from a plurality of key frames in a thumbnail stream, the thumbnail stream comprising said first plurality of key frames separated by null frames and based, at least in part, on the user\nrequest and the extraction of the plurality of thumbnail images, generate a manifest file comprising a plurality of addresses corresponding to a location of the plurality of thumbnail images.\nIn another embodiment, the plurality of instructions are further configured to, when executed, cause the apparatus to segment portions of an encoded stream for delivery via the content delivery network.  The generated manifest file further\ncomprises a second plurality of addresses corresponding to the plurality of segmented portions of the encoded stream.  The extraction of the plurality of thumbnail images comprises skipping the null frames in the thumbnail stream.  In another embodiment,\nthe extraction of the plurality of thumbnail images is based, at least in part, on a user request for the thumbnail images.  In a further embodiment, the generation of the manifest file is based on a device type of a requesting device.  In an additional\nembodiment, the plurality of instructions are further configured to, when executed, cause the apparatus to remove the extracted plurality of thumbnail images.  In another embodiment the plurality of instructions are further configured to, when executed,\ncause the apparatus to resize the extracted plurality of thumbnail images based, at least in part, on a device type of a requesting device.\nIn a third aspect, a method for providing images for display during trick mode operation is disclosed.  In one embodiment, the method comprises receiving a plurality of encoded files, receiving a request from a user device for thumbnail images\nassociated with said source file.  Based, at least in part, on the receipt of the request, the method includes: extracting a plurality of thumbnail images from the first plurality of key frames in the thumbnail stream, generating a manifest file\ncomprising addresses of the extracted plurality of thumbnail images, and providing the manifest file to the user device.  The plurality of encoded files include a thumbnail stream, said thumbnail stream comprising a first plurality of key frames\nseparated by null frames and comprising no delta frames and a video stream, said video stream comprising a second plurality of key frames separated by delta frames, where said first plurality of key frames and said second plurality of key frames are\nsynchronized.\nIn another embodiment, the method also includes receiving a second request from the user device for the video stream of the source file and based, at least in part, on receiving the second request, segmenting the video stream of the source file\ninto a plurality of segments wherein the manifest file further comprises addresses of the plurality of segments.  In another embodiment, the key frames comprise I-frames and the delta frames comprise B-frames.  In a further embodiment, the method further\nincludes resizing each of the extracted plurality of thumbnail images.  Additionally the resizing is based, at least in part on a device type of the request.  Additionally, in an embodiment the method includes serving at least one of the plurality of\nthumbnail images to the user device.  In a further embodiment, the method also includes deleting the plurality of thumbnail images based, at least in part, on a termination of a session with the user device.\nIn a fourth aspect, a consumer premises equipment (CPE) is disclosed.\nIn a fifth aspect, a computer readable medium is disclosed.  In one embodiment, the computer readable medium comprises one or more instructions, which when executed by the processor, are configured to cause a device to perform a variety of\nfunctions.  In a further embodiment, the computer readable medium is non-transitory.\nIn a sixth aspect, image recognition, automatic box-art generation, search optimization features, and quality of service checking/correcting are disclosed.\nThese and other aspects shall become apparent when considered in light of the disclosure provided herein. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nFIG. 1 is a functional block diagram illustrating an exemplary hybrid fiber network configuration useful with various aspects of the present disclosure.\nFIG. 1a is a functional block diagram illustrating one exemplary network headend configuration.\nFIG. 1b is a functional block diagram illustrating one exemplary local service node configuration useful with various aspects of the present disclosure.\nFIG. 1c is a functional block diagram illustrating one exemplary broadcast switched architecture (BSA) network.\nFIG. 1d is a functional block diagram illustrating one exemplary packetized content delivery network architecture useful with various aspects of the present disclosure.\nFIG. 1e is a functional block diagram illustrating a second exemplary packetized content delivery network architecture useful with various aspects of the present disclosure.\nFIG. 2 is a functional block diagram of an exemplary network useful in conjunction with various principles described herein.\nFIG. 3a is a simplified input/output diagram of an encoder useful in conjunction with various principles described herein.\nFIG. 3b is a simplified input/output diagram of a Just in Time Packager (JITP) useful in conjunction with various principles described herein.\nFIG. 4 is a graphical representation of various stream types useful in conjunction with various principles described herein.\nFIG. 5a is a logical flow diagram representing one embodiment of a method for encoding a content stream.\nFIG. 5b is a logical flow diagram representing one embodiment of a method for operating a content delivery network.\nFIG. 5c is a logical flow diagram representing one embodiment of a method for operating a Just in Time Packager.\nFIG. 5d is a logical flow diagram representing one embodiment of a method for operating a consumer premises equipment (CPE).\nFIG. 5e is a logical flow diagram representing another embodiment of a method for operating a consumer premises equipment (CPE).\nFIG. 6 is a functional block diagram of an exemplary network useful in conjunction with various principles described herein.\nAll figures .COPYRGT.  Copyright 2015 Time Warner Enterprises LLC All rights reserved.\n<BR><BR>DETAILED DESCRIPTION\nReference is now made to the drawings wherein like numerals refer to like parts throughout.\nAs used herein, the term \"application\" refers generally and without limitation to a unit of executable software that implements a certain functionality or theme.  The themes of applications vary broadly across any number of disciplines and\nfunctions (such as on-demand content management, e-commerce transactions, brokerage transactions, home entertainment, calculator etc.), and one application may have more than one theme.  The unit of executable software generally runs in a predetermined\nenvironment; for example, the unit could include a downloadable Java Xlet.TM.  that runs within the JavaTV.TM.  environment.\nAs used herein, the term \"client device\" includes, but is not limited to, set-top boxes (e.g., DSTBs), gateways, modems, personal computers (PCs), and minicomputers, whether desktop, laptop, or otherwise, and mobile devices such as handheld\ncomputers, PDAs, personal media devices (PMDs), tablets, \"phablets\", and smartphones.\nAs used herein, the term \"codec\" refers to a video, audio, or other data coding and/or decoding algorithm, process or apparatus including, without limitation, those of the MPEG (e.g., MPEG-1, MPEG-2, MPEG-4/H.264, etc.), Real (RealVideo, etc.),\nAC-3 (audio), DiVX, XViD/ViDX, Windows Media Video (e.g., WMV 7, 8, 9, 10, or 11), ATI Video codec, or VC-1 (SMPTE standard 421M) families.\nAs used herein, the term \"computer program\" or \"software\" is meant to include any sequence or human or machine cognizable steps which perform a function.  Such program may be rendered in virtually any programming language or environment\nincluding, for example, C/C++, Fortran, COBOL, PASCAL, assembly language, markup languages (e.g., HTML, SGML, XML, VoXML), and the like, as well as object-oriented environments such as the Common Object Request Broker Architecture (CORBA), Java.TM. \n(including J2ME, Java Beans, etc.) and the like.\nThe term \"Customer Premises Equipment (CPE)\" refers without limitation to any type of electronic equipment located within a customer's or subscriber's premises and connected to or in communication with a network.\nAs used herein, the term \"digital processor\" is meant generally to include all types of digital processing devices including, without limitation, digital signal processors (DSPs), reduced instruction set computers (RISC), general-purpose (CISC)\nprocessors, microprocessors, gate arrays (e.g., FPGAs), PLDs, reconfigurable compute fabrics (RCFs), array processors, and application-specific integrated circuits (ASICs).  Such digital processors may be contained on a single unitary IC die, or\ndistributed across multiple components.\nAs used herein, the term \"display\" means any type of device adapted to display information, including without limitation CRTs, LCDs, TFTs, plasma displays, LEDs (e.g., OLEDs), incandescent and fluorescent devices, or combinations/integrations\nthereof.  Display devices may also include less dynamic devices such as, for example, printers, e-ink devices, and the like.\nAs used herein, the term \"DOCSIS\" refers to any of the existing or planned variants of the Data Over Cable Services Interface Specification, including for example DOCSIS versions 1.0, 1.1, 2.0, 3.0 and 3.1.\nAs used herein, the term \"headend\" refers generally to a networked system controlled by an operator (e.g., an MSO) that distributes programming to MSO clientele using client devices.  Such programming may include literally any information\nsource/receiver including, inter alia, free-to-air TV channels, pay TV channels, interactive TV, and the Internet.\nAs used herein, the terms \"Internet\" and \"internet\" are used interchangeably to refer to inter-networks including, without limitation, the Internet.\nAs used herein, the term \"memory\" includes any type of integrated circuit or other storage device adapted for storing digital data including, without limitation, ROM.  PROM, EEPROM, DRAM, SDRAM, DDR/2 SDRAM, EDO/FPMS, RLDRAM, SRAM, \"flash\"\nmemory (e.g., NAND/NOR), and PSRAM.\nAs used herein, the terms \"microprocessor\" and \"digital processor\" are meant generally to include all types of digital processing devices including, without limitation, digital signal processors (DSPs), reduced instruction set computers (RISC),\ngeneral-purpose (CISC) processors, microprocessors, gate arrays (e.g., FPGAs), PLDs, reconfigurable computer fabrics (RCFs), array processors, secure microprocessors, and application-specific integrated circuits (ASICs).  Such digital processors may be\ncontained on a single unitary IC die, or distributed across multiple components.\nAs used herein, the terms \"MSO\" or \"multiple systems operator\" refer to a cable, satellite, or terrestrial network provider having infrastructure required to deliver services including programming and data over those mediums.\nAs used herein, the terms \"network\" and \"bearer network\" refer generally to any type of telecommunications or data network including, without limitation, hybrid fiber coax (HFC) networks, satellite networks, telco networks, and data networks\n(including MANs, WANs, LANs, WLANs, internets, and intranets).  Such networks or portions thereof may utilize any one or more different topologies (e.g., ring, bus, star, loop, etc.), transmission media (e.g., wired/RF cable, RF wireless, millimeter\nwave, optical, etc.) and/or communications or networking protocols (e.g., SONET, DOCSIS, IEEE Std. 802.3, ATM, X.25, Frame Relay, 3GPP, 3GPP2, WAP, SIP, UDP, FTP, RTP/RTCP, H.323, etc.).\nAs used herein, the term \"network interface\" refers to any signal or data interface with a component or network including, without limitation, those of the FireWire (e.g., FW400, FW800, etc.), USB (e.g., USB2), Ethernet (e.g., 10/100,\n10/100/1000 (Gigabit Ethernet), 10-Gig-E, etc.), MoCA, Coaxsys (e.g., TVnet.TM.), radio frequency tuner (e.g., in-band or OOB, cable modem, etc.), Wi-Fi (802.11), WiMAX (802.16), Zigbee.RTM., Z-wave, PAN (e.g., 802.15), power line carrier (PLC), or IrDA\nfamilies.\nAs used herein, the term \"QAM\" refers to modulation schemes used for sending signals over cable networks.  Such modulation scheme might use any constellation level (e.g. QPSK, 16-QAM, 64-QAM, 256-QAM, etc.) depending on details of a cable\nnetwork.  A QAM may also refer to a physical channel modulated according to the schemes.\nAs used herein, the term \"server\" refers to any computerized component, system or entity regardless of form which is adapted to provide data, files, applications, content, or other services to one or more other devices or entities on a computer\nnetwork.\nAs used herein, the term \"storage\" refers to without limitation computer hard drives, DVR device, memory, RAID devices or arrays, optical media (e.g., CD-ROMs, Laserdiscs, Blu-Ray, etc.), or any other devices or media capable of storing content\nor other information.\nAs used herein, the term \"wireless\" means any wireless signal, data, communication, or other interface including without limitation Wi-Fi, Bluetooth, 3G (3GPP/3GPP2), HSDPA/HSUPA, TDMA, CDMA (e.g., IS-95A, WCDMA, etc.), FHSS, DSSS, GSM,\nPAN/802.15, WiMAX (802.16), 802.20, Zigbee.RTM., Z-wave, narrowband/FDMA, OFDM, PCS/DCS, LTE/LTE-A, analog cellular, CDPD, satellite systems, millimeter wave or microwave systems, acoustic, and infrared (i.e., IrDA).\nOverview\nIn one aspect, a server that generates thumbnail information for a plurality of video files is disclosed.  In one exemplary embodiment, the thumbnails are generated by first generating a video file which assigns a majority of its bits to the key\nframe (e.g., intra-coded frame (I-frame)), and which minimizes the number of bits which are assigned to \"delta frames\" (e.g., forward predicted (P)- and bidirectionally predicted (B)-frames).  The video file is encoded along with other Adaptive Bitrate\n(ABR) files.  This allows for precise timing between \"playback time code\" and the thumbnail display time.  A still frame is then extracted from each key frame for use as a thumbnail during trick mode operation (e.g., fast forward, rewind, pause, or\nrandom seek operation).  When the encoded video file is given to a Just in Time Packager (JITP), the JITP extracts the image files (via the key frames) to be used as thumbnails.  Upon registration of a user, a JITP generates a manifest file listing all\ncomponents for playback.  These components, including the timecode and \"playback\" location for all thumbnail files are included.  Information regarding a naming convention and/or how to reference these files is stored in a master manifest file and may be\nreformatted by the JITP to fit the needs of any device which requires the file.  In a variant, the JITP reformats the image files extracted from the encoded video for operation on a number of devices (e.g., a variety of resolutions, frame rates, picture\nencodings, and color spaces, etc.).\nIn another embodiment, when a user enters a trick mode function (e.g., fast forward, rewind, pause, stop, random seek, etc.), the player uses the manifest file to begin making calls for each image in order to display thumbnail images to the user\nto browse through the content.  If this is the first request, the content delivery network will request the images from the JITP, and the JITP extracts the images, reformats the images, and provides the images to the requesting player via the content\ndelivery network.  The image may be viewed as a standard web file from a cache of images at the user device.\nIn a further embodiment, additional services may utilize the thumbnail stream and/or the JITP for image recognition (and associated advertising), automatic box art generation, search engine optimization (SEO), and quality of service (QoS) and\nerror checking and correcting.  These services may be performed in real-time because much of the pre-processing associated with the images has been performed in advance (by e.g., the encoder) via the creation of the thumbnail stream and/or real time\nextraction of thumbnails by the JITP.\nVarious other operational and/or business-related rules are disclosed.\nIn addition, content protection schemes may be advantageously deployed at e.g., the gateway, the client device, and/or one or more network entities, consistent with the various aspects disclosed herein.\n<BR><BR>DETAILED DESCRIPTION OF EXEMPLARY EMBODIMENTS\nExemplary embodiments of the apparatus and methods of the present disclosure are now described in detail.  While these exemplary embodiments are described in the context of the previously mentioned hybrid fiber coax (HFC) cable architecture\nhaving a multiple systems operator (MSO), digital networking capability, IP delivery capability, and a plurality of client devices/CPE, the general principles and advantages of the disclosure may be extended to other types of networks and architectures\nthat are configured to deliver digital media data (e.g., text, video, and/or audio).  Such other networks or architectures may be broadband, narrowband, wired or wireless, or otherwise.\nIt will also be appreciated that while described generally in the context of a network providing service to a customer or consumer (i.e., residential) end user domain, the present disclosure may be readily adapted to other types of environments\nincluding, e.g., commercial/enterprise, and government/military applications.  Myriad other applications are possible.\nIt is further noted that while exemplary embodiments are described primarily in the context of a cable system with 6 MHz RF channels, the present disclosure is applicable to literally any network topology or paradigm, and any frequency/bandwidth\nor transport modality.\nAlso, while certain aspects are described primarily in the context of the well-known Internet Protocol (described in, inter alia, RFC 791 and 2460), it will be appreciated that the present disclosure may utilize other types of protocols (and in\nfact bearer networks to include other internets and intranets) to implement the described functionality.\nOther features and advantages of the present disclosure will immediately be recognized by persons of ordinary skill in the art with reference to the attached drawings and detailed description of exemplary embodiments as given below.\nBearer Network--\nFIG. 1 illustrates a typical content delivery network configuration.  The various components of the network 100 include (i) one or more data and application origination points 102; (ii) one or more content sources 103, (iii) one or more\napplication distribution servers 104; (iv) one or more VOD servers 105, and (v) customer premises equipment (CPE) 106.  The distribution server(s) 104, VOD servers 105 and CPE(s) 106 are connected via a bearer (e.g., HFC) network 101.  A simple\narchitecture comprising one of each of the aforementioned components 102, 104, 105, 106 is shown in FIG. 1 for simplicity, although it will be recognized that comparable architectures with multiple origination points, distribution servers, VOD servers,\nand/or CPE devices (as well as different network topologies) may be utilized consistent with the present disclosure.  For example, the headend architecture of FIG. 1a (described in greater detail below), or others, may be used.\nThe data/application origination point 102 comprises any medium that allows data and/or applications (such as a VOD-based or \"Watch TV\" application) to be transferred to a distribution server 104.  This may include for example a third party data\nsource, application vendor website, CD-ROM, external network interface, mass storage device (e.g., RAID system), etc. Such transference may be automatic, initiated upon the occurrence of one or more specified events (such as the receipt of a request\npacket or ACK), performed manually, or accomplished in any number of other modes readily recognized by those of ordinary skill.  The application distribution server 104 comprises a computer system where such applications enter the network system. \nDistribution servers are well known in the networking arts, and accordingly not described further herein.\nThe VOD server 105 comprises a computer system where on-demand content is received from one or more of the aforementioned data sources 102 and enter the network system.  These servers may generate the content locally, or alternatively act as a\ngateway or intermediary from a distant source.\nThe CPE 106 includes any equipment in the \"customers' premises\" (or other locations, whether local or remote to the distribution server 104) that may be accessed by a distribution server 104.\nThe VOD server 105 and application distribution servers 104 are a part of the headend architecture of the network 100.  The headend 150 is connected to an internetwork (e.g., the Internet) 111.\nReferring now to FIG. 1a, one exemplary embodiment of a headend architecture is described.  As shown in FIG. 1a, the headend architecture 150 comprises typical headend components and services including billing module 152, subscriber management\nsystem (SMS) and CPE configuration management module 154, cable-modem termination system (CMTS) and 00B system 156, as well as LAN(s) 158, 160 placing the various components in data communication with one another.  It will be appreciated that while a bar\nor bus LAN topology is illustrated, any number of other arrangements as previously referenced (e.g., ring, star, etc.) may be used consistent with the disclosure.  It will also be appreciated that the headend configuration depicted in FIG. 1a is\nhigh-level, conceptual architecture, and that each MSO may have multiple headends deployed using custom architectures.\nThe exemplary architecture 150 of FIG. 1a further includes a conditional access system (CAS) 157 and a multiplexer-encrypter-modulator (MEM) 162 coupled to the HFC network 101 adapted to process or condition content for transmission over the\nnetwork.  The distribution servers 164 are coupled to the LAN 160, which provides access to the MEM 162 and network 101 via one or more file servers 170.  The VOD servers 105 are coupled to the LAN 160 as well, although other architectures may be\nemployed (such as for example where the VOD servers are associated with a core switching device such as an 802.3z Gigabit Ethernet device).  As previously described, information is carried across multiple channels.  Thus, the headend must be adapted to\nacquire the information for the carried channels from various sources.  Typically, the channels being delivered from the headend 150 to the CPE 106 (\"downstream\") are multiplexed together in the headend, as previously described and sent to neighborhood\nhubs (FIG. 1b) via a variety of interposed network components.\nIt will also be recognized, however, that the multiplexing operation(s) need not necessarily occur at the headend 150 (e.g., in the aforementioned MEM 162).  For example, in one variant, at least a portion of the multiplexing is conducted at a\nBSA switching node or hub (see discussion of FIG. 1c provided subsequently herein).  As yet another alternative, a multi-location or multi-stage approach may be used, such as that described in U.S.  Pat.  No. 7,602,820, entitled \"APPARATUS AND METHODS\nFOR MULTI-STAGE MULTIPLEXING IN A NETWORK\" incorporated herein by reference in its entirety, which discloses inter alia improved multiplexing apparatus and methods that allow such systems to dynamically compensate for content (e.g., advertisements,\npromotions, or other programs) that is inserted at a downstream network node such as a local hub, as well as \"feed-back\" and \"feed forward\" mechanisms for transferring information between multiplexing stages.\nContent (e.g., audio, video, data, files, etc.) is provided in each downstream (in-band) channel associated with the relevant service group.  To communicate with the headend or intermediary node (e.g., hub server), the CPE 106 may use the\nout-of-band (OOB) or DOCSIS channels and associated protocols.  The OCAP 1.0, 2.0, 3.0 (and subsequent) specification provides for exemplary networking protocols both downstream and upstream, although the present disclosure is in no way limited to these\napproaches.\n\"Switched\" Networks--\nFIG. 1c illustrates an exemplary \"switched\" network architecture.  While a so-called \"broadcast switched architecture\" or BSA network is illustrated in this exemplary network architecture embodiment, it will be recognized that the present\ndisclosure is in no way limited to such architectures.\nSwitching architectures allow improved efficiency of bandwidth use for ordinary digital broadcast programs.  Ideally, the subscriber is unaware of any difference between programs delivered using a switched network and ordinary streaming\nbroadcast delivery.\nFIG. 1c shows the implementation details of one exemplary embodiment of this broadcast switched network architecture.  Specifically, the headend 150 contains switched broadcast control 190 and media path functions 192; these element cooperating\nto control and feed, respectively, downstream or edge switching devices 194 at the hub site which are used to selectively switch broadcast streams to various service groups.  BSA media path 192 may include a staging processor 195, source programs, and\nbulk encryption in communication with a switch 275.  A BSA server 196 is also disposed at the hub site, and implements functions related to switching and bandwidth conservation (in conjunction with a management entity 198 disposed at the headend).  An\noptical transport ring 197 is utilized to distribute the dense wave-division multiplexed (DWDM) optical signals to each hub in an efficient fashion.\nCo-owned U.S.  Patent Application Publication No. 2003/0056217 filed on Sep. 20, 2001 and entitled \"TECHNIQUE FOR EFFECTIVELY PROVIDING PROGRAM MATERIAL IN A CABLE TELEVISION SYSTEM\", issued as U.S.  Pat.  No. 8,713,623 on Apr.  29, 2014, and\nincorporated herein by reference in its entirety, describes one exemplary broadcast switched digital architecture, although it will be recognized by those of ordinary skill that other approaches and architectures may be substituted.\nIn addition to \"broadcast\" content (e.g., video programming), the systems of FIGS. 1a and 1c (and 1d discussed below) also deliver Internet data services using the Internet protocol (IP), although other protocols and transport mechanisms of the\ntype well known in the digital communication art may be substituted.  One exemplary delivery paradigm comprises delivering MPEG-based video content, with the video transported to user PCs (or IP-based STBs) over the aforementioned DOCSIS channels\ncomprising MPEG (or other video codec such as H.264 or AVC) over IP over MPEG.  That is, the higher layer MPEG- or other encoded content is encapsulated using an IP protocol, which then utilizes an MPEG packetization of the type well known in the art for\ndelivery over the RF channels.  In this fashion, a parallel delivery mode to the normal broadcast delivery exists; i.e., delivery of video content both over traditional downstream QAMs to the tuner of the user's STB or other receiver device for viewing\non the television, and also as packetized IP data over the DOCSIS QAMs to the user's PC or other IP-enabled device via the user's cable modem.  Delivery in such packetized modes may be unicast, multicast, or broadcast.\nReferring again to FIG. 1c, the IP packets associated with Internet services are received by edge switch 194, and in one embodiment forwarded to the cable modem termination system (CMTS) 199.  The CMTS examines the packets, and forwards packets\nintended for the local network to the edge switch 194.  Other packets are discarded or routed to another component.\nThe edge switch 194 forwards the packets receive from the CMTS 199 to the QAM modulator 189, which transmits the packets on one or more physical (QAM-modulated RF) channels to the CPE.  The IP packets are typically transmitted on RF channels\n(e.g., DOCSIS QAMs) that are different that the RF channels used for the broadcast video and audio programming, although this is not a requirement.  The CPE 106 are each configured to monitor the particular assigned RF channel (such as via a port or\nsocket ID/address, or other such mechanism) for IP packets intended for the subscriber premises/address that they serve.\n\"Packetized Networks\"--\nWhile the foregoing network architectures described herein can (and in fact do) carry packetized content (e.g., IP over MPEG for high-speed data or Internet TV, MPEG2 packet content over QAM for MPTS, etc.), they are often not optimized for such\ndelivery.  Hence, in accordance with another embodiment of the disclosure, a \"packet optimized\" delivery network is used for carriage of the packet content (e.g., IPTV content).  FIG. 1d illustrates one exemplary implementation of such a network, in the\ncontext of a 3GPP IMS (IP Multimedia Subsystem) network with common control plane and service delivery platform (SDP), as described in co-pending U.S.  Provisional Patent Application Ser.  No. 61/256,903 filed Oct.  30, 2009 and entitled \"METHODS AND\nAPPARATUS FOR PACKETIZED CONTENT DELIVERY OVER A CONTENT DELIVERY NETWORK\", which is now published as U.S.  Patent Application Publication No. 2011/0103374 of the same title filed on Apr.  21, 2010, each of which is incorporated herein by reference in\nits entirety.  Such a network provides, inter alia, significant enhancements in terms of common control of different services, implementation and management of content delivery sessions according to unicast or multicast models, etc.; however, it is\nappreciated that the various features of the present disclosure are in no way limited to this or any of the other foregoing architectures.\nReferring now to FIG. 1e, another exemplary network architecture for the delivery of packetized content disclosure useful with the present disclosure.  In addition to on-demand and broadcast content (e.g., video programming), the system of FIG.\n1e may deliver Internet data services using the Internet protocol (IP), although other protocols and transport mechanisms of the type well known in the digital communication art may be substituted.\nThe network 1000 generally comprises a local headend 1001 in communication with at least one hub 1003 via an optical ring 1007.  The distribution hub 1003 is able to provide content to various user devices, CPE 1022, and gateway devices 1020,\nvia a network 1005.\nVarious content sources 1002 are used to provide content to a content server 1004.  For example, content may be received from a local, regional, or network content library as discussed in co-owned U.S.  patent application Ser.  No. 12/841,906\nfiled on Jul.  22, 2010 and entitled \"APPARATUS AND METHODS FOR PACKETIZED CONTENT DELIVERY OVER A BANDWIDTH-EFFICIENT NETWORK\", issued as U.S.  Pat.  No. 8,997,136 on Mar.  31, 2015, which is incorporated herein by reference in its entirety. \nAlternatively, content may be received from linear analog or digital feeds, as well as third party content sources.  Internet content sources 1010 (such as e.g., a web server) provide internet content to a packetized content server 1006.  Other IP\ncontent may also be received at the packetized content server 1006, such as voice over IP (VoIP) and/or IPTV content.  Content may also be received from subscriber and non-subscriber devices (e.g., a PC or smartphone-originated user made video).  In one\nembodiment, the functionality of both the content server 1004 and packetized content server 1006 may be integrated into a single server entity.\nA central media server located in the headend 1001 may be used as an installed backup to the hub media servers as (i) the primary source for lower demand services, and (ii) as the source of the real time, centrally encoded programs with PVR\n(personal video recorder) capabilities.  By distributing the servers to the hub stations 1003 as shown in FIG. 1e, the size of the fiber transport network associated with delivering VOD services from the central headend media server is advantageously\nreduced.  Hence, each user has access to several server ports located on at least two servers.  Multiple paths and channels are available for content and data distribution to each user, assuring high system reliability and enhanced asset availability. \nSubstantial cost benefits are derived from the reduced need for a large content distribution network, and the reduced storage capacity requirements for hub servers (by virtue of the hub servers having to store and distribute less content).\nIt will also be recognized that a heterogeneous or mixed server approach may be utilized consistent with the disclosure.  For example, one server configuration or architecture may be used for servicing cable, satellite, HFCu, etc. subscriber\nCPE-based session requests, while a different configuration or architecture may be used for servicing mobile client requests.  Similarly, the content servers 1004, 1006 may either be single-purpose/dedicated (e.g., where a given server is dedicated only\nto servicing certain types of requests), or alternatively multi-purpose (e.g., where a given server is capable of servicing requests from different sources).\nThe network 1000 of FIG. 1e may further include a legacy multiplexer/encrypter/modulator (MEM; not shown) coupled to the network 1005 adapted to \"condition\" content for transmission over the network.  In the present context, the content server\n1004 and packetized content server 1006 may be coupled to the aforementioned LAN, thereby providing access to the MEM and network 1005 via one or more file servers (not shown).  The content server 1004 and packetized content server 1006 are coupled via\nthe LAN to a headend switching device 1008 such as an 802.3z Gigabit Ethernet (or incipient \"10 G\") device.  Video and audio content is multiplexed at the headend 1001 and transmitted to the edge switch device 1012 (which may also comprise an 802.3z\nGigabit Ethernet device).\nIn one exemplary delivery paradigm MPEG-based video content may be delivered, with the video transported to user PCs (or IP-based CPE) over the relevant transport (e.g., DOCSIS channels) comprising MPEG (or other video codec such as H.264 or\nAVC) over IP over MPEG.  That is, the higher layer MPEG- or other encoded content may be encapsulated using an IP protocol, which then utilizes an MPEG packetization of the type well known in the art for delivery over the RF channels or other transport,\nsuch as via a multiplexed transport stream (MPTS).  In this fashion, a parallel delivery mode to the normal broadcast delivery exists; e.g., in the cable paradigm, delivery of video content both over traditional downstream QAMs to the tuner of the user's\nSTB or other receiver device for viewing on the television, and also as packetized IP data over the DOCSIS QAMs to the user's PC or other IP-enabled device via the user's cable modem.  Delivery in such packetized modes may be unicast, multicast, or\nbroadcast.  Delivery of the IP-encapsulated data may also occur over the non-DOCSIS QAMs.\nIndividual CPEs 1022 of the implementation of FIG. 1e may be configured to monitor the particular assigned RF channel (such as via a port or socket ID/address, or other such mechanism) for IP packets intended for the subscriber premises/address\nthat they serve.\nIn the switched digital variant, the IP packets associated with Internet services are received by edge switch, and forwarded to the cable modem termination system (CMTS) 1016.  The CMTS examines the packets, and forwards packets intended for the\nlocal network to the edge switch.  Other packets are in one variant discarded or routed to another component.\nThe edge switch forwards the packets receive from the CMTS to the QAM modulator, which transmits the packets on one or more physical (QAM-modulated RF) channels to the CPE.  The IP packets are typically transmitted on RF channels that are\ndifferent than the RF channels used for the broadcast video and audio programming, although this is not a requirement.  As noted above, the CPE are each configured to monitor the particular assigned RF channel (such as via a port or socket ID/address, or\nother such mechanism) for IP packets intended for the subscriber premises/address that they serve.\nIn one embodiment, both IP data content and IP-packetized audio/video content is delivered to a user via one or more universal edge QAM devices 1018.  According to this embodiment, all of the content is delivered on DOCSIS channels, which are\nreceived by a premises gateway 1020 (described subsequently herein) and distributed to one or more CPE 1022 in communication therewith.  Alternatively, the CPE 1022 may be configured to receive IP content directly without need of the gateway or other\nintermediary.  As a complementary or back-up mechanism, audio/video content may also be provided in downstream (in-band) channels as discussed above; i.e., via traditional \"video\" in-band QAMs.  In this fashion, a co-enabled digital set top box (DSTB) or\nother CPE could readily tune to the new (in-band) RF video QAM in the event that their IP session over the DOCSIS QAM is for some reason interrupted.  This may even be accomplished via appropriate logic within the CPE (e.g., autonomously, or based on\nsignaling received from the headend or other upstream entity, or even at direction of a user in the premises; e.g., by selecting an appropriate DSTB or other CPE function).\nIn the embodiment illustrated in FIG. 1e, IP packetized content is provided to various user devices via the network 1005.  For example, content may be delivered to a gateway apparatus 1020 which distributes content received thereat to one or\nmore CPE 1022 in communication with the apparatus 1020.\nIn another variant, elements in both the headend and CPE 1022 are specially adapted to utilize transmission infrastructure to transmit and receive both multiplexed wideband content and legacy content as is described in co-owned U.S.  patent\napplication Ser.  No. 11/013,671 filed on Dec.  15, 2004 and entitled \"METHOD AND APPARATUS FOR WIDEBAND DISTRIBUTION OF CONTENT\", issued as U.S.  Pat.  No. 9,723,267 on Aug.  1, 2017, which is incorporated by referenced herein in its entirety.  As\ndiscussed therein, the CPE 1022 or gateway 1020 of this embodiment may be configured to contain multiple tuners (or a single wide-band tuner) which allow the device to receive the signals from all of the relevant physical carriers simultaneously.  The\ncarriers are demodulated, and channel-based decryption and basic demultiplexing (recombination) is performed.  If multiplexed, the streams are then delivered to a transport demultiplexer which demultiplexes all of the streams resident within the\nstatistical multiplex.\nMethods and apparatus for the switched delivery of content may also be utilized consistent with the present disclosure.  For example, only that content for which there is at least one request from a user device may be provided.  In one\nembodiment, the methods and apparatus disclosed in co-owned U.S.  patent application Ser.  No. 09/956,688 entitled \"TECHNIQUE FOR EFFECTIVELY PROVIDING PROGRAM MATERIAL IN A CABLE TELEVISION SYSTEM\" and filed on Sep. 20, 2001, and issued as U.S.  Pat. \nNo. 8,713,623 on Apr.  29, 2014, which is incorporated herein by reference in its entirety, may be utilized for providing \"switched\" delivery of the IP content.  For example, a mechanism may be employed whereby the delivery of a session is based at least\nin part on logic to determine whether any users for the session are active; e.g., a multicast with no remaining \"viewers\" (or session participants) may be collapsed, and the bandwidth reclaimed.\nIn another variant, IP simulcast content and existing on-demand, voice, and broadcast content are all provided to the headend switch device 1008 of FIG. 1e.  The headend switch 1008 then provides the content to the optical ring 1007 for\nprovision to one or more distribution hubs 1003.  IP simulcast content is in one exemplary implementation retrieved from a plurality of content sources at an IPTV server.\nThe IP-packet content is transmitted to subscriber devices via the universal edge QAM 1018 and the edge network 1005.  The IP video (\"simulcast\") content is presented to client devices capable of receiving content over the DOCSIS QAMs.  For\nexample, the aforementioned gateway device 1020 (as well as an advanced CPE 1022 such as an IP-enabled DSTB may receive the IP simulcast.  Legacy CPE may receive content via the gateway device 1020, or via an audio/video \"back-up\" MPEG transport stream\nas previously described.\nIt is further appreciated that content may be delivered to various Worldwide Interoperability for Microwave Access (WiMAX)-enabled mobile devices (e.g., PMD or non-legacy CPE) via a WiMAX distribution hub of the type now ubiquitous in the\nwireless arts.  WiMAX is a wireless technology that provides high-throughput broadband connections over longer distances (as compared to short-range technologies such as WLAN, Bluetooth or PAN).  WiMAX can be used for a number of applications, including\n\"last mile\" broadband connections, cellular backhaul, hotspot coverage, and high-speed enterprise connectivity, as well as broadband delivery to mobile devices.\nMoreover, the aforementioned WiMAX technology may be used in conjunction with a WiMAX-enabled gateway (not shown) or CPE, such that content is delivered wirelessly to the gateway or CPE from the distribution hub, irrespective of the indigenous\nwired or optical distribution network infrastructure.\nIn the illustrated embodiment, the gateway device 1020 serves as a gateway to the IP content for other client devices (such as other CPE 1022 and PMD).  The gateway device 1020 may communicate with one or more connected CPE 1022, as well as\nutilize Wi-Fi capabilities (where so equipped) to communicate wirelessly to other devices.  It will also be recognized that the present disclosure may be configured with one or more short-range wireless links such as Bluetooth for lower bandwidth\napplications (or UWB/PAN for greater bandwidth applications).\nIn another embodiment, content received at a first user CPE 1022 may be transmitted to CPE 1022 of other premises in a peer-to-peer (P2P) fashion.  For example, first content may be requested and received at a first CPE 1022.  Then, when a\nsecond CPE 1022 in the same region or division requests the same content, the request may be examined by a headend entity (not shown), or the gateway 1020 acting as a peer proxy, to determine that the requesting second device CPE 1022 is entitled to\nreceive the content and that the content is available at the first CPE 1022.  The headend entity directs a peer-to-peer communication to be established between the authorized second CPE 1022 and the CPE 1022 having the requested content.  It is\nappreciated that while described herein in the context of a single CPE 1022 providing content to a second CPE 1022, several CPE 1022 having the content thereon may be contacted for simultaneous delivery of the content to one or more second CPE 1022.  In\none such implementation, the peer-to-peer communication methods and apparatus disclosed in co-owned U.S.  patent application Ser.  No. 11/726,095 entitled \"METHODS AND APPARATUS FOR CONTENT DELIVERY AND REPLACEMENT IN A NETWORK\" filed Mar.  20, 2007,\nwhich is incorporated herein by reference in its entirety, may be utilized in conjunction with the present disclosure.  As discussed therein, these P2P methods and apparatus also advantageously improve the \"robustness\" or capability of the network with\nrespect to ensuring that subscribers or other users can receive and access desired content when they want, as well as seamlessly repairing or reconstituting damaged or missed portions of that content (including even an entire streamed program, broadcast\nor download).\nIt is still further appreciated that the delivery of content may include delivery from an \"off-net\" distribution hub (not shown) to another network (not shown), not associated with the MSO.  In this embodiment, a requesting device (such as CPE\n1022 or gateway 1020) may request content from a local headend 1001 which is transferred over both MSO-maintained (\"on-net\") and \"off-net\" networks advantageously.\nPackeized Content Delivery Network Architecture for Recording Media--\nReferring now to FIG. 2, an exemplary configuration of a network useful with the present disclosure is illustrated.  While described in the context of an Internet Protocol network, it will be recognized that the principles of the disclosure may\nbe extended to other transport modalities and network paradigms.\nFIG. 2 discloses an exemplary configuration of an architecture 200 for providing video content to users 210 via a content delivery network (CDN) 206.  The CDN 206 is in communication with users 210 via the network 208.  In one embodiment of the\npresent disclosure, the network 208 comprises an internet, such as e.g., the Internet.  As shown, the CDN 206 is in communication with an encoder 202 and a Just in Time Packager (JITP) 204.  In one embodiment of the present disclosure, the encoder 202\nencodes a source file into at least one further encoding format (e.g., transcodes a source file from one format to at least one other format).  In another variant, the source file is encoded into a plurality of encodings that correspond to a respective\nplurality of one or more device types, codecs, resolutions, file formats, audio encodings, bit rates, etc. The variety of encodings may be utilized by the CDN 206 (and the JITP 204) via adaptive bitrate (ABR) streaming.\n\"Adaptive bitrate (ABR) streaming\" refers to streaming multimedia over computer networks such that a source file thereof is encoded into multiple bitrates.  Encoded content segments (or chunks) of varying bit rates are sent to a user. \nNon-limiting examples of ABR streaming include, without limitation, MPEG-Dynamic Adaptive Streaming over HTTP (DASH), Adobe.RTM.  Dynamic Streaming for flash, Apple.RTM.  HTTP Adaptive Streaming, Microsoft.RTM.  Smooth Streaming, QuavStreams.RTM. \nAdaptive Streaming over HTTP, and upLynk.RTM..\nThe encoder 202 produces a thumbnail stream from which thumbnail images are extracted for use in trick mode operation (e.g., fast forward, rewind, pause, and/or random seek) when viewing the content.  The JITP 204 extracts the thumbnails from\nthe key frames of the video track.  The JITP 204 then provides the extracted thumbnails to the CDN 206, which in turn provides them to a requesting user 210.\nThe requesting device 210 may include home gateway devices and/or media client devices.  In one embodiment a media client device is a portable device.  Common examples of portable devices include, without limitation, tablets, phablets, smart\nphones, smart televisions (TVs), desktop and laptop personal computers (PC), and portable media players.  In another embodiment, the media client device may comprise a file server; file servers are common in both commercial and residential use.  For\nexample, a subscriber may have a PC which can play media files, but which also serves his/her other consumer electronics (e.g., smart phone and tablet).\nIn yet another embodiment, the functionality of both the encoder 202 and the JITP 204 may be integrated into a single apparatus.  In a still further embodiment, the encoder 202 and the JITP 204 may be combined with the CDN 206 into a single\napparatus.  Additionally, the encoder 202 can, in another variant, feed data directly to the JITP 204, or may operate separately from the JITP 204.  Thus, each of the CDN 206, encoder 202, and JITP 204 may be housed in a separate server apparatus, may\ncomprise different processes running on a single server apparatus, or may comprise any number of distributed applications with functionality across any number of apparatus.  Each server apparatus 202, 204, and 206 may include memory and storage, one or\nmore processors, and interfaces to connect to other components/networks as shown in FIG. 2.\nEncoder--\nReferring now to FIG. 3a, a simplified input/output diagram 300 of an encoder 202 is illustrated.  A source file 302 is an input to the encoder 202.  The source file may be encoded in a variety of formats (both audio and video), bit rates,\nresolutions, which are each playable on a variety of devices.  Accordingly, one or more output streams 306 are produced by the encoder 202.  For example, a content delivery network may enable a wide variety of user devices to play a certain piece of\ncontent.  Accordingly, a network operator selects to have the encoder 202 encode the content into multiple formats for use on the variety of players.  In a further embodiment, a network operator selects to utilize adaptive bitrate streaming such that\nmultiple bit rate streams are utilized by selecting an optimized stream 306, e.g., the stream 306 that best utilizes the viewer's device and current bandwidth constraints to provide an optimal playback experience.  The optimization occurs via a process\nor application running at the encoder 202.\nAs noted previously, various content sources may provide source files 302 to the encoder 202.  For example, content may be received from a local, regional, or network content library as discussed in co-owned U.S.  application Ser.  No.\n12/841,906 filed on Jul.  22, 2010 and entitled \"APPARATUS AND METHODS FOR PACKETIZED CONTENT DELIVERY OVER A BANDWIDTH-EFFICIENT NETWORK\", issued as U.S.  Pat.  No. 8,997,136 on Mar.  31, 2015, which is incorporated herein by reference in its entirety. \nAlternatively, content may be received from linear analog or digital feeds, as well as third party content sources.  Internet content sources (such as e.g., a web server) may also provide Internet content to the encoder 202.  In yet another embodiment,\ncontent may be received from subscriber and/or non-subscriber devices (e.g., a PC or smartphone-originated user made video).\nParameters used by the encoder 202 to encode the source file 302 include: (i) whether an output should include an audio only track, or a thumbnail track with audio included, (ii) whether the streams should be encrypted and via which encryption\nformat (e.g., AES, DES, public key encryption, etc.), (iii) an appropriate key frame period, (iv) a frame rate, (v) segmentation duration, (vi) video resolutions, (vii) video bitrate, (viii) audio bit rate (where necessary), (ix) audio sample rate, (x) a\nnumber of audio channels, (xi) aspect ratio, (xii) video codec, (xiii) specific device profiles, (xiv) audio volume, (xv) file type and extension, and (xvi) standard specific encoding profiles.  One example of a standard specific encoding profiles,\nincludes e.g., h.264 which includes different standard encoding profiles for baseline, main, and high encoding.  Additionally, the encoder 202 may utilize information used for cutting out other resolutions/aspect ratios from a higher resolution/different\naspect ratio file.  For example, the encoder 202 may center-punch a standard definition (SD) image or video from a high definition (HD) source.  Similarly, an HD image or video may be center punched from a 4K, 8K, 16K source.  In an alternative\nembodiment, the foregoing parameters may be used by the JITP 204, or a combination of any of the foregoing parameters may be utilized between both the JITP 204 and encoder 202.  For example, the JITP 204 may utilize the segmentation duration parameters\nin order to splice the content streams 306 (or thumbnail stream 308) into chunks.  In another example, the JITP 204 may utilize capabilities information of the end device 210 to create chunks from the output streams 306 that meet the specifications of\nthe requesting device 210 (and put those addresses of those chunks in a manifest file).\nIn one variant, the encoder 202 up-converts source files 302 to produce higher bit rate and higher resolution output streams 306.  This could allow smaller bit rate source files 302 to be provided to the encoder 202 or allow a source file 302\nthat is natively lower resolution to be encoded into a wide array of output streams 306 for use on a wider variety of user devices 210 with a greater number of resolutions.  The source file 302 may comprise an uncompressed source file 302 (when received\nat the encoder 202) and/or a source file 302 that has various degrees of compression.\nAlongside the output streams 306, a thumbnail stream 308 is generated from the source file 302.  The generation of the thumbnail stream 308 contemporaneous to the generation of the output streams 306 allow for precise timing (e.g.,\nsynchronization) between the playback time code and the thumbnail display time.  The thumbnail stream 308 comprises a stream of key frames (e.g., I-frames) at a certain temporal distance (e.g., 2 seconds or 10 seconds) apart.  Key frames may be placed in\nany number of temporal distances from 0 (i.e., every frame is a key frame), to a single key frames and all delta frames per stream.  A greater number of key frames allows the video stream to be segmented in a greater number of places (however, the amount\nof compression available may be limited where the number of key frames used is higher, but processing of the video frame would correspondingly be less because the full image does not have to be generated from predictive delta frames as frequently).  In\nthe thumbnail stream 308, transitional data may be minimized or eliminated through the use of null frames.  In a further embodiment, certain transitional data (e.g., B- or P-frames) is included in the thumbnail stream 308 to further compress the stream\nand provide a greater variety of thumbnail images.  In an additional embodiment, a plurality of thumbnail streams 308 are outputted from the encoder 202.  Each of the plurality of thumbnail streams 308 may include the key frames at different resolutions\nor encodings or may have different data in the portions between key frames.  The different data may include descriptive metadata of the content, or demographics/psychographic profiles of expected users, or information regarding how the data within the\nstream is formatted.\nIn one embodiment, the generated thumbnail images 358 or the thumbnail stream 308 may be used by the JITP 204 (or a parallel processing service) for image recognition and quality control processes.  In an additional embodiment, a content curator\nmay use the thumbnail stream 308 to scrub for the best possible image to represent the content.  The curator may request from the JITP 204 a particular time code and final image details such as image proportions, image resolution, and/or file type and a\nthumbnail file 358 could be extracted and sent to the content management system (CMS) of the curator for use as box art or thumbnail art to represent the content.\nThe output streams 306 are provided to users for display of the content contained therein.  The Just in Time Packager (JITP) 204 extracts thumbnails 358 from the thumbnail stream 308 (as discussed in further detail below with respect to FIG.\n3b).  In further embodiments, the encoder 202 may also break the encoded output streams 306 into chunks for use by the CDN 206 to serve to users 210.  Furthermore, the encoder 202 in such embodiments generates manifest files that reference the locations\nof the chunks.  An exemplary manifest file 354 is discussed in greater detail below with respect to FIG. 3b.\nWhile output streams 306 and thumbnail stream 308 are shown as separate files (for example MPEG 4 transport stream files (.ts)).  In a further embodiment of the present disclosure, all of the streams (i.e., streams 306 and 308) are all present\nin a single \"super\" file.  Having a single comprehensive file comprising multiple streams (including the thumbnail stream) will lower the number of files the CDN 206 must manage.  This allows for easy management around the thumbnails.  A content curator\nwould only have to manage the single file.  The CMS only has to represent the single file.  And the operator only would have to confirm the existence of the single file.\nThe encoder 202 may encode output streams 306 with audio tracks (e.g., AC3 audio).  Different encoding formats and bit rates may be selected based on the requirements of the stream, end user equipment, and the protocols and formats used by the\nCDN 206.  The thumbnail stream 308 may not be encoded with an audio track.  This may allow the thumbnail stream file to have a smaller file size.  In one embodiment, the thumbnail stream 308 has an audio track as discussed in further detail below.\nJust in Time Packager--\nReferring now to FIG. 3b, a simplified input/output diagram 350 of a Just in Time Packager (JITP) 204 is illustrated.  Encoded output streams 306 and a thumbnail stream 308 are utilized by the JITP 204 to provide a manifest file 354, video\nchunks 356, and/or thumbnails 358 to a requesting CDN 206 or user.\nSpecifically, the manifest (or index/playlist) file 354 is a data structure comprising a listing of addresses for each of the chunks 356 of a stream of data (such as video chunks 356 or thumbnails 358) including bitrates, thumbnails, closed\ncaptioning, audio, etc. Different ABR models may use different manifest files 354.  For example, with HTTP Smooth Streaming (HSS), each of the components (thumbnails, closed captioning, audio, etc.) are in separate files with addresses for each in the\nmanifest.  With HTTP Live Streaming (HLS), audio is embedded in the chunks 356 and thus are not separately listed in the manifest file.  In an exemplary embodiment, a media client of the type discussed in co-owned co-pending U.S.  application Ser.  No.\n14/220,021 filed on Mar.  19, 2014 and entitled \"APPARATUS AND METHODS FOR RECORDING A MEDIA STREAM\", which is incorporated herein by reference in its entirety may be utilized.  The media client replays stored \"chunked\" media content 356 based on a\nstream manifest file 354.  In one exemplary embodiment, stored video content streams chunks 356 are decompressed for playback based on information stored within an associated data structure (e.g., stream manifest file 354).  Examples of information\nstored within the manifest file 354 may include e.g., encryption keys, supported resolutions, and digital rights information.  A video client may be further configured to retrieve additional data to augment the stored chunked video content 356.  For\ninstance, a video client with a high resolution (e.g., 1920.times.1080) display may consult the stream manifest file 354 for missing and/or replacement chunks 356, when attempting to replay a previously recorded lower resolution recording (e.g.,\n640.times.480).  By downloading the appropriate chunks 356, the video client may support the desired high resolution.\nIn another embodiment, the network stream manifest 354 includes metadata, and a listing of media chunk entries.  Metadata refers to information used by the media client device to interpret or otherwise manage the media chunks (metadata is also\ncolloquially referred to as \"data regarding data\" or \"data relating to data\").  Common examples of metadata include e.g., version information, protocol, file formats, supported codecs, resolution, encryption, temporal information (transmission time, time\nof presentation, time stamps, etc.), geographic information (restricted locations, locations for presentation, etc.), content type indicia, synchronization information, control data, etc. Stated differently, the metadata describes the media chunks 356,\nand can be used as a reference file when assessing or otherwise making use of the media chunks 356.\nIn one implementation, the list of media chunk entries in the manifest file 354 comprises a list of network addresses where the corresponding chunks 356 of media content may be accessed and/or downloaded.  For instance, each of the media chunk\nentries may be listed by a Uniform Resource Locator (URL).  In some embodiments, the entries may be in computing resource \"path\" format.  Computing paths may be either absolute (i.e., the path provides the fully elaborated and unique location of the\nchunk 356 in a file structure 354) or relative (i.e., the path provides a relative location of the chunk 356 in a file structure 354).  Additionally, in some embodiments, the entries may be in symbolic format, such that at least a portion of the entry\nmust be further interpreted (i.e., is not human-readable).  Common examples of this may include e.g., HyperText Markup Language (HTML) tags, proprietary tags, Java, Javascript, etc. Moreover, some implementations may substitute or intermingle any of the\nforegoing techniques to flexibly accommodate various operational models.\nIn another embodiment, the service provider or MSO is represented as a single logical entity (a single network domain) represented by a characteristic URL (e.g., www.timewarnercable.com).  In other embodiments, the service provider may be a\nconglomeration of multiple logical entities.  Multiple logical entities may be useful to further distribute services over various network resources or enable additional features provided by partnered corporations or providers.  Multiple logical entities,\nfor example, may provide local content for a particular service group or geographic area.  Furthermore, having content providing entities closer to end users may offer lower latency and may add network redundancy.  Common examples of network resources\ninclude e.g., broadcast, multicast, video-on-demand, advertisement services, local services, etc. In one specific example, one exemplary stream manifest file may include entries from: www.timewarnercable.com, vod.timewarner.com (video on demand\nservices), www.nhk.jp (3.sup.rd party content), www.adserver.com (3.sup.rd party advertisement services), etc.\nIn another example, the media chunk listing may include a listing of URL links which is further punctuated with HTML tags or Javascript, which is configured to enable advertisement insertion and/or execution of complementary programming.  For\ninstance, the video client may substitute tailored locally stored advertisements for commercial breaks, rather than e.g., the default broadcasted commercial.  In other embodiments, the video client may run a Javascript Applet that allows the subscriber\nto execute a command or otherwise provide feedback (e.g., to order pizza, vote on a reality show, etc.).\nThe thumbnail stream 308 or generated thumbnail images 358 may be used for advertisement insertions or search engine optimization (SEO).  The stream 308 or images 358 may be processed in parallel by the JITP 204 or another server apparatus\n(e.g., a dedicated image recognition service) in communication with the CDN 206.  This parallel processing allows for image recognition within the content, for example, an actor's cameo in a movie (e.g., Tom Cruise's cameo in Tropic Thunder). \nFurthermore, parallel processing allows for the generation of in and out points for the times the actor is in the piece of content based on all of the thumbnails 358 pulled from the JITP 204 by the image recognition service.  Product placements may also\nbe time stamped via the parallel image recognition processing.  Network operators may use advertisement breaks during the presentation of the content or adjacent to the time stamp of the product placement to utilize the product placement time stamp to\nsell or place the advertisement block.  The network operator may sell or place an advertisement for the company whose product has been placed (or, e.g., to a competitor of that company).  For example, if a Coke.RTM.  can is used in a scene of a movie,\nadvertisement blocks could be sold to Coca-Cola.RTM.  or to Pepsi.RTM.  during an advertising segment adjacent to the Coke can's appearance in the movie.\nIn the exemplary embodiment, each media chunk 356 is an encoded (and optionally encrypted) subsection or segment of media content.  The media chunks 356 (decrypted if necessary), when decoded and played in the appropriate order, render the\noriginal media content.  In one implementation, each media chunk 356 represents a portion of video associated with a specific resolution, codec, and time stamp.  The media chunks 356 are assembled according to a time stamp sequence.\nIn another embodiment, however, non-time-based segments may be used in the manifest 354.  For example, playback may occur according to the context of the sequence and not because of any implied meaning of the filename, or time stamp value.  The\ntrue duration of a video segment is based on its contents and its presentation time stamp (PTS), which may not be represented in the manifest.  The sequence of the next file in the manifest is simply what comes next.  Specifically, any schema could be\nused for the transport stream files in the manifest, including 1.ts, 2.ts, 3.ts, etc .  . . Or A.ts, B.ts, or C.ts.\nBased on the registration of a user, manifest files 354 listing all components for playback of a piece of content are generated by the JITP 204.  This information includes the timecode and \"playback\" location for all thumbnail files 358.  In an\nalternative embodiment, the manifest file 354 (or a plurality of manifest files 354) is pre-generated for use with one a particular ABR format.  In this embodiment, the thumbnail files 358 are not generated until an address (e.g., URLS) of thumbnails are\nrequested at which time the JITP 204 generates the thumbnail file 358 from the thumbnail stream 308.\nFurthermore, manifest files 354 contain addresses (e.g., URLs) of thumbnails generated by the JITP 204 for use when a user inputs a trick (e.g., fast forward, rewind, pause, or random seek) command.  The manifest files 354 are generated based on\nthe specific device and requirements of an end user device.  For example, the Microsoft.RTM.  Xbox.RTM.  360 and Xbox.RTM.  One video game systems require different manifest files 354 to operate.  Furthermore, different streaming standards may require\ndifferent manifest files 354 to operate.  For example, the MPEG-Dynamic Adaptive Streaming over Hyper Text Transfer Protocol (DASH) protocol may be implemented differently with respect to Hyper Text Transfer Protocol (HTTP) live streaming and\nWindows.RTM.  Media Streaming.  Thus, each may require different manifest files 354.\nVideo chunks 356 chunks are generated by the JITP 204.  The chunks 356 may be of predetermined length.  In addition, metadata describing the chunks may be generated at the JITP 204, or, alternatively at the encoder 202.  As discussed herein, the\nfile chunks 356 form the basis for the generation of a network stream manifest file 354.  It is appreciated, however, that the foregoing functionality may be accomplished at various other network entities (such as at the encoder 202 or CDN 206), the\nforegoing being merely exemplary.  For example, chunking and encryption prior to a request for the content may optimize time to playback particularly for the first requestor.  However, such a setup may pose a challenge for content management and storage\nscale.  For example, if an encryption model is changed due to a hack the video content may need to be re-encrypted prior to service.\nAccess control such as Digital Rights Management (DRM), conditional access (CA), trusted domain (TD), etc. may be implemented by the JITP 204 (or alternatively the CDN 206).  One example of utilization of the foregoing technologies is described\nwithin co-owned U.S.  patent application Ser.  No. 13/710,308 filed on Dec.  10, 2012 and entitled \"APPARATUS AND METHODS FOR CONTENT TRANSFER PROTECTION\", issued as U.S.  Pat.  No. 9,565,472 on Feb.  7, 2017, which is incorporated herein by reference in\nits entirety.  As discussed therein content is delivered via a managed content distribution network (such as a cable or satellite or HFCu network having an MSO), and the MSO manages the rights and restrictions of the content outside of a premises, and in\na data center or headend, by providing requested content to a gateway device within the premises of a user.\nThe content is, in the exemplary embodiment, provided in a first encryption format and encoded using a first codec, both of which are compatible with the gateway device.  In order to provide for a transfer of the content within and outside of\nthe premises network, the gateway is configured to transcrypt the content into an encryption format, and transcode using a codec, that are each compatible with a device which requests the content therefrom.  In one implementation, the content is received\nat the gateway as MPEG-2 content encrypted using Powerkey conditional access (CA) technology.  The gateway uses its associated CableCard to decrypt the content, and a transcoder entity to transcode the content to e.g., MEPG-4 (or other appropriate\nformat).  The content is then re-encrypted to DRM using a content key obtained from a DRM server and a transcrypter of the gateway.  This approach advantageously preserves content rights, and asserts restrictions on use or distribution of content, via,\ne.g., the user's premises gateway.\nAdditionally, the video chunks 356 (and thumbnail files 358) may be encrypted by the JITP 204 (such as via a DES or AES algorithm via a symmetric or asymmetric key approach) prior to transfer over the network.  The video chunks 356 (and\nthumbnail images 358) may then be decrypted by a user device.\nThumbnail files 358 are generated by the JITP 204 using the thumbnail stream 308.  Alternatively, thumbnail files are generated by the JITP 204 using one or more of the output streams 306.  The thumbnail stream 308 contains key frames (i.e.,\nwhole frames that fully define an image).  These key frames are extracted by the JITP 204 and outputted as thumbnail images 358.  In an embodiment, some but not all key frames from the thumbnail stream 308 are extracted for use as thumbnails.  In an\nalternative embodiment, all key frames are extracted.  Reference to these thumbnail images 358 is made in the manifest file 354.  The JITP 204 extracts the key frames and skips over the other (e.g., null) frames in the thumbnail stream 308.  The small\nsize of the thumbnail stream 308 makes it easier to parse and extract than using the output streams 306.  The thumbnail images 358 can be resized, encoded, or altered by the JITP 204 during extraction to allow for a wide variety of file types from a\nsingle thumbnail stream 308.  This extraction can occur on an as needed basis upon the request of the CDN 206 or user.  Thus, a multitude of thumbnail images would not need to be stored and managed by the CDN 206.  The thumbnail stream 308 could be\nparsed and the thumbnails 358 extracted and resized/altered/encoded on the fly, in real time, by the JITP 204.\nThe JITP 204 (or, in an alternative embodiment the CDN 206 or another server or process) removes or deletes the extracted thumbnail images.  In an embodiment, the extracted thumbnail images are removed after the requesting user has finished\nviewing the content.  Alternatively, the extracted thumbnail images can be removed after the user session completes, when there are no longer any pending user sessions requesting the content, the thumbnail images, and/or the manifest file expires.  In a\nfurther alternative embodiment, the JITP 204 may be utilized for extracting thumbnail images from a thumbnail stream and an encoder 202 segments and stores various content chunks.\nABR Streams and the Thumbnail Stream--\nReferring now to FIG. 4, a diagram 400 illustrating various stream types are shown.  Streams 402, 404, and 406 illustrate three different adaptive bitrate (ABR) level streams.  Stream 408 illustrates a thumbnail stream.  As shown in FIG. 4, each\nof the key frames is spaced N seconds apart.  The spacing of key frames may be as little or as much as desired for the particular application however having longer chunks may coincide with less ability to change ABR level 402, 404, 406.  This is useful\nwhen using ABR as the content is broken up into standardized (i.e., equal time) chunks and so different ABR levels 402,404, or 406 may be selected for each chunk and the timing of key frames within chunks will line up.  As will be understood by those of\nordinary skill, having more chunks, may coincide with increased overall file sizes (due to more key frames) and greater file management.  Chunks are, in one embodiment, broken up along key frame boundaries; chunks may comprise multiple key frames (and\ntransitional frames) or only a single key frame.  This depends on the length of the chunk and the level of compression of the content.  In an alternative embodiment, chunks are not broken up at key frame boundaries.\nEach ABR level 402, 404, and 406 is packaged a separate stream file (for example an MPEG 4 stream file).  The thumbnail stream 408 is packaged in a separate stream file (e.g., MP4 file) as well.  Alternatively, all of the streams can be packaged\ntogether in a single file allowing the CDN 206 to manage fewer files.\nEach ABR level 402, 404, and 406 comprise key frames 410, 412, and 414.  Between key frames are transitional frames 418, 420, and 422.  The key frames 416 of thumbnail stream 408 are also separated by N seconds like in the ABR streams 402, 404,\nand 406.  Unlike the ABR streams 402, 404, and 406, the thumbnail stream 408 contains no transitional data 418, 420, and 422 between key frames.  Instead, null frames 424 separate the key frames 416.  The null frames 416 allow the key frames 416 to be\nseparated temporally in the video stream 408 but take up a minimal amount of storage.  This allows the stream to be quickly and easily parsed to extract the key frames 416 as thumbnail images.\nIn an alternative embodiment, the key frames 416 in the thumbnail stream 408 are separated by a different temporal distance than the key frames 410, 412, and 414 in the ABR streams 402, 404, and 406.  In further alternative embodiments, the null\nframes 424 are used to store other, non-content data such as stream metadata.  Such non-content data can be stripped by the JITP 204 when extracting the thumbnail images.\nWhen requested by a user or CDN 206, the JITP 204 parses the thumbnail stream 408 and extracts the key frames 416 as image files.  The extracted image files may be resized or reformatted from the original images in the key frames of the\nthumbnail stream 408.  That is, the resolution, compression, aspect ratio (e.g., cutting out or center punching the images or adding black bars), etc. may be changed depending on the request by the user or CDN 206.  Furthermore, extracted image files may\nhave filters or effects applied or closed captioning information or other data can be applied to the extracted images.  The JITP 204 then temporarily (or, alternatively, permanently) stores the image files and publishes the addresses of these image files\nin a manifest file for use by the CDN 206 or user.  Thus, thumbnail files would not need to be created and managed for each protocol, device, and resolution but instead may be generated on the fly as they are requested by the user.  This solution saves\ndisk space, requires less file management, and is scalable for the many different end user devices that would utilize the thumbnails.\nABR streams 402, 404, and 406 may contain or be associated with audio tracks.  In an embodiment of the present disclosure, thumbnail track 408 does not contain or is not associated with an audio track so as to reduce the file size of the\nthumbnail stream 408.  In an alternative embodiment, the thumbnail track 408 is associated with an audio track to create a very low bit rate stream for users that still provides a visual context for users.  This embodiment provides improvements over an\naudio track alone or an audio track and a single thumbnail as a user may still follow the visual elements on screen.\nMethods--\nAs referenced above, various embodiments of the present disclosure are directed to schemes for the creation, storage, and playback of content streams and thumbnails.\nReferring now to FIG. 5a, one embodiment of a method 500 for encoding a content stream is disclosed.\nAt step 502 of the method 500, the encoder 202 receives inputs of a source file 302.  The encoder 202 may also receive encoding requirements of the CDN 206 and/or users.  The encoder 202, at step 504, encodes the source file 302 into a plurality\nof encoding types (i.e., creates multiple versions of the source file 302 each having a different encoding type).  These may include various resolutions and formats for use by a variety of user devices; the generated versions of the source file 302 may\nbe used for ABR streaming.  At step 506, a thumbnail stream 308 is encoded.  The encoding process may include stripping a previously encoded stream of its transitional frames and audio track, leaving only key frames, and encoding the thumbnail stream 308\ntherefrom.  As discussed above, the thumbnail stream 308 comprises key frames separated by null frames.  Alternatively, the encoding process processes the source file 302 and only includes certain ones of the key frames in the thumbnail stream 308.\nReferring now to FIG. 5b, one embodiment of a method 520 for operating a content delivery network is disclosed.\nAt step 522 of the method 520, a CDN 206 receives a request for content from a user of the CDN 206.  At step 524, the CDN 206 parses the request and forwards the request to a JITP 204.  Along with the request for content, the CDN 206 may also\nsend device capabilities of the user device 210 including, but not limited to, the device type, manufacturer, model number, serial number, current display resolution, file formats accepted, codecs that are displayable, bit rates that are playable, and\nstreaming technologies accepted.  Furthermore, in the case of a gateway device 210, the gateway device 210 may send the foregoing properties of the display equipment 210 (e.g., CPE, television, smart television, computer, tablet, phone, etc) or some or\nall of the devices 210 that the gateway device 210 serves.  These device capabilities may be used to format the streams for later display on the end user devices 210.\nIn one embodiment, a user makes a request from a user device that passes the request through a home gateway device as disclosed in co-owned U.S.  patent application Ser.  No. 11/818,236 entitled \"PREMISES GATEWAY APPARATUS AND METHODS FOR USE IN\nA CONTENT-BASED NETWORK\" filed on Jun.  13, 2007, issued as U.S.  Pat.  No. 7,954,131 on May 31, 2011, which is incorporated herein by reference in its entirety.  In this embodiment, the home gateway device passes the user device details and the request\nfor content to the CDN 206 (which can be passed to the JITP 204).\nIn an alternative embodiment, the CDN 206 predicts that content will be requested at a future time period by the client based on, e.g., previous user requests or requests of content from similarly situated users (e.g., users who are\ndemographically or psychographically similar).  For example, content may be selected to align with user preferences as discussed in co-owned U.S.  application Ser.  No. 12/414,576 filed on Mar.  30, 2009 and entitled \"RECOMMENDATION ENGINE APPARATUS AND\nMETHODS\", issued as U.S.  Pat.  No. 9,215,423 on Dec.  15, 2015, which is incorporated herein by reference in its entirety.  Alternatively, the user may subscribe to a particular piece of content.  In such embodiments, the CDN 206, during a period of low\nnetwork activity (e.g., late night), may request that the JITP 204 generate chunks 356, a manifest file 354, and/or thumbnails 358 and have the user device download such content to preload the content at the user device for later viewing.  In a further\nalternative, the CDN 206 preloads a content stream file, e.g., the highest ABR stream, or a plurality of content stream files, and the thumbnail stream 308 on the home gateway device at the user premises (with appropriate digital rights management (DRM))\nand have a JITP 204 running at (or co-located with) the home gateway device generate content chunks 356, manifest files 354, and thumbnails 358 for a single or a plurality of user devices 210 (e.g., a high definition television and a user's cellular\ntelephone) in real-time.  In a further alternative, the CDN 206 preloads a source stream 302 and have the gateway device encode the different ABR streams 306 and thumbnail stream 308.\nIn an alternative embodiment, a virtual channel is streamed to the CPE 210 via CDN 206.  The channel may comprise content that is selected for the user by the CDN 206 or the user device 210.  The virtual channel may utilize content encoded by\nthe encoder 202 and packaged by the JITP 204.  For example, the content on the virtual channel may be targeted content as discussed in co-owned co-pending U.S.  application Ser.  No. 12/414,554 filed on Mar.  30, 2009 and entitled \"PERSONAL MEDIA CHANNEL\nAPPARATUS AND METHODS\", which is incorporated herein by reference in its entirety.\nAt step 526, the CDN 206 receives a response to the request for content from the JITP 204.  The JITP 204 provides a manifest file 354, chunks of a content file 356, and/or thumbnail files 358.  At step 528, one or more of the provided files 354,\n356, and/or 358 are sent to the user device 210.\nReferring now to FIG. 5c, one embodiment of the method 540 of operating a JITP 204 is disclosed.\nAt step 542, the JITP 204 receives a request for content from a CDN 206 on behalf of a user.  At step 544, the JITP 204 determines the user requirements from the request, these include the content that is requested, the format for the content\n(e.g., resolution, bit rate, etc.).  At step 546, thumbnails 358, program chunks 356, and/or a manifest file 354 is generated.  In an embodiment, the JITP 204 determines that a user requests a number of thumbnails 358 via a trick mode request.  The JITP\n204 will generate the needed thumbnails 358 for the user from the thumbnail stream 308.  At step 548, the JITP 204 stores the thumbnails 358 and provides the user a manifest file 354 with the addresses for the content.  Alternatively, the JITP 204 sends\nthe thumbnail files 358 to the user directly (or via the CDN 206).\nReferring now to FIG. 5d, one embodiment of the method 560 of operating consumer premises equipment (CPE) 210 is disclosed.\nAt step 562, the CPE 210 sends device specifications and/or network conditions to the CDN 206.  The device specifications may include the device type, encryption options and settings, device aspect ratio, and supported codecs and file types. \nNetwork conditions may include current throughput, predicted available bandwidth, network latency.  For example, latency may be determined as discussed in co-owned U.S.  application Ser.  No. 12/944,648 filed on Nov.  11, 2010 and entitled \"APPARATUS AND\nMETHODS FOR IDENTIFYING AND CHARACTERIZING LATENCY IN A CONTENT DELIVERY NETWORK\", issued as U.S.  Pat.  No. 8,930,979 on Jan.  6, 2015, which is incorporated herein by reference in its entirety.  Alternatively (or additionally), the device may send an\nacceptable bitrate for the content stream based on network conditions.  For example, a user could detect throughput of 3 Mbps (Megabits per second) and request a bitrate of 2.25 Mbps (75% of the detected throughput).  The proportion of available\nbandwidth can be based on other current bandwidth uses, predicted bandwidth uses, or the number (or ratio) of retransmission requests sent.\nAt step 564, a user enters a trick mode command (e.g., fast forward, rewind, pause, or random seek), and the CPE 210 requests thumbnail images 358 based on the trick mode request.  At step 566, the CPE 210 receives the thumbnails 358 or a\nmanifest file 354 containing the locations of the thumbnails 358 on the network.  In the later embodiment, the CPE 210 then downloads the thumbnails 358 from the locations listed in the manifest file 354.\nIn one implementation, the manifest 354 has a fixed number of content chunk entries which are sequentially ordered.  For example, typical playlists consist of ten (10) six-second (6 s) chunks of video content, representing a minute of video.  As\neach chunk is played, a replacement chunk is added (similar to a First-In-First-Out (FIFO) buffer).  Other implementations may expand or contract or have non-sequential ordering according to e.g., various network considerations.  For example, the network\nstream manifest may dynamically adjust length based on available servers, available bandwidth, etc. In other cases, the network stream manifest may have a first section (e.g., for program content), and a second lower priority section for background\nloading (e.g., for advertisements).  For streams which support multiple different technologies and/or resolutions, the stream manifest may be \"striped\" or split into portions, each stripe or portion associated with a particular technology or resolution,\netc. Furthermore, the manifest 354 may contain a fixed number of thumbnail images 358 which are sequentially ordered.  For example, a playlist may contain 20 thumbnails of video content representing two minutes of thumbnails (i.e., one thumbnail per six\nseconds).  Such thumbnails 358 represent one forward minute and one rewind minute of thumbnail images.\nAt step 568, the CPE displays the thumbnail images 358 to the user while the user browses the trick mode operation.  For example, when a user enters the rewind command a minute of rewind thumbnail images (in reverse) are loaded into a buffer and\ndisplayed for the user while the user remains in rewind mode.  The display of thumbnail images follows the current location in the piece of content.  In an embodiment, the display of thumbnail images accelerate the longer the command is executed or\nremain the same speed throughout.  In another example embodiment, when a user enters the fast forward command a minute of forward thumbnail images are loaded into a buffer and displayed for the user while the user remains in fast forward mode.  The\ndisplay of thumbnail images follows the current location in the piece of content.  In an embodiment, the display of thumbnail images accelerate the longer the command is executed or upon multiple button presses or remain the same speed throughout.\nIn one exemplary embodiment the user seeks at a random location on a location bar in a piece of content, if a thumbnail 358 needed is contained in the current manifest file 354, the thumbnail image 358 can be displayed.  When the thumbnail 358\ncorresponds to a portion of video content that is outside the current manifest file 354, the user device 210 requests a new manifest file 354 with the needed thumbnails 358 (and the JITP 204 generates the new manifest file 354 and thumbnails 358 from the\nthumbnail stream 308).  In an alternative embodiment, the manifest file 354 contains addresses to all thumbnail images 358 and the device 210 downloads them when needed in response to user action.\nReferring now to FIG. 5e, another embodiment of the method 580 of operating consumer premises equipment (CPE) 210 is disclosed.\nAt step 582, the CPE 210 receives a request from a user for a piece of content.  In response, the CPE 210 generates a request for a master manifest file from the CDN 206.  In return, the CDN 206 will send the CPE 210 a copy of the master\nmanifest file.\nAt step 584, the CPE 210 receives the master manifest file from the CDN 206 and processes the master manifest file.\nAt step 586, the CPE 210 requests sub-manifests from the CDN 210 based on the master manifest file received.  These sub-manifests 354 may be generated by the JITP 352 upon registration of the asset, upon the first request by any user and then\ncached by the CDN 210, or generated in real-time.  The sub-manifest files 354 comprise chunks and image listings and the addresses of these chunks 356 and images 358.  The chunks 356 and images 358 listed may include a variety of chunks 356 and thumbnail\nimages 358 for use on a variety of devices or with different bit rates/file sizes for use with adaptive bitrate (ABR) streaming.  The CDN 210 will then send the requested sub-manifest files to the CPE 210.\nAt step 588, the CPE 210 uses the sub-manifest files 354 to request content chunks 356 and/or thumbnail images 358 from the CDN 210 based on a user request.  For example, when a user utilized the rewind function, the CPE 210 requests a number of\nthumbnail images (in reverse) for display to the user during the trick mode operation.  Similarly, with fast forward functionality, the CPE 210 requests a number of thumbnail images (going forward) for display to the user.  During random seek operation,\nthe CPE 210 may request thumbnails both before and after the time of the content.\nWhen the CDN 210 receives a request for a chunk/thumbnail the CDN 210 will serve the respective file 356 or 358 immediately if it is cached.  If the file 356 or 358 is not cached, the CDN 210 calls the JITP 352 and requests the JITP 352\ngenerate, package, and process the content chunk 356 or thumbnail 358.  The CDN 210 then caches and serves the requested content file(s) 356 or 358 to the CPE 210.\nAt step 590, the CPE 210, upon receipt of the requested chunks 356 and/or thumbnails 358 may display the received chunk 356 or thumbnail 358 or cache the thumbnail 358 or chunk 356 for later display.\nWhen the asset expires at the CDN 210, the cached thumbnails 358/chunks 356 are removed and the pre-generated manifests by the JITP 204 may be removed.  If a request is made for these files after removal, the CPE 210 requesting the files will\nreceive an error that the file is not found (e.g. a 404 error) or the respective file will be generated by the JITP 204 and served by the CDN 210, in one embodiment.\nAdditional Services--\nReferring now to FIG. 6, an exemplary configuration of a network useful with the present disclosure is illustrated.  FIG. 6 discloses an exemplary configuration of an architecture 600 for providing video content and additional services to users\n210 via a content delivery network (CDN) 206.  The CDN 206 is in communication with users 210 via the network 208.\nIn one embodiment of the present disclosure, the network 208 comprises an internet, such as e.g., the Internet.  As shown, the CDN 206 is in communication with an encoder 202, a Just in Time Packager (JITP) 204, and additional services 602.  In\none embodiment of the present disclosure, the encoder 202 encodes a source file into at least one further encoding format (e.g., transcodes a source file from one format to at least one other format) including encoding a thumbnail stream 308.\nAdditional services 602 may include image recognition, quality of service (QoS) checking, and search engine optimization (SEO) services.  Such services 602 may utilize the thumbnail stream 308 and/or the service of the JITP 204 to perform\nreal-time services for a client device 210.\nIn one embodiment, Quality Control services are performed.  These services may be performed by a separate entity 602, or in the JITP 204 (or encoder 202).  The entity 602 may check via the thumbnail stream 308 or thumbnails 358 whether there are\nany all black frames or groups of frames in the thumbnail stream 308.  In a variant, all black frames or groups of black frames within the thumbnail stream 308 raises a flag that a problem has occurred at the encoder 202, by the JITP 204, or another\nportion of the network (e.g., CDN 206).  When such an error is found the error may be (i) logged, (ii) thumbnails 358 may be regenerated, (iii) the thumbnail stream 308 may be re-encoded (via encoder 202), and/or (iv) all or parts of the source content\nfile may be re-encoded (via encoder 202).  These checks may be done in real-time (or faster than real-time) or just prior to a user's request on forward video and corrections may be made to the content/images in real-time to improve the user experience.\nIn a further embodiment Search Engine Optimization (SEO) services are performed.  These services may be performed by a separate entity 602, or in the JITP 204 (or encoder 202).  Image recognition may be performed on the thumbnail stream 308 or\nindividual thumbnail images 358 to identify faces, products, or corporate logos.  The service 602 may pull still images from the JITP 352 and perform such recognition in real-time for a user or may perform such recognition any time following encoding or\npackaging.  The faces of actors/products/or logos may be identified in the thumbnail images 358 or the thumbnail stream 308 and this data (e.g., time stamps of the locations, listings of the products/logos/actors) may be recorded within the asset itself\n(e.g., in an encoded file), or in a file containing meta-data.  For example, a user may search for all of the Brad Pitt scenes in Ocean's Eleven or all movie scenes containing Brad Pitt generally.  The search may query the metadata that was gathered via\nimage recognition of the thumbnail stream 308 or images 358 or may perform a search through the thumbnail stream 308 of an asset in real time based on the search criteria of a user.  The same image recognition may occur for corporate logos in movies, for\nexample finding all of the scenes where the Coca-Cola logo is displayed.  Advertising may be sold to these corporations whose logos/products appear (or their competitors) in advertising spots adjacent to (e.g., before or after) these logo/product\nappearances which may occur in real-time for real-time advertisement insertion.  Furthermore, advertisements or alerts may be displayed on screen to a user watching content when a product or logo appears allowing the user to interact with that element.\nIn a further embodiment, box art (e.g. display artwork) may be scrubbed from thumbnail files 358 from a thumbnail stream 308 of an asset.  These thumbnails 358 may be used in an electronic programming guide (EPG) particularly when another\npre-selected image is not made available.  In a variant, the thumbnail stream 308 (or a portion thereof) may be displayed in the EPG itself to potential viewers.  Real-time generated box art may be used in search results as well.  Default time codes may\nrequested from the JITP 352 for particular thumbnail images or particular images may be curated.  Image recognition may be performed on the thumbnail image to determine if the image is suitable for all audiences (e.g., to determine there is no adult\ncontent) or to determine if a main character (or a particular objector logo) is in the box art.  Text, e.g., the title of the asset, may be written in real-time on the image or the image may be manipulated (e.g., aspect ratio changed) as well.  In a\nfurther variant, the box art also may be curated and chosen using a thumbnail image 358 generated from the stream 308.\nAs will be readily understood by those of ordinary skill in the art given the present disclosure, virtually any use of thumbnail converted video (or a thumbnail stream) may utilize embodiments of the present disclosure.\nIt will be recognized that while certain aspects of the disclosure are described in terms of a specific sequence of steps of a method, these descriptions are only illustrative of the broader methods of the disclosure, and may be modified as\nrequired by the particular application.  Certain steps may be rendered unnecessary or optional under certain circumstances.  Additionally, certain steps or functionality may be added to the disclosed embodiments, or the order of performance of two or\nmore steps permuted.  All such variations are considered to be encompassed within the disclosure disclosed and claimed herein.\nWhile the above detailed description has shown, described, and pointed out novel features of the disclosure as applied to various embodiments, it will be understood that various omissions, substitutions, and changes in the form and details of\nthe device or process illustrated may be made by those skilled in the art without departing from the disclosure.  This description is in no way meant to be limiting, but rather should be taken as illustrative of the general principles of the disclosure. \nThe scope of the disclosure should be determined with reference to the claims.", "application_number": "14686584", "abstract": " Apparatus and methods for thumbnail generation. In one embodiment, a\n     thumbnail stream is generated where one or more bits are assigned to key\n     frames and minimizes the number of bits assigned to delta frames. Each\n     key frame may be used to extract a still image for use in thumbnails\n     during trick mode operation (e.g., fast forward, rewind, pause, or random\n     seek operation). When the encoded video file is given to a Just in Time\n     Packager (JITP), the JITP extracts the image files (via the key frames).\n     Information regarding how to reference these files is stored in generated\n     manifest file and may be reformatted by the JITP to fit the needs of the\n     current product. In a variant, the JITP reformats the image files\n     extracted from the encoded video for operation on a number of devices.\n", "citations": ["4195351", "6636238", "7602820", "7954131", "8484511", "8520850", "8621540", "8660181", "8713623", "8799943", "8813124", "8826347", "8863164", "8917977", "8924580", "8930979", "8997136", "9066115", "9066138", "9083513", "9215423", "9247317", "9710469", "20020135621", "20020164149", "20030056217", "20050123381", "20060025869", "20060130113", "20060161563", "20070058926", "20070136742", "20070237225", "20080015999", "20080181575", "20080235746", "20080276173", "20080313691", "20090150557", "20090165067", "20090190652", "20100175088", "20100251304", "20100251305", "20110016482", "20110050860", "20110067049", "20110080940", "20110103374", "20110110515", "20110283311", "20120011225", "20120030314", "20120030723", "20120047542", "20120185905", "20130019273", "20130046849", "20130060958", "20130067052", "20130104162", "20130111517", "20130282876", "20130318629", "20140040026", "20140108585", "20140164760", "20140189743", "20140237520", "20140245346", "20140355624", "20150006752", "20150089557", "20150095460", "20150095511", "20150188974", "20150261600", "20150271234", "20150271541", "20160103830", "20160164841", "20160165173", "20170127298", "20170171766"], "related": []}, {"id": "20160358209", "patent_code": "10373195", "patent_name": "Methods and systems to evaluate and determine degree of pretense in online\n     advertisement", "year": "2019", "inventor_and_country_data": " Inventors: \nShiravi Khozani; Mohammad Ali (Fredericton, CA), Mokhtari; Ehsan (Fredericton, CA), Shiravi Khozani; Hadi (Frederiction, CA), Frankoff; Sergei (Fredericton, CA)  ", "description": "<BR><BR>FIELD\nThis relates generally to the field of online advertising and malicious software.\n<BR><BR>BACKGROUND\nThere exists malicious software that compromises computers to imitate legitimate users for the purpose of committing online advertising fraud or other forms of deceit.\nAccordingly, entities engaged in online advertising may desire systems and methods that help facilitate determining whether an interaction with an online advertisement is through a legitimate user or malicious software.\n<BR><BR>SUMMARY\nIn accordance with one aspect, there is provided a method of determining a degree of deceptiveness associated with a session presenting at least one online advertisement, the method comprising: capturing electronic interactions among at least\ntwo entities involved in the session, the captured interactions comprising interaction-related properties and time; defining one or more relations between the at least two entities involved in the session, the relations comprising an associated set of\nattributes; extracting, aggregating and filtering a plurality of attributes for the one or more relations from the captured electronic interactions, each attribute defining a data type and corresponding data value, wherein the extraction is performed by\nintegration with the two or more entities; applying one or more models to each of the one or more relations; determining a degree of deceptiveness for each of the one or more relations based on the one or more applied models; and aggregating the degree\nof deceptiveness for each of the one or more relations; generating the degree of deceptiveness for the session based on the aggregated degree of deceptiveness for each of the one or more relations.\nIn accordance with another aspect, there is provided a system for determining a degree of deceptiveness for a session presenting at least one online advertisement, the system comprising: a network infrastructure; a tracking server comprising a\nprocessor configured to: request tracking to capture electronic interactions among at least two entities involved in the session, wherein the tracking server is coupled to the at least two entities by the network infrastructure, wherein the captured\ninteractions comprise interaction-related properties and time, define one or more relations between the at least two entities, wherein the relations comprise an associated set of attributes; extract a plurality of attributes for the one or more relations\nfrom the captured electronic interactions, each attribute defining a data type and corresponding data value, wherein the extraction is performed by integration with the two or more entities; receive the captured electronic interactions and the extracted\nplurality of attributes; determine the degree of deceptiveness for each of the one or more relations; and generate the degree of deceptiveness for the digital advertisement session based on the determined degree of deceptiveness for each of the one or\nmore relations.\nIn accordance with another aspect, there is provided a method for delivering content, the method comprising: receiving, at a tracking server, a request for a tracking pixel from a tag comprising code instructions to retrieve at least one online\nadvertisement from an associated advertising server; receiving, at the tracking server, multiple data fields upon requesting the tracking pixel, the data fields comprising a campaign identifier, a client identifier, and a pixel type; logging, at the\ntracking server, the request for the tracking pixel with a timestamp, the campaign identifier, the client identifier, and the pixel type; executing the code embedded with the tracking pixel to capture attributes relating to at least one of placement of\nthe at least one online advertisement, one or more entities with relations to the at least one online advertisement, and interaction between the one or more entities; receiving, filtering and aggregating the captured attributes; and generating a degree\nof deceptiveness and a report based on the aggregated captured attributes.\nIn accordance with an aspect, there is provided a method for a content delivery system.  The method may involve receiving, at a tracking server, a request for a tracking pixel from an ad tracking tag embedded as code instructions within at least\none of electronic content having an ad tag with a request to retrieve at least one online advertisement from an associated advertising server, the ad tag or the online advertisement; receiving, at the tracking server, multiple data fields from the ad\ntracking tag upon requesting the tracking pixel, the data fields comprise a campaign identifier, a client identifier, and a pixel type; logging, at the tracking server, the request for the tracking pixel with a timestamp, the campaign identifier, the\nclient identifier, and the pixel type; triggering execution of code embedded with the tracking pixel to capture dynamic attributes regarding placement of the at least one online advertisement, one or more entities with relations to the at least one\nonline advertisement, or interaction between the one or more entities; and receiving, filtering and aggregating the dynamic attributes to generate a degree of deceptiveness and a report for the at least one online advertisement, the one or more entities\nwith relations to the at least one online advertisement, or the interaction between the one or more entities.\nIn accordance with a further aspect, there is provided a system for determining a degree of deceptiveness for a session for providing at least one online advertisement.  The system having a tracking server with a processor configured to: request\ntracking to capture electronic interactions among at least two entities in a digital advertisement session for serving at least one online advertisement, the tracking server indirectly or directly coupled to the at least two entities by network\ninfrastructure, the captured interactions having attributes comprising interaction-related properties, time, and other attributes of the at least two entities involved, the at least two entities having relations being indirect or direct, the relations\nhaving an associated set of attributes; extract, at the time of serving the advertisement, a plurality of attributes for the one or more relations from the captured electronic interactions, each attribute defining a data type and corresponding data\nvalue, the extraction by integration with the two or more entities; receive the captured electronic interactions and the extracted attributes; and determine the degree of deceptiveness for each of the one or more relations to generate the degree of\ndeceptiveness for the digital advertisement session.\nIn accordance with one aspect, there is provided a method for determining a degree of deceptiveness for a session, the session having one or more relationships between one or more properties of interactions with an online advertisement, the\nmethod comprising: extracting one or more characteristics associated with each of the one or more relationships; applying one or more models to each of the one or more relationships to determine a degree of deceptiveness for each of the one or more\nrelationships; aggregating the degree of deceptiveness for each of the one or more relationships to generate the degree of deceptiveness for the session.\nIn another aspect, the one or more properties of interactions with an online advertisement include at least one of user interactions, web-page rendering of the online advertisement, publisher interactions and advertising network interactions.\nIn another aspect, each of the one or more relationships is defined based on different combinations of the relationship between two or more properties of interactions with the online advertisement.\nIn another aspect, extracting one or more characteristics associated with each of the one or more relationships uses at least one of a specially configured web browser and a web browser plug in.\nIn another aspect, extracting one or more characteristics associated with each of the one or more relationships includes analyzing the integration of the online advertisement with at least one of a publisher, a web page and an advertising\ntechnology network.\nIn another aspect, the model applied to each of the one or more relationships to determine the degree of deceptiveness for each of the one or more relationships flags loading the online advertisement on a web page where the online advertisement\nis on a portion of the web page that is not viewable by a user.\nIn another aspect, a tracking pixel is used to extract the one or more characteristics associated with each of the one or more relationships.\nIn another aspect, an ad-tracking tag is used to extract the one or more characteristics associated with each of the one or more relationships.\nIn another aspect, a cookie is used to extract the one or more characteristics associated with each of the one or more relationships.\nIn another aspect, the one or more models are parametric models.\nIn another aspect, the one or more models are non-parametric models.\nIn another aspect, the one or more models are parametric or non-parametric models.\nIn another aspect, the one or more models is an external model having a list of predetermined good or bad internet addresses, domains, publishers or advertisers.\nIn another aspect, the one or more characteristics associated with each of the one or more relationships includes historical characteristics.\nIn another aspect, aggregating the degree of deceptiveness for each of the one or more relationships to generate the degree of deceptiveness for the session includes taking a weighted average of the degree of deceptiveness for each of the one or\nmore relationships.\nMany further features and combinations thereof concerning embodiments described herein will appear to those skilled in the art following a reading of the instant disclosure. <BR><BR>DESCRIPTION OF THE FIGURES\nIn the figures, which illustrate example embodiments,\nFIG. 1 illustrates an example system that captures and reports static and dynamic metrics regarding ad placement.\nFIG. 2 is a block schematic diagram of an example system.\nFIG. 3 illustrates the operation of content delivery system during an exemplary interaction between a user device and a server.\nFIGS. 4 and 5 are example workflows related to the extraction and processing of session data.\nFIG. 6 is a schematic diagram of an example computing device.\n<BR><BR>DETAILED DESCRIPTION\nOnline advertising may be susceptible to various forms of deception, misrepresentation, and concealment, which may lead to the defrauding of advertisers, publishers, ad-tech networks, and users.  Various forms of advertisement deception,\nmisrepresentation, and concealment can be detected, reduced and/or prevented using some embodiments of the methods and systems described herein.\nEmbodiments described herein are applicable to online advertising, and may also be applicable to website page views and mobile app views.  Rather than only being used for monitoring a digital advertisement, some embodiments allow for the\nmonitoring of website page views or mobile app views for fraud using similar or the same technology.  As such, it will be appreciated that references to \"online advertising\" in this specification are made in the context of discussing example embodiments,\nand are not intended to limit the applicability of the systems and methods described herein to online advertisements.  The monitoring of website page views and app views for fraud is also contemplated by the embodiments described herein.\nAdvertising fraud schemes may involve various individuals and/or associated electronic components being used to misrepresent and deceive the advertiser into believing that some form of event has occurred in relation to their advertisement and\nthe end-user, for example, loading an advertisement on a page where it is hidden from view.  In some scenarios, there may be other parties that may be connected to the fraud scheme, such as innocent third parties whose devices may be surreptitiously\ncontrolled, third parties providing networking infrastructure and/or services (e.g., internet service providers, domain name systems), or the like.\nA form of advertisement deception attempts to create the impression that an event has occurred in relation to an advertisement, for example, masquerading the interactions of a human user through automated means (e.g., clicking on ads).  This\nfraudulent activity potentially incurs costs for advertisers and/or other parties.  In some embodiments, various methods may be described wherein through the gathering and analysis of the mutual relationships between various entities (e.g., the user,\nadvertisement, web-page, browser, publisher, ad-tech network, and advertiser), a metric or value representing a degree of pretense (e.g., a measure of deceptiveness) can be computed.\nThe degree of pretense may be a measurement related to online advertisements, in particular, a measurement associated with various forms of deception, misrepresentation, and/or concealment.\nIn some embodiments, various systems, methods, computer program products and/or non-transitory computer readable memories having programs stored thereupon, are provided for determining or computing the degree of pretense (e.g., a degree of\ndeceptiveness) for a session.  The session data may be defined as having one or more relationships between one or more properties of interactions with an online advertisement.\nThe degree of pretense may be related, for example, to fraudulent uses of an online advertising ecosystem, which may be targeted by sophisticated advertisement fraud schemes.  Impacted parties may include at least one of advertisers, purchasers\nof advertising, consumers of advertisements, publishers, advertising networks, and various intermediaries and parties associated with online advertisements.\nIn the context of online advertisements, various types of malicious activities may be undertaken by various individuals, for example, controlling the operation of various machines and/or systems, such that online advertising systems may falsely\nbelieve that some sort of event (e.g., human interaction such as click-through, hover-over, auto-play, selection, information input) has occurred in relation to their advertisement.\nThese activities may closely mimic human interactions by the target audience of an advertisement, and may lead to advertisers believing that an advertisement was able to reach its intended audience with one or more advertising messages.  In some\nscenarios, the advertiser has paid for, or will pay for various outcomes or activities associated with the advertisements, such as the loading of an advertisement, the interaction with an advertisement (e.g., click through, hover over, auto-play,\nselection, information input), and subsequent action associated with the advertisement (e.g. transaction to purchase goods or services).\nThese fraudulent activities may have potential deleterious impacts on one or more of the advertisers and associated advertising platforms, as the metrics and measurements may not be accurately tracking legitimate advertising activities, and\nfurther, payments and commissions may be made to parties which may not otherwise merit such payments and commissions.\nFurther, these fraudulent activities may also be used to improperly increase the page ranking of a particular webpage (e.g., search engine optimization), associate various keywords with various web pages, disrupt web indexing, improperly\ninfluence search results, mislead audiences, and mislead website crawlers.\nIn some embodiments, the advertiser may directly provide or track advertisements and related data.  In some embodiments, the advertiser may utilize one or more advertising networks (e.g., a link exchange, an online advertising service, an\nadvertising hub) and other intervening platforms (e.g., a social media platform, a communications platform, a content management system, a hosting services provider) to deliver various forms of advertising messages to target audiences.  The target\naudiences may, for example, receive these advertisements through one or more displays and devices that the target audience may be using, such as a workstation, a laptop, a tablet computer, a mobile device (e.g., a smart phone), or the like.\nThe serving of advertisements and/or various interactions with the advertisements may be tracked through various electronic mechanisms, such as a tracking pixel, information stored in cookies, surveys, web request logging (e.g., tracking IP\naddresses that request a particular picture embedded into the ad and hosted on an ad exchange's tracking server, and or other web analytics measurements, PHP request logging, HTTP GET and POST logging, network traffic logging), or the like.\nIn some embodiments, a system may be provided to determine a degree of pretense based on various information sensed, extracted, processes or otherwise received in relation to various aspects of online advertising.  The system may be configured\nto track or extract various characteristics associated with the online advertisement.  As there may be various indicators associated with the characteristics that indicate fraudulent, automated or other deceptive or undesirable activities, various models\ncan be generated and applied to the characteristic information such that a determination (e.g., a score, a metric, a probability, or other value) may be generated to define a degree of pretense in relation to the potential for at least one of a session,\nan activity and a source to be deceptive in nature.\nAccordingly, a system is provided that extracts one or more characteristics associated with each of the one or more relationships and applies one or more models to each of the one or more relationships to determine a degree of deceptiveness for\neach of the one or more relationships.  The system aggregates the degree of deceptiveness for each of the one or more relationships to generate the degree of deceptiveness or pretense for the session.\nThe properties of interactions with an online advertisement may include user interactions, web-page rendering of the online advertisement, publisher interactions and advertising network interactions.  The interactions may be captured as\nelectronic data signals defining various metadata, attributes or other properties of the interaction with an online advertisement.\nRelationships between various activities may be defined based on different combinations of the relationship between properties of interactions with the online advertisement, and the characteristics of interactions can be extracted, for example,\nusing a specially configured web browser and a web browser plug in.\nFIG. 1 illustrates an example system that captures and reports static and dynamic metrics regarding advertisement (or \"ad\") placement.  The system 100 includes one or more user devices (e.g., desktop computer 102a, portable computer 102b, tablet\n102c, and smartphone 102d) coupled to one or multiple servers 104a, 104b, 104c over a public or private data network 106 (e.g., the Internet).  A server produces content (e.g., web pages, smartphone apps, or the like) that may be accessed by user via an\napplication (e.g. web browser, smartphone app, or the like) on one of the user devices.  The system 100 may capture several attributes for a variety of relationships.\nFIG. 2 is a schematic diagram of an example system 200.  The system 200 may determine a degree of pretense based on various information extracted, sensed or provided in relation to various aspects of online advertising.\nThe system 200 may be configured to interoperate with various entities.  Some example entities are provided below along with example extractable properties:\nTABLE-US-00001 Entity Entity properties (including, but not limited to) Advertisement Contents, library classification, and target demographic.  User IP, location, platform Browser Vendor, version, plug-ins, extensions, platform, identification,\noperating system, language, dimensions Publisher Websites, classification Ad-tech Ad-tech components (Demand Side Platforms (DSP), (advertising Data Management Platforms (DMP)) technology) Network Web-page Width, height, number of colors, FPS, active\nplugins, markup version, contents, elements on page, audio-visual elements\nThe system 200 may interact with one or more of various entities, individuals and their associated computing systems, including users 202a .  . . 202n, publishers 204a .  . . 204n and advertisers 206a .  . . 206n.\nUsers 202a .  . . 202n include various individuals who may be using various programs and/or accessing various websites, and these individuals may be served advertisements that, for example, may be tailored based on different properties,\nincluding their particular usage type, device type, known characteristics, etc. However, it may not be clear to the system whether users 202a .  . . 202n are indeed human individuals, or various programs attempting to deceive publishers 204a .  . . 204n\nand advertisers 206a .  . . 206n.  The deception may occur with various desired outcomes in mind, including to modify a page ranking or to receive an advertising commission related to the paid advertising.\nPublishers 204a .  . . 204n may include individuals, organizations or services that host, provide or otherwise publish content.  The content may, in some cases, be associated with various advertisements that may be served near, next to, or\notherwise in relation to the published content.  Publishers 204a .  . . 204n may include web services that provide for web pages having content generated by individuals, such as blogging platforms, web publishing platforms, or the like.  Publishers 204a\n.  . . 204n may also include various companies that publish content on their websites.\nAdvertisers 206a .  . . 206n may include individuals or organizations that perform one or more of creating, delivering, distributing, aggregating and allocating marketing communications.  These marketing communications may be targeted to users\n202a .  . . 202n and may be tailored based on characteristics of the user 202a .  . . 202n, the user 202a .  . . 202n's activities, or both.  In some embodiments, the advertisers 206a .  . . 206n serve their ads through at least one of publishers 204a . \n. . 204n and advertising network systems 208.\nMarketing communications may include various types of advertisements, such as one or more of video, audio, textual, image and photographic advertisements, or the like.  The marketing communications and activities related to marketing\ncommunications may be tracked or otherwise related to various commissions or benefits paid out by the advertisers 206a .  . . 206n.  For example, the advertisers 206a .  . . 206n may provide compensation that is related to interactions that a user has\nwith the marketing communications, such as click-throughs, hovers, the amount of time it is displayed on a user's screen, or the like.  Various tracking mechanisms may also be used, such as referral links, tracking pixels, cookies, Javascript, or the\nlike.\nAdvertising network systems 208 may be various entities, such as link exchanges, advertising cooperatives, referral engines, or the like.\nThe example system 200 comprises of various configured components, including, an interaction capture engine 210, a tracking element dispatcher 212, a relation identification engine 214, a relation information extractor 216, a relation DOP\nevaluator 218, a model application subsystem 220, and a session DOP evaluator 222.  The system 200 further comprises a non-transitory computer-readable storage element 250, which may also interoperate with external systems 290.\nThe components of system 200 may be implemented as one or more of cloud computing and distributed networking components.  For example, the components of system 200 can be implemented as one or more virtual entities for which one or more actual\ncomponent instances are created and distributed across one or more computer systems or managed as a single entity.  A component may be registered with a controller component, which may be configured to consider various metrics in order to determine\nplacement, quantity and the amount of resources usable by the instance.\nThese metrics include, but are not limited to, the load on instance, the total load on computer system, and the amount of computing resources available.  The behavior of the controller component may also be fully or partially manipulated through\nconfiguration received from a component or a configuration file.  A messaging component or a communication fabric (CF) may be provided where other interfacing components can pass and retrieve messages between each other.\nIn respect of interactions with one or more advertisements, input data from various sources may be provided in a streaming manner from network 270.  A challenge may be to utilize computing resources to perform the necessary functionality in a\npractical amount of time.\nThe interaction capture engine 210 is configured to perform at least one of capturing, recording and tracking various electronic data signals representing interactions that a user (via user device) may have with an electronic advertisement that\nis being presented on a webpage.  The interaction information may be provided from network 270; the interaction capture engine 210 may track HTTP/HTTPs requests, web traffic, field entries, element interactions, keystrokes, mouse clicks, mouse movements,\ngestures, or the like.\nThe tracking element dispatcher 212 is configured to provision various elements that may be used to track interactions with advertisements, including, but not limited to, tracking pixels, referral links, HTTP GET/POST requests, cookies, embedded\nJavascript, Adobe Flash.TM.  elements, or the like.\nThe relation identification engine 214 may be configured to identify defined relationships between various entities involved, such as between users, advertisements, browsers, publishers, advertising technology networks, web pages, etc.\nFor each relation, two entities may be involved (e.g., entity A and entity B).  As described further in the specification, one or more models may be applied to each relation in order to evaluate that particular relation.\nThe relation information extractor 216 is configured to perform at least one of determining various elements of information and associating these elements of information with a particular relationship between two entities (e.g., a relation). \nFor example, a user's interactions with a rendered web-page may be associated with a defined relationship between the user and the webpage, and the information that may be extracted may include mouse clicks, mouse movements, hover over time, or the like.\nAs another example, when an advertisement is served or otherwise provided or loaded, direct and indirect relationships may be formed between the various entities involved, as well as between various devices and components.  Each relation has an\nassociated set of attributes which may be extracted through the capturing of the interaction among the relation's entities.  For every captured interaction, one or more attributes may be defined for that relation containing data defining at least one of\nthe interaction-related properties, time, and various attributes for each entity involved.\nThe attributes for a relation can be extracted through scripts loaded at the time of at least one of serving the advertisement, integration with the publisher, integration with the advertising technology network, and through the specific\nimplementations of the browser or special plug-ins for available browsers.  Other methods and techniques for extraction are also contemplated.\nAn example embodiment of such relations and their respective associated attributes may be described as follows:\n(User, Advertisement)\nA user may interact with the displayed advertisement using different available input devices (e.g., mouse, touch sensitive display, control device, microphone, buttons, camera, sensors, or the like).  The interaction may include, but is not\nlimited to, mouse clicks, mouse hover, mouse drag-drop, eye movement, touching, tapping, body gestures, focusing, blurring, appearing, disappearing, or the like.\n(User, Browser)\nA user may interact with the web-page rendering container or browser (e.g., at the operating system level) using different available input devices.  The interaction may include, but is not limited to, container activation, focusing, blurring,\nresizing, movement, dragging and dropping, and closing.  The interaction can also include viewing/retrieving browser and user properties.\n(User, Web-Page)\nA user may interact with the rendered web-page using different available input devices.  The interaction includes, but is not limited to, mouse clicks, mouse movements, mouse drag-drop, eye movement, touching, tapping, body gestures, focusing,\nblurring, appearance, disappearance, key-presses, scrolling of internal elements on the web-page, information input, HTTP requests, PHP interactions, Javascript.TM., various plugins and related interactions (e.g., Adobe Flash.TM., dynamic HTML, HTML),\nmobile interactions (e.g., detected gestures, swipes, shakes, multi-touch, pinches, zooms, tilts), or the like.\n(User, Publisher)\nA user may interact with publisher (via components or devices controlled or used by publisher) using different available input devices.  For example, the relationship may be at least one of determined, extracted, computed, sensed and provided as\na result of the user loading a web-page served by a publisher through a browser.  The relationship may include attributes from the user and the publisher.\n(User, Ad-Tech Network)\nA user may interact with ad-tech network components or devices using different available input devices.  The relationship may be at least one of determined, computed, extracted, sensed and provided as a result of the user being presented with an\nadvertisement through an advertising technology component.  The relationship includes attributes from the user and the advertising technology network component.  For example, an ad-tech component may know more information about a user--such as other\nbrowsing information (e.g., DoubleClick tracking cookies) which may impact how the interaction is recorded as a relationship and the type of data used to define the interaction and properties.\n(User, Advertiser)\nA user may interact with an advertiser (via components or devices controlled or used by the advertiser) using different available input devices.  The relationship may be at least one of determined, sensed and provided as a result of the user\nbeing presented with an advertisement endorsed by an advertiser.  The relationship includes attributes from the user and the advertiser.\n(Advertisement, Web-Page)\nA relationship between an advertisement and a web-page may be at least one of determined, sensed and provided as a result of an advertisement placed on a web-page.  The relationship can include attributes from the advertisement and the web-page. It can also include attributes such as the position of advertisement on the web-page, the size, metadata describing context for the web-page and advertisement, the structure of the section of the page containing the advertisement, the particular frame an\nadvertisement is loaded in, or the like.\n(Advertisement, Browser)\nA relationship between an advertisement and a browser may be at least one of determined, sensed and provided as a result of an advertisement served by a browser.  The relationship includes attributes from the advertisement and the browser.  It\ncan also include attributes such as the number of requests made by the browser to retrieve the advertisement, the type of browser used, the particular configuration of a browser, relevant network protocols used, encryption techniques, content retrieval\ntechniques, platform used (e.g., HTML, Adobe Flash.TM., Javascript.TM., Microsoft Silverlight.TM.), or the like.\n(Advertisement, Publisher)\nA publisher (via components or devices controlled or used by the publisher) may interact with an advertisement using different available input devices.  The relationship may be at least one of determined, sensed and provided as a result of an\nadvertisement being presented on a publisher's web-page.  The relationship includes attributes from the advertisement and the publisher.\n(Advertisement, Ad-Tech Network)\nComponents or devices of the ad-tech network may interact with an advertisement to compile, serve, monitor, locate, transmit the advertisement, or the like.  The relationship may be at least one of extracted, determined, sensed and provided as a\nresult of an advertisement being served by a component of an ad-tech network.  The relationship includes attributes from the advertisement and the ad-tech network.\n(Advertisement, Advertiser)\nAn advertiser (via components or devices controlled or used by the advertiser) may interact with an advertisement using different available input devices.  The relationship may be at least one of determined, sensed and provided as a result of an\nadvertisement sponsored by an advertiser.  The relationship includes attributes from the advertisement and the advertiser.\n(Web-Page, Browser)\nA web-page may interact with a browser (and vice versa) regarding an advertisement.  The relationship may be at least one of determined, sensed and provided as a result of a web-page being rendered by a browser.  The relationship includes\nattributes from the web-page and the browser.\n(Web-Page, Publisher)\nA publisher (via components or devices controlled or used by the publisher) may interact with a webpage using different available input devices.  The relationship may be at least one of determined, sensed and provided as a result of a serving a\nweb-page from a publisher.  The relationship includes attributes from the web-page and the publisher.  It also includes attributes such as asynchronous/synchronous calls to the publisher.\n(Web-Page, Ad-Tech Network)\nComponents of the ad-tech network may interact with a web-page regarding the advertisement (and vice versa) The relationship may be at least one of determined, sensed and provided as a result of an advertisement served on a web-page by an\nad-tech component.  The relationship includes attributes from the web-page and the ad-tech network.\n(Web-Page, Advertiser)\nAn advertiser (via components or devices controlled or used by the advertiser) may interact with a webpage using different available input devices.  The relationship may be at least one of determined, sensed and provided as a result of an\nadvertisement endorsed by an advertiser served on a web-page.  The relationship includes attributes from the web-page and the advertiser.\n(Browser, Publisher)\nA publisher (via components or devices controlled or used by the publisher) may interact with a browser using different available input devices.  The relationship may be at least one of determined, sensed and provided as a result of rendering a\npublisher's web-page within a browser.  The relationship includes attributes from the browser and the publisher.\n(Browser, Ad-Tech Network)\nComponents of the ad-tech network may interact with the browser (and vice versa) regarding the advertisement.  The relationship may be at least one of determined, sensed and provided as a result of rendering an advertisement served an ad-tech\ncomponent.  The relationship includes attributes from the browser and the ad-tech network.\n(Browser, Advertiser)\nComponents of the advertiser may interact with the browser (and vice versa) regarding the advertisement.  The relationship may be at least one of determined, sensed and provided as a result of rendering an advertisement endorsed by an\nadvertiser.  The relationship includes attributes from the browser and the advertiser.\n(Publisher, Ad-Tech Network)\nComponents of the ad-tech network may interact with the publisher (and vice versa) regarding the advertisement.  The relationship may be at least one of determined, sensed and provided as a result of serving an advertisement by an ad-tech\ncomponent.  The relationship includes attributes from the publisher and the ad-tech network.\n(Publisher, Advertiser)\nComponents of the advertiser may interact with the publisher (and vice versa) regarding the advertisement.  The relationship may be at least one of determined, sensed and provided as a result of publishing an advertisement endorsed by an\nadvertiser on a publisher's web-page.  The relationship includes attributes from the publisher and the advertiser.\n(Ad-Tech Network, Advertiser)\nComponents of the ad-tech network may interact with the advertiser (and vice versa) regarding the advertisement.  The relationship may be at least one of determined, sensed and provided as a result of an advertisement endorsed by an advertiser\nserved by a component of an ad-tech network.  The relationship includes attributes from the advertiser and the ad-tech network.\nRelation Info Extraction\nThe attributes for a relation can be extracted through various means, depending on the entities involved in a relation.  Relations may also be referred to herein as relationships.\nAttributes may also be referred to herein as properties, characteristics, and metadata.  The following processes could potentially extract this information:\nTailored Browser: A specifically tailored browser can extract the various relationships or relations, especially relationships relating to the browser.  For example, the browser may include the use of various logging components (potentially one\nor more of server side and client side), tracking components, packet sniffing components, or the like.\nBrowser plugin: Many browsers provide means to extend the behavior of the browser through some implementation of exposed API.  A browser plugin can utilize such APIs in order to extract relationship information, and the browser plugin may be\nconfigured specifically for one or more of monitoring and retrieving information associated with the use of the browser, and viewing of advertisements.\nIntegration with the publisher: Logs generated through users accessing a publisher's website can be parsed in order to extract relationship information.  For example, access logs, IP address logs, analytics logs, and the like may be parsed.\nIntegration with the web page: A web page can potentially load a number of scripts that could in turn extract the required relationship information.  Various logging activities may occur at the backend, including the use of scripts to track\nvarious activities (e.g., keystrokes, entered data, mouse clicks/mouse hovers, gestures) or API function calls.\nIntegration with the Ad-tech network: Many components work together to enable an advertisement to be presented to the user.  By integrating with components in this area, relationship information can be extracted.\nIntegration with the Advertisement: When an advertisement is presented, depending on the specific technology used, scripts can be injected into the page in order to extract relationship information.  There may be other logs utilized, and the\nadvertisement itself may be configured for one or more of tracking and monitoring, including tracking of a pixel or image, cookie, an ad-tracking tag, a hashtag, and other ad-tracking technology.\nThe various relation/relation information extraction processes described above may be used to obtain information regarding a session.  A session refers to a user accessing a web-page.  The session begins when a request is made to the required\nsources for data.  When the required data is available, scripts may be evaluated and, if required, more sources may be requested.  This can cycle indefinitely and in some embodiments, a threshold is applied to cut the session off.  In some embodiments, a\nnew session may not be created unless the web page resources are re-requested.\nOnce obtainable information is extracted from a session, the information for the session is combined together as one or more data structures.  These one or more data structures define the context for that particular session.\nA degree of pretense (DOP) for a session is computed as one or more of a metric, rank, score and other type of measurement or indication that is related to how a session appears to be deceptive in nature.  For example, the degree of pretense may\nbe a quantification, through various information received and processed, of how likely actions or interactions taken during the session were to be fraudulent in nature.\nAn embodiment of such deception involves the falsifying or misrepresenting the relationship or any related elements defined within that relationship.  For example, loading an advertisement on a page where it is hidden from view is considered a\nmisrepresentation if such information (hidden from view) is not properly disclosed to the required entity which, depending on circumstances, could be the publisher, advertiser, or the ad-tech network.\nIn order to calculate the degree of pretense for a session, the degree of pretense for all available relationships within a session may be calculated individually by the relation DOP evaluator 218.  The relation DOP evaluator 218 may be\nimplemented using a processor configured with an evaluator module which receives, for example, a set of tuples as an input, including a tuple that contains a relationship reference or indicator, a tuple containing the relationship, and its context along\nwith all defined attributes.\nIn some embodiments, the relation DOP evaluator 218 is implemented as a partial evaluator using code which takes a set of parameters as an input and outputs a value of numeric type along with the possible range of the output value.  As an\nexample, a partial evaluator may be configured to calculate the reparation value of a publisher on which the ad has been loaded.  The output result may be a numerical number in the range of [0, 1].\nIn order to calculate the degree of pretense for a relation, a number of previously generated (and stored) models may be retrieved for that relation.  These models may be stored in data storage 250.  Each model may be applied to the session and\nits related context or attributes.  Each model may generate and output a partial degree of pretense (PDOP).\nFor each relation ((A,B), (A,B,C), and so on) defined, one or more parametric or non-parametric models may be at least one of generated, developed and retrained by the model application subsystem 220.\nEach model is specifically generated to evaluate that particular relation.  By way of example, consider a relation between two entities, A and B. Several models may be generated for the resulting relation (A,B).  During the evaluation phase,\neach of the resulting models may take information pertaining to the relation between A and B and produce a PDOP which may be collected and collectively utilized to produce a relation DOP.  For example, given a User-Browser relation, during model\ngeneration, three models may be generated, one for each of the past three months.  During the evaluation phase, each generated model may receive data on the User and the Browser relation for a particular session and it may produce a PDOP.  These values\nmay then be combined using a data fusion mechanism to produce a DOP for the User-Browser relation.\nFor the example relation that consists of entities A and B, several models may be applied for the relation (A,B).  Again, each of the models may utilize information pertaining to the relation between A and B and compute a partial degree of\npretense (PDOP) which may be collected and collectively utilized to produce a relation degree of pretense (RDOP).  For example, data fusion may be used to combine all PDOPs into a DOP for that relation.  Once all DOPs for all relations are calculated, a\nfinal model may take all of the relation DOPs and produce a final DOP.  The final DOP may be stored in data storage 250 along with session relation information and context.\nFor example, there may be two general types of models provided: Models based on external intelligence obtained through external sources.  The training data for these models may include historical session data.  For example, the external sources\nmay provide relationship information, a list of predetermined good or bad internet or electronic addresses, domains, publishers or advertisers, or the like.  For example, model information may be provided by external systems 290.  Some data may be\nobtained through means other than the extraction mechanisms used to obtain relation information; and Models based on historical session data.  Several time-windows are defined.  For each time-window, session data within that time-window may be queried to\ncreate a parametric or non-parametric model.  Cross-validation techniques may be utilized such that error rates for each model do not fall below predefined thresholds.  Once the final DOP for a session is determined, all composing elements of the session\n(Context or attributes, PDOPs, DOP, or the like) are stored together under that session ID.  These are referred to as Historical Session Data.  This historical session data can then be analyzed to generate new relation models or re-train previously\ngenerated models.  For example, during the model generation phase for the User-Browser relation, a predominant pattern may be discovered which correlates a particular subset of user IP addresses with a certain type of browser and a certain range of DOP. \nA new model may then be generated and added to the list of previously generated models for later evaluation.  This model may also be retrained at a later date to reflect future changes to user IP addresses and browser usage.\nThe models may be stored in data storage 250 or otherwise associated with a relationship and invoked during session scoring (to determine a degree of pretense).\nSeveral factors may determine when models are generated or retrained.  This includes, but is not limited to, one or more of new data received and otherwise provided from external intelligence sources, a significant deviation of degree of\npretenses, time, or the quantity of sessions.\nEach model may be applied to the session by the model application subsystem 220.  Each model may then output a partial degree of pretense (PDOP).  The partial degrees of pretense may be one or more of aggregated and otherwise combined to develop\na degree of pretense that may be specific for a relationship (a relationship DOP).\nFor example, various techniques may be used, such as data fusion, weighted averages, or the like.  Data fusion may refer to the process of integrating multiple data sources representing the same real-world object or thing into a consistent,\naccurate, and useful representation.  The integration may produce new data sets.  In some embodiments, heuristic techniques may be used to vary the weights associated with various partial degrees of pretense over time.\nIn some example embodiments, each relation may involve two entities.  Other examples may involve more than two entities.\nAs an example, for a user-browser relation, three models may be generated (e.g., one for each of the past three months).  Each generated model may receive data relating to the user and the browser relation for a particular session and the model\nmay produce a PDOP.  These values are then combined/aggregated/processed together (e.g., using a data fusion mechanism) to produce a DOP for the user-browser relation.\nOnce all DOPs for all relationships are calculated, the session DOP evaluator 222 may be invoked to utilize a model, applied by the model application subsystem 220, to process all of the relationship DOPs and produce a session DOP for the\nsession.  For example, a weighted average may be taken of the relationship DOPs.  The session DOP evaluator 222 may be configured to receive a set of tuples as input.  Each tuple may contain a relationship reference or indicator, a DOP value, and a\nrange.  This may be given to a previously generated statistical model in order to derive a final DOP.\nThis session DOP may then be stored in a data storage element, and in some cases, the DOP may be stored along with the session relationship information and context.  The relationship DOPs and the PDOPs may also be stored.  In some embodiments,\nthe historical information and stored calculations are used to calculate future DOPs and PDOPs.\nFIG. 3 illustrates the operation of content delivery system during an exemplary interaction between a user device and a server, according to some embodiments.\nThe example system captures several attributes for a variety of relations.  The system includes one or more user devices 302 (for example, desktop computer 102a, portable computer 102b, tablet 102c, smartphone 102d) coupled to one or multiple\nservers 304a, 304b, 304c over a public or private data network (e.g., the Internet).  A server 304a produces content (e.g., web pages, smartphone apps, etc.) that is accessed by user via an application (e.g. web browser, smartphone app, etc.) on one of\nthe user devices 302.\nContent may include one or more ad tags that instructs the application to retrieve a corresponding advertisement (an ad unit) from an associated ad server 304b over the network.  The served content, ad tag or retrieved advertisement, for\nexample, may include embedded code, such as a third party ad tracking tag, that requests an extraction script (e.g., JavaScript code, Adobe Flash.TM.  object, etc.) from the tracker server 304c.  For example, an ad tracking tag may be a HTML JavaScript\ntag that sends multiple data fields to the tracker server 304c upon requesting the extraction script code.  These fields include, but are not limited to, campaign ID, client ID and script type.  The following is an example of a JavaScript ad tracking\ntag.  &lt;script type=\"text/javascript\" src=\"http://trackerserver/p? clientID=sampleclient&campaignID=samplecampaign&scripttype=script01\"&gt;&- lt;/script&gt;\nThe tracker server 304c may log request data upon receiving the request from the ad tracking tag, including, but not limited to, data such as a time stamp, user IP address, campaign ID, client ID, or the like.  The tracker server 304c may then\ngenerate and serve an extraction Script to the user.  The tracker server 304c may be configured to incorporate custom data fields including, but not limited to, time stamp, client ID, campaign ID, and session ID into the extraction script code before\nserving it to a user.  Some or all extracted relationships may be captured and associated with the timestamp at which it was captured.  An extraction script may be invoked to report captured data via multiple web requests (e.g., HTTP/HTTPS GET/POST)\nrequests to the tracker server 304c.\nAs an example, the extraction script may report captured information to the tracking server 304c via adding a 1.times.1 pixel HTML image element (e.g., a tracking pixel) with a custom source URL to the webpage content.\nThe following code demonstrates an example of such HTML image element.  &lt;img src=\"http://trackerserver/report?info=captured_information_in_pro- prietory_format\" style=\"display:none\"&gt;\nThe HTML image element may cause the browser to issue a web request (e.g., a HTTP/HTTPS GET/POST request) to a custom URL which points to the tracker server 304c and may include captured data fields formatted using various communication\nprotocols.  The tracker server 304c may respond to this HTTP/HTTPS GET/POST request by returning a transparent 1.times.1 image (e.g., a tracker pixel, which may be a GIF file) or by a status code (e.g., HTTP status-code 204), indicating that the server\nhas fulfilled the request but does not need to return an entity-body.  An extraction script may divide captured data into portions (e.g., chunks) using various algorithms and add a header to create data segments.\nA segment may contain a segment header and a data section.  The segment header may contain a number of mandatory fields along with a number of optional fields.\nA sample header may contain information related to the data type, segment size and sequence number as the mandatory fields, while information related to data section size and time stamp may be set as optional header fields.  The extraction\nscript may then report each segment to the tracker server 304c using an HTTP/HTTPS GET/POST request.\nA tracker server 304c may be configured to use various code instructions, data structures and processes to send and receive data through the network to other connected entities.  The data includes, but is not limited to, the data sent by the\nextraction script and data sent to user device using HTTP/HTTPS protocol through a network.  A tracker server 304c may be directly accessible through a network or it may be communicating through network management and protection systems including, but\nnot limited to, firewalls, intrusion detection systems, load balancers, or the like.\nUpon execution of a third party ad tracking tag, a HTTP/HTTPS request may be issued to the tracker server 304c to download an extraction script.  The tracker server 304c may generate an extraction script for this request based on multiple\nfactors, including, but not limited to, data fields provided by the third party ad tracking tag's HTTP/HTTPS initial request.  The tracker server 304c may also be configured to incorporate multiple data fields into the generated extraction script,\nincluding but not limited to a user ID and session ID, which according to a protocol may remain constant for some or all communications during the lifecycle of the generated extraction script.  In some embodiments, the user ID and/or the session ID may\nbe unique.\nThe tracker server 304c may log multiple data fields upon, during and after receiving or processing each and every request, including, but not limited to, HTTP/HTTPS request headers, client IP address, request query string, or the like.  The\ntracker server may be configured to preserve logs using different approaches including, but not limited to, keeping logs in memory, persisting logs to local disk, persist logs to database, transfer logs to a remote end point via network, or the like.  In\nan exemplary scenario, the tracker server 304c may use a communication fabric such as message-bus framework to transfer logs to a backend server for further processing.  The tracker server 304c may push log records as independent message data structures\ninto message-bus frameworks such as Kafka.TM., RabbitMQ.TM., etc. A message-bus framework may be configured to deliver messages to a number of local or remote servers for further processing.\nIn an exemplary scenario, the tracker server 304c may use a message-bus framework to transfer logs to a backend server for further processing.\nIn order to establish a complete session, data may be collected, correlated, filtered, computed and aggregated from various extraction sources.  Once a session has been established, the session may be evaluated using various models to determine\nthe Degree of Pretense (DOP).\nIn some embodiments, the ad tracking tag requests a tracking pixel instead of the extraction script.  Accordingly, the term \"script\" as used herein may refer to a tracking pixel, and vice versa.  As an example, the served content, ad tag or\nretrieved advertisement includes embedded code called a third party Ad tracking tag that requests a tracking pixel (e.g., JavaScript code, flash object, or the like) from the tracker server 304c.  An ad tracking tag may be an HTML JavaScript tag that\nsends multiple data fields to tracker server 304c upon requesting the tracking pixel code.  These fields include, but are not limited to, campaign ID, client ID and pixel type.  The following is an example of a JavaScript ad tracking tag.  &lt;script\ntype=\"text/javascript\" src=\"http://trackerserver/p?clientID=sampleclient&campaignID=samplecampai- gn&pixeltype=samplepixel\"&gt;&lt;/script&gt;\nTracker server 304c logs the request data upon receiving the request from ad tracking tag including but not limited to time stamp, user IP address, campaign ID, client ID, or the like.  Tracker server 304c then generates and serves a tracking\npixel to the user.  Tracker server 304c might incorporate custom data fields including, but not limited to time stamp, client ID, campaign ID, and session ID into the tracking pixel code before serving it to the user.  The tracking pixel includes\nproprietary code that identifies, captures and reports static and dynamic characteristics or attributes regarding ad placement, user and user device 302 (or other entities).  Such characteristics include, but are not limited to mouse clicks, mouse\nmovements, browser user agent, screen resolution, publisher, referrer, or the like.  Captured characteristics may be associated with the exact timestamp at which they were captured, creating time series data fields.  The tracking pixel may report\ncaptured data via multiple HTTP/HTTPS Get requests to the tracker server 304c.  As an example, the tracking pixel may report captured information to the tracking server 304c via adding a 1.times.1 pixel HTML image element with custom source URL to the\nwebpage content.  The following code demonstrates an example of such HTML image element.  &lt;img src=\"http://trackerserver/report?info=captured_information_in_proprietory- _format\" style=\"display:none\"&gt;\nThe HTML image element forces the browser to issue a HTTP/HTTPS Get request to the custom URL which points to the tracker server 304c and includes captured data fields formatted using a proprietary communication protocol.  The tracker server\n304c responds to this HTTP/HTTPS Get request by returning a transparent 1.times.1 image (normally a GIF file) or by HTTP status-code 204 indicating that the server has fulfilled the request but does not need to return an entity-body.  The tracking pixel\nmay divide captured data into portions using a proprietary algorithm and add a custom proprietary header to create data segments.  A segment contains a segment header and a data section.  The segment header contains a number of mandatory fields along\nwith a number of optional fields.  A sample header may contain data type, segment size and sequence number as mandatory fields, while data section size and time stamp may be set as optional header fields.  The tracking pixel then reports each segment to\nthe tracker server 304c using an independent HTTP/HTTPS Get request.\nTracker server 304c includes code, data structures and processes to send and receive data via a network.  Such data includes, but is not limited to, data sent by tracking pixel and data sent to user device using HTTP/HTTPS protocol via the\nnetwork.  Tracker server 304c might be directly accessible via the network or may be communicating through network management and protection systems including, but not limited to, firewalls, intrusion detection systems, load balancers, or the like.\nUpon execution of a third party ad tracking tag, a HTTP/HTTPS request is issued to tracker server 304c to download a tracking pixel.  Tracker server 304c generates a tracking pixel for this request based on multiple factors including, but not\nlimited to, data fields provided by third party Ad tracking tag HTTP/HTTPS initial request.  Tracker server 304c also incorporates multiple data fields into the generated tracking pixel including, but not limited to, unique user ID and unique session ID\nwhich, according to a proprietary protocol, may remain constant for all communications during the lifecycle of the newly generated tracking pixel.\nTracker server 304c logs multiple data fields upon, during and after receiving or processing each and every request including but not limited to HTTP/HTTPS request headers, client IP address, request query string, or the like.  Tracker server\n304c preserves logs using different approaches including, but not limited to, keeping logs in memory, persisting logs to a local disk, persisting logs to a database, transferring logs to a remote end point via a network, or the like.  In an exemplary\nscenario, tracker server 304c uses a communication fabric such as message-bus framework to transfer logs to a backend server for further processing.  Tracker server may also push log records as independent message data structures into message-bus\nframeworks such as Kafka, RabbitMQ, or the like.  The message-bus framework may deliver messages to arbitrary number of local or remote servers for further processing.\n<BR><BR>Example Embodiments\nThe following section describes further example embodiments.  A person skilled in the art will appreciate that modifications and other variations not explicitly described herein are contemplated.\nReferring to FIGS. 4 and 5, example workflows are provided wherein session data is extracted and processed.  FIG. 5 provides granular blocks for determining a DOP for a specific relation between entities A and B.\nIn the following examples, sample information may be obtained or extracted for a session at block 402:\nTABLE-US-00002 Data Relation Metric Value User-Publisher PublisherID F836-A3193 Publisher Controlled Somedomainxyz.com Domains and IPs Othedomainxyz.com 23.24.1.23 UserID 3B593 User Device PC-x86 User Session ID 93A6 User IP 67.34.23.1 User\nCountry Gondor User-Browser Browser Dimensions 1250 .times.  630 px Browser resize event 23 s, 167 s times Browser Vendor Google UserID Y7A23 User Device PC-Dell-Optiplex 755 User Session ID 93A6 User IP 192.168.1.45 User-Website Website URL\nsomedomainxyz.com/page 1 HTML Doc-type -//W3C//DTD HTML 4.01//EN Website category Tech User mouse movement 190 events UserID PT6541 User Session ID 93A6 User IP 67.34.23.1 User Interaction time 130 s\nThe data may be aggregated by the matching of session IDs at block 404, and filtered and cleansed at block 406.\nOnce the above information is gathered, aggregated and filtered, the information may be recorded as a data structure referred to as \"context\" for other attributes at block 408.  At block 410, each relation with entities (A, B) is retrieved.  For\neach of the extracted relations, if previously generated models exist, the information for that relation along with the context is applied to each available model to generate a respective PDOP at block 412.  At block 414, the respective PDOP is stored.\nReferring to FIG. 5, at block 502, a relation with entities (A, B) is retrieved.  At block 504, a model is applied to the context and relation data to generate a PDOP value.  At block 506, the PDOP value is stored.  At block 508, once all PDOPs\nare calculated, a data fusion technique is used to derive a DOP for that relation.\nReturning to FIG. 4, once all relation DOPs are calculated, all DOPs along with the context are provided to a final model to derive the final DOP at block 416.\nThe table below follows these functional blocks using a sample averaging data fusion method for relation DOP calculation and a minimum function for the final DOP.\nTABLE-US-00003 PDOP PDOP PDOP PDOP for for for for DOP Model Model Model Model for #1 #2 #3 #4 relation User-Publisher 0.4 0.7 0.1 0.4 User-Browser 0.7 0.3 0.5 User-Website 0.6 0.9 0.3 0.2 0.5 Session DOP (Based on 0.4 the Minimum Function)\nSample Weighted Average\nThe above example, the final model may be a Parametric and Non-Parametric Statistical Model which takes in all calculated relation DOPs and produces a final DOP value.  The example above utilizes a simple model as an illustrative example and it\nstates the minimum of all relation DOPs as the final DOP for that session.  In another example embodiment, other models are used which include a weighted average over all relation DOPs.  An example is illustrated in the table below:\nTABLE-US-00004 User- User- User- Advertisement- Relation Publisher Browser Website Browser .  . . . . . Weight 0.4 0.2 0.8 0.3 0.5 0.2 Relation OOP 0.4 0.5 0.5 0 0 0 Weight times OOP 0.16 0.1 0.13 0 0 0 0.39 FinalDOP\nIn order to establish a complete session, data may be collected, correlated, filtered, and aggregated from multiple extraction sources.  This may occur within a limited amount of time.  Once a session has been established, it is evaluated using\nvarious models to determine the Degree of Pretense (DOP).  Input data from various sources may arrive in a streaming manner.  The system may expand and contract in order to accommodate the volume of input data.  Some embodiments may use computing\nresources to perform the necessary functionality and meet the necessary deadlines.  A system of components may be devised to process the incoming volume of data.\nExamples of such components are described herein.\nMessage: a Message may be a self-identified container to store various data items.  Each Message may include an identifier as an indication of its contents.  Each data item may have a corresponding unique implicit or explicit name.  If the name\nis explicitly defined, it may accompany the data item.  The Message may be able to output the corresponding data item given the implicit or explicit name associated with that particular data item.\nDistributed Component: a Distributed Component is a virtual entity for which one or more actual component instances are created and distributed across one or more computer systems and managed as a single entity.  A Distributed Component may\nregister itself with a Controller Component.  A Controller Component may consider various metrics in order to determine placement, quantity and the amount of resources usable by the instance.  These metrics may include but are not limited to the load on\ninstance, the total load on the computer system, and the amount of computing resources available.  The behavior of the Controller Component may also be fully or partially controlled through configuration information or instructions received from a\nDistributed Component or a configuration file.\nPartial Evaluator: a Partial Evaluator is a module which takes a set of parameters as input and produces a value of numeric type along with the possible range of the output value.  The Partial Evaluator produces the output value through the\napplication of one or more previously generated statistical models.\nCommunication Fabric (CF): a Distributed Component through which other interfacing components can pass and retrieve messages between each other.  A sender may send messages to the CF and a consumer may retrieve messages from the CF.  The sender\nmay specify a set of attributes to accompany the message.  However, the exact final destination of the messages is optional.  The consumer may ask the CF to be sent all messages matching a particular pattern over the accompanying set of message\nattributes.\nThe consumer may choose to pull messages from the CF given the required pattern or the consumer may choose to have incoming matched messages pushed to it.  In either case, all messages matching the pattern may be queued for a specified or\ncalculated amount of time, or for a specified or calculated amount of memory or disk space.\nThe CF may distribute input messages across one or more machines to satisfy redundancy requirements, as defined by configuration parameters.\nAggregation: a Distributed Component may be responsible for the assembly of messages received over a period of time.  A message may only be sent to one instance of this component through the Communication Fabric.  The messages may come in any\norder.  Each message carries an Identifier.  Messages containing the same identifier are always sent to the same Aggregation Component instance.  Within an Aggregation component instance, messages with the same Identifier are stored together as a Message\nSet.  The Message Sets form the Active Sets.  Upon the reception of a message, if it is determined that the message does not belong to an available Message Set, using its ID, a Message Set is created and added to the Active Set along with the original\nmessage.  The aggregation component maintains an Active Time Window which determines when a Message Set is to be removed from the Active Sets and sent to downstream components.  The Active Time Window expands or contracts based on a variety of factors,\nbut it may not go below a minimum threshold.  A combination of Parametric and Non-Parametric Statistical Models are used to determine the length of the Active Time Window.  These models consider various attributes such as time of message arrivals, time\nbetween message arrivals, length of messages, data from received messages, and data received through various auxiliary channels.  These Statistical Models may be updated as new messages arrive.\nFilter: a Distributed Component may be responsible for the separation of Message Sets which contain messages that do not adhere to a deterministic set of rules.  Each Message Set is received by a single instance of the Filter Component.  An\nexample of such rules for messages contained within a single Message Set include messages with absent data fields, inconsistent timestamps and versions, abnormal formats, or show signs of tampering.  Message Sets which do not adhere to the rules are\nlabeled as such and are separated from the other Message Sets.\nSession Generator: a Distributed Component may be responsible for the creation of a single Message from a Message Set.  Each Message Set is received by a single instance of the Session Generator Component.  Redundant and duplicate data\nattributes are removed.  Compressed data is decompressed.  Encrypted data is mapped and decrypted.  Values are normalized and a new Message is created using these values.\nRelation DOP Evaluator (RDOPE): an RDOPE is a Partial Evaluator which takes a set of tuples as input; a tuple that contains a relation reference or indicator, a tuple containing the relation and its context along with all defined attributes. \nThe RDOP instance may first attempt to calculate all Partial DOPs (PDOPs) through the invocation of relevant models defined for the respective relation.  It may then combine all PDOPs into a single DOP as the final result for each relation through Data\nFusion techniques.\nFinal DOP Evaluator (FDOPE): an FDOPE is a Partial Evaluator which takes a set of tuples as input.  Each tuple may contain a relation reference or indicator, a DOP value, and a range.  This is given to a previously generated statistical model in\norder to derive a final DOP.\nDOP Calculator: a Distributed Component responsible for evaluating the Degree of Pretense for an input session.  Each session is received by a single instance of this component in the form of a Message.  A DOP Calculator instance may first\nattempt to calculate all Partial DOPs (PDOPs) through the invocation of relevant DOP Evaluators for each defined relation.  It may then combine all PDOPs into a single DOP for each relation.  It may then invoke the FDOPE in order to derive the final DOP. The final DOP is then added as a data item to the original input session.\nStorage: Sessions in the form a Message are stored in persistent storage.  The type of storage depends on the Message identifier.  Messages are queried using predefined key sets.\nThese example definitions and components are provided for illustrative purposes.  The components are referred to and described herein in different ways and this section is not intended to be limiting.\nA message may be self-descriptive.  The message may be implemented in a language-independent data format.  Code for parsing and generating the message data may be readily implementable in different programming languages.  One candidate for\nimplementing a message is JSON data format.  JSON or JavaScript Object Notation is an open standard format that uses human-readable text to transmit data objects consisting of attribute-value pairs.  It is used primarily to transmit data between a server\nand web application, as an alternative to XML.  An example of JSON format for a message is shown below:\nTABLE-US-00005 { \"timestamp\": \"2014-12-02T03-26-48.593\", \"querystring\": { \"AG\": [ \"s159!2t\" ], \"clientID\": [ \"client\" ], \"Campaign\": [ \"put-campaign-name-here\" ], \"Creative\": [ \"put-creative-name-here\" ] }, \"IPAddress\": \"199.167.xx.xxx\",\n\"extraparams\": { \"userID\": \"db209684-4d83-4a8f-b5e7-7a95b60f09cf\", \"sessionID\": \"eebe8762-168e-4982-9cae-209f4f994f44\" }, \"requestheader\": { \"Referer\": [ \"http://referrer_url.com\" ], \"Connection\": [ \"keep-alive\" ], \"Accept\": [ \"*/*\" ], \"Cache-Control\": [\n\"max-age=259200\" ], \"Accept-Language\": [ \"en-us\" ], \"Accept-Encoding\": [ \"gzip, deflate\" ], \"User-Agent\": [ \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 2.0.50727)\" ], \"Host\": [ \"samplehost.com\" ] } }\nA distributed component is a computational unit that autonomously executes a piece of code that may involve accessing data storage and communicating with other distributed components.  Distributed components communicate with each other by\nmessage passing.  Distributed components are managed by one or more controller components, also known as coordinators based on the architecture.\nAn example of a distributed computation framework is Apache Storm.  Apache Storm is an open source distributed real-time computation system that processes unbounded streams of data for real-time processing.  Storm is cross-platform and may\nprovide parallelizing tasks and pipelining in relation to incoming streams of data for processing.\nA Bolt in Storm architecture could represent a distributed component.  It autonomously runs a predefined functions and it may access the data storage to process data and store the result.  Storm provides the framework to add and remove\ndistributed components on demand.\nA partial evaluator is a piece of code which takes a set of parameters as input and produces a value of numeric type along with the possible range of the output value.  As an example, a partial evaluator might calculate the reparation value of a\npublisher on which the ad has been loaded.  The output result may be a numerical number in the range of [0, 1].\nA partial evaluator may be executed by a distributed component such as an Apache Storm Bolt, for example.\nThe Communication Fabric (CF) is a Distributed Component consisting of several individual instances which communicate with each other in a peer-to-peer manner.  Upon the reception of a message, through various mechanism such as digest\ngeneration, it may be stored on at least one instance.  If configuration parameters include a required redundancy level, the message is also replicated across multiple instances in such a fashion as to minimize the number of messages that are\ninaccessible during the failure of one or more instances.\nClients of the CF can request messages matching a particular pattern.  A combination of a client and a pattern is considered an Address.  Multiple clients can specify the same pattern and request that messages be distributed amongst them using a\npredefined scheduling algorithm (for example, round-robin).  Otherwise, each client may receive the message independently from other clients requesting the same pattern.  In such case, an Address is defined as a collection of clients, a scheduling\nprocess and a pattern.\nEach address has an associated FIFO (first in, first out) queue for which messages matching the Address pattern are queued for consumption.  Consumption can be carried out through a push model, wherein messages are sent to client(s) once the\nmessages arrive in the respective queue.  Consumption can also be carried out through a pull model, wherein messages are queued and it is the responsibility of the client(s) to obtain the messages from the CF.\nIn situations where multiple clients seek the same pattern, the scheduling process may be used to distribute the messages amongst the available clients.  Once a message has been sent to or read by the client(s), a signal is required to be sent\nto the CF to remove the message from the queue.\nA configuration parameter may determine when queued messages may be expunged and removed from the CF irrespective of whether the queued messages were read or not.\nThe deterministic set of rules incorporate regulations to determine the validity of messages processed through the system.  Invalid messages may be received due to various technical glitches, bugs, or system failures.  Invalid messages may also\nbe of a malicious nature generated outside of the system.\nExamples of such rules include: completeness of data included in a message, correlation of certain extracted properties such as user IP addresses obtained through some relations, and the number of messages included within a session.\nThe embodiments of the devices, systems and methods described herein may be implemented in a combination of both hardware and software.  These embodiments may be implemented on programmable computers, each computer including at least one\nprocessor, a data storage system (including volatile memory or non-volatile memory or other data storage elements or a combination thereof), and at least one communication interface.\nProgram code is applied to input data to perform the functions described herein and to generate output information.  The output information is applied to one or more output devices.  In some embodiments, the communication interface may be a\nnetwork communication interface.  In embodiments in which elements may be combined, the communication interface may be a software communication interface, such as those for inter-process communication.  In still other embodiments, there may be a\ncombination of communication interfaces implemented as hardware, software, and combination thereof.\nThroughout the foregoing discussion, numerous references may be made regarding servers, services, interfaces, portals, platforms, or other systems formed from computing devices.  It should be appreciated that the use of such terms is deemed to\nrepresent one or more computing devices having at least one processor configured to execute software instructions stored on a computer readable tangible, non-transitory medium.  For example, a server can include one or more computers operating as a web\nserver, database server, or other type of computer server in a manner to fulfill described roles, responsibilities, or functions.\nThe following discussion provides many example embodiments.  Although each embodiment represents a single combination of inventive elements, other examples may include all possible combinations of the disclosed elements.  Thus if one embodiment\ncomprises elements A, B, and C, and a second embodiment comprises elements B and D, other remaining combinations of A, B, C, or D, may also be used.\nThe term \"connected\" or \"coupled to\" may include both direct coupling (in which two elements that are coupled to each other contact each other) and indirect coupling (in which at least one additional element is located between the two elements).\nThe technical solution of embodiments may be in the form of a software product.  The software product may be stored in a non-volatile or non-transitory storage medium, which can be a compact disk read-only memory (CD-ROM), a USB flash disk, or a\nremovable hard disk.  The software product includes a number of instructions that enable a computer device (personal computer, server, or network device) to execute the methods provided by the embodiments.\nThe embodiments described herein are implemented by physical computer hardware, including computing devices, servers, receivers, transmitters, processors, memory, displays, and networks.  The embodiments described herein provide useful physical\nmachines and particularly configured computer hardware arrangements.  The embodiments described herein are directed to electronic machines and methods implemented by electronic machines adapted for processing and transforming electromagnetic signals\nwhich represent various types of information.  The embodiments described herein pervasively and integrally relate to machines, and their uses; and the embodiments described herein have no meaning or practical applicability outside their use with computer\nhardware, machines, and various hardware components.  Substituting the physical hardware particularly configured to implement various acts for non-physical hardware, using mental steps for example, may substantially affect the way the embodiments work. \nSuch computer hardware limitations are clearly essential elements of the embodiments described herein, and they cannot be omitted or substituted for mental means without having a material effect on the operation and structure of the embodiments described\nherein.  The computer hardware is essential to implement the various embodiments described herein and is not merely used to perform steps expeditiously and in an efficient manner.\nIn FIG. 6, for simplicity, only one example computing device 600 is shown but system may include more computing devices 600 operable by users to access remote network resources and exchange data.  The computing devices 600 may be the same or\ndifferent types of devices.  The computing device 600 comprises at least one processor 602, a data storage device 604 (including volatile memory or non-volatile memory or other data storage elements or a combination thereof), and at least one\ncommunication interface.  The computing device components may be connected in various ways including directly coupled, indirectly coupled via a network, and distributed over a wide geographic area and connected via a network (which may be referred to as\n\"cloud computing\").\nFor example, and without limitation, the computing device may be a server, network appliance, set-top box, embedded device, computer expansion module, personal computer, laptop, personal data assistant, cellular telephone, smartphone device,\nUMPC tablets, video display terminal, gaming console, electronic reading device, and wireless hypermedia device or any other computing device capable of being configured to carry out the methods described herein\nFIG. 6 is a schematic diagram of computing device, exemplary of an embodiment.  As depicted, computing device 600 includes at least one processor 602, memory 604, at least one I/O interface 606, and at least one network interface 608.\nEach processor 602 may be, for example, any type of general-purpose microprocessor or microcontroller, a digital signal processing (DSP) processor, an integrated circuit, a field programmable gate array (FPGA), a reconfigurable processor, a\nprogrammable read-only memory (PROM), or any combination thereof.\nMemory 604 may include a suitable combination of any type of computer memory that is located either internally or externally such as, for example, random-access memory (RAM), read-only memory (ROM), compact disc read-only memory (CDROM),\nelectro-optical memory, magneto-optical memory, erasable programmable read-only memory (EPROM), and electrically-erasable programmable read-only memory (EEPROM), Ferroelectric RAM (FRAM) or the like.\nEach I/O interface 606 enables computing device 600 to interconnect with one or more input devices, such as a keyboard, mouse, camera, touch screen and a microphone, or with one or more output devices such as a display screen and a speaker.\nEach network interface 608 enables computing device 600 to communicate with other components, to exchange data with other components, to access and connect to network resources, to serve applications, and perform other computing applications by\nconnecting to a network (or multiple networks) capable of carrying data including the Internet, Ethernet, plain old telephone service (POTS) line, public switch telephone network (PSTN), integrated services digital network (ISDN), digital subscriber line\n(DSL), coaxial cable, fiber optics, satellite, mobile, wireless (e.g. W-Fi, WMAX), SS7 signaling network, fixed line, local area network, wide area network, and others, including any combination of these.\nAlthough the embodiments have been described in detail, it should be understood that various changes, substitutions and alterations can be made herein without departing from the scope as defined by the appended claims.\nMoreover, the scope of the present application is not intended to be limited to the particular embodiments of the process, machine, manufacture, composition of matter, means, methods and steps described in the specification.  As one of ordinary\nskill in the art may readily appreciate from the disclosure of the present invention, processes, machines, manufacture, compositions of matter, means, methods, or steps, presently existing or later to be developed, that perform substantially the same\nfunction or achieve substantially the same result as the corresponding embodiments described herein may be utilized.  The examples described above and illustrated are intended to be exemplary only.", "application_number": "15170196", "abstract": " Systems and methods are provided in the field of online advertising and\n     malicious software. In some embodiments, a method for determining a\n     degree of deceptiveness for a session is provided, the session having one\n     or more relationships between one or more properties of interactions with\n     an online advertisement, the method comprising: extracting one or more\n     characteristics associated with each of the one or more relationships;\n     applying one or more models to each of the one or more relationships to\n     determine a degree of deceptiveness for each of the one or more\n     relationships; aggregating the degree of deceptiveness for each of the\n     one or more relationships to generate the degree of deceptiveness for the\n     session.\n", "citations": ["20030220901", "20070239604", "20070288312", "20080077462", "20090157417", "20140164313"], "related": ["62169878"]}, {"id": "20170109354", "patent_code": "10373060", "patent_name": "Answer scoring by using structured resources to generate paraphrases", "year": "2019", "inventor_and_country_data": " Inventors: \nBoxwell; Stephen A. (Columbus, OH), Smythe; Jared M. (Cary, NC)  ", "description": "<BR><BR>BACKGROUND\nThe present disclosure relates to creating sets of scored paraphrases based on a structured resource and using the sets of scored paraphrases to identify and score candidate answers that correspond to a user question.\nA traditional search engine produces the most accurate answers when words in a question match a passage in a corpus of documents in the same order.  For example, if the traditional search engine receives a question of \"Who is the president of\nCompany ABC?\", the traditional search engine produces accurate results if the corpus includes a document with the passage \"Bill Smith is the president of Company ABC.\" If a passage in the corpus of documents does not closely match the words of the\nquestion in order, the traditional search engine is less likely to produce an accurate answer.\nIn reality, a corpus of documents may not have matching paraphrases, but rather have passages that include answers to a question such as \"Bill Smith leads Company ABC\", \"The CEO of Company ABC is Bill Smith\", or \"Company ABC's Chairperson is\nBill Smith\".  Unfortunately, the traditional search engine may not detect information in these passages to determine candidate answers.\n<BR><BR>BRIEF SUMMARY\nAccording to one embodiment of the present disclosure, an approach is provided in which a knowledge manager creates a pattern set that includes paraphrases and corresponding paraphrase scores.  The paraphrase scores are based on a set of first\ncandidate answers obtained from querying a first set of resource data.  The knowledge manager uses the paraphrases to perform a search on a second set of resource data and identify a set of second candidate answers.  In turn, the knowledge manager scores\nthe set of second candidate answers based on the paraphrase scores corresponding to the paraphrases utilized to identify the set of second candidate answers.\nThe foregoing is a summary and thus contains, by necessity, simplifications, generalizations, and omissions of detail; consequently, those skilled in the art will appreciate that the summary is illustrative only and is not intended to be in any\nway limiting.  Other aspects, inventive features, and advantages of the present disclosure, as defined solely by the claims, will become apparent in the non-limiting detailed description set forth below. <BR><BR>BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF\nTHE DRAWINGS\nThe present disclosure may be better understood, and its numerous objects, features, and advantages made apparent to those skilled in the art by referencing the accompanying drawings, wherein:\nFIG. 1 is a block diagram of a data processing system in which the methods described herein can be implemented; and\nFIG. 2 provides an extension of the information handling system environment shown in FIG. 1 to illustrate that the methods described herein can be performed on a wide variety of information handling systems which operate in a networked\nenvironment;\nFIG. 3 is an exemplary diagram depicting a knowledge manager that groups semantically similar paraphrases into pattern sets and utilizes the semantically similar paraphrases to generate new questions for use in a traditional search engine;\nFIG. 4 is a diagram depicting a pattern set generator grouping semantically similar paraphrases and corresponding syntactic patterns into pattern sets;\nFIG. 5 is an exemplary diagram depicting a training pattern generator creating training syntactic patterns and paraphrases based upon training data;\nFIG. 6 is an exemplary diagram depicting a semantically similar question generator matching a user question's syntactic pattern to a syntactic pattern in a pattern set and generating new questions based upon paraphrases included in the pattern\nset;\nFIG. 7 is an exemplary flowchart showing steps taken by a knowledge manager to generate pattern sets from training data;\nFIG. 8 is an exemplary flowchart showing steps taken by a knowledge manager to match a question's syntactic pattern to semantic patterns in a pattern set and generate new questions based upon paraphrases found in a pattern set corresponding to a\nmatched syntactic pattern; and\nFIG. 9 is an exemplary flowchart showing steps taken to generate new questions from paraphrases and perform a query on a resource.\n<BR><BR>DETAILED DESCRIPTION\nThe terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the disclosure.  As used herein, the singular forms \"a\", \"an\" and \"the\" are intended to include the plural forms as\nwell, unless the context clearly indicates otherwise.  It will be further understood that the terms \"comprises\" and/or \"comprising,\" when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and/or\ncomponents, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof.\nThe corresponding structures, materials, acts, and equivalents of all means or step plus function elements in the claims below are intended to include any structure, material, or act for performing the function in combination with other claimed\nelements as specifically claimed.  The description of the present disclosure has been presented for purposes of illustration and description, but is not intended to be exhaustive or limited to the disclosure in the form disclosed.  Many modifications and\nvariations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the disclosure.  The embodiment was chosen and described in order to best explain the principles of the disclosure and the practical\napplication, and to enable others of ordinary skill in the art to understand the disclosure for various embodiments with various modifications as are suited to the particular use contemplated.\nThe present invention may be a system, a method, and/or a computer program product.  The computer program product may include a computer readable storage medium (or media) having computer readable program instructions thereon for causing a\nprocessor to carry out aspects of the present invention.\nThe computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device.  The computer readable storage medium may be, for example, but is not limited to, an electronic\nstorage device, a magnetic storage device, an optical storage device, an electromagnetic storage device, a semiconductor storage device, or any suitable combination of the foregoing.  A non-exhaustive list of more specific examples of the computer\nreadable storage medium includes the following: a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a static random access memory (SRAM), a\nportable compact disc read-only memory (CD-ROM), a digital versatile disk (DVD), a memory stick, a floppy disk, a mechanically encoded device such as punch-cards or raised structures in a groove having instructions recorded thereon, and any suitable\ncombination of the foregoing.  A computer readable storage medium, as used herein, is not to be construed as being transitory signals per se, such as radio waves or other freely propagating electromagnetic waves, electromagnetic waves propagating through\na waveguide or other transmission media (e.g., light pulses passing through a fiber-optic cable), or electrical signals transmitted through a wire.\nComputer readable program instructions described herein can be downloaded to respective computing/processing devices from a computer readable storage medium or to an external computer or external storage device via a network, for example, the\nInternet, a local area network, a wide area network and/or a wireless network.  The network may comprise copper transmission cables, optical transmission fibers, wireless transmission, routers, firewalls, switches, gateway computers and/or edge servers. \nA network adapter card or network interface in each computing/processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium\nwithin the respective computing/processing device.\nComputer readable program instructions for carrying out operations of the present invention may be assembler instructions, instruction-set-architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, firmware\ninstructions, state-setting data, or either source code or object code written in any combination of one or more programming languages, including an object oriented programming language such as Smalltalk, C++ or the like, and conventional procedural\nprogramming languages, such as the \"C\" programming language or similar programming languages.  The computer readable program instructions may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package,\npartly on the user's computer and partly on a remote computer or entirely on the remote computer or server.  In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area network\n(LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).  In some embodiments, electronic circuitry including, for example, programmable logic\ncircuitry, field-programmable gate arrays (FPGA), or programmable logic arrays (PLA) may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to personalize the electronic\ncircuitry, in order to perform aspects of the present invention.\nAspects of the present invention are described herein with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems), and computer program products according to embodiments of the invention.  It will be\nunderstood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer readable program instructions.\nThese computer readable program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute\nvia the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.  These computer readable program instructions may also be\nstored in a computer readable storage medium that can direct a computer, a programmable data processing apparatus, and/or other devices to function in a particular manner, such that the computer readable storage medium having instructions stored therein\ncomprises an article of manufacture including instructions which implement aspects of the function/act specified in the flowchart and/or block diagram block or blocks.\nThe computer readable program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other device to cause a series of operational steps to be performed on the computer, other programmable apparatus or\nother device to produce a computer implemented process, such that the instructions which execute on the computer, other programmable apparatus, or other device implement the functions/acts specified in the flowchart and/or block diagram block or blocks.\nThe flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present invention. \nIn this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of instructions, which comprises one or more executable instructions for implementing the specified logical function(s).  In some alternative\nimplementations, the functions noted in the block may occur out of the order noted in the figures.  For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse\norder, depending upon the functionality involved.  It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special\npurpose hardware-based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.  The following detailed description will generally follow the summary of the disclosure, as set\nforth above, further explaining and expanding the definitions of the various aspects and embodiments of the disclosure as necessary.\nFIG. 1 depicts a schematic diagram of one illustrative embodiment of a question/answer creation (QA) system 100 in a computer network 102.  Knowledge manager 100 may include a computing device 104 (comprising one or more processors and one or\nmore memories, and potentially any other computing device elements generally known in the art including buses, storage devices, communication interfaces, and the like) connected to the computer network 102.  The network 102 may include multiple computing\ndevices 104 in communication with each other and with other devices or components via one or more wired and/or wireless data communication links, where each communication link may comprise one or more of wires, routers, switches, transmitters, receivers,\nor the like.  Knowledge manager 100 and network 102 may enable question/answer (QA) generation functionality for one or more content users.  Other embodiments of knowledge manager 100 may be used with components, systems, sub-systems, and/or devices\nother than those that are depicted herein.\nKnowledge manager 100 may be configured to receive inputs from various sources.  For example, knowledge manager 100 may receive input from the network 102, a corpus of electronic documents 106 or other data, a content creator 108, content users,\nand other possible sources of input.  In one embodiment, some or all of the inputs to knowledge manager 100 may be routed through the network 102.  The various computing devices 104 on the network 102 may include access points for content creators and\ncontent users.  Some of the computing devices 104 may include devices for a database storing the corpus of data.  The network 102 may include local network connections and remote connections in various embodiments, such that knowledge manager 100 may\noperate in environments of any size, including local and global, e.g., the Internet.  Additionally, knowledge manager 100 serves as a front-end system that can make available a variety of knowledge extracted from or represented in documents,\nnetwork-accessible sources and/or structured resource sources.  In this manner, some processes populate the knowledge manager with the knowledge manager also including input interfaces to receive knowledge requests and respond accordingly.\nIn one embodiment, the content creator creates content in a document 106 for use as part of a corpus of data with knowledge manager 100.  The document 106 may include any file, text, article, or source of data for use in knowledge manager 100. \nContent users may access knowledge manager 100 via a network connection or an Internet connection to the network 102, and may input questions to knowledge manager 100 that may be answered by the content in the corpus of data.  As further described below,\nwhen a process evaluates a given section of a document for semantic content, the process can use a variety of conventions to query it from the knowledge manager.  One convention is to send a well-formed question.  Semantic content is content based on the\nrelation between signifiers, such as words, phrases, signs, and symbols, and what they stand for, their denotation, or connotation.  In other words, semantic content is content that interprets an expression, such as by using Natural Language (NL)\nProcessing.  In one embodiment, the process sends well-formed questions (e.g., natural language questions, etc.) to the knowledge manager.  Knowledge manager 100 may interpret the question and provide a response to the content user containing one or more\nanswers to the question.  In some embodiments, knowledge manager 100 may provide a response to users in a ranked list of answers.\nIn some illustrative embodiments, knowledge manager 100 may be the IBM Watson.TM.  QA system available from International Business Machines Corporation of Armonk, N.Y., which is augmented with the mechanisms of the illustrative embodiments\ndescribed hereafter.  The IBM Watson.TM.  knowledge manager system may receive an input question which it then parses to extract the major features of the question, that in turn are then used to formulate queries that are applied to the corpus of data. \nBased on the application of the queries to the corpus of data, a set of hypotheses, or candidate answers to the input question, are generated by looking across the corpus of data for portions of the corpus of data that have some potential for containing\na valuable response to the input question.\nThe IBM Watson.TM.  QA system then performs deep analysis on the language of the input question and the language used in each of the portions of the corpus of data found during the application of the queries using a variety of reasoning\nalgorithms.  There may be hundreds or even thousands of reasoning algorithms applied, each of which performs different analysis, e.g., comparisons, and generates a score.  For example, some reasoning algorithms may look at the matching of terms and\nsynonyms within the language of the input question and the found portions of the corpus of data.  Other reasoning algorithms may look at temporal or spatial features in the language, while others may evaluate the source of the portion of the corpus of\ndata and evaluate its veracity.\nThe scores obtained from the various reasoning algorithms indicate the extent to which the potential response is inferred by the input question based on the specific area of focus of that reasoning algorithm.  Each resulting score is then\nweighted against a statistical model.  The statistical model captures how well the reasoning algorithm performed at establishing the inference between two similar passages for a particular domain during the training period of the IBM Watson.TM.  QA\nsystem.  The statistical model may then be used to summarize a level of confidence that the IBM Watson.TM.  QA system has regarding the evidence that the potential response, i.e. candidate answer, is inferred by the question.  This process may be\nrepeated for each of the candidate answers until the IBM Watson.TM.  QA system identifies candidate answers that surface as being significantly stronger than others and thus, generates a final answer, or ranked set of answers, for the input question. \nMore information about the IBM Watson.TM.  QA system may be obtained, for example, from the IBM Corporation website, IBM Redbooks, and the like.  For example, information about the IBM Watson.TM.  QA system can be found in Yuan et al., \"Watson and\nHealthcare,\" IBM developerWorks, 2011 and \"The Era of Cognitive Systems: An Inside Look at IBM Watson and How it Works\" by Rob High, IBM Redbooks, 2012.\nTypes of information handling systems that can utilize knowledge manager 100 range from small handheld devices, such as handheld computer/mobile telephone 110 to large mainframe systems, such as mainframe computer 170.  Examples of handheld\ncomputer 110 include personal digital assistants (PDAs), personal entertainment devices, such as MP3 players, portable televisions, and compact disc players.  Other examples of information handling systems include pen, or tablet, computer 120, laptop, or\nnotebook, computer 130, personal computer system 150, and server 160.  As shown, the various information handling systems can be networked together using computer network 100.  Types of computer network 102 that can be used to interconnect the various\ninformation handling systems include Local Area Networks (LANs), Wireless Local Area Networks (WLANs), the Internet, the Public Switched Telephone Network (PSTN), other wireless networks, and any other network topology that can be used to interconnect\nthe information handling systems.  Many of the information handling systems include nonvolatile data stores, such as hard drives and/or nonvolatile memory.  Some of the information handling systems shown in FIG. 1 depicts separate nonvolatile data stores\n(server 160 utilizes nonvolatile data store 165, and mainframe computer 170 utilizes nonvolatile data store 175.  The nonvolatile data store can be a component that is external to the various information handling systems or can be internal to one of the\ninformation handling systems.  An illustrative example of an information handling system showing an exemplary processor and various components commonly accessed by the processor is shown in FIG. 2.\nFIG. 2 illustrates information handling system 200, more particularly, a processor and common components, which is a simplified example of a computer system capable of performing the computing operations described herein.  Information handling\nsystem 200 includes one or more processors 210 coupled to processor interface bus 212.  Processor interface bus 212 connects processors 210 to Northbridge 215, which is also known as the Memory Controller Hub (MCH).  Northbridge 215 connects to system\nmemory 220 and provides a means for processor(s) 210 to access the system memory.  Graphics controller 225 also connects to Northbridge 215.  In one embodiment, PCI Express bus 218 connects Northbridge 215 to graphics controller 225.  Graphics controller\n225 connects to display device 230, such as a computer monitor.\nNorthbridge 215 and Southbridge 235 connect to each other using bus 219.  In one embodiment, the bus is a Direct Media Interface (DMI) bus that transfers data at high speeds in each direction between Northbridge 215 and Southbridge 235.  In\nanother embodiment, a Peripheral Component Interconnect (PCI) bus connects the Northbridge and the Southbridge.  Southbridge 235, also known as the I/O Controller Hub (ICH) is a chip that generally implements capabilities that operate at slower speeds\nthan the capabilities provided by the Northbridge.  Southbridge 235 typically provides various busses used to connect various components.  These busses include, for example, PCI and PCI Express busses, an ISA bus, a System Management Bus (SMBus or SMB),\nand/or a Low Pin Count (LPC) bus.  The LPC bus often connects low-bandwidth devices, such as boot ROM 296 and \"legacy\" I/O devices (using a \"super I/O\" chip).  The \"legacy\" I/O devices (298) can include, for example, serial and parallel ports, keyboard,\nmouse, and/or a floppy disk controller.  The LPC bus also connects Southbridge 235 to Trusted Platform Module (TPM) 295.  Other components often included in Southbridge 235 include a Direct Memory Access (DMA) controller, a Programmable Interrupt\nController (PIC), and a storage device controller, which connects Southbridge 235 to nonvolatile storage device 285, such as a hard disk drive, using bus 284.\nExpressCard 255 is a slot that connects hot-pluggable devices to the information handling system.  ExpressCard 255 supports both PCI Express and USB connectivity as it connects to Southbridge 235 using both the Universal Serial Bus (USB) the PCI\nExpress bus.  Southbridge 235 includes USB Controller 240 that provides USB connectivity to devices that connect to the USB.  These devices include webcam (camera) 250, infrared (IR) receiver 248, keyboard and trackpad 244, and Bluetooth device 246,\nwhich provides for wireless personal area networks (PANs).  USB Controller 240 also provides USB connectivity to other miscellaneous USB connected devices 242, such as a mouse, removable nonvolatile storage device 245, modems, network cards, ISDN\nconnectors, fax, printers, USB hubs, and many other types of USB connected devices.  While removable nonvolatile storage device 245 is shown as a USB-connected device, removable nonvolatile storage device 245 could be connected using a different\ninterface, such as a Firewire interface, etcetera.\nWireless Local Area Network (LAN) device 275 connects to Southbridge 235 via the PCI or PCI Express bus 272.  LAN device 275 typically implements one of the IEEE .802.11 standards of over-the-air modulation techniques that all use the same\nprotocol to wireless communicate between information handling system 200 and another computer system or device.  Optical storage device 290 connects to Southbridge 235 using Serial ATA (SATA) bus 288.  Serial ATA adapters and devices communicate over a\nhigh-speed serial link.  The Serial ATA bus also connects Southbridge 235 to other forms of storage devices, such as hard disk drives.  Audio circuitry 260, such as a sound card, connects to Southbridge 235 via bus 258.  Audio circuitry 260 also provides\nfunctionality such as audio line-in and optical digital audio in port 262, optical digital output and headphone jack 264, internal speakers 266, and internal microphone 268.  Ethernet controller 270 connects to Southbridge 235 using a bus, such as the\nPCI or PCI Express bus.  Ethernet controller 270 connects information handling system 200 to a computer network, such as a Local Area Network (LAN), the Internet, and other public and private computer networks.\nWhile FIG. 2 shows one information handling system, an information handling system may take many forms, some of which are shown in FIG. 1.  For example, an information handling system may take the form of a desktop, server, portable, laptop,\nnotebook, or other form factor computer or data processing system.  In addition, an information handling system may take other form factors such as a personal digital assistant (PDA), a gaming device, ATM machine, a portable telephone device, a\ncommunication device or other devices that include a processor and memory.\nFIGS. 3 through 9 depict an approach that can be executed on an information handling system.  The information handling system analyzes training data against a structured resource and creates pattern sets that each include semantically similar\nparaphrases and corresponding syntactic patterns.  The information handling system also includes paraphrase scores in the pattern sets that are based on a set of first candidate answers obtained from querying the structured resource.  When the\ninformation handling system receives a user question, the information handling system generates a syntactic pattern of the question and compares the generated syntactic pattern to the syntactic patterns in the pattern sets.  When a match is found, the\ninformation handling system retrieves paraphrases from the pattern set containing the matched syntactic pattern.  The information handling system then generates new questions based on the retrieved paraphrases and utilizes the new questions to create a\nsecond query that queries a second set of data, such as an unstructured resource, which identifies a set of candidate answers.  In turn, the information handling system scores the set of second candidate answers based on the paraphrase scores\ncorresponding to the paraphrases utilized to identify the candidate answers.\nFIG. 3 is an exemplary diagram depicting a knowledge manager that groups semantically similar paraphrases into pattern sets and utilizes the semantically similar paraphrases to generate new questions for use in a traditional search engine.\nKnowledge manager 100 receives training data 310 from subject matter experts 300.  In one embodiment, training data 310 includes training question/answer pairs.  In another embodiment, training data 310 includes training sentences or statements\nthat encompass training questions and answers.  For each training pair, pattern set generator 320 generates a training syntactic pattern, a training paraphrase, and a training query.  In one embodiment, pattern set generator 320 uses knowledge base 106,\nwhich includes structured resource data, to generate the training queries.\nPattern set generator 320 compares a newly generated training query against previously generated training queries stored in pattern set store to determine whether a match exists.  If a match exists between a newly generated training query and a\npreviously generated training query, pattern set generator 320 stores, in pattern set store 330, a corresponding new training paraphrase and a new training syntactic pattern in a pattern set corresponding to the matched previously generated training\nquery.  Once pattern set generator 320 finishes evaluating training data 310, pattern set store 330 stores pattern sets that each include multiple syntactic patterns and corresponding semantically similar paraphrases.  For example, one of pattern set\nstore 330's pattern sets may include semantically similar paraphrases such as \"Y is the CEO of X\", \"Y leads X\", and \"Y is the boss of X\" (see FIG. 4 and corresponding text for further details).\nIn one embodiment, pattern set generator 320 assigns paraphrase scores to paraphrases, which indicate a relative accuracy of a corresponding training query to the training pair when the training query queries knowledge base 106.  For example, if\na training question is \"What countries border the United States?\" and the training query returned \"Canada\", the corresponding paraphrase may be assigned a score of 50%.\nA user question, such as question X 360, is provided through GUI 350.  Question X 360 may not be related to knowledge base 106.  Semantically similar question generator 340 creates a syntactic pattern from question X 360 and compares the\nsyntactic pattern against syntactic patterns stored in pattern store 330.  When semantically similar question generator 340 finds a match in a pattern set, semantically similar question generator 340 retrieves paraphrases from the matching pattern set\nand generates new questions 370.\nIn turn, semantically similar question generator 340 provides question X 360 and new questions 370 to traditional search engine 380.  Traditional search engine 380 generates queries based on the received questions and queries resource data in\nsource data 390, which corresponds to question X 360.  For example, if question X 360 is \"How fast does the Ford Mustang go,\" source data 390 may include unstructured data obtained from automotive articles.  In turn, traditional search engine 380\nprovides candidate answers 395 resulting from the query to the user through GUI 350.  In one embodiment, the candidate answers 395 may be scored based upon the paraphrase scores assigned to the paraphrases by pattern set generator 320 discussed above\n(see FIGS. 7 through 9 and corresponding text for further details).\nFIG. 4 is a diagram depicting a pattern set generator grouping semantically similar paraphrases and corresponding syntactic patterns into pattern sets.  Pattern set generator 320 receives training data that includes training question 400 and\ntraining answer 410.  Training pattern generator 420, in one embodiment, uses training question 400 to generate a focus phrase.  In this embodiment, training pattern generator 420 then uses the focus phrase to generate training syntactic pattern 430\nbased upon the syntactic relationships between training entities within the focus phrase.  Training pattern generator 420 also generates training paraphrase 440 based on training question 400.\nTraining query generator 450 identifies database paths in knowledge base 106 (e.g., a structured resource) that the training question 400 to training answer 410.  For example, training query generator 450 finds a database path between the top\nspeed of the F-14 Tomcat and Mach 2.3 in knowledge base 106.  In turn, training query generator 450 generates training query 460.\nIn one embodiment, training query generator 450 queries knowledge base 106 using training query 460 and compares the returned answers with training answer 410 to generate a training score.  For example, if a training question is \"What countries\nboarder the United States,\" the training answers include Canada and Mexico.  In this example, if a training query returned only Canada, the training score would not be as high as a training query that returned both Canada and Mexico.  Training query\ngenerator 450, in turn, produces training score 465, which are assigned to training paraphrase 440 by pattern mapper 470 (discussed below).\nPattern mapper 470 compares training query 460 with previously generated training queries in in pattern set store 330 to identify matches or correlations.  A match or correlation between training queries indicates semantic similarities between\nparaphrases within the pattern set.  When pattern mapper 470 detects a match or correlation, pattern mapper 470 stores training syntactic pattern 430, training paraphrase 440, and training score 465 in the pattern set corresponding to the matched\npreviously generated training query.\nHowever, when pattern mapper 470 does not detect a match or correlation, pattern mapper 470 creates a new pattern set and stores training syntactic pattern 430, training paraphrase 440, training score 465, and training query 460 in the new\npattern set (see FIG. 7 and corresponding text for further details).  FIG. 4 shows that pattern set store 330 includes two pattern sets A and B, each of which includes three syntactic patterns and corresponding paraphrases/scores.\nFIG. 5 is an exemplary diagram depicting a training pattern generator creating training syntactic patterns and paraphrases based upon training data.\nTraining data 330 includes three training pairs 500, 510, and 520.  From training data 330, training pattern generator 420 generates three sets of training syntactic patterns and corresponding paraphrases, which are included in entries 530, 540,\nand 550.  As such, when a user question is received whose syntactic pattern matches one of the syntactic patterns in entries 530, 540, or 550, semantically similar question generator 340 generates new questions based upon the paraphrases in entries 530,\n540, and 550.  For example, if a user question is \"How fast is the Ford Mustang?\", semantically similar question generator 340 replaces X with \"The Ford Mustang\" and generates new questions: \"What is the top speed of the Ford Mustang?,\" \"The Ford mustang\ngoes what?,\" and \"The Ford mustang can reach what?\" (see FIG. 6 and corresponding text for further details).\nFIG. 6 is an exemplary diagram depicting a semantically similar question generator matching a user question's syntactic pattern to a syntactic pattern in a pattern set and generating new questions based upon paraphrases included in the pattern\nset.\nQuestion pattern generator 600 receives question X 360 and generates syntactic pattern X 605.  Paraphrase mapper 610 compares pattern X 605 against syntactic patterns in pattern set store 330.  When paraphrase mapper 610 detects a match, such as\npattern X=pattern A1 shown in FIG. 6, paraphrase mapper 610 retrieves paraphrases from the matching pattern set.  As can be seen, paraphrase mapper 610 retrieves paraphrases A1, A2 and A3 because pattern X=pattern A1 which is in pattern set A.\nIn turn, paraphrase mapper 610 generates new questions 370 from paraphrases A1, A2 and A3 by replacing variables in the paraphrases with corresponding nouns, verbs, etc. in question X 360.  For example, if question X is \"How fast does the Ford\nMustang go\" and paraphrase A1 is \"What is the X of Y,\" paraphrase mapper generates the question of \"What is the top speed of the Ford Mustang?\"\nTraditional search engine 380 generates queries 620 based on question X 360 and new questions 370.  In turn, traditional search engine 380 scores the candidate answers from the queries, such as based on paraphrase scores previously assigned to\nthe paraphrases, and provides answers 395 to a user through GUI 350.\nFIG. 7 is an exemplary flowchart showing steps taken by a knowledge manager to generate pattern sets from training data.  FIG. 7 processing commences at 700 whereupon, at step 710, the process selects a first training pair, training sentence, or\ntraining statement.  At step 720, the process generates a training syntactic pattern and a training paraphrase from a training question included in the training pair.\nAt step 730, the process generates a training query based upon the training pair when evaluated against knowledge base 106 as discussed previously.  In one embodiment, the process assigns a paraphrase score to the training paraphrase based upon\ncomparing training answers received from the training query against training answers in the training pair.\nAt step 740, the process compares the generated training query against previously generated training queries in pattern set store 330.  The process determines as to whether the generated training query matches (or correlates) to one of the\npreviously generated training query (decision 750).  If the generated training query does not match one of the previously generated training queries, then decision 750 branches to the `no` branch.  At step 760, the process creates a new pattern set and\nadds the training syntactic pattern, paraphrase score, training paraphrase, and training query to the new pattern set.\nOn the other hand, if the generated training query matches one of the previously generated training queries, then decision 750 branches to the `yes` branch.  At step 770, the process adds the training syntactic pattern, paraphrase score, and\ntraining paraphrase to the pattern set that includes the matched training query.\nThe process determines as to whether to continue (decision 780).  If the process should continue, then decision 780 branches to the `yes` branch which loops back to select and process the next training pair.  This looping continues until each\ntraining pair has been processed, at which point decision 780 branches to the `no` branch exiting the loop.  FIG. 7 processing thereafter ends at 795.\nFIG. 8 is an exemplary flowchart showing steps taken by a knowledge manager to match a question's syntactic pattern to semantic patterns in a pattern set and generate new questions based upon paraphrases found in a pattern set corresponding to a\nmatched syntactic pattern.\nFIG. 8 processing commences at 800 whereupon, at step 810, the process receives a question and at step 820, generates a question semantic pattern from the received question.  At step 825, the process compares the question semantic pattern\nagainst pattern set semantic patterns found in pattern set store 330.\nThe process determines as to whether a match (or correlation) exists between the question semantic pattern and semantic patterns found in pattern store 330 (decision 830).  If a match is not found, then decision 830 branches to the `no` branch\nand the process sends the question to a traditional search engine to process.  FIG. 8 processing thereafter ends at 850.\nOn the other hand, if a match is found between the question semantic pattern and one of the semantic patterns stored in pattern set store 330, then decision 830 branches to the `yes` branch.  At step 860, the process retrieves paraphrases from\nthe pattern set that includes the matched semantic pattern.  At step 870, the process generates new questions based on the retrieved paraphrases and the original question as discussed previously.\nAt step 875, the process provides new questions and the original user question to a traditional search engine to perform a query.  At predefined process 880, the process generates new questions from the paraphrases and queries a resource on\nsource data 390 using the newly generated questions.  (see FIG. 8 and corresponding text for processing details).  Source data 390 may include, for example, unstructured data that corresponds to the original question.  At step 890, the process provides\nscored answers to a user and FIG. 8 processing thereafter ends at 895.\nFIG. 9 is an exemplary flowchart showing steps taken to generate new questions from paraphrases and perform a query on a resource.  FIG. 9 processing commences at 900 whereupon, at step 910, the process generates queries based upon the original\nuser question and the new questions generated as discussed earlier.  At step 920, the process queries source data 390 using the generated queries and identifies document passages that are returned by the query.\nAt step 930, the process selects the first matching document passage and, at step 940, the process scores the matched document passage based upon the matching paraphrase's paraphrase score, which was generated at step 730 during the training\nstage.\nThe process determines as to whether there are more matching document passages to evaluate (decision 950).  If there are more matching document passages to evaluate, then decision 950 branches to the `yes` branch which loops back to select the\nnext document passage.  This looping continues until there are no more document passages to evaluate, at which point decision 950 branches to the `no` branch exiting the loop.  This looping continues until there are no more document passages to evaluate,\nat which point decision 950 branches to the `no` branch exiting the loop.  FIG. 9 processing thereafter ends at 960.\nWhile particular embodiments of the present disclosure have been shown and described, it will be obvious to those skilled in the art that, based upon the teachings herein, that changes and modifications may be made without departing from this\ndisclosure and its broader aspects.  Therefore, the appended claims are to encompass within their scope all such changes and modifications as are within the true spirit and scope of this disclosure.  Furthermore, it is to be understood that the\ndisclosure is solely defined by the appended claims.  It will be understood by those with skill in the art that if a specific number of an introduced claim element is intended, such intent will be explicitly recited in the claim, and in the absence of\nsuch recitation no such limitation is present.  For non-limiting example, as an aid to understanding, the following appended claims contain usage of the introductory phrases \"at least one\" and \"one or more\" to introduce claim elements.  However, the use\nof such phrases should not be construed to imply that the introduction of a claim element by the indefinite articles \"a\" or \"an\" limits any particular claim containing such introduced claim element to disclosures containing only one such element, even\nwhen the same claim includes the introductory phrases \"one or more\" or \"at least one\" and indefinite articles such as \"a\" or \"an\"; the same holds true for the use in the claims of definite articles.", "application_number": "14886023", "abstract": " An approach is provided in which a knowledge manager creates a pattern\n     set that includes paraphrases and corresponding paraphrase scores. The\n     paraphrase scores are based on a set of first candidate answers obtained\n     from querying a first set of resource data. The knowledge manager\n     performs a search, which is based on the paraphrases and a user question,\n     on a second set of resource data and identifies a set of second candidate\n     answers. In turn, the knowledge manager scores the set of second\n     candidate answers based on the paraphrase scores corresponding to the\n     paraphrases utilized to identify the set of second candidate answers.\n", "citations": ["7519529", "7546235", "8271453", "8346792", "8554540", "9613025", "20090119090", "20120078888", "20130226846", "20150331850"], "related": []}, {"id": "20170237729", "patent_code": "10375054", "patent_name": "Securing user-accessed applications in a distributed computing environment", "year": "2019", "inventor_and_country_data": " Inventors: \nUppalapati; Raju (San Jose, CA)  ", "description": "<BR><BR>TECHNICAL FIELD\nThe present disclosure relates generally to the management of access permissions associated with applications accessible in a networked environment.  Specifically, the disclosure relates to the management of access permissions relating to user\naccess to applications in a distributed computing environment.\n<BR><BR>BACKGROUND\nIncreasingly, information is stored and accessed via remote computing devices.  Local computing devices, such as a client device, can be used to access and interact with remote computing devices to retrieve information such as the documents,\nvideos, images, applications, and others.  These remote computing devices may also interact with local computing devices to deploy applications coded on the local computing devices into clusters of remote computing devices, such as a distributed\ncomputing environment, commonly referred to as \"a cloud\" or \"the cloud.\"\nSome of the information that is remotely accessed is information that is sensitive in some regard.  For example, the information may include sensitive personal or financial information or may include information protected by copyright or other\nlegal structures.  Authorizing one set of users to access one set of information while denying access to another set or other sets of information is an area in which progress has been made over the years.\nHowever, the increasing reliance and utilization of distributed computing environments has complicated the provisioning of access to information, resources, and functions to those entities that should be authorized to access that information,\nthose resources, and those functions.  Accordingly, current approaches to providing access permissions within distributed computing environments have not been entirely satisfactory. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nFIG. 1 is a block diagram of access management system including an access management server that may grant, revoke, and reinstate access, according to some aspects of the present disclosure.\nFIG. 2 is a block diagram of a computing device that may be employed as the access management server of FIG. 1, according to some aspects of the present disclosure.\nFIGS. 3, 4, 5, 6, and 7 are exemplary user interfaces that may be provided by the access management server of FIG. 1, according to some aspects of the present disclosure.\nFIG. 8 is a flowchart illustrating a method of managing access permissions in a distributed computing environment, according to some aspects of the present disclosure.\nFIG. 9 is a flowchart illustrating a method of managing access permissions in a distributed computing environment, according to some aspects of the present disclosure.\nFIG. 10 is a block diagram of distributed computing environment including an authorization control system to oversee user access to a plurality of remote resources, according to some aspects of the present disclosure.\nFIG. 11 is a block diagram of a computing device that may be employed in a variety of capacities in the computing environment of FIG. 10, according to some aspects of the present disclosure.\nFIG. 12 is an exemplary user device having an interface, according to some aspects of the present disclosure.\nFIG. 13 is another block diagram showing aspects of the distributed computing environment of FIG. 10, according to some aspects of the present disclosure.\nFIG. 14 is a flowchart illustrating a method of securing applications in a distributed computing environment, according to some aspects of the present disclosure.\nFIG. 15 is a flowchart illustrating another method of securing applications in a distributed computing environment, according to some aspects of the present disclosure.\nThese drawings will be better understood by those of ordinary skill in the art by reference to the following detailed description.\n<BR><BR>DETAILED DESCRIPTION\nWith references to the drawings briefly described above, exemplary applications of systems and methods according to the present disclosure are described in this section.  These examples are provided to add context and aid in the understanding of\nthe invention.  It will thus be apparent to one skilled in the art that the present invention may be practiced without some or all of these specific details.  In other instances, some well-known process steps have not been described in specific detail in\norder to avoid unnecessarily obscuring the present disclosure.  Additionally, other applications of the concepts and principles described herein are possible, such that the following examples should not be taken as limiting.  For example, while many of\nthe examples disclosed herein are directed to the detection and management of API access in the context of a streaming video service, the principles and concepts described may be applied to provide more generally for the revocation, reinstatement, and\nrequest of API access for a plurality of applications deployed at scale.\nIn the following detailed description, references are made to the accompanying drawings, which form a part of the description and in which are shown, by way of illustration, some specific embodiments of the present disclosure.  Although these\nembodiments are described in sufficient detail to enable one skilled in the art to practice the invention, it is understood that these embodiments are not limiting, such that other embodiments may be used, and changes may be made without departing from\nthe spirit and scope of the invention.  For example, changes within the scope of this disclosure may include performing an operation on a different system or device, etc.\nDevices, systems, and methods are provided for managing access permissions in a distributed computing environment.  Managing access permissions may include granting access, revoking access, and reinstating access after revocation.  The access\npermissions may provide or deny access to a plurality of objects accessible within or from the distributed computing environment.  Access to an object may include access to make calls on specific application programming interfaces (APIs) presented or\nexposed by distributed computing infrastructure.\nAs a non-limiting example, reference may be made here in to Amazon Web Services (AWS) as an example of a distributed computing infrastructure or cloud computing infrastructure.  The distributed computing infrastructure may provide resources for\ncomputing as well as for storage, networking, and other functions or services provided by collections of remote computing devices.  For example, the distributed computing infrastructure may include scalable storage, virtual servers and virtual networks,\ndatabases, analytics services, application services (e.g., email, push notification, workflow services) and application deployment and management services.\nAs part of the application deployment and management services, the distributed computing infrastructure may provide user activity logging.  This activity logging may include a log of requests made by resources or resource instances associated\nwith one or more accounts of a consumer or customer of the distributed computing infrastructure.  For example, AWS includes a service referred to as CLOUDTRAIL.TM..  CLOUDTRAIL.TM.  is an example of a distributed computing service that records API calls\nwithin the AWS environment and delivers a log to a device operated by the consumer.  Comparable API call logs may be provided in other distributed computing infrastructure embodiments.  The log may include information identifying the caller of the API,\nthe time of the call, the source IP address of the caller, the parameters included in the call, and response elements such as error messages resulting from the call.  In some embodiments, a region of the distributed computing infrastructure in which the\nAPI call originated may also be specified in the log.  In general the application deployment and management services may provide such information to and access management service operated by the consumer of distributed computing services.\nA consumer of distributed computing services may have information regarding one or more applications deployed to the distributed computing infrastructure.  Such information may include an access policy associated with the one or more\napplications.  The access policy of a given application may indicate the permissions associated with that application.  For example, an access policy may indicate or list APIs that an application is permitted to call within the distributed computing\ninfrastructure.  In general, the access management server may be provided with a set of permissions, e.g. an access policy, associated with any entity existing within the purview of the access management system.  Such entities may include users having\nuser accounts as well as individual applications.  In some embodiments, each individual application may be handled in the system as if it were a user, having its own individual user account.\nFor example, AWS includes a service to provide AWS identity and access management (IAM).  IAM provides services to manage users, which may be referred to as IAM users, as well as roles, referred to as IAM roles.  IAM users or individual\napplications may be assigned an IAM role.  The application may be provided by another service of the distributed computing infrastructure, such as another AWS service such as an Amazon Elastic Compute Cloud (EC2) instance or an Amazon Simple Storage\nService (S3) instances, for example.\nWhen an application deployed within the distributed computing infrastructure makes an API call in order to access another service or application operating within the distributed computing infrastructure, the request for access may be logged as\nwell as responded to by the service or application at issue.  When an error is detected, such as an error indicating that one or more parameters associated with the call is improper or an error indicating that access is denied, this error may also be\nlogged in association with the request and with the associated application.\nIn order to provide for secure operating environment, it may be desirable that an application be provided with the access to other applications and services that it needs and be denied access to other applications and services that it does not\nneed to serve its function.  For example, an application may, at least temporarily, be provided with access to an API used in obtaining sensitive financial information, such as a credit card number, of a user of services provided by the customer of the\ndistributed computing infrastructure via the distributed computing infrastructure.  The access may be revoked if it is determined not to be necessary.  In a first state, a new application may be provided with an access policy that includes access to more\nobjects or other APIs then the application may be determined to need in the future.  For example, the new application may have a scheduled rollout that includes additional services that are not operational upon initial deployment of the application. \nHowever, it is anticipated that the additional services will, in the future, rely on specific permissions, such as permissions to call a specific API.  Given this situation, the new application may be deployed with an overly permissive access policy in\nview of the scheduled rollout.\nThe access management system may retrieve information from the request log indicating what information, files, or functions have been requested by the new application during a predetermined time or period of observation, such as two weeks or a\nmonth.  If the access management system determines that the number of requests to access the information, files, or functions is below a threshold value or threshold number, the access management system may direct that the access policy associated with\nthe new application be modified to remove access to the unused or underused information, files, or functions.  In this way, even though the new application may be deployed with an overly permissive access policy, the access policy may be curtailed or\nmodified to remove certain access.  For example, if an application makes no calls to an API or makes fewer calls to an API than a predetermined threshold number of calls, the access policy associated with that application may be modified to remove access\nto the API.\nAs noted, when the first application is associated with a scheduled rollout of features or services, the features or services associated with the API to which access was removed may become operational after the removal of necessary access.  The\naccess management system may provide a user, such as a developer or any developer from a team of developers, with the ability to request reinstatement of access to the API that was removed from the access policy.  For example, after one month of\ninsufficient use of an API by a first application, the access management system may revoke or remove access to that API by the first application.  One week later, the scheduled features that are to utilize the API may be ready to be deployed.  A\ndeveloper associated with the first application may request reinstatement of access to the API.  The access management system may recognize that the original access policy included access to the API and may recognize that the access was revoked only one\nweek before.  The access management may determine from this information and/or other information that the access to the API should be restored or reinstated.  The access management system may automatically restore access to the API, without the\nintervention of any administrative security personnel.\nAdditionally, when the request logs indicate that an error is resulting frequently from a call made by a specific application, the access management system may revoke access to that call from an access policy associated with a specific\napplication.  The access management system may notify administrative security personnel and/or a developer or developer team associated with the specific application.  This notification, which may be an email, a short message service (SMS) message, a\npush notification, or other such communication, may encourage corrective measures, such as a repair to the code or a replacement of the code, or another appropriate corrective measure.\nIn some embodiments of distributed computing infrastructure, in the event that the application continues to make calls resulting in errors, including access denied errors, the distributed computing infrastructure may throttle calls associated\nwith the application.  In some distributed computing infrastructures, such as AWS, other calls associated with the AWS account may also be throttled or rate limited even if those calls are not resulting in errors.  Accordingly, the prevention of\nerroneous API calls on the part of a single application may improve performance, or prevent a degrading in performance, of all systems and services being provided in connection with a specific AWS account or other distributed computing infrastructure\naccount.\nThe access management system may provide one or more user interfaces by which users such as developers and users such as administrative security personnel may access information associated with modifications to access policies associated with\nspecific applications.  For example, a user interface may provide a developer with interface elements that may be manipulated to request reinstatement of a revoked permission to call a specific API.  Depending on the conditions associated with the\nrequest, the access management system may automatically reinstate access by automatically modifying the access policy associated with the specific application, or the access management system may send a communication to administrative security personnel\nrequesting that the administrative security personnel determine whether or not access should be reinstated.\nOther examples describing the components and the operations of access management system are provided herein.  Combinations of these components and operations are within the scope of the present disclosure, including combinations specifically\ndescribed and combinations that would be apparent to one of ordinary skill in the art based upon a careful reading of the present disclosure.  Embodiments of the present disclosure may permit for automatic modification of access policies associated with\napplications deployed in a distributed computing infrastructure or environment.\nReferring now to FIG. 1, shown therein is a block diagram of an access management system 100 that includes an access management server 102.  As described herein, embodiments of the access management server 102 may include a processing device in\ncommunication with one or more storage systems or devices, which may store instructions for an access management application 104.  The access management server 102 may be configured to receive information from a plurality of networked resources and,\nbased on that information, modify an access policy associated with one or more applications deployed within a distributed computing environment.  For example, the access management server 102 may be operated by Netflix, Inc.  of Los Gatos, Calif., which\nmay also operate systems for the creation of user accounts and for the streaming or other consumption of media content, such as movies, television shows, and other video-based and/or audio-based programming.\nThe access management server 102 is configured in communication with a database system 106 that may include information regarding entities existing within the purview of the access management server 102 and information regarding the requests for\naccess to objects by those entities.  Additionally, the database system 106 may include information describing access policies associated with those entities.  As described herein, an entity may be a human user having a user account or a nonhuman user\nsuch as an application or service.  The objects for which access is requested may be files, applications, services, APIs, or other digital objects present in the environment of the access management system 100.  The database system 106 may include\ninformation obtained from other networked devices or systems included within the overall access management system 100.\nFor example, the access management server 102 may communicate over a network 110 with a distributed computing infrastructure 120, with an administration system 130, and with one or more client devices such as client devices 140A and 140B.  As\ndescribed, data communications between the access management server 102 and other components and devices illustrated in FIG. 1, may be transmitted over the network 110, which may include one or more networks such as a LAN, a WAN, a WWAN, a WLAN, a mobile\ntelephone network, a landline telephone network, as well as other suitable networks.  The network 110 may include a plurality of networks coupled together.  As shown in FIG. 1, network 110 may include a private network, such that communication between\nthe access management server 102 and the administration system 130 may not pass over publicly available communication lines or may be suitably encrypted for transit over the publicly available communication lines, while communication between the access\nmanagement server 102 and the distributed computing infrastructure 120 may pass over publicly available communication lines.  Accordingly, the network 110 may include both a private network, including a virtual private network (VPN), and a public\nnetwork.\nAs noted, FIG. 1 illustrates that the access management server 102 communicates with the distributed computing infrastructure 120 over network 110.  The distributed computing infrastructure 120 may include a plurality of servers and storage\ndevices configured to provide a plurality of resource instances 122A, 122B, 122C, and 122D.  A commercially available example of an appropriate distributed computing infrastructure 120 may be or be similar to the distributed computing infrastructure\nprovided by Amazon Web Services, Inc.  (AWS).  In an embodiment in which the distributed computing infrastructure 120 is AWS, the resource instances 122A-D include a plurality of Amazon EC2 instances, a plurality of database instances such as an Amazon\nRDS instance, and storage instances such as an Amazon S3 bucket, and each of instances 122A-D may be provided by a plurality of computing and/or storage devices.  The operator of the access management server 102 may deploy a plurality of applications to\nthe distributed computing infrastructure 120 such that the applications are executed on hardware owned by a third-party operator, such as AWS.\nAs illustrated in FIG. 1, the distributed computing infrastructure 120 includes a request log 124 which may be a log of access requests made to or within the distributed computing infrastructure 120.  For example, a first application executing\non resource instance 122A may request to access information stored on resource instance 122D.  In some embodiments, the resource instance 122A may communicate with the resource instance 122D over a virtual network 126 provided by the distributed\ncomputing infrastructure 120.  In order to request to access information stored on the resource instance 122D, the resource instance 122A may call a specific API.  When the resource instance 122A calls the specific API, a monitoring service 128 operating\nwithin the distributed computing infrastructure 120 may detect the call and record associated information in the request log 124.  The monitoring service 128 may provide application deployment and management services within the distributed computing\ninfrastructure 120.  The request log 124 may include text information and/or numerical information describing each API call associated with the account.  The information associated with a particular API call may include an identifier of the user or role\n(or application having been assigned a role) that called the API, the API called, the time of the call, the source IP address of the caller, parameters included in the API call, and any responses returned including error messages such as an access denied\nerror.\nThe information in the request log 124 may be obtained upon request by the access management server 102.  In some embodiments, the access management server 102 may be configured to automatically duplicate the request log 124 in the database\nsystem 106.  The monitoring service 128 may be configured to update the request log 124 according to a schedule, e.g. every 15 minutes, every half hour, or every hour.  The access management server 102 may access and duplicate the contents of the request\nlog 124 on the same schedule or on a different schedule.\nThe administration system 130, which may be operated by the operator of the access management server 102, includes a security application 132 executing thereon.  The security application 132 includes a plurality of access policies, illustrated\nas access policies 134A and 134B.  Each of the access policies 134A and 134B may be associated with a specific entity, such as a specific user or a specific application.  For example, the access policy 134A may be associated with a first application\ndeployed within the distributed computing infrastructure 120.  The access policy may indicate a list of objects that the application may access.  For example, the access policy 134A may list a plurality of APIs that the first application has permission\nto call.  In some embodiments, the access policies 134A and 134B may describe levels of access or categories of access that can be translated by an intermediary to determine whether the associated user or application should be provided with access to a\nspecific object.  The security application 132 may communicate with the distributed computing infrastructure 120 to determine access policies present within the distributed computing infrastructure 120 and may copy those access policies to provide the\naccess policies 134A and 134B.  In some embodiments, a copy of the access policies 134A and 134B is included in the database system 106 to be more readily accessible to the access management server 102 and the access management application 104 running\nthereon.  The security application 132 may operate continuously and collect a log of access information to generate the access policies 134A and 134B.\nThe access management server 102 may access information, which may be stored in the request log 124, the access policies 134A and 134B, and/or in the database system 106 and determine that access to a first object by a first entity should be\nremoved.  This determination may be based on usage, such as a count of times that the first entity accesses the first object during a predetermined time or a period of observation.  When the count is less than a threshold value, the access management\nserver 102 may cause a modification to the access policy associated with the first entity that removes or curtails access to the first object.  For example, the access management server 102 may determine that a first application has not accessed a first\nAPI during a period of observation.  The access management server 102 may cause the access policy associated with the first application to be modified to omit or remove access to call the first API.  Alternatively, the access management server 102 may\ndetermine that the first application called the first API fewer than 30 or 40 times (or another threshold value) during the period of observation.  In some embodiments, the access management server 102 uses a threshold value of zero, such that it\ndetermined whether the first application has called the first API or not during the period of observation.\nThe period of observation may be used as an alternative or as a second thresholding variable.  If the first application calls the first API at all during the period of observation, then the access management server 102 may not modify the access\npolicy of the first application.  Accordingly, the period of observation may be adjusted to a longer or a shorter time as desired.  For example, the period of observation may be set to a day, a week, or a month.  Additionally, the period of observation\nmay be dependent on the particular API itself.  During normal operation of services, a first API may be called more than a second API.  In some embodiments, the access management server 102 may include a table of periods of observation, with a desired\nperiod of observation associated with each API.  The access management server 102 may modify the access policy associated with the first application.  In some embodiments, the access management server 102 may communicate with the security application 132\nto modify the access policy associated with the first application.\nIn some embodiments, the determination that access by the first application to the first API should be removed includes determining from the request log that more than a threshold number of errors are being produced when the application calls\nthe first API.  For example, the access management server 102 may modify an access policy associated with the first application when the first application produces more than 5 or 10 errors during a period of observation in calls to the first API. \nDepending on the particular API, the threshold number of errors may be more than 100 or more than 1000 during the period of observation.  The request log 124 may indicate in an error message the type of error being triggered by the call to the first API. The threshold value applied to determine whether or not access to the first API by the first application should be removed may depend on the type of error resulting from the API call.  For example, when the request log 124 indicates that the first\napplication provoked a \"denied access\" type error when calling the first API, the threshold value applied may be a lower threshold value, such as 5 to 10 calls.  The access policy associated with the first application may be modified to prevent the first\napplication from calling the first API and thereby triggering more errors.  Additionally, the threshold value may depend on the API being called.  For example, the distributed computing infrastructure 120 may allow for certain APIs to be called thousands\nof times a minute, while other APIs may only be called once per minute.  The errors permitted for each API before throttling occurs may depend on the particular API.  When a more than a threshold number of errors are triggered or more than a threshold\nnumber of allowable calls are made, the distributed computing infrastructure 120 may begin throttling responses to subsequent calls.  The throttling may be applied to the first application as well as to other applications deployed within the distributed\ncomputing infrastructure 120.  Accordingly, in some embodiments performance of all of the services associated with an account in the distributed computing infrastructure 120 may be improved by limiting API access of a single service or single application\nassociated with the account.\nEach application may be assigned a role or identity within the distributed computing infrastructure 120.  For example, when the distributed computing infrastructure 120 is an AWS infrastructure, the applications may each be assigned to or\ndeployed as an JAM role.  Each IAM role contains an IAM policy, a document that explicitly defines permissions associated with the IAM role.  To revoke an application's permission to call a particular API, the access management server 102 may communicate\nwith the distributed computing infrastructure 120 to modify the IAM policy associated with the application, for example, by editing text of the IAM policy.\nThe access management server 102 may schedule modification of an access policy after the determination that the access policy should be modified.  For example, the access management server may schedule modification of an access policy associated\nwith a first application for one day or one week after either the application calls a specific API more than the threshold number of times during the period of observation for that API or the application's calls to the specific API result in more than a\nthreshold number of errors during a period of observation.  In some embodiments, the access management server 102 may provide administrative security personnel with a user interface in which to set a default scheduling of modifications.\nWhen the access management server 102 determines that access to a first object by a first entity should be revoked or removed, the access management server 102 may produce and transmit a communication to administrative security personnel.  The\ncommunication or a similar communication may also be transmitted to the first entity, which may be a first user or one or more users associated with a first application.  When the entity is an application deployed within the distributed computing\ninfrastructure 120, the access management server 102 may send a communication to the responsible for the application, who may be a lead developer.  In some embodiments, the access management server 102 may send the communication to every developer\nassociated with the application.  By providing notification of a scheduled change in access, before the change is to occur, a developer or administrative security personal may have the opportunity to review the scheduled change and modify, cancel, or\nrequest cancellation of the scheduled change.\nThe communication generated and transmitted by the access management server 102 may include an email, an SMS message, push notification, or any other suitable type of electronic communication that may be conveyed from one computing device to\nanother computing device.  The communication may be received on one of the client devices 140A and 140B.  The client devices 140A and 140B shown in FIG. 1 may be computing devices such as personal computers, laptops, mobile-computing devices, such as\ntablet computers or smartphones, wearable computing devices, and/or any other computing devices having computing and/or communications capabilities in accordance with the described embodiments.  The client devices 140A and 140B each include a processing\ndevice in communication with a data storage device or memory and are able to execute instructions corresponding to system programs and application programs to perform various computing and/or communications operations.  Exemplary system programs may\ninclude, without limitation, an operating system (e.g., iOS.RTM., Android.RTM.  OS, LINUX.RTM.  OS, Firefox OS.TM., Windows.RTM., OS X.RTM., Binary Run-time Environment for Wireless (BREW) OS, JavaOS, a Wireless Application Protocol (WAP) OS, and\nothers), device drivers, programming tools, utility programs, software libraries, (APIs), and so forth.  As shown in FIG. 1, the client device 140A and 140B each execute software to provide an interface 142A and 142B, respectively.  In some embodiments,\nthe interfaces 142A and 142B may be provided in connection with a web-browsing program such as Internet Explorer.RTM., Chrome.RTM., etc. The client devices 140A and 140B may communicate with access management server 102, the administration system 130 and\nthe distributed computing infrastructure 120 over the network 110.\nIn some embodiments, the client device 140A may be a client device used by administrative security personnel while the client device 140B is a client device used by a developer.  The administrative security personnel and the developer may be\nrequired to log into or authenticate the client devices 140A and 140B to the access management server 102 to interact with the access management server 102 and the application 104 executing thereon.  Exemplary interfaces that may be employed or provided\nby the access management server 102 in connection with the client devices 140A and 140B are described herein with respect to the exemplary interfaces in FIGS. 3-7.\nIn the illustrated embodiment of the access management system 100, the component systems such as the access management server 102, the administration system 130, and at least some of the hardware underlying the distributed computing\ninfrastructure 120 may be provided by a server or cluster of servers.  For example, the access management server 102 may be a computing device that comprises or implements one or more servers and/or software components that operate to perform various\noperations in accordance with the described embodiments.  Exemplary servers may include, for example, stand-alone and enterprise-class servers operating a server operating system (OS) such as a MICROSOFT.RTM.  OS, a UNIX.RTM.  OS, a LINUX.RTM.  OS, or\nanother suitable server-based operating system.  It should also be appreciated that the server 102 illustrated in FIG. 1 may be deployed in other ways and that the operations performed and/or the services provided by such server or servers may be\ncombined or separated for a given implementation and may be performed by a greater number or fewer number of individual server devices.\nFurther, although the access management server 102 and the administration system 130 are depicted in FIG. 1 as being separate from the distributed computing infrastructure 120, some embodiments of the present disclosure may include the access\nmanagement server 102 and/or the administration system 130 as being provided by one of the resources 122A-D of the distributed computing infrastructure 120.  For example, in embodiments in which the distributed computing infrastructure 120 is AWS, the\naccess management server 102 may be provided by an Amazon EC2 instance executing an application defining the services and functions of the access management server 102.\nReferring now to FIG. 2, shown therein is an exemplary embodiment of a computing device 200, embodiments of which may provide the access management server 102 of FIG. 1 as described herein or the client devices 140A and/or 140B.  The computing\ndevice 200 includes a processing device 202, such as one or more processors or CPUs, in communication with a data storage device or memory 204 over a bus 206.  The bus 206 further couples to a network interface device 208 and an I/O device interface 210. The network interface device 208 may be a network interface card or network interface controller (NIC) that permits the computing device 200 to communicate over the network 110 of FIG. 1.  The I/O device interface 210 enables the computing device 200 to\ncommunicate information to be rendered to a display 212 to display information such as data and command options in one or more graphical user interfaces associated therewith.  The I/O device interface 210 may further communicate with I/O devices such as\ninfra-red or radio-frequency remote controls, keyboards, mice, touchscreens, etc. in some embodiments, the I/O device interface 210 may be an application programming interface (API) by which the other systems of FIG. 1 may communicate with the computing\ndevice 200.\nThe processing device 202 may execute software and/or other instructions 203 stored in the memory 204.  The software and information stored in the memory 204 and the instructions 203 may be different depending on whether the computing device 200\nis configured as the access management server 102, the administration system 130, one of the client devices 140A and 140B, or as a server underlying the distributed computing infrastructure 120.  Additionally, the computing device 200 may be a virtual\ncomputing device in some embodiments, such as a configured resource instance in the distributed computing infrastructure 120.\nThe access management application 104 of FIG. 1 may be provided by executing instructions 203 on the processing device 202.  As illustrated in FIG. 2, the memory 204 stores an access management application 220 and a browser 222.  The access\nmanagement application 220 may include a plurality of rules and/or algorithms that may be implemented as part of the access management application 220 to determine when access to a particular object should be revoked or removed from an access policy\nassociated with a particular entity.  For example, the access management application 220 may be configured to determine that a first application should no longer have access to a first API.  In embodiments of the computing device 200 that may provide for\nthe client device 140A or the client device 140B, the access management application 220 may be a client application configured to communicate with the access management server 102.  Other embodiments of the computing device 200 that may provide for the\nclient device 140A or 140B to include the browser 222 to communicate with the access management application 220 of an embodiment of the computing device 200 configured to provide the access management server 102.\nThe memory 204 may be a collection of memories of different types that are included in a housing of the client computing device or coupled to the computing device 200 and in communication therewith.  For example, the memory 204 may include cache\nmemory, RAM, ROM, a solid-state hard drive, a disk-based hard drive, and/or other types of non-transitory memory devices.  The components depicted as stored on the memory 204 may be stored on and/or accessed from any combination of these different types\nof memories.\nAs illustrated in FIG. 2, the memory 204 includes a set of user account information 226, which may include information identifying entities having access permission, including an associated access policy, within the environment of the access\nmanagement system 100.  For example, the user account information 226 may include an account associated with a first application, an account associated with a developer or a developer team associated with the first application, an account associated with\nadministrative security personnel.  When the account included in the user account information 226 is associated with an application, the user account information 226 may include contact information for a developer or developer team that is associated\nwith the application.  In this way, when the access management server 102 transmits a communication associated with the application the communication may be directed to the developer or developer team.\nReferring now to FIGS. 3, 4, 5, 6, and 7 shown therein are exemplary user interfaces that may be provided by the access management server 102 described herein in connection to FIGS. 1 and 2.  The exemplary user interfaces may facilitate\ncommunication and interaction between users of the access management server 102, such as a developer or developer team and administrative security personnel of an organization, and the access management server 102 itself.  The exemplary user interfaces\nmay facilitate communication between the access management server 102 and the users thereof.  For example, a user may log into the access management server 102 and be presented with one of the exemplary user interfaces to receive a notification or alert\nof a modification made to an access policy or a modification scheduled to be made to access policy.  Additionally, the user interfaces may facilitate the handling of requests to grant access to one or more objects, such as APIs, requests to reinstate\naccess to one or more objects, the access to which has been previously removed, and other requests.  In some embodiments, the exemplary user interfaces may facilitate responses to such requests.  For example, administrative security personnel may respond\nto requests for access to an API for an application by a developer of the application.\nFIG. 3 illustrates an exemplary user interface 300 that may be presented to a developer after logging into the access management application 104 executing on the access management server 102.  The exemplary user interface 300 may be displayed in\na window 302 rendered to a display 304.  The display 304 may be part of or coupled to the client device 140A to provide the user interface 300 as the interface 142A of FIG. 1.  The exemplary user interface 300 may provide an overview tab 310 that\nincludes a plurality of fields each providing an overview of a specific type of information to the user.  The illustrated embodiment of the user interface 300 includes an overview table 312 that presents information to the user regarding one or more\napplications that are monitored by the access management server 102 to be presented to the user.  The illustrated user interface 300 further includes an upcoming changes list 320 and a recent changes list 330.\nThe overview table 312 may include a row for each application being monitored by the access management server 102.  As illustrated, the overview table 312 includes columns associated with an identifier of each application, an account in the\ndistributed computing infrastructure 120 associated with the application, a type of application with respect to the distributed computing infrastructure 120, and a name of the application.  The overview table 312 further includes counts of API calls, API\nerrors, and access denied errors of each application.  The overview table 312 further includes an indication of the date at which the information for each application was last updated.  The overview table 312 may be sorted according to any of these\ncolumns.  Additionally, a search field 314 may permit a user to search through the many applications listed in the overview table 312.  In some embodiments, the user may be able to select an application in order to be presented with additional\ninformation specific to that application.\nThe upcoming changes list 320 may provide information to the user regarding scheduled modifications associated with a particular entity, such as the user or an application with which the user is associated.  The upcoming changes list 320 may\nfurther indicate when each scheduled modification is scheduled to occur.  By selecting on an application, a pop-up or other user interface element may provide information regarding the scheduled modification.  The upcoming changes list 320 may present an\naction field 322 including a plurality of buttons or other user interfaces to permit the user to request that a scheduled modification be stopped or that permit the user to comment on a scheduled modification.  A selection of the comment user interface\nelement may cause the comment field to be presented to the user.  The user may submit a comment in the comment field, which may then be relayed by the access management server 102 to administrative security personnel.\nThe recent changes list 330 may present information describing modifications recently enacted by the access management server 102.  For example, when the access management server 102 removes access to an API by a first application, the first\napplication may be identified along with the API and a date on which the modification occurred.  The recent changes list 330 may include an action field 332 that includes a plurality of user interface elements whereby the user may request reinstatement\nof a removed access permission, such as reinstatement of access to the API.\nReferring now to FIG. 4, shown therein is a user interface 400 that is configured to provide information with respect to an application or a user.  The user interface 400 may be used to view the current policies associated with the entity, view\nrequest log data associated with the entity, and request modifications to the policies associated with the entity for example.  For example, the user interface 400 may be used by a user to request reinstatement of a previously removed access permission,\nor the user interface 400 may be used to request a new access permission.\nAs illustrated in FIG. 4, a current policies tab 412 is selected and includes a current policy field 414.  The current policy field 414 includes a list of each of the policies associated with the application named \"DiscoveryRole.\" The current\npolicy field 414 includes a policy 416 that may be used to grant and/or deny permissions to the DiscoveryRole application or to multiple instances thereof.  As illustrated in FIG. 4, the policy 414 is presented in the JSON format, an open standard format\nwith human-readable text.  The user interface 400 includes a details element 418.  The details element 418 may be toggled between a \"brief\" mode and a \"verbose\" mode.  The policy 416 includes a wildcard policy \"swf.*\" that represents or stands in for a\nplurality of policies that include an \"swf.\" prefix.  When the \"brief\" mode is selected, the wildcard policy may be presented to the user in the policy 416.  When the \"verbose\" mode is selected by the details element 418, each policy associated with the\n\"swf.\" prefix may be displayed in the policy 416.\nReferring now to FIG. 5, shown therein is the user interface 400 with the request log data tab 420 selected.  With the request log data tab 420 selected, the user interface 400 presents information obtained from the request log 124 of the\ndistributed computing infrastructure 120 by the monitoring service 128 of FIG. 1.  The request log 124 includes information regarding access calls made by a user, application, or other entity.  The request log 124 further includes a count of errors and a\ncount of access denied errors.  As illustrated in FIG. 5, the request log data 420 includes a usage table 422, all errors table 424, and an access denied errors table 426.  The usage table 422 includes a listing of APIs called by the \"DiscoveryRole\"\napplication and a count of each API call.  The all errors table 424 includes a listing of API calls that resulted in errors and a count of those errors.  The access denied errors table 426 includes a listing of API calls according to a count of access\ndenied errors resulting from the API calls.  As illustrated, the listings in the usage table 422, the all errors table 424, and the access denied errors table 426 are presented in order from highest count to lowest count.  Each of these listings may be\nsorted according to another metric, such as alphabetically by API call.  The user interface 400 of FIG. 5 further includes a period of observation selector 428.  A user may interact with the period of observation selector 428 to select a period of\nobservation, such as the last 15 minutes, the last day, or the last month.  As illustrated, the user of the user interface 400 has selected a period of observation of 14 days.\nReferring now to FIG. 6, shown therein is the user interface 400 with the modifications tab 430 selected.  When the modifications tab 430 is selected, a modified policy field 432 is presented in the user interface 400.  The modified policy field\n432 lists a modified policy 434 for the \"DiscoveryRole\" application.  The modified policy 434 may include a subset of permissions as listed in the current policy field 414 of FIG. 4.  For example, the access management server 102 may determine that\npermission to call a specific API is not warranted to the \"DiscoveryRole\" application based on information included in the request log 124 of FIG. 1.  The access management server 102 may enact changes to the policy or policies associated with the\n\"DiscoveryRole\" application by editing a text file defining the policy.  These changes may be part of a policy modification process.  The access management server 102 may automatically schedule a time at which the modified policy will be implemented. \nThis scheduled time is displayed in the scheduled time field 436.  A user of the user interface 400 may interact with the scheduled time field 436 to alter the scheduled time.  For example, the user may enter a different schedule time in the scheduled\ntime field 436.\nReferring now to FIG. 7, shown therein is the user interface 400, gain with the modifications tab 430 selected.  The modifications tab 430 includes a modified policy summary 440.  The modified policy summary 440 may be a table or collection of\ntables that summarize the modified access policy.  As illustrated, the modified policy summary 440 includes a proposed actions list 442, a removed actions list 444, and an unsupported actions list 446.  The proposed actions list 442 includes a listing of\nitems to which the \"DiscoveryRole\" application is to have access after the modified access policy is implemented by the access management server 102 in the distributed computing infrastructure 120 of FIG. 1.  The removed actions list 444 includes a\nlisting of items (in the illustrated example, API calls) that are to be removed from the current policy or policies associated with the \"DiscoveryRole\" application.  The unsupported actions list 446 includes a listing of API calls that are not monitored\nby the monitoring service 128 of FIG. 1.  Access to such API calls may not be automatically modified by the access management server 102, and so may be provided in the user interface 400 to inform a user that these API calls may need to be monitored\nmanually.\nReferring now to FIG. 8, shown therein is a flowchart of a method 800 for managing access permissions.  As illustrated, method 800 includes several enumerated steps or operations.  Embodiments of the method 800 may include additional steps or\noperations before, after, in between, or as part of the enumerated operations.  Some embodiments of the present disclosure include instructions like the instructions 203 of FIG. 2 stored on a non-transitory computer-readable storage medium, like the\nmemory 204 of FIG. 2.  When executed, the instructions may cause a processing device, like the processing device 202 of computing device 200 to perform operations associated with embodiments of the method 800.  Embodiments of the method 800 may be\nperformed by execution of an application, such as the access management applications 104 and 220 of FIGS. 1 and 2, respectively.\nEmbodiments of the method 800 may begin at step 802 at which a processing device in communication with one or more storage devices receives application request information from a request log of a distributed computing infrastructure.  The\napplication request information describing requests made by a first application deployed in the distributed computing infrastructure.  For example, the processing device of the access management server 102 may receive information from the request log 124\nof the distributed computing infrastructure 120 is illustrated in FIG. 1.  The request log 124 may be a searchable database of information regarding requests for access to an object, such as APIs associated with resources 122A, 122B, 122C, and/or 122D. \nThe distributed computing infrastructure 120 may include a search engine operating on a search server, such as an Elasticsearch server, to facilitate access to information included in the request log 124.  In some embodiments, the request log 124 may be\nprovided by AWS CloudTrail and may include an identity of the user or application that called the API, the time of the API call, a source and dress of the API collar, a region of the distributed computing infrastructure 120 that is associated with the\ncall, call parameters, and/or response elements returned by the distributed computing infrastructure 120.\nAt step 804, the processing device may receive an access policy describing a set of accessible objects associated with the first application.  For example, the processing device of the access management server 102 may communicate with the\nsecurity application 132 executing on the administration system 130 of FIG. 1.  The security application 132 may communicate one or more of the access policies defined therein, such as the access policy 134A or access policy 134B.  The access policy may\ndefine, in a variety of implementations, one or more objects that the first application is permitted access.  For example, the access policy may define a set of APIs that are accessible to the first application, or which the first application may call\nwithin the distributed computing infrastructure 120.  The access policy may define the set of APIs accessible to the first application by listing the APIs or by providing a level of access that corresponds to a predefined subset of APIs exposed by the\ndistributed computing infrastructure 120 on the account within the distributed computing infrastructure 120 associated with the first application.\nAt step 806, the processing device, based on the application request information, may determine that that the first application does not require access to a first object of the set of accessible objects included in a particular access policy\nassociated with the first application.  For example, the processing device of the access management server 102 may determine that the first application does not require access to a first API included in a set of accessible APIs defined by an access\npolicy associated with the first application.  In some embodiments, determining that the first application \"does not require access\" to the first API may include determining that the first application has not used or called the first API more than a\nthreshold number of times during a given period of observation.  In other embodiments, determining that the first application does not require access to the first API may include determining that the first application is producing more than a threshold\nnumber of errors when calling first API in the distributed computing infrastructure 120.  The threshold number of errors may be defined in terms of the type of error produced by the API calls.  For example, when calls to the first API result in access\ndenied errors, the threshold number of errors may be lower than when the first API calls result in different types of errors, for example errors resulting from improper parameters included in the API call.\nAt step 808, the processing device may remove access to the first object from the access policy associated with the first application to produce a modified access policy associated with the first application.  This modified access policy may be\nstored in the administration system 130 in association with the security application 132.  For example, the processing device of the access management server 102 may modify an access policy associated with a first application to remove an API from a set\nof APIs that were accessible to the first application according to the API from a list of APIs included in the access policy or by altering an access level indicated by the access policy.\nEmbodiments of the method 800 may further include operations, performed by the processing device, such as receiving a request from a user associated with the first application to reinstate access to the first object, or in some embodiments, the\nfirst API.  The request may be received through a user interface such as the user interface 300 of FIG. 3, by which the user may select the first API from a listing of recent changes in the access policy associated with the first application.  The user\nmay select a button or other user interface element to cause the request to be sent from a client device associated with the user.  Embodiments of the method 800 may further include determining that the request satisfies predetermined conditions for\nreinstatement of access to the first object and adding access to the first object in the modified access policy associated with the first application to produce a newly modified access policy associated with the first application.  For example, the\naccess management server 102 may receive the request initiated by the user via the user interface 300.  The access management server 102 may check to see whether the request is received before predetermined time after removal of access to the first\nobject, whether the removal of access to the first object or first API resulted from a lack of requests to access the first object during the period of observation, and/or whether the first object is not included on a list of secure objects.  For\nexample, the security application 132 may include a list of APIs that may require the intervention of administrative security personnel for reinstatement.  When the requested API is on such a list of APIs, the access management server 102 may generate a\ncommunication and transmit that communication to administrative security personnel via a user interface, such as the user interface 700 of FIG. 7.\nIn some embodiments of the method 800, the processing device may monitor usage of a second object by the first application during the period of observation.  For example, the access management server 102 may receive application request\ninformation from the request log 124 of the distributed computing infrastructure 120.  Based on the usage or number of calls to a second API included in an access list of the first application, the processing device may determine that the usage of the\nsecond API the first application is less than a threshold usage value.  The processing device of the access management server may schedule removal of access to the second API based on the usage thereof.  Upon scheduling the removal of access to the\nsecond API, the access management server 102 may transmit a communication to a first user associated with the first application.  For example, the access management server 102 may send an email, text message, or push notification, to a developer or\ndeveloper team having responsibility for the first application.  The communication transmitted by the access management server 102 may identify the first application, the second API, and the time at which removal of access to the second API is scheduled. The communication may be received by the user in the user interface 300 of FIG. 3.\nEmbodiments of the method 800 may further include operations of receiving a request from the first user to prevent the scheduled removal of access to the second object, determining that the request satisfies one or more conditions, and\nautomatically preventing the scheduled removal of access to the second object by the first application.  For example, via the user interface 300 of FIG. 3, the user may manipulate a user interface element such as the stop button to request that the\nscheduled removal of access to a second API be unscheduled or be stopped.  For example, the second API may have been included in an earlier stage of development of the first application, wherein the earlier stage of development did not require access to\nthe second API.  When the first application enters or approaches a follow-on stage of development, the first application may need to call the second API to implement features to be deployed by the first application in this subsequent stage of\ndevelopment.  Using the user interface 300 of FIG. 3 or another user interface provided by the access management server 102 in communication with a client device, a developer associated with the first application may be able to request that the scheduled\nremoval not occur.  Depending on the satisfaction of one or more conditions, such as the second API to which access is scheduled to be removed, the access management server 102 may automatically prevent the scheduled removal of access to the second API.\nReferring now to FIG. 9, illustrated therein is a flowchart of a method 900 of managing access permissions within a distributed computing infrastructure.  The access permissions may be granted, revoked, reinstated as part of a management\nprocess.  As illustrated, method 900 includes several enumerated steps or operations.  Embodiments of the method 900 may include additional steps or operations before, after, in between, or as part of the enumerated operations.  Some embodiments of the\npresent disclosure include instructions like the instructions 203 of FIG. 2 stored on a non-transitory computer-readable storage medium, like the memory 204 of FIG. 2.  When executed, the instructions may cause a processing device, like the processing\ndevice 202 of computing device 200 to perform operations associated with embodiments of the method 900.  Embodiments of the method 900 may be performed by execution of an application, such as the access management applications 104 and 220 of FIGS. 1 and\n2, respectively.\nEmbodiments of the method 900 may begin at step 902 at which a processing device receives application request information from a request log of a distributed computing infrastructure.  The application request information may describe API calls\nmade by a first application deployed in the distributed computing infrastructure.  For example, the access management server 102 may include a processing device configured to receive application request information from a request log of the distributed\ncomputing infrastructure 120, as described herein.\nAt step 904, the processing device may receive an access policy describing a set of accessible APIs associated with the first application.  For example, the access management server 102 may receive an access policy 134A, associated with the\nfirst application, from a security application 132 running on the administration system 130 of FIG. 1.  At 906, based on the application request information and the access policy, the processing device may determine that access to a first API of the set\nof accessible APIs is to be removed from the access policy.  For example, the access management server 102 may determine that an insufficient number of calls to the first API have been made during an observation period.  As another example, the access\nmanagement server 102 may determine that an excessive number of errors have resulted from calls and to the first API made during an observation or at a rate that is higher than a permitted rate.\nAt step 908, the processing device may remove access to the first API from the access policy associated with the first application to produce a modified access policy associated with the first application.  Accordingly, the modified access\npolicy associated with the first application may have more limited access to a set of APIs as compared to the access policy prior to modification by the access management server 102.\nIn some embodiments of the method 900, the processing device may receive a request to reinstate access to the first API in the access policy associated with the first application.  For example, the access management server 102 may receive a\nrequest to reinstate the first API from a client device associated with a first user, such as one of the client devices 140A and 140B.  Thereafter, the processing device may reinstate access to the first API in the access policy associated with the first\napplication based on the received request.\nEmbodiments of the presently disclosed systems, servers, devices, and methods may provide for management of access permissions, including access permissions to APIs exposed in a distributed computing infrastructure 120.  Some embodiments of the\npresent disclosure may permit an entity such as an application to begin with a first level of access that is curtailed or limited over time based on the usage of objects accessible at the first level of access.  For example, a set of accessible APIs may\nbe curtailed depending on the usage of the APIs during a period of observation.  Some embodiments of the present disclosure may permit the curtailment of access when a rate of errors or count of errors associated with the access exceeds a defined\nthreshold rate or count.  This may prevent throttling of associated applications in an account on the distributed computing infrastructure 120, thereby improving performance of the associated applications.\nAs noted herein, an entity requesting access to object may be a human user having a user account and/or login credentials to access objects, such as applications, made available through a networked environment such as a corporate VPN.  In\npractical applications, embodiments of the present disclosure may provide a layer of protection to prevent unauthorized access to protected applications in the event that a bad actor gains access to a primary user device, like a laptop.  In some\ninstances, the bad actor may be able to gain access to the device after the authorized user of that device has already logged into the device.  For example, the user may step away from the device, left on a table in a coffee shop, to pick up an order of\ncoffee.  Because walking over to pick up an order of coffee may take so little time, the user may opt to leave the device unlocked and logged-in. A bad actor may act quickly to take the user's unattended laptop while in its unlocked, logged-in state.\nEmbodiments of the present disclosure limit exposure to certain information by limiting access to one or more web-based applications even when the user has been properly logged into a distributed computing environment, such as logged into a\ncorporate virtual private network (VPN).  The system may determine whether a particular application is one that the authorized user has accessed within a predetermined time period.  If the last access was outside the time period, the system may prevent\naccess unless a secondary device, like a smartphone that belongs to the user, is used to send a response to a push notification that confirms the user's initial request.  Upon receipt of the response from the smartphone, the system may then grant access,\npermitting the user's laptop to connect to the protected application.  This protection occurs in real-time, i.e. when access to an application is requested, rather than at the time the user first logs into the VPN that provides access to the application. Additionally, the delay experienced by the user because of the protection may be less than a few seconds.  In this way, the cost of the protection imposed on the user may be small compared to the benefit of protecting sensitive application data from\ntheft by a bad actor.\nAs a non-limiting example, a user may attempt to access a protected application that requires authorization to permit access.  The computing environment may include many such protected applications, which may provide access to existing\nconfidential information or to enter or edit new confidential information, such as billing information for a streaming media service or contact information from media item providers, such as television and movie studios.  Although such confidential\ninformation may be encrypted, access to such information should be limited.  In order to simplify the process of gaining access to various resources or applications, a user request to access a particular protected application may be directed to an\nauthorization control system.  If the user does not have an active session or authenticated cookies in a browser, the authorization control system may redirect the user to an authentication system or service.  The user may then provide credentials to the\nauthentication system, such as a username and password.  Additionally, the authentication system may require another factor in addition to the username and password combination.  Accordingly, the authentication system may require multifactor\nauthentication.  For example after receipt of the users username and password, the authentication system may cause a push notification more than notification to be sent to the user with a code for the user to enter, such as a temporary alphanumeric code\nthat is valid for a short period of time, such as minute.  If required, the user may then provide the alphanumeric code to the authentication system.  Upon receipt of valid credentials, the authentication system may redirect the browser back to the\nauthorization control system, which in turn may direct the browser to the particular protected application the user requested to access.  Where the authentication system provides for single sign-on authentication, the user may additionally be granted\naccess to other protected applications in the event that the user requests to access those other protected applications.\nIn order to prevent loss of data, the authorization control system may also check a list of previous logins associated with the user.  The list of previous logins may be present in a database may include a listing of applications that the user\nhas accessed within a specified period of time and may include a last access date, indicating for each application the last time when the user accessed the application.  Any large organization, many users are theoretically able to access many resources\nand applications in addition to those absolutely necessary for each user's ordinary work roles.  For example, a software developer and the organization may have access to a customer service application but may not have a need to access that application\nas part of the developer's ordinary work.  Similarly, an employee in the human resources department may utilize the same authentication system to access a human resources application that the developer uses to access a development application. \nAccordingly, while the single sign-on authentication system may allow users of an organization to avoid having to authenticate to many different applications during the course of the day, this approach may provide more access than is desirable.\nAccording to the present disclosure, such problems may be mitigated by checking a list of applications accessed by a user within a specified period of time to determine how recently a user has accessed an application the user now seeks to\naccess.  For example, the authorization control system may communicate within access determination system having access feed information that lists the last time at which the user accessed a plurality of applications, including the application for which\naccess is now sought.  If the user has access the application in the recent past or the application is white listed, the access determination system may provide an access response to the authorization control system to allow access to the application. \nIf the user has not access the application within the period of time, the access determination system may provide a conditional access response to the authorization control system.  This access response or instructions to respond may direct the\nauthorization control system to issue a communication to the user via another user device.  For example, if the user is seeking to access the application on a primary device, such as a laptop or a desktop computing device, the access response may direct\nthe authorization control system to issue a communication or directed a communication be issued to the user via a secondary device, such as a smart phone or a tablet computing device.  This may be done by requesting that a push notification service send\na push notification to the user's secondary device.\nThe push notification service may operate in connection with an application or service running on the user's secondary device to request confirmation from the user that the user has requested access to the application via the user's primary\ndevice.  The push notification may provide the user with an option to confirm that the user requested access to the application or to deny that the user requested such access.  If the user confirms the user's previous request, the response is sent\nthrough the push notification service back to the authorization control system, which response by granting the user access to the protected service.  If the user denies that the user requested access, the authorization control system denies access to the\nprimary device.  This primary device may be in the possession of an unauthorized user who should not be able to access protected applications.  The actions taken by the authorization control system may be communicated to the access determination system\nand the users access to the application may be recorded in a set of access feed information, which may be stored locally and/or remotely relative to the access determination system and used in subsequent access determinations.\nIn this way, embodiments of the present disclosure may provide for a desired level of access to protect applications for users and include an additional security measure at the time that infrequently used applications are being accessed.  The\nprocess of receiving a request from a primary device to access a protected application, determining that a notification should be sent, sending the notification to a secondary device, receiving a confirming response or an affirming response to the\nnotification, and granting access to the primary device may be brief.  For example, the process may take less than 30 seconds, less than 10 seconds, or less than five seconds.  Because the added measures are taken with respect to applications that are\ninfrequently accessed, the inconvenience to the user may be minimal, while the added security may be advantageous to the organization.\nOther examples describing the components and the operations of authorization control system and the access determination system are provided herein.  Combinations of these components and operations are within the scope of the present disclosure,\nincluding combinations specifically described and combinations that would be apparent to one of ordinary skill in the art based upon a careful reading of the present disclosure.  Embodiments of the present disclosure may permit for a user using a first\nuser device to be denied access in the absence of a confirming response to a notification from the user's second user device.  Upon receipt of such a confirming response, the user's first user device may quickly be granted access to the desired resource.\nReferring now to FIG. 10, shown therein is a block diagram of a distributed computing environment 1000, according to some aspects of the present disclosure.  The distributed computing environment 1000 includes a plurality of remote resources,\nlike the exemplary remote resource 1002.  The exemplary remote resource 1002 may be a server or a virtual machine executing on a server that provides a network-based application 1004.  In some instances, the application 1004 may be referred to as a\nremote resource.  The application 1004, like other applications included in the distributed computing environment 1000 may provide information and capabilities to one or more client computing systems.  FIG. 10 shows two client computing systems: a first\nuser device 1010A and a second user device 1010B, which are collectively referred to as user devices 1010.  Reference is made to a single user device 110, may apply to either or both of the user devices 1010A and 1010B.  These are devices 1010 may be any\ntype of client computing system, such as a desktop computer, a laptop computer, tablet computer, a smart phone, etc. The devices 1010 may include one or more processors or processing devices, and one or more memories having executable instructions\nthereon, such as applications.  As illustrated, the devices 1010A and 1010B each include an interface 1012A and 1012B, respectively.  The interfaces 1012A and 1012B may be referred to collectively as interfaces 1012 or as a single interface 1012, when\nfeatures common to both the interfaces 1012A and 1012B are described.  In some embodiments, the interfaces 1012 may be web browsers, such as Internet Explorer.RTM., Chrome.RTM., etc. the interfaces 1012 may permit the user devices 1010 to communicate\nwith the depicted plurality of remote resources, including the exemplary remote resource 1002 and/or an application 1004 executing thereon.\nBoth of the user devices 1010 may belong to and be used by a single user 1020.  In order for the user 1020 to access either of the devices 1010, the user 1020 may be required to provide one or more credentials.  For example to log into the user\ndevice 1010A, the user 1020 may be required to enter a username and password.  Continuing with this example, the user 1020 may be required to enter a passcode or supply a biometric authentication credential, such as a fingerprint or an image of the face\nof the user 1020.\nWhen the user 1020 causes the first user device 1010A to request access to the application 1004, the request may be passed on to or redirected to an authorization control system 1030.  As described herein, the authorization control system 1030\nmay receive a request to access the application 1004, which is a protected resource.  The authorization control system 1030 may determine whether the user device 1010A and/or the interface 1012A has an active and authenticated session.  In some\nembodiments, this may be determined by checking for a valid authentication cookie.  However, other techniques for determining whether the user device 1010A is in an authenticated session may be utilized in other embodiments.  In some embodiments, the\nauthorization control system 1030 may communicate with an authentication system 1032 to determine whether the first user device 1010A has a valid authorized session.  The authorization control system 1030 may communicate with the authentication system\n1032 by making calls to an application programming interface (API), such as a RESTful API.  In other embodiments, the authorization control system 1030 in the authentication system 1032 may be integrated into a single service or system.  In some\nembodiments, the authentication system 1032 may be third party authentication system, such as Google Authenticator provided by Google, Inc.  of Mountain View, Calif., or PingID.TM.  MFA provided by Ping Identity Corporation of Denver, Colo.  The\nauthentication system 1032 may provide for multifactor authentication.  For example, if the authorization control system 1030 determines that the user device 1010A does not have a valid, authenticated session, the authorization control system 1030 may\nredirect the user device 1010A to the authentication system 1032 to establish such a session.  The authentication system 1032 may request authentication credentials such as a username and password and may require an additional credential such as an\nalphanumeric code or numeric code sent to a device of the user 1020, such as the second user device 1010B.  After the user 1020 provides sufficient authentication credentials, the authentication system 1032 may communicate with the authorization control\nsystem 1030 to establish a valid and active session for the user device 1010A.\nWhen the authorization control system 1030 determines that there is a valid session, the authorization control system 1030 communicates with an access determination system 1034 to determine whether access should be granted to the application\n1004.  The authorization control system 1030 may call an API exposed by the access determination system 1034.  The call may include resource or application request information that identifies the user 1020, by credentials provided via the first user\ndevice 1010A, and an identifier of the application 1004.  Using the provided application request information, the access determination system 1034 queries an access information feed system, which may include an access feed storage system 1036.  As\ndescribed herein, the storage system 1036 may include local and remote storage devices that include a list of users of the environment 1000.  The list may be searched based on an identifier of the user, such as a username, and/or the identifier of the\napplication to which access is sought.\nThe access determination system 1034 may determine an appropriate access response based on information contained in the storage system 1036.  Some embodiments of the access determination system 1034 may provide for three kinds of access\nresponses.  First, when the access determination system 1034 determines that the user 1020 has accessed the application 1004 within a predetermined time period or the application 1004 is a white listed application, the access response instructs the\nauthorization control system 1030 to redirect the interface 1012A of the first user device 1010A to the application 1004, providing access to the application 1004.  Second, when the access determination system 1034 receives application requests\ninformation that omits one or more critical items, the access response is to deny access to the application 1004 by the first user device 1010A.  These critical items may include one or more of a user identifier, an application identifier, and an\nInternet protocol (IP) address of the first user device 1010A.  Third, when the access determination system 1034 determines that the user 1020 of the first user device 1010A has not accessed the application 1004 within the predetermined time period, the\naccess determination system 1034 issues on access response to the authorization control system 1030 to request additional information from the user 1020.\nIn some embodiments, the authorization control system 1030 and the access determination system 1034 may be collectively referred to as a resource or application access system.  While depicted as separate components in FIG. 10, the authorization\ncontrol system 1030 and the access determination system 1034 may be provided by an integrated service operating on a single server or a cluster of servers.\nIn order to gain additional information from the user 1020, the authorization control system 1030 may communicate with a notification system 1040.  The notification system 1040 may act as an intermediary between the authorization control system\n1030 and the second user device 1010B.  For example, the notification system 1040 may be a push notification system such as a push notification system provided by Duo Security, Inc.  of Ann Arbor, Mich.  The authorization control system 1030 requests\nthat the notification system 1040 issue a notification to the user 1020.  The notification system 1040 transmits a notification to the user device 1010B.  In some embodiments, the request from the authorization control system 1030 may identify whether\nthe user 1020 is using the user device 1010A or the user device 1010B to request access to the application 1004.  The notification system 1040 may determine which of the user devices 1010 should receive the notification, by sending the notification to\nwhichever device is not being used to make the request.  In other embodiments, the authorization control system 1030 may provide an indication of which device 1010 of the user 1020 should receive the notification.\nThe notification may be rendered in the second user device 1010B to present the user 1020 with one or more user interface elements.  For example, the user 1020 may be presented with a message such as, \"Did you request access to [Application\nIdentifier]?,\" in which the message includes an identifier of the application 1004.  In some embodiments, the message may further include an identifier of the first user device 1010A, such as a device type and/or IP address, and a time of the request for\naccess.  The notification may include one or more buttons, such as a \"yes\" button\" and a \"no\" button, the selection of which is communicated from the user device 1010B back to the notification system 1040.  When the response from the user to the\nnotification is an affirming or confirming response (i.e., a response that affirms or confirms the user has requested access to the application 1004), the authorization control system 1030 redirects the user device 1010A to the application 1004.  When\nthe response from the user 1020 to the notification indicates that the user 1020 did not make the request to access application 1004, the authorization control system 1030 may direct the user device 1010A elsewhere.  For example, the authorization\ncontrol system 1030 may direct the user device 1010A to an error page.  In some instances, the notification system 1040 may start a timer when the notification is communicated to the second user device 1010B.  When the time on the timer has elapsed\nwithout a response from the second user device 1010B, the notification system 1040 may provide an indication to the authorization control system 1030 that the notification has timed out.  In such circumstances, the authorization control system 1030 may\nregister the response from the notification system 1040 as a denial that the user 1020 requested access to the application 1004, and redirect the user device 1010A to an error page.\nThe authorization control system 1030 may provide an access result to the access determination system 1034.  The access result includes information identifying the corresponding access request issued earlier by the access determination system\n1034 to the authorization control system 1030 or may include information sufficient to identify the user 1020 and the application 1004.  When the access result is to grant access by the user device 1010A to the application 1004, this result may be stored\nin the access feed storage system 1036 to include an entry indicative of the last time that the user 1020 and/or the user device 1010A accessed the application 1004.  Similarly, the access determination system 1034 may produce a record that the user 1020\nand/or the user device 1010A were denied access to the application 1004.  This information may be used by the access determination system 1034 to identify potential anomalies that may indicate a security threat associated with the user 1020 and/or the\nuser device 1010A.\nIn some embodiments, the authorization control system 1030 may deny access to the application 1004 even after receipt of a confirming response from the second user device 1010B.  For example, when the IP address associated with the first user\ndevice 1010A indicates that the device 1010A is in an area, such as a region or country, to which access is to be denied, the authorization control system 1030 may redirect the first user device 1010A to an error page.  The error page may include contact\ninformation or other mechanisms whereby the user 1020 may request assistance in obtaining access to the application 1004.  For example, the error page may include a phone number to an automatic or manual phone system that enables the user 1020 to receive\nassistance.\nAdditionally, some embodiments of the access determination system 1034 may include a predetermined time period for the application 1004 that is different than a predetermined time period for another application.  For example, the predetermined\ntime period associated with the application 1004 may be 90 days, while the predetermined time period associated with another, more sensitive application may be 30 days.  The predetermined time period associated with each application may be based on a\nrank of the importance of the particular application.  The importance of the application may be determined by the type of information that may be read or written via the application, so that application providing access to more sensitive information may\nrequire more frequent access in order to avoid the requirement of additional factors to obtain access.\nReferring now to FIG. 11, shown therein is an exemplary computing system 1100.  Embodiments of the computing system 1100 may provide the user devices 1010, one of the remote resources 1002, the authorization control system 1030, the\nauthentication system 1032, the access determination system 1034.  Embodiments of the computing system 1100 may include additional features or omit depicted features to appropriately suit the computing system 1100 for its use.  In general, the computing\nsystem 1100 includes a processor 1102 and a memory 1104.  The processor 1102 may include a plurality of individual processors or processing cores.  Similarly, the memory 1104 may include multiple memory components, such as a disk-based and/or a\nsolid-state hard drive, RAM memory, cache memory, etc. an operating system 1106 may be executed on the processor 1102 to provide frameworks to execute individual applications and interface with in between various hardware components of the computing\nsystem 1100.  The memory 1104 may include data 1110 and executable instructions 1112.  The data 1110 may include information such as listings of users and records of the users last time of accessing various programs.  The executable instructions may\ninclude instructions for many different computer programs.  Exemplary system programs may include, without limitation, an operating system (e.g., iOS.RTM., Android.RTM.  OS, LINUX.RTM.  OS, Firefox OS.TM., Windows.RTM., OS X.RTM., Binary Run-time\nEnvironment for Wireless (BREW) OS, JavaOS, a Wireless Application Protocol (WAP) OS, and others), device drivers, programming tools, utility programs, software libraries, (APIs), and so forth.  The executable instructions 1112 may include instructions\nthat are being executed by the processor 1102 to provide services and functionalities to users of the computing system 1100, which may be other computing systems in some embodiments.\nThe computing system 1100 may further include a network interface 1114, such as a network interface controller (NIC) or a collection of network interface hardware to enable the computing system 1100 to communicate with other systems via wired\nand/or wireless networks, represented by the network 1116.  In some embodiments, the network 1116 may include multiple networks, such as a cellular network and the Internet.  The networks that make up the network 1116 may be configured to communicate\nwith each other through gateways or other devices and systems.\nReferring to FIG. 12, shown therein is an exemplary embodiment of the computing system 1100, configured as a smart phone 1200.  As shown in FIG. 12, the smart phone 1200 includes a display 1202, which may be a touchscreen display configured to\nact as an input device in addition to displaying information to a user.  The display 1202 includes a notification 1204, which may be presented to a user of the smart phone 1200 by the authorization control system 1030 to verify that the user has\nrequested access to an application via another user device, such as a laptop computer.  When the user, such as the user 1020 of FIG. 10, is presented with the notification 1204, the user may select from among interface elements to respond to a prompt or\nmessage included in the notification 1204.  The depicted embodiment of the notification 1204 includes the message: \"Did you request access to the customer payments application?\" The user of the smart phone 1200 may select the user interface element 1206A\nor the user interface element 1206B, to indicate in an affirmative response or a negative response, respectively.  As described herein, after selecting the user interface element 1206A, a primary device of the user may be connected to the desired\napplication.  Additionally, the smart phone 1200 may include other interface elements, such as the hardware-based interface element 1208.  In some implementations, the interface element 1208 may be pushed or selected by the user to ignore the\nnotification 1204 and to resume use of other applications on the smart phone 1200.\nReferring now to FIG. 13, shown therein are some of the components of the environment 1000 as depicted in FIG. 10.  For example, FIG. 13 includes the authorization control system 1030 and the access determination system 1034.  FIG. 13 differs\nfrom FIG. 10 in depicting the access feed storage system 1036 an additional detail.  The depicted embodiment of the access feed storage system 1036 includes a cache 1300, which stores a database or table of user access information 1302A.  The cache 1300\nmay be memory local to or contained within the access determination system 1034.  In some implementations, the cache 1300 is or includes a distributed off box cache in addition to a fallback local cache.  The access feed storage system 1036 may include a\ntable that includes entries associated with many different users and the last time each of those users accessed a variety of applications or other protected resources.  For instance, an entry in the user access information 1302 a may include: an\nidentifier of a user, and application to which the user requested access, an indication of whether the request was granted or denied, and the time of the granted access or denial.  Accordingly, for any given user the user access information 1302A may\ninclude tens, hundreds, or thousands of entries, which may be listed based on a query of the cache 1300.  The access determination system 1034 may provide the information for the table of user access information 1302A to the cache 1300.  This information\nmay also be provided to the cluster computing infrastructure 1304.  For example, the cluster computing infrastructure 1304 may include one or more servers configured to operate as a cluster.  The cluster computing infrastructure 1304 may run a cluster\ncomputing framework, such as Apache Spark or Apache Hadoop.\nThe authorization control system 1030 in the access determination system 1034 may obtain information associated with all successful logins to the protected remote resources 1002.  Some embodiments may include information associated with\nunsuccessful logins in addition to successful logins.  This information may be stored by the access determination system 1034 in a large-scale data warehouse infrastructure 1306.  The data warehouse infrastructure 1306 may provide data summarization,\nquery, and analysis.  In some embodiments, the data warehouse infrastructure 1306 may be provided by an Apache Hive instance, although other embodiments may rely on other mechanisms to provide access to the table of login information.  The information\nmay be processed, encrypted, and stored in a remote storage system 1308 like an S3 bucket, referring to the Amazon Simple Storage Service (S3) cloud storage service provided by Amazon Web Services of Seattle, Wash.  Other remote storage systems may be\nused in other embodiments.  A cluster-computing job may be configured and performed by a workflow orchestration and scheduling framework, like the Meson workflow framework produced by Netflix, Inc.  of Los Gatos, Calif.  The job may be a Spark job, in\nsome embodiments, and may be configured to run on a schedule such as once daily.  The associated job may be run more or less often in various embodiments.  When the job is completed, the workflow framework may cause the output of the job to be loaded\ninto the cache 1300 as the user access information 1302A.  The workflow may also store the information as user access information 1302B in the remote storage system 1308.  When the access determination system 1034 submits a query for information\nassociated with the user, the query may use an identifier of the user, such as the user's email address or the user's username.  In this way, a certain period of data associated with the user may be accessed to determine whether or not the user has\naccessed the application, currently being requested, within the period of time.  The data warehouse infrastructure 1306 may be configured to include login information for a specified period only, deleting any information having a date outside of the\nspecified period.  As noted herein, the specified period may be individualized on a per application basis or a per user basis.\nReferring now to FIG. 14, shown therein is a flowchart of a method 1400 for securing user access to applications in a distributed computing environment, like the environment 1000 of FIG. 10.  Embodiments of the method 1400 may include additional\nsteps or operations in between, before, after, or as part of the enumerated operations shown in FIG. 14.  Additionally, some embodiments of the method 1400 may omit one or more of the enumerated operations.  Some embodiments of the method 1400 may be\nexecutable instructions stored on a non-transitory, tangible medium that may be read by a computer or processor thereof to perform the operations of the method.\nAs illustrated, the method 1400 may begin an operation 1402 when a processor of an access determination system receives application request information from an authorization control system.  The application request information may include an\nidentifier of a first web-based application and an identifier of a first user.  For example, the access determination system 1034 may receive a request from the authorization control system 1030 to determine how to respond to a request from the first\nuser to access the protected application 1004.  The request may include resource request information that includes an identifier of the application 1004 and an identifier of the user 1020 and/or the first user device 1010A.  For example, identifiers may\ninclude a username, an application name, a device identifier, and/or IP addresses associated with the first user device 1010A and the remote resource 1002.\nAt operation 1404, the processor determines an access response based on the application request information and access information feed.  For example, the access determination system 1034 may receive a username of the user 1020 and identifier of\nthe application 1004.  In some embodiments, the access determination system 1034 may also receive an IP address associated with the user device issuing the request, which may be the first user device 1010A.  As described herein, the access determination\nsystem 1034 may query an access feed storage system 1036 (shown in FIGS. 1 and 13) to retrieve access feed information associated with the user 1020.  The access determination system 1034 may determine, from the access feed information, the last time\nthat the user 1020 access the application 1004.  Based on a determination of whether the last time of access was within a predetermined time period during which access to the application 1004 by the user 1020 has been monitored.  The predetermined time\nperiod may vary depending on the application and/or the user.  For example, when the user 1020 is a contractor of an organization rather than an employee of the organization, the predetermined time period may be shorter, such as a 30 day time period for\na contractor and a 90 day time period for an employee.  The access response may be to allow access to the first web-based application, to allow access to the first web-based application when the response from the second user device to the notification is\na conforming response, or to deny access to the first web-based application.\nAt operation 1406, the processor transmits the access response to the authorization control system.  For example, the access determination system 1034 may respond to the authorization control system 1030 by transmitting the determined access\nresponse as determined based on information associated with the user 1020 and the access feed information accessible to the access determination system 1034.\nAt operation 1408, the processor receives an access result from the authorization control system.  The access result may be based on a response from a second user device to a notification.  For example, the access determination system 1034 may\nreceive an indication of the result of the access response provided previously to the authorization control system 1030 by the access determination system 1034.  The result may indicate that the user of a second user device 1010B has provided a\nconfirming response to a push notification sent by a notification system 1040.  In some instances, the result may indicate that the user of the second user device 1010B has provided an indication that access was not requested by the user 122 the\napplication 1004.  This may indicate that the first user device 1010A is being used by an authorized user, e.g., a user other than the user 1020.  In some instances, the authorization control system 1030 may deny access to the application 1004 even when\na confirming response to a push notification has been received.  For example, access may be denied to the first user device 1010A when an IP address associated with the first user device 1010A indicates that the user 1020 (or at least the user device\n1010A) is in an area that is to be denied access or is outside a specified area in which access is permitted.\nAt operation 1410, the processor stores the access result in an access information feed system.  For example, the access determination system 1034 may communicate with the cache 1300, the cluster computing infrastructure 1304, the data warehouse\ninfrastructure 1306 and/or the remote storage system 1308, to store the access result in a set of user access information, such as the user access information 1302A or 1302B.  In this way, the access result may be stored for later access to determine\nwhether a subsequent request by the user device 1010A for access to the protected application 1004 should be granted.  In some embodiments, this access result may be logged in a different manner.  For example, the application 1004 and/or the remote\nresource 1002 may be programmed to communicate successful logins and or unsuccessful logins to the access determination system 1034 or to a component of the access feed storage system 1036, directly.\nAs described herein, the operations are performed by a processor.  This processor may be one or more processors, such as one or more processors of the access determination system 1034.  As noted, additional or alternative operations may be\nincluded in different embodiments of the method 1400.\nReferring now to FIG. 15, shown therein is a flowchart of another method, method 1500, securing user access to applications in a distributed computing environment.  Embodiments of the method 1500 may include additional steps or operations in\nbetween, before, after, or as part of the enumerated operations shown in FIG. 15.  Additionally, some embodiments of the method 1500 may omit one or more of the enumerated operations.  Some embodiments of the method 1500 may be executable instructions\nstored on a non-transitory, tangible medium that may be read by a computer or processor thereof to perform the operations of the method.\nSome embodiments of the method 1500 may begin at operation 1502 when a processor receives an authentication indication from an authentication service.  The authentication indication may be associated with a request by a first user device to\naccess a protected service.  For example, the authorization control system 1030 may receive an indication from the authentication system 1032 of FIG. 10, that the user 1020 has authenticated the first user device 1010A to the authentication system.  For\nexample, the first user device 1010A may be used to provide a username, which may be an email address or another identifier, a password, and a one-time passcode, for a multifactor authentication scheme.  Other embodiments may use fewer factors or\ndifferent factors in authenticating the user of the first user device 1010A.  The authentication system 1032 may be used as a single sign-on service by which the first user device 1010A may gain access to remote resources 1002 and applications 1004. \nHowever, the authorization control system 1030 may limit access even when the user of the first user device 1010A has been authenticated by the authentication system 1032.\nAt operation 1504, when the authentication indication indicates successful authentication, the processor requests an access decision from an access determination service.  As noted, even when a user has successfully authenticated the first user\ndevice 1010A with the authentication system 1032, the authorization control system 1030 may deny access to a particular application depending on when the first user device 1010A, or a user 1020 identified by a username and other credentials, last\naccessed the particular application.  As described in connection with the method 1400 of FIG. 14, the authorization control system 1030 may query the access determination system 1034 to request an access response indicating whether the user device 1010A\nmay be granted access to the application 1004 without a confirming response to a push notification or only with a confirming response or be denied access to the application due to a negative response, a lack of a timely response, or despite a confirming\nresponse.  In some embodiments, the access determination system may report the last time the user 1020 or the user device 1010A access the application 1004 and the authorization control system may make the determination of whether a confirming response\nto a push notification sent to the second user device 1010B should be received before access to the application 1004 is granted.\nAfter determining whether additional information should be collected from the user 1020 before granting access to the application 1004 to the first user device 1010A, the processor requests that a notification system issue a notification to a\nsecond user device, at operation 1506.  For example, the authorization control system 1030 may communicate with the notification system 1040 to request that the notification system 1040 send a notification, such as a push notification to a second user\ndevice 1010B that is associated with the user 1020.  The authorization control system 1030 may include in the request, and identifier of the user 1020 and/or the first user device 1010A that can be associated by the push notification system 1040 with the\nsecond user device 1010B.  The authorization control system 1030 may also include an identifier of the application 1004 so that the notification system 1040 may notify the second user device 1010B of the application to which access has been requested. \nIn some embodiments, the authorization control system 1030 or the access determination system 1034 may include a table associating the user 1020, the first user device 1010A, and the second user device 1010B, such that the authorization control system\n1030 may provide an identifier of the second user device 1010B to the notification system 1040.\nAt operation 1508, the processor receives a response to the notification from the second user device.  For example, the authorization control system 1030 may receive a response to the notification via the notification system 1040.  As described\nin connection with FIG. 12, the second user device 1010B may be a smart phone like the smart phone 1200.  The user 1020 may select a user interface element to either confirm or deny that the user 1020 requested access to the application 1004.  In some\nembodiments, the notification system 1040 may be programmed to respond with a negative response in the event that no response is received from the second user device 1010B within a predetermined time period.  In some other responses, the authorization\ncontrol system 1030 may be programmed to generate a negative response or a timed out response in the event that no response is received from the second user device 1010B.\nAt operation 1510, the processor grants access to the protected service to the first user device, after and in response to receipt of a confirming response has been received from the second user device.  For example, the authorization control\nsystem 1030 may redirect or direct the interface 1012A of the first user device 1010A to the application 1004, after the authorization control system 1030 receives a confirming response from the second user device 1010B.\nEmbodiments of the present disclosure may provide a layer of protection to prevent unauthorized access to protect applications in the event that a bad actor has one device that has already been authorized with respect to some applications, like\na logged-in computer left open for a moment in a library or a coffee shop.  In this way, embodiments of the present disclosure may improve the security of distributed computing environments, such as an enterprise network having tens or hundreds of\napplications accessible to employees once they are logged in to the network.\nCertain aspects of the present disclosure are set out the new following numbered clauses:\n1.  An application access system comprising: an access determination server having a processing device in communication with one or more additional networked systems; an authorization control system being included in the one or more additional\nnetworked systems; a first web-based application accessible over a network to a first user device of a first user; and wherein the processing device of the access determination server: receives application request information from the authorization\ncontrol system, the application request information including an identifier of the first web-based application and an identifier of the first user, determines an access response based on the application request information and access information feed;\ntransmits the access response to the authorization control system; receives an access result from the authorization control system, the access result being based on a response from a second user device to a notification; and stores the access result in\nan access information feed system.\n2.  The application access system of clause 1, wherein the access determination system includes a local store of previous access results.\n3.  The application access system of any of clauses 1-2, wherein the access determination system communicates with a remote database system to access a remote store of previous access results.\n4.  The application access system of any of clauses 1-3, wherein the access response is one of: allowing access to the first web-based application; allowing access to the first web-based application when the response from the second user device\nto the notification is a conforming response; and denying access to the first web-based application.\n5.  The application access system of any of clauses 1-4, wherein the conforming response is an affirmative response and is received within a predetermined time.\n6.  The application access system of any of clauses 1-5, wherein the authorization control system implements the access response transmitted from the access determination system.\n7.  The application access system of any of clauses 1-6, wherein the authorization control system interfaces with a single sign-on service to control access to the first web-based application over the network by the first user device.\n7.1 A non-transitory computer-readable storage medium storing instructions that, when executed by one or more processing devices, cause the processing devices to perform any of the features recited in any of clauses 1-7.\n7.2.  A method that, when implemented by one or more processing devices, performs operations providing any of the features recited in any of clauses 1-7.\n8.  A computer-implemented method of controlling access to a web-based application, the method comprising: receiving resource request information from an authorization control system, the resource request information including an identifier of\nthe web-based application and an identifier of a first user received from a first user device, determining an access response based on the application request information and access information feed, wherein the access response is to allow access, by the\nauthorization control system, to the web-based application based on a response from a second user device to a notification; and transmitting the access response to the authorization control system to permit access to the web-based application.\n9.  The method of clause 8, further comprising: receiving an access result from the authorization control system, the access result being based on the response from the second user device to the notification; and storing the access result in an\naccess information feed system.\n10.  The method of any of clauses 8-9, wherein storing the access result in the access information feed system comprises storing the access result in a local cache.\n11.  The method of any of clauses 8-10, wherein the access information feed system includes the local cache and includes remotely stored access information in a remote storage system.\n12.  The method of any of clauses 8-11, further comprising comparing information in the local cache with information in the remote storage system to determine the access response.\n13.  The method of any of clauses 8-12, further comprising determining that the first user device does not have an authorized session when the resource request information is received from the authorization control system.\n14.  The method of any of clauses 8-13, wherein the response from the second user device to the notification is an affirmative response and is received by the authorization control system from the second user device within a predetermined time.\n15.  The method of any of clauses 8-14, wherein the resource request information includes an Internet protocol (IP) address associated with the first user device and wherein the access response is further based on the IP address.\n16.  The method of claim any of clauses 8-15, wherein the access information feed comprises a table of access information and collected over a predetermined period.\n17.  The method of any of clauses 8-16, wherein the table of access information comprises successful login information associated with the first user for a plurality of web-based applications and times at which the web-based applications were\nlast accessed.\n17.1 A non-transitory computer-readable storage medium storing instructions that, when executed by one or more processing devices, cause the processing devices to perform any of the features recited in any of clauses 8-17.\n17.2 A system configured to perform the operations described by the features recited in any of clauses 8-17.\n18.  A computer-implemented method for determining an access response to provide to a first user device requesting access to a first web-based application over a network, the method comprising: receiving an authentication indication from an\nauthentication service, the authentication indication being associated with a request by a first user device to access a protected service; when the authentication indication indicates successful authentication, requesting an access decision from an\naccess determination service; requesting that a notification system issue a notification to a second user device; receiving a response to the notification from the second user device; and granting access to the protected service to the first user device.\n19.  The method of clause 18, wherein the notification is a push notification and wherein a type of the second user device is different than a type of the first user device.\n20.  The method of any of clauses 18-19, wherein the authentication service is a third-party authentication service requiring a multi-factor interaction for authentication.\n20.1 A non-transitory computer-readable storage medium storing instructions that, when executed by one or more processing devices, cause the processing devices to perform any of the features recited in any of clauses 18-20.\n20.2 A system configured to perform the operations described by the features recited in any of clauses 18-20.\nAlthough the foregoing aspects of the present disclosure have been described in detail by way of illustration and example for purposes of clarity and understanding, it will be recognized that the above described invention may be embodied in\nnumerous other specific variations and embodiments without departing from the spirit or essential characteristics of the invention.  Various changes and modifications may be practiced, and it is understood that the invention is not to be limited by the\nforegoing details, but rather is to be defined by the scope of the claims.", "application_number": "15584453", "abstract": " Provided herein are systems and methods of controlling access to a\n     web-based application. Such a system may include an access determination\n     server, an authorization control system, and a first web-based\n     application accessible over a network to a first user device of a first\n     user. The processing device of the access determination server receives\n     application request information from the authorization control system,\n     the application request information including an identifier of the first\n     web-based application and an identifier of the first user, determines an\n     access response based on the application request information and access\n     information feed, transmits the access response to the authorization\n     control system, receives an access result from the authorization control\n     system, the access result being based on a response from a second user\n     device to a notification, and stores the access result in an access\n     information feed system.\n", "citations": ["5068782", "5428803", "6092155", "6553464", "6715082", "7072984", "7437362", "7516285", "8051491", "9009111", "9239912", "20050262132", "20080091978", "20080243869", "20080313721", "20120226712", "20140189880", "20150081918", "20150244684", "20160197915"], "related": ["14876629"]}, {"id": "20170237899", "patent_code": "10372988", "patent_name": "Systems and methods for automatically varying privacy settings of wearable\n     camera systems", "year": "2019", "inventor_and_country_data": " Inventors: \nWexler; Yonatan (Jerusalem, IL), Shashua; Amnon (Mevaseret Zion, IL)  ", "description": "<BR><BR>BACKGROUND\nTechnical Field\nThis disclosure generally relates to devices and methods for capturing and processing images from air environment of a user, and using information derived from captured images.  More particularly, this disclosure relates to devices and methods\nfor using a wearable device including a camera for capturing information related to the user's environment, and to systems for processing data received from the wearable device.\nBackground Information\nToday, technological advancements make it possible for wearable devices to automatically capture images and store information that is associated with the captured images.  Certain devices have been used to digitally record aspects and personal\nexperiences of one's life in an exercise typically called \"lifelogging.\" Some individuals log their life so they can retrieve moments from past activities, for example, social events, trips, etc. Lifelogging may also have significant benefits in other\nfields (e.g., business, fitness and healthcare, and social research).  Lifelogging devices, while useful for tracking daily activities, may be improved with capability to enhance one's interaction in his environment with feedback and other advanced\nfunctionality based on the analysis of captured image data.\nEven though users can capture images with their smartphones and some smartphone applications can process the captured images, smartphones may not be the best platform for serving as lifelogging apparatuses in view of their size and design. \nLifelogging apparatuses should be small and light, so they can be easily worn.  Moreover, with improvements in image capture devices, including wearable apparatuses, additional functionality may be provided to assist users in navigating in and around an\nenvironment, identifying persons and objects they encounter, and providing feedback to the users about their surroundings and activities.  Therefore, there is a need for apparatuses and methods for automatically capturing and processing images to provide\nuseful information to users of the apparatuses, and for systems and methods to process and leverage information gathered by the apparatuses.\n<BR><BR>SUMMARY\nEmbodiments consistent with the present disclosure provide devices and methods for automatically capturing and processing images from an environment of a user, and systems and methods for processing information related to images captured from\nthe environment of the user.\nConsistent with a disclosed embodiment, a wearable imaging device is provided.  The wearable imaging device may include an image capture device, a transmitter, and at least one processing device.  The at least one processing device may be\nprogrammed to: obtain at least one image captured by the image capture device; analyze the at least one image to detect a contextual situation associated with the at least one image; based on the detected contextual situation, associate with the at least\none image a category tag, wherein the category tag is associated with a selected function; determine image-related information associated with the detected contextual situation; and cause the transmitter to transmit the determined image-related\ninformation to a device paired with the wearable imaging device to cause the paired device to execute the selected function based on the determined image-related information.\nConsistent with another disclosed embodiment, another wearable imaging device is provided.  This wearable imaging device may also include an image capture device, a transmitter, and at least one processing device.  The at least one processing\ndevice may be programmed to: receive a request, from a device paired with the wearable imaging device to transmit information associated with a category tag, wherein the category tag is associated with a selected function; obtain at least one image\ncaptured by an image capture device included in the wearable imaging device; analyze the at least one image to detect a contextual situation associated with the at least one image; determine, based on the category tag, image-related information\nassociated with the detected contextual situation; and cause the transmitter to transmit the determined image-related information to the paired device to cause the paired device to execute the selected function based on the determined image-related\ninformation.\nConsistent with another disclosed embodiment, a method is provided.  The method may include: receiving a request from a device paired with a wearable imaging device to transmit information associated with a category tag, wherein the category tag\nis associated, with a selected function; obtaining at least one image captured by an image capture device included in the wearable imaging device; analyzing the at least one image to detect a contextual situation associated with the at least one image;\ndetermining image-related information associated with the detected contextual situation; and transmitting the determined image-related information to the paired device to cause the paired device to execute the selected function based on the determined\nimage-related information.\nConsistent with another disclosed embodiment, a wearable apparatus for visually pairing with an external device is disclosed.  The wearable apparatus includes at least one transmitter, a memory, at least one image sensor configured to capture a\nstream of images from an environment of a user of the wearable apparatus, and at least one processing device.  The at least one processing device is programmed to receive the stream of images from the at least one image sensor, analyze the stream of\nimages to detect the external device in the environment of the user, and cause the at least one transmitter to transmit an interrogation signal, the interrogation signal being configured to cause a change in at least one aspect of the external device. \nThe at least one processing device is further programmed to analyze the stream of images to detect the change in the at least one aspect of the external device and, after detection of the change in the at least one aspect of the external device, store in\nthe memory information relating to the external device.\nConsistent with another disclosed embodiment, a method for visually pairing with an external device is disclosed.  The method includes receiving a stream of images captured from an environment of a user of a wearable apparatus, analyzing the\nstream of images to detect the external device in the environment of the user, and causing at least one transmitter associated with the wearable apparatus to transmit an interrogation signal, the interrogation signal being configured to cause a change in\nat least one aspect of the external device.  The method further includes analyzing the stream of images to detect the change in the at least one aspect of the external device, and, after detection of the change in the at least one aspect of the external\ndevice, storing information relating to the external device.\nConsistent with another disclosed embodiment, a system for controlling one or more controllable devices includes a transceiver and at least one processing device.  The processing device is programmed to obtain one or more images captured by an\nimage sensor included in a wearable apparatus, analyze the one or more images to identify a controllable device in an environment of a user of the wearable apparatus, analyze the one or more images to detect a visual trigger associated with the\ncontrollable device and, based on the detection of the visual trigger, transmit, via the transceiver, a command.  The command is configured to change at least one aspect of the controllable device.\nConsistent with another disclosed embodiment, a method for controlling one or more controllable devices includes obtaining one or more images captured by an image sensor included in a wearable apparatus, analyzing the one or more images to\nidentify a controllable device in an environment of a user of the wearable apparatus, analyzing the one or more images to detect a visual trigger associated with the controllable device, and, based on the detection of the visual trigger, transmitting a\ncommand.  The command is configured to change at least one aspect of the controllable device.\nCertain embodiments of the present disclosure relate to a server-based system for interacting with a plurality of wearable apparatuses.  Each wearable apparatus may be associated with a different user.  The system may include a data interface\nand at least one processing device.  The at (east one processing device may be programmed to receive, via the data interface and for each of the plurality of wearable apparatuses, a date stream including image-based information associated with images\ncaptured by a camera present on a particular wearable apparatus from among the plurality of wearable apparatuses.  The system may also analyze the image-based information of the data streams received from each of the plurality of wearable apparatuses to\ndetermine at least one trait common to two or more of the different users of the plurality of wearable apparatuses.  The system may also store in a database information relating to the determined at least one trait.\nCertain embodiments of the present disclosure also relate to a method for interacting with a plurality of wearable apparatuses.  Each wearable apparatus may be associated with a different user.  The method may include receiving, for each of the\nplurality of wearable apparatuses, a data stream including image-based information associated with images captured by a camera present cm a particular wearable apparatus from among the plurality of wearable apparatuses.  The method may also include\nanalyzing the image-based information of the data streams received from each of the plurality of wearable apparatuses to determine at least one trait common to two or more of the different users of the plurality of wearable apparatuses.  The method may\nfurther include storing in a database information relating to the determined at least one trait.\nConsistent with a disclosed embodiment, a wearable apparatus is provided for identifying a contextual situation related to a wearer.  The wearable apparatus may include a wearable image sensor configured to capture a plurality of images from an\nenvironment of the wearer.  The wearable apparatus may further include a transmitter and at least one processing device.  The at least one processing device may be programmed to: analyze the plurality of images to identify the contextual situation\nrelated to the wearer; determine information associated with the contextual situation; and cause the transmitter to transmit the determined information to a device paired with the wearable apparatus to cause the paired device to provide at least one\nalert to the wearer based on the determined information associated with the contextual situation.\nConsistent with another disclosed embodiment, a method is provided for identifying a contextual situation related to a wearer of a wearable apparatus.  The method includes: receiving a plurality of images captured from an environment of the\nwearer; analyzing the plurality of images to identify the contextual situation related to the wearer; determining information associated with the contextual situation; and causing a device paired with the wearable apparatus to provide at least, one alert\nto the wearer based on the determined information associated with the contextual situation.\nConsistent with yet another disclosed embodiment, a software product stored on a non-transitory computer readable medium and comprising data and computer implementable instructions for carrying a method for identifying a contextual situation\nrelated to a wearer of a wearable, apparatus, is provided.  The method includes: receiving a plurality of images captured from an environment of the wearer; analyzing the plurality of images to identify the contextual situation related to the wearer:\ndetermining information associated with the contextual situation; and presenting on a display at least one alert to the wearer based on the determined information associated with the contextual situation.\nCertain embodiments of the present disclosure relate to a system for facilitating collaboration between individuals.  The system may include a transceiver and at least one processing device.  The at least one processing device may be programmed\nto obtain one or more images captured by an image sensor included in a wearable apparatus.  The at least one processing device may also be programmed to analyze the one or more images to detect a visual trigger in an environment of a wearer of the\nwearable apparatus.  The visual trigger may be associated with a collaborative action to be taken.  The at least, one processing device may also be programmed to transmit, via the transceiver, an indicator relating to the visual trigger associated with\nthe collaborative action to be taken.\nCertain embodiments of the present disclosure also relate to a server-based system for facilitating collaboration among users of a plurality of wearable apparatuses.  The system may comprise a data interface and at least one processing device. \nThe at least one processing device may be programmed to receive, via the data interface, a data stream including image-based information associated with images captured by a camera present on a particular wearable apparatus from among the plurality of\nwearable apparatuses.  The at least one processing device may also be programmed to analyze the image-based information to determine a visual trigger associated with a collaborative action to be taken.  In some embodiments, the collaborative action may\ninclude distributing information to two or more devices.  And the at least one processing device may also be programmed to distribute the information to the two or more devices based on the visual trigger.\nCertain embodiments of the present disclosure also relate to a method for facilitating collaboration between individuals.  The method may comprise obtaining one or more images captured by an image sensor included in a wearable apparatus.  The\nmethod may also include analyzing the one or more images to detect a visual trigger in an environment of a wearer of the wearable apparatus.  In some embodiments, the visual trigger may be associated with a collaborative action to be taken.  The method\nmay also include transmitting an indicator relating to the visual trigger associated with the collaborative action to be taken.\nCertain embodiments of the present disclosure also relate to a method for facilitating collaboration among users of a plurality of wearable apparatuses.  The method may include receiving a data stream including image-based information associated\nwith images captured by a camera present on a particular wearable apparatus from among the plurality of wearable apparatuses.  The method may also include analyzing the image-based information to determine a visual trigger associated with a collaborative\naction to be taken.  In some embodiments, the collaborative action may include distributing information to two or more devices.  The method may further include, based on the visual trigger, distributing the information to the two or more devices.\nIn accordance with a disclosed embodiment, a wearable imaging apparatus having variable privacy settings is provided.  The apparatus may comprise a wearable image sensor configured to capture a plurality of images from an environment of a wearer\nof the wearable imaging apparatus, a memory for storing privacy mode triggers and associated privacy mode settings, and at least one processing device.  The at least one processing device may be programmed to analyze the plurality of images and recognize\nwithin one or more of the plurality of images a presence of at least one of the privacy mode triggers.  Further, the processor device may be programmed to automatically cause one or more adjustments to the wearable imaging apparatus based on the privacy\nmode settings associated with the at least one recognized privacy mode trigger.\nIn accordance with another disclosed embodiment, a method for adjusting variable privacy settings of a wearable imaging apparatus is provided.  The method includes receiving a plurality of images captured from an environment of a wearer of the\nwearable imaging apparatus.  The method further includes analyzing the plurality of images and recognizing within one or more of the plurality of images a presence of at least one privacy mode trigger.  Also, the method includes automatically causing one\nor more adjustments to the wearable imaging apparatus based on a privacy mode setting associated with the at least one recognized privacy mode trigger.\nConsistent with other disclosed embodiments, non-transitory computer-readable storage media may store program instructions, which are executed by at least one processor and perform any of the methods described herein.\nThe foregoing genera) description and the following detailed description are exemplary and explanatory only and are not restrictive of the claims. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe accompanying drawings, which are incorporated in and constitute a part of this disclosure, illustrate various disclosed embodiments.  In the drawings:\nFIG. 1A is a schematic illustration of an example of a user wearing a wearable apparatus according to a disclosed embodiment.\nFIG. 1B is a schematic illustration of an example of the user wearing a wearable apparatus according to a disclosed embodiment.\nFIG. 1C is a schematic illustration of an example of the user wearing a wearable apparatus according to a disclosed embodiment.\nFIG. 1D is a schematic illustration of an example of the user wearing a wearable apparatus according to a disclosed embodiment.\nFIG. 2 is a schematic illustration of an example system consistent with the disclosed embodiments.\nFIG. 3A is a schematic illustration of an example of the wearable apparatus shown in FIG. 1A.\nFIG. 3B is an exploded view of the example of the wearable apparatus shown in FIG. 3A.\nFIG. 4A is a schematic illustration of an example of the wearable apparatus shown in FIG. 1B from a first viewpoint.\nFIG. 4B is a schematic illustration of the example of the wearable apparatus shown in FIG. 1B from a second viewpoint.\nFIG. 5A is a block diagram illustrating an example of the components of a wearable apparatus according to a first embodiment.\nFIG. 5B is a block diagram illustrating an example of the components of a wearable apparatus according to a second embodiment.\nFIG. 5C is a block diagram illustrating an example of the components of a wearable apparatus according to a third embodiment.\nFIG. 6 illustrates an exemplary embodiment of a memory containing software modules consistent, with the present disclosure.\nFIG. 7 is a schematic illustration of an embodiment of a wearable apparatus including an orientable image capture unit.\nFIG. 8 is a schematic illustration of an embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.\nFIG. 9 is a schematic illustration of a user wearing a wearable apparatus consistent with an embodiment of the present disclosure.\nFIG. 10 is a schematic illustration of an embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.\nFIG. 11 is a schematic illustration of an embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.\nFIG. 12 is a schematic illustration of an embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.\nFIG. 13 is a schematic illustration of an embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.\nFIG. 14 is a schematic illustration of an embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.\nFIG. 15 is a schematic illustration of an embodiment of a wearable apparatus power unit including a power source.\nFIG. 16 is a schematic illustration of an exemplary embodiment of a wearable apparatus including protective circuitry.\nFIG. 17A is a schematic illustration of a first example of a contextual situation that triggers a device paired with a wearable apparatus to execute a selected function according to a disclosed embodiment.\nFIG. 17B is a schematic illustration of a second example of a contextual situation that triggers the paired device to execute a selected function according to a disclosed embodiment.\nFIG. 17C is a schematic illustration of a third example of a contextual situation that triggers the paired device to execute a selected function according to a disclosed embodiment.\nFIG. 17D is a schematic illustration of a fourth example of a contextual situation that triggers the paired device to execute a selected function according to a disclosed embodiment.\nFIG. 18A is a message flow diagram that depicts the communication between a wearable apparatus and a paired de vice for three different types of selected functions consistent with disclosed embodiments.\nFIG. 18B is a schematic illustration of three different types of scenarios where multiple selected functions are triggered consistent with disclosed embodiments.\nFIG. 19 is a flowchart showing an exemplary process for causing a device paired with a wearable apparatus to execute a selected function consistent with disclosed embodiments.\nFIG. 20 is a block diagram illustrating an example of the components a wearable apparatus for visually pairing with an external device.\nFIG. 21 is a flowchart illustrating an exemplary process for visually pairing with an external device.\nFIGS. 22A, 22B, and 22C are schematic illustrations of an exemplary visual pairing between a wearable apparatus and an external device.\nFIG. 23 is a block diagram illustrating an example of the components of a wearable apparatus for controlling an external device.\nFIG. 24 is a flowchart illustrating an exemplary process for controlling an external device.\nFIGS. 25A, 25B, and 25C are schematic illustrations of examples showing an external device being controlled by a wearable apparatus.\nFIG. 26 illustrates an example environment consistent with the disclosed embodiments.\nFIG. 27A illustrates an exemplary embodiment of a memory containing software modules consistent with the present disclosure.\nFIG. 27B illustrates an exemplary embodiment of a data stream consistent with the present disclosure.\nFIG. 28 is a flowchart illustrating an exemplary method of determining a trait consistent with the disclosed embodiments.\nFIG. 29A is a schematic illustration of a first example of a contextual situation that triggers provisioning of an alert according to a disclosed embodiment.\nFIG. 29B is a schematic illustration of a second example of a contextual situation that triggers provisioning of an alert according to a disclosed embodiment.\nFIG. 29C is a schematic illustration of a third example of a contextual situation that triggers provisioning of an alert according to a disclosed embodiment.\nFIG. 29D is a schematic illustration of a fourth example of a contextual situation that triggers provisioning of an alert according to a disclosed embodiment.\nFIG. 30 is a flow diagram showing an exemplary process for providing alerts to a user consistent with, disclosed embodiments.\nFIG. 31 is a flowchart showing an exemplary process for providing alerts to a user consistent with disclosed embodiments.\nFIG. 32A illustrates an example collaborative environment consistent with the disclosed embodiments.\nFIG. 32B illustrates an exemplary hand gesture as a visual trigger associated with a collaborative action consistent with the disclosed embodiments.\nFIG. 32C illustrates an example collaborative environment consistent with the disclosed embodiments.\nFIG. 33A illustrates an exemplary visual trigger associated with a collaborative action consistent with the disclosed embodiments.\nFIG. 33B illustrates an exemplary visual trigger associated with a collaborative action consistent with the disclosed embodiments.\nFIG. 33C illustrates an example environment consistent with the disclosed embodiments.\nFIG. 33D illustrates an exemplary visual trigger associated with a collaborative action consistent with the disclosed embodiments.\nFIG. 34 illustrates an exemplary embodiment of a memory containing software modules consistent with the present disclosure.\nFIG. 35 is a flowchart illustrating an exemplary method of determining a visual trigger consistent with the disclosed embodiments.\nFIG. 36A is a block diagram illustrating an example of the components of a wearable apparatus according to a fourth embodiment.\nFIG. 36B is a block diagram illustrating an example of the components of a wearable apparatus according to a fifth embodiment.\nFIG. 36C is a block diagram illustrating an example of the components of a wearable apparatus according to a sixth embodiment.\nFIG. 37 is a block diagram of an exemplary memory of a wearable apparatus storing software modules and at least one database.\nFIGS. 38A, 38B, and 38C are example illustrations of image data captured by an image sensor associated with a wearable apparatus, consistent with disclosed embodiments.\nFIG. 39 is an example of a process for automatically varying settings associated with a privacy mode for an image sensor associated with a wearable apparatus, consistent with disclosed embodiments.\nThe following detailed description refers to the accompanying drawings.  Wherever possible, the same reference numbers are used in the drawings and the following description to refer to the same or similar parts.  While several illustrative\nembodiments are described herein, modifications, adaptations and other implementations are possible.  For example, substitutions, additions or modifications may be made to the components illustrated in the drawings, and the illustrative methods described\nherein may be modified by substituting, reordering, removing, or adding steps to the disclosed methods.  Accordingly, the following detailed description is not limited to the disclosed embodiments and examples.  Instead, the proper scope is defined by\nthe appended claims.\nFIG. 1A illustrates a user 100 wearing an apparatus 110 that is physically connected (or integral) to glasses 130, consistent with the disclosed embodiments.  Glasses 130 may be prescription glasses, magnifying glasses, non-prescription glasses,\nsafety glasses, sunglasses, etc. Additionally, in some embodiments, glasses 130 may include pails of a frame and earpieces, nosepieces, etc., and one or no lenses.  Thus, in some embodiments, glasses 130 may function primarily to support apparatus 110,\nand/or an augmented reality display device or other optical display device.  In some embodiments, apparatus 110 may include an image sensor (not shown in FIG. 1A) for capturing real-time image data of the field-of-view of user 100.  The term \"image data\"\nincludes any form of data retrieved from optical signals in the near-infrared, infrared, visible, and ultraviolet spectrums.  The image data may include video clips and/or photographs.\nIn some embodiments, apparatus 110 may communicate wirelessly or via a wire with a computing device 120.  In some embodiments, computing device 120 may include, for example, a smartphone, or a tablet, or a dedicated processing unit, which may be\nportable (e.g., can be carried in a pocket of user 100).  Although shown in FIG. 1A as an external device, in some embodiments, computing device 120 may be provided as part of wearable apparatus 110 or glasses 130, whether integral thereto or mounted\nthereon.  In some embodiments, computing device 120 may be included in an augmented reality display device or optical head mounted display provided integrally or mounted to glasses 130.  In other embodiments, computing device 120 may be provided as part\nof another wearable or portable apparatus of user 100 including a wrist-strap, a multifunctional watch, a button, a clip-on, etc. And in other embodiments, computing device 120 may be provided as part of another system, such as an on-board automobile\ncomputing or navigation system.  A person skilled in the art can appreciate that different types of computing devices and arrangements of devices may implement the functionality of the disclosed embodiments.  Accordingly, in other implementations,\ncomputing device 120 may include a Personal Computer (PC), laptop, an Internet server, etc.\nFIG. 1B illustrates user 100 wearing apparatus 110 that is physically connected to a necklace 140, consistent with a disclosed embodiment.  Such a configuration of apparatus 110 may be suitable for users that do not wear glasses some or all of\nthe time.  In this embodiment, user 100 can easily wear apparatus 110, and take it off.\nFIG. 1C illustrates user 100 wearing apparatus 110 that is physically connected to a belt 150, consistent with a disclosed embodiment.  Such a configuration of apparatus 110 may be designed as a belt buckle.  Alternatively, apparatus 110 may\ninclude a clip for attaching to various clothing articles, such as belt 150, or a vest, a pocket, a collar, a cap or hat or other portion of a clothing article.\nFIG. 1D illustrates user 100 wearing apparatus 110 that is physically connected to a wrist strap 160, consistent with a disclosed embodiment.  Although the aiming direction of apparatus 110, according to this embodiment, may not match the\nfield-of-view of user 100, apparatus 110 may include the ability to identify a hand-related trigger based on the tracked eye movement of a user 100 indicating that user 100 is looking in the direction of the wrist strap 160.  Wrist strap 160 may also\ninclude an accelerometer, a gyroscope, or other sensor for determining movement or orientation of a user's 100 hand for identifying a hand-related trigger.\nFIG. 2 is a schematic illustration of an exemplary system 200 including a wearable apparatus 110, worn by user 100, and an optional computing device 120 and/or a server 250 capable of communicating with apparatus 110 via a network 240,\nconsistent with disclosed embodiments, hi some embodiments, apparatus 110 may capture and analyze image data, identify a hand-related trigger present in the image data, and perform an action and/or provide feedback to a user 100, based at least in part\non the identification of the hand-related trigger.  In some embodiments, optional computing device 120 and/or server 250 may provide additional functionality to enhance interactions of user 100 with his or her environment as described in greater detail\nbelow.\nAccording to the disclosed embodiments, apparatus 110 may include an image sensor system 220 for capturing real-time image data of the field-of-view of user 100.  In some embodiments, apparatus 110 may also include a processing unit 210 for\ncontrolling and performing the disclosed functionality of apparatus 110, such as to control the capture of image data, analyze the image data, and perform an action and/or output a feedback based on a hand-related trigger identified in the image data. \nAccording to the disclosed embodiments, a hand-related trigger may include a gesture performed by user 100 involving a portion of a hand of user 100.  Further, consistent with some embodiments, a hand-related trigger may include a wrist-related trigger. \nAdditionally, in some embodiments, apparatus 110 may include a feedback outputting unit 230 for producing an output of information to user 100.\nAs discussed above, apparatus 110 may include an image sensor 220 for capturing image data.  The term \"image sensor\" refers to a device capable of detecting and converting optical signals in the near-infrared, infrared, visible, and ultraviolet\nspectrums into electrical signals.  The electrical signals may be used to form an image or a video stream (i.e. image data) based on the detected signal.  The term \"image data\" includes any form of data retrieved from optical signals in the\nnear-infrared, infrared, visible, and ultraviolet spectrums.  Examples of image sensors may include semiconductor charge-coupled devices (CCD), active pixel sensors in complementary metal-oxide-semiconductor (CMOS), or N-type metal-oxide-semiconductor\n(NMOS, Live MOS).  In some cases, image sensor 220 may be pail of a camera included in apparatus 110.\nApparatus 110 may also include a processor 210 for controlling image sensor 220 to capture image data and for analyzing the image data according to the disclosed embodiments.  As discussed in further detail below with respect to FIG. 5A,\nprocessor 210 may include a \"processing device\" for performing logic operations on one or more inputs of image data and other data according to stored or accessible software instructions providing desired functionality.  In some embodiments, processor\n210 may also control feedback outputting unit 230 to provide feedback to user 100 including information based on the analyzed image data and the stored software instructions.  As the term is used herein, a \"processing device\" may access memory where\nexecutable instructions are stored or, in some embodiments, a \"processing device\" itself may include executable instructions (e.g., stored in memory included in the processing device).\nIn some embodiments, the information or feedback information provided to user 100 may include time information.  The time information may include any information related to a current time of day and, as described further below, may be presented\nin any sensory perceptive manner.  In some embodiments, time information may include a current time of day in a preconfigured format (e.g., 2:30 pm or 14:30).  Time information may include the time in the user's current time zone (e.g., based on a\ndetermined location of user 100), as well as an indication of the time zone and/or a time of day in another desired location.  In some embodiments, time information may include a number of hours or minutes relative to one or more predetermined times of\nday.  For example, in some embodiments, time information may include an indication that three hours and fifteen minutes remain until a particular hour (e.g., until 6:00 pm), or some other predetermined time.  Time information may also include a duration\nof time passed since the beginning of a particular activity, such as the start of a meeting or the start of a jog, or any other activity.  In some embodiments, the activity may be determined based on analyzed image data.  In other embodiments, time\ninformation may also include additional information related to a current time and one or more other routine, periodic, or scheduled events.  For example, time information may include an indication of the number of minutes remaining until the next\nscheduled event, as may be determined from a calendar function or other information retrieved from computing device 120 or server 250, as discussed in further detail below.\nFeedback outputting unit 230 may include one or more feedback systems for providing the output of information to user 100.  In the disclosed embodiments, the audible or visual feedback may be provided via any type of connected audible or visual\nsystem or both.  Feedback of information according to the disclosed embodiments may include audible feedback to user 100 (e.g., using a Bluetooth.TM.  or other wired or wirelessly connected speaker, or a bone conduction headphone).  Feedback outputting\nunit 230 of some embodiments may additionally or alternately produce a visible output of information to user 100, for example, as part of an augmented reality display projected onto a lens of glasses 130 or provided via a separate heads up display in\ncommunication with apparatus 110, such as a display 260 provided as part of computing device 120, which may include an onboard automobile heads up display, an augmented reality device, a virtual reality device, a smartphone, PC, table, etc.\nThe term \"computing device\" refers to a device including a processing unit and having computing capabilities.  Some examples of computing device 120 include a PC, laptop, tablet, or other computing systems such as an on-board computing system of\nan automobile, for example, each configured to communicate directly with apparatus HO or server 250 over network 240.  Another example of computing device 120 includes a smartphone having a display 260.  In some embodiments, computing device 120 may be a\ncomputing system configured particularly for apparatus 110, and may be provided integral to apparatus 110 or tethered thereto.  Apparatus 110 can also connect to computing device 120 over network 240 via any known wireless standard (e.g., Wi-Fi,\nBluetooth.RTM., etc.), as well as near-filed capacitive coupling, and other short range wireless techniques, or via a wired connection.  In an embodiment in which computing device 120 is a smartphone, computing device 120 may have a dedicated application\ninstalled therein.  For example, user 100 may view on display 260 data (e.g., images, video clips, extracted information, feedback information, etc.) that originate from or are triggered by apparatus 110.  In addition, user 100 may select part of the\ndata for storage in server 250.\nNetwork 240 may be a shared, public, or private network, may encompass a wide area or local area, and may be implemented through any suitable combination of wired and/or wireless communication networks.  Network 240 may further comprise an\nintranet or the Internet.  In some embodiments, network 240 may include short range or near-field wireless communication systems for enabling communication between apparatus 110 and computing device 120 provided in close proximity to each other, such as\non or near a user's person, for example.  Apparatus 110 may establish a connection to network 240 autonomously, for example, using a wireless module (e.g., Wi-Fi, cellular).  In some embodiments, apparatus 110 may use the wireless module when being\nconnected to an external power source, to prolong battery life.  Further, communication between apparatus 110 and server 250 may be accomplished through any suitable communication channels, such as, for example, a telephone network, an extranet, an\nintranet, the Internet, satellite communications, off-line communications, wireless communications, transponder communications, a local area network (LAN), a wide area network (WAN), and a virtual private network (VPN).\nAs shown in FIG. 2, apparatus 110 may transfer or receive data to/from server 250 via network 240.  In the disclosed embodiments, the data being received from server 250 and/or computing device 120 may include numerous different types of\ninformation based on the analyzed image data, including information related to a commercial product, or a person's identity, an identified landmark, and any other information capable of being stored in or accessed by server 250.  In some embodiments,\ndata may be received and transferred via computing device 120.  Server 250 and/or computing device 120 may retrieve information from different data sources (e.g., a user specific database or a user's social network account or other account, the Internet,\nand other managed or accessible databases) and provide information to apparatus 110 related to the analyzed image data and a recognized trigger according to the disclosed embodiments.  In some embodiments, calendar-related information retrieved from the\ndifferent data sources may be analyzed to provide certain time information or a time-based context for providing certain information based on the analyzed image data.\nAn example of wearable apparatus 110 incorporated with glasses 130 according to some embodiments (as discussed in connection with FIG. 1A) is shown in greater detail in FIG. 3A.  In some embodiments, apparatus 110 may be associated with a\nstructure (not shown in FIG. 3A) that enables easy detaching and reattaching of apparatus 110 to glasses 130.  In some embodiments, when apparatus 110 attaches to glasses 130, image sensor 220 acquires a set aiming direction without the need for\ndirectional calibration.  The set aiming direction of image sensor 220 may substantially coincide with the field-of-view of user 100.  For example, a camera associated with image sensor 220 may be installed within apparatus 110 in a predetermined angle\nin a position facing slightly downwards (e.g., 5-15 degrees from the horizon).  Accordingly, the set aiming direction of image sensor 220 may substantially match the field-of-view of user 100.\nFIG. 3B is an exploded view of the components of the embodiment discussed regarding FIG. 3A.  Attaching apparatus 110 to glasses 130 may take place in the following way.  Initially, a support 310 may be mounted on glasses 130 using a screw 320,\nin the side of support 310.  Then, apparatus 110 may be clipped on support 310 such that it is aligned with the field-of-view of user 100.  The term \"support\" includes any device or structure that enables detaching and reattaching of a device including a\ncamera to a pair of glasses or to another object, (e.g., a helmet).  Support 310 may be made from plastic (e.g., polycarbonate), metal (e.g., aluminum), or a combination of plastic and metal (e.g., carbon fiber graphite).  Support 310 may be mounted on\nany kind of glasses (e.g., eyeglasses, sunglasses, 3D glasses, safety glasses, etc.) using screws, bolts, snaps, or any fastening means used in the art.\nIn some embodiments, support 310 may include a quick release mechanism for disengaging and reengaging apparatus 110.  For example, support 310 and apparatus 110 may include magnetic elements.  As an alternative example, support 310 may include a\nmale latch member find apparatus 110 may include a female receptacle, in other embodiments, support 310 can be an integral part of a pair of glasses, or sold separately and installed by an optometrist.  For example, support 310 may be configured for\nmounting on the arms of glasses 130 near the frame front, but before the hinge.  Alternatively, support 310 may be configured for mounting on the bridge of glasses 130.\nIn some embodiments, apparatus 110 may be provided as part of a glasses frame 130, with or without lenses.  Additionally, in some embodiments, apparatus 110 may be configured to provide an augmented reality display projected onto a lens of\nglasses 130 (if provided), or alternatively, may include a display for projecting time information, for example, according to the disclosed embodiments.  Apparatus 110 may include the additional display or alternatively, may be in communication with a\nseparately provided display system that may or may not be attached to glasses 130.\nIn some embodiments, apparatus 110 may be implemented in a form other than wearable glasses, as described above with respect to FIGS. 1B-1D, for example.  FIG. 4A is a schematic illustration of an example of an additional embodiment of apparatus\n110 from a first viewpoint.  The viewpoint shown in FIG. 4A is from the front of apparatus 110.  Apparatus 110 includes an image sensor 220, a clip (not shown), a function button (not shown) and a hanging ring 410 for attaching apparatus 110 to, for\nexample, necklace 140, as shown in FIG. 1B.  When apparatus 110 hangs on necklace 140, the aiming direction of image sensor 220 may not fully coincide with the field-of-view of user 100, but the aiming direction would still correlate with the\nfield-of-view of user 100.\nFIG. 4B is a schematic illustration of the example of a second embodiment of apparatus 110, from a second viewpoint.  The viewpoint shown in FIG. 4B is from a side orientation of apparatus 110.  In addition to hanging ring 410, as shown in FIG.\n4B, apparatus 110 may further include a clip 420.  User 100 can use clip 420 to attach apparatus 110 to a shirt or belt 150, as illustrated in FIG. 1C.  Clip 420 may provide an easy mechanism for disengaging and reengaging apparatus 110 from different\narticles of clothing.  In other embodiments, apparatus 110 may include a female receptacle for connecting with a male latch of a car mount or universal stand.\nIn some embodiments, apparatus 110 includes a function button 430 for enabling user 100 to provide input to apparatus 110.  Function button 430 may accept different types of tactile input (e.g., a tap, a click, a double-click, a long press, a\nright-to-left slide, a left-to-right slide).  In some embodiments, each type of input may be associated with a different action.  For example, a tap may be associated with the function of taking a picture, while a right-to-left slide may be associated\nwith the function of recording a video.\nThe example embodiments discussed above with respect to FIGS. 3A, 3B, 4A, and 4B are not limiting.  In some embodiments, apparatus 110 may be implemented in any suitable configuration for performing the disclosed methods.  For example, referring\nback to FIG. 2, the disclosed embodiments may implement an apparatus 110 according to any configuration including an image sensor 220 and a processor unit 210 to perform image analysis and for communicating with a feedback unit 230.\nFIG. 5A is a block diagram illustrating the components of apparatus 110 according to an example embodiment.  As shown in FIG. 5A, and as similarly discussed above, apparatus 110 includes an image sensor 220, a memory 550, a processor 210, a\nfeedback outputting unit 230, a wireless transceiver 530, and a mobile power source 520.  In other embodiments, apparatus 110 may also include buttons, other sensors such as a microphone, and inertia measurements devices such as accelerometers,\ngyroscopes, magnetometers, temperature sensors, color sensors, light sensors, etc. Apparatus 110 may further include a data port 570 and a power connection 510 with suitable interfaces for connecting with an external power source or an external device\n(not shown).\nProcessor 210, depicted in FIG. 5A, may include any suitable processing device.  The term \"processing device\" includes any physical device having an electric circuit that performs a logic operation on input or inputs.  For example, processing\ndevice may include one or more integrated circuits, microchips, microcontrollers, microprocessors, all or part of a central processing unit (CPU), graphics processing unit (GPU), digital signal processor (DSP), field-programmable gate array (FPGA), or\nother circuits suitable for executing instructions or performing logic operations.  The instructions executed by the processing device may, for example, be pre-loaded into a memory integrated with or embedded into the processing device or may be stored\nin a separate memory (e.g., memory 550).  Memory 550 may comprise a Random Access Memory (RAM), a Read-Only Memory (ROM), a hard disk, an optical disk, a magnetic, medium, a flash memory, other permanent, fixed, or volatile memory, or any other mechanism\ncapable of storing instructions.\nAlthough, in the embodiment illustrated in FIG. 5A, apparatus 110 includes one processing device (e.g., processor 210), apparatus 110 may include more than one processing device.  Each processing device may have a similar construction, or the\nprocessing devices may be of differing constructions that are electrically connected or disconnected from each other.  For example, the processing devices may be separate circuits or integrated in a single circuit.  When more than one processing device\nis used, the processing devices may be configured to operate independently or collaboratively.  The processing devices may be coupled electrically, magnetically, optically, acoustically, mechanically or by other means that permit them to interact.\nIn some embodiments, processor 210 may process a plurality of images captured from the environment of user 100 to determine different parameters related to capturing subsequent images.  For example, processor 210 can determine, based on\ninformation derived from captured image data, a value for at least one of the following: an image resolution, a compression ratio, a cropping parameter, frame rate, a focus point an exposure time, an aperture size, and a light sensitivity.  The\ndetermined value may be used in capturing at least one subsequent image.  Additionally, processor 210 can detect images including at least one hand-related trigger in the environment of the user and perform an action and/or provide an output of\ninformation to a user via feedback outputting unit 230.\nIn another embodiment, processor 210 can change the aiming direction of image-sensor 220.  For example, when apparatus 110 is attached with clip 420, the aiming direction of image sensor 220 may not coincide with the field-of-view of user 100. \nProcessor 210 may recognize certain situations from the analyzed image data and adjust the aiming direction of image sensor 220 to capture relevant image data.  For example, in one embodiment, processor 210 may detect an interaction with another\nindividual and sense that the individual is not fully in view, because image sensor 220 is tilted down.  Responsive thereto, processor 210 may adjust the aiming direction of image sensor 220 to capture image data of the individual.  Other scenarios are\nalso contemplated where processor 210 may recognize the need to adjust an aiming direction of image sensor 220.\nIn some embodiments, processor 210 may communicate data to feedback-outputting unit 230, which may include any device configured to provide information to a user 100.  Feedback outputting unit 230 may be provided as part of apparatus 110 (as\nshown) or may be provided external to apparatus 110 and communicatively coupled thereto.  Feedback-outputting unit 230 may be configured to output visual or nonvisual feedback based on signals received from processor 210, such as when processor 210\nrecognizes a hand-related trigger in the analyzed image data.\nThe term \"feedback\" refers to any output or information provided in response to processing at least one image in an environment, in some embodiments, as similarly described above, feedback may include an audible or visible indication of time\ninformation, detected text or numerals, the value of currency, a branded product, a person's identity, the identity of a landmark or other environmental situation or condition including the street names at an intersection or the color of a traffic light,\netc., as well as other information associated with each of these.  For example, in some embodiments, feedback may include additional information regarding the amount of currency still needed to complete a transaction, information regarding the identified\nperson, historical information or times and prices of admission etc, of a detected landmark etc. In some embodiments, feedback may include an audible tone, a tactile response, and/or information previously recorded by user 100.  Feedback-outputting unit\n230 may comprise appropriate components for outputting acoustical and tactile feedback.  For example, feedback-outputting unit 230 may comprise audio headphones, a hearing aid type device, a speaker, a bone conduction headphone, interfaces that provide\ntactile cues, vibrotaetile stimulators, etc. In some embodiments, processor 230 may communicate signals with an external feedback outputting unit 230 via a wireless transceiver 530, a wired connection, or some other communication interface.  In some\nembodiments, feedback outputting unit 230 may also include any suitable display device for visually displaying information to user 100.\nAs shown in FIG. 5A, apparatus 110 includes memory 550.  Memory 550 may include one or more sets of instructions accessible to processor 210 to perform the disclosed methods, including instructions for recognizing a hand-related trigger in the\nimage data.  In some embodiments memory 550 may store image data (e.g., images, videos) captured from the environment of user 100.  In addition, memory 550 may store information specific to user 100, such as image representations of known individuals,\nfavorite products, personal items, and calendar or appointment information, etc. In some embodiments, processor 210 may determine, for example, which type of image data to store based on available storage space in memory 550.  In another embodiment,\nprocessor 210 may extract information from the image data stored in memory 550.\nAs further shown in FIG. 5A, apparatus 110 includes mobile power source 520.  The term \"mobile power source\" includes any device capable of providing electrical power, which can be easily earned by hand (e.g., mobile power source 520 may weigh\nless than a pound).  The mobility of the power source enables user 100 to use apparatus 110 in a variety of situations.  In some embodiments, mobile power source 520 may include one or more batteries (e.g., nickel-cadmium batteries, nickel-metal hydride\nbatteries, and lithium-ion batteries) or any other type of electrical power supply.  In other embodiments, mobile power source 520 may be rechargeable and contained within a casing that holds apparatus 110.  In yet other embodiments, mobile power source\n520 may include one or more energy harvesting devices for converting ambient energy into electrical energy (e.g., portable solar power units, human vibration units, etc.).\nMobile power source 520 may power one or more wireless transceivers (e.g., wireless transceiver 530 in FIG. 5A).  The term \"wireless transceiver\" refers to any device configured to exchange transmissions over an air interface by use of radio\nfrequency, infrared frequency, magnetic field, or electric field.  Wireless transceiver 530 may use any known standard to transmit and/or receive data (e.g., Wi-Fi, Bluetooth.RTM., Bluetooth Smart, 80215.4, or ZigBee).  In some embodiments, wireless\ntransceiver 530 may transmit data (e.g., raw image data, processed image data, extracted information) from apparatus 110 to computing device 120 and/or server 250.  Wireless transceiver 530 may also receive data from computing device 120 and/or server\n250.  In other embodiments, wireless transceiver 530 may transmit data and instructions to an external feedback outputting unit 230.\nFIG. 5B is a block diagram illustrating the components of apparatus 110 according to another example embodiment.  In some embodiments, apparatus 110 includes a first image sensor 220a, a second image sensor 220b, a memory 550, a first processor\n210a, a second processor 210b, a feedback outputting unit 230, a wireless transceiver 530, a mobile power source 520, and a power connector 510.  In the arrangement shown in FIG. 5B, each of the image sensors may provide images in a different image\nresolution, or face a different direction.  Alternatively, each image sensor may be associated with a different camera (e.g., a wide angle camera, a narrow angle camera, an IR camera, etc.).  In some embodiments, apparatus 110 can select which image\nsensor to use based on various factors.  For example, processor 210a may determine, based on available storage space in memory 550, to capture subsequent images in a certain resolution.\nApparatus 110 may operate in a first processing-mode and in a second processing-mode, such that the first processing-mode may consume less power than the second processing-mode.  For example, in the first processing-mode, apparatus 110 may\ncapture images and process the captured images to make real-time decisions based on an identifying hand-related trigger, for example.  In the second processing-mode, apparatus 110 may extract information from stored images in memory 550 and delete images\nfrom memory 550.  In some embodiments, mobile power source 520 may provide more than fifteen hours of processing in the first processing-mode and about three hours of processing in the second processing-mode.  Accordingly, different processing-modes may\nallow mobile power source 520 to produce sufficient power for powering apparatus 110 for various time periods (e.g., more than two hours, more than four hours, more than ten hours, etc.).\nIn some embodiments, apparatus 110 may use first processor 210a in the first processing-mode when powered by mobile power source 520, and second processor 210b in the second processing-mode when powered by external power source 580 that is\nconnectable via power connector 510.  In other embodiments, apparatus 110 may determine, based on predefined conditions, which processors or which processing modes to use.  Apparatus 110 may operate in the second processing-mode even when apparatus 110\nis not powered by external power source 580.  For example, apparatus 110 may determine that it should operate in the second processing-mode when apparatus 110 is not powered by external power source 580, if the available storage space in memory 550 for\nstoring new image data is lower than a predefined threshold.\nAlthough one wireless transceiver is depicted in FIG. 5B, apparatus 110 may include more than one wireless transceiver (e.g., two wireless transceivers).  In air arrangement with more than one wireless transceiver, each of the wireless\ntransceivers may use a different standard to transmit and/or receive data.  In some embodiments, a first wireless transceiver may communicate with server 250 or computing device 120 using a cellular standard (e.g., LTE or GSM), and a second wireless\ntransceiver may communicate with server 250 or computing device 120 using a short-range standard (e.g., Wi-Fi or Bluetooth.RTM.).  In some embodiments, apparatus 110 may use the first wireless transceiver when the wearable apparatus is powered by a\nmobile power source included in the wearable apparatus, and use the second wireless transceiver when the wearable apparatus is powered by an external power source.\nFIG. 5C is a block diagram illustrating the components of apparatus 110 according to another example embodiment including computing device 120.  In this embodiment, apparatus 110 includes an image sensor 220, a memory 550a, a first processor\n210, a feedback-outputting unit 230, a wireless transceiver 530a, a mobile power source 520, and a power connector 510.  As further shown in FIG. 5C, computing device 120 includes a processor 540, a feedback-outputting unit 545, a memory 550b, a wireless\ntransceiver 530b, and a display 260.  One example of computing device 120 is a smartphone or tablet having a dedicated application installed therein.  In other embodiments, computing device 120 may include any configuration such as an on-board automobile\ncomputing system, a PC, a laptop, and any other system consistent with the disclosed embodiments.  In this example, user 100 may view feedback output in response to identification of a hand-related trigger on display 260.  Additionally, user 100 may view\nother data (e.g., images, video clips, object information, schedule information, extracted information, etc.) on display 260.  In addition, user 100 may communicate with server 250 via computing device 120.\nIn some embodiments, processor 210 and processor 540 are configured to extract information from captured image data.  The term \"extracting information\" includes any process by which information associated with objects, individuals, locations,\nevents, etc., is identified in the captured image data by any means known to those of ordinary skill in the art.  In some embodiments, apparatus 110 may use the extracted information to send feedback or other real-time indications to feedback outputting\nunit 230 or to computing device 120.  In some embodiments, processor 210 may identify in the image data the individual standing in front of user 100, and send computing device 120 the name of the individual and the last time user 100 met the individual. \nIn another embodiment, processor 210 may identify in the image data, one or more visible triggers, including a hand-related trigger, and determine whether the trigger is associated with a person other than the user of the wearable apparatus to\nselectively determine whether to perform an action associated with the trigger.  One such action may be to provide a feedback to user 100 via feedback-outputting unit 230 provided as part of (or in communication with) apparatus 110 or via a feedback unit\n545 provided as part of computing device 120.  For example, feedback-outputting unit 545 may be in communication with display 260 to cause the display 260 to visibly output information.  In some embodiments, processor 210 may identify in the image data a\nhand-related trigger and send computing device 120 an indication of the trigger.  Processor 540 may then process the received trigger information and provide an output via feedback outputting unit 545 or display 260 based on the hand-related trigger.  In\nother embodiments, processor 540 may determine a hand-related trigger and provide suitable feedback similar to the above, based on image data received from apparatus 110.  In some embodiments, processor 540 may provide instructions or other information,\nsuch as environmental information to apparatus 110 based on an identified hand-related trigger.\nIn some embodiments, processor 210 may identify other environmental information in the analyzed images, such as an individual standing in front user 100, and send computing device 120 information related to the analyzed information such as the\nname of the individual and the last time user 100 met the individual.  In a different embodiment, processor 540 may extract statistical information from captured image data and forward the statistical information to server 250.  For ex an) pie, certain\ninformation regarding the types of items a user purchases, or the frequency a user patronizes a particular merchant, etc. may be determined by processor 540.  Based on this information, server 250 may send computing device 120 coupons and discounts\nassociated with the user's preferences.\nWhen apparatus 110 is connected or wirelessly connected to computing device 120, apparatus 110 may transmit at least part of the image data stored in memory 550a for storage in memory 550b.  In some embodiments, after computing device 120\nconfirms that transferring the part of image data was successful, processor 540 may delete the part of the image data.  The term \"delete\" means that the image is marked as `deleted` and other image data may be stored instead of it, but does not\nnecessarily mean that the image data was physically removed from the memory.\nAs will be appreciated by a person skilled in the art having the benefit of this disclosure, numerous variations and/or modifications may be made to the disclosed embodiments.  Not all components are essential for the operation of apparatus 110. Any component may be located in any appropriate apparatus and the components may be rearranged into a variety of configurations while providing the functionality of the disclosed embodiments.  For example, in some embodiments, apparatus 110 may include a\ncamera, a processor, and a wireless transceiver for sending data to another device.  Therefore, the foregoing configurations are examples and, regardless of the configurations discussed above, apparatus 110 can capture, store, and/or process images.\nFurther, the foregoing and following description refers to storing and/or processing images or image data.  In the embodiments disclosed herein, the stored and/or processed images or image data may comprise a representation of one or more images\ncaptured by image sensor 220.  As the term is used herein, a \"representation\" of an image (or image data) may include an entire image or a portion of an image.  A representation of an image (or image data) may have the same resolution or a lower\nresolution as the image (or image data), and/or a representation of an image (or image data) may be altered in some respect (e.g., be compressed, have a lower resolution, have one or more colors that are altered, etc.).\nFor example, apparatus 110 may capture an image and store a representation of the image that is compressed as a .JPG file.  As another example, apparatus 110 may capture an image in color, but store a black-and-white representation of the color\nimage.  As yet another example, apparatus 110 may capture an image and store a different representation of the image (e.g., a portion of the image).  For example, apparatus 110 may store a portion of an image that includes a face of a person who appears\nin the image, but that does not substantially include the environment surrounding the person.  Similarly, apparatus 110 may, for example, store a portion of an image that includes a product that appears in the image, but does not substantially include\nthe environment surrounding the product.  As yet another example, apparatus 110 may store a representation of an image at a reduced resolution (i.e., at a resolution that is of a lower value than that of the captured image).  Storing representations of\nimages may allow apparatus 110 to save storage space in memory 550.  Furthermore, processing representations of images may allow apparatus 110 to improve processing efficiency and/or help to preserve battery life.\nIn addition to the above, in some embodiments, any one of apparatus 110 or computing device 120, via processor 210 or 540, may further process the captured image data to provide additional functionality to recognize objects and/or gestures\nand/or other information in the captured image data.  In some embodiments, actions may be taken based on the identified objects, gestures, or other information.  In some embodiments, processor 210 or 540 may identify in the image data, one or more\nvisible triggers, including a hand-related trigger, and determine whether the trigger is associated with a person other than the user to determine whether to perform an action associated with the trigger.\nSome embodiments of the present disclosure may include an apparatus securable to an article of clothing of a user.  Such an apparatus may include two portions, connectable by a connector.  A capturing unit may be designed to be worn on the\noutside of a user's clothing, and may include an image sensor for capturing images of a users environment.  The capturing unit may be connected to or connectable to a power unit, which may be configured to house a power source and a processing device. \nThe capturing unit may be a small device including a camera or other device for capturing images.  The capturing unit may be designed to be inconspicuous and unobtrusive, and may be configured to communicate with a power unit concealed by a user's\nclothing.  The power unit may include bulkier aspects of the system, such as transceiver antennas, at least one battery, a processing device, etc. In some embodiments, communication between the capturing unit and the power unit may be provided by a data\ncable included in the connector, while in other embodiments, communication may be wirelessly achieved between die capturing unit and the power unit.  Some embodiments may permit alteration of the orientation of an image sensor of the capture unit, for\nexample to better capture images of interest.\nFIG. 6 illustrates an exemplary embodiment of a memory containing software modules consistent with the present disclosure.  Included in memory 550 are orientation identification module 601, orientation adjustment module 602, and motion tracking\nmodule 603.  Modules 601, 602, 603 may contain software instructions for execution by at least one processing device, e.g., processor 210, included with a wearable apparatus.  Orientation identification module 601, orientation adjustment module 602, and\nmotion tracking module 603 may cooperate to provide orientation adjustment for a capturing unit incorporated into wireless apparatus 110.\nFIG. 7 illustrates an exemplary capturing unit 710 including an orientation adjustment unit 705.  Orientation adjustment unit 705 may be configured to permit the adjustment of image sensor 220.  As illustrated in FIG. 7, orientation adjustment\nunit 705 may include an eye-ball type adjustment mechanism.  In alternative embodiments, orientation adjustment unit 705 may include gimbals, adjustable stalks, pivotable mounts, and any other suitable unit for adjusting an orientation of image sensor\n220.\nImage sensor 220 may be configured to be movable with the head of user 100 in such a manner that, an aiming direction of image sensor 220 substantially coincides with a field of view of user 100.  For example, as described above, a camera\nassociated with image sensor 220 may be installed within capturing unit 710 at a predetermined angle in a position facing slightly upwards or downwards, depending on an intended location of capturing unit 710.  Accordingly, the set aiming direction of\nimage sensor 220 may match the field-of-view of user 100.  In some embodiments, processor 210 may change the orientation of image sensor 220 using image data provided from image sensor 220.  For example, processor 210 may recognize that a user is reading\na book and determine that the aiming direction of image sensor 220 is offset from the text.  That is because the words in the beginning of each line of text are not fully in view, processor 210 may determine that image sensor 220 is tilted in the wrong\ndirection.  Responsive thereto, processor 210 may adjust the aiming direction of image sensor 220.\nOrientation identification module 601 may be configured to identify an orientation of an image sensor 220 of capturing unit 710.  An orientation of an image sensor 220 may be identified, for example, by analysis of images captured by image\nsensor 220 of capturing unit 710S by tilt or attitude sensing devices within capturing unit 710, and by measuring a relative direction of orientation adjustment unit 705 with respect to the remainder of capturing unit 710.\nOrientation adjustment, module 602 may be configured to adjust an orientation of image sensor 220 of capturing unit 710.  As discussed above, image sensor 220 may be mounted on an orientation adjustment unit 705 configured for movement. \nOrientation adjustment unit 705 may be configured for rotational and/or lateral movement in response to commands from orientation adjustment module 602.  In some embodiments orientation adjustment unit 705 may be adjust an orientation of image sensor 220\nvia motors, electromagnets, permanent magnets, and/or any suitable combination thereof.\nIn some embodiments, monitoring module 603 may be provided for continuous monitoring.  Such continuous monitoring may include tracking a movement of at least a portion of an object included in one or more images captured by the image sensor. \nFor example, in one embodiment, apparatus 110 may track an object as long as the object remains substantially within the field-of-view of image sensor 220.  In additional embodiments, monitoring module 603 may engage orientation adjustment module 602 to\ninstruct orientation adjustment unit 705 to continually orient image sensor 220 towards an object of interest.  For example, in one embodiment, monitoring module 603 may cause image sensor 220 to adjust an orientation to ensure that a certain designated\nobject, for example, the face of a particular person, remains within the field-of view of image sensor 220, even as that designated object moves about.  In another embodiment, monitoring module 603 may continuously monitor an area of interest included in\none or more images captured by the image sensor.  For example, a user may be occupied by a certain task, for example, typing on a laptop, while image sensor 220 remains oriented in a particular direction and continuously monitors a portion of each image\nfrom, a series of images to detect a trigger or other event.  For example, image sensor 210 may be oriented towards apiece of laboratory equipment and monitoring module 603 may be configured to monitor a status light on the laboratory equipment for a\nchange in status, while the user's attention is otherwise occupied.\nIn some embodiments consistent with the present disclosure, capturing unit 710 may include a plurality of image sensors 220.  The plurality of image sensors 220 may each be configured to capture different image data.  For example, when a\nplurality of image sensors 220 are provided, the image sensors 220 may capture images having different resolutions, may capture wider or narrower fields of view, and may have different levels of magnification.  Image sensors 220 may be provided with\nvarying lenses to permit these different configurations.  In some embodiments, a plurality of image sensors 220 may include image sensors 220 having different orientations.  Thus, each of the plurality of image sensors 220 may be pointed in a different\ndirection to capture different images.  The fields of view of image sensors 220 may be overlapping in some embodiments.  The plurality of image sensors 220 may each be configured for orientation adjustment, for example, by being paired with an image\nadjustment unit 705.  In some embodiments, monitoring module 603, or another module associated with memory 550, may be configured to individually adjust the orientations of the plurality of image sensors 220 as well as to turn each of the plurality of\nimage sensors 220 on or off as may be required.  In some embodiments, monitoring an object or person captured by an image sensor 220 may include tracking movement of the object across the fields of view of the plurality of image sensors 220.\nEmbodiments consistent with the present disclosure may include connectors configured to connect a capturing unit and a power unit of a wearable apparatus.  Capturing units consistent with the present disclosure may include least one image sensor\nconfigured to capture images of an environment of a user.  Power units consistent with the present disclosure may be configured to house a power source and/or at least one processing device.  Connectors consistent with the present disclosure may be\nconfigured to connect the capturing unit and the power unit, and may be configured to secure the apparatus to an article of clothing such that the capturing unit is positioned over an outer surface of the article of clothing and the power unit is\npositioned under an inner surface of the article of clothing.  Exemplary embodiments of capturing units, connectors, and power units consistent with the disclosure are discussed in further detail with respect to FIGS. 8-14.\nFIG. 8 is a schematic illustration of an embodiment of wearable apparatus 110 securable to an article of clothing consistent with the present disclosure.  As illustrated in FIG. 8, capturing unit 710 and power unit 720 may be connected by a\nconnector 730 such that capturing unit 710 is positioned on one side of an article of clothing 750 and power unit 720 is positioned on the opposite side of the clothing 750.  In some embodiments, capturing unit 710 may be positioned over an outer surface\nof the article of clothing 750 and power unit 720 may be located under an inner surface of the article of clothing 750.  The power unit 720 may be configured to be placed against the skin of a user.\nCapturing unit 710 may include an image sensor 220 and an orientation adjustment unit 705 (as illustrated in FIG. 7).  Power unit 720 may include mobile power source 520 and processor 210.  Power unit 720 may further include any combination of\nelements previously discussed that may be a part of wearable apparatus 110, including, but not limited to, wireless transceiver 530, feedback outputting unit 230, memory 550, and data port 570.  Connector 730 may include a clip 715 or other mechanical\nconnection designed to clip or attach capturing unit 710 and power unit 720 to an article of clothing 750 as illustrated in FIG. 8.  As illustrated, clip 715 may connect to each of capturing unit 710 and power unit 720 at a perimeter thereof, and may\nwrap around an edge of the article of clothing 750 to affix the capturing unit 710 mid power unit 720 in place.  Connector 730 may further include a power cable 760 and a data cable 770.  Power cable 760 may be capable of conveying power from mobile\npower source 520 to image sensor 220 of capturing unit 710.  Power cable 760 may also be con figured to provide power to any other elements of capturing unit 710, e.g., orientation adjustment unit 705.  Data cable 770 may be capable of conveying captured\nimage data from image sensor 220 in capturing unit 710 to processor 800 in the power unit 720.  Data cable 770 may be further capable of conveying additional data between capturing unit 710 and processor 800, e.g., control instructions for orientation\nadjustment unit 705.\nFIG. 9 is a schematic illustration of a user 100 wearing a wearable apparatus 110 consistent with an embodiment of the present disclosure.  As illustrated in FIG. 9, capturing unit 710 is located on an exterior surface of the clothing 750 of\nuser 100.  Capturing unit 710 is connected to power unit 720 (not seen in this illustration) via connector 730, which wraps around an edge of clothing 750.\nIn some embodiments, connector 730 may include a flexible printed circuit board (PCB).  FIG. 10 illustrates an exemplary embodiment wherein connector 730 includes a flexible printed circuit board 765.  Flexible printed circuit board 765 may\ninclude data connections and power connections between capturing unit 710 and power unit 720.  Thus, in some embodiments, flexible printed circuit board 765 may serve to replace power cable 760 and data cable 770.  In alternative embodiments, flexible\nprinted circuit board 765 may be included in addition to at least one of power cable 760 and data cable 770.  In various embodiments discussed herein, flexible printed circuit board 765 may be substituted for, or included in addition to, power cable 760\nand data cable 770.\nFIG. 11 is a schematic illustration of another embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.  As illustrated in FIG. 11, connector 730 may be centrally located with respect to\ncapturing unit 710 and power unit 720.  Central location of connector 730 may facilitate affixing apparatus 110 to clothing 750 through a hole in clothing 750 such as, for example, a button-hole in an existing article of clothing 750 or a specialty hole\nin an article of clothing 750 designed to accommodate wearable apparatus 110.\nFIG. 12 is a schematic illustration of still another embodiment of wearable apparatus 110 securable to an article of clothing.  As illustrated in FIG. 12, connector 730 may include a first magnet 731 and a second magnet 732.  First magnet 731\nand second magnet 732 may secure capturing unit 710 to power unit 720 with the article of clothing positioned between first magnet 731 and second magnet 732.  In embodiments including first magnet 731 and second magnet 732, power cable 760 and data cable\n770 may also be included.  In these embodiments, power cable 760 and data cable 770 may be of any length, and may provide a flexible power and data connection between capturing unit 710 and power unit 720.  Embodiments including first magnet 731 and\nsecond magnet 732 may further include a flexible PCB 765 connection in addition to or instead of power cable 760 and/or data cable 770.  In some embodiments, first magnet 731 or second magnet 732 may be replaced by an object comprising a metal material.\nFIG. 13 is a schematic illustration of yet another embodiment of a wearable apparatus 110 securable to an article of clothing.  FIG. 13 illustrates an embodiment wherein power and data may be wirelessly transferred between capturing unit 710 and\npower unit 720.  As illustrated in FIG. 13, first magnet 731 and second magnet 732 may be provided as connector 730 to secure capturing unit 710 and power unit 720 to an article of clothing 750.  Power and/or data may be transferred between capturing\nunit 710 and power unit 720 via any suitable wireless technology, for example, magnetic and/or capacitive coupling, near field communication technologies, radiofrequency transfer, and any other wireless technology suitable for transferring data and/or\npower across short distances.\nFIG. 14 illustrates still another embodiment of wearable apparatus 110 securable to an article of clothing 750 of a user.  As illustrated in FIG. 14, connector 730 may include features designed for a contact fit.  For example, capturing unit 710\nmay include a ring 733 with a hollow center having a diameter slightly larger than a disk-shaped protrusion 734 located on power unit 720.  When pressed together with fabric of an article of clothing 750 between them, disk-shaped protrusion 734 may fit\ntightly inside ring 733, securing capturing unit 710 to power unit 720.  FIG. 14 illustrates an embodiment that does not include any cabling or other physical connection between capturing unit 710 and power unit 720.  In this embodiment, capturing unit\n710 and power unit 720 may transfer power and data wirelessly.  In alternative embodiments, capturing unit 710 and power unit 720 may transfer power and data via at least one of cable 760, data cable 770, and flexible printed circuit board 765.\nFIG. 15 illustrates another aspect of power unit 720 consistent with embodiments described herein.  Power unit 720 may be configured to be positioned directly against the user's skin.  To facilitate such positioning, power unit 720 may further\ninclude at least one surface coated with a biocompatible material 740.  Biocompatible materials 740 may include materials that will not negatively react with the skin of the user when worn against the skin for extended periods of time.  Such materials\nmay include, for example, silicone, PTFE, kapton, polyimide, titanium, nitinol, platinum, and others.  Also as illustrated in FIG. 15, power unit 720 may be sized such that an inner volume of the power unit is substantially filled by mobile power source\n520.  That is, in some embodiments, the inner volume of power unit 720 may be such that the volume does not accommodate any additional components except for mobile power source 520.  In some embodiments, mobile power source 520 may take advantage of its\nclose proximity to the skin of user's skin.  For example, mobile power source 520 may use the Peltier effect to produce power and/or charge the power source.\nIn further embodiments, an apparatus securable to an article of clothing may further include protective circuitry associated with power source 520 housed in in power unit 720.  FIG. 16 illustrates an exemplary embodiment including protective\ncircuitry 775.  As illustrated in FIG. 16, protective circuitry 775 may be located remotely with respect to power unit 720.  In alternative embodiments, protective circuitry 775 may also be located in capturing unit 710, on flexible printed circuit board\n765, or in power unit 720.\nProtective circuitry 775 may be configured to protect image sensor 220 and/or other elements of capturing unit 710 from potentially dangerous currents and/or voltages produced by mobile power source 520.  Protective circuitry 775 may include\npassive components such as capacitors, resistors, diodes, inductors, etc., to provide protection to elements of capturing unit 710.  In some embodiments, protective circuitry 775 may also include active components, such as transistors, to provide\nprotection to elements of capturing unit 710.  For example, in some embodiments, protective circuitry 775 may comprise one or more resistors serving as fuses.  Each fuse may comprise a wire or strip that melts (thereby braking a connection between\ncircuitry of image capturing unit 710 and circuitry of power unit 720) when current flowing through the fuse exceeds a predetermined limit (e.g., 500 milliamps, 900 milliamps, 1 amp, 11 amps, 2 amp, 21 amps, 3 amps, etc.) Any or all of the previously\ndescribed embodiments may incorporate protective circuitry 775.\nIn some embodiments, the wearable apparatus may transmit data to a computing device (e.g., a smartphone, tablet, watch, computer, etc.) over one or more networks via any known wireless standard (e.g., cellular, Wi-Fi, Bluetooth.RTM., etc.), or\nvia near-Hied capacitive coupling, other short range wireless techniques, or via a wired connection.  Similarly, the wearable apparatus may receive data from the computing device over one or more networks via any known wireless standard (e.g., cellular,\nWi-Fi, Bluetooth.RTM., etc.), or via near-filed capacitive coupling, other short range wireless techniques, or via a wired connection.  The data transmitted to the wearable apparatus and/or received by the wireless apparatus may include images, portions\nof images, identifiers related to information appearing in analyzed images or associated with analyzed audio, or any other data representing image and/or audio data.  For example, an image may be analyzed and an identifier related to an activity\noccurring in the image may be transmitted to the computing device (e.g., the \"paired device\").  In the embodiments described herein, the wearable apparatus may process images and/or audio locally (on board the wearable apparatus) and/or remotely (via a\ncomputing device).  Further, in the embodiments described herein, the wearable apparatus may transmit data related to the analysis of images and/or audio to a computing device for further analysis, display, and/or transmission to another device (e.g., a\npaired device).  Further, a paired device may execute one or more applications (apps) to process, display, and/or analyze data (e.g., identifiers, text, images, audio, etc.) received from the wearable apparatus.\nSome of the disclosed embodiments may involve systems, devices, methods, and software products for determining at least one keyword.  For example, at least one keyword may be determined based on data collected by apparatus 110.  At least one\nsearch query may be determined based on the at least one keyword.  The at least one search query may be transmitted to a search engine.\nIn some embodiments, at least one keyword may be determined based on at least one or more images captured by image sensor 220.  In some cases, the at least one keyword may be selected from a keywords pool stored in memory.  In some cases,\noptical character recognition (OCR) may be performed on at least one image captured by image sensor 220, and the at least one keyword may be determined based on the OCR result.  In some cases, at least one image captured by image sensor 220 may be\nanalyzed to recognize: a person, an object, a location, a scene, and so forth.  Further, the at least one keyword may be determined based on the recognized person, object, location, scene, etc. For example, the at least one keyword may comprise: a\nperson's name, an object's name, a place's name, a date, a sport team's name, a movie's name, a book's name, and so forth.\nIn some embodiments, at least one keyword may be determined based on the user's behavior.  The user's behavior may be determined based on an analysis of the one or more images captured by image sensor 220.  In some embodiments, at least one\nkeyword may be determined based on activities of a user and/or other person.  The one or more images captured by image sensor 220 may be analyzed to identify the activities of the user and/or the other person who appears in one or more images captured by\nimage sensor 220.  In some embodiments, at least one keyword may be determined based on at least one or more audio segments captured by apparatus 110.  In some embodiments, at least one keyword may be determined based on at least GPS information\nassociated with the user.  In some embodiments, at least one keyword may be determined based on at least the current time and/or date.\nIn some embodiments, at least one search query may be determined based on at least one keyword.  In some cases, the at least one search query may comprise the at least one keyword.  In some cases, the at least one search query may comprise the\nat least one keyword and additional keywords pro vided by the user.  In some cases, the at least one search query may comprise the at least one keyword and one or more images, such as images captured by image sensor 220.  In some cases, the at least one\nsearch query may comprise the at least one keyword and one or more audio segments, such as audio segments captured by apparatus 110.\nIn some embodiments, the at least one search query may be transmitted to a search engine.  In some embodiments, search results provided by the search engine in response to the at least one search query may be provided to the user.  In some\nembodiments, the at least one search query may be used to access a database.\nFor example, in one embodiment, the keywords may include a name of a type of food, such as quinoa, or a brand name of a food product; and the search will output information related to desirable quantities of consumption, facts about the\nnutritional profile, and so forth.  In another example, in one embodiment, the keywords may include a name of a restaurant, and the search will output information related to the restaurant, such as a menu, opening hours, reviews, and so forth.  The name\nof the restaurant may be obtained using OCR on an image of signage, using GPS information, and so forth.  In another example, in one embodiment, the keywords may include a name of a person, and the search will provide information from a social network\nprofile of the person.  The name of the person may be obtained using OCR on an image of a name tag attached to the person's shirt, using face recognition algorithms, and so forth.  In another example, in one embodiment, the keywords may include a name of\na book, and the search will output information related to the book, such as reviews, sales statistics, information regarding the author of the book, and so forth.  In another example, in one embodiment, the keywords may include a name of a movie, and the\nsearch will output information related to the movie, such as reviews, box office statistics, information regarding the cast of the movie, show times, and so forth.  In another example, in one embodiment, the keywords may include a name of a sport team,\nand the search will output information related to the sport team, such as statistics, latest results, future schedule, information regarding the players of the sport team, and so forth.  For example, the name of the sport team may be obtained using audio\nrecognition algorithms.\nIn some embodiments, wearable apparatus 110 may cause a paired device, such as computing device 120, to execute a search query based on one or more keywords determined from an identified contextual situation.  Contextual situations may refer to\na combination of circumstances that may influence a user's actions, as described below.\nIn some embodiments, wearable apparatus 110 may be used to control a controllable device, as described below.  For example, the controllable device may include a screen in an environment of wearable apparatus 110, and be configured to display\nsearch results of a search query that is based on at least one keyword determined by wearable apparatus 110.  Wearable apparatus 110 may cause the controllable device to browse through the search results, for example, in response to one or more hand\ngestures detected by wearable apparatus 110.\nIn some embodiments, wearable apparatus 110 may enter a privacy mode in certain situations, as described below.  In some examples, the privacy mode may control what keywords and/or search queries are transmitted and/or executed and/or presented. For example, some private search queries may be withheld when other persons are in the vicinity of the user of wearable apparatus 110, when children are in the vicinity of the user of wearable apparatus 110, and so forth.  In some examples, based on\nwearable apparatus 110 entering a privacy mode, some search queries may be performed in an incognito or a private browsing mode.  In some examples, based on wearable apparatus 110 entering a privacy mode, keywords may or may not be posted on a social\nmedia profile.\nTrigging Selected Functions Using Image Analysis\nIn some embodiments, wearable apparatus 110 may cause a paired device, such as computing device 120, to execute a selected function based on information determined from an identified contextual situation.  Contextual situations may refer to a\ncombination of circumstances that may influence a user's actions.  Examples of factors that may differentiate contextual situations include: the identity of other people in the vicinity of user 100 (e.g., certain individual, family members, coworkers,\nstrangers, and more), the type of activity user 100 is engaged (e.g., watching a movie, meeting with an individual, visiting a location, interacting with an object, entering a car, participating in a sport activity, eating a meal, and more), the time in\nwhich the situation took place (e.g., the time of the day, the time of the year, and more), the location in which the situation occurs (e.g., home, working place, shopping mall, and more).\nUser 100 may encounter a significant number of diverse types of contextual situations during the course of a given day.  Identifying and transmitting information about all the contextual situations that user 100 encounters throughout his/her day\nmay drain the battery power of wearable apparatus 110.  To address this drawback, wearable apparatus 110 may determine and transmit information only when encountering contextual situations that user 100 is interested in. However, determining which\ncontextual situations are of interest to user 100 may be complex.  For example, one user may want to know about the food he/she ate, another user may want to know about people he/she met, and yet another user may want to track activities that he/she\nparticipated in. Therefore, wearable apparatus 110 may enable user 100 and/or paired devices to dynamically indicate the contextual situations he/she is interested in, and computing device 120 may execute a selected function after user 100 encounters the\nselected contextual situation.  In another example, at one time and/or after detecting a particular context, a paired device (e.g., computing device 120) may execute one or more applications that use information related to one set of contextual\nsituations, while executing applications that use information related to a second set of contextual situations at another time and/or alter detecting a particular context.  In another example, a first, paired device may execute one or more applications\nthat use information related to one set of contextual situations, while a second paired device may execute one or more applications that use information related to a second set of contextual situations, and wearable apparatus 110 may transmit different\ninformation to different paired devices according to the contextual situations.\nOne way for wearable apparatus 110 to determine which contextual situations are of interest to user 100 is using category tags.  Category tags may include digital data that characterizes contextual situations.  The characterization of a\ncontextual situation may be general (e.g., a type of activity, a general location, a typo of product, etc.) or may be more specific (e.g., a mime of a person, an address, a name of a product, etc.).\nOne or more category tags may be assigned to an image based on the content identified in the image and/or based on metadata information (e.g., location, time) associated with the image.  One skilled in the art will appreciate that any image\nassociated with at least one contextual situation may include a plurality of category tags.\nIn one embodiment, wearable apparatus 110 may receive a request from computing device 120 to transmit information associated with a particular category tag.  The request may include a specific combination of circumstances and/or factors that\ndefines the looked-for contextual situation associated with that category tag.  For example, user 100 may want to keep records of his encounters with certain individuals outside working hours.  In some embodiments, the request may be provided by an\napplication executing on a device remote from wearable apparatus 110 (e.g., a smartphone, tablet, or smart watch paired with wearable device 110, or a server in communication with wearable device 110).  For example, user 100 may make use of an\napplication or have registered with an application to keep track of his encounters with other people.  As part of configuring user preferences with such an application, user 100 may have specified an interest, in particular, of tracking encounters with\nco-workers outside of working hours.  The configuring may involve, for example, selecting and/or enabling functionality to consider images that have been classified (e.g., via category tags) related to co-workers (e.g., certain identified individuals)\nand a particular time period (e.g., outside of typical working hours).\nIn operation, wearable apparatus 110 may associate a category tag with at least one image obtained from image sensor 220.  With regards to the example above, an image captured after 6 p.m.  that includes at least one of the identified\nindividuals may be associated with a category lag such, as for example, \"Hanging out with my coworkers.\" The category tag may also be associated with a selected function that a paired device may execute.  The selected function associated with a category\ntag may be to store information (e.g., when, where, how long, who was there) about the detected contextual situation.  With regards to the example above, wearable apparatus 110 may process images with the category tag \"Hanging out with my coworkers\" to\ndetermine information about the encounter and to transmit the information to computing device 120 to cause computing device 120 to store the information in memory 550b and/or in a remote server.  Computing device 102 and/or the remote server may then\nprocess the received information and provide feedback to user 100 (e.g., a summary of co-workers that user 100 encounter after 6 p.m.  Additional exemplary embodiments of contextual situations, category tags, and the types of selected functions that may\nbe triggered are discussed in further detail with respect to FIGS. 17A-17D.\nFIG. 17A is a schematic illustration of a contextual situation that may trigger computing device 120 to execute one or more selected functions consistent with the present disclosure.  The contextual situation illustrated in this figure is a\nperson present in an area in front of user 100.  Wearable apparatus 110 may identify this contextual situation by analyzing one or more images, such as image 1710.  After identifying this contextual situation, wearable apparatus 110 may cause computing\ndevice 120 to execute the one or more selected functions.\nIn one embodiment, the processing device may receive a request from computing device 120 to transmit image-related information associated with the category tag.  The requested image-related information may include an image of the person, and the\nselected function may include providing information about the person included in the image to user 100.  For example, assuming user 100 is registered to an online dating service and previously selected that he/she is interested in blond women.  The\ndating application may communicate with wearable apparatus 110 and request facial images of any unknown blond women encountered by user 100.\nAs shown, the category tags 1700 that may be assigned to image 1710 may include a \"new people\" tag, a \"coffee shop\" tag, and a \"dating app\" tag.  In this case, the \"new people\" tag was assigned to image 1710 because wearable apparatus 110 does\nnot recognize the woman in front of user 100; the \"coffee shop\" tag was assigned to image 1710 based on, for example, information from a Global Positioning System (GPS); and the \"dating app\" tag was assigned to image 1710 because the woman in front of\nuser 100 is blond.  Thereafter, wearable apparatus 110 may transmit an image including the woman's face to computing device 120.  The dating application installed on the user's smartphone may search for the image and find that the image is of a woman who\nis registered to a dating service.  Computing device 120 may then send the woman's profile page 1715 to user 100.\nAlthough the above example relates to identifying a person for purposes of retrieving a dating profile, the \"new people\" tag may be used by other applications.  For example, applications requesting images tagged with a \"new people\" tag may\ninclude those requesting images of unidentified persons to check whether a person included in an image may be a missing person or a person who is wanted by law enforcement.  Accordingly, computing device 120 may provide an alert related to a\nmissing-persons status of the person, or an alert related to a wanted-by-law-enforcement status of the person.  Further, in some embodiments, wearable apparatus 110 may be used by law enforcement officers to receive real-time indications about wanted\nsuspects in present in their surroundings on a paired device (e.g., an alert as to a wanted status or that an encountered individual has a criminal record).\nFIG. 17B is a schematic illustration of another contextual situation that may trigger computing device 120 to execute one or more selected functions consistent with the present disclosure.  The contextual situation illustrated in this figure is\nan object present in an area in front of user 100.  Wearable apparatus 110 may identify this contextual situation by analyzing one or more images, such as image 1720.  After identifying this contextual situation, wearable apparatus 110 may cause\ncomputing device 120 to execute the one or more selected functions.\nIn one embodiment, computing device 120 may request image-related information that includes at least one detail about the object.  For example, the object may be a business card and the at least one detail may include contact information.  In\nthe contextual situation illustrated in FIG. 17B, user 100 is registered to a business-oriented social networking service and had installed an application of this service on his/her smartphone.  The installed application may communicate with wearable\napparatus 110 and request contact information of any business card that user 100 holds that is not his/her own.  As shown, the category tags 1700 that may be assigned to image 1720 may include an \"objects\" lag, a \"business card\" tag, and a \"business\nconnection website\" tag.  In this case, as \"the business connection website\" tag was assigned to image 1720 because the business card that user 100 holds is other than his/her own, wearable apparatus 110 may transmit the contact information of John Doe\nlisted on the business card to computing device 120 to execute a selected function.  In one embodiment, the selected function may include storing at least one detail from the business card.  The at least one detail may be obtained by, for example,\nexecuting an optical character recognition (OCR) function.  Computing device 120 may then store the contact information of John Doe in memory 550b.  In another embodiment, the selected function may include opening a website and automatically including at\nleast one detail from the business card in, for example, a profile (e.g., a social networking site profile) of user 100.  For example, after receiving the contact, information of John Doe, computing device 120 may enable user 100 to connect with John Doe\nvia a business connection website.\nIn other embodiments, computing device 120 may request other details such as a product type or a product name.  For example, user 100 may walk in a store and look at a product, and wearable apparatus 110 may be programmed to transmit the product\ntype or the product name to computing device 120.  For example, wearable apparatus 110 may provide an image of the product and/or wearable apparatus 110 may execute an OCR function to identify the product name from a label included in an image.  In this\nexample, the selected function may include opening a website that compares prices, which may automatically search the product name to enable user 100 to review the prices of the same product in other stores.\nFIG. 17C is a schematic illustration of yet another contextual situation that may trigger computing device 120 to execute one or more selected functions consistent with the present disclosure.  The contextual situation illustrated in this figure\nis the presence of food in front of user 100.  Wearable apparatus 110 may identify this contextual situation by analyzing one or more images, such as image 1730.  After identifying this contextual situation, wearable apparatus 110 may cause computing\ndevice 120 to execute one or more selected functions.\nIn the contextual situation illustrated in FIG. 17C, computing device 120 may request image-related information that includes an indication that user 100 is engaging with food associated with a dietary restriction.  In one embodiment, the\nselected function may include updating a database with information related to the food associated with the dietary restriction.  In another embodiment, the selected function may include providing an alert 1735 regarding the food in front of user 100. \nFor example, the requested image-related information may include an indication that user 100 is engaging with food associated with a dietary restriction, such as a food allergy.  In this example, user 100 may have previously installed an application on a\npaired device that maintains a list of known ingredients that user 100 is allergic to.  The installed application may communicate with wearable apparatus 110 and request images of any packaged food that user 100 is engaged with.  As shown, the category\ntags 1700 that may be assigned to image 1730 may include a \"food\" tag, a \"lunch\" tag, and an \"allergy check\" tag.  When processor 210 detects that user 100 is engaging with the packaged food, wearable apparatus 110 may transmit an image of the packaged\nfood to computing device 120.  The installed application may search an offline database or an online database to determine if the packaged food that user 100 is engaging with includes an ingredient that may be dangerous to user 100 and provide a\nreal-time warning to user 100.  In other embodiments, the requested image-related information may include an indication that user 100 is engaging with food associated with a dietary restriction, where the dietary restriction relates to a disease (e.g.,\ndiabetes) or a weight loss goal.\nFIG. 17D is a schematic illustration of still another contextual situation that may trigger computing device 120 to execute one or more selected functions consistent with the present disclosure.  The contextual situation illustrated in this\nfigure is performing a health-related activity.  Wearable apparatus 110 may identify this contextual situation by analyzing a one or more images, such as image 1740.  After identifying this contextual situation, wearable apparatus 110 may cause computing\ndevice 120 to execute one or more selected functions.\nIn the contextual situation illustrated in FIG. 17D, computing device 120 may request image-related information that includes an indication that user 100 is engaged in the health-related activity.  In one embodiment, the selected function may\ninclude providing information to an individual related to user 100.  The individual related to user 100 may be the user's physician or a family member.  For example, the information provided to the individual related to user 100 may include a medicine\nreminder concerning the user and an alert related to a health status of user 100.  In another embodiment, the selected function may include providing information associated with the health-related activity to user 100.  The health-related activity may\ninclude: taking medicine, performing a physical exercise, receiving a treatment, and more.  In the example illustrated in FIG. 17D, user 100 installed an application on a paired device that monitors whether he/she takes all his/her medicine on time.  The\ninstalled application may communicate with wearable apparatus 110 and request a notification if user 100 takes medicine.  As shown, the category tags 1700 that may be assigned to image 1740 may include a \"drinking water\" tag and \"health monitor\" tag. \nWhen processor 210 detects that user 100 is engaging with the health-related activity, wearable apparatus 110 may transmit an indication to computing device 120.  The indication may include an image of the medicine.  The installed application may check\nthat user 100 is about to take the correct medicine as admitted and provide a real-time feedback 1745 to user 100.\nA person skilled in the art can appreciate that many more types of contextual situations (not shown in the figures) may trigger computing device 120 to execute one or more selected functions consistent with the present disclosure.  In addition,\nwhile specific selected functions were described with regards to certain types of contextual situations, the present disclosure is not limited to the disclosed examples.  As described below, any combination of selected functions may be triggered in\nresponse to any contextual situations, and any number of separated operations may be included in a selected function.\nFurthermore, in some embodiments, for example, analyzing one or more images captured by wearable apparatus 110 may involve edge identification, in which an image is analyzed to detect pixels at which discontinuities (e.g., sudden changes in\nimage brightness) occur and edges (e.g., edges of the external object) are identified to coincide with the detected pixels.  Alternatively or additionally, in some embodiments analyzing one or more images may involve identifying in and/or extracting from\nan image pixels representative of objects in the environment, such as the external object.  Pixels may be determined to be representative of an external object based on, for example, other images of the external device, or similar external devices\nmaintained, e.g., in a database and/or predetermined data describing the external object maintained, e.g., in a database.  Alternatively or additionally, pixels may be determined, to be representative of an external object based on, for example, a\ntrained neural network configured to detect predetermined external objects.  Other types of analysis are possible as well, including, but not limited to, gradient matching, grey scale matching, scale-invariant feature transform (SIFT) matching, and/or\ninterpretation trees.\nFIG. 18A is a message flow diagram that depicts the communication between wearable apparatus 110 and computing device 120 for three different types of selected functions.  The first type of selected functions involves computing device 120. \nSpecifically, computing device 120 may send wearable apparatus 110 a request 1800 that defines a contextual situation.  After wearable apparatus 110 detects the contextual situation, it may send image-related information 1810 to computing device 120. \nExamples for the first type of selected function include: storing data on memory 550b (e.g., counting the occurrences of a certain event) and providing alerts to user 100 (e.g., as illustrated in FIGS. 29A-29D).  The second type of selected functions\ninvolves computing device 120 communicating with server 250.  Specifically, after computing device 120 receives image-related information 1810, it may transmit information 1820 to server 250.  Information 1820 may be identical to image-related\ninformation 1810 or data derived from image-related information 1810.  Examples of the second type of selected function include: updating a database (e.g., updating a cloud-based life logging service), forwarding a message to an individual (e.g.,\ninforming a parent that a child is engaging in a dangerous activity), opening a website (e.g., opening a weather forecast website when user 100 looks at his/her umbrella).  The third type of selected functions involves retrieving information from server\n250.  Specifically, after computing device 120 receives image-related information 1810, it may transmit an inquiry 1830 to server 250, and receive back information 1840 to be presented to user 100 via computing device 120.  Examples for this type of\nselected function include: providing information about a person or an object in the environment of user 100 (e.g., as illustrated in FIG. 17A and FIG. 17C).\nFIG. 18B is a schematic illustration of three different types of scenarios where wearable apparatus 110 may trigger multiple selected functions.  In the first scenario, wearable apparatus 110 may receive multiple requests from a paired device\n(e.g., computing device 120).  Each of the requests may be associated with a different contextual situation and trigger a different selected function.  Specifically, a processing device (e.g., processor 210) may receive a first request 1800A and a second\nrequest 1800B from computing device 120 to transmit image-related information associated with first and second category tags.  The first category tag is associated with a first selected function and the second category tag is associated with a second\nselected function.  Thereafter, the processing device may analyze obtained images to detect first and second contextual situations.  Based on the detected first and second contextual situations, the processing device may associate at least one image with\nthe first category tag and at least one other image with the second category tag.  The processing device may also determine first image-related information 1810A associated with the detected first contextual situation and the first category tag, and\nsecond image-related information 1810B associated with the detected second contextual situation and the second category tag.  The processing device may further cause a transmitter (e.g., transceiver 530) to transmit first image-related information 1810A\nand second image-related information 1810B to computing device 120 to cause computing device 120 to execute the first and second selected functions based on the determined image-related information.  For example, the first contextual situation may be a\nperson present in an area in front of user 100 (as illustrated in FIG. 17A), and the second contextual situation may be the presence of food present in front of user 100 (as illustrated in FIG. 17C).\nIn the second scenario, wearable apparatus 110 may receive multiple requests from multiple paired devices.  Bach of the requests may be associated with a different contextual situation and may trigger a different, selected function. \nSpecifically, the processing device may receive first request 1800A from first computing device 120A and second request 1800B from second computing device 120B.  Thereafter, the processing device may analyze obtained images to detect the first and the\nsecond contextual situations.  Based on the detected first and second contextual situations, the processing device may associate at least one image with the first category tag and at least one other image, with the second category tag.  The processing\ndevice may also determine first image-related information 1810A associated with the first contextual situation and the first category tag, and second image-related information 1810B associated with the second contextual situation and the second category\ntag.  The processing device may further cause a transmitter (e.g., transceiver 530) to transmit first image-related information 1810A to first computing device 120A and to transmit second image-related information 1810B to second computing device 120B,\nand thereby cause first computing device 120A to execute the first selected function based on first image-related information 1810A, and second computing device 120B to execute the second selected function based on second image-related information 1810B. For example, the first computing device 120A may be the user's smartphone and the second computing device 120B may be the user's smartwatch.  Accordingly, a warning that user 100 is engaging with food including a dangerous ingredient may be provided to\nthe user's smartwatch, and information about a person in front of user 100 may be provided to the user's smartphone.\nIn the third scenario, wearable apparatus 110 may receive multiple requests from multiple paired devices.  The requests may be associated with the same contextual situation, but may trigger different selected functions.  Specifically, the\nprocessing device may receive first request 1800A from first computing device 120A and second request 1800B from second computing device 120B.  Thereafter, the processing device may analyze obtained images to detect a contextual situation.  Based on the\ndetected contextual situation, the processing device may associate at least one image with the first category tag and the second category tag.  The processing device may also determine first image-related information 1810A associated with the detected\ncontextual situation and the first category tag, and second image-related information 1810B associated with the detected contextual situation and the second category tag.  The processing device may further cause the transmitter to transmit first\nimage-related information 1810A to first computing device 120A and second image-related information 1810B to second computing device 120B, and thereby cause first computing device 120A to execute the first selected function based on first image-related\ninformation 1810A, and second computing device 120B to execute the second selected function based on second image-related information 1810B.  For example, if the contextual situation includes the presence of food including an ingredient that may be\ndangerous, the user's smart watch may provide a real-time warning to user 100, while the user's smartphone may provide information to an individual related to user 100.  In some embodiments, wearable apparatus 110 may cause the first selected function\nand the second selected function to be executed concurrently or subsequently.  The term \"concurrently\" means that the two selected functions occur during coincident or overlapping time periods, either where one begins and ends during the duration of the\nother, or where a later one starts before the completion of the other.  In another scenario (not illustrated in the figure) a single paired device may execute concurrently or subsequently multiple selected functions based on a single contextual\nsituation.\nFIG. 19 is a flowchart showing an exemplary process 1900 for causing computing device 120 to execute a selected function based on information determined from an identified contextual situation, consistent with disclosed embodiments.  Wearable\napparatus 110 may implement process 1900 to trigger one or more selected functions based on different contextual situations, for example, as illustrated in FIGS. 17A-17D.\nAs illustrated in FIG. 19, at step 1910, a processing device (e.g., processor 210) may receive a request from computing device 120 to transmit information associated with a category tag.  At step 1920, the processing device may obtain at least\none image and at step 1930 the processing device may analyze the at least one image to detect a contextual situation associated with the at least one image.  At step 1940, processing device may determine image-related information associated with the\ndetected contextual situation.  And at step 1950, the processing device may cause the paired device to execute the selected function based on the determined image-related information.  These steps of process 1900 are discussed in greater detail below.\nSpecifically, at step 1910, the processing device may receive a request from computing device 120 to transmit information associated with a category tag.  The request may be generated by user 100 interacting with an application running on\ncomputing device 120 or generated by computing device 120 without a direct interaction of user 100.  In some embodiments, the category tag may be associated with a selected function.  In a first example, when the selected function includes providing\ninformation related to the person included in the at least one image to user 100, the category tag may include a people category.  In this example, the requested information includes at least part of the at least one image.  In a second example, when the\nselected function includes storing information related to at least one detail about an objected located in front of user 100, the category tag may include an object category.  In this example, the requested information may include the at least one detail\nabout an object included in the at least one image.  In a third example, when the selected function includes updating a database based on information related to the food associated with the dietary restriction, the category tag may include a food\ncategory.  In this example, the requested information may include an indication that user 100 is engaging with food associated with a dietary restriction.  In a fourth example, when the selected function includes providing information to an individual\nrelated to user 100, the category tag may include a health-related category.  In this example, the requested information may include an indication that user 100 is engaging in a health-related activity.\nAt step 1920, the processing device may obtain at least one image, and at step 1930 the processing device may analyze the at least one image to detect a contextual situation associated with the at least one image.  In some embodiments, the\nprocessing device may be programmed to detected in the at least one image a contextual situation that includes a presence of an object of interest.  For example, the presence of a known person, the presence of text or a logo, the presence of recognized\nobject, or the presence of an indicator of an event type.  The event type may include at least one of: a sporting event, a family event, a work-related event, driving, reading, eating, and socializing.  In other embodiments, the processing device may be\nprogrammed to receive audio information and to detect the contextual situation based on the received audio information and the obtained at least one image.  The audio information may include a voice command, a name of a known individual, recognizable\nbackground noises, and more.  In other embodiments, processing device may be programmed to retrieve additional information from one or more sources that may assist in identifying or improving the certainty level in the identification of the contextual\nsituation.  In one example, the processing device may access location information from the GPS to better detect a contextual situation.  Alternatively, the processing device may derive location information from other sources, such as available Wi-Fi\nnetworks' service set identifier (SSID).\nAt step 1940, the processing device may determine image-related information associated with the detected contextual situation.  Consisted with the present disclosure, computing device 120 may define in the request the type of image-related\ninformation that wearable apparatus 110 should determine and transmit after detecting the contextual situation.  Alternatively, the type of image-related information may be selected as the result of default settings.  In some embodiments, as mentioned\nabove, the requested information may include at least part of the at least one image.  Specifically, some selected functions may involve procedures, such as Optical Character Recognition (OCR) and/or facial recognition.  Since these procedures may not\nrequire all of the images captured by image sensor 220, determining the image-related information may include identifying a Region of Interest (ROI) and enabling transmission of the ROI.  In the example depicted in FIG. 17A, the determined image-related\ninformation may include a cropped image that includes a face of the woman in front user 100.  In other embodiments, also as mentioned above, the requested information may include an indication that user 100 is engaging with a person and/or with an\nobject.  The indication may be a one-bit message, processed data including metadata information (e.g., time, location), and/or at least a portion of the captured image data that includes a representation of the contextual situation.  As illustrated in\nthe third scenario of FIG. 18B, in some cases different types of image-related information may be determined in response to detecting a single contextual situation.  The different types may correspond with different selected functions.\nAt step 1950, after determining the image-related information, the processing device may cause computing device 120 to execute the selected function based on the determined image-related information.  The selected function may be executed in\nreal-time or at a later time (e.g., when computing device is being charged).  One type of selected function may include providing information to user 100.  Another type of selected function may include storing information in a memory.  In one embodiment,\neven when the selected function is providing an alert to user 100 (such as illustrated in FIGS. 29A-29D), computing device 120 may be programmed to store information about the provided alert in memory 550b or in a remote sewer.  In another embodiment,\nthe selected function may include initiating a search related to the determined image-related information in a database.  In this case, the selected function may include causing information related to a result of the search to be displayed on a display\nassociated with computing device 120.  In addition, computing device 120 may determine that the received image-related information is not sufficient to execute the selected function, and requests from wearable apparatus 110 for additional information. \nIn the example depicted in FIG. 17A, computing device 120 may receive an image that includes the face of the woman in front user 100, but the image was not good enough for the facial recognition search engine.  Accordingly, computing device 120 can\nrequest from wearable apparatus 110 to transmit another image.  As those who are skilled in the art will appreciate, at least some of the steps depicted in FIG. 19 can be performed simultaneously or in a different order than that shown in the figure.\nVisual Fairing of External Devices with a Wearable Apparatus\nIn some embodiments, an external device may be recognized in image data captured by wearable apparatus 110.  An external device may be any device in an environment of wearable apparatus 110 that is configured to pair with wearable apparatus 110. Example external devices may include, but are not limited to computing devices, personal electronic devices, mobile devices, desktop devices, entertainment devices, household devices, audio and/or visual devices, illumination devices, appliances,\nfixtures, thermostats, televisions, coffee makers, printers, lights, lamps, Wi-Fi support devices, net work devices, etc.\nIn some cases, the external device may provide one or more controllable functions.  A controllable function may be any changeable aspect of the external device.  Example controllable functions may include, but are not limited to, mode (e.g., a\npower-saving mode, a color printing mode, a shuffle mode, etc.), brightness, intensity, volume, position, on/off state, stop/play state, station and/or channel selection (e.g., for an audio and/or visual device), temperature, and/or speed.\nIn some cases, it may be desirable for wearable apparatus 110 to determine if the external device is one with which wearable apparatus 110 may pair (e.g., with which a communication path may be established).  Pairing may permit wearable\napparatus 110, for example, to control a controllable function of the external device or simply to exchange information with the external device.  In order to determine whether the external device is one with which wearable apparatus 110 may pair,\nwearable apparatus (10 may transmit a signal configured to cause a response by the external device, such as a change in at least one aspect of the external device.  Such a signal may be referred to as an interrogation signal.  The interrogation signal\nmay be transmitted by wearable apparatus 110 itself and/or through one or more intermediate devices, such as a device paired with wearable apparatus 110.\nFIG. 20 is a block diagram illustrating an example of the components of wearable apparatus 110 for visually pairing with an external device.  As shown in FIG. 20, wearable apparatus 110 includes image sensor 220, wireless transceiver 530, memory\n550, and processor 210.  While only one image sensor 220, wireless transceiver 530, memory 550, and processor 210 are shown, it will be understood that more of any of these components may be included.  Further, while these components are shown to be\nincluded in wearable apparatus 110, in other embodiments one or more of these components may be remote from and configured to communicate with wearable apparatus 110 (e.g., distributed over one or more servers in communication with wearable device 110\nover a network).  In other embodiments, wearable apparatus 110 may include other components, such as any of the components described above in connection with FIGS. 5A-5C.\nImage sensor 220 may take any of the forms described above in connection with FIGS. 2,3A, 4A-4B, 5A-5C, and 7.  Similarly, wireless transceiver 530 may take any of the forms described above in connection with FIGS. 5A-5C.  Memory 550 may\nlikewise take any of the forms described above in connection with FIGS. 5A-5C (including memory 550a and 550b), and processor 210 may take any of the forms described above in connection with FIGS. 2 and 5A-5C.\nImage sensor 220 may be any device configured to capture images and/or a stream of images from an environment of a user of the wearable apparatus 110.  The environment may include, for example, one or more external devices.  The images and/or\nstream of images may include, for example, real-time image data of a field-of-view of the user.  As discussed above, image sensor 220 may be configured to detect, and convert optical signals into electrical signals, and the electrical signals may be used\nto form an image or a video stream (i.e., the stream of images) based on the detected signal.\nMemory 550 may contain software modules consistent with the present disclosure.  As shown, included in memory 550 are an external device detection module 2002, an interrogation signal module 2004, and a control signal module 2006.  Modules 2002,\n2004, and 2006 may contain software instructions for execution by processor 210, as described below.  External device detection module 2002, interrogation signal module 2004, and control signal module 2006 may cooperate, to facilitate visual pairing of\nwireless apparatus 110 with the external device.\nProcessor 210 may be configured to receive the stream of images from image sensor 220 and to analyze the stream of images to detect the external device in the environment of the user.  In some embodiments, processor 210 may be configured to\nexecute software instructions in external device detection module 2002 to receive the stream of images from image sensor 220 and analyze the stream of images to detect the external device in the environment.  As described above, processor 210 may be\nconfigured to extract information from the stream of images.  Extracting information, as described above, includes any process by which information associated with objects, individuals, locations, events, etc., is identified in the stream of images by\nany means known to those of ordinary skill in the art.  Processor 210 may be configured to identify information associated with the external device in the stream of images.\nAnalyzing the stream of images may involve any analysis by which the external device may be detected based on the stream of images.  In some embodiments, for example, analyzing the stream of images may involve edge identification, in which an\nimage is analyzed to detect pixels at which discontinuities (e.g., sudden changes in image brightness) occur and edges (e.g., edges of the external object) are identified to coincide with the detected pixels.  Alternatively or additionally, in some\nembodiments analyzing the stream of images may involve identifying in and/or extracting from an image pixels representative of one or more objects in the environment, such as the external object.  Pixels may be determined to be representative of an\nexternal object based on, for example, other images of the external device or similar external devices maintained, e.g., in a database and/or predetermined data describing the external object maintained, e.g., in a database.  Alternatively or\nadditionally, pixels may be determined to be representative of an external object based on, for example, a trained neural network configured to detect predetermined external objects.  Oilier types of analysis are possible as well, including, but not\nlimited to, gradient matching, grayscale matching, scale-invariant feature transform (SIFT) matching, and/or interpretation trees.\nProcessor 210 may be further configured to cause wireless transceiver 530 to transmit an interrogational signal to the external device, and wireless transceiver 530 may be configured to transmit the interrogation signal.  In some embodiments,\nprocessor 230 may be configured to execute software instructions in interrogation signal module 2004 to cause wireless transceiver 530 to transmit the interrogation signal.  The interrogation signal may be any signal configured to cause a change in at\nleast one aspect of the external device.  The aspect may be, for example, a feature of the external device's appearance or another visually recognizable and/or detectable attribute.  For example, the interrogation signal may be configured to cause the\nexternal device to illuminate and/or blink a light and/or display on the external device, modify a position of the external device and/or some component of the external device, and/or display certain information.\nThe interrogation signal may take the form of, for example, a radio frequency (RF) signal (e.g., a radio frequency identification (RFID) signal), a Bluetooth signal, an optical signal (e.g., an infrared signal, a visible light signal), and/or a\nWi-Fi signal (e.g., an IEEE 802.11 signal).  In some embodiments, different interrogation signals may be used for different external devices and/or different types of external devices.  In some embodiments, interrogation signal module 2004 may determine\nwhich interrogation signal to use based on the detected external device.  Alternatively or additionally, interrogation signal module 2004 may attempt more than one interrogation signal (e.g., in a predetermined order, an order dependent on the detected\nexternal device, etc.).\nWireless transceiver 530 may take different forms for different interrogation signals.  For example, wireless transceiver 530 may take the form of a radio frequency transmitter and/or transceiver, a Bluetooth radio, and/or an optical transmitter\n(e.g., an LED).  Where the external device is configured to pair with wearable apparatus 110, the external device may include a component configured to receive the interrogation signal, such as a radio transceiver, a Bluetooth detector, and/or an optical\nreceiver (e.g., a photo diode detector).  In some embodiments, the external device may include more than one component for receiving more than one type of interrogation signal (e.g., an external device may be configured to receive both an optical and a\nBluetooth interrogation signal).\nAs described above, the interrogation signal may be configured to cause a change in at least one aspect of the external device.  In some embodiments, the interrogation signal may include instructions (e.g., a command) to cause the external\ndevice to change the aspect(s) of the external device.  For example, the interrogation signal may command the external device to illuminate, pulse, and/or blink a light and/or display on the external device, change an intensity of a light emitted by the\nexternal device, modify a position of the external device or some component of the external device, and/or display certain information.\nIn some embodiments, the external device may include one or more components for carrying out the command included in the interrogation signal.  For example, where the interrogation signal commands the external device to illuminate a light (e.g.,\nan LED array) on the external device, the external device may include a light configured to be illuminated or deilluminated.  As another example, where the interrogation signal commands the external device to modify a position of the external device, the\nexternal device may include one or more motion mechanisms, such as an actuator, electric motor, wheels, or gears, and so forth, which may enable the external device to modify its position.  As still another example, where the interrogation signal\ncommands the external device to modify a position of some component of the external device, the external device may include, in addition to one or more motion mechanisms, a component adapted for movement, such as an arm.  In some embodiments, the\ncomponents configured to carrying out the command included in the interrogation signal may be components specially adapted to carry out the command.  Alternatively, the components may have other purposes in the external device.  For example, where the\ninterrogation signal commands the external device to display certain information, the external device may display the information on a display of the external device that additionally serves to display other information at the external device.\nIn some embodiments, the change in the aspect(s) of the external device may be between binary states on the external device.  For example, the external device may change a light from an on state to an off state.  As another example, the external\ndevice may change a position of the external device from a resting position to a ready position.  Alternatively, in some embodiments the interrogation signal may specify an extent of change in the aspects) of the external device.  For example, the\ninterrogation signal may specify that the external device is to move a component of the external device (e.g., raise an arm) by a specified distance and/or in a specified direction.  As another example, the interrogation signal may specify that the\nexternal device is to change an intensity of a light by a specified amount and/or in a specified direction (e.g., reduce an intensity of the light by a specified amount).  As another example, the interrogation signal may specify that the external device\nis to change the color of an emitted light.  The change in the aspect(s) may be temporary.  For example, the interrogation signal may specify that the external device is to modify a display to display certain information for a specified period of time. \nAs another example, the interrogation signal may specify that the external device is to blink a light for a specified period of time.\nProcessor 210 may be further configured to analyze the stream of images to detect the change in the at least one aspect, of the external device, as caused by the interrogation signal.  In some embodiments, processor 210 may be configured to\nexecute software instructions in external device detection module 2002 to detect the change in the at least one aspect of the external device.  For example, where the interrogation signal caused the external device to illuminate and/or blink a light or\ndisplay on the external device, processor 210 may be configured to extract information from the stream of images indicating the illumination and/or blinking.  As another example, where the interrogation signal caused the external device to modify a\nposition of the external device or some component of the external device, processor 210 may be configured to extract information from the stream of images indicating the changed position.  As still another example, where the interrogation signal caused\nthe external device to display certain information, processor 210 may be configured to extract information from the stream of images indicating the displayed information.\nProcessor 210 may be further configured to store in memory 550 information relating to the external device after detection of the change in the at least one aspect of the external device.  The information may include, for example, information\nrelating to a location associated with the external device, a type associated with the external device, information related to the appearance of the external device, a picture of the external device, and/or an indication of at least one controllable\nfunction associated with the external device.  In another example, the external device may be a wearable external device, and processor 210 may be further configured to identify the person wearing the wearable external device and/or to determine\ninformation related to the person wearing the wearable external device, such as the person's gender, height, information related to the appearance of the person, a picture of the person, a picture of the face of the person, and so forth.\nIn some embodiments, a location of the external device may be determined by, for example, querying the external device (e.g., where the external device includes a global positioning system or is otherwise configured to determine its own\nlocation).  Alternatively or additionally, a location of the external device may be determined relative to wearable apparatus 110 and/or another reference in the environment using one or more of stereo image analysis, time-of-flight analysis,\ntriangulation, structured light techniques, and/or modulated light techniques using, for example, radar, lidar, and/or infrared detection.  In some embodiments, wearable apparatus 110 may include and/or may pair will) another device that includes a\ncomponent configured for determining one or both of a geographic position of wearable apparatus 110, a geographic position of the external device, and/or a relative position of the external device to wearable apparatus 110, such as a global positioning\nsystem and/or a radar or lidar transceiver.  In some embodiments, wearable apparatus 110 and/or another device paired with wearable apparatus 110 may be configured to generate a map or other representation of the environment (e.g., using simultaneous\nlocation and mapping (SLAM) techniques) indicating a relative location of the external device to wearable apparatus 110 and/or one or more other objects in the environment.  The map may be, for example, a grid map or a topological map.\nIn some embodiments, wearable apparatus 110 and the external device may be paired, such that wearable apparatus 110 may, for example, exchange information with the external device or control a controllable function of the external device. \nProcessor 210 may execute software instructions in control signal module 2006 to cause a control signal to be transmitted to the external device.  The control signal may be configured to control one or more controllable functions of the external device,\nas further described below.\nFIG. 21 is a flowchart illustrating an exemplary process 2100 for visually pairing with an external device.  Process 2100 is described with reference to FIGS. 22A-22C, which are schematic illustrations of an exemplary pairing between wearable\napparatus 110 and an external device 2202 in an environment 2200.  One of ordinary skill in the art will recognize that FIGS. 22A-22C are examples, and other kinds of devices may be paired with wearable apparatus 110.\nAs shown, process 2100 may begin at step 2102 with receiving a stream of images.  The stream of images may be received from, for example, at least one image sensor (e.g., image sensor 220) of the wearable apparatus 110, which/nay be configured\nto capture the stream of images from an environment of a user of the wearable apparatus 110.\nExample environment 2200 is shown in FIG. 22A.  As shown, environment 2200 includes a user 100 wearing a wearable apparatus 110.  While wearable apparatus 110 is shown to be configured to connect on an exterior surface of clothing of the user\n100, similar to wearable apparatus 110 described in connection with FIG. 9, above, it will be understood that wearable apparatus 110 may take any of the forms described herein, including but not limited to those shown in FIGS. 1A-D, 3A-B, 4A-B, and 8-16. Wearable apparatus 110 may include and/or be communicatively coupled to an image sensor, such as image sensor 220 described above, that is configured to capture a stream of images from environment 2200.  While environment 2200 is shown as an indoor\nenvironment, it will be understood that environment 2200 may be any proximity surrounding user 100, whether indoor, outdoor, or both.\nReturning to FIG. 21, process 2100 may continue at step 2104 with analyzing the stream of images to detect an external device.  For example, at least one processing device (e.g., processor 210) in the wearable apparatus 110 may be programmed to\nreceive the stream of images from the image sensor 220 and analyze the stream of images to detect the external device in the environment of the user.\nAn example external device 2202 is shown in FIG. 22A.  As shown, external device 2202 may be a light source, such as a visible light source or an infrared light source.  While the external device 2202 is shown to be a light source, external\ndevice 2202 may take other forms as well.  For example, rather than being a light source, external device 2202 may include, be included in, be communicatively coupled to, and/or otherwise associated with a light source.  As another example, external\ndevice 2202 may be, include, be included in, be communicatively coupled to, and/or otherwise associated with a computing device (e.g., a desktop computer, laptop computer, tablet computer, printer, database, server, etc.), a personal electronic device\n(e.g., mobile device, cellular device, table, smartphone, smart watch, e-reader device, etc.), an entertainment device (e.g., television, digital media player, music player, radio, etc.), a household device (e.g., refrigerator, oven, stove, microwave,\nalarm system, appliance, fixture, etc.), audio and/or visual devices (e.g., display, projector, speaker, etc.), an illumination device (e.g., lamp, etc.), an appliance (e.g., washing machine, dryer, blender, etc.), a fixture (e.g., ceiling fan, lock,\nsafe, garage door, etc.), and/or a wearable device.\nThe stream of images from environment 2200 captured by the image sensor(s) 220 of wearable apparatus 110 may include a representation of external device 2202, such that a processor 210 included in and/or communicatively coupled to wearable\napparatus 110 may analyze the stream of images to detect external device 2202 in environment 2200 of user 100.\nReturning to FIG. 21, process 2100 may continue at step 2106 with transmitting an interrogation signal.  For example, the processing device(s) in the wearable apparatus 110 may be programmed to cause at least one transmitter (e.g., wireless\ntransceiver 530) in the wearable apparatus 110 to transmit the interrogation signal.  The interrogation signal may be configured to cause a change in at least one aspect of the external device.\nAn example interrogation signal 2204 is illustrated in FIG. 22A.  As shown, wearable apparatus 110 may transmit (e.g., using a wireless transceiver 530 included in and/or communicatively coupled to wearable apparatus 110) interrogation signal\n2204 to external device 2202.  In the event external device 2202 is a device with which wearable apparatus 110 cannot pair, interrogation signal 2204 may have no effect on external device 2202.  However, in the event external device 2202 is a device with\nwhich wearable apparatus 110 can pair, interrogation signal 2204 may cause a change in at least one aspect of external device 2202.\nAn example change 2206 in at least, one aspect of external device 2202 is shown in FIG. 22B.  External device 2202 may exhibit change 2206 in response to interrogation signal 2204.  As shown, change 2206 may include, for example, a change in\nillumination of external device 2202, such as external device 2202 blinking.  Other example changes 2206 may include turning off, turning on, dimming, and/or brightening.  In some embodiments, for example where external device 2202 is not, does not\ninclude, is not included in and/or is not associated with a light source, change 2206 be another alteration of a feature of an appearance and/or other visually recognizable or detectable attribute of external device 2202.  For example, external device\n2202 may illuminate and/or blink a display on external device 2202, modify a position of external device 2202 or some component of external device 2202, and/or display certain information.  The change 2206 may be permanent or may be temporary.\nReturning to FIG. 21, process 2100 may continue at step 2108 with analyzing the stream of images to detect the change.  For example, the processing device may be programmed to analyze the stream of images to detect the change in the at least one\naspect of the external device.  As shown in FIG. 22B, the stream of images from environment 2200 captured by the image sensor(s) 220 of wearable apparatus 110 may include a representation of external device 2202, including change 2206, such that, the\nprocessor 210 included in and/or communicatively coupled to wearable apparatus 110 may analyze the stream of images to detect external device 2202 and change 2206.\nReturning to FIG. 21, process 2100 may continue at step 2110 with storing information relating to the external device.  For example, the processing device may be programmed to, after detection of the change in the at least one aspect of the\nexternal device, store in a memory (e.g., memory 550) of the wearable apparatus 110 information relating to the external device.\nThe information may include, for example, a location associated with the external device, such as a geographic location of the external device and/or a location of the external device relative to one or more of the wearable apparatus 110; the\nuser, and/or another object in the environment.  Alternatively or additionally, the information may include a type associated with the external device.  The type may, for example, describe the external device.  For example, the type may indicate a type\nof the external device (e.g., a light source, a microwave, or a garage opener), a brand, style, a version, and/or a design of the external device (e.g., a brand of television, a style of ceiling fan, a design of speaker, etc.), an identifier that\nuniquely identifies the external, device (e.g., a MAC address, an IP address, etc.), an identifier of a particular type or brand of the external device, or other feature of the external device.  Still alternatively or additionally, the information may\ninclude a picture of the external device.  Still alternatively or additionally, the information may include an indication of at least one controllable function associated with the external device.  The indication of the controllable function may, for\nexample, describe the function that can be controlled, such as a mode (e.g., a power-saving mode, a color printing mode, a shuffle mode, etc.), brightness, intensity, volume, position, state, on/off state, stop/play state, station or selection (e.g., for\nan audio and/or visual device), temperature, and/or speed.  Alternatively or additionally, the indication of the controllable function may, for example, describe how the function can be controlled, such as options (e.g., among modes, among, positions,\namong stations, etc.) and levels (e.g., of volume, brightness, etc.).  Still alternatively or additionally, the information may include information related to devices coupled with the external device.  For example, the external device may comprise a\nset-top box or a game console, the coupled device may comprise a TV connected to the external device, and the information related to the TV may comprise a location, a brand, a version, size, a picture of the TV, and so forth.  In another example, the\nexternal device may comprise a thermostat, the coupled device may comprise an HVAC unit connected to the thermostat, and the information related to the HVAC unit may comprise a location, a brand, a version, a state, a picture of the HVAC unit, and so\nforth.  Still alternatively or additionally, the external device may be a wearable external device, and the in form a lion may comprise information related to a person wearing the wearable external device, such as the person's gender, height, information\nrelated to the appearance of the person, a picture of the person, a picture of the face of the person, and so forth.\nExample information relating to external device 2202 in FIG. 22B may include a location of external device 2202, such as a geographic location of external device 2202 and/or a location of external device 2202 relative to one or more of wearable\napparatus 110, user 100, and/or another object in the environment (e.g., relative to the table shown).  Alternatively or additionally, example information relating to external device 2202 may include a type associated with external device 2202, such as a\ntype of external device 2202 (e.g., a light source), a brand of external device 2202 (e.g., a brand of the light source), a style of external device 2202 (e.g., whether the light source emits visible or infrared light, what type and/or how many light\nbulbs are used in the light source, a type of shade included in the light source, whether the light source is movable or fixed, etc.), a design of external device 2202 (e.g., whether the light source can be dimmed, whether the light source operates on a\ntimer, etc.), and/or other feature of external device 2202.  Still alternatively or additionally, the information may include an indication of at least one controllable function associated with external device 2202, such as a mode (e.g., the information\nmay indicate that the light source can be turned on and/or off, that the light source can operate in a power-saving mode, that the light source can operate on a timer, etc.), a brightness and/or intensity of the light source, a position of the light\nsource (if the light source is movable), or another function of the light source.  Alternatively or additionally, the indication of the controllable function may, for example, describe how the function of external device 2202 can be controlled, such as\noptions (e.g., among modes or positions of the light source, etc.) and levels (e.g., of brightness, intensity, etc.).\nIn some embodiments, the wearable apparatus 110 may detect a gesture or gestures by the user and, in response, may cause at least one aspect of the external device to be controlled, for example using the information relating to the external\ndevice stored in memory.  For example, the processing device may be further programmed to analyze the stream of images to detect at least, one recognized gesture (e.g., a hand-related trigger, as described above) made by the user.  As shown in FIG. 22C,\nfor example, user 100 may make a recognized gesture 2208.  While the recognized gesture 2208 is shown to be a raise of an arm of user 100, it will be understood that other recognized gestures are possible as well.\nBased on the detected recognized gesture(s), the processing device may cause a control signal to be transmitted (e.g., by wireless transceiver 530) to the external device.  The control signal may be configured to control at least one aspect of\nthe external device that is associated with the recognized gesture.  In some embodiments, the control signal may be configured to cause the external device to activate (e.g., turn on), deactivate (e.g., turn off), change setting (e.g., changing the\nsettings of an HVAC system, changing the illumination brightness, changing the volume of a sound system, and so forth).  As shown in FIG. 22C, for example, based on the recognized gesture 2208, wearable apparatus 110 may transmit control signal 2210 to\nexternal device 2202, thereby controlling at least one aspect 2212 of external device 2202.  Aspect 2212 may be associated with recognized gesture 2208.\nAs shown, aspect 2212 may be an illumination of external source 2202, and control signal 2210 may be configured to control the illumination of external source 2202 by, for example, turning on, turning off, dimming, or brightening the\nillumination.  Alternatively or additionally, controlling aspect 2212 may involve controlling any feature of an appearance or other visually recognizable or detectable attribute of external device 2202, including modifying a position of external device\n2202 or some component of external device 2202 and/or displaying certain information.  The control may be permanent or may be temporary.  Alternatively or additionally, controlling aspect 2212 may involve controlling any of the controllable functions\ndescribed above, such as a mode of external device 2202, a brightness and/or intensity of external device 2202, a position of external device 2202, or another function of the light source.\nWhile certain environments, external devices, changes, recognized gestures, and aspects are shown in FIGS. 22A-22C, it will be understood that these are merely exemplary and that other environments, external devices, changes, recognized\ngestures, and aspects are possible as well.\nIn some embodiments, step 2104 may detect in the stream of images one or more external devices in the environment of the user, and step 2106 may detect one or more external devices able to receive an interrogation signal, for example, through a\nwireless communication protocol.  The number of external devices detected by step 2104 may be smaller, larger, or equal to the number of external devices detected by step 2106.  In some examples, the visual appearance in the stream of images of the\nexternal devices detected by step 2104 may hint at which external devices detected by step 2104 correlate to which external devices detected by step 2106.  In some examples, process 2100 may go through one or more of the external devices detected by step\n2106, transmit interrogation signals to the external devices, and step 2108 may analyze the stream of images to detect the change corresponding to the transmitted interrogation signals, differentiating among the detected external devices based on the\ndetected change.  For example, the transmitted interrogation signals may be transmitted at different times, and step 2108 may differentiate among the detected external devices based on the timing of the detected change.  For example, the transmitted\ninterrogation signals may be configured to cause different changes, and step 2108 may differentiate among the detected external devices based on the type of the detected change.  For example, step 2106 may detect, for example through a wireless\ncommunication protocol, two external devices able to receive an interrogation signal, and step 2104 may detect four external devices.  Step 2106 may transmit interrogation signals to the two external devices detected by step 2106, and step 2108 may\nidentify which, if any, of the four external devices detected by step 2104, corresponds to the external devices detected by step 2106, based on a detected change in the appearance of the external device detected by step 2104.  In another example, step\n2106 may detect, for example, through a wireless communication protocol, two wearable devices and one light source, and based on the visual appearance in die stream of images of the external devices, step 2104 may detect four external devices and\nrecognize one as a wearable device and three external devices as light sources.  Step 2106 may transmit interrogation signals to the two wearable devices detected by step 2106, and step 2108 may identify which, if any, of the two wearable devices\ndetected by step 2106, is the wearable device detected by step 2104, based on a detected change in the appearance of the wearable device detected by step 2104.  Step 2106 may transmit interrogation signals to the light source detected by step 2106, and\nstep 2108 may identify which, if any, of the three light sources detected by step 2104, is the light source detected by step 2106, based on a detected change in the appearance, or lack thereof, of the light sources detected by step 2104.\nControlling an External Device Using a Wearable Apparatus.\nIn some embodiments, wearable apparatus 110 may be used to control a controllable device.  A controllable device may be any device in an environment of wearable apparatus 110 that can be controlled by wearable apparatus 110.  Example\ncontrollable devices may include, but are not limited to, computing devices, personal electronic devices, mobile devices, desktop devices, entertainment devices, household devices, audio and/or visual devices, illumination devices, appliances, fixtures,\nthermostats, televisions, coffee makers, printers, light sources, lamps, Wi-Fi support devices, network devices, wearable devices, etc.\nIn some cases, wearable apparatus 110 may control (e.g., change) one or more aspects of the controllable device.  Example controllable aspects may include, but are not limited to, mode (e.g., a power-saving mode, a color printing mode, a shuffle\nmode, etc.), brightness, intensity, volume, position, on/off state, locked/unlocked state, armed/unarmed state, stop/play state, station or channel selection (e.g., for an audio and/or visual device), temperature, time, and/or speed.  As an example, the\ncontrollable device may be or may include a light source, and wearable apparatus 110 may cause the controllable device to change an illumination state of the light source.  As another example, the controllable device may be or may include a heating\ndevice, ventilation device, air conditioning device, and/or HVAC system, and wearable apparatus 110 may be configured to change one or more settings (e.g., a temperature or fan speed) of the heating device, ventilation device, air conditioning device,\nand/or HVAC system.  As still another example, the controllable device may be or may include a locking mechanism, and wearable apparatus 101 may be configured to cause the controllable device to lock or unlock the locking mechanism.\nIn some cases, wearable apparatus 110 may control the aspect(s) of the controllable device in response to detecting a visual trigger.  A visual trigger may be any trigger that is visually detectable by wearable apparatus 110, such as a hand or\nbody gesture by a user of wearable apparatus 110 and/or movement of an object associated with the user, such as a stylus or glove.\nIn order to control a controllable device, wearable apparatus 110 may be configured to identify the controllable device and detect the visual trigger by analyzing images obtained by wearable apparatus 110.  Based on the detection of the trigger,\nwearable apparatus 110 may transmit a signal configured to change the aspect(s) of the controllable device.  The signal may be referred to as a command.  The command may be transmitted to the controllable device by wearable apparatus 110 itself and/or\nthrough one or more intermediate devices, such as a device paired with wearable apparatus 110.\nFIG. 23 is a block diagram illustrating an example of the components of a wearable apparatus 110 for controlling a controllable device.  As shown in FIG. 23, wearable apparatus 110 may include image sensor 220, wireless transceiver 530, memory\n550, and processor 210.  While only one image sensor 220, wireless transceiver 530, memory 550, and processor 210 are shown, it will be understood that more of any of these components may be included.  Further, while these components are shown to be\nincluded in wearable apparatus 110, in other embodiments one or more of these components may be remote from and configured to communicate with wearable apparatus 110 (e.g., distributed over one or more servers in communication with wearable device 110\nover a network).  In other embodiments, wearable apparatus 110 may include other components, such as any of the components described above in connection with FIGS. 5A-5C.\nImage sensor 220 may take any of the forms described above in connection with FIGS. 2, 3A, 4A-4B, 5A-5C, and 7.  Similarly, wireless transceiver 530 may take any of the forms described above in connection with FIGS. 5A-5C.  Memory 550 may\nlikewise take any of the forms described above in connection with FIGS. 5A-5C (including memory 550a, 550b), and processor 210 may take any of the forms described above in connection with FIGS. 2 and 5A-5C.\nImage sensor 220 may be any device (e.g., camera, CCD, etc.) configured to obtain images from an environment of a user of the wearable apparatus 110.  The environment may include, for example, one or more controllable devices.  The images may\ninclude, for example, real-time image data of a field-of-view of the user.  As discussed above, image sensor 220 may be configured to detect and convert optical signals into electrical signals, and the electrical signals may be used to form an image or a\nvideo stream (i.e., the stream of images) based on the detected signal.\nMemory 550 may contain software modules consistent with the present disclosure.  As shown, included in memory 550 are a controllable device identification module 2302, a visual trigger detection module 2304, and a command module 2306.  Modules\n2302, 2304, and 2306 may contain software instructions for execution by processor 210, as described below.  Controllable device identification module 2302, visual trigger detection module 2304, and command module 2306 may cooperate to facilitate\ncontrolling of a controllable device be wearable apparatus 110.\nProcessor 210 may be configured to receive the images obtained by image sensor 220 and to analyze the images to identify the control (able device in the environment of the user.  In some embodiments, processor 210 may be configured to execute\nsoftware instructions in controllable device identification module 2302 to obtain the images from image sensor 220 and analyze the images to identify the controllable device in the environment.  As described above, processor 210 may be configured to\nextract information from the stream of images.  Extracting information, as described above, includes any process by which information associated with objects, individuals, locations, events, etc., is identified in the stream of images by any means known\nto those of ordinary skill in the art.  Processor 210 may be configured to identify information associated with the controllable device in the images.\nProcessor 210 may be further configured to analyze the images to detect a visual trigger associated with the controllable device.  In some embodiments, processor 210 may be configured to execute software instructions in visual trigger detection\nmodule 2304 to analyze the images to detect the visual trigger.  As described above, processor 210 may be configured to extract information from the stream of images.  Extracting information, as described above, includes any process by which information\nassociated with objects, individuals, locations, events, etc., is identified in the stream of images by any means known to those of ordinary skill in the art.  Processor 210 may be configured to identify information associated with the visual trigger in\nthe images.\nAnalyzing the stream of images to identify the controllable device and/or detect the visual trigger may involve any analysis by which the controllable device and/or visual trigger may be identified or detected based on the images.  In some\nembodiments, for example, analyzing the images may involve edge identification, in which an image is analyzed to detect pixels at which discontinuities (e.g., sudden changes in image brightness) occur and edges (e.g., edges of the controllable object, a\nbody part of the user, and/or an object associated with the user) are identified to coincide with the detected pixels.  Alternatively or additionally, in some embodiments analyzing the images may involve identifying in and/or extracting from an image\npixels representative of one or more objects in the environment, such as the controllable object, a body part of the user, and/or an object associated with the user.  Pixels may be determined to be representative of an object based on, for example, other\nimages of the object maintained, e.g., in a database and/or predetermined data describing the object maintained, e.g., in a database (e.g., other images of the controllable device, of the body part of the user, and/or of the device associated with the\nuser).  Alternatively or additionally, pixels may be determined to be representative of an object based on, for example, a trained neural network configured to detect predetermined objects (e.g., predetermined controllable devices, body parts of the\nuser, and/or devices associated with the user).  Other types of analysis are possible as well, including, but not limited to, gradient matching, greyscale matching, scale-invariant feature transform (SIFT) matching, and/or interpretation trees.\nProcessor 210 may be further configured to, based on the detection of the visual trigger, cause wireless transceiver 530 to transmit a command to the controllable device.  In some embodiments, processor 210 may be configured to execute software\ninstructions in command module 2306 to cause wireless transceiver 530 to transmit the command.  The command may be configured to change at least one aspect of the controllable device.  The aspect may be, for example, a feature of the controllable\ndevice's appearance or function.  Example aspects may include, but are not limited to, mode (e.g., a power-saving mode, a color printing mode, a shuffle mode, etc.), brightness, intensity, volume, position, on/off state, locked/unlocked state,\narmed/unarmed state, stop/play state, station or channel selection (e.g., for an audio and/or visual device), temperature, time, and/or speed.\nThe command may take the form of, for example, a radio frequency (RF) signal (e.g., a radio frequency identification (RFID) signal), a Bluetooth signal, an optical signal (e.g., an infrared signal, a visible light signal), or a Wi-Fi signal\n(e.g., an IEEE 802.11 signal).  In some embodiments, different commands may be used for different controllable devices and/or different types of controllable devices.  In some embodiments, control module 2304 may determine which command to use based on\nthe identified controllable device.  Alternatively or additionally, control module 2304 may attempt more than one command (e.g., in a predetermined order, an order dependent on the identified controllable device, etc.).\nWireless transceiver 530 may take different forms for different commands.  For example, wireless transceiver 530 may take the form of a radio frequency transmitter or transceiver, a Bluetooth radio, and/or an optical transmitter (e.g., an LED). \nLikewise, the controllable device may include a component configured to receive the command, such as a radio transceiver, a Bluetooth detector, and/or an optical receiver (e.g., a photo diode detector).  In some embodiments, the controllable device may\ninclude more than one component for receiving more than one type of command (e.g., a controllable device may be configured to receive both an optical and a Bluetooth command).\nAs described above, the command may be configured to cause a change in at least one aspect of the controllable device.  In some embodiments, the command may include instructions to cause the controllable device to change the aspect(s) of the\ncontrollable device.  For example, the command may cause the controllable device to change a mode in which the controllable device is operating (e.g., a power-saving mode, a color printing mode, a shuffle mode, etc.) or change a brightness, intensity,\nvolume, position, on/off state, stop/play state, station or channel selection (e.g., for an audio and/or visual device), temperature, and/or speed of the controllable device.\nIn some embodiments, the controllable device may include one or more components for carrying out the command.  For example, where the wearable apparatus 110 commands the controllable device to change a brightness of the controllable device, the\ncontrollable device may include a light configured to be brightened or dimmed.  As another example, where the wearable apparatus 110 commands the controllable device to change from an unlocked state to a locked state, the controllable device may include\na locking mechanism that enables the controllable device to change to a locked state.  In some embodiments, the components configured to carrying out the command may be components specially adapted to carry out the command.  Alternatively, the components\nmay have other purposes in the controllable device.  For example, where the command commands the controllable device to display certain information, the controllable device may display the information on a display of the controllable device that\nadditionally serves to display other information at the controllable device.\nIn some embodiments, the change in the aspect(s) of the controllable device may be between binary states on the controllable device.  For example, the controllable device may change a light from an on state to an off state.  As another example,\nthe controllable device may change from a power-saving mode to a non-power-saving mode.  Alternatively, in some embodiments the command may specify an extent of change in the aspect(s) of the controllable device.  For example, the command may specify a\nchange in brightness find/or color of a light or may specify that the controllable device is to change a time for performing a function (e.g., a cook time on a microwave) by a specified amount and/or in a specified direction (e.g., add 30 seconds to the\ncook lime).  The change in the aspect(s) may be temporary.  For example, the command may specify that the controllable device is to modify a display device to display certain information for a specified period of time.  As another example, the command\nmay specify that the controllable device is to remain in a power-saving mode for a specified period of time.\nFIG. 24 is a flowchart illustrating an exemplary process 2400 for controlling a controllable device.  Process 2400 is described with reference to FIGS. 25A-C, which are schematic illustrations of an exemplary controlling of a controllable device\n2502 by a wearable apparatus 110 in an environment 2500.  One of ordinary skill in the art will recognize that FIGS. 25A-C are examples, and other kinds of devices may be paired with wearable apparatus 110.\nAs shown, process 2400 may begin at step 2402 with obtaining images.  The images may be obtained from, for example, at least one image sensor (e.g., image sensor 220) of the wearable apparatus 110, which may be configured to capture the images\nfrom an environment of a user of the wearable apparatus 110.\nExample environment 2500 is shown in FIG. 25A.  As shown, environment 2500 includes a user 100 wearing a wearable apparatus 110.  While wearable apparatus 110 is shown to be configured to connect on an exterior surface of clothing of the user\n100, similar to wearable apparatus 110 described in connection with FIG. 9, above, it will be understood that wearable apparatus 110 may take any of the forms described herein, including but not limited to those shown in FIGS. 1A-D, 3A-B, 4A-B, and 8-16. Wearable apparatus 110 may include and/or be communicatively coupled to an image sensor, such as image sensor 220 described above, that is configured to capture images from environment 2500.  While environment 2500 is shown as an indoor environment, it\nwill be understood that environment 2500 may be any proximity surrounding user 100, whether indoor, outdoor, or both.\nReturning to FIG. 24, process 2400 may continue at step 2404 with analyzing the images to identify a controllable device.  For example, at least one processing device (e.g., processor 210) in the wearable apparatus 110 may be programmed to\nanalyze the images to identify the controllable device in the environment of the user of the wearable apparatus 110.\nIn some embodiments, in addition to identifying the controllable device, wearable apparatus 110 (e.g., at least one processing device in the wearable apparatus 110) may further analyze the images to identify a current state of the controllable\ndevice.  The state may be any current setting or level of an aspect of the controllable device.  The state may be, for example, a binary state of the controllable device, such as on/off state, a locked/unlocked state, an armed/unarmed state, a stop/play\nstate, etc. Alternatively or additionally, the state may be a variable state of the controllable device, such as a brightness, intensity, volume, position, station or channel selection (e.g., for an audio and/or visual device), temperature, time, and/or\nspeed of the controllable device.  Still alternatively or additionally, the state may be a mode (e.g., a power-saving mode, a color printing mode, a shuffle mode, etc.) of the controllable device.\nAlternatively or additionally, in some embodiments, in addition to identifying the controllable device, wearable apparatus 110 (e.g., at least one processing device in the wearable apparatus 110) may further analyze the images to identify a\ncontext associated with the controllable device.  The context may be any features of the environment in which the controllable device is identified.  The context may be, for example, a context associated with a particular user behavior, such as waking\nup, going to sleep, arriving, leaving, cleaning, preparing a meal, eating a meal, hosting a party, exercising, an object held by the user, etc. Alternatively or additionally, the context may be a context associated with a particular environmental\nfeature, such as a time of day, an indoor or outdoor temperature, a state of another controllable device, an action involving the controllable device, etc.\nAn example controllable device 2502 is shown in FIG. 25A.  As shown, controllable device 2502 may include a security system.  While only a keypad of the security system/alarm is shown, controllable device 2502 may be the keypad itself and/or a\nsecurity alarm system (e.g., comprising sensors, alerting devices, locks, etc.) of which the keypad is a part.  Moreover, while the controllable device 2502 is shown to be a security alarm, controllable device 2502 may take other forms as well.  For\nexample, rather than being a security alarm, controllable device 2502 may include, be included in, be communicatively coupled to, and/or otherwise associated with a security alarm.  As another example, controllable device 2502 may be, include, be\nincluded in, be communicatively coupled to, and/or otherwise associated with a computing device (e.g., a desktop computer, laptop computer, tablet computer, printer, database, server, etc.), personal electronic device (e.g., mobile device, cellular\ndevice, table, smartphone, smart watch, e-reader device, toy, etc.), entertainment device (e.g., television, digital media player, music player, radio, etc.), household device (e.g., refrigerator, oven, stove, microwave, alarm system, appliance, fixture,\nheating device, ventilation device, air conditioning device, HVAC system, security system, etc.), audio and/or visual devices (e.g., display, projector, speaker, etc.), illumination device (e.g., lamp, etc.), appliance (e.g., washing machine, dryer,\nblender, etc.), vehicle (e.g., automobile, lawn equipment, bicycle, boat, etc.), wearable device, and/or fixture (e.g., ceiling fan, lock, safe, door, garage door, etc.).\nThe images from environment 2500 captured by the image sensor(s) 220 of wearable apparatus 110 may include a representation of controllable device 2502, such that a processor 210 included in and/or communicatively coupled to wearable apparatus\n110 may analyze the images to identify controllable device 2502 in environment 2500 of user 100.  In some embodiments, wearable apparatus 110 may further determine a current state of the controllable device 2502, as described above.  For example,\nwearable apparatus 110 may determine that controllable device 2502 is in an armed state, at a certain brightness level, and/or a door associated with controllable device 2502 is unlocked.  Alternatively or additionally, in some embodiments wearable\napparatus 110 may further determine a context associated with, the controllable device 2502, as described above.  For example, wearable apparatus 110 may determine a context associated with an action involving controllable device 2502, an object held by\nuser 100, and/or a state of controllable device 2502.\nIn some embodiments, instead or in addition to step 2404, process 2400 may also comprise a procedure for identifying controllable devices in the vicinity of wearable apparatus 110 by detecting signals transmitted by the controllable devices. \nFor example, wireless transceiver 530 may detect wireless signals transmitted by controllable devices in the vicinity of wearable apparatus 110.  In another example, a RFID reader and/or RFID scanner may detect RFID enabled devices in the vicinity of\nwearable apparatus 110.  In some examples, instead or in addition to step 2404, process 2400 may also determine the current state of the controllable device by receiving information related to the current state in signals transmitted by the controllable\ndevices.\nIn some embodiments, in process 2400, step 2404 may be executed before, after or simultaneously with step 2606.  In some embodiments, in process 2400, step 2404 and/or step 2606 may be executed after or simultaneously with step 2402.\nIn some embodiments, wearable apparatus 110 may pair (e.g., establish a communication path) with the controllable device, as described above in connection with FIGS. 20-22A-C. Fairing may permit wearable apparatus 110 to, for example, transmit\ncommand signals to control the controllable device, as described below, or simply exchange information with the controllable device.  In some embodiments, for example, wearable apparatus 110 may transmit an interrogation signal configured to cause a\nchange in at least one aspect of the controllable device, analyze one or more images captured by the image sensor after transmission of the interrogation signal to detect the change in the aspect(s) of the controllable device and, after detecting the\nchange, store in the memory information relating to the controllable device.  The information relating to the controllable device may include, for example, a location associated with the controllable device, a type associated with the controllable\ndevice, and/or at least one controllable function associated with the controllable device.  The interrogation signal, the aspect(s) of the controllable device, and/or the information may take any of the forms described above in connection with FIGS.\n20-22A-C.\nIn some embodiments, as described above in connection with FIGS. 20-22A-C, wearable apparatus 110 may additionally be configured to analyze one or more images captured by the image sensor to detect at least one recognized gesture made by the\nuser and transmit a control signal configured to control at least, one aspect of the controllable device.  The aspect(s) of the controllable device may be associated with the recognized gesture, as described above.  The control signal and the aspect(s)\nof the controllable device may take any of the forms described above in connection with FIGS. 20-22A-C.\nReturning to FIG. 24, process 2400 may continue at step 2406 with analyzing the images to detect a visual trigger.  For example, at least one processing device (e.g., processor 210) in the wearable apparatus 110 may be programmed to analyze the\nimages to detect a visual trigger associated with the controllable device.  Example visual triggers may include, but are not limited to, movements of the user 100, a body part of the user 100, and/or an object associated with the user 100 such as a\nstylus or glove.  Alternatively or additionally, the visual trigger may include a hand gesture, an action involving a hand of the user 100, and/or an action involving the controllable device.  In some embodiments, wearable apparatus 110 may store in\nmemory 550 associations between visual triggers and controllable devices.  Such associations may be predetermined, user-defined, and/or provided by the controllable device (e.g., during pairing with the controllable device, as described above).  In some\nembodiments, wearable device 110 may further identify a type associated with the visual trigger.  For example, wearable device 110 may identify if the visual trigger is a hand gesture, an action involving the user's hand, an action involving the\ncontrollable device, etc.\nAn example visual trigger 2506 is illustrated in FIG. 25B.  While the visual trigger 2506 is shown to be a raise of an arm of user 100, it will be understood that other visual triggers are possible as well.  For example, the visual trigger 2506\nmay be another movement by the same and/or another body part of the user 100, such as a hand gesture (e.g., an upward flick of an index linger).  As another example, the visual trigger 2506 may be a movement by an object associated with the user 100,\nsuch as a stylus held by the user 100 or a glove worn by the user 100.  As still another example, the visual trigger 2506 may be an action involving the controllable device 2502, such as tapping the controllable device 2502 or making a movement near the\ncontrollable device 2502.\nThe images from environment 2500 captured by the image sensor(s) 220 of wearable apparatus 110 may include a representation of the visual trigger 2506, such that a processor 210 included in and/or communicatively coupled to wearable apparatus\n110 may analyze the images to detect the visual trigger 2506.  In embodiments where wearable apparatus 110 determines a context associated with the controllable device 2502, as described above, the context may alternatively or additionally be associated\nwith the visual trigger.\nIn some embodiments, instead or in addition to step 2406, process 2400 may also comprise a procedure for detecting a non-visual trigger, such as a non-visual trigger produced by user 100.  Some examples of such non-visual triggers may include a\npress of a button, an audible trigger, and so forth.  In some embodiments, instead or in addition to step 2404, process 2400 may also determine a type associated with the non-visual trigger.  For example, when the non-visual trigger comprises a press of\na button, the type of the non-visual trigger may be based on the identity of the pressed button, and the duration of the press, on the intensity of the press, and so forth, in another example, when the non-visual trigger comprises an audible trigger, the\ntype of the non-visual trigger may be based on sounds and/or speech included in and/or surrounding the audible trigger.\nProcess 2400 may continue at step 2408, with, based on the visual trigger and/or the non-visual trigger, transmitting a command.  For example, the processing device(s) in the wearable apparatus 110 may be programmed to cause at least one\ntransmitter (e.g., wireless transceiver 530) in the wearable apparatus 110 to transmit the command.  The command may be configured to cause a change in at least one aspect of the controllable device.  In embodiments where the controllable device is\npaired with wearable apparatus 110 (e.g., through visual pairing, as described above), the command may be transmitted via the communication path established by the pairing.\nIn embodiments where wearable apparatus 110 identifies a current state of the controllable device, wearable apparatus 110 may select the command from among a plurality of commands based on the identified current state of the controllable device. For example, if the controllable device is identified to be in an off state, wearable apparatus 110 may select, based on the identified off state, a command to power on the controllable device.  As another example, if the controllable device is\nidentified to be tuned to a particular channel or station, wearable apparatus 110 may select, based on the identified channel or station, a command to tune the controllable device to another channel or station.\nAlternatively or additionally, in embodiments where wearable apparatus 110 identifies a context associated with the controllable device and/or the user and/or the environment, wearable apparatus 110 may select values for one or more parameters\nassociated with the command based on the identified context.  For example, if the controllable device is a microwave, and an identified context is associated with a meal to be prepared held by the user, the one or more parameters may include settings for\nthe microwave determined based on the content of the meal.  As another example, if the controllable device is a vehicle, and the identified context is associated with a user preparing to leave, the one or more parameters may include starting the vehicle,\nadjusting a seat, position of the vehicle, and/or opening a garage door associated with the vehicle based on the user that is leaving.\nAlternatively or additionally, in embodiments where wearable apparatus 110 identifies a type associated with the visual trigger and/or the non-visual trigger, wearable device 110 may select the command from among a plurality of commands based on\nthe identified type and the visual trigger.  For example, if the identified type is a hand gesture, and the visual trigger is an upward flick of an index finger, and this type and visual trigger are associated with a command for turning on a light\nswitch, wearable device 110 may select the command to turn on the light switch.\nAlternatively or additionally, in some embodiments, wearable apparatus 110 may be further configured to obtain audio data captured by at least one audio sensor included in wearable apparatus 110.  The audio sensor may take the form of, for\nexample, a microphone, and wearable apparatus 110 may be further configured to process the audio data using one or more types of audio signal processing including, but not limited to, digital signal processing and/or voice recognition techniques, such as\nthose based on Hidden Markov Models, dynamic time warping (DTW) techniques, and/or neural networks.  In these embodiments, wearable apparatus 110 may select the command from among a plurality of commands based, at least in part, on the audio data.  For\nexample, if the controllable device is identified to be tuned to a particular channel or station, and the audio data indicates a requested channel or station (e.g., the user 100 states aloud the name or number of a channel or station), wearable apparatus\n110 may select, based on the audio data, a command to tune the controllable device to the requested channel or station.\nAs shown in FIG. 25C, for example, based on the visual trigger 2506, wearable apparatus 110 may transmit command 2508 to controllable device 2502, thereby controlling at least one aspect of controllable device 2502.  The controlled aspect(s) may\nbe associated with visual trigger 2506.\nFor example, based on the visual trigger 2506, wearable apparatus 110 may transmit command 2508 to cause controllable device 2502 to change from an unarmed state to an armed state.  As another example, based on the visual trigger 2506, wearable\napparatus 110 may transmit command 2508 to cause controllable device 2502 to lock a door associated with controllable device 2502.  As still another example, based on the visual trigger 2506, wearable apparatus 110 may transmit command 2508 to cause\ncontrollable device 2502 to contact emergency personnel, such as the police.  The control may be permanent or may be temporary (e.g., command 2508 may cause controllable device 2502 to power down for a period of time or change a setting of controllable\ndevice 2502 permanently).  Alternatively or additionally, controlling the aspect(s) may involve controlling any of the aspects described above, such as a mode of controllable device 2502, a brightness and/or intensity of controllable device 2502, a\nposition of controllable device 2502, or another function of controllable device 2502.\nAs described above, in some embodiments, wearable apparatus 110 may determine a current state of the controllable device 2502.  For example, wearable apparatus 110 may determine whether a door associated with controllable device 2502 is locked\nor unlocked.  In these embodiments, wearable apparatus 110 may select the command 2508 from among a plurality of commands based on the identified current state of controllable device 2502.  For example, wearable apparatus 110 may select a command 2508 to\nlock the door where it is determined that the door is unlocked, while wearable apparatus 110 may select a command 2508 to arm the security alarm where it is determined that the door is locked.\nIn some embodiments, wearable apparatus 110 may determine a context associated with the controllable device 2502.  For example, wearable apparatus 110 may determine a context indicating that user 100 is going to sleep.  In these embodiments,\nwearable apparatus 110 may select values for one or more parameters associated with the command 2508 based on the identified context associated with the controllable device 2502.  For example, when the context indicates that user 100 is going to sleep,\nwearable apparatus 110 may set parameters associated with the command 2508 to lock all the doors and windows, activate a motion sensor, and/or arm the security alarm.\nIn some embodiments, wearable apparatus 110 may receive an indication that the aspect(s) has changed.  For example, the controllable device and/or another device paired with wearable apparatus 110 may generate and provide the indication.  In\nanother example, wearable apparatus 110 may analyze the stream of images to identity a change in the aspect(s) of the controllable device.  After receiving the indication and/or identifying the change in the aspect(s) of the controllable device, wearable\napparatus 130 may provide feedback to user 100 indicative of the change in the aspect(s) of the controllable device.  Example feedback may include visual, audio, and/or haptic feedback.\nIn some embodiments, wearable apparatus 110 may receive an indication that the command transmitted by step 2408 was received, for example by the controlled device, by an intermediate device, by a paired device, and so forth.  For example, the\nindication may be received as an incoming signal using transceiver 530.  After receiving the indication, wearable apparatus 110 may provide feedback to user 100 indicative that the command was received.  Example feedback may include visual, audio, and/or\nhaptic feedback.\nWhile certain environments, controllable devices, visual triggers, and aspects are shown in FIGS. 25A-25C, it will be understood that these are merely exemplary and that other environments, controllable devices, visual triggers, and aspects are\npossible as well.  It should be noted that while wearable apparatus 110 may communicate commands directly to one or more controllable devices, such commands may also be provided to one or more intermediate devices.  For example, in some embodiments,\nwearable apparatus 110 may communicate a state or command with one or more intermediate hubs or controllers, and those hubs or controllers may distribute subcommands to devices (e.g., lights, locks, etc.) linked on a network to the hubs or controllers,\nthe subcommands being generated for accomplishing the received commands of wearable apparatus 110.\nCrowd-Sourced Vision-Based Information Collection\nIn some embodiments, a plurality of wearable apparatuses 110 may capture image data and may stream the image data to server 250 and/or computing device 120 and/or another wearable apparatus 110 for further processing.  Server 250 and/or\ncomputing device 120 and/or another wearable apparatus 110 may analyze the plurality of data streams to determine a trait common to two or more of the users of the plurality of wearable apparatuses.  As such, the analysis of the plurality of data streams\nmay be carried out, for example, by a computing device, such as server 250, computing device 120 and/or another wearable apparatus 110.  For exemplary purposes, the analysis of the plurality of data streams is described as being performed by server 250. \nIn one example of determining a trait common to two or more users wearing wearable apparatuses 110, the two or more users may face the same person, and their wearable apparatuses 110 may capture image data of that person.  The image data may be streamed\nto server 250 and analyzed to determine that the trait of the two users is an interaction with the same person.  The server may store the data representing the determined trait in a database and/or report information relating to the trait to the user.\nFIG. 26 is an illustration of an example environment 2600 including users (100, 101, 102) wearing wearable apparatuses 110 and a server 250 capable of communicating with wearable apparatuses 110 via network 240, consistent with disclosed\nembodiments.  Server 250 may be used in environment 2600 to determine traits of users 100, 101, and 102, for example relating to image data acquired from each wearable apparatus.  In some embodiments, each wearable apparatus 110 may be configured as\nshown in FIG. 7.  For example, wearable apparatuses 110 may include an orientation adjustment unit 705 configured to permit adjustment of image sensor 220.\nBy way of example, FIG. 26 shows three users, each wearing wearable apparatus 110, experiencing different situations.  Although only three users are shown, any number of wearable apparatuses 110 worn by users may communicate to server 250, for\nexample, through network 240.  The data sent to server 250 is, therefore, crowd-sourced because it comes from many different wearable apparatuses 110.  Each wearable apparatus 110 may acquire image-based information associated with images captured by the\ncamera on the particular wearable apparatus.  Image-based information may include raw images captured from the camera and formatted as jpeg, pic, tiff, mpeg, or any other suitable image format.  Image-based information may also include images\npre-processed by a processing device on the wearable apparatus 110.  The pre-processed images may be categorized, enhanced, compressed, or otherwise altered.  Image-based information may include logos, words and/or facial information extracted from the\nimages.  Image-based information may also include names of products, people, etc. that the users interact with.\nIn some embodiments, the image-based information may be of the situation or environment of the user wearing the wearable apparatus 110.  For example, user 100 may be shopping in a grocery store and retrieving an item from a shelf.  The wearable\napparatus 110 may capture images of the user reaching for the product (as well as nearby products) and transmit the image-based information to server 250.  In the second example, user 101 may be meeting another person.  Wearable apparatus 110 affixed to\nuser 101, thus, may acquire images of the person as user 101 is facing the person.  In the third example, user 102 is looking at a computer screen with a particular website displayed on the screen.  The wearable apparatus 110 being worn by user 102 may\nacquire images of the computer screen.\nIn some embodiments, each wearable apparatus 110 may transmit image-based information to server 250.  In some examples, wearable apparatuses 110 may first transmit the information to a network and then to server 250.  In other examples, the\nwearable apparatuses 110 may transmit the information directly to server 250.  In some embodiments, the image-based information may be streamed to server 250.  The data stream may occur in real-time (e.g., shortly after the image data is acquired, for\nexample within one second), or the stream may be delayed by a predetermined amount of time.  Thus, server 250 may receive one or more data streams of image-based information from each wearable apparatus 110.\nIn some embodiments, server 250 may include a data interface (not shown) that allows server 250 to receive the data streams from wearable apparatuses 110.  The data interface may include hardware and/or software to interface with network 240 or\ndirectly to wearable apparatuses 110.  For example, the data interface may include transmitting and receiving circuitry for a wired or wireless connection to network 240.  Server 250 may also include a processing device operatively coupled to memory for\nstoring instruction for the processing device.\nIn some embodiments, devices other than or in addition to server 250 may receive the transmitted data streams from wearable apparatuses 110.  For example, computing device 120 may receive the data streams.  Alternatively, any one or all of the\nwearable apparatuses 110 may receive the data streams from the other wearable apparatuses 110.\nIn some examples, server 250 may analyze the image-based information in each of the received data streams.  For example, server 250 may receive a data stream from a particular wearable apparatus 110 using the data interface.  Server 250 may,\nusing the processing device, unpack or extract the image-based information in the data stream.  In some examples, the image-based information may be a series of images captured by the camera on the wearable apparatus 110.  In other examples, the\nimage-based information may include camera settings, such as f-stop, focal length, light and color content of the image, etc.\nServer 250 may analyze the image-based information to determine at least one trait of the user wearing the wearable apparatus 110.  A trait in this aspect may refer to a descriptor associated with the user wearing the wearable apparatus 110. \nThe descriptor may refer to any aspect or aspects of the user's interactions with the user's environment.  In some examples, a trait may involve a situation, environment, and/or activity of the user.  A trait may also involve an action and/or interaction\nby the user.  For example, a trait may include interactions with people, interactions with certain products or types of products, activities engaged in, materials read and/or reviewed, etc. In some embodiments, server 250 may determine the frequency at\nwhich a trait occurs.  Server 250 may, for example, increment a counter each time the particular trait is determined and associate the counter with the trait.  Thus, each trait, or category of traits, may be associated with a respective counter to keep\ntrack of the number of occurrences of the trait.  Moreover, timers may be used to track the number of occurrences over time.  The frequency of occurrence may be associated with the trait and stored, as described below, in a database.  In some examples,\nserver 250 may use image recognition algorithms and/or machine vision algorithms to determine objects and/or persons in the received images.  For example, optical character recognition (OCR) may be used to determine words in an image, such as on a paper,\nsign, book, etc. Detected words may be used to recognize consumer products, brand names, and/or categories of products.  In some examples, edge and shape recognition may be used to determine objects in images, such as a ball, a tree, line on a playing\nfield, etc. Facial recognition may be used to determine features on a human face, where the features may be compared to a database of features to determine the identity of a person.  In some examples, context analysis may be used to determine situations\ninvolving the recognized words, object, people, etc. For example, an image may be captured and analyzed to determine the presence of a ball, grass, lines on the grass, and a soccer player.  Contextual analysis may then determine that the user is\nattending a soccer game.  Other non-exhaustive examples of context analysis include: water+boat=boating; grass+ball=sport (the type of ball may also be recognized to determine if the sport is, for example, soccer, baseball, football, etc.); recognize\nlines of a playing field; aisles products--grocery store; read text on object to recognize brands of products (e.g., Colgate, etc.); recognize general descriptive words (e.g., potatoes, milk, etc.), for example, at a produce market.\nFurthermore, in some embodiments, for example, analyzing images may involve edge identification, in which an image is analyzed to detect pixels at which discontinuities (e.g., sudden changes in image brightness) occur and edges (e.g., edges of\ntire external object) are identified to coincide with the detected pixels.  Alternatively or additionally, in some embodiments analyzing images may involve identifying in and/or extracting from an image pixels representative of objects in the\nenvironment, such as the external object.  Pixels may be determined to be representative of an external object based on, for example, other images of the external device or similar external devices maintained, e.g., in a database and/or predetermined\ndata describing the external object maintained, e.g., in a database.  Alternatively or additionally, pixels may be determined to be representative of an external object based on, for example, a trained neural network configured to detect predetermined\nexternal objects.  Other types of analysis are possible as well, including, but not limited to, gradient matching, greyscale matching, scale-invariant feature transform (SIFT) matching, and/or interpretation trees.\nIn the case where multiple images are received over a period of time, server 250 may compare sequential images to determine actors and/or actions taking place in the images.  For example, as illustrated in FIG. 26, user 100 is reaching towards a\nproduct on the shelf in a grocery store.  Analysis of the image-based data received from the wearable apparatus 110 being worn by user 100 may determine that the user is reaching towards a specific commercial product, such as a can of peas, and it may\nfurther be determined the particular brand of peas.  Analysis of the image-based information may also determine the brands of products near to the particular brand that the user is interacting with.  Thus, server 250 may determine that the user 100 is\ninteracting with a commercial product and classify this interaction as a trait.  Server 250 may further determine that user 100 is shopping based on extracted visual cues in the received image-based information.  Thus, a separate trait may be determined\nand classified as shopping.  In some examples, server 250 may also use non-visual information in order to obtain such determinations.  Examples of such non-visual information may include, but are not limited to: financial information, including billing\ninformation; positioning information, such as geolocation data; temporal information, including time of day, date, etc.; information obtained from a calendar and/or day planner; information obtained from a social network; information based on captured\naudio; and so forth.\nIn some embodiments, server 250 may determine the frequency at which user 100 interacts with commercial products.  For example, server 250 may keep track of the number of times a certain trait occurs with the user.  Server 250 may, for example,\nincrement a counter each time the particular trait is determined and associate the counter with the trait.  The frequency of occurrence may be associated with the trait and stored, as described below, in a database.\nSimilarly, server 250 may analyze the received data stream from wearable apparatus 110 being worn by user 101.  In this example, user 101 is interacting with a person (e.g., shaking hands, a face appearing within a predetermined distance,\ndetection of eyes having a looking direction toward the user, etc.).  The image-based information received from user 101 may be analyzed to determine several aspects of the interaction.  For example, the identity of the person may be determined using\nfacial recognition.  The type of interaction with the person may also be determined.  For example, edge and contour recognition may determine that user 101's arm and the other person's arm are meeting between their bodies, and context analysis may be\nused to deter mine that a handshake is taking place.  The mood of the person may also be determined by, for example, employing trained neural networks to determine characteristics related to mood, such as furrowed brow--angry, smiling, laughing,\ntears/crying--sad etc. The environment surrounding the interaction may also be determined by recognizing objects and landscapes, etc. Thus, in the example shown, server 250 may determine that the user 101 is interacting with a person (e.g., greeting the\nperson by shaking hands) and classify this interaction as a trail.  Again, multiple traits may be determined from the same situation.  For example, the trait may be classified as a \"greeting,\" a \"salutation,\" a \"business event\" (if the data indicate that\nthe person is a business colleague), etc. In some examples, server 250 may also use non-visual information in order to obtain such determinations, as described above.  For example, server 250 may use information from a calendar and/or day planner to\nidentify the nature of the interaction, the identity of the person, and so forth.  Along the same lines, server 250 may receive a data stream from wearable apparatus 110 being worn by user 102.  In this case, user 102 is looking at a computer screen\nwhere a particular website is being displayed.  Server 250 may receive images of the website as content in the data stream.  Server 250 may analyze the received data stream to determine and/or identity the website.  For example, server 250 may determine\nthe URL of the website, recognize images on the web page and compare the images to known images for particular websites, recognize logos, read an entity name from the displayed page, etc., and determine that user 102 is visiting the identified website. \nThus, server 250 may determine that the user 102 is visiting a particular website and classify this interaction as a trait.  Furthermore, in some examples, server 250 may analyze the received data stream to determine one or more activities associated\nwith the website, such as searches, posting, and so forth.\nIn some embodiments, server 250 may determine the frequency at which user 102 visits a particular website, interacts with a particular person, performs a particular task, chooses a particular consumer product, etc. As explained above, timers and\ncounters associated with particular traits or categories of traits.\nIn other examples, the image-based information may relate to a hand gesture captured by wearable apparatus 110 (not shown in FIG. 26).  For example, the user may point to an object, swipe in a particular direction in front of the camera, wave to\na person, etc. The hand gesture captured in the image-based information may indicate an action to be taken, such as retrieving information about a product, saving a location of an item, deleting an item for a database, etc. The hand gesture may also be\nused by server 250 in contextual analysis of a situation.  For example, if a user waves to another person, server 250 may determine that the user and the person are friends.  Wearable apparatus 110 may acquire images of the hand gesture and stream the\nimage-based information to server 250.  Server 250 may analyze the image-based information to determine the gesture performed.  In some examples, server 250 may analyze image-based information from two or more wearable apparatuses 110 to determine the\ngesture.  For example, the wearable apparatus of a first user may capture partial images of the hand gesture such that the gesture is not fully recognized.  A second user with a different wearable apparatus may be in the same environment and also capture\nimages of the first user's hand gesture.  Server 250 may recognize that the images from the second user include a hand gesture from the first user and subsequently recognize the hand gesture associated with the first user.\nSeveral examples of traits have been described above, but other traits are possible.  Further examples of traits may be: experiencing a sporting event, (e.g., the field, type of ball, uniforms, etc. may be recognized to determine particular\nsporting event), a birthday party (e.g., objects, such as a cake, candles, decorations, etc. may be recognized), eating a meal (e.g., the location, logo of a restaurant, type of food may be recognized), driving a car (e.g., steering wheel may be\nrecognized along with acceleration occurring when the images were captured), etc.\nIn some embodiments, two or more users may be experiencing or may have experienced the same or similar situation (e.g., trait).  In this case, server 250 may determine that two or more users have at least one trait in common.  For example, two\nof the users may be shopping in the same grocery store.  Server 250 may analyze the data streams from the two users and determine that both users have interacted with the same commercial product.  Thus, the common trait may be identified.  In some\nexamples, the common trait may be interaction with the same person, performing the same action (e.g., shaking hands), visiting the same website, etc.\nIn some embodiments, server 250 may analyze the received data streams in parallel to determine the common trait.  For example, image-based information with images of the same situation from two or more wearable apparatuses 110 may be analyzed. \nServer 250 may recognize that the images provide views from different angles of the same environment.  For example, server 250 may use location coordinates of where the images were taken to determine that they are of the same environment.  Server 250 may\nalso recognize names of places or other location identifiers to determine that the images are of the same environment.  In other examples, server 250 may use timestamps to determine that the images were taken at the same or similar time.  Furthermore,\nserver 250 may recognize the same objects or persons in images from the different data streams.  Analyzing the images from the multiple data streams in the same environment may aid in determining the trait for each user.  For example, the image-based\ninformation may be correlated to, for example, recognize individuals, products, activities, contexts, etc. from different angles.  Moreover, server 250 may also determine the frequency at which each user experiences the particular trait.  For example,\nserver 250 may determine a frequency or an average time of interaction for the common trail and store the frequency or average time, as described below.\nIn some embodiments, server 250 may store the information relating to the determined trait or traits in a storage resource 2610.  The storage resource 2610 may be local to server 250, at a remote location, distributed (e.g., in multiple\nlocations on part of multiple systems), and so forth.  In some examples, the storage resource 2610 may include internal memory of server 250 and/or computing device 120.  Storage resource 2610 may also include memory within one or more of wearable\napparatuses 110.  In some examples, storage resource 2610 may be a nonvolatile storage medium, such as a hard disk or a solid state disk.  Server 250 may store the information relating to the determined trait or traits in a database on storage resource\n2610.  The database may be of any known kind, such as a relational database or a self-referencing database.  The database may include demographic information about the user or users associated with the trait or traits.  For example, a database entry\ncontaining information relating to the determined trait or trails may also contain biographical information (e.g., demographics) about the user or users associated with the trait or traits.\nIn some embodiments the stored information and/or information based on the stored information may be output to a user.  For example, server 250 may include a monitor 2620 and display a graphical user interface (GUI) containing the information\nrelating to the determined traits.  In other examples, the stored information may be displayed on computing device 120.  In still other examples, the stored information may be presented to the user through a display on a wearable apparatus.  In some\nembodiments, the stored information may be communicated to a user audibly.  For example, the results of a query to the database may be read by devices, such as server 250, computing device 120, and/or wearable apparatus 110.  The displayed or otherwise\ncommunicated information may include the determined trait and/or demographics information about the users of the wearable apparatuses to which the trail belongs.\nIn some embodiments, server 250 may receive a query pertaining to the stored information.  Server 250 may respond to the query by transmitting information requested in the query to the requesting device.  For example, computing device 120 may\nsend a query to server 250 requesting particular information about a trait, such as shopping in a grocery store.  Server 250 may access the database containing the stored information, sort the information and organize a response.  For example, the\nresponse may contain the most common commercial product chosen, the most active grocery store, where in a grocery store a particular item is located, etc.\nIn some embodiments, the data stream may include more information than just image-based information.  For example, the data stream may also include position information, recognizing the location of where the images were captured.  In some\nexamples, position information may include global position system information (e.g., latitude and longitude), proximity to Wi-Fi hotspots or cellular towers, or other absolute positioning information.  Position information may also include names of\nlocations where the images were captured.  For example, a user may input location information, such as \"home\" or \"work\" or \"Leaning Tower of Pisa,\" In other examples, image recognition may be used to identify the location of images.  Wearable apparatus\n110 and/or server 250 may analyze the Images and determine a location based on identifying characteristics of the images.  By recognizing specific objects, places, words, etc. in the images.  For example, the words \"Hank's Supermarket\" may be recognized\non an identified building that the user walks toward.  Server 250 may search a database to determine the location of \"Hank's Supermarket\" and assign a location to the images.\nIn some embodiments, server 250 may analyze the received data stream containing position information to determine the position of wearable apparatus 110.  For example, the processing device may be programmed to extract the position information\nfrom the one or more data streams received from wearable apparatuses 110, and based on the position information, determine the location of each wearable apparatus.  Server 250 may store the determined location of the wearable devices in the database as\nadditional information about the determined trait or traits.\nIn some embodiments, the one or more data streams may include timing information, which may identify when a particular image is captured by a wearable apparatus 110 or the duration between captured images.  For example, each wearable apparatus\n110 may provide a timestamp (including a date and/or time) associated with each captured image.  The timestamp may be integrated into the image-based information and/or associated a particular image by reference.  In some examples, server 250 may extract\nthe timing information to order received image-based information prior to or while analyzing the image-based information to determine the trait or traits.\nIn some embodiments, interaction frequency, schedule, and/or duration may be determined, in part, using the timing information.  For example, timing information may be associated with the determination of a trail for each user.  Based on the\ntrait timing information, server 250 may determine a pattern of interaction, such as a schedule, and associate the schedule with the user and trait.  In other examples, timing information may be used to determine how long a particular trait lasts.  For\nexample, timing information may be used to determine how long a user (e.g., user 102) spends on a particular website.\nIn some embodiments, the one or more data streams may include information based on audio captured from an environment of wearable apparatus 110.  For example, the information based on audio may include conversation topics, transcriptions, noise\nlevel, musical genre, and so forth.  In some examples, the information based on audio may be used to determine a type of interaction a user is involved in, a preference of the user, and so forth.\nIn some embodiments, the one or more data streams may include motion information related to motion of the wearable apparatus 110.  For example, each wearable apparatus 110 may include a motion sensor, such as a multi-axis accelerometer,\ngyroscope (e.g., MEMs-based), pressure sensor, magnetic sensor (e.g., compass), etc. Motion information may be associated with the captured images in the image-based information and/or include in the data stream with reference to captured images. \nBecause the motion information may correspond to motion of the wearable apparatus, the motion information may also be unassociated with captured images and transmitted in the data stream as independent data.  In some embodiments, server 250 may receive\nthe motion information as part, of the received data stream.  Server 250 may extract the motion information from the data stream and use the information to determine the trait or traits of each user.  For example, server 250 may determine that user 101\nwalks towards the person before shaking hands.  The received motion information would indicate that the handshake is a greeting and the trait may be classified accordingly.  In another example, the motion information may be used to determine or confirm\nparticipation in particular activities.  For example, server 250 may recognize a soccer ball on a grass field and tentatively determine that the user is playing soccer.  Server 250 may confirm the tentative determination by analyzing the motion\ninformation and proximity of the ball to confirm that the user is playing soccer.\nFIG. 27A illustrates an exemplary embodiment of memory 2710 containing software modules to determine a trait common to two or more users.  For example, one or more of server 250, computing device 120, or wearable apparatus 110 may execute\ninstructions from the modules to perform one or more of the functions as described with respect to FIG. 26, above.  Included in the memory are receiving module 2711, analysis module 2712, and storing module 2713.  Modules 2711, 2712, and 2713 may contain\nsoftware instructions for execution by at least one processing device included with a server-based system, such as is included in server 250.\nReceiving module 2711 may be configured to receive one or more data streams transmitted by wearable apparatuses.  Receiving module 2711 may interact with a data interface to receive the one or more data streams.  Receiving module 2711 may\ncontrol the data interface to receive multiple data streams simultaneously from one or more transmission sources.  For example, data streams may be received through a wired connection or through a wireless connection or through both.\nAnalysis module 2712 may be configured to analyze the one or more received data streams to determine one or more traits associated with different users wearing wearable apparatuses originating the data streams.  Analysis module 2712 may extract\ninformation from the data streams to aid in determination of specific traits of the users.  For example, analysis module 2712 may extract image-based information, position information, timing information, and/or motion information from the data streams. \nAnalysis module 2712 may use facial detection and recognition to determine traits, such as interaction with other persons.  Analysis module 2712 may also use machine vision algorithms to determine traits such as identifying commercial products,\nlandscape, objects, locations, etc. Furthermore, analysis module 2712 may use position information to determine a location of a trait; timing information to determine frequency, scheduling, and/or duration of a trait; information based on audio; and\nmotion information to determine specific interaction states, such as movement towards an object.\nStoring module 2713 may be configured to store information relating to a trait, such as a trait that is common to two or more user wearing wearable apparatuses.  Storing module 2713 may include instructions to interact with internal or external\nstorage resources, such as memory, solid state hard drives, or removable memory devices.  Storing module 2713 may also interact with the data interface to access remote storage resources such as computing device 120 or wearable apparatuses 110.  Storing\nmodule 2713 may also store information relating to the determined traits in cloud computing storage devices.\nFIG. 27B illustrates the contents of a data stream 2720 received by a processing device, such as server 250, computing device 120, or wearable apparatus 110.  The data stream 2720 may consist of packet or frames that are transmitted by any\nsuitable transmission means.  For example, the data stream 2720 may be transmitted through the Internet using a TCP/IP protocol, over wireless connection using Bluetooth.RTM., or other suitable transmission means.  The packets or frames may be divided\ninto segments containing image-based information 2721, position information 2722, timing information 2723, and/or motion information 2724.  As shown in FIG. 27B, position information 2722, timing information 2723, and motion information 2724 are\noptional, as depicted by the dashed line.  In some examples, position information 2722, timing information 2723, and/or motion information 2724 may be included in image-based information so that the data stream 2720 would use only one segment.  In other\nexamples, position information 2722, timing information 2723, and/or motion information 2724 may be associated with image-based information 2721, such as by reference.  The association of position information 2722, timing information 2723, and/or motion\ninformation 2724 may be to the image-based information in the same or different packets or streams.  In some examples, the data stream may contain a header identifying the wearable apparatus 110 from which the data stream is transmitted.  The header may\nalso contain demographic information about the user wearing the wearable apparatus 110.\nIn some examples, image-based information 2721 may comprise one or more images captured by the camera in wearable apparatus 110.  The one or more images may be compressed and may be represented in any suitable format, such as JPEG, TIFF, PDF,\nGIF, PNG, BMP, SVG, etc. In some examples, image-based information 2721 may comprise video in any suitable format, such as MPEG, AVX, MOV, etc. Image-based information 2721 may also contain meta-data with additional image related information, such as\ncamera settings, location, time, etc. Alternatively, in some embodiments, image-based information 2721 may include information derived from analysis of one or more images (e.g., a description of a context of an image, an identifier of a person, object,\nor location in an image, etc.).  In some examples, image-based information 2721 may comprise information derived from the one or more images captured by the camera by wearable apparatus 110.\nPosition information 2722 may be, included in data stream 2720.  Position information 2722 may include GPS location coordinates of the camera in wearable apparatus 110.  Position information may also include other triangulated coordinates, such\nas derived from Wi-Fi, cellular towers, etc. Positional information 2722 may be embedded in image-based information 2721 or be included in a separate segment of data stream 2720.\nTiming information 2723 may be included in data stream 2720.  Timing information 2723 may include absolute or relative time.  For example, Timing information 2723 may include the time of day and/or date that an image was captured.  Timing\ninformation 2723 may also include the time since a last image was captured, providing relative timing information between images.  Timing information 2723 may be embedded in image-based information 2721 or be included in a separate segment of data stream\n2720.\nMotion information 2724 may be included in data stream 2720.  Motion information 2724 may include acceleration information in multiple direction (e.g., 3-axis, 6-axis, 9-axis), speed information, or other data that indicates motion of wearable\napparatus 110.  Motion information 2724 may be embedded in image-based information 2721 or be included in a separate segment of data stream 2720.\nIn some examples, other types of information may be included in data stream 2720.  For example, data stream 2720 may include audio information 2725.  Audio information 2725 may include audio captured by wearable apparatus 110 and/or information\nbased on audio captured by wearable apparatus 110.\nFIG. 28 is a flowchart illustrating an exemplary method 2800 of determining and storing a trait of a user or a trait common to two or more users consistent with the disclosed embodiments.  The method may be implemented in a system such as shown\nin FIG. 5 and/or FIG. 26.\nAt step 2805, one or more data streams are received.  In some examples, the data streams may be received from wearable apparatuses 110 and be configured as described above in connection with FIG. 27B.  The one or more data streams may be\nreceived from one or more wearable apparatuses 110.  The one or more data streams may contain image-based information, position information, timing information, motion information, audio information, and so forth.  The one or more data streams may be\nsent in packets, frames, and so forth.  The data streams may be continuously received over time, received at predetermined intervals, received at selected times, received in response to a trigger, and so forth.\nAt optional step 2810, the contents of the one or more data streams may be extracted.  For example, the data streams may contain image-based information, position information, timing information, and/or motion information.  The data streams may\nbe processed by a processor device to extract the type of information desired for further processing.\nAt step 2815, the one or more data streams may be analyzed to determine at least one trait.  In some examples, the trait is common to two or more users from which the data streams are received.  In some embodiments, server 250 may analyze the\ndata streams.  In other embodiments, computing device 120 or wearable apparatus 110 may analyze the data streams.  For example, the image-based information in the data streams may be analyzed by executing instructions stored in analysis module 2721.  In\nsome embodiments, the image-based information may be analyzed to determine whether a trait identified in a data stream received from one user is common with a trait identified in a data stream received from a different user.  In some embodiments,\nposition information, timing information, and/or motion information may be analyzed in addition to image-based information in order to determine a trait.  For example, image-based information may be analyzed using machine vision or image recognition\nalgorithms to determine that a user's hand is near a commercial product.  Timing and motion information may be used to determine that the user's hand is moving towards the commercial product.  Thus, a trait of choosing a commercial product may be\ndetermined.  In some examples, two or more users may be choosing the same or similar product.  Therefore, the trait is determined to be common to the two or more different users.\nAt step 2820, information related to the at least one identified trait may be stored.  The at least one identified trait may be common to two or more different users.  The at least one identified trait may be stored in a database, such as a\nrelational database or self-referencing database.  The database may be stored, for example, in server 250, computing device 120, wearable apparatus 110, among distributed systems, or in a separate storage resource, such as a cloud computing device.  In\nsome embodiments, the stored information relating to the at least one trait, or information based on the stored information, may be output to a graphical display.  For example, the information may be output to a graphical display of a device (e.g.,\ncomputing device 120) paired with wearable device 110 and/or to a display associated with server 250 (e.g., monitor 2620).\nContext-Based Suggestions Through Image Analysis\nIn some embodiments, apparatus 110 may cause a paired device, such as computing device 120, to provide one or more alerts to user 100 based on information determined from an identified contextual situation.  As mentioned above, contextual\nsituations may refer to a combination of circumstances that may influence the user's action.  Examples of factors that may differentiate contextual situations include: the identity of other people in the vicinity of user 100 (e.g., certain individual,\nfamily members, coworkers, strangers, and more), the type of activity user 100 is doing (e.g., watching a movie, meeting with an individual, visiting a location, interacting with an object, entering a car, participating in a sport activity, eating a\nmeal, and more), the time in which the situation took place (e.g., the time of the day, the time of the year, and more), the location in which the situation occurs (e.g., home, working place, shopping mall, and more).  During a typical day, user 100 may\nexperience dozens, if not hundreds, of contextual situations.  Identifying these contextual situations can assist in categorizing and organizing the personal experiences of the user's life.  Moreover, consistent with this aspect of the disclosure,\napparatus 110 can analyze in real-time the images captured from the environment of user 100 to identify a current contextual situation.  By identifying contextual situations substantially in real-time, apparatus 110 can provide added value to user 100. \nFor example, after identifying a contextual situation, apparatus 110 may cause computing device 120 to provide one or more alerts to user 100.  In some cases, the task of determining which alerts to provide user 100 may be very complex, because of the\nhuge amount of similar contextual situations that user 100 experiences every day and the desire to provide only relevant alerts.  For example, when the contextual situation includes an identification of a worker at a work site, apparatus 110 may provide\nan alert that indicates that the worker is not using and/or wearing safety equipment.  Additional exemplary embodiments of contextual situations and the type of alerts that may be provided to user 100 are discussed in further detail with respect to FIGS.\n29A-29D.\nFIG. 29A is a schematic illustration of a contextual situation that may trigger provisioning of an alert consistent with the present disclosure.  The contextual situation illustrated in this figure is transitioning from indoors to outdoors. \nApparatus 110 may identify this contextual situation by analyzing a plurality of images, such as image 2900.  After identifying this contextual situation, apparatus 110 may cause computing device 120 to provide different alerts to user 100.\nOne type of alert that may be provided to user 100 includes a suggestion 2902 to remember a key.  In one example, suggestion 2902 may include the last captured image of the key 2904.  Another type of alert provided to user 100 includes a\nsuggestion (not shown) to remember rain gear.  In one embodiment, prior to providing this alert, computing device 120 may check the weather forecast online to determine the likelihood that a rain gear will be needed.  Another type of alert that may be\nprovided to user 100 includes a suggestion (not shown) to change the heating, ventilation, and air conditioning (HVAC) settings.  In one embodiment, prior to providing this suggestion, apparatus 110 may determine that there are no other individuals\nindoors.  Another type of alert that may be provided to user 100 includes a suggestion (not shown) to feed a cat.\nUser 100 may transition from indoors to outdoors numerous times a day.  Not ail the alerts may be relevant to all of the users, and not all the alerts will be relevant all the time.  In some embodiments, apparatus 110 (or computing device 120)\nmay determine which alerts to provide and when to provide them to user 100.  For example, apparatus 110 may cause computing device 120 to provide user 100 with suggestion 2902 to remember his/her key in some cases and avoid providing user 100 with\nsuggestion 2902 in other cases.  In order to determine which cases are relevant, apparatus 110 (or computing device 120) may use predefined context, rules.  The context rules may be determined over time using machine learning, be selected by user 100, or\nmay be the result of default settings.  Examples of context rules may include providing user 100 with suggestion 2902 only when user 100 leaves his/her own house, or only when the transitioning from indoors to outdoors occurs between 6 a.m.  and 9 a.m.,\nor only if apparatus 110 determined that user 100 didn't hold his/her keys in the last 30 minutes.  Apparatus 110 (or computing device 120) may apply different combinations of these context rules using logical operators (e.g., AND, OR, NOT, etc.) to\ndetermine when to provide user 100 the alert.  For example, suggestion 2902 may be provided only when user 100 leaves his/her own house AND the time is between 6 AM and 9 AM.\nFIG. 29B is a schematic illustration of another contextual situation that may trigger provisioning of an alert consistent with some embodiments of the present disclosure.  The contextual situation illustrated in this figure is exiting a vehicle. Apparatus 110 may identify this contextual situation by analyzing a plurality of images, such as image 2910.  After identifying this contextual situation, apparatus 110 may cause computing device 120 to provide different alerts to user 100.\nOne type of alert that may be provided to user 100 may include a reminder 2922, indicating that a child is present in the vehicle.  In some embodiments, apparatus 110 may cause computing device 120 to provide reminder 2922 after a determination\nwas made that the child is indeed present in the vehicle.  For example, apparatus 110 may have previously captured images of the child in the vehicle after user 100 entered the vehicle.  Another type of alert provided to user 100 may include a suggestion\n(not shown) to pay for parking using a parking application.  In some embodiments, apparatus 110 may cause computing device 120 to provide this suggestion after a determination was made that the vehicle is parked in area where payment is required. \nAdditional details regarding the determination process is provided below with reference to FIG. 30.\nFIG. 29C is a schematic illustration of yet another contextual situation that may trigger provisioning of an alert consistent with some embodiments of the present disclosure.  The contextual situation illustrated in this figure is entering a\ngrocery store.  Apparatus 110 may identify this contextual situation by analyzing a plurality of images, such as image 2920.  After identifying this contextual situation, apparatus 110 may cause computing device 120 to provide different alerts to user\n100.\nOne type of alert that may be provided to user 100 may include a suggestion 2922 to purchase one or more items.  The one or more items may be previously identified by apparatus 110 as products that user 100 needs or wishes to buy.  Specifically,\nsuggestion 2922 may be provided based on a determination using prior captured images, that a container associated with the item was discarded by user 100.  For example, apparatus 110 may have previously identified that user 100 threw an empty container\nof milk into a waste receptacle, which will automatically cause the milk product to be included in the grocery list.  Alternatively, the one or more items may have been previously identified by user 100 as products that he/she wants to buy.  For example,\nuser 100 may see an interesting recipe in a book and use a predefined hand movement, or voice command to indicate a selection of this recipe.  Another type of alert provided to user 100 may include a coupon 2924 for a specific product.  Apparatus 110 may\nuse computing device 120 to search for coupons available in the specific grocery store that user 100 entered to and which may have value to user 100.\nFIG. 29D is a schematic illustration of still another contextual situation that may trigger provisioning of an alert consistent with some embodiments of the present disclosure.  The contextual situation illustrated in this figure is a document\npresent in an area in front of user 100.  Tire document in front of user 100 may be printed on paper or digitally displayed on a screen.  Apparatus 110 may identify this contextual situation by analyzing a plurality of images, such as image 2930.  After\nidentifying this contextual situation, apparatus 110 may cause computing device 120 to provide different alerts to user 100.\nIn the embodiment illustrated in FIG. 29D the document in front of user 100 is a business card.  In this embodiment, apparatus 110 may transmit details (e.g., text and/or image) from the business card as the determined information associated\nwith the contextual situation.  In some embodiments, text may be determined using optical character recognition (OCR).  Transmitting the details to computing device 120 may cause the computing device 120 to add a new contact based on the received\ndetails.  In addition, apparatus 110 may be configured to transmit the details to computing device 120 in response to a visual trigger or a voice command from user 100.  For example, the visual trigger may include placing the business card opposite to\napparatus 110 for a predefined period of time, such as three seconds.  In another embodiment, the document may include an address and apparatus 110 may transmit the address appearing on the document as the determined information associated with the\ncontextual situation.  Transmitting the address to computing device 120 may cause the computing device 120 to add the address to a user interface associated with a navigation assistance application.  For example, the document may be an invitation to an\nevent and after apparatus 110 recognized the invitation in front of user 100, apparatus 110 may cause the user's smartphone to start navigating to the location of the event.  In another embodiment, the document in front of user 100 is a financial\ndocument.  In this embodiment, apparatus 110 may transmit details (e.g., text, numbers, figures) from a financial document as the determined information associated with the contextual situation.  Transmitting the details from the financial document to\ncomputing device 120 may cause at least some of the details from the financial document to appear in a user interface associated with a financial-related application on computing device 120.  For example, the financial document may be a bank account\ncheck, and once apparatus 110 recognizes the check in front of user 100, it may causes the user's smartphone to open his/her online banking application and starts the process of depositing the check.\nA person skilled in the art can appreciate that many more types of contextual situations (not shown in the figures) may trigger event more types of alerts consistent with the present disclosure.  For example, as discussed above, the contextual\nsituation may include an identification of at least one worker at a work site and the alert indicates that the worker is not using and/or wearing safety equipment.  The present disclosure is not limited to the disclosed exemplary contextual situations\nand exemplary alerts.  In addition, the different types of alerts discussed above with respect to FIGS. 29A-29D may be part of predefined functions that were triggered by the transmission of image-related information.  The image-related information may\nbe determined based on a request associated with a category tag from the paired device, as discussed above with respect to FIG. 19.\nAs discussed above, apparatus 110 may analyze the plurality of images to identify a contextual situation related to user 100.  Consistent with present disclosure, analyzing the plurality of images to identify the contextual situation may include\nexecuting a region-of-interest (ROI) analysis to detect at least one object in an image.  The at least one object in the image may be represented by one or more ROI that can be varied in types and formats.  Examples of region of interest descriptions\ninclude, but are not limited to bounding boxes, masks, areas, and polygons.  Apparatus 110 may detect the at least one object using a ROI database that contains searchable fields and vectors representing the image characteristics of objects.  In some\nembodiment, each entry in the ROI database may have a data structure that includes one or more of the following components: an ROI type field that contains the type of the region of interest, an ROI descriptor field that contains the description of the\nregion of interest, a color vector, a shape vector, a texture vector, a size vector, a location vector, and a content vector.  Apparatus 110 may detect the at least one object in the image by comparing the one or more ROI that are found in the image with\nthe ROT found in the ROI database.  After detecting the at least one object in the image, apparatus 110 may start the process of identifying the contextual situation.  In one embodiment, each contextual situation may include data identifying one or more\nobjects that a particular type of contextual situation has to include, data identifying one or more objects that the particular type of contextual situation may include, and data identifying one or more objects that the particular type of contextual\nsituation could not include.  By considering the detected objects in the image, apparatus 110 may select a plurality of candidate contextual situations that may be appropriate for the image being analyzed.  To improve the certainty level, apparatus 110\nmay use image-information from previously obtained images, and additional information that does not originate from image data, such as, information from a Global Positioning System (GPS).  In one embodiment, apparatus 110 may rank the plurality of\ncandidate contextual situations.  Accordingly, in one embodiment, the identified contextual situation may be the highest ranking contextual situation from the plurality of candidate contextual situations.\nFurthermore, in some embodiments, for example, analyzing the plurality of images may involve edge identification, in which an image is analyzed to detect pixels at which discontinuities (e.g., sudden changes in image brightness) occur and edges\n(e.g., edges of the external object) are identified to coincide with the detected pixels.  Alternatively or additionally, in some embodiments analyzing the plurality of images may involve identifying in and/or extracting from an image pixels\nrepresentative of objects in the environment, such as the external object.  Pixels may be determined to be representative of an external object based on, for example, other images of the external device or similar external devices maintained, e.g., in a\ndatabase and/or predetermined data describing the external object maintained, e.g., in a database.  Alternatively or additionally, pixels may be determined to be representative of an external object based on, for example, a trained neural network\nconfigured to detect predetermined external objects.  Other types of analysis are possible as well, including, but not limited to, gradient matching, greyscale matching, scale-in variant feature transform (SIFT) matching, and/or interpretation trees.\nFIG. 30 is a flowchart of an example process 3000 for providing one or more alerts to user 100.  The process starts when apparatus 110 identifies a contextual situation (block 3002) and ends when computing device 120 provides one or more\nrelevant alerts to user 100 (block 3022).  In one example, the one or more relevant alerts may include all the alerts determined to be relevant to user 100.  The identification of the contextual situation may be done by apparatus 110 when apparatus 110\nanalyzes the plurality of images captured from an environment user 100.  Relevant alerts may be provided by computing device 120 as a visual output or as an audible output.\nFIG. 30 depicts two ways to complete process 3000.  The first way, marked as path \"A,\" is to transmit information about the identified contextual situation to computing device 120 (block 3004) and thereafter continuing the process (blocks\n3006-3022) at computing device 120.  The second way, marked as path \"B,\" is to continue the process (blocks 3006-3018) at apparatus 110 and afterwards transmit information related to the one or more relevant alerts to computing device 120 (block 3020).\nA processing device, either processor 210 or processor 540, may access a plurality of alerts associated with the identified contextual situation (block 3006).  The alerts may be stored in an associated memory device, for example, memory 550 or\nmemory 550b.  For example, in case the identified contextual situation is transitioning from indoors to outdoors, the potential alerts may include: a suggestion to remember a key, a suggestion to remember rain gear, a suggestion to change an HVAC\nsetting, a suggestion to feed a pet, and more.  The processing device may select one alert associated with the identified contextual situation (block 3008), for example, the suggestion to change an HVAC setting.\nNext, the processing device may access a plurality of context rules associated with the selected alert (block 3010).  The plurality of context rules may be also stored in the associated memory device.  For the selected alert \"suggestion to\nchange an HVAC setting,\" the plurality of context rules may include; pro vide the suggestion when the identified contextual situation occurs took place at a predefined location (e.g., home, work), provide the suggestion when the identified contextual\nsituation occurs at a predefined period of time (e.g., between 6 a.m.  and 9 a.m., or after 7 p.m.), provide the suggestion when there are no other individuals indoors, provide the suggestion when there is an indication that HVAC is working, and more. \nIn other words, the context rules may be used to define the combination of conditions in which it would be appropriate to provide a certain alert for the identified contextual situation.\nThereafter, the processing device may retrieve information related to the plurality of context rules (block 3012).  In one embodiment, the retrieval of information related to the plurality of context rules may be based on prior captured images. \nFor example, to determine that there are no other individuals indoors, the processing device may review captured images from the last 15 minutes.  In another embodiment, the retrieval of information related to the plurality of context rules may be based\non additional sensors and/or publicly available data sources.  For example, to determine that the HVAC is working, processing device may determine the indoors temperature using a sensor in the paired device, and the outdoors temperature using an online\ndatabase.  Then, the processing device may make a determination whether the selected alert is relevant based on the plurality of context rules and the retrieved information (block 3014).  In some embodiments, the processing device may facilitate machine\nlearning algorithms to determine the likelihood that providing the alert will caused user 100 to perform an action.  Alerts that are determined to have a likelihood of relevant exceeding, for example, a threshold may be classified as relevant alerts to a\nparticular user.  For example, when user 100 leaves a work location at 7 PM, a relevant alert might be to turn off the HVAC while a non-relevant alert might be to feed the user's pet.\nIf the relevant alerts have been identified (decision block 3016), the processing device may aggregate information about one or more alerts determined to be relevant (block 3018) and transmit the one or wore relevant alerts to a paired device\n(e.g., computing device 120) (block 3018).  If all of the relevant alerts have not been identified, the processing device may select another alert from the list of alerts and repeat blocks 3010 to 3014.  In embodiments proceeding according to path \"B,\"\nfollowing block 3020, the process may proceed to block 120, and the paired device (e.g., computing device 120) may provide the one or more relevant alerts to the user (e.g., by displaying the alert on a display of computing device 120).  In embodiments\nproceeding according to path \"A\", following block 3018, the process may proceed to block 3022, the one or more relevant alerts may be provided to the user via computing device 120.\nFIG. 31 is a flowchart showing an exemplary process 3100 for identifying a contextual situation related to user 100, consistent with disclosed embodiments.  Apparatus 110 or computing device 120 may implement one or more steps of process 3100 to\nidentify contextual situations illustrated in, for example, FIGS. 29A-29D.\nAs illustrated in FIG. 31, at step 3110, a processing device (e.g., processor 210, processor 540) may receive a plurality of images from an environment of user 100.  At step 3120, the processing device may analyze the plurality of images to\nidentify a contextual situation related to user 100.  At step 3130, the processing device may determine information associated with the contextual situation.  After determining the information associated with the contextual situation, at step 3140, the\nprocessing device may cause a paired device (e.g., computing device 120) to provide at least one alert to user based on the determined information associated with the contextual situation.  These steps of process 3100 are discussed in further detail\nbelow.\nSpecifically, at step 3110, the processing device may receive a plurality of images from an environment of user 100.  In some embodiments the plurality of images are captured by image sensor 220.  The field of view of image sensor 220 may\ninclude an area in front of user 100.  For example an optical axis of image sensor 220 may extend generally in a walking direction of user 100.  The field of view of image sensor 220 may be more than 100.degree., for example, more than 120.degree., or\nmore than 150.degree..  The image sensor may be included in a capturing unit (such as capturing unit 710).  In addition, apparatus 110 may include a directional microphone configured to detect or receive a sound or sound wave.  Accordingly, the\nprocessing device may receive a plurality of images from an environment of user 100 associated with an audio data stream.\nA step 3120, the processing device may analyze the plurality of images to identify a contextual situation related to user 100 (e.g., transitioning from indoors to outdoors, transitioning from outdoors to indoors, entering a grocery store,\nexiling a grocery store, entering a vehicle, exiting a vehicle, detection of one or more persons, identification of one or more persons, a document present in an area in front of user 100, etc.).  In some embodiments, analyzing the plurality of images\nmay include determining an existence of elements in the plurality of images that may be indicative of a contextual situation (e.g., existence of a door), and comparing the determined elements with the sample elements in the database (e.g., an image\nsample of the door in the user's home).  After determining the existence of elements, the processing device may be programmed to retrieve additional information from one or more sources that may assist in identifying or improving the certainty level in\nthe identification of the contextual situation.  For example, the processing device may access location information from a Global Positioning System (GPS) to confirm that user 100 is currently exiting his/her own home.  In another example, the processing\ndevice may be programmed to analyze the audio data stream to assist in identifying or improving the certainty level in the identification of the contextual situation.  Consistent with the present disclosure, analyzing part of the pictures captured by\nimage sensor 220, may help reduce the time needed to identify the contextual situation.  For example, repetitive images of the environment of user 100 may not all be necessary for identifying a contextual situation related to user 100.  Two images may be\nconsidered repetitive when the images show the same, similar, or overlapping image content.  Therefore, when analyzing the plurality of images, the processing device may perform methods to discard and/or ignore at least some of the repetitive images.\nAt step 3130, the processing device may determine information associated with the contextual situation.  The determined information may be specific to the identified contextual situation.  In addition, the determined information may be\nassociated with the types of alerts and/or the list of context rules associated with each contextual situation.  For example, when the contextual situation is \"transitioning from indoors to outdoors,\" the determined information may be: \"where and when\nwas the last appearance of the users keys in the captured images.\" In another example, when the contextual situation is \"a document present in an area in front of the user,\" the determined information may be: \"an address appearing on the document.\" In\nsome embodiments, the determined information may be determined using information derived from previously captured images.  In other embodiments, the determined information may be determined using information from multiple independent sources, such as\nadditional sensors in apparatus 110, additional sensors in computing device 120, and/or information stored in a remote database.\nAt step 3140, the processing device may cause a paired device to provide at least one alert to a user based on the determined information associated with the contextual situation.  The paired device (e.g., computing device 120) may be at least\none of: a smartphone, a tablet, a smarthome controller, or a smart watch.  When the processing device is part of apparatus 110 (e.g., processor 210), step 3140 may include causing a transmitter (e.g., wireless transceiver 530) to transmit the determined\ninformation to the paired device to cause the paired device to provide the at least one alert.  In one embodiment, apparatus 110 is configured to select which paired device should provide the at least one alert.  To do so, apparatus 110 may determine the\ndistance of each paired device from apparatus 110 and transmit the determined information to the selected paired device.  For example, when user 110 is at his/her home and not in proximity to his/her smartphone, apparatus 110 may select to transmit the\ndetermined information to a smarthome controller that will provide a voice alert, using associated speakers.\nWhen the processing device is part of computing device 120 (e.g., processor 540), step 3140 may include using feedback outputting unit 545 or display 260 to provide the at least one alert to user 100.  In some embodiments, the at least one alert\nmay be associated with at least one of a determined reading speed of user 100, a last page read of a book, or progress toward a predetermined reading goal.  In addition, the processing device may record one or more reactions of user 100 to the at feast\none alert, and to take into consideration past reactions to a certain type of alert, when determining if and how to provide future alerts to user 100.  For example, the processing device may determine that user 100 ignores visual alerts presented on\ndisplay 260 in some cases (e.g., while user 100 is in movement).  Accordingly, the processing device may determine to use feedback outputting unit 545 when identifying contextual situations in these cases.\nIn another embodiment, after transmission of the determined information in step 3140, apparatus 110 may continue to capture at least one image using image sensor 220.  The processing device may analyze the at least one image to identify a second\ncontextual situation related to user 100.  The second contextual situation may be related to the first identified contextual situation (e.g., a repetitive of the same contextual situation, a contextual situation that involves the same individuals, or a\ncontextual situation that involves associated objects), or not related at all.  The processing device may then determine a time difference between the first contextual situation and the second contextual situation.  Using the identified first contextual\nsituation, the identified second contextual situation, and the determined time difference, the processing device may withhold transmission associated with the second contextual situation to computing device 120.  The processing device may withhold the\ntransmission for an unlimited period of time, a predetermined period of time, or until the next time the apparatus 110 is being charged.  In one of the examples mentioned above, the first contextual situation may include an identification of a worker at\na work site not wearing safety equipment.  In this example, the second contextual situation may include another identification of the same worker at the same work site still not wearing safety equipment.  The processing device, may determine to withhold\nthe transmission of information associated with the second contextual situation to computing device 120 if the determined time difference between the two times that apparatus 110 identified that the worker is not wearing safety equipment is under or\nabove a predefined threshold.  For example, the predefined threshold may be twelve hours, six hours, two hours, thirty minutes, five minutes, one minute, thirty seconds, etc.\nCollaboration Facilitator for Wearable Devices\nIn some embodiments, a wearable apparatus 110 may capture image data (e.g., one, or more images) of an environment of the wearer of the wearable apparatus 110.  The image data may be analyzed to detect a visual trigger in the environment.  Hie\nvisual trigger may indicate that a collaborative action is to be taken.  An indication based on the visual trigger may then be transmitted to other wearable apparatuses 110, computing devices 120 that are paired with wearable apparatuses 110, and/or to\nserver 250.  Receipt of the indication of the visual trigger may cause an action by the receiving device, such as distributing information to other devices.\nFIG. 32A illustrates an example environment 3200 including a user 3205 wearing a wearable apparatus 110, computing devices 3210 and 3220, and meeting attendees 3230.  Environment 3200 may depict a meeting in which user 3205 is standing at a\nblackboard, video screen, or other presentation device 3207, and giving a presentation to a group of meeting attendees 3230.  User 3205 may be wearing wearable apparatus 110, which may have a field of view (FOV) such that the wearable apparatus 110 may\ncapture images of at least part of presentation-device 3207.  Wearable apparatus 110 may also be used in environment 3200 to detect a visual trigger in the environment.  In some examples, a system independent of wearable apparatus 110 may be used to\ncapture images and detect the visual trigger, or to detect the visual trigger in images captured by wearable apparatus 110.  For example, a smart phone, computer with an attached camera, a digital camera, or other image capturing device may be used.  By\nway of example, the following embodiments will describe the system with respect to wearable apparatus 110.\nIn some embodiments, wearable apparatus 110 (or other system) may include a transceiver and at least one processing device, as described above for FIG. 5.  The processing device may be coupled to memory storing instructions, which when executed\nmay cause the device to perform operations.  In some embodiments the processing device may be programmed to cause the wearable apparatus 110 to capture images of the environment of user 3205.  The wearable apparatus 110 may include an image sensor, as\ndescribed above, to capture images.  The processor may obtain the captured images from the image sensor.  In some examples, the captured images may be of the presentation device 3207.  For example, the FOV of wearable apparatus 110 may be such that\nwearable apparatus 110 generally \"sees\" what user 3207 sees.  In other examples, the captured images may be of a wide angle view capturing an image with an FOV of, for example, 270 degrees.  The wide angle FOV may allow an image to be captured that\nincludes meeting attendees or other persons or objects that may or may not be visible to user 3205.\nIn some embodiments, environment 3200 may include instances where a collaborative action may be taken.  For example, a collaborative action may be an action common to or shared amongst the wearer of wearable apparatus 110 and at least one other\nindividual.  In some examples, the collaborative action may be a group work project, updating a project status, treating a patient, updating a patient record, updating a task list, and/or assembling an item.  For example, environment 3200 may include a\nmeeting with attendees involved in a group work project, such as designing an automobile.  Meeting attendees may work together on specific facets of the project and share information with one another based on visual triggers detected by wearable\napparatus 110 when analyzing captured images of the environment, in other examples, the collaborative action may be commercial in nature, such as selling a product, marketing a product, or advertising a product.  The collaborative action may also be\nrecreational, such as playing a game or driving a car.  In some instances, the collaborative action may be preparing a meal, cleaning a residence, buying groceries, or assembling furniture.\nAs described above, wearable apparatus 110 may capture images in environment 3200.  The processing device may analyze the images to detect a visual trigger in the images.  As discussed, the visual trigger may be associated with a collaborative\naction to be taken.  Wearable apparatus 110 may transmit an indicator of the detected visual trigger using the transceiver to other devices.  For example, the indicator may be transmitted to paired device 3210, which may be paired to wearable apparatus\n110.  In some examples, paired device 3210 may be any device capable of receiving information through a paired connection to wearable apparatus 110, such as a smartphone, a tablet, and/or a smart watch.  Paired device 3210 may be worn by user 3205, such\nas on a belt, or integrated into a belt.  Alternatively, paired device 3210 may be held by user 3205 or be located in the vicinity of user 3205 (e.g., be located in the same room).  In other examples, paired device 3210 may be located in a remote\nlocation (e.g., in another room, building, city, etc.).\nIn some embodiments, the paired device 3210 may process the received indicator of the visual trigger and determine an action to be taken.  In some examples, the indicator and/or visual trigger may be a command to perform a pail of the\ncollaborative action.  The action may be a function associated with the collaborative action.  The function may include actions that the device normally performs, such as display of data on the device, or the function may include distributing information\nto other devices, such as remote devices 3220.  Remote devices 3220 may also be a smartphone, a tablet, a smart watch, and/or a server.  For example, paired device 3210 may receive an indicator that a visual trigger has occurred in environment 3200. \nReceipt of the indicator may cause paired device 3210 to distribute information to remote devices 3220.  The information may be related to the visual trigger and/or the collaborative action (e.g., an image of the presentation device, an update to a task\nlist, an indication that the meeting has finished, etc.).  The information may include images, documents, spreadsheets, or other information useful in the collaborative action.\nIn some examples, the information may be distributed to multiple devices, as depicted in FIG. 32A.  Remote devices 3220 may be used by attendees 3230.  For example, each attendee 3230 may hold a remote device 3220 and view the information on the\ndevice.  The information received by remote devices may supplement and/or augment the presentation by user 3205.\nIn some embodiments, the information may identify a step in a task and/or the amount, of time spent on a step of a task.  For example, paired device 3210 may receive the indicator of the visual trigger, which may include a swiping hand gesture\nindicating to display the next picture or page of information regarding the task.  Paired device 3210 may then distribute information to the remote devices 3220 identifying the current step, such as designing the interior of a car, and the next task,\nsuch as determining the color of the interior.  The information may also include how long the group spent on the current task or step.  In some examples, the amount of time may be factored into a budget.  For example, the budget may indicate how long\ncertain tasks require and may be used to properly budget, future tasks.  In some examples, the result of the current task may be distributed before progressing to the indicated next task.  In this case, attendees may be able to view a task list on remote\ndevices 3220.  The task list may show the current task, with amount of time spent on the task, and the next task to be completed.  The task list may also display all tasks in the collaborative action, whether or not completed.\nFIG. 32B illustrates an exemplary hand gesture 3208 as a visual trigger associated with a collaborative action.  For example, user 3205 may be wearing wearable apparatus 110 and presenting a design of a new car to an audience.  At some point\nduring the presentation, user 3205 may swipe his arm upward, for example, as in the gesture 3208 to indicate an action.  The action may indicate an end to a task, such as moving to the next, slide, signaling that a consensus has been reached, a decision\nhas been made, a task has been completed, an item has been bought, or the like.  In other examples, the gesture 3208 may indicate movement of the image, such as moving it from one side of the presentation to the other.  In still another example, the\ngesture 3208 may indicate that the displayed image be transmitted to attendees and/or information in a document or spreadsheet (e.g., describing the image) be transmitted to attendees.\nFIG. 32C illustrates and example environment 3201 in which information is distributed to other wearable apparatuses 110 in response to a hand gesture 3208.  In the example, a user 3205 may be presenting to local meeting participants (not shown),\nwhile also presenting to remote individuals 3240, who may be at a remote location.  Additionally or alternatively, a user 3205 may be presenting to local meeting participants, and remote individuals 3240 may be involve in a different task associated with\na collaborative action common to user 3205 and remote individuals 3240.  Additionally or alternatively, a user 3205 may be presenting to local meeting participants, where the meeting and/or the presentation may be associated with a collaborative action\ncommon to user 3205 and individuals 3240, while remote individuals 3240 may be involved in a task that is unrelated to the collaborative action.  For example, paired device 3210 may distribute (e.g., transmit) the information to other wearable\napparatuses 110, which are worn by attendees (or other persons not in the environment).  In some examples, the other wearable apparatuses 110 may be paired with other devices and transmit information based on the received information to those paired\ndevices.  By way of example, environment 3201 depicts a user 3205 presenting a car design to meeting attendees (as shown in FIG. 32A).  User 3205 may be wearing a wearable apparatus 110, which is paired to another device 3210, such a smart phone.  User\n3205 may point to the presentation and make a gesture 3208, which may cause the displayed image to move.  Wearable apparatus 110 may capture an image of the presentation and the gesture, analyze the captured image, and determine that a visual trigger has\noccurred (e.g., the gesture 3208).  Wearable apparatus 110 may transmit an indicator of the visual trigger to the paired device 3210.  Wearable apparatus 110 may also transmit the image or other information to the paired device 3210.  The paired device\n3210 may receive the indicator and/or other information, which may cause the paired device 3210 to distribute information (e.g., a received image, an indicator of the slide in a presentation, steps in a task, etc.) to other wearable apparatuses 110.  The\nother wearable apparatuses may be worn by attendees 3230 (not shown) or by remote individuals 3240.  The other wearable, apparatuses 110 may also be paired to devices, such as display device 3209.  In some examples, one or more of the other wearable\napparatuses 110 may transmit the received information to the display device 3209 for display to the remote individuals 3240.  In some embodiments, other wearable apparatuses 110 worn by remote individuals 3240 may also capture images and detect visual\ntriggers at the remote location.  For example, one of the remote individuals 3240 may gesture to the display device 3209, which gesture may be detected as a visual trigger by one of the other wearable apparatuses 110.  In response to detecting the visual\ntrigger, feedback may be sent to either paired device 3210 or wearable device 110 being worn by user 3205.\nFIG. 33A illustrates an exemplary visual trigger associated with an environment 3300.  In the embodiment, a physician 3320 may enter an examination room where a patient 3315 is waiting.  Physician 3320 may be wearing wearable apparatus 110.  In\nsome examples, wearable apparatus 110 may capture images depicting a transition of the physician 3320 from the lobby to the examination room (e.g., changing the environment of the physician 3320).  In some examples, wearable apparatus 110 may capture\nimages depicting a meeting with patient 3315.  The transition and/or meeting may be detected as a visual trigger.  An indicator of the visual trigger may, for example, be transmitted to a paired device, a remote device, a server, and/or other devices, to\nindicate, for example, that the physician 3320 is meeting with the patient 3315.  The transmitted indicator may cause the paired device, for example, to update a task list associated with the patient 3315.  In another example, the wearable apparatus 110\nmay capture an image of the patient 3315 as the physician 3320 enters the room.  Wearable apparatus 110 may analyze the captured image and determine that the patient 3315 is present.  The presence of the patient 3315 may cause wearable apparatus 110 to\ndetect a visual trigger that the physician 3320 will examine and/or speak with the patient 3315.  In some examples, an indication of the visual trigger may be transmitted to another device, where it may cause the device to perform a function related to\nthe collaboration (e.g., update a task list, provide treatment options for the patient, update a patient record, etc.).  For example, the indication of the visual trigger may be transmitted to a server 250, which includes a database of records for\npatient 3315.  Receipt of the indication may cause the server 250 to update the records to indicate that the patient 3315 has arrived for her appointment.  In another example, receipt of the indication may cause server 250 (or other device) to distribute\nto a paired device used by physician 3320 information related to patient 3315, such as a list of ailments that patient 3315 experiences, a treatment plan for patient 3315, test results for patient 3315, and so forth.\nFIG. 33B illustrates another exemplary visual trigger associated with an environment 3301.  In the environment, user 3205 may be presenting at a meeting with remote individuals 3240 at a remote location and a new meeting participant, person\n3330, may join the meeting.  Alternatively, person 3330 may leave the meeting.  In response to joining the meeting, person 3330 may receive information about the presentation, such as tasks associated with the meeting, the duration of the meeting,\ninstructions pertaining to participation in the meeting, etc. In some embodiments, user 3205 may be wearing wearable apparatus 110, which may include an image sensor with a field of view that includes a door to the room.  In some embodiments, wearable\napparatus 110 may capture an image of a person 3330 entering (or exiting) the room where the presentation is taking place.  The image may be analyzed to detect a visual trigger that the person 3330 has entered (or exited) the room and an indicator of the\nvisual trigger may be transmitted to a remote device.  For example, the indicator may be transmitted to a device being used by person 3330.  The indicator may cause the presentation to be displayed on the device find/or an indication of the current task\nor step in a task may be displayed.  In another example, the indicator may be transmitted to remote attendees 3240.\nFIG. 33C illustrates an exemplary environment 3302 including a user 3205 wearing a wearable apparatus 110, an audience 3340, a network 240, and a server 250.  In some embodiments, each audience member may also have a wearable apparatus 110. \nEach wearable apparatus 110 may acquire image-based information associated with images captured by the camera on the particular wearable apparatus.  Image-based information may include raw images captured from the camera and formatted as jpeg, pic, tiff,\nmpeg, or any other suitable image format.  Additionally or alternatively, image-based information may include images pre-processed by a processing device on the wearable apparatus 110.  The pre-processed images may be categorized, enhanced, compressed,\nor otherwise altered.  Additionally or alternatively, image-based information may include logos, words and/or facial information extracted from the images.  Additionally or alternatively, image-based information may include information related to\nproducts, people, etc. that the users interact with, such as identifying information, names, and so forth.  The image-based information may be of the situation or environment 3302 of the user 3205 wearing the wearable apparatus 110.  In some embodiments,\neach wearable apparatus 110 may transmit image-based information to server 250.  In some examples, wearable apparatuses 110 may first transmit the information to a network 240 and then to server 250.  In other examples, the wearable apparatuses 110 may\ntransmit the information directly to server 250.\nIn some embodiments, the image-based information is streamed to server 250 in a data stream.  As discussed above in connection with FIG. 27B, the data stream may consist of packet or frames that are transmitted by any suitable transmission\nmeans.  The packets may include image-based information, position information, timing information, motion information, and so forth.  The data stream (e.g., as shown in FIG. 27B) may occur in real-time (e.g., shortly alter the image data is acquired, for\nexample within one second), or the stream may be delayed by a predetermined amount of time.  Thus, server 250 may receive one or more data streams of image-based information from each wearable apparatus 110.  In some examples, server 250 may also receive\none or more data streams of image-based information from one or more wearable apparatuses 110 used by individuals not present in environment 3302.\nIn some embodiments, server 250 may include a data interface (not shown) that allows server 250 to receive one or more data streams from wearable apparatus(es) 110.  The data interface may include hardware and/or software to interface with\nnetwork 240 or directly to wearable apparatuses 110.  For example, the data interface may include transmitting and receiving circuitry for a wired or wireless connection to network 240.  Server 250 may also include a processing device operatively coupled\nto memory for storing instruction for the processing device.\nIn some embodiments, devices other than or in addition to server 250 may receive the transmitted data streams from wearable apparatus 110.  For example, computing device 120 may receive the data streams.  Alternatively, any one or all of the\nwearable apparatuses 110 may receive the data streams from the other wearable apparatuses 110.\nIn some examples, server 250 may analyze the image-based information in each of the received data streams.  For example, server 250 may receive a data stream from a particular wearable apparatus 110 using the data interface.  Server 250 may,\nusing the processing device, unpack or extract the image-based information in the data stream.  In some examples, the image-based information may be a series of images captured by the camera on the particular wearable apparatus 110.  In other examples,\nthe image-based information may include camera settings, such as f-stop, focal length, light and color content of the image, etc. In other examples, the image-based information may include an indication of at least one visual trigger, for example, an\nindication of a visual trigger associated with a collaborative action.  In other examples, the image-based information may include information related to persons, objects, events, actions, etc, captured by wearable apparatus 110.\nServer 250 may analyze the image-based information to determine at least one visual trigger associated with a collaborative action.  In some examples, server 250 may use image recognition algorithms and/or machine vision algorithms to determine\nobjects and/or persons in the received images.  For example, optical character recognition (OCR) may be used to determine words in an image, such as on a paper, sign, book, etc. Detected words may be used to recognize consumer products, brand names,\nand/or categories of products.  In some examples, edge and shape recognition may be used to determine objects in images, such as a ball, a tree, lines on a playing field, etc. Facial recognition may be used to determine features on a human face, where\nthe features may be compared to a database of features to determine the identity of a person.  In some examples, context analysis may be used to determine situations involving the recognized words, object, people, etc. For example, an image may be\ncaptured and analyzed to determine the presence of a ball, grass, lines on the grass, and a soccer player.  Contextual analysis may then determine that the user is attending a soccer game.  Other non-exhaustive examples of context analysis include:\nwater+boat=boating; grass+ball=sport (the type of ball may also be recognized to determine if the sport is, for example, soccer, baseball, football, etc.); recognize lines of a playing field; aisles+products=grocery store; read text on object to\nrecognize brands of products (e.g., Colgate, etc.); recognize general descriptive words (e.g., potatoes, milk, etc.), for example, at a produce market.\nFurthermore, in some embodiments, for example, analyzing images may involve edge identification, in which an image is analyzed to detect pixels at which discontinuities (e.g., sudden changes in image brightness) occur and edges (e.g., edges of\nthe external object) are identified to coincide with the detected pixels.  Alternatively or additionally, in some embodiments analyzing images may involve identifying in and/or extracting from an image pixels representative of objects in the environment,\nsuch as the external object.  Pixels may be determined to be representative of an external object based on, for example, other images of the external device or similar external devices maintained, e.g., in a database and/or predetermined data describing\nthe external object maintained, e.g., in a database.  Alternatively or additionally, pixels may be determined to be representative of an external object based on, for example, a trained neural network configured to detect predetermined external objects. \nOther types of analysis are possible as well, including, but not limited to, gradient matching, grey scale matching, scale-invariant feature transform (SIFT) matching, and/or interpretation trees.  In the case where multiple images are received over a\nperiod of time, server 250 may compare sequential images to determine actors and actions taking place in the images.\nIn some examples, the collaborative action includes distributing (e.g., transmitting) information to other devices.  The server may transmit the information through network 240 to, for example, wearable apparatuses 110, or to other de vices,\nsuch as remote devices 3220 or computing device 120 (e.g., being used by audience 3340).  The information may be as described above and distributed to other devices based on the visual trigger.\nBy way of example, wearable apparatus 110 being worn by user 3205 may stream image-based information to server 250 for analysis.  Server 250 may detect one or more visual triggers in the image-based data.  For example, the visual trigger may be\na hand gesture such as pointing to a particular aspect of the presentation.  Server 250 may interpret the gesture as a command to distribute a questionnaire, a task guide, or other interactive information to audience members 3340.  Server 250 may\ntransmit the information through network 240 to wearable apparatuses being worn by audience members 3340 or alternatively to computing devices being used by audience members 3340.  Audience members 3340 may then provide feedback to user 3205, such as\nreal-time results from, a survey.\nIn some embodiments, server 250 may receive two or more data streams from different wearable devices.  Server 250 may analyze the two or more data streams and determine multiple visual triggers.  In some embodiments, server 250 may determine at\nleast one visual trigger based on the two or more data streams being analyzed in parallel.  For example, images from difference data streams may contain information that when analyzed together cause the server to detect a visual trigger.  For example,\nserver 250 may determine that two or more users of wearable apparatuses 110 are working on the same step of a collaborative action and performing redundant actions, such as buying the same item, editing the same document, processing the same information,\nand so forth.  In another example, server 250 may determine that a need of a first user of a first wearable apparatus 110 may be fulfilled by a second user of a second wearable apparatus 110.  For example, that the first user is lacking an item that the\nsecond user has excess of, that the location of the second user enables the second user to assist the first user, and so forth.\nIn some embodiments, server 250 may store the information relating to the determined visual trigger in a storage resource 3350.  The storage resource 3350 may be local to server 250 or at a remote location.  In some examples, the storage\nresource 3350 may be internal memory of server 250 and/or computing device 120.  Storage resource 3350 may also be memory within any one of wearable apparatuses 110.  In some examples, storage resource 3350 may be a nonvolatile storage medium, such as a\nhard disc or a solid state disc.  Server 250 may store the information relating to the determined visual trigger in a database on storage resource 3350.  The database may be of any known kind, such as a relational database or a self-referencing database. The database may include demographic information about, the user or users associated with the visual triggers.  For example, a database entry containing information relating to the determined visual trigger may also contain biographical information\n(e.g., demographics) about the user or users associated with the trigger.\nIn some embodiments, the stored database information and/or information based on the stored database information may be output to a user, wearable apparatus 110, remote device, or other computing device 120.  For example, server 250 may include\na monitor 3360 and display a graphical user interface (GUI) containing the information relating to the visual trigger or associated collaborative action.  In other examples, the stored information may be displayed on computing device 120.  In still other\nexamples, the stored information may be presented to the user through a display on a wearable apparatus 110 or paired device 3210.  In some embodiments, the stored information may be communicated to a user audibly.  The displayed or otherwise\ncommunicated information may include the determined visual trigger, collaborative action, and/or demographics information about the users of the wearable apparatuses to which the visual trigger or collaborative action belongs.\nFIG. 33D illustrates another exemplary visual trigger associated with an environment 3303.  In the environment, a user 3370 may be wearing wearable apparatus 110 and working at a computer 3380.  One of ordinary skill in the art will recognize\nthat computer 3380 may include any computing device, such as a laptop, desktop computer and associated monitor, or a handheld device (e.g., a smartphone, tablet, smart watch, etc.)\nIn the example shown in FIG. 3D, user 3370 may be entering information into a graphical user interface (GUI) 3390, such as a spreadsheet or word processing program.  Wearable apparatus 110 may detect that the user 3370 is entering the\ninformation and determine that entering the information is a visual trigger.  In some examples, wearable apparatus 110 may capture images of the GUI 3390 and determine which program is displayed on the screen.  For example, wearable apparatus 110 may\ndetect a logo present on the GUI 3390, the arrangement items on the screen, or other indicators of the type of program.  Wearable apparatus 110 may also analyze the captured images for changes due to inputting of information by user 3370.  Identification\nof the GUI 3390 and the inputting of information may cause detection of a visual trigger.  An indication of the visual trigger may be sent to a paired device, or alternatively, to computer 3380.  Receipt of the indication may cause, for example, computer\n3380 to retrieve data from a database and input the information into the GUI 3390 (e.g., in the case that the user 3340 is entering tax return information, etc.).  In other examples, the user 3370 may be completing a checklist by indicating that tasks\nassociated with a collaborative action have been completed.  Completion of the checklist may be detected as a visual trigger and an indication may be sent to a paired device, or other computing device.  Receipt of the indication may cause the paired\ndevice to distribute information to other individuals involved with the collaborative action to inform them of its completion.\nFIG. 34 illustrates an exemplary embodiment of memory 3440 containing software modules to determine a visual trigger.  For example, one or more of server 250, computing device 120, or wearable apparatus 110 may execute instructions from the\nmodules to perform one or more of the functions as described with respect to FIGS. 32-33, above.  Included in the memory are receiving module 3441, analysis module 3442, and transmitting module 3443.  Modules 3441, 3442, and 3443 may contain software\ninstructions for execution by at least one processing device included with a server-based system, such as is included in server 250, a wearable apparatus 110, or other computing device.\nReceiving module 3441 may be configured to receive one or more data streams or image-based data transmitted by wearable apparatuses.  Receiving module 3441 may interact with the data interface to receive the one or more data streams.  Receiving\nmodule 3441 may control the data interface to receive multiple data streams simultaneously from one or more transmission sources.  For example, data streams may be received through a wired connection or through a wireless connection or through both.\nAnalysis module 3442 may be configured to analyze the one or more received data streams or image-based data to determine one or more visual triggers associated with collaborative action to be taken with respect to a user in an environment. \nAnalysis module 3442 may extract information from the data streams to aid in determination of the visual trigger.  For example, analysis module 3442 may extract image-based information, position information, timing information, and/or motion information\nfrom the data streams.  Analysis module 3442 may use facial detection and recognition to determine triggers, such as interaction with other persons.  Analysis module 3442 may also use machine vision algorithms to determine triggers such as identifying\ncommercial products, landscape, objects, locations, gestures, assembly of items, entry of data, etc. Furthermore, analysis module 3442 may use position information to determine a location of the trigger, timing information to determine frequency,\nscheduling, and/or duration of the trigger, and motion information to determine specific interaction states, such as movement towards an object.\nTransmitting module 3443 may be configured to distribute information related to the collaborative action to be taken.  Transmitting module 3443 may interact with the data interface to transmit the information to network 240, wearable apparatus\n110, paired device 3210, and/or computing device 120.  Transmitting module 3443 may control the data interface to transmit information to multiple devices simultaneously.\nFIG. 35 is a flowchart illustrating an exemplary method 3500 of detecting a visual trigger and transmitting an indicator relating to the visual trigger consistent with disclosed embodiments.\nAt step 3505, one or more images may be obtained.  The images may be captured by wearable apparatus 110.  In some embodiments, the images may be obtained by a processing device from an image sensor.  In other embodiments, the images may be\nobtained from data streams transmitted by wearable apparatus 110 to a remote computing device 120 and/or server 250.  The one or more data streams may contain image-based information, position information, timing information, and/or motion information,\nand be sent in packets or frames.  The data streams may be continuously received over time or received at predetermined intervals.\nIn some embodiments, the images may be of the environment of a user of wearable apparatus 110.  For example, the images may capture actions of other persons, hand gestures of the user, and/or objects and features of the user's environment.  As\ndescribed above, images may be captured during a meeting presentation, a doctor visit, during a spoiling event, while assembling an object, etc.\nAt step 3510, the one or images may be analyzed to detect at least one visual trigger.  In some examples, the visual trigger is determined from two or more users from which the images are received.  In some embodiments, server 250 may analyze\nthe images.  In other embodiments, computing device 120 or wearable apparatus 110 may analyze the images.  For example, the images may be analyzed by executing instructions stored in analysis module 34-42.  In some embodiments, the images may be analyzed\nto determine a visual trigger associated with a collaborative action to be taken (as described above).  In some embodiments, position information, timing information, and/or motion information may be analyzed in addition to image-based information in\norder to determine a visual trigger.  For example, image-based information may be analyzed using machine vision, image recognition, OCR, etc. algorithms, as described above, to determine features and contexts within images or sets of images, such as a\nhand gesture or recognize another person.  In some examples, timing and motion information may be used to determine a hand gesture.  Thus, a visual trigger may be determined relating to the motion of a user's hand.  In other examples, objects, logos,\ndata types being input into a GUI, interactions with people or object, etc. may be determined.\nAt step 3515, an indicator related to the determined visual trigger may be transmitted.  The indicator may represent the visual trigger and cause functions or actions to be performed by the receiving device.  For example, the visual trigger\n(through the indicator) may be recognized as a command to perform a collaborative action or distribute information.  As described above, a device receiving the indicator of the visual trigger may distribute task lists, timing information related to task\nduration, interactive surveys, etc., to other devices used by other individuals.\nPrivacy Mode for a Wearable Device\nIn some embodiments, a wearable apparatus may enter a privacy mode in certain situations and send substitute images (e.g., a cartoon version of an image) or censored images (e.g., blurred) when a private situation is detected.  In some\nembodiments, the wearable apparatus may store in memory indicators of a plurality of private contextual situations and those indicators may be transmitted to a paired device so that images from that time period are censored, not shared, or both.  In\ncases where the wearable apparatus may be configured to remain on throughout the day, this feature may protect user privacy.\nFIG. 36A is a block diagram illustrating components of wearable apparatus 110 according to an example embodiment.  FIG. 36A is similar to FIG. 5A and includes all the features of FIG. 5A, with the only difference being the addition of an audio\nsensor 3610 depicted in wearable, apparatus 110.  Adding audio sensor 3610 in FIG. 36A is only included for ease of description and reference.  It does not imply that the system shown in FIG. 5A does not include such an audio sensor or that audio sensor\n3610 is a necessary component in the system shown in FIG. 5A.  As described above in connection with FIG. 5A, other sensors including a microphone (e.g., an example of audio sensor 3610) may be included in wearable apparatus 110.  On the other hand, some\nembodiments may omit audio sensor 3610.\nFIG. 36B is a block diagram illustrating components of wearable apparatus 110 according to an example embodiment.  FIG. 36B is similar to FIG. 5B and includes all the features of FIG. 5B, with the only difference being the addition of audio\nsensor 3610 in the figure.  Again, adding audio sensor 3610 in FIG. 36B is only included for ease of description and reference.  It does not imply that, the system shown in FIG. 5B does not include such an audio sensor or that audio sensor 3610 is a\nnecessary component in the system shown in FIG. 36B.\nFIG. 36C is a block diagram illustrating components of wearable apparatus 110 according to an example embodiment.  FIG. 36C is similar to FIG. 5C and includes all the features of FIG. 5C, with the only difference being the addition of audio\nsensor 3610 in the figure.  Again, adding audio sensor 3610 in FIG. 36C is only included for ease of description and reference.  It does not imply that the system shown in FIG. 5C does not include such an audio sensor or that audio sensor 3610 is a\nnecessary component in the system shown in FIG. 36C.\nFor any of the embodiments depicted in FIGS. 36A-36C, system 200 may be configured to automatically change settings and configuration of wearable apparatus 110 based on analysis of information collected by wearable apparatus HO.  As described\nabove, wearable apparatus 110 may be worn by user 100 in various ways.  Wearable apparatus 110 may collect data in the environment of user 100, such as capturing images, recording sound, etc. The collected data, which may or may not be preprocessed by\nwearable apparatus 110, may be transmitted to computing device 120, which may be paired with wearable apparatus 110 through a wired or wireless communication link.  Computing device 120 may analyze the received data, alone or in combination with server\n250 through network 240, may store predefined privacy mode triggers and associated privacy mode settings in memory 550/550a/550b, may determine information associated with the analyzed data indicating the presence of at least one of the predefined\nprivacy mode triggers, and/or may automatically cause one or more adjustments to the wearable imaging apparatus based on the privacy mode settings associated with the at least one recognized predefined privacy mode trigger.\nReferring to FIG. 36C, wearable apparatus 110 may establish wireless communication (also referred to as wireless pairing) with computing device 120 (also referred to as a paired device or an external device).  As described above, computing\ndevice 120 may include one or more smartphones, one or more tablets, one or more smartwatches, one or more personal computers, one or more wearable devices, a combination thereof, and so forth, in some embodiments, wearable apparatus 110 and computing\ndevice 120 may be paired through a short range communication link such as Bluetooth, WiFi, near-field communication (NFC), etc. In some embodiments, wearable apparatus 110 may be connected to network 240 and communicate with computing device 120 and/or\nserver 250 through network 240.  For example, wireless pairing may be established via communication between wireless transceiver 530a in wearable apparatus 110 and wireless transceiver 530b in computing device 120.  Wireless transceiver 530a may act as a\ntransmitter to send image and/or sound data captured by image sensor 220 and/or audio sensor 3610 to wireless transceiver 530b, which may act as a receiver, for processing and analysis by computing device 120, alone or in combination with server 250.  In\nsome embodiments, wireless transceiver 530a may transmit information associated with the captured image/sound data to computing device 120.  In some embodiments, after a content item is selected by computing device 120, the selected content item may be\ntransmitted by wireless transceiver 530b, which may act as a transmitter, to wireless transceiver 530a, which may act as a receiver.  The content item may then be output to user 100 through feedback outputting unit 230.\nIn some embodiments, wearable apparatus 110 may be configured to collect data (e.g., image and/or audio data) and transmit the collected data to computing device 120 and/or server 250 without preprocessing the data.  For example, processor 210\nmay control image sensor 220 to capture a plurality of images and/or control audio sensor 3610 to record sound.  Then, processor 210 may control wireless transceiver 530a to transmit the captured images/sound data, and/or information associated with the\nimages/sound, to computing device 120 and/or server 250 for analysis without performing preprocessing or analysis using the computational power of processor 210.  In some embodiments, processor 210 may perform limited preprocessing on the collected data,\nsuch as identifying a predefined privacy mode trigger in the images or in the sound data, performing optical character recognition (OCR), compressing the image/sound data, sampling the image/sound data, identifying user behavior related images/sound,\netc. The preprocessed data may then be transmitted to computing device 120 and/or server 250 for further analysis.  In some embodiments, processor 210 may perform the entirety of the privacy mode trigger detection and privacy mode settings adjustment\nprocesses (as will be discussed in further detail below).\nIn some embodiments, computing device 120 may be configured to perform some or all of the privacy mode trigger detection and privacy mode settings adjustment processes.  For example, wearable apparatus 110 may transmit image/sound data, either\nunprocessed or preprocessed, to computing device 120.  After receiving the data, processor 540 may analyze the data to identify, for example, one or more images or sounds from the environment of user 100.  Processor 540 may then determine, based on the\nanalysis, information associated with the one or more images or sounds.  The information may include a scene, a person, an object, a trigger, etc. that is included in the image(s) or sound(s).  The information may also include the time and/or location of\ncapturing the image(s) or sound(s).  The information may also include historical data relating to the scene, person, object, trigger, etc. depicted in the image(s) or sound(s).  Other suitable information relating to the image(s) or sound(s) may also be\ndetermined.\nAs described above, one or more privacy mode trigger detection and privacy mode settings adjustment processes may also be performed by server 250 and/or wearable apparatus 110.  Tasks such as image/sound data analysis, information determination,\nand content selection, may be divided among wearable apparatus 110, computing device 120, and server 250 in any suitable manner.  In some embodiments, two or more devices (110, 120, and/or 250) may also collaboratively perform any one task or process. \nFor example, wearable apparatus 110 may preprocess the captured image/sound data, select a plurality of images/sounds from the environment of user 100, and transmit the plurality of images/sounds to computing device 120.  Computing device 120 may analyze\nthe plurality of images/sounds to identify one or more images/sounds relevant to one or more privacy mode trigger detection and privacy mode settings adjustment processes, and transmit the identified one or more images to server 250.  Server 250 may\nperform one or more tasks related to one or more privacy mode trigger detection and privacy mode settings adjustment processes, and transmit information and/or feedback to computing device 120 and/or wearable apparatus 110.\nThere are various ways of distributing and dividing tasks or subtasks among wearable apparatus 110, computing device 120, and server 250.  Regardless of which task or subtask is performed by which device, any suitable allocation of computation\nresources among the devices for performing the above described tasks and/or processes are within the purview of the present application.\nFIG. 37 illustrates exemplars' software modules contained in one or more memory units, such as memory 550, memory 550a, and/or memory 550b.  As shown in FIG. 37, the exemplary software modules include an image analysis module 3710, an audio\nanalysis module 3720, an image alteration module 3730, an action execution module 3740, a database access module 3750, and one or more databases 3760.  As described above, computational tasks involved in system 200 relevant to one or more privacy mode\ntrigger detection and privacy mode settings adjustment processes may be allocated among wearable apparatus 110, computing device 120, and server 250.  Therefore, software modules shown in FIG. 37, which are functionally similar to the computational\ntasks, are not necessarily stored in a single memory unit.  Rather, the software modules can be allocated, similar to the computational tasks, among the various devices having computational power in system 200.  For example, memory 550a may contain\nmodules 3710 and or 3720, while memory 550b may contain modules 3730, 3740, and 3750, as well as database(s) 3760.  In another embodiment, all modules shown in FIG. 37 may be contained in memory 550b.  In yet another embodiment, all modules shown hi FIG.\n37 may be contained in memory 550 (as shown in FIGS. 5A and 5B and/or 36A and 36B).  In some embodiments, multiple memory units, for example memories 550a and 550b, may both contain certain modules, such as modules 3710 and 3720, and the computational\ntasks of image/sound analysis may be dynamically allocated or shifted between wearable apparatus 110 and computing device 120, depending on their respective work load.  Therefore, the memory unit shown in FIG. 37 is collectively referred to as\n550/550a/550b, indicating that the software modules shown in FIG. 37 may or may not be contained in a single memory unit.\nSimilar to modules 601, 602, and 603 shown in FIG. 6, the software modules shown in FIG. 37 may contain software instructions for execution by at least one processing device, e.g., processor 210 and/or processor 540.  Image analysis module 3710,\naudio analysis module 3720, image alteration module 3730, action execution module 3740, and database access module 3750 may cooperate to perform one or more privacy mode trigger detection and privacy mode settings adjustment processes.\nIn some embodiments, image analysis module 3710 may contain software instructions for analyzing one or more images and/or for performing optical character recognition (OCR) of at least one image captured by image sensor 220.  For example,\nreferring to FIG. 36C, processor 210 may execute the image analysis module 3710 stored in memory 550a to perform analysis of one or more images captured by image sensor 220, and transmit a result of the analysis to computing device 120 via wireless\ntransceiver 530a.  Processor 540 of computing device 120 may be programmed to receive the result via wireless transceiver 530b.\nAs discussed above, image analysis module 3710 upon execution by processor 210/540, may enable processor 210/540 to process the captured image data and identity elements of images and/or textual information within the captured image data.  In\ncertain aspects, textual information consistent with the disclosed embodiments may include, but is not limited to, printed text (e.g., text disposed on a page of a newspaper, magazine, book), handwritten text, coded text, text displayed to a user through\na display unit of a corresponding device (e.g., an electronic book, a television a web page, or an screen of a mobile application), text disposed on a flat or curved surface of an object within a field-of-view of apparatus 110 (e.g., a billboard sign, a\nstreet sign, text displayed on product packaging), text projected onto a corresponding screen (e.g., during presentation of a movie at a theater), and any additional or alternate text disposed within images captured by image sensor 220.  Image analysis\nmodule 3710 may provide functionality for apparatus 110 to analyze sets of real-time image data captured by image sensor 220.  Processor 210/540 may execute image analysis module 3710, for example, to determine the presence of a visual trigger in one or\nmore sets of image data, to determine the presence of specific events and/or actions in one or more sets of image data, to determine the presence of objects in one or more sets of image data, to determine the positions of objects in one or more sets of\nimage data over time, and to determine the relative motion of those objects.\nFurthermore, in some embodiments, for example, analyzing one or more images may involve edge identification, in which an image is analyzed to detect pixels at which discontinuities (e.g., sudden changes in image brightness) occur and edges\n(e.g., edges of a device, a body part of the user, and/or an object associated with the user) are identified to coincide with the detected pixels.  Alternatively or additionally, in some embodiments analyzing one or more images may involve identifying in\nand/or extracting from an image pixels representative of one or more objects, actions., events, and/or visual triggers in the environment, such as an object, a body part of the user, and/or an object associated with the user.  Pixels may be determined to\nbe representative of an object, an action, an event, and/or a visual trigger based on, for example, other images of the object maintained, e.g., in a database and/or predetermined data describing the object, action, event, and/or visual trigger\nmaintained, e.g., in a database (e.g., other images of the device, of the body part of the user, and/or of the device associated with the user).  Alternatively or additionally, pixels may be determined to be representative of an object, an action, an\nevent, and/or a visual trigger based on, for example, a trained neural network configured to detect predetermined objects, actions, events, and/or visual triggers (e.g., predetermined devices, body parts of the user, and/or devices associated with the\nuser).  Other types of analysis are possible as well, including, but not limited to, gradient matching, grayscale matching, scale-invariant feature transform (SIFT) matching, and/or interpretation trees.\nIn some embodiments, audio analysis module 3720 may contain software instructions for analyzing sound recorded by audio sensor 3610.  For example, audio sensor 3610 may record sound continuously and store the recorded sound data in memory\n550/550a/550b.  Memory 550/550a/550b may store the sound data in a buffer, which may have a size sufficient for storing a predetermined length of sound, such as 5 seconds, 10 seconds, 30 seconds, 60 seconds, etc. Sound data stored in memory 550/550a/550b\nmay be transmitted to computing device 120, for example after a privacy mode trigger is recognized in at least one of the captured images.  For example, processor 210/540 may receive and analyze the images captured by image sensor 220 to recognize a\nprivacy mode trigger, such as a hand gesture, a person, an object, a location, a scene, etc. After the privacy mode trigger is recognized, processor 210 may transmit the sound data stored in memory 550/550a/550b to computing device 120.  Processor 210\nmay also transmit sound data recorded after the recognition of the trigger to computing device 120, for example, for a designated time period (e.g., 5 seconds, 10 seconds, 30 seconds, 60 seconds, etc.).  After receiving the sound data, processor 540 may\nexecute software instructions of audio analysis module 3720, for example, to extract information from the sound data recorded before and/or after the recognition of the trigger.\nIn some embodiments, other components of system 200 may perform tasks or functions based on either or both analysis results of modules 3710 and 3720.  For example, actions may be executed by action execution module 3740 based on image(s) and/or\nOCR result without sound information.  In another example, action execution may be based on sound information alone, such as identifying a person's name, an object, a place, a date, a time point, or other information from the sound.  In another example,\naction execution may be based on both images and sound.  As described above, a privacy mode trigger can be identified from an OCR result of one or more images captured by image sensor 220.  Based on the trigger, sound data may be analyzed by audio\nanalysis module 3720.\nImage alteration module 3730 may provide functionality for wearable apparatus 110 to alter image data captured by image sensor 220 in response to detection of a privacy mode trigger by wearable apparatus 110.  Image alteration module 3730 may be\nconfigured to alter the image data in response to a command from action execution module 3740, database access module 3750, or both.  In other embodiments, image alteration module 3730 may be configured to alter the image data, independent of modules\n3740-3750.  Alterations performable by image alteration module 3730 include, but are not limited to, placing limitations on image data that may be transmitted to a mobile device (such as computing system 120); altering the image data to enable capturing\nbut not storing of the images; prohibiting transmission of captured image data; lowering the resolution of captured image data; distorting the captured image data; blurring the captured image data; enabling transmission of image data to another device\nafter alterations such as those discussed above have been executed; enabling transmission of caricature or cartoon representations of captured image data; altering the captured image data in a manner prohibiting that image data from being posted on\nsocial media; applying an image filter to image data captured by image sensor 220; and applying a convolution model filter to image data captured by image sensor 220.\nImage alteration module 3730 may be configured to process and alter images using one or more processing schemes based on the privacy mode trigger and the associated privacy mode settings, which will be discussed in further detail below.  In some\nembodiments, processor 210/540 may be programmed to identify multiple portions of a single image and perform processing actions on each portion independently.  For example, based on the detected presence in the captured image data of one or more\npredefined privacy mode triggers, image alteration module 3730 may identify a first and a second portion of at least one image of the plurality of images, and may process the first portion using a first processing scheme, and process the second portion\nusing a second processing scheme.  Each of the first, second, or more processing schemes may incorporate one or more of the alterations discussed above, or other alterations.\nAction execution module 3740 may provide functionality for wearable apparatus 110 to execute various functions in response to stimuli, be they privacy mode triggers detected by apparatus 110, appearance of objects or sounds within the vicinity\nof apparatus 110, or other events occurring while apparatus 110 is in operation.  Action execution module 3740 may, for example, coordinate the configuration and execution of one or more alternative actions that may be available to wearable apparatus 110\nupon positive identification of an object, an event, an action, a trigger, a sound, and/or a particular situation.  These alternative actions may include, but not be limited to, suspending image capture by image sensor 220; suspending storage of images\ncaptured by image sensor 220; limiting information transmitted to a paired device (such as computing system 120); prohibiting all transmission of images captured by image sensor 220; enabling transmission of images that have passed through image\nalteration module 3730; suspending capture of audio information from audio sensor 3610; suspending transmission of audio information captured from audio sensor 3610; prohibiting posting of images captured by image sensor 220 on social media; causing\ntransmission to a paired device (such as computing system 120) of information indicative of the identity of an individual detected in one or more images captured by image sensor 220, without transmitting those images to the paired device, to thereby\nenable the paired device to execute a function relating to the individual without receiving the one or more images; displaying information relating to an individual detected in one- or more images captured by image sensor 220; and blocking at least part\nof an application program interface (\"API\") functionality.\nDatabase access module 3750 may provide functionality for wearable apparatus 110 to compare objects, actions, events, visual triggers, and/or sounds detected in the user environment to objects, actions, events, visual triggers, sounds, and/or\ncategories of same in a database, such as database(s) 3760, to be described in detail below.  In some embodiments, database access module 3750 may derive information from real time image data received from image sensor 220 or audio sensor 3610.  In other\nembodiments, other software elements or processors may derive the information and provide it to database access module 3750.  Processor 210/540 may execute database access module 3750 to access one or more of the described databases, and compare the\ninformation derived from the received real time image/sound data with information in the databases.  If the derived information corresponds to information found in one or more of the databases, database access module 3750 may provide an indication to\naction execution module 3740 to that effect as discussed in further detail below in association with FIG. 39.\nDatabase(s) 3760 may comprise one or more databases that store information and are accessed and/or managed through memory 550/550a/550b.  By way of example, database(s) 3760 may include document management systems, Microsoft.TM.  SQL databases,\nSharePoint.TM.  databases, Oracle.TM.  databases, Sybase.TM.  databases, files, data structures, container data structures, and/or other relational databases or non-relational databases, such as Hadoop sequence files, HBase, or Cassandra.  The databases\nor other files may include, for example, data and information related to the source and destination of a network request, the data contained in the request, etc. Systems and methods of disclosed embodiments, however, are not limited to separate\ndatabases.  Database(s) 3760 may contain software code or macros that facilitate rapid searching and comparison by database access module 3750.  In some embodiments, database(s) 3760 may be configured to store one or more predetermined privacy mode\ntriggers, or relevant information thereto.  The stored information may comprise image data, sound data, textual data, or a combination of these forms.  Additionally or alternatively, database(s) 3760 may be configured to store privacy mode settings for\nwearable apparatus 110 associated with the one or more predetermined privacy mode triggers.  The stored information relating to privacy mode settings may comprise image data, sound data, textual data, or a combination of these forms.  The information may\nbe preserved in a form that is readily readable, accessible, and/or executable by one or more of database access module 3750 and action execution module 3740.\nImage analysis module 3710, audio analysis module 3720, image alteration module 3730, action execution module 3740, database access module 3750, and/or database(s) 3760 may be implemented in software, hardware, firmware, a mix of any of those,\nor the like.  For example, if the modules are implemented in software, they may be stored in memory 550/550a/550b, as shown in FIG. 37.  Other components of apparatus 110 may be configured to perform process to implement and facilitate operations of the\nmodules.  Thus, image analysis module 3710, audio analysis module 3720, image alteration module 3730, action execution module 3740, database access module 3750, and/or database(s) 3760 may include software, hardware, or firmware instructions (or a\ncombination thereof) executable by one or more processors (e.g., processor 210/540), alone or in various combinations with each other.  For example, the modules may be configured to interact with each other and/or other modules of wearable apparatus 110\nto perform functions consistent with disclosed embodiments.  In some embodiments, any of the disclosed modules (e.g., image analysis module 3710, audio analysis module 3720, image alteration module 3730, action execution module 3740, database access\nmodule 3750, and/or database(s) 3760) may each include dedicated sensors (e.g., IR, image sensors, etc.) and/or dedicated application processing devices to perform the functionality associated with each module.\nAs used herein, real-time image data may refer to image data captured in real-time or near real-time.  For example, image analysts module 3710 may monitor the field of view of wearable apparatus 110 to detect inputs while one or more of image\nalteration module 3730 and action execution module 3740 may determine whether to initiate one or more actions.  Accordingly, image analysis module 3710, image alteration module 3730, and action execution module 3740 may operate in parallel or in any\ncombination to process image data received from image sensor 220.  That is, wearable apparatus 110 may capture and analyze image data via image sensor 220 in parallel, or may institute a queue-like implementation whereby image data is captured and then\nanalyzed in a continuous fashion (i.e., a first image is captured and analyzed while a subsequent image is captured and then subsequently analyzed).\nConsistent with disclosed embodiments, wearable apparatus 110 may have a privacy mode, with associated automatically variable privacy settings.  When the privacy mode is enacted, such as by detection of a privacy mode trigger, wearable apparatus\n110 may, depending on the associated privacy settings, execute one or more actions such as stopping or suspending capturing images of the environment of user 100, stopping or suspending capturing sounds from the environment of user 100, stopping or\nsuspending storing captured images, stopping or suspending storing captured sounds, or other actions.  The devices and methods discussed under this heading may be combined with any of the devices and/or methods discussed above and below.\nIn some embodiments, the privacy mode may be associated with one or more private contextual situations.  The private contextual situation may be a situation where the privacy of a person or a plurality of persons is of concern, including that of\nuser 100.  These situations may make it inappropriate for wearable apparatus 110 to capture images or sounds including or associated with the person or persons.  In these embodiments, one or more privacy mode triggers may be associated with a particular\nprivate contextual situation.  The privacy mode triggers may be of a visual, audio, or textual nature.  In these embodiments, each privacy mode trigger may be associated with one or more privacy mode settings that may be automatically adjusted by\nwearable apparatus 110 and associated components to protect the privacy of the person or persons.  Adjustment of the privacy mode settings may, as non-limiting examples, alter captured images, suspend storage or capture of images or sounds, etc.\nVisual privacy mode triggers associated with particular private contextual situations may be predefined, and may include, but not be limited to, entry into a bathroom, exit from a bathroom, entry into a private zone, exit from a private zone, a\nchild, nudity, a sign prohibiting recording, a tag associated with a limitation on recording, a predefined hand gesture, a restroom sign, a toilet, and/or a face of an individual.  As an example, a predefined hand gesture comprising a visual privacy mode\ntrigger may include a hand gesture from an individual within a field of view of wearable apparatus 110 and image sensor 120 to stop capturing image of the person.  In some embodiments, being near or in the restroom or toilet may be a private contextual\nsituation.  In some embodiments, nudity of a person or a particular part of the human body being within the field of view of wearable apparatus 110 and image sensor 220 may be a private contextual situation.\nAudio privacy mode triggers associated with particular private contextual situations may be predefined, and may include, but not be limited to, sounds associated with a bathroom or a toilet, an indicator of a confidential conversation, an\nindicator of a desire of an individual not to be recorded, an indicator of an individual warning that the scene or other information should not be recorded, etc.\nThe visual and audio privacy mode triggers may be stored in database(s) 3760, as described above.  Associated privacy mode settings may also be stored in database(s) 3760, and may be stored with or otherwise associated with one or more\nparticular privacy mode triggers.  For example, folders or other organizational techniques may be used to group all privacy mode triggers and privacy mode settings for a particular private contextual situation.\nIn some embodiments, once wearable apparatus 110, image sensor 220, and/or audio sensor 3610 detect that a private contextual situation exists, the privacy mode settings may be automatically varied by the disclosed components and modules\nconsistent with disclosed embodiments.  In some embodiments, disclosed components and modules, consistent with disclosed embodiments, may revert the privacy mode settings to their prior state and may resume normal operation once the private contextual\nsituation is determined to no longer exist.\nFIGS. 38A-C show example environments that user 100 and wearable apparatus 110 may encounter, consistent with disclosed embodiments.  FIGS. 38A-C are depicted as example fields of view that may be perceived by user 100 through an wearable\napparatus 110 equipped with an image sensor 220.  In some embodiments, the wearable apparatus 110 may be associated with a pair of glasses 130 as discussed above.  In other embodiments, wearable apparatus 110 may be attached to clothing worn by user 100,\nor may otherwise be associated with user 100's body.  In some embodiments, wearable apparatus 110 may be associated or affixed to a remote structure, including but not limited to a stick, a pole, a drone, a vehicle, a robot, or another such unit.\nFIG. 38A depicts an example environment involving a bathroom, which may be associated with a private contextual situation.  The environment of user 100 may, for example include a restroom, signified in FIG. 38A by restroom door 3810.  In other\nembodiments, there may be a sign or other visual indicator, or a toilet.  In still other embodiments, there may be an audio indicator, such as a flushing toilet, operation of a sink, a hand dryer, etc. After identifying restroom door 3810 from an image\ncaptured by wearable apparatus 110 and image sensor 220, or otherwise identifying associated sounds via audio sensor 3610, processor 210/540 may compare the image of restroom door 3810 with the visual triggers stored in database(s) 3760, and/or the\ndetected sounds with the audio triggers stored in database(s) 3760, as will be described in further detail below in association with process 3900.  Based on operations and feedback associated with one or more software modules stored in memory\n550/550a/550b, processor 210/540 may determine that image sensor 220 and/or audio sensor 3610 is capturing an image or sound of a toilet or restroom, which indicates that there is a private contextual situation within the field of view of user 100 and\nwearable apparatus 110.  Wearable apparatus 110 may stop, suspend, or otherwise alter image and sound data capture of the environment including the rest room.  When user 100 walks away from the restroom such that the images no longer indicate the\npresence of the private contextual situation (such as restroom door 3810), and/or the sounds no longer indicate the situation, wearable apparatus 110 may resume normal capture and/or storage of the images and sounds of the environment of user 100.\nFIG. 38B depicts an example environment involving an individual 3820, which may be associated with a private contextual situation.  The environment of user 100 may include individual 3820.  In some embodiments, individual 3820 may perceive the\npresence of user 100, and may further perceive that user 100 is in possession of and/or is operating wearable apparatus 110.  In some embodiments, individual 3820 may provide an audible warning to user 100 to stop operation of wearable apparatus 110,\nsuch as audible warning 3830 (\"TURN IT OFF\").  In some embodiments, audible warning 3830 may not be present.  In some embodiments, individual 3820 may not perceive the presence of user 100 and/or may not perceive that user 100 is in possession of and/or\nis operating wearable apparatus 110.  After identifying individual 3820 from an image captured by wearable apparatus 110 and image sensor 220, and optionally identifying associated sounds such as audible warning 3830 via audio sensor 3610, processor\n210/540 may compare the image of individual 3820 with the visual triggers stored in database(s) 3760, and/or the detected sounds such as audible warning 3830 with the audio triggers stored in database(s) 3760, as will be described in further detail below\nin association with process 3900.  Based on operations and feedback associated with one or more software modules stored in memory 550/550a/550b, processor 210/540 may determine that image sensor 220 and/or audio sensor 3610 is capturing an image or sound\nindicating a private contextual situation within the field of view of user 100 and wearable apparatus 110 that may be associated with individual 3820, or with another individual, or with the scene generally.  Wearable apparatus 110 may stop, suspend, or\notherwise alter image and sound data capture of the environment.  When user 100 walks away from the scene (and/or from individual 3820) such that the images no longer indicate the presence of the private contextual situation and/or the sounds no longer\nindicate the situation, wearable apparatus 110 may resume normal capture and/or storage of the images and sounds of the environment of user 100.\nFIG. 38C depicts an example environment involving an environment typically associated with a private contextual situation; here, a courtroom.  The environment of user 100 may include visual indicators of a courtroom, such as judge 3840.  In some\nembodiments, judge 3840 may perceive the presence of user 100, and may further perceive that user 100 is in possession of and/or is operating wearable apparatus 110.  In some embodiments, judge 3840 may provide an audible warning to user 100 to stop\noperation of wearable apparatus 110.  In some embodiments, no audible warning may be present.  In some embodiments, judge 3840 may not perceive the presence of user 100 and/or may not perceive that user 300 is in possession of and/or is operating\nwearable apparatus 110.  In some embodiments, further visual or audio indicia present within the environment of user 100 may indicate a private contextual situation, such as sign 3850 (\"NO AUDIO OR VISUAL RECORDING\").  After identifying objects such as\njudge 3840 and sign 3850 from an image captured by wearable apparatus 110 and image sensor 220, and optionally identifying associated sounds such as an audible warning from judge 3840 via audio sensor 3610, processor 210/540 may compare the image(s) with\nthe visual triggers stored in database(s) 3760, and/or the detected sound(s) with the audio triggers stored in database(s) 3760, as will be described in further detail below in association with process 3900.  Based on operations and feedback associated\nwith one or more software modules stored in memory 550/550a/550b, processor 210/540 may determine that image sensor 220 and/or audio sensor 3610 is capturing an image or sound indicating a private contextual situation within the field of view of user 100\nand wearable apparatus 110 that may be associated with the courtroom (or similar environment/situation).  Wearable apparatus 110 may stop, suspend, or otherwise alter image and sound data capture of the environment.  When user 100 walks away from the\nscene (i.e., leaves the courtroom, or court adjourns) such that the images no longer indicate the presence of the private contextual situation and/or the sounds no longer indicate the situation, wearable apparatus 110 may resume normal capture and/or\nstorage of the images and sounds of the environment of user 100.\nFIG. 39 is a flowchart illustrating an example of a process 3900 for automatically varying settings associated with a privacy mode for an image sensor associated with a wearable apparatus, consistent with disclosed embodiments.  Process 3900, as\nwell as any or all of the individual steps therein, may be performed by various aspects of wearable apparatus 110, such as processor 210/540, image sensor 220, audio sensor 3610, image analysis module 3710, audio analysis module 3720, image alteration\nmodule 3730, action execution module 3740, database access module 3750, and database(s) 3760, or any subcomponents therein, hi some embodiments, one or more steps of process 3900 may be performed by a remote computing system, such as computing system 120\nor server 250, for exemplary purposes, FIG. 39 and process 3900 are described as being performed by processor 210, executing software instructions stored within memory 550.\nProcessor 210 may execute software instructions via image analysis module 3710 that enable wearable apparatus 110 to capture real-time image data from the environment of a user 100 using a camera associated with an image sensor, such as image\nsensor 220 (Step 3910).  In some embodiments, the captured first set of real-time image data may be received as a single streaming video file.  In other embodiments, the real-time image data may be received as a series of still images.  When the captured\nimage data is received, processor 210 may store the data in memory 550.  In some embodiments, the image data may be processed in real-time and not stored.  In some embodiments, wearable apparatus 110 may additionally or alternatively via processor 210\nexecute software instructions via audio analysis module 3720 that enable wearable apparatus 110 to capture real-time audio data from the environment of a user 100 using an audio sensor 3610, for example, a microphone.  When the captured audio data is\nreceived, processor 210 may store the data in memory 550.  In some embodiments, the audio data may be processed in real-time and not stored.\nProcessor 210 may execute software instructions via one or more of image analysis module 3710 and/or audio analysis module 3720 that enable wearable apparatus 110 to detect that user 100 has perceived the presence of a privacy mode trigger (Step\n3920).  As discussed above, the term \"privacy mode trigger\" includes any information in the image data or audio data that may be associated with a private contextual situation.  Privacy mode triggers may be visual and/or audio in nature.  Examples of\nvisual privacy mode triggers that may be detected by wearable apparatus 110 through image sensor 220 and image analysis module 3710 may include, but not be limited to, a presence in the one or more of the plurality of images captured in step 3910 of\nfeatures associated with a bathroom; entry into a bathroom; exit from a bathroom; entry into a private zone; exit from a private zone; a child; nudity; a sign prohibiting recording; a tag associated with a limitation on recording; or an appearance in the\none or more of the plurality of images a representation of a recognized individual.  Examples of audio privacy mode triggers that may be detected by wearable apparatus 110 through audio sensor 3610 and audio analysis module 3720 may include, but not be\nlimited to, an indicator of a confidential conversation; an indicator of a desire of an individual not to be recorded; or the presence of the voice of a recognized individual.  In some embodiments, step 3920 may be performed by a remote computing system,\nsuch as computing system 120 or server 250.\nIn some embodiments, wearable apparatus 110 may include a transmitter, such as wireless transceiver 530, and processor 210 may be configured to transmit image and/or audio data (either stored data or in real time) to a remote system such as\ncomputing system 120 or server 250 for purposes of analyzing the image and/or audio data to determine whether a privacy mode trigger is present in the data.  In other embodiments, processor 210 may be configured not to transmit the data, and may instead\nexecute software instructions stored on memory 550, such as image analysis module 3710, audio analysis module 3720, image alteration module 3730, action execution module 3740, database access module 3750, and database(s) 3760.\nThese modules and/or databases may further be executed to analyze information related to the privacy mode trigger, such as associated privacy mode settings (Step 3930).  For example, database access module 3750 may receive information from one\nor more of image analysis module.  3710 and/or audio analysis module 3720 relating to a detected privacy mode trigger and/or private contextual situation.  Database access module 3750 may be configured to search database(s) 3760 for one or more entries\nassociated with one or more privacy mode settings for the predefined, recognized privacy mode trigger.  In some embodiments, if the detected privacy mode trigger is not one of the predefined privacy mode triggers with information stored in database(s)\n3760, user 100 may be prompted by wearable apparatus 110 or computing system 120 through display 260 to configure privacy mode settings associated with the new privacy mode trigger.\nDatabase access module 3750 may determine one or more actions associated with the privacy mode settings for the detected privacy mode trigger that, when executed by processor 210, may automatically cause one or more adjustments to wearable\napparatus 110 based on those settings (Step 3940).  In some embodiments, step 3940 may be performed by a remote computing system, such as computing system 120 or server 250.  The action and/or adjustment associated with a particular privacy mode trigger\nmay be defined by software instructions or other information associated with the privacy mode settings found within database(s) 3760, and may be tailored in a manner to be unique to a particular privacy mode trigger and private contextual situation.\nFor example, if a privacy mode trigger is detected by image analysis module 3710 in image data captured via image sensor 220, the action/adjustment may include one or more of causing a suspension of image capture by image sensor 220 or causing a\nsuspension of storage of images captured by image sensor 220.  In embodiments such as that depicted in FIGS. 36A-36C where wearable apparatus 110 includes a transmitter, such as wireless transceiver 530, the action/adjustment may include a limitation\nplaced on a type of information transmitted to a paired device, for example, computing system 120.  In some embodiments, the action/adjustment may include enabling capturing of images by image sensor 220, but not storing them, such as in memory 550 or\ndatabase(s) 3760.  In other embodiments, the action/adjustment may include prohibiting transmission of captured images.  In still other embodiments, the action/adjustment may include transmission of caricature or cartoon representations of captured\nimages from image sensor 220, or transmission of low resolution, distorted, and/or blurred versions of captured images.  In other embodiments, the action/adjustment may include applying an image filter to images captured via image sensor 220, including\nbut not limited to applying a convolution model filter.  In some embodiments, the action/adjustment may include prohibiting the posting of captured images on social media.  In other embodiments, the action/adjustment may include blockage of at least part\nof an application program interface (\"API\") functionality; for example, if a secret product or idea is being discussed, the device may prohibit misappropriation of the idea or visual/audio representations thereof through third party software programs.\nIn some embodiments, the privacy mode trigger may include an appearance in the one or more of the plurality of images captured from image sensor 220 of a representation of a recognized individual, as determined by database access module 3750 and\ndatabase(s) 3760.  In these embodiments, a determined action/adjustment associated with a privacy mode setting for these triggers may include causing transmission (such as via wireless transceiver 530) of information indicative of the identity of the\nindividual (such as individual 3820 or judge 3840) to a paired device, such as computing system 120.  In these embodiments, processor 210 may ensure that the one or more of the plurality of images themselves are not transmitted to the paired device.  In\nthese embodiments, the system and incorporated components enable the paired device to execute one or more functions relating to the detected individual without receiving the one or more images.  This configuration protects the privacy of the individual\nand of user 100.  In these embodiments, the executable function may include, but not be limited to, displaying information relating to the individual, such as on display 260.\nIn other embodiments, if a privacy mode trigger is detected by audio analysis module 3720 in audio data captured via audio sensor 3610, the action/adjustment may include one or more of suspending capture of audio information from audio sensor\n3610 (e.g., a microphone), or suspending transmission of audio information from audio sensor 3610.\nVia one or more of image alteration module 3730 and/or action execution module 3740, processor 210 may be configured to execute the determined action relating to the privacy mode settings so as to automatically cause one or more adjustments to\nwearable apparatus 110 (Step 3950).  In some embodiments, Step 3950 may be performed by a remote computing system, such as computing system 120 or server 250.  The actions may be any of the actions described above or below, as defined by the\ninformational content of the privacy mode settings associated with the detected privacy mode trigger.  In some embodiments, feedback relating to the action/adjustment may be generated via feedback outputting unit 230 and displayed to user 100 via a\npaired device, such as via display 260 of computing system 120.\nIn some embodiments, processor 210, via image alteration module 3730, may be further programmed to identify one or more portions of at least one image from a plurality of images captured by image sensor 220.  In these embodiments, the presence\nof a particular privacy mode trigger may necessitate different adjustments or actions; similarly, if multiple privacy mode triggers are present, multiple actions/adjustments may be necessary.  Based on the presence of the privacy mode trigger(s), image\nalteration module 3730 may identify at least a first portion and a second portion of the image, and may process the first portion using a first processing scheme, and process the second portion using a second processing scheme.  The first and second\nprocessing schemes may comprise software instructions that include or are associated with the privacy mode settings stored in database(s) 3760 for a given privacy mode trigger and private contextual situation.\nThe foregoing description has been presented for purposes of illustration.  It is not exhaustive and is not limited to the precise forms or embodiments disclosed.  Modifications and adaptations will be apparent to those skilled in the art from\nconsideration of the specification and practice of the disclosed embodiments.  Additionally, although aspects of the disclosed embodiments are described as being stored in memory, one skilled in the art will appreciate that these aspects can also be\nstored on other types of computer readable media, such as secondary storage devices, for example, hard disks or CD ROM, or other forms of RAM or ROM, USB media, DVD, Blu-ray, Ultra HD Blu-ray, or other optical drive media.\nComputer programs based on the written description and disclosed methods are within the skill of an experienced developer.  The various programs or program modules can be created using any of the techniques known to one skilled in the art or can\nbe designed in connection with existing software.  For example, program sections or program modules can be designed in or by means of .Net Framework, .Net Compact Framework (and related languages, such as Visual Basic, C, etc.), Java, C++, Objective-C,\nHTML, HTML/AJAX combinations, XML, or HTML with included Java applets.\nMoreover, while illustrative embodiments have been described herein, the scope of any and all embodiments having equivalent elements, modifications, omissions, combinations (e.g., of aspects across various embodiments), adaptations and/or\nalterations as would be appreciated by those skilled in the art based on the present disclosure.  The limitations in the claims are to be interpreted broadly based on the language employed in the claims and not limited to examples described in the\npresent specification or during the prosecution of the application.  The examples are to be construed as non-exclusive.  Furthermore, the steps of the disclosed methods may be modified in any manner, including by reordering steps and/or inserting or\ndeleting steps.  It is intended, therefore, that the specification and examples be considered as illustrative only, with a true scope and spirit being indicated by the following claims and their full scope of equivalents.", "application_number": "15493894", "abstract": " A wearable imaging apparatus is provided for capturing and processing\n     images from an environment of a user. In one implementation, the wearable\n     apparatus may be configured with a memory for storing privacy mode\n     triggers and associated automatically variable privacy mode settings, and\n     at least one processing device. The processing device may analyze the\n     images captured by the wearable apparatus, and recognize the presence of\n     at least one of the privacy mode triggers within the images. After\n     recognizing the at least one trigger, the processing device may cause one\n     or more adjustments to the wearable apparatus based on the privacy mode\n     settings associated with the at least one trigger.\n", "citations": ["9141194", "20060193387", "20080170130", "20080228821", "20080298571", "20090023472", "20090245633", "20120287217", "20140342331", "20150106623", "20150213305", "20150347823", "20160063893", "20160127641", "20160275781", "20160371436", "20170076110", "20170094018", "20170111844", "20170169237"], "related": ["15399227", "62275531"]}, {"id": "20170270919", "patent_code": "10373612", "patent_name": "Anchored speech detection and speech recognition", "year": "2019", "inventor_and_country_data": " Inventors: \nParthasarathi; Sree Hari Krishnan (Seattle, WA), Hoffmeister; Bjorn (Seattle, WA), King; Brian (Seattle, WA), Maas; Roland (Seattle, WA)  ", "description": "<BR><BR>BACKGROUND\nSpeech recognition systems have progressed to the point where humans can interact with computing devices using speech.  Such systems employ techniques to identify the words spoken by a human user based on the various qualities of a received\naudio input.  Speech recognition combined with natural language understanding processing techniques enable speech-based user control of a computing device to perform tasks based on the user's spoken commands.  The combination of speech recognition and\nnatural language understanding processing techniques is commonly referred to as speech processing.  Speech processing may also convert a user's speech into text data which may then be provided to various text-based software applications.\nSpeech processing may be used by computers, hand-held devices, telephone computer systems, kiosks, and a wide variety of other devices to improve human-computer interactions. <BR><BR>BRIEF DESCRIPTION OF DRAWINGS\nFor a more complete understanding of the present disclosure, reference is now made to the following description taken in conjunction with the accompanying drawings.\nFIG. 1 shows a system configured to perform speech recognition using a recurrent neural network representation of a lattice according to embodiments of the present disclosure.\nFIG. 2 is a conceptual diagram of a speech processing system according to embodiments of the present disclosure.\nFIG. 3 illustrates a neural network for speech recognition according to embodiments of the present disclosure.\nFIG. 4 illustrates a neural network for speech recognition according to embodiments of the present disclosure.\nFIG. 5 illustrates phoneme processing and word processing according to embodiments of the present disclosure.\nFIG. 6 illustrates a speech recognition lattice according to embodiments of the present disclosure.\nFIG. 7 illustrates different ways of traversing the lattice of FIG. 6 according to embodiments of the present disclosure.\nFIG. 8 illustrates an N-best list of hypotheses associated with the lattice of FIG. 6 according to embodiments of the present disclosure.\nFIG. 9 illustrates operation of an encoder according to embodiments of the present disclosure.\nFIG. 10 illustrates identifying a wakeword in input audio data according to embodiments of the present disclosure.\nFIG. 11 illustrates identifying reference audio data from a wakeword portion of input audio data according to embodiments of the present disclosure.\nFIG. 12 illustrates identifying reference audio data from pre-stored audio data according to embodiments of the present disclosure.\nFIG. 13 illustrates identifying reference audio data from first input audio data according to embodiments of the present disclosure.\nFIG. 14 illustrates encoding reference audio data into a feature vector according to embodiments of the present disclosure.\nFIG. 15 illustrates using a reference feature vector to classify input audio data according to embodiments of the present disclosure.\nFIG. 16A illustrates performing voice activity detection using input audio data and a label corresponding to the input audio data according to embodiments of the present disclosure.\nFIG. 16B illustrates performing ASR using input audio data and a label corresponding to the input audio data according to embodiments of the present disclosure.\nFIG. 17 illustrates classifying input audio data according to embodiments of the present disclosure.\nFIG. 18 illustrates data stored and associated with user profiles according to embodiments of the present disclosure.\nFIG. 19 is a block diagram conceptually illustrating example components of a device according to embodiments of the present disclosure.\nFIG. 20 is a block diagram conceptually illustrating example components of a server according to embodiments of the present disclosure.\nFIG. 21 illustrates an example of a computer network for use with the system.\n<BR><BR>DETAILED DESCRIPTION\nAutomatic speech recognition (ASR) is a field of computer science, artificial intelligence, and linguistics concerned with transforming audio data associated with speech into text representative of that speech.  Similarly, natural language\nunderstanding (NLU) is a field of computer science, artificial intelligence, and linguistics concerned with enabling computers to derive meaning from text input containing natural language.  ASR and NLU are often used together as part of a speech\nprocessing system.\nASR and NLU can be computationally expensive.  That is, significant computing resources may be needed to process ASR and NLU within a reasonable time.  Because of this, a distributed computing environment may be used when performing speech\nprocessing.  A typical distributed environment may involve a local device having one or more microphones configured to capture sounds from a user speaking and convert those sounds into an audio signal.  Thus, the mechanical sound wave comprising the\naudio may be converted to an audio signal/data which is data representing the audio, for example pulse-code modulation (PCM) audio data.  The audio signal/data may then be sent to a downstream remote device for further processing, such as converting the\naudio signal into an ultimate command.  The command may then be executed by a combination of remote and local devices depending on the command itself.\nIn certain situations, an environment in which a speech command is spoken may be crowded, loud, or otherwise noisy in a manner that may interfere with speech processing.  For example, a user may speak an utterance including a command for a\nsystem to execute, but the device capturing the user's utterance may be in an environment with other individuals who are also talking.  A system may have difficulty in such situations identifying audio corresponding to the user who spoke the command\n(i.e., the desired speaker) so that speech recognition may be performed on that desired audio rather than on speech from other persons who are not speaking commands to the system.\nOffered is a system and method that improves the ability of the system to identify speech from a desired user during a command interaction with a user in a manner that does not significantly impact latency yet still allows the system to\ndistinguish desired speech from undesired speech.\nFIG. 1 shows a system 100 configured to perform speech recognition using improved desired speaker detection according to embodiments of the disclosure.  Although FIG. 1, and lower figures/discussion, illustrate the operation of the system in a\nparticular order, the steps described may be performed in a different order (as well as certain steps removed or added) without departing from the intent of the disclosure.  As shown in FIG. 1, a system 100 may include one or more devices 110 local to a\ndesired user(s) 10, as well as one or more networks 199 and one or more servers 120 connected to device 110 across network(s) 199.  The server(s) 120 (which may be one or more different physical devices) may be capable of performing traditional speech\nprocessing (such as ASR, NLU, query parsing, etc.) as described herein.  A single server may be capable of performing all speech processing or multiple server(s) 120 may combine to perform the speech processing.  Further, the server(s) 120 may be\nconfigured to execute certain commands, such as answering queries spoken by user 10.  In addition, certain speech detection or command execution functions may be performed by device 110.\nAs shown in FIG. 1, a device 110 receives an audio input 11 corresponding to a spoken utterance from a desired user 10.  The input audio 11 may also include speech or other noise from other persons 15 who are also local to device 110.  The\ndevice 110 may convert the audio 11 into audio data 111 and send the audio data to the server(s) 120.  A server(s) 120 may then receive (130) the audio data 111 corresponding to the spoken command via the network 199.  The server 120 determines (132)\nreference audio data corresponding to the desired speaker of the input audio data 111.  The reference audio data may be a first portion of the input audio data 111 or may be other reference audio data as discussed below.  The server 120 encodes (134) the\nreference audio data to obtain encoded reference audio data.  If the reference audio data was available ahead of time, this may occur in advance of receiving the audio data 111.  If the reference audio data is taken from the audio data 111 the encoding\nmay occur after receipt of the audio data 111.  Various encoding techniques, including those discussed below in reference to FIG. 9, may be used.  The server 120 then processes (136) further input audio data (such as audio feature vectors corresponding\nto further audio frames) using the encoded reference audio data.  An audio frame corresponds to a particular set of audio data, for example 25 ms worth of PCM or similar audio data.  For example, the server 120 may use a classifier or other trained\nmachine learning model to determine if the incoming audio feature vectors represent speech from the same speaker as the speech in the reference audio data by using the encoded reference audio data.  The server then labels (138) each audio feature vector\n(and/or the corresponding audio frame) as including desired speech, non-desired speech or non-speech.  This labeling may include assigning, for each audio feature vector/input audio frame, a first probability that the particular audio frame corresponds\nto desired speech, a second probability that the particular audio frame corresponds to non-desired speech, and a third probability that the particular audio frame corresponds to non-speech.  The process may be repeated for each audio feature vector.  The\nsystem then performs (140) ASR processing on the input audio frames (or corresponding audio feature vectors) using the encoded reference audio data and/or the individual frame labels/probabilities.  This may enable the system to focus its ASR processing\non the portions of the input audio corresponding to the desired speaker 10.  The system may then determine (142) ASR results, perform NLU (144) on the ASR results and execute (146) a command, which should correspond to the spoken utterance of the desired\nuser 10.\nFurther details of performing speech recognition using the present improvements are discussed below, following a discussion of the overall speech processing system of FIG. 2.  FIG. 2 is a conceptual diagram of how a spoken utterance is\ntraditionally processed, allowing a system to capture and execute commands spoken by a user, such as spoken commands that may follow a wakeword.  The various components illustrated may be located on a same or different physical devices.  Communication\nbetween various components illustrated in FIG. 2 may occur directly or across a network 199.  An audio capture component, such as a microphone of device 110, captures audio 11 corresponding to a spoken utterance.  The device 110, using a wakeword\ndetection module 220, then processes the audio, or audio data corresponding to the audio, to determine if a keyword (such as a wakeword) is detected in the audio.  Following detection of a wakeword, the device sends audio data 111 corresponding to the\nutterance, to a server 120 that includes an ASR module 250.  The audio data 111 may be output from an acoustic front end (AFE) 256 located on the device 110 prior to transmission.  Or the audio data 111 may be in a different form for processing by a\nremote AFE 256, such as the AFE 256 located with the ASR module 250.\nThe wakeword detection module 220 works in conjunction with other components of the device, for example a microphone (not pictured) to detect keywords in audio 11.  For example, the device 110 may convert audio 11 into audio data, and process\nthe audio data with the wakeword detection module 220 to determine whether speech is detected, and if so, if the audio data comprising speech matches an audio signature and/or model corresponding to a particular keyword.\nThe device 110 may use various techniques to determine whether audio data includes speech.  Some embodiments may apply voice activity detection (VAD) techniques implemented by a voice activity detector 222.  Such techniques may determine whether\nspeech is present in an audio input based on various quantitative aspects of the audio input, such as the spectral slope between one or more frames of the audio input; the energy levels of the audio input in one or more spectral bands; the\nsignal-to-noise ratios of the audio input in one or more spectral bands; or other quantitative aspects.  In other embodiments, the device 110 may implement a limited classifier configured to distinguish speech from background noise.  The classifier may\nbe implemented by techniques such as linear classifiers, support vector machines, and decision trees.  In still other embodiments, Hidden Markov Model (HMM) or Gaussian Mixture Model (GMM) techniques may be applied to compare the audio input to one or\nmore acoustic models in speech storage, which acoustic models may include models corresponding to speech, noise (such as environmental noise or background noise), or silence.  Still other techniques may be used to determine whether speech is present in\nthe audio input.\nOnce voice activity is detected in the audio received by the device 110 (or separately from speech detection), the device 110 may use the wakeword detection module 220 to perform wakeword detection to determine when a user intends to speak a\ncommand to the device 110.  This process may also be referred to as keyword detection, with the wakeword being a specific example of a keyword.  Specifically, keyword detection is typically performed without performing linguistic analysis, textual\nanalysis or semantic analysis.  Instead, incoming audio (or audio data) is analyzed to determine if specific characteristics of the audio match preconfigured acoustic waveforms, audio signatures, or other data to determine if the incoming audio \"matches\"\nstored audio data corresponding to a keyword.\nThus, the wakeword detection module 220 may compare audio data to stored models or data to detect a wakeword.  One approach for wakeword detection applies general large vocabulary continuous speech recognition (LVCSR) systems to decode the audio\nsignals, with wakeword searching conducted in the resulting lattices or confusion networks.  LVCSR decoding may require relatively high computational resources.  Another approach for wakeword spotting builds hidden Markov models (HMM) for each key\nwakeword word and non-wakeword speech signals respectively.  The non-wakeword speech includes other spoken words, background noise etc. There can be one or more HMMs built to model the non-wakeword speech characteristics, which are named filler models. \nViterbi decoding is used to search the best path in the decoding graph, and the decoding output is further processed to make the decision on keyword presence.  This approach can be extended to include discriminative information by incorporating hybrid\nDNN-HMM decoding framework.  In another embodiment the wakeword spotting system may be built on deep neural network (DNN)/recurrent neural network (RNN) structures directly, without HMM involved.  Such a system may estimate the posteriors of wakewords\nwith context information, either by stacking frames within a context window for DNN, or using RNN.  Following-on posterior threshold tuning or smoothing is applied for decision making.  Other techniques for wakeword detection, such as those known in the\nart, may also be used.\nOnce the wakeword is detected, the local device 110 may \"wake\" and begin transmitting audio data 111 corresponding to input audio 11 to the server(s) 120 for speech processing.  Audio data corresponding to that audio may be sent to a server 120\nfor routing to a recipient device or may be sent to the server for speech processing for interpretation of the included speech (either for purposes of enabling voice-communications and/or for purposes of executing a command in the speech).  The audio\ndata 111 may include data corresponding to the wakeword, or the portion of the audio data corresponding to the wakeword may be removed by the local device 110 prior to sending.\nUpon receipt by the server(s) 120, the system 100 may use various techniques for determining the beginning and end of speech to be processed.  For purposes of illustration, in system 100 the beginpoint of speech is described as determined by the\ndevice 110 and the endpoint of the speech is described as determined by the server 120 (after receipt of corresponding audio data from the device 110), but different components may perform the beginpointing/endpointing without diverging from the present\ndisclosure.\nTo determine the beginning or end of an audio command, a number of techniques may be used.  In one embodiment the system may determine pauses in spoken words and may interpret those pauses as potential breaks in a conversation.  Thus, while the\ndiscussion herein may refer to determining or declaring an endpoint, what the system does is estimate that a spoken command has ended (i.e., reached an endpoint) based on the various considerations described herein.  Breaks in a conversation may be\nconsidered as breaks between utterances and thus considered the beginning (beginpoint) or end (endpoint) of an utterance.  The beginning/end of an utterance may also be detected using speech/voice characteristics.  Other techniques may also be used to\ndetermine the beginning of an utterance (also called beginpointing) or end of an utterance (endpointing).  Beginpointing/endpointing may be based, for example, on the number of silence/non-speech audio frames, for instance the number of consecutive\nsilence/non-speech frames.  For example, some systems may employ energy based or acoustic model based VAD techniques.  Such techniques may determine whether speech is present in an audio input based on various quantitative aspects of the audio input,\nsuch as the spectral slope between one or more frames of the audio input; the energy levels (such as a volume, intensity, amplitude, etc.) of the audio input in one or more spectral bands; zero-crossing rate; the signal-to-noise ratios of the audio input\nin one or more spectral bands; or other quantitative aspects.  These factors may be compared to one or more thresholds to determine if a break in speech has occurred that qualifies as a beginpoint/endpoint.  Such thresholds may be set according to user\ninput, or may be set by a device.  In some embodiments, the beginpointing/endpointing may be further configured to determine that the audio input has an energy level satisfying a threshold for at least a threshold duration of time.  In such embodiments,\nhigh-energy audio inputs of relatively short duration, which may correspond to sudden noises that are relatively unlikely to include speech, may be ignored.  The beginpointing/endpointing may compare the energy level to the energy level threshold (and\noptionally to the threshold duration) to determine whether the energy level threshold is met.\nIn certain embodiments, HMM or GMM techniques may be applied to compare the audio input to one or more acoustic models in speech storage, which acoustic models may include models corresponding to speech, noise (such as environmental noise or\nbackground noise), or silence/non-speech.  Non-speech frames may not necessarily represent complete silence (for example, certain noise may still be present in the audio), but the frames may lack acoustic characteristics typical of speech and thus may be\ndeemed to be non-speech frames.  Still other techniques may be used to determine whether speech begins/ends in the input audio data.\nThe length of a pause sufficient to qualify the pause as a beginpoint/endpoint may depend on the identity of the speaker.  If the system is configured to perform speaker identification (techniques for which are known in the art), the system may\nidentify the speaker and adjust the pause length sufficient to determine an endpoint accordingly.  The system may also be configured to learn pause tendencies of different speakers and to adjust its endpointing processing accordingly.  For example,\nduring system training/enrollment, a speaker's pause tendencies between utterances or between topics may be recorded and used to train the endpointing processing of the system.  Such tendencies may also be recorded at runtime and used to further adjust\nthe system.  Different pause lengths may also be configured for different spoken languages as the pause length may vary depending on the language spoken (for example pauses in conversational English may be different from pauses in conversational\nSpanish).  The begin/end of an utterance may also be determined by various characteristics of the speech including pitch, prosody, volume, rhythm, stress, intonation, cepstrum, etc. of the speech which may be determined by audio and/or speech processing\ncomponents of the device.  For example, a rising or falling tone of a voice may indicate a new utterance, the end of a command, etc. The system may train on voice characteristics (which may or may not be also tied to speaker identity) that indicate when\nan utterance ends, and thus when an end of the speech should be marked by the system.  These techniques may be used to modify/customize the language models discussed above, such that expected pauses in the language model(s) may be based on an identity of\na speaker.\nUsing variations of the techniques described above, the beginpointing/endpointing may determine a confidence level whose value corresponds to a likelihood that the location of the point in question (i.e., the point in the audio data at which the\nbeginpointing/endpointing occurs) represents the start/end of an utterance/command.  The confidence score may depend on factors such as the technique used to determine the marker, the length of the pause, the speaker identity, etc. For example, if the\nconfidence level satisfies a confidence level threshold, it may be determined that a detected silence is sufficient (e.g., the length of a pause in the speech exceeds a threshold), that speech is present in the audio input, and that an utterance\nbegin/end may be marked.  However, if the confidence level does not satisfy the confidence level the system may determine that there is no speech in the audio input.\nOnce a wakeword/beginpoint is detected, the device 110 may begin sending audio data to the server 120.  The server 120 will continue speech processing on the audio data until an endpoint is detected.  The audio data from the beginpoint to the\nendpoint is thus considered by the system 100 when performing the speech processing for the utterance.\nIn certain configurations, the process for endpointing has been similar to the process for beginpointing as discussed above.  One difference between endpointing and beginpointing, however, is that during endpointing, the system has some\nknowledge of the contents of the utterance that is being endpointed.  Thus, while beginpointing may sometimes occur when there is no prior speech, endpointing occurs when there is prior speech, namely the speech of the utterance whose endpoint is being\ndetected.\nFurther, one drawback to certain VAD or other techniques that rely solely on pause length, is that they have difficulty distinguishing between mid-utterance pauses, and pauses that truly indicate the end of an utterance.  Further, certain\nsystems may encounter difficulty in noisy environments where separating speech from noise impacts proper pause detection.\nOnce audio data corresponding to speech is identified, an ASR module 250 may convert the audio data 111 into text.  The ASR transcribes audio data into text data representing the words of the speech contained in the audio data.  The text data\nmay then be used by other components for various purposes, such as executing system commands, inputting data, etc. A spoken utterance in the audio data is input to a processor configured to perform ASR which then interprets the utterance based on the\nsimilarity between the utterance and pre-established language models 254 stored in an ASR model storage 252c.  For example, the ASR process may compare the input audio data with models for sounds (e.g., subword units or phonemes) and sequences of sounds\nto identify words that match the sequence of sounds spoken in the utterance of the audio data.\nThe different ways a spoken utterance may be interpreted (i.e., the different hypotheses) may each be assigned a probability or a confidence score representing the likelihood that a particular set of words matches those spoken in the utterance. \nThe confidence score may be based on a number of factors including, for example, the similarity of the sound in the utterance to models for language sounds (e.g., an acoustic model 253 stored in an ASR Models Storage 252), and the likelihood that a\nparticular word which matches the sounds would be included in the sentence at the specific location (e.g., using a language or grammar model).  Thus each potential textual interpretation of the spoken utterance (hypothesis) is associated with a\nconfidence score.  Based on the considered factors and the assigned confidence score, the ASR process 250 outputs the most likely text recognized in the audio data.  The ASR process may also output multiple hypotheses in the form of a lattice or an\nN-best list with each hypothesis corresponding to a confidence score or other score (such as probability scores, etc.).\nThe device or devices performing the ASR processing may include an acoustic front end (AFE) 256 and a speech recognition engine 258.  The acoustic front end (AFE) 256 transforms the audio data from the microphone into data for processing by the\nspeech recognition engine.  The speech recognition engine 258 compares the speech recognition data with acoustic models 253, language models 254, and other data models and information for recognizing the speech conveyed in the audio data.  The AFE may\nreduce noise in the audio data and divide the digitized audio data into frames representing a time intervals for which the AFE determines a number of values, called features, representing the qualities of the audio data, along with a set of those values,\ncalled an audio feature vector, representing the features/qualities of the audio data within the frame.  In one configuration each audio frame includes 25 ms of audio and the frames start at 10 ms intervals resulting in a sliding window where adjacent\naudio frames include 15 ms of overlapping audio.  Many different features for a particular frame may be determined, as known in the art, and each feature represents some quality of the audio that may be useful for ASR processing.  A number of approaches\nmay be used by the AFE to process the audio data, such as mel-frequency cepstral coefficients (MFCCs), perceptual linear predictive (PLP) techniques, neural network feature vector techniques, linear discriminant analysis, semi-tied covariance matrices,\nor other approaches known to those of skill in the art.  Thus the AFE may create an audio feature vector including a variety of data representing a particular audio frame.\nThe speech recognition engine 258 may process the output from the AFE 256 with reference to information stored in speech/model storage (252).  Alternatively, post front-end processed data (such as audio feature vectors) may be received by the\ndevice executing ASR processing from another source besides the internal AFE.  For example, the device 110 may process audio data into audio feature vectors (for example using an on-device AFE 256) and transmit that information to a server across a\nnetwork 199 for ASR processing.  Audio feature vectors may arrive at the server encoded, in which case they may be decoded prior to processing by the processor executing the speech recognition engine 258.\nThe speech recognition engine 258 attempts to match received audio feature vectors to language phonemes and words as known in the stored acoustic models 253 and language models 254.  The speech recognition engine 258 computes recognition scores\nfor the audio feature vectors based on acoustic information and language information.  The acoustic information is used to calculate an acoustic score representing a likelihood that the intended sound represented by a group of audio feature vectors\nmatches a language phoneme.  The language information is used to adjust the acoustic score by considering what sounds and/or words are used in context with each other, thereby improving the likelihood that the ASR process will output speech results that\nmake sense grammatically.  The specific models used may be general models or may be models corresponding to a particular domain, such as music, banking, etc.\nThe speech recognition engine 258 may use a number of techniques to match audio feature vectors to phonemes, for example using Hidden Markov Models (HMMs) to determine probabilities that audio feature vectors may match phonemes.  Sounds received\nmay be represented as paths between states of the HMM and multiple paths may represent multiple possible text matches for the same sound.  Instead of (or in addition to) phonemes, senons may be used as an acoustic unit.  A senon is an acoustic\nrealization of a phoneme.  Each phoneme may have a number of different sounds depending on its context (e.g., the surrounding phonemes).  While English may have approximately 50 phonemes it has several thousand senons.  Use of senons in ASR processing\nmay allow for improved ASR results.\nFollowing ASR processing, the ASR results may be sent by the speech recognition engine 258 to other processing components, which may be local to the device performing ASR and/or distributed across the network(s) 199.  For example, ASR results in\nthe form of a single textual representation of the speech, an N-best list including multiple hypotheses and respective scores, lattice, etc. may be sent to a server, such as server 120, for natural language understanding (NLU) processing, such as\nconversion of the text into commands for execution, either by the device 110, by the server 120, or by another device (such as a server running a specific application like a search engine, etc.).\nThe device performing NLU processing 260 (e.g., server 120) may include various components, including potentially dedicated processor(s), memory, storage, etc. A device configured for NLU processing may include a named entity recognition (NER)\nmodule 252 and intent classification (IC) module 264, a result ranking and distribution module 266, and NLU storage 273.  The NLU process may also utilize gazetteer information (284a-284n) stored in entity library storage 282.  The gazetteer information\nmay be used for entity resolution, for example matching ASR results with different entities (such as song titles, contact names, etc.) Gazetteers may be linked to users (for example a particular gazetteer may be associated with a specific user's music\ncollection), may be linked to certain domains (such as shopping), or may be organized in a variety of other ways.\nThe NLU process takes textual input (such as processed from ASR 250 based on the utterance 11) and attempts to make a semantic interpretation of the text.  That is, the NLU process determines the meaning behind the text based on the individual\nwords and then implements that meaning.  NLU processing 260 interprets a text string to derive an intent or a desired action from the user as well as the pertinent pieces of information in the text that allow a device (e.g., device 110) to complete that\naction.  For example, if a spoken utterance is processed using ASR 250 and outputs the text \"call mom\" the NLU process may determine that the user intended to activate a telephone in his/her device and to initiate a call with a contact matching the\nentity \"mom.\"\nThe NLU may process several textual inputs related to the same utterance.  For example, if the ASR 250 outputs N text segments (as part of an N-best list), the NLU may process all N outputs to obtain NLU results.\nThe NLU process may be configured to parsed and tagged to annotate text as part of NLU processing.  For example, for the text \"call mom,\" \"call\" may be tagged as a command (to execute a phone call) and \"mom\" may be tagged as a specific entity\nand target of the command (and the telephone number for the entity corresponding to \"mom\" stored in a contact list may be included in the annotated result).\nTo correctly perform NLU processing of speech input, the NLU process 260 may be configured to determine a \"domain\" of the utterance so as to determine and narrow down which services offered by the endpoint device (e.g., server 120 or device 110)\nmay be relevant.  For example, an endpoint device may offer services relating to interactions with a telephone service, a contact list service, a calendar/scheduling service, a music player service, etc. Words in a single text query may implicate more\nthan one service, and some services may be functionally linked (e.g., both a telephone service and a calendar service may utilize data from the contact list).\nThe name entity recognition module 262 receives a query in the form of ASR results and attempts to identify relevant grammars and lexical information that may be used to construe meaning.  To do so, a name entity recognition module 262 may begin\nby identifying potential domains that may relate to the received query.  The NLU storage 273 may include a databases of devices (274a-274n) identifying domains associated with specific devices.  For example, the device 110 may be associated with domains\nfor music, telephony, calendaring, contact lists, and device-specific communications, but not video.  In addition, the entity library may include database entries about specific services on a specific device, either indexed by Device ID, User ID, or\nHousehold ID, or some other indicator.\nA domain may represent a discrete set of activities having a common theme, such as \"shopping\", \"music\", \"calendaring\", etc. As such, each domain may be associated with a particular language model and/or grammar database (276a-276n), a particular\nset of intents/actions (278a-278n), and a particular personalized lexicon (286).  Each gazetteer (284a-284n) may include domain-indexed lexical information associated with a particular user and/or device.  For example, the Gazetteer A (284a) includes\ndomain-index lexical information 286aa to 286an.  A user's music-domain lexical information might include album titles, artist names, and song names, for example, whereas a user's contact-list lexical information might include the names of contacts. \nSince every user's music collection and contact list is presumably different, this personalized information improves entity resolution.\nA query is processed applying the rules, models, and information applicable to each identified domain.  For example, if a query potentially implicates both communications and music, the query will be NLU processed using the grammar models and\nlexical information for communications, and will be processed using the grammar models and lexical information for music.  The responses based on the query produced by each set of models is scored (discussed further below), with the overall highest\nranked result from all applied domains is ordinarily selected to be the correct result.\nAn intent classification (IC) module 264 parses the query to determine an intent or intents for each identified domain, where the intent corresponds to the action to be performed that is responsive to the query.  Each domain is associated with a\ndatabase (278a-278n) of words linked to intents.  For example, a music intent database may link words and phrases such as \"quiet,\" \"volume off,\" and \"mute\" to a \"mute\" intent.  The IC module 264 identifies potential intents for each identified domain by\ncomparing words in the query to the words and phrases in the intents database 278.\nIn order to generate a particular interpreted response, the NER 262 applies the grammar models and lexical information associated with the respective domain.  Each grammar model 276 includes the names of entities (i.e., nouns) commonly found in\nspeech about the particular domain (i.e., generic terms), whereas the lexical information 286 from the gazetteer 284 is personalized to the user(s) and/or the device.  For instance, a grammar model associated with the shopping domain may include a\ndatabase of words commonly used when people discuss shopping.\nThe intents identified by the IC module 264 are linked to domain-specific grammar frameworks (included in 276) with \"slots\" or \"fields\" to be filled.  For example, if \"play music\" is an identified intent, a grammar (276) framework or frameworks\nmay correspond to sentence structures such as \"Play {Artist Name},\" \"Play {Album Name},\" \"Play {Song name},\" \"Play {Song name} by {Artist Name},\" etc. However, to make recognition more flexible, these frameworks would ordinarily not be structured as\nsentences, but rather based on associating slots with grammatical tags.\nFor example, the NER module 260 may parse the query to identify words as subject, object, verb, preposition, etc., based on grammar rules and models, prior to recognizing named entities.  The identified verb may be used by the IC module 264 to\nidentify intent, which is then used by the NER module 262 to identify frameworks.  A framework for an intent of \"play\" may specify a list of slots/fields applicable to play the identified \"object\" and any object modifier (e.g., a prepositional phrase),\nsuch as {Artist Name}, {Album Name}, {Song name}, etc. The NER module 260 then searches the corresponding fields in the domain-specific and personalized lexicon(s), attempting to match words and phrases in the query tagged as a grammatical object or\nobject modifier with those identified in the database(s).\nThis process includes semantic tagging, which is the labeling of a word or combination of words according to their type/semantic meaning.  Parsing may be performed using heuristic grammar rules, or an NER model may be constructed using\ntechniques such as hidden Markov models, maximum entropy models, log linear models, conditional random fields (CRF), and the like.\nFor instance, a query of \"play mother's little helper by the rolling stones\" might be parsed and tagged as {Verb}: \"Play,\" {Object}: \"mother's little helper,\" {Object Preposition}: \"by,\" and {Object Modifier}: \"the rolling stones.\" At this point\nin the process, \"Play\" is identified as a verb based on a word database associated with the music domain, which the IC module 264 will determine corresponds to the \"play music\" intent.  No determination has been made as to the meaning of \"mother's little\nhelper\" and \"the rolling stones,\" but based on grammar rules and models, it is determined that these phrase relate to the grammatical object of the query.\nThe frameworks linked to the intent are then used to determine what database fields should be searched to determine the meaning of these phrases, such as searching a user's gazette for similarity with the framework slots.  So a framework for\n\"play music intent\" might indicate to attempt to resolve the identified object based {Artist Name}, {Album Name}, and {Song name}, and another framework for the same intent might indicate to attempt to resolve the object modifier based on {Artist Name},\nand resolve the object based on {Album Name} and {Song Name} linked to the identified {Artist Name}.  If the search of the gazetteer does not resolve the a slot/field using gazetteer information, the NER module 262 may search the database of generic\nwords associated with the domain (in the NLU storage 273).  So for instance, if the query was \"play songs by the rolling stones,\" after failing to determine an album name or song name called \"songs\" by \"the rolling stones,\" the NER 262 may search the\ndomain vocabulary for the word \"songs.\" In the alternative, generic words may be checked before the gazetteer information, or both may be tried, potentially producing two different results.\nThe comparison process used by the NER module 262 may classify (i.e., score) how closely a database entry compares to a tagged query word or phrase, how closely the grammatical structure of the query corresponds to the applied grammatical\nframework, and based on whether the database indicates a relationship between an entry and information identified to fill other slots of the framework.\nThe NER modules 262 may also use contextual operational rules to fill slots.  For example, if a user had previously requested to pause a particular song and thereafter requested that the voice-controlled device to \"please un-pause my music,\" the\nNER module 262 may apply an inference-based rule to fill a slot associated with the name of the song that the user currently wishes to play--namely the song that was playing at the time that the user requested to pause the music.\nThe results of NLU processing may be tagged to attribute meaning to the query.  So, for instance, \"play mother's little helper by the rolling stones\" might produce a result of: {domain} Music, {intent} Play Music, {artist name} \"rolling stones,\"\n{media type} SONG, and {song title} \"mother's little helper.\" As another example, \"play songs by the rolling stones\" might produce: {domain} Music, {intent} Play Music, {artist name} \"rolling stones,\" and {media type} SONG.\nThe output from the NLU processing (which may include tagged text, commands, etc.) may then be sent to a command processor 290, which may be located on a same or separate server 120 as part of system 100.  The destination command processor 290\nmay be determined based on the NLU output.  For example, if the NLU output includes a command to play music, the destination command processor 290 may be a music playing application, such as one located on device 110 or in a music playing appliance,\nconfigured to execute a music playing command.  If the NLU output includes a search request, the destination command processor 290 may include a search engine processor, such as one located on a search server, configured to execute a search command.\nNeural networks may be used to perform ASR processing including acoustic model processing and language model processing.  An example neural network for ASR is illustrated in FIG. 3.  A neural network may be structured with an input layer 302, a\nmiddle layer 304, and an output layer 306.  The middle layer may also be known as the hidden layer.  Each node of the hidden layer is connected to each node in the input layer and each node in the output layer.  Although illustrated in FIG. 3 with a\nsingle hidden layer, a neural network may include multiple middle layers.  In this case each node in a hidden layer will connect to each node in the next higher layer and next lower layer.  Each node of the input layer represents a potential input to the\nneural network and each node of the output layer represents a potential output of the neural network.  Each connection from one node to another node in the next layer may be associated with a weight or score.  A neural network may output a single output\nor a weighted set of possible outputs.\nIn one aspect, the neural network may be constructed with recurrent connections such that the output of the hidden layer of the network feeds back into the hidden layer again for the next set of inputs.  Such a neural network is illustrated in\nFIG. 4.  Each node of the input layer 402 connects to each node of the hidden layer 404.  Each node of the hidden layer 404 connects to each node of the output layer 406.  As illustrated, the output of the hidden layer 404 is fed back into the hidden\nlayer for processing of the next set of inputs.  A neural network incorporating recurrent connections may be referred to as a recurrent neural network (RNN).\nIn the case where an acoustic model uses a neural network, each node of the neural network input layer may represents an acoustic feature of an audio feature vector of acoustic features, such as those that may be output after the first pass of\nperforming speech recognition, and each node of the output layer represents a score corresponding to a subword unit (such as a phoneme, triphone, etc.) and/or associated states that may correspond to the sound represented by the audio feature vector. \nFor a given input to the neural network, it outputs a number of potential outputs each with an assigned score representing a probability that the particular output is the correct output given the particular input.  The top scoring output of an acoustic\nmodel neural network may then be fed into an HMM which may determine transitions between sounds prior to passing the results to a language model.\nIn the case where a language model uses a neural network, each node of the neural network input layer may represent a previous word and each node of the output layer may represent a potential next word as determined by the trained neural network\nlanguage model.  As a language model may be configured as a recurrent neural network which incorporates some history of words processed by the neural network, such as the network illustrated in FIG. 4, the prediction of the potential next word may be\nbased on previous words in an utterance and not just on the most recent word.  The language model neural network may also output weighted predictions for the next word.\nProcessing by a neural network is determined by the learned weights on each node input and the structure of the network.  Given a particular input, the neural network determines the output one layer at a time until the output layer of the entire\nnetwork is calculated.\nConnection weights may be initially learned by the neural network during training, where given inputs are associated with known outputs.  In a set of training data, a variety of training examples are fed into the network.  Each example typically\nsets the weights of the correct connections from input to output to 1 and gives all connections a weight of 0.  As examples in the training data are processed by the neural network, an input may be sent to the network and compared with the associated\noutput to determine how the network performance compares to the target performance.  Using a training technique, such as back propagation, the weights of the neural network may be updated to reduce errors made by the neural network when processing the\ntraining data.  In some circumstances, the neural network may be trained with an entire lattice to improve speech recognition when the entire lattice is processed.\nAs noted above, during speech recognition, an ASR module 250/speech recognition engine 258 may utilize acoustic models 253 to determine possible phonemes or other phonetic units that match the incoming audio data feature vectors.  The probable\nphonemes and related states/state transitions may be formed into paths traversing a lattice of potential phonemes.  Each path represents a progression of phonemes that potentially match the audio data represented by the audio feature vectors.  One path\nmay overlap with one or more other paths depending on the recognition scores calculated for each phoneme.  Certain probabilities are associated with each transition from state to state.  A cumulative path score may also be calculated for each path.  This\nprocess of determining scores based on the audio feature vectors may be called acoustic modeling.  When combining scores as part of the ASR processing, scores may be multiplied together (or combined in other ways) to reach a desired combined score or\nprobabilities may be converted to the log domain and added to assist processing.\nThe speech recognition engine 258 may also compute scores of branches of the paths based on language models or grammars.  Language modeling involves determining scores for what words are likely to be used together to form coherent words and\nsentences.  Application of a language model may improve the likelihood that the ASR module 250 correctly interprets the speech contained in the audio data.  For example, for an input audio sounding like \"hello,\" acoustic model processing that returns the\npotential phoneme paths of \"H E L O\", \"H A L O\", and \"Y E L O\" may be adjusted by a language model to adjust the recognition scores of \"H E L O\" (interpreted as the word \"hello\"), \"H A L O\" (interpreted as the word \"halo\"), and \"Y E L O\" (interpreted as\nthe word \"yellow\") based on the language context of each word within the spoken utterance.\nFIG. 5 illustrates the relationship between acoustic modeling and language modeling.  As illustrated, each processed phoneme included in the path 502 is associated with an acoustic model score AM.sub.1 through AM.sub.7.  The language model is\nthen applied to associate each word in the path 504 with a language model score LM.sub.1 or LM.sub.2.\nAs part of the language modeling (or in other phases of the ASR processing) the speech recognition engine 258 may, to save computational resources, prune and discard low recognition score states or paths that have little likelihood of\ncorresponding to the spoken utterance, either due to low recognition score pursuant to the language model, or for other reasons.  Such pruned paths are considered inactive.  Further, during the ASR processing the speech recognition engine 258 may\niteratively perform additional processing passes on previously processed utterance portions.  Later passes may incorporate results of earlier passes to refine and improve results.  Paths which are being currently processed and considered as a potential\noutput of the system are considered active hypotheses.\nThe speech recognition engine 258 may combine potential paths into a lattice representing speech recognition results.  A sample lattice is shown in FIG. 6.  The lattice 602 shows multiple potential paths of speech recognition results.  Paths\nbetween large nodes represent potential words (for example \"hello\", \"yellow\", etc.) and paths between smaller nodes represent potential phonemes (for example \"H\", \"E\", \"L\", \"O\" and \"Y\", \"E\", \"L\", \"O\").  For purposes of illustration, individual phonemes\nare only shown for the first two words of the lattice.  The two paths between node 604 and node 606 represent two potential word choices, \"hello how\" or \"yellow now\".  Each path point between nodes (such as a potential word) is associated with a\nrecognition score.  Each path across the lattice may also be assigned a recognition score.  The highest recognition score path, where the recognition score is a combination of the acoustic model score, the language model score, and/or other factors, may\nbe returned by the speech recognition engine 258 as the ASR result for the associated audio feature vectors.\nDifferent arcs may also be associated with the same time data.  For example, the arc \"hello\" may be associated with time data for a first group of audio frames that were processed by the speech recognition engine 258 to traverse the arc\ncorresponding to \"hello.\" The same time data may also be associated with the arc \"yellow\" as the same first group of audio frames were processed to traverse the arc \"yellow\" as the arc corresponding to \"hello,\" that is the speech recognition engine 258\ndetermined two potential words that may correspond to that first group of audio frames.  The time data may be associated with those arcs for purposes of a matrix/vector representation of the lattice 602.\nIllustrated in FIG. 7 are different potential paths along the lattice 602.  As shown in FIG. 7, path 702 results in \"hello how are,\" path 704 results in \"yellow now are\" and path 706 results in \"yellow wow our.\" As can be seen, many such paths\nare possible even in the small example lattice 602.  An example of such paths 802-810 are shown in FIG. 8.  It is not uncommon for a speech recognition process to consider many thousands of different paths, i.e., hypotheses, when performing speech\nprocessing.  Each hypotheses may be associated with a score, ranking it relative to other hypotheses.  The score may be based on the input audio data, the processing from the acoustic model, the training of the language model, etc. As noted above, an ASR\ncomponent may output an N-best list, such as the list of potential results shown in FIG. 8, may output a single top scoring answer (e.g., 802) or may output an entire lattice.\nEncoding is a general technique for projecting a sequence of features into a vector space.  One goal of encoding is to project data points into a multi-dimensional vector space so that various operations can be performed on the vector\ncombinations to determine how they (or the data they contain) related to each other.  For example, if usage of two sentences such as \"What's the weather today?\" and \"Is it going to rain today?\" are projected into a vector space (where each vector is\npopulated with data points representing how or when the sentences are used), the two sentences would likely end up being close to each other in the vector projection space, thus representing the similar usage of the two sentences.  It can be valuable to\nencode certain features into a vector space to perform various operations.\nIn mathematical notation, given a sequence of feature data representations x.sub.1, .  . . x.sub.n, .  . . x.sub.N, with x.sub.n being a D-dimensional vector (where D represents a configurable number of specific values in each feature data\nrepresentation), an encoder E(x.sub.1, .  . . x.sub.N)=y projects the feature sequence to y, with y being a F-dimensional vector.  F is a fixed length of the vector and is configurable depending on user of the encoded vector and other system\nconfigurations.  For example, F may be between 100 and 1000 values for use in speech processing, but any size may be used.  Any particular encoder 950 will be configured to output vectors of the same size, thus ensuring a continuity of output encoded\nvector size from any particular encoder 950 (though different encoders may output vectors different fixed sizes).  The value y may be called an embedding of the sequence x.sub.1, .  . . x.sub.N.  The length of x.sub.n and y are fixed and known a-priori,\nbut the length of N of feature sequence x.sub.1, .  . . x.sub.N is not necessarily known a-priori.  The encoder E may be implemented as a recurrent neural network (RNN), for example as an long short-term memory RNN (LSTM-RNN) or as a gated recurrent unit\nRNN (GRU-RNN).  An RNN is a tool whereby a network of nodes may be represented numerically and where each node representation includes information about the preceding portions of the network.  For example, the RNN performs a linear transformation of the\nsequence of feature vectors which converts the sequence into a fixed size vector.  The resulting vector maintains features of the sequence in reduced vector space that can otherwise be arbitrarily long.  The output of the RNN after consuming the sequence\nof feature data values is the encoder output.  There are a variety of ways for the RNN encoder to consume the encoder output, including but not limited to: linear, one direction (forward or backward), bi-linear, essentially the concatenation of a forward\nand a backward embedding, or tree, based on parse-tree of the sequence, In addition, an attention model can be used, which is another RNN or DNN that learns to \"attract\" attention to certain parts of the input.  The attention model can be used in\ncombination with the above methods of consuming the input.\nFIG. 9 illustrates operation of the RNN encoder 950.  The input feature value sequence, starting with feature value x.sub.1 902, continuing through feature value x.sub.n 904 and concluding with feature value x.sub.N 906 is input into the RNN\nencoder 950.  The RNN encoder 950 may process the input feature values as noted above.  The RNN encoder 950 outputs the encoded feature vector y 910, which is a fixed length feature vector of length F. An encoder such as 950 may be used with speech\nprocessing as indicated below.\nFor ASR processing the base input is typically audio data in the form of audio feature vectors corresponding to audio frames.  As noted above, typically acoustic features (such as log-filter bank energies (LFBE) features, MFCC features, or other\nfeatures) are determined and used to create audio feature vectors for each audio frame.  It is possible to feed audio data into an RNN, using the amplitude and (phase) spectrum of a fast-Fourier transform (FFT), or other technique that projects an audio\nsignal into a sequence of data.  If alignment of the acoustic features exists, it may be added as an additional input.  The alignment information can be either provided as a one-hot vector using the Viterbi alignment, or as a probability distribution\nover all possible states using a Baum-Welch alignment.  Alignments can be provided at the level of senons, phonemes, or any other level suitable for the application.\nFor NLU processing the base input is typically text in the form of word sequences.  A word sequence is usually represented as a series of one-hot vectors (i.e., a Z-sized vector representing the Z available words in a lexicon, with one bit high\nto represent the particular word in the sequence).  The one-hot vector is often augmented with information from other models, which have been trained on large amounts of generic data, including but not limited to word embeddings that represent how\nindividual words are used in a text corpus, labels from a tagger (e.g., part-of-speech (POS) or named entity tagger), labels from a parser (e.g., semantic or dependency parser), etc.\nTo encode a word sequence using an RNN, for example, the RNN encoder is presented with each word of the sequence one by one.  The RNN processes the first word, then the second word, and so on.  The RNN has mechanism to preserve its state which\nhas all the information from all previous states.  Thus, for each word the RNN processes the word and its internal state, thus operating in a recurrent fashion.  After the last word, the final state is a representation of the entire vector corresponding\nto the word sequence.  Now the word sequence is represented as a fixed size vector (i.e., the encoder output) in a vector space and operated on accordingly.\nThe encoder RNN may be trained using known techniques, for example the stochastic gradient descent (SGD) method with the backpropagation-through-time (BTT) algorithm to propagate an error signal through the sequence thereby learning the\nparameters of the encoder network.\nA classifier is a known machine learning based tool to classify inputs into certain configured classes.  A classifier may be trained in a manner to use the RNN encoded vectors discussed above.  Thus, a classifier may be trained to classify an\ninput set of features x.sub.1, .  . . N into a fixed number of classes 1 .  . . C (where C may be two, and the classifier may be configured to simply classify an input feature vector into one category or the other).  To configure a classifier to operate\non RNN encoded data a DNN with a softmax layer and an RNN-encoder may be used.  Depending on the output size a hierarchical softmax layer can be used as known in the art.  The DNN takes the RNN-encoder output as input and produces a probability\ndistribution over all classes where the highest scoring class may be selected.  In mathematical notation, given a sequence x.sub.1, .  . . x.sub.N, and an encoder E, the classifier H may be expressed as: H.sub.E(x.sub.1, .  . . x.sub.N):=argmax\np(c|E(x.sub.1, .  . . x.sub.N)) (1) where p(c|y) is implemented as a DNN.\nEncoder RNN E and classifier H may be trained jointly using the SGD method with the cross-entropy objective function and the backpropagation-through-time (BTT) algorithm.  Instead of SGE, any other machine learning technique that applies to\nlearning neural networks can be applied to BTT.  The encoder E may be trained on sample sequences of feature data.  The classifier H may be trained on example feature vectors output together with encoder E. Known machine learning techniques may be used\nto train H and E, for example using a gradient feedback technique to update parameters/weights in H and E.\nThe above encoding technique may be used to improve a number of speech processing tasks.  In particular, it may be used to encode a reference portion of speech, that may then be used for certain downstream tasks, for example speech detection and\nspeech recognition.  Speech detection is the task of determining whether input audio includes speech or non-speech (i.e., silence, noise, etc.).  Further, speech detection may also include the task of determining whether detected speech is a \"desired\"\nspeech, as in speech from a particular person as opposed to speech from any other person (or noise).  Speech recognition is the task of determining what words are detected in input audio.  Speech recognition may also include the task of determining what\nwords in the input audio correspond to the \"desired\" speaker rather than words being spoken by undesired speakers, such as speech from other individuals in a room or otherwise detectable by a speech processing component.\nTo assist speech detection and speech recognition, reference audio data may be used to help the system determine what input speech corresponds to a desired speaker.  Such reference audio data (also referred to as anchor audio data) may\ncorrespond to speech of the desired speaker (i.e., an anchor audio segment).  By using the encoder techniques above, a system may compute an embedding of reference audio data in a fixed-size vector-space that helps a speech detector or a speech\nrecognizer to distinguish desired speech from undesired speech and/or noise.\nFor example, in a certain circumstance the system may desire (or be configured to) only detect/recognize speech from speaker S. Thus, given a reference speech sample from speaker S, the audio frames for the speech same x'.sub.1 .  . . x'.sub.m\nmay be obtained.  These frames may be referred to as the reference audio data.\nThe reference speech sample may be obtained in a number of ways.  In a first example, for a particular incoming utterance where a user speaks an interaction with the system, the utterance may begin with a wakeword.  For example, \"Alexa, play\nmusic\" may be an utterance to the system to play music where \"Alexa\" is the wakeword.  In such a situation, the system may want to isolate the speech of the speaker of the wakeword as this individual is the individual likely to be commanding the system,\nand thus becomes the desired speaker for purposes of speech detection/ASR.  Thus, for a wakeword triggered interaction the system may determine the start and end time of the wakeword and thus may isolate audio data corresponding to the speaking of the\nwakeword audio.  That isolated audio data may be used as the reference audio data.\nFor example, as illustrated in FIG. 10, input audio data 111 may be processed by a wakeword confirmation module 1020 to identify a wakeword in the input audio data 111 using wakeword/keyword detection techniques, such as those described above. \nThe wakeword confirmation module 1020 may then output a confirmation 1030 that the wakeword is detected or not detected in the input audio data 111.  If the wakeword is detected, the input audio data may be passed to a downstream component (such as an\nASR module 250) for further processing.  Also, the wakeword confirmation module 1020 may determine timestamps corresponding to the wakeword start time 1032 and wakeword end time 1034 in the input audio data 111.  The timestamps 1032 and 1034 may be\nindicators of start/end frames or audio feature vectors corresponding to the wakeword or other indicators of start/end time for the wakeword/.  Thus the timestamps 1032 and 1034 may thus be used to demarcate the wakeword portion in the input audio data\n111 as illustrated in FIG. 11.\nThe wakeword portion of the audio data may include a first portion of the audio data.  As shown in FIG. 11, the start timestamp 1032 may indicate the start of the wakeword and the end timestamp 1034 may indicate the end of the wakeword.  Thus\nthe first portion of audio data 1102 may start at the start location and end at the end location and may include a first plurality of audio feature vectors in between.  (Note that the input audio data 111 may include some audio data that occurs prior to\nthe wakeword due to buffering or other processing configurations.  Such audio data may be processed or may be ignored as part of speech processing.) Thus the audio data 111 may be divided into at least two portions, the first portion 1102 that includes\nthe wakeword and the second portion 1104 that includes further audio data.  The non wakeword portion of the input audio data 111 is sometimes referred to as the payload, which may be the focus of downstream speech processing.  The payload may include the\nsecond portion, a third portion, fourth portion, etc. The individual portions may be comprised of audio feature vectors.  The audio feature vectors making up the first portion 1102 (i.e., the feature vectors that correspond to the wakeword) may be\nselected as the reference audio data.\nIn a second example, a wakeword may not necessarily begin a particular incoming audio data signal, such as in a non-wakeword system or in an utterance that is part of an ongoing session with the system where the wakeword is not necessary.  In\nsuch a situation an ASR process may make a first pass at recognizing words in input audio.  The system may determine that some first portion of that input audio corresponds to the desired speaker such as the first word, the first two words, the first\nthree words, etc. That early portion of the input audio may be determined to be the reference audio and the frames corresponding to the early portion may be the reference audio data.  That reference audio data may then be used as detailed below for\nfurther processing, such as speech detection, a second pass at ASR, or the like.  Thus, in the illustration of FIG. 11 the first portion of audio data 1102 may still be used as the reference audio data even if it does not include the wakeword.\nIn a third example, reference audio data may be taken from a previous recording from the desired speaker, for example a recording taken during a voice training session.  For example, as shown in FIG. 12, during a configuration session a system\nserver 120 may send prompt audio data 1202 to a local device 110.  The prompt audio data may include, for example, audio data corresponding to a prompt to a user 10 to \"Please speak a sample sentence.\" The local device 110 may output audio corresponding\nto the prompt.  The user 10 may then speak a sample sentence such as \"hello, my name is Jo.\" The audio 11 corresponding to the sample sentence may be captured by the device 110 and converted into audio data 1204 which the local device 110 sends to the\nserver 120.  The audio data 1204 may then be sent to user profile storage 1802 to be stored in a user profile associated with user 10.  For further commands coming from a device associated with user 10 (such as device 110 or perhaps a phone, tablet, or\nother device associated with the user profile of user 10), the audio data 1204 may be used as the reference audio data.\nIn a fourth example, the system may make an assumption that the speaker who spoke a previous sentence to the system (for example, the previous utterance received by a particular input device) is the desired speaker.  Thus, a portion of the input\naudio data from the previous sentence may be used as the reference audio data.  For example, as illustrated in FIG. 13, at runtime a device 110 may capture audio corresponding to a first utterance, such as \"Alexa, set a timer.\" The device 110 may send\nfirst audio data 1302 corresponding to the first utterance to the server 120 for speech processing.  The server 120 may perform speech processing on the first audio data 1302 and may determine that further information is needed to execute a command.  The\nsystem may then determine and send prompt audio data 1304 to be output by device 110, such as \"for how long?\" The device 110 may then capture second audio corresponding to a second utterance, such as \"five minutes.\" The device 110 may send second audio\ndata 1306 corresponding to the second utterance to the server 120 for speech processing.  The server 120 may know that the second audio data 1306 is part of a same session or exchange as the first audio data 1302 and may use the first audio data 1302 as\nthe reference audio data.\nOnce determined, the reference audio data (including feature vectors x'.sub.1 .  . . x'.sub.m) may be encoded by an encoder to result in encoded reference audio data E(x'.sub.1 .  . . x'.sub.m).  This encoded reference audio data (which may be\nan encoded feature vector) may then be used for speech detection and/or speech recognition.  For example, as shown in FIG. 14, the audio features vectors for the reference audio data may include audio feature vector x'.sub.1 1402 through audio feature\nvector x'.sub.m 1404.  In the example of the reference audio data corresponding to the wakeword, audio feature vector x'.sub.1 1402 may correspond to the wakeword start time 1032 and audio feature vector x'.sub.m 1404 may correspond to the wakeword end\ntime 1034.  The audio feature vectors may be processed by RNN encoder 1450 to create encoded reference feature vector y.sub.reference 1410, which by virtue of the RNN encoding represents the entire reference audio data from audio feature vector x'.sub.1\n1402 to audio feature vector x'.sub.m 1404 in a single feature vector.  The RNN encoder 1450 may be configured to process a first input audio feature vector (e.g., input audio feature vector x'.sub.1 1402) first, or may be configured to process input\naudio feature vectors in a reverse order (e.g., input audio feature vector x'.sub.m 1404 first) depending on system configuration.  The RNN encoder 1450 may include a gated recurrent unit (GRU), long short term memory (LSTM) RNN or other possible model\nthat has backward looking (e.g., recurrent) properties.\nThe reference audio data may be encoded using an encoder 1450 with log-filterbank energy (LFBE) features that are normalized by applying conventional recursive log-amplitude mean subtraction (LAMS).  The encoder may be configured to capture a\nfixed length vector representation of the desired speech segment.  This vector may then be used to determine whether further audio data matches the speaker of the reference audio data.  For example, the reference feature vector 1410 may be used with the\nfeatures of incoming audio data of an utterance to make a frame-level decision on whether the frame includes desired speech as explained below.\nAs noted above, input audio data may include a sequence of audio frames where each frame is made of a sequence of features derived from an acoustic signal.  Typical features include log filterbank energies (LFBE), mel-frequency cepstral\ncoefficients (MFCCs), perceptual linear prediction (PLP), or any other meaningful features that can be derived from the audio signal including the digitalized audio signal itself.  One goal of speech detection is to label each input audio frame as (1)\ndesired speech, (2) non-desired speech or (3) non-speech.  In particular, the system may assign different probabilities for each audio frame as corresponding to one of the three above categories.  Thus, downstream processes may use the labels and/or\ndifferent probabilities to do different things with desired speech versus non-desired speech versus non-speech.\nSpeech detection in a real-time system (i.e. a system that classifies input audio frames reasonably quickly as they come in without undue latency) may be causal.  That is, the system may consider past audio frames when classifying a current\nframe but may not consider a large number of future audio besides a look-ahead window of a small fixed size.\nA frame-wise speech detector may have the form H(n; x.sub.1 .  . . x.sub.n+d) and may predicts the probability of Pr(n-th frame is \"desired speech\"|x.sub.1 .  . . x.sub.n+d).  H can be implemented in different ways, a common state-of-the-art\nchoice is to implement H as a (deep) neural network (DNN) or recurrent neural network (RNN).  H may also be implemented to use the encoded reference audio data vector as an input.  Thus, the system may use the encoder approach to project the anchor\nsegment into a fixed size vector space, which is then fed as an additional feature into the frame-wise speech detector: H(n; x.sub.1 .  . . x.sub.n+d,E(x'.sub.1 .  . . x'.sub.m)) (2) where H considers as inputs the encoded reference audio data vector,\nthe particular audio frame to be classified/labeled, a certain number of audio frames before the particular audio frame, and a certain number of audio frames after the particular audio frame.  Thus, a sliding window of audio frames may be used to provide\nsome context to the classifier H when labeling any particular audio frame.\nAs shown in FIG. 15, for a particular audio frame n, the audio feature vector x.sub.n 1502 corresponding to frame n is fed into the classifier H 1520 along with several audio feature vectors that appear in the input audio data before audio\nfeature vector x.sub.n (for example, audio feature vector x.sub.n-d 1504 through audio feature vector x.sub.n-1 (not illustrated)) and several audio feature vectors that appear in the input audio data after audio feature vector x.sub.n (for example,\naudio feature vector x.sub.n+1 (not illustrated) through feature vector x.sub.n+d 1506).  In one example the sliding window size is five frames, thus the feature vector for frame n is fed in with two audio feature vectors before audio feature vector\nx.sub.n and two audio feature vectors after audio feature vector x.sub.n.  Other window sizes may also be configured.\nThe output of the classifier H may include different scores 1530 for each desired label, for example a first score that the particular audio data frame corresponds to desired speech, a second score that the particular audio data frame\ncorresponds to undesired speech, and a third score that the particular audio data frame corresponds to non-speech.  Alternatively, the classifier H may simply a label 1540 for the particular audio frame as to which category the particular frame\ncorresponds to (e.g., desired speech) along with a particular score.  This implementation may be considered to be giving the particular audio frame a first probability of 1, a second probability of 0 and a third probability of 0.  The\nclassification/labeling process may be repeated for a plurality of input audio frames.  The label may include an indication that the particular audio frame n (and/or audio feature vector x.sub.n) corresponds to desired speech (i.e., speech from the same\nspeaker as the reference audio data), undesired speech (i.e., speech from a different speaker as the reference audio data), or non-speech.\nWhile certain system configurations may result in classifier H 1520 being trained to output a label corresponding to the particular audio feature vector x.sub.n, in other configurations the output label (and/or score) may correspond to the group\nof feature vectors input into the classifier H 1520.  Thus the system may evaluate multiple frames worth of data as a group (rather than frame by frame with the sliding window arrangement).  The output label and/or score may then be used for various\ndownstream purposes.\nE(x'.sub.1 .  . . x'.sub.m) contains information about how the desired speech \"looks\" like, and x.sub.1 .  . . x.sub.n+d contains the information how the current speech \"looks\" like.  Thus the encoded reference audio data vector provides a\nreference point for H to classify each audio frame with a probability that the audio frame corresponds to the desired speech.\nH may be implemented either as a DNN or RNN (can be an LSTM-RNN or GRU-RNN or any other RNN variant).  H and E may be trained jointly using the method of stochastic gradient descent (SGD) with the backpropagation-through-time (BTT) algorithm or\nany other suitable learning algorithm.  At training time frames containing desired speech are marked as positive examples, whereas other frames are marked as negative examples (i.e., corresponding to non-desired speech or non-speech).  Classifier H 1520\nmay be trained at the same time as RNN Encoder 1450 so that the Encoder 1450 produces reference feature vectors that are useful for the classifier 1520 and so the classifier 1520 learns how to classify inputs using vectors in the form output by RNN\nEncoder 1450.\nIn other embodiments of the disclosure, the encoded reference audio data may be used in other ways in determining speech detection.  For example, the system may estimate the mean of the reference audio data in the log-filterbank energy (LFBE)\ndomain, then subtracting it from all subsequent audio feature vectors of the same utterance in order to expose energy level differences relative to the reference audio data.  This approach may be referred to as log-amplitude mean subtraction (LAMS).  The\nnormalized features are then used for feed-forward deep neural network (DNN) based classification.  Thus, for reference audio data (which may correspond to a wakeword or may correspond to other speech from a desired user), the system may extract\nsufficient information that is specific to the desired user that can be used by subtracting the information from input audio data to sufficiently test whether the input audio data corresponds to speech from the desired user.\nOne technique for making a speech processing system robust to noisy conditions is cepstral mean subtraction.  Cepstral coefficients are created by computing the short-time Fourier transform (STFT) of the time-domain audio signal, combining the\nfilterbank energies into a mel-spaced filterbank, taking the logarithm of the coefficients, and then transforming them with a discrete cosine transform (DCT).  The present system may use log filterbank energies (LFBEs) which follow the same processing\nchain as cepstral coefficients but do not have the final DCT transformation applied to them.  The normalization technique may be LAMS.\nLAMS helps normalize the speech transfer function characteristics.  Modeling the speech signal as X.sub.t=S.sub.t*H.sub.t, where X.sub.t, S.sub.t, and H.sub.t, are the time-domain far field audio data (X.sub.t), speech signal (S.sub.t), and\ntransfer function (H.sub.t).  With a stationary transfer function, an estimate of the speech signal can be retrieved as: log(S.sub.k,n).apprxeq.log(X.sub.k,n)-log H.sub.k (3) The transfer function can be estimated in offline and online fashions.  In the\noffline method, the per-feature mean is first calculated over the desired speech segment (.SIGMA..sub.n=1.sup.N X.sub.k,n).  Then the per-feature means are subtracted from the original features.\nThe above system works well in environments where the speech and noise characteristics are relatively stationary throughout the analyzed segment.  In online system or more dynamic acoustic environments, the mean statistics are instead\ncontinually updated over time.  One popular choice is to update the time-varying mean estimation using an autoregressive/recursive update.  H.sub.k,m+1=.alpha.H.sub.k,m+(1-.alpha.)X.sub.k,m for 0&lt;.alpha..ltoreq.1 (4) .alpha.  is chosen to allow the\nestimator to capture the static or slowly-changing environmental characteristics without capturing the faster-moving speech characteristics.  A continually-updating online LAMS estimate can transform desired and interfering speech features look more\nsimilar, which is in opposition to our goal.  For example, in the case where there is an anchor word followed by interfering speech and then desired speech, the recursive LAMS causes energy peaks in interfering and desired speech to overlap.\nThe LAMS method may allow the system to keep the features in the desired range and for better distinguishing features between the desired and interfering speech.  For the mean estimator, the system may compute the average feature values over the\nreference audio data.  For the task of recognizing speech from the desired talker, this constraint is advantageous.  The reference audio data may be used as an example of the desired talker's speech, and then by subtracting the LAMS, the system may shift\nthe features corresponding to the desired speaker closer to being zero-mean.  This allows the system to train a classifier, e.g., a DNN, to better classify a desired talker's speech.  This method can be considered a feature normalization method that\ndepends on the characteristics of the utterance's anchor word.  Such a method allows the features to be normalized in a dynamic fashion for each utterance because the LAMS is always estimated for each new reference audio data.\nThus, the system may obtain reference audio data from a desired user.  The reference audio data may be audio data corresponding to a wakeword portion of a runtime utterance.  The system may then estimate a mean value of the reference audio data\nin the LFBE domain.  That mean may then be subtracted from subsequent feature audio feature vectors of the same utterance which will result in an energy level difference between the particular audio feature vector and the reference audio data.  This\ntechnique may be referred to as log-amplitude mean subtraction.  The energy level difference (which is normalized due to the subtraction) may then be fed into a feed-forward deep neural network (DNN) or other machine learning trained model for\nclassification.  The model may be configured to classify energy level differences as representing speech belonging to the desired user (who spoke the reference audio data) or as representing non-speech or speech belonging to a different person.\nThe goal of speech recognition (i.e., ASR) is to recognize spoken words corresponding to input audio data.  The statistical approach to speech recognition solves the task of finding the most likely sequence of words W given the observed features\nx.sub.1, .  . . x.sub.N: W=argmax.sub.W p(W|x.sub.1, .  . . x.sub.N) (5)\nA refined goal of ASR is to recognize a desired word sequence corresponding to input audio data.  The \"desired\" word sequence may be considered to cover any speech in input audio data or may be considered to cover speech from a desired\nparticular person, but not speech from any other person (other speakers s).  This problem may be expressed as:\n.times..times..times..times..times..times..times..function..times..times.- .times..function..times..function.  ##EQU00001## The state s.sub.n refers to a state in an HMM modeling a word or a phone or a senon or any other subword unit.  Taking\nthe phone as example, the quantity p(s.sub.n=A|x.sub.1 .  . . x.sub.n+d) is the probability estimate of phone \"A\" being spoken at position \"n\", p(s.sub.n=B|x.sub.1 .  . . x.sub.n+d) is the probability estimate of phone \"B\" being spoken at position \"n\",\netc. One or a few special \"phones\" are used to represent silence and noise.\nAs above with speech detection, the encoded reference audio data vector E(x'.sub.1 .  . . x'.sub.m) may be provided as an additional input to \"guide\" the speech recognition system towards the desired word sequence.  Thus, W=argmax.sub.W\np(W|x.sub.1, .  . . x.sub.N;E(x'.sub.1 .  . . x'.sub.m)) (7)\nOne implementation is to make the computation of the frame-wise state probability during ASR dependent on E(x'.sub.1 .  . . x'.sub.m): p(s.sub.n|x.sub.1 .  . . x.sub.n+d,E(x'.sub.1 .  . . x'.sub.m)) (8)\nHere, p may be implemented either as a DNN or RNN (can be an LSTM-RNN or GRU-RNN or any other RNN variant) and p and E are jointly trained as described above.  One difference between speech detection is that in speech recognition the decision is\nnot only made between (desired) speech and non-speech, but also between the units of speech (phones, senons, etc.).  If p and E are trained on training data for which undesired speech is mapped to an existing non-speech class, or a newly defined\nundesired-speech class, then the approach can learn both ignoring undesired speech and improving the distinction between the units of speech and between speech and noise.  If the training data does not contain any non-desired speech, then the approach is\nlikely to learn a speaker and/or acoustic condition adaptation, i.e., improve the distinction between the units of speech and between speech and noise.\nThe system may use the labels/scores from the speech labeling of FIG. 15 to perform voice activity detection (VAD).  For example, as shown in FIG. 16A, the label(s) 1540 (and/or score(s) 1530) corresponding to the audio feature vector 1502 may\nbe input to VAD module 222.  (The audio feature vector 1502 itself may also be input to the VAD module 222 depending on system configuration).  The VAD module 222 may thus consider whether the audio feature vector is labeled as desired speech or\nundesired speech in whether or not to declare that voice activity is detected, thus triggering further downstream operations of the speech processing system.  For example, if input audio corresponds to speech, but not necessarily to desired speech, the\nVAD module 222 may be configured to not declare speech detected so as not to cause the system to process undesired speech.  In this manner the VAD module 222 may be trained to declare speech detected only upon a sufficient quantity of desired speech\nbeing detected.\nThe system may also use the labels/scores determined from the speech labelling process as an input to the ASR module for purposes of ASR.  For example, as shown in FIG. 16B, a the label(s) 1540 (and/or score(s) 1530) corresponding to the audio\nfeature vector 1502 may be input to an ASR module 250 (and thus to a speech recognition engine 258).  (The audio feature vector 1502 itself may also be input to the ASR module 250 depending on system configuration).  The ASR module 250 may then consider\nthe label 1540 and/or score(s) 1530 when performing ASR.  For example, an audio feature vector that is labeled as corresponding to desired speech may be weighted more heavily (i.e., more likely to ultimately impact a top hypothesis) than an audio feature\nvector that is labeled as corresponding to undesired speech (or non-speech).  For ASR purposes, the classifier H 1520 may take the form of an acoustic model, where labels/scores 1530 (or labels 1540) may correspond to a particular speech unit.  For\nexample, an acoustic model classifier H 1520 may output a list of senons (or other acoustic unit) along with corresponding scores for each particular senon (which may also correspond to an encoder 1450 that is trained to output a reference feature vector\n1410 that may be used for such complex acoustic modeling).  The resulting output list of senons and scores may then be used by a downstream language model or other ASR component part of ASR module 250 to produce the text of the ASR output.\nFIG. 17 illustrates an example of classifying input audio data as desired speech or undesired speech using reference data that includes a wakeword.  In this example a first user speaks an utterance \"Alexa, play .  . . some music.\" However, while\nthe first user is speaking a second user walks into the room and says \"hello.\" However the second user's speaking of \"hello\" happens in between the first user saying \"play\" and \"some.\" Thus, the audio data 111 transcribed would result in text of \"Alexa\nplay hello some music.\" While performing NLU and further processing on such text may result in the desired action of the first user (particularly if the user wished the system to play Adele's \"Hello,\" the first user's default music choice may be\nsomething else, thus resulting in the second user's utterance interfering with the intended command of the first user.\nAs shown the audio data 111 includes a first portion 1102 of audio data that includes the wakeword \"Alexa\" as spoken by the first user.  The first portion may then be used to create a reference encoded feature vector corresponding to reference\naudio data 1702 that is used when classifying frames of the second portion 1104 of audio data as explained above.  Using the classification technique, the system will be able to determine that the audio data corresponding to the words \"play\" (1704) and\n\"some music\" (1708) match the voice of the speaker of \"Alexa\" and thus corresponds to desired speech, while the audio data corresponding to the word \"hello\" (1706) does not match the voice of the speaker of \"Alexa\" and thus corresponds to undesired\nspeech.  The system may thus ignore the \"hello\" and only process the text \"Alexa, play some music\" as spoken by the first user.\nWhile the system may also attempt to identify the identity of the speaker of the wakeword (or other portion of incoming audio), a technique called speaker identification, the techniques herein to label audio as desired speech/undesired\nspeech/non-speech, and to use such labels, are separate from speaker identification in that the identity of the speaker is not needed and thus the described techniques may be performed without performing speaker identification.\nFurther, while the RNN encoder 1450 and classifier 1520 are illustrated as configured to encode reference audio data for purposes of classifying incoming audio data for speech detection, and thus may be located upstream of an ASR module 250,\nthey may also be implemented as part of the ASR module 250 and as such may result in different features being included in the encoded vector for purposes of performing reference-based speech recognition.  Further, the labels 1540 determined by the\nclassifier 1520 may be used for other purposes.  An endpointing module may use the labels to declare a speech endpoint.  For example, if a consecutive number of frames/audio feature vectors are classified as undesired or non-speech, the endpointing\nmodule may declare that the end of the desired speech has been reached.\nThe server 120 may include or refer to data regarding user accounts, shown by the user profile storage 1802 illustrated in FIG. 18.  The user profile storage may be located proximate to server 120, or may otherwise be in communication with\nvarious components, for example over network 165.  The user profile storage 1802 may include a variety of information related to individual users, households, accounts, etc. that interact with the system 100.  For illustration, as shown in FIG. 18, the\nuser profile storage 1802 may include data regarding the devices associated with particular individual user accounts 1804.  In an example, the user profile storage 1802 is a cloud-based storage.  Such data may include device identifier (ID) and internet\nprotocol (IP) address information for different devices as well as names by which the devices may be referred to by a user.  Further qualifiers describing the devices may also be listed along with a description of the type of object of the device. \nFurther, the user account 1804 may include or be associated with sample user speech which may be used as reference audio data as described above in reference to FIG. 12.  Further, while the user profile storage 1802 may include stored reference audio\ndata 1204, it may also, or instead, store an encoded reference feature vector 1410 corresponding to the stored reference audio data 1204 so that at runtime the system may simply refer to the stored encoded reference feature vector 1410 rather than having\nto encode the reference audio data at runtime.\nFIG. 19 is a block diagram conceptually illustrating a local device 110 that may be used with the described system.  FIG. 20 is a block diagram conceptually illustrating example components of a remote device, such as a remote server 120 that may\nassist with ASR, NLU processing, or command processing.  Multiple such servers 120 may be included in the system, such as one server(s) 120 for training ASR models, one server(s) for performing ASR, one server(s) 120 for performing NLU, etc. In\noperation, each of these devices (or groups of devices) may include computer-readable and computer-executable instructions that reside on the respective device (110/120), as will be discussed further below.\nEach of these devices (110/120) may include one or more controllers/processors (1904/2004), that may each include a central processing unit (CPU) for processing data and computer-readable instructions, and a memory (1906/2006) for storing data\nand instructions of the respective device.  The memories (1906/2006) may individually include volatile random access memory (RAM), non-volatile read only memory (ROM), non-volatile magnetoresistive (MRAM) and/or other types of memory.  Each device may\nalso include a data storage component (1908/2008), for storing data and controller/processor-executable instructions.  Each data storage component may individually include one or more non-volatile storage types such as magnetic storage, optical storage,\nsolid-state storage, etc. Each device may also be connected to removable or external non-volatile memory and/or storage (such as a removable memory card, memory key drive, networked storage, etc.) through respective input/output device interfaces\n(1902/2002).\nComputer instructions for operating each device (110/120) and its various components may be executed by the respective device's controller(s)/processor(s) (1904/2004), using the memory (1906/2006) as temporary \"working\" storage at runtime.  A\ndevice's computer instructions may be stored in a non-transitory manner in non-volatile memory (1906/2006), storage (1908/2008), or an external device(s).  Alternatively, some or all of the executable instructions may be embedded in hardware or firmware\non the respective device in addition to or instead of software.\nEach device (110/120) includes input/output device interfaces (1902/2002).  A variety of components may be connected through the input/output device interfaces, as will be discussed further below.  Additionally, each device (110/120) may include\nan address/data bus (1924/2024) for conveying data among components of the respective device.  Each component within a device (110/120) may also be directly connected to other components in addition to (or instead of) being connected to other components\nacross the bus (1924/2024).\nReferring to the device 110 of FIG. 19, the device 110 may include a display 1918, which may comprise a touch interface 1919.  Or the device 110 may be \"headless\" and may primarily rely on spoken commands for input.  As a way of indicating to a\nuser that a connection between another device has been opened, the device 110 may be configured with a visual indicator, such as an LED or similar component (not illustrated), that may change color, flash, or otherwise provide visual indications by the\ndevice 110.  The device 110 may also include input/output device interfaces 1902 that connect to a variety of components such as an audio output component such as a speaker 1960, a wired headset or a wireless headset (not illustrated) or other component\ncapable of outputting audio.  The device 110 may also include an audio capture component.  The audio capture component may be, for example, a microphone 1950 or array of microphones, a wired headset or a wireless headset (not illustrated), etc. The\nmicrophone 1950 may be configured to capture audio.  If an array of microphones is included, approximate distance to a sound's point of origin may be performed acoustic localization based on time and amplitude differences between sounds captured by\ndifferent microphones of the array.  The device 110 (using microphone 1950, wakeword detection module 220, ASR module 250, etc.) may be configured to determine audio data corresponding to detected audio data.  The device 110 (using input/output device\ninterfaces 1902, antenna 1914, etc.) may also be configured to transmit the audio data to server 120 for further processing or to process the data using internal components such as a wakeword detection module 220.\nFor example, via the antenna(s), the input/output device interfaces 1902 may connect to one or more networks 199 via a wireless local area network (WLAN) (such as WiFi) radio, Bluetooth, and/or wireless network radio, such as a radio capable of\ncommunication with a wireless communication network such as a Long Term Evolution (LTE) network, WiMAX network, 3G network, etc. A wired connection such as Ethernet may also be supported.  Through the network(s) 199, the speech processing system may be\ndistributed across a networked environment.\nThe device 110 and/or server 120 may include an ASR module 250.  The ASR module in device 110 may be of limited or extended capabilities.  The ASR module 250 may include the language models 254 stored in ASR model storage component 252, and an\nASR module 250 that performs the automatic speech recognition process.  If limited speech recognition is included, the ASR module 250 may be configured to identify a limited number of words, such as keywords detected by the device, whereas extended\nspeech recognition may be configured to recognize a much larger range of words.  The ASR module 250 (or another component) may also be configured to check the ASR confidence using the techniques described above.\nThe device 110 and/or server 120 may include a limited or extended NLU module 260.  The NLU module in device 110 may be of limited or extended capabilities.  The NLU module 260 may comprising the name entity recognition module 262, the intent\nclassification module 264 and/or other components.  The NLU module 260 may also include a stored knowledge exchange and/or entity library, or those storages may be separately located.\nThe device 110 and/or server 120 may also include a command processor 290 that is configured to execute commands/functions associated with a spoken command as described above.\nThe device 110 may include a voice activity detection (VAD) module 222 that performs voice activity detection as described above.  The VAD module 222 may incorporate techniques described above, including considering audio feature vectors and\ncorresponding labels as discussed in reference to FIG. 16A.\nThe device 110 may include a wakeword detection module 220, which may be a separate component or may be included in an ASR module 250.  The wakeword detection module 220 receives audio signals and detects occurrences of a particular expression\n(such as a configured keyword) in the audio.  This may include detecting a change in frequencies over a specific period of time where the change in frequencies results in a specific audio signature that the system recognizes as corresponding to the\nkeyword.  Keyword detection may include analyzing individual directional audio signals, such as those processed post-beamforming if applicable.  Other techniques known in the art of keyword detection (also known as keyword spotting) may also be used.  In\nsome embodiments, the device 110 may be configured collectively to identify a set of the directional audio signals in which the wake expression is detected or in which the wake expression is likely to have occurred.\nThe wakeword detection module 220 receives captured audio and processes the audio (for example, using model(s) 232) to determine whether the audio corresponds to particular keywords recognizable by the device 110 and/or system 100.  The storage\n1908 may store data relating to keywords and functions to enable the wakeword detection module 220 to perform the algorithms and methods described above.  The locally stored speech models may be preconfigured based on known information, prior to the\ndevice 110 being configured to access the network by the user.  For example, the models may be language and/or accent specific to a region where the user device is shipped or predicted to be located, or to the user himself/herself, based on a user\nprofile, etc. In an aspect, the models may be pre-trained using speech or audio data of the user from another device.  For example, the user may own another user device that the user operates via spoken commands, and this speech data may be associated\nwith a user profile.  The speech data from the other user device may then be leveraged and used to train the locally stored speech models of the device 110 prior to the user device 110 being delivered to the user or configured to access the network by\nthe user.  The wakeword detection module 220 may access the storage 1108 and compare the captured audio to the stored models and audio sequences using audio comparison, pattern recognition, keyword spotting, audio signature, and/or other audio processing\ntechniques.\nThe server may also include an RNN encoder 950 for encoding data into a vector form as described above.  The server may also include a model training component 2070 for training or retraining various model or classifiers discussed above. \nVarious machine learning techniques may be used to perform various steps described above, such as training/retraining an RC, entity tagger, semantic parser, etc. Models may be trained and operated according to various machine learning techniques.  Such\ntechniques may include, for example, neural networks (such as deep neural networks and/or recurrent neural networks), inference engines, trained classifiers, etc. Examples of trained classifiers include Support Vector Machines (SVMs), neural networks,\ndecision trees, AdaBoost (short for \"Adaptive Boosting\") combined with decision trees, and random forests.  Focusing on SVM as an example, SVM is a supervised learning model with associated learning algorithms that analyze data and recognize patterns in\nthe data, and which are commonly used for classification and regression analysis.  Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into one category\nor the other, making it a non-probabilistic binary linear classifier.  More complex SVM models may be built with the training set identifying more than two categories, with the SVM determining which category is most similar to input data.  An SVM model\nmay be mapped so that the examples of the separate categories are divided by clear gaps.  New examples are then mapped into that same space and predicted to belong to a category based on which side of the gaps they fall on.  Classifiers may issue a\n\"score\" indicating which category the data most closely matches.  The score may provide an indication of how closely the data matches the category.\nIn order to apply the machine learning techniques, the machine learning processes themselves need to be trained.  Training a machine learning component such as, in this case, one of the first or second models, requires establishing a \"ground\ntruth\" for the training examples.  In machine learning, the term \"ground truth\" refers to the accuracy of a training set's classification for supervised learning techniques.  Various techniques may be used to train the models including backpropagation,\nstatistical learning, supervised learning, semi-supervised learning, stochastic learning, or other known techniques.\nThe server 120 may also include a wakeword confirmation module 1020 that can operate as described above in reference to FIG. 10.  The server 120 may also include a speech labeling module 2050 that can label incoming audio data using classifier\n1520 as described above in reference to FIG. 15.  The speech labeling module 2050 may also include RNN encoder 1450 to create encoded reference feature vectors 1410.\nAs noted above, multiple devices may be employed in a single speech processing system.  In such a multi-device system, each of the devices may include different components for performing different aspects of the speech processing.  The multiple\ndevices may include overlapping components.  The components of the devices 110 and server 120, as illustrated in FIGS. 19 and 20, are exemplary, and may be located a stand-alone device or may be included, in whole or in part, as a component of a larger\ndevice or system.\nAs illustrated in FIG. 21 multiple devices (120, 120x, 110a to 110f) may contain components of the system 100 and the devices may be connected over a network 199.  Network 199 may include a local or private network or may include a wide network\nsuch as the internet.  Devices may be connected to the network 199 through either wired or wireless connections.  For example, a speech controlled device 110a, a tablet computer 110b, a smart phone 110c, a refrigerator 110d, a smart watch 110e, and/or a\nvehicle 110f may be connected to the network 199 through a wireless service provider, over a WiFi or cellular network connection or the like.  Other devices are included as network-connected support devices, such as a server 120, application developer\ndevices 120x, or others.  The support devices may connect to the network 199 through a wired connection or wireless connection.  Networked devices 110 may capture audio using one-or-more built-in or connected microphones 1950 or audio capture devices,\nwith processing performed by ASR, NLU, or other components of the same device or another device connected via network 199, such as an ASR 250, NLU 260, etc. of one or more servers 120.\nThe concepts disclosed herein may be applied within a number of different devices and computer systems, including, for example, general-purpose computing systems, speech processing systems, and distributed computing environments.\nThe above aspects of the present disclosure are meant to be illustrative.  They were chosen to explain the principles and application of the disclosure and are not intended to be exhaustive or to limit the disclosure.  Many modifications and\nvariations of the disclosed aspects may be apparent to those of skill in the art.  Persons having ordinary skill in the field of computers and speech processing should recognize that components and process steps described herein may be interchangeable\nwith other components or steps, or combinations of components or steps, and still achieve the benefits and advantages of the present disclosure.  Moreover, it should be apparent to one skilled in the art, that the disclosure may be practiced without some\nor all of the specific details and steps disclosed herein.\nAspects of the disclosed system may be implemented as a computer method or as an article of manufacture such as a memory device or non-transitory computer readable storage medium.  The computer readable storage medium may be readable by a\ncomputer and may comprise instructions for causing a computer or other device to perform processes described in the present disclosure.  The computer readable storage media may be implemented by a volatile computer memory, non-volatile computer memory,\nhard drive, solid-state memory, flash drive, removable disk and/or other media.  In addition, components of one or more of the modules and engines may be implemented as in firmware or hardware, such as the acoustic front end 256, which comprise among\nother things, analog and/or digital filters (e.g., filters configured as firmware to a digital signal processor (DSP)).\nAs used in this disclosure, the term \"a\" or \"one\" may include one or more items unless specifically stated otherwise.  Further, the phrase \"based on\" is intended to mean \"based at least in part on\" unless specifically stated otherwise.", "application_number": "15196228", "abstract": " A system configured to process speech commands may classify incoming\n     audio as desired speech, undesired speech, or non-speech. Desired speech\n     is speech that is from a same speaker as reference speech. The reference\n     speech may be obtained from a configuration session or from a first\n     portion of input speech that includes a wakeword. The reference speech\n     may be encoded using a recurrent neural network (RNN) encoder to create a\n     reference feature vector. The reference feature vector and incoming audio\n     data may be processed by a trained neural network classifier to label the\n     incoming audio data (for example, frame-by-frame) as to whether each\n     frame is spoken by the same speaker as the reference speech. The labels\n     may be passed to an automatic speech recognition (ASR) component which\n     may allow the ASR component to focus its processing on the desired\n     speech.\n", "citations": ["9558749", "20100223056", "20130166296", "20140274203", "20150127336", "20150294670", "20160019889", "20160180838", "20160293167", "20170069327", "20170140761"], "related": ["62311082"]}, {"id": "20170272489", "patent_code": "10375139", "patent_name": "Method for downloading and using a communication application through a web\n     browser", "year": "2019", "inventor_and_country_data": " Inventors: \nKatis; Thomas E. (Jackson, WY), Panttaja; James T. (San Francisco, CA), Panttaja; Mary G. (San Francisco, CA), Ranney; Matthew J. (Oakland, CA)  ", "description": "<BR><BR>BACKGROUND\nField of the Invention\nThis invention pertains to communications, and more particularly, to downloading and using a communication application through a web browser, the communication application enabling users to conduct voice conversations in either a synchronous\nreal-time mode, asynchronously in a time-shifted mode, and with the ability to seamlessly transition between the two modes.\nDescription of Related Art\nElectronic voice communication has historically relied on telephones and radios.  Conventional telephone calls required one party to dial another party using a telephone number and waiting for a circuit connection to be made over the Public\nSwitched Telephone Network or PSTN.  A full-duplex conversation may take place only after the connection is made.  More recently, telephony using Voice over Internet Protocol (VoIP) has become popular.  With VoIP, voice communication occurs using IP over\na packet-based network, such as the Internet.\nMany full-duplex telephony systems have some sort of message recording facility for unanswered calls such as voicemail.  If an incoming call goes unanswered, it is redirected to a voicemail system.  When the caller finishes the message, the\nrecipient is alerted and may listen to the message.  Various options exist for message delivery beyond dialing into the voicemail system, such as email or \"visual voicemail\", but these delivery schemes all require the entire message to be left by the\ncaller before the recipient can listen to the message.\nMany home telephones have answering machine systems that record missed calls.  They differ from voicemail in that the caller's voice is often played through a speaker on the answering machine while the message is being recorded.  The called\nparty can pick up the phone while the caller is leaving a message, which causes most answering machines to stop recording the message.  With other answering machines, however, the live conversation will be recorded unless the called party manually stops\nthe recording.  In either situation, there is no way for the called party to review the recorded message until after the recording has stopped.  As a result, there is no way for the recipient to review any portion of the recorded message other than the\ncurrent point while the message is ongoing and is being recorded.  Only after the message has concluded can the recipient go back and review the recorded message.\nSome more recent call management systems provide a \"virtual answering machine\", allowing callers to leave a message in a voicemail system, while giving called users the ability to hear the message as it is being left.  The actual answering\n\"machine\" is typically a voicemail-style server, operated by the telephony service provider.  Virtual answering machine systems differ from standard voice mail systems in that the called party may use either their phone or a computer to listen to\nmessages as they are being left.  Similar to an answering machine as described in the preceding paragraph, however, the called party can only listen at the current point of the message as it is being left.  There is no way to review previous portions of\nthe message before the message is left in its entirety.\nCertain mobile phone handsets have been equipped with an \"answering machine\" feature inside the handset itself that behaves similarly to a landline answering machine as described above.  With these answering machines, callers may leave a voice\nmessage, which is recorded directly on the phone of the recipient.  While the answering machine functionality has been integrated into the phone, the limitations of these answering machines, as discussed above, are still present.\nWith most current PTT systems, incoming audio is played on the device as it is received.  If the user does not hear the message, for whatever reason, the message is irretrievably lost.  Either the sender must resend the message or the recipient\nmust request the sender to retransmit the message.  PTT messaging systems are known.  With these systems, message that are not reviewed live are recorded.  The recipient can access the message from storage at a later time.  These systems, however,\ntypically do not record messages that are reviewed live by the recipient.  See for example U.S.  Pat.  No. 7,403,775, U.S.  Publications 2005/0221819 and 2005/0202807, EP 1 694 044 and WO 2005/101697.\nWith the growing popularity of the world wide web, more people are communicating through the Internet.  With most of these applications, the user is interfacing through a browser running on their computer or other communication device, such as a\nmobile or cellular phone or radio, communicating with others through the Internet and one or more communication servers.\nWith email for example, users may type and send text messages to one another through email clients, located either locally on their computer or mobile communication device (e.g., Microsoft Outlook) or remotely on a server (e.g., Yahoo or Google\nWeb-based mail).  In the remote case, the email client \"runs\" on the computer or mobile communication device through a web browser.  Although it is possible to send time-based (i.e., media that changes over time, such as voice or video) as an attachment\nto an email, the time-based media can never be sent or reviewed in a \"live\" or real-time mode.  Due to the store and forward nature of email, the time-based media must first be created, encapsulated into a file, and then attached to the email before it\ncan be sent.  On the receiving side, the email and the attachment must be received in full before it can be reviewed.  Real-time communication is therefore not possible with conventional email.\nSkype is a software application intended to run on computers that allows people to conduct voice conversations and video-conferencing communication.  Skype is a type of VoIP system, and it is possible with Skype to leave a voice mail message. \nAlso with certain ancillary products, such as Hot Recorder, it is possible for a user to record a conversation conducted using Skype.  However with either Skype voice mail or Hot Recorder, it is not possible for a user to review the previous media of the\nconversation while the conversation is ongoing or to seamlessly transition the conversation between a real-time and a time-shifted mode.\nSocial networking Web sites, such as Facebook, also allow members to communicate with one another, typically through text-based instant messaging, but video messaging is also supported.  In addition, mobile phone applications for Facebook are\navailable to Facebook users.  Neither the instant messaging, nor the mobile phone applications, however, allow users to conduct voice and other time-based media conversations in both a real-time and a time-shifted mode and to seamlessly transition the\nconversation between the two modes.\n<BR><BR>SUMMARY OF THE INVENTION\nThe invention involves a method for downloading a communication application onto a communication device.  Once downloaded, the communication application is configured to create a user interface appearing within one or more web pages generated by\na web browser running on the communication device.  The communication enables the user to engage in voice conversations in (i) a real-time mode or (ii) a time-shifted mode and provides the ability to seamless transition the conversation back and forth\nbetween the two modes (i) and (ii).  In the real-time mode, the communication application is configured to transmit voice media as the user speaks and render voice media as it is transmitted and received from a sender.  The communication application also\nprovides for the persistent storage of transmitted and received voice media.  With persistent storage, the voice media may be rendered at a later arbitrary time defined by the user in the time-shifted mode.\nThe communication application is preferably downloaded along with web content.  Accordingly, when the user interface appears within the web browser, it is typically within the context of a web site, such as an on-line social networking, gaming,\ndating, financial or stock trading, or any other on-line community.  The user of the communication device can then conduct conversations with other members of the web community through the user interface within the web site appearing within the browser.\nIn another embodiment, both the communication device and communication servers responsible for routing the voice media of the conversation between participants are \"late-binding\".  With late-binding, voice media is progressively transmitted as\nit is created and as soon as a recipient is identified, without having to first wait for a complete discovery path to the recipient to be discovered.  Similarly, the communication servers can progressively transmit received voice media as it is\navailable, before the voice media is received in full, as soon as the next hop is discovered, and before the complete delivery route to the recipient is fully known.  Late binding thus solves the problems with current communication systems, including the\n(i) waiting for a circuit connection to be established before \"live\" communication may take place, with either the recipient or a voice mail system associated with the recipient, as required with conventional telephony or (ii) waiting for an email to be\ncomposed in its entirety before the email may be sent.\nIn yet another embodiment, a number of addressing techniques may be used, including unique identifiers that identify a user within a web community, or globally unique identifiers, such as telephone numbers or email addresses.  The unique\nidentifier, regardless if global or not, may be used for both authentication and routing.  Anyone of a number of real-time transmission protocols, such as SIP, RTP, VoIP, Skype, UDP, TCP or CTP, may be used for the actual transmission of the voice media.\nIn yet another embodiment, email addresses, the existing email infrastructure and DNS may be used for addressing and route discovery.  In addition with this embodiment, existing email protocols may be modified so that voice media of\nconversations may be transmitted as it is created and rendered as it is received.  This embodiment, sometimes referred to as \"progressive emails\", differs significantly from conventional emails, which are store and forward only and are unable to support\nthe transmission of \"live\" voice media in real-time. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe invention may best be understood by reference to the following description taken in conjunction with the accompanying drawings, which illustrate specific embodiments of the invention.\nFIG. 1 is diagram of a non-exclusive embodiment of a communication system embodying the principles of the present invention.\nFIG. 2 is a diagram of a non-exclusive embodiment of a communication application embodying the principles of the present invention.\nFIG. 3A is a block diagram of an exemplary communication device.\nFIG. 3B is a block diagram illustrating the communication application of FIG. 2 running on a client communication device.\nFIG. 3C is a diagram illustrating a non-exclusive embodiment of a sequence for implementing the principles of the present invention.\nFIG. 4 is a diagram of an exemplary graphical user interface for managing and engaging in conversations on a client communication device according to the principles of the present invention.\nFIGS. 5A through 5D are diagrams illustrating a non-exclusive examples of web browsers incorporating a user interface of the communication application within the context of various web pages according to the principles of the present invention.\nFIGS. 6A and 6B are diagrams of an exemplary user interface displayed on a mobile client communication device within the context of web pages according to the principles of the present invention.\nIt should be noted that like reference numbers refer to like elements in the figures.\nThe above-listed figures are illustrative and are provided as merely examples of embodiments for implementing the various principles and features of the present invention.  It should be understood that the features and principles of the present\ninvention may be implemented in a variety of other embodiments and the specific embodiments as illustrated in the Figures should in no way be construed as limiting the scope of the invention.\n<BR><BR>DETAILED DESCRIPTION OF SPECIFIC EMBODIMENTS\nThe invention will now be described in detail with reference to various embodiments thereof as illustrated in the accompanying drawings.  In the following description, specific details are set forth in order to provide a thorough understanding\nof the invention.  It will be apparent, however, to one skilled in the art, that the invention may be practiced without using some of the implementation details set forth herein.  It should also be understood that well known operations have not been\ndescribed in detail in order to not unnecessarily obscure the invention.\n<BR><BR>Messages and Conversations\n\"Media\" as used herein is intended to broadly mean virtually any type of media, such as but not limited to, voice, video, text, still pictures, sensor data, GPS data, or just about any other type of media, data or information.  Time-based media\nis intended to mean any type of media that changes over time, such as voice or video.  By way of comparison, media such as text or a photo, is not time-based since this type of media does not change over time.\nAs used herein, the term \"conversation\" is also broadly construed.  In one embodiment, a conversation is intended to mean a thread of messages, strung together by some common attribute, such as a subject matter or topic, by name, by\nparticipants, by a user group, or some other defined criteria.  In another embodiment, the messages of a conversation do not necessarily have to be tied together by some common attribute.  Rather one or more messages may be arbitrarily assembled into a\nconversation.  Thus a conversation is intended to mean two or more messages, regardless if they are tied together by a common attribute or not.\n<BR><BR>The Communication System\nReferring to FIG. 1, an exemplary communication system including one or more communication servers 10 and a plurality of client communication devices 12 is shown.  A communication services network 14 is used to interconnect the individual client\ncommunication devices 12 through the servers 10.\nThe server(s) 10 run an application responsible for routing the metadata used to set up and support conversations as well as the actual media of messages of the conversations between the different client communication devices 12.  In one\nspecific embodiment, the application is the server application described in commonly assigned co-pending US application Ser.  No. 12/028,400 (U.S Patent Publication No. 2009/0003558), Ser.  No. 12/192,890 (U.S Patent Publication No. 2009/0103521), and\nSer.  No. 12/253,833 (U.S Patent Publication No. 2009/0168760), each incorporated by reference herein for all purposes.\nOne or more of the server(s) 10 may also be configured as a web server.  Alternatively, one or more separate web servers may be provided or accessible over the network 14.  The web servers are responsible for serving web content to the client\ncommunication devices 12.\nThe client communication devices 12 may be a wide variety of different types of communication devices, such as desktop computers, mobile or laptop computers, tablet-PCs, notebooks, e-readers, WiFi devices such as the iPod by Apple, mobile or\ncellular phones, Push To Talk (PTT) devices, PTT over Cellular (PoC) devices, radios, satellite phones or radios, VoIP phones, or conventional telephones designed for use over the Public Switched Telephone Network (PSTN).  The above list should be\nconstrued as exemplary and should not be considered as exhaustive or limiting.  Any type of communication device may be used.\nThe network 14 may in various embodiments be the Internet, PSTN, a circuit-based network, a mobile communication network, a cellular network based on CDMA or GSM for example, a wired network, a wireless network, a tactical radio network, a\nsatellite communication network, any other type of communication network, or any combination thereof.  The network 14 may also be either heterogeneous or homogeneous network.\n<BR><BR>The Communication Application\nThe server(s) 10 are also responsible for downloading a communication application to the client communication devices 12.  The downloaded communication application is very similar to the above-mentioned application running on the servers 10, but\ndiffers in several regards.  First, the downloaded communication application is written in a programming language so that it will run within the context of the web page appearing within the browser of the communication device.  Second, the communication\napplication is configured to create a user interface that appears within the web page appearing within by a web browser running on the client communication device 12.  Third, the downloaded communication application is configured to cooperate with a\nmulti-media platform, such as Flash by Abode Systems, to support various input and output functions on the client communication device 12, such as a microphone, speaker, display, touch-screen display, camera, video camera, keyboard, etc. Accordingly when\nthe application is downloaded, the user has the experience that the user interface is an integral part of a web page running within a browser on the client communication device 12.\nReferring to FIG. 2, a block diagram of a communication application 20 is illustrated.  The communication application 20 includes a Multiple Conversation Management System (MCMS) module 22, a Store and Stream module 24, and an interface 26\nprovided between the two modules.  The key features and elements of the communication application 20 are briefly described below.  For a more detailed explanation, see U.S.  application Ser.  Nos.  12/028,400, 12/253,833, 12/192,890, and 12/253,820 (U.S\nPatent Publication Nos.  2009/0003558, 2009/0168760, 2009/0103521, and 2009/0168759), all incorporated by reference herein.\nThe MCMS module 22 includes a number of modules and services for creating, managing, and conducting multiple conversations.  The MCMS module 22 includes a user interface module 22A for supporting the audio and video functions on the client\ncommunication device 12, rendering/encoding module 22B for performing rendering and encoding tasks, a contacts service module 22C for managing and maintaining information needed for creating and maintaining contact lists (e.g., telephone numbers, email\naddresses or other unique identifiers), a presence status service module 22D for sharing the online status of the user of the client communication device 12 and which indicates the online status of the other users and the MCMS data base 22E, which stores\nand manages the metadata for conversations conducted using the client communication device 12.\nThe Store and Stream module 24 includes a Persistent Infinite Memory Buffer or PIMB 28 for storing in a time-indexed format the time-based media of received and sent messages, The store and stream module 24 also includes four modules for encode\nreceive 26A, transmit 26C, net receive 26B and render 26D.  The function of each module is described below.\nThe encode receive module 26A performs the function of progressively encoding and persistently storing in the PIMB 28 in a time-indexed format the media created using the client communication device 12 as the media is created.\nThe transmit module 26C progressively transmits the media created using the client communication device 12 to other recipients over the network 14 as the media is created and progressively stored in the PIMB 28.\nThe encode receive module 26A and the transmit module 26C perform their respective functions at approximately the same time.  For example, as a person speaks into their client communication device 12 during a conversation, the voice media is\nsimultaneously and progressively encoded, persistently stored and transmitted as the voice media is created.\nThe net receive module 26B is responsible for progressively storing media received from others in the PIMB 28 in a time-indexed format as the media is received.\nThe render module 24D enables the rendering of persistently stored media either synchronously in the near real-time mode or asynchronously in the time-shifted mode by retrieving media stored in the PIMB 28.  In the real-time mode, the render\nmodule 24D renders media simultaneously as it received and persistently stored by the net received module 26B.  In the time-shifted mode, the render module 24D renders media previously stored in the PIMB at an arbitrary time after the media was stored. \nThe rendered media could be either received media, transmitted media, or both received and transmitted media.  Synchronous and asynchronous communication should be broadly construed herein and generally mean the sender and receiver are concurrently or\nnot concurrently engaged in communication respectively.\nThe version of the application running on the server(s) 10 will typically not include the encode receive module 24A and render module 24D since encoding and rendering functions are typically not performed on the server(s) 10.\nThe PIMB 28 located on the communication application 20 may not be physically large enough to indefinitely store all of the media transmitted and received by a user.  The PIMB 28 is therefore configured like a cache, and stores only the most\nrelevant media, while the PIMB located on a server 10 acts as backup or main storage.  As physical space in the memory used for the PIMB 28 runs out, certain media on the client 12 may be replaced using any well-known algorithm, such as least recently\nused or first-in, first-out.  In the event the user wishes to review replaced media, then the media is retrieved from the server 10 and locally stored in the PIMB 28.  Thereafter, the media may be rendered out of the PIMB 28.  The retrieval time is\nideally minimal so as to be transparent to the user.\n<BR><BR>Client Communication Devices\nReferring to FIG. 3A, a block diagram of a client communication device 12 according to a non-exclusive embodiment of the invention is shown.  The client communication device 12 includes a network connection 30 for connecting the client\ncommunication device 12 to the network 14, a number of input/output devices 31 including a speaker 31A for rendering voice and other audio based media, a mouse 31B for cursor control and data entry, a microphone 31C for voice and other audio based media\nentry, a keyboard or keypad 31D for text and data entry, a display 31E for rendering image or video based media, and a camera 31F for capturing either still photos or video.  It should be noted that elements 31A through 31F are each optional and are not\nnecessarily included on all implementations of a client communication device 12.  In addition, the display 31E may be a touch-sensitive display capable of receiving inputs using a pointing element, such as a pen, stylus or finger.  In yet other\nembodiments, client communication devices 12 may optionally further include other media generating devices (not illustrated), such as sensor data (e.g., temperature, pressure), GPS data, etc.\nThe client communication device 12 also includes a web browser 32 configured to generate and display HTML/Web content 33 on the display 31E.  An optional multi-media platform 34, such as the Adobe Flash player, provides audio, video, animation,\nand other interactivity features within the Web browser 33.  In various embodiments, the multi-media platform 34 may be a plug-in application or may already reside on the device 12.\nThe web browser 32 may be any well-known software application for retrieving, presenting, and traversing information resources on the Web.  In various embodiments, well known browsers such as Internet Explorer by Microsoft, Firefox by the\nMozilla Foundation, Safari by Apple, Chrome by Google, Opera by Opera Software for desktop, mobile, embedded or gaming systems, or any other browser may be used.  Although the browser 32 is primarily intended to access the world-wide-web, in alternative\nembodiments, the browser 32 can also be used to access information provided by servers in private networks or content in file systems.\nThe input/output devices 31A through 31F, the browser 32 and multi-media platform 34 are all intended to run on an underlying hardware platform 35.  In various embodiments, the hardware platform may be any microprocessor or microcontroller\nplatform, such as but not limited to those offered by Intel Corporation or ARM Holdings, Cambridge, United Kingdom, or equivalents thereof.\nReferring to FIG. 3B, the same client communication device 12 after the communication application 20 has been downloaded is illustrated.  After the download, the client communication device 12 includes a web browser plug-in application 36 with a\nbrowser interface layer 37.  The multi-media platform 34 communicates with an underlying communication application 20 using remote Application Programming Interfaces or APIs, as is well known in the art.  The web browser plug-in application 36 takes\nadvantage of the multi-media platform 34 and the functionality and services offered by the browser 32.  The browser interface layer 37 acts as an interface between the web browser 32 and the communication application 20.  The browser interface layer 37\nis responsible for (i) invoking the various user interface functions implemented by the communication application 20 and presenting the appropriate user interface within the content presented through browser 32 to the user of client communication device\n12 and (ii) receiving inputs from the user through the browser 32 and other inputs on the client communication device 12, such as microphone 31C, mouse 31B, keyboard 31D, or touch display 31E and providing these inputs to the communication application\n20.  As a result, the user of the client communication device 12 may control the operation of the communication application 20 when setting up, participating in, or terminating conversations through the web browser 32 and the other input/output devices\noptionally provided on the client communication device 12.\nIt should be noted that the emerging next generation HTTP5 standard, as currently proposed, supports some of the multimedia functions performed by the multi-media platform 34, web-browser plug-in 36, and/or browser interface layer 37.  To the\nextent the functionality performed by 34, 36 and 37 is supported by the native HTTP in the future, it may be possible to eliminate the need of some or all of these elements on the client communication devices 12 respectively.  Consequently, FIG. 3B\nshould not be construed as limiting in any regard.  Rather it should be anticipated that the elements 34, 36 and 37 be fully or partially removed from the device 12 as their functionality is replaced by native HTTP in the future.\nReferring to FIG. 3C, a diagram 100 illustrating a non-exclusive embodiment of a sequence for implementing the principles of the present invention is shown.  In the initial step 102, a web server is maintained on a network.  As noted above, one\nor more of the servers 10 may be configured as a web server or one or more separate web servers on may be accessed.  In the next step 104, a user of a communication device 12 accesses one of the web servers over the network 14 and requests, as needed,\nthe multi-media platform 34, the communication application 20, the browser plug-in application 36, and browser interface layer 37.  In reply, these software plug-in modules are downloaded, as needed, in step 106 to the client device 12 of the user.  In\nstep 108, web content is served to the client communication device 12.  The downloaded communication application 20 and multi-media platform 34 cooperate along with the served content to create a user interface within the web pages appearing within the\nbrowser 32.  In step 112, the user participates in one or more conversations through the user interface.  The server(s) 10 route the transmitted and received media among the participants of the conversation in step 114.\nThe communication application 20 enables the user of the client communication device 12 to set up and engage in conversations with other client communication devices 12 (i) synchronously in the real-time mode, (ii) asynchronously in the\ntime-shifted mode and to (iii) seamlessly transition the conversation between the two modes (i) and (ii).  The conversations may also include multiple types of media besides voice, including text, video, sensor data, etc. The user participates in the\nconversations through the user interface appearing within the browser 32, the details of which are described in more detail below.\n<BR><BR>The User Interface\nFIG. 4 is a diagram of an exemplary user interface 40, rendered by the browser 32 on the display 31E of a client communication device 12.  The interface 40 enables or facilitates the participation of the user in one or more conversations on the\nclient device 12 using the communication application 20.\nThe interface 40 includes a folders window 42, an active conversation list window 44, a window 46 for displaying the history of a conversation selected from the list displayed in window 44, a media controller window 48, and a window 49\ndisplaying the current time and date.  Although not illustrated, the interface also includes one or more icons for creating a new conversations and defining the participant(s) of the new conversation.\nThe folders window 42 includes a plurality of optional folders, such an inbox for storing incoming messages, a contact list, a favorites contact list, a conversation list, conversation groups, and an outbox listing outgoing messages.  It should\nbe understood that the list provided above is merely exemplary.  Individual folders containing a wide variety of lists and other information may be contained within the folders window 42.\nWindow 44 displays the active conversations the user of client communication device 12 is currently engaged in. In the example illustrated, the user is currently engaged in three conversations.  In the first conversation, a participant named\nJane Doe previously left a text message, as designated by the envelope icon, at 3:32 PM on Mar.  28, 2009.  In another conversation, a participant named Sam Fairbanks is currently leaving an audio message, as indicated by the voice media bubble icon. \nThe third conversation is entitled \"Group 1.\" In this conversation, the conversation is \"live\" and a participant named Hank Jones is speaking.  The user of the client communication device 12 may select any of the active conversations appearing in the\nwindow 44 for participation.\nFurther in this example, the user of client communication device 12 has selected the Group 1 conversation for participation.  As a result, a visual indicator, such as the shading of the Group 1 conversation in the window 44 different from the\nother listed conversations, informs the user that he or she is actively engaged in the Group 1 conversation.  Had the conversation with Sam Fairbanks been selected, then this conversation would have been highlighted in the window 44.  It should be noted\nthat the shading of the selected conversation in the window 44 is just one possible indicator.  In various other embodiments, any indicator, either visual, audio, a combination thereof, or no indication may be used.\nWithin the selected conversation, a \"MUTE\" icon and an \"END\" icon are optionally provided.  The mute icon allows the user to disable the microphone 24 of client communication device 12.  When the end icon is selected, the user's active\nparticipation in the Group 1 conversation is terminated.  At this point, any other conversation in the list provided in window 44 may be selected.  In this manner, the user may transition from conversation to conversation within the active conversation\nlist.  The user may return to the Group 1 conversation at anytime.\nThe conversation window 46 shows the history of the currently selected conversation, which in this example again, is the Group 1 conversation.  In this example, a sequence of media bubbles each represent the media contributions to the\nconversation respectively.  Each media bubble represents the media contribution of a participant to the conversation in time-sequence order.  In this example, Tom Smith left an audio message that is 30 seconds long at 5:02 PM on Mar.  27, 2009.  Matt\nJones left an audio message 1 minute and 45 seconds in duration at 9:32 AM on Mar.  28, 2009.  Tom Smith left a text message, which appears in the media bubble, at 12:00 PM on Mar.  29, 2009.  By scrolling up or down through the media bubbles appearing\nin window 46, the entire history of the Group 1 conversation may be viewed.\nThe window 46 further includes a number of icons allowing the user to control his or her participation in the selected Group 1 conversation.  A \"PLAY\" icon allows the user to render the media of a selected media bubble appearing in the window\n46.  For example, if the Tom Smith media bubble is selected, then the corresponding voice message is accessed and rendered through the speaker 31A on the client communication device 12.  With media bubbles containing a text message, the text is typically\ndisplayed within the bubble.  In either case, when an old message bubble is selected, the media of the conversation is being reviewed in the time-shifted mode.\nThe \"TEXT\" and the \"TALK\" icons enable the user of the client communication device 12 to participate in the conversation by either typing or speaking a message respectively.  The \"END\" icon removes the user from participation in the\nconversation.\nWhen another conversation is selected from the active list appearing in window 44, the history of the newly selected conversation appears in the conversation history window 46.  Thus by selecting different conversations from the list in window\n44, the user may switch participation among multiple conversations.\nThe media controller window 48 enables the user of the client communication device 12 to control the rendering of voice and other media of the selected conversation.  The media controller window operates in two modes, the synchronous real-time\nmode and the asynchronous time shifted mode, and enables the seamless transition between the two modes.\nIn the time-shifted mode, the media of a selected message is identified within the window 48.  For example (not illustrated), if the previous voice message from Tom Smith sent at 5:02 PM on Mar.  27, 2009, is selected, information identifying\nthis message is displayed in the window 48.  The scrubber bar 52 allows the user to quickly traverse a message from start to finish and select a point to start the rendering of the media of the message.  As the position of the scrubber bar 52 is\nadjusted, the timer 54 is updated to reflect the time-position relative to the start time of the message.\nThe pause icon 57 allows the user to pause the rendering of the media of the message.  The jump backward icon 56 allows the user to jump back to a previous point in time of the message and begin the rendering of the message from that point\nforward.  The jump forward icon 58 enables the user to skip over media to a selected point in time of the message.\nThe rabbit icon 55 controls the rate at which the media of the message is rendered.  The rendering rate can be either faster, slower, or at the same rendering rate the media of the message was originally encoded.\nIn the real-time mode, the participant creating the current message is identified in the window 48.  In the example illustrated, the window identifies Hank Jones as speaking.  As the message continues, the timer 50 is updated, providing a\nrunning time duration of the message.  The jump backward and pause icons 56 and 57 operate as mentioned above.  By jumping from the head of the conversation in the real-time mode back to a previous point using icon 56, the conversation may be seamlessly\ntransitioned from the live or real-time mode to the time-shifted mode The jump forward icon 58 is inoperative when at the head of the message since there is no media to skip over when at the head.\nThe rabbit icon 55 may also be used to implement a rendering feature referred to as Catch up To Live or \"CTL\".  This feature allows a recipient to increase the rendering rate of the previously received and persistently stored media of an\nincoming message until the recipient catches up to the media as it is received.  For example, if the user of the client device joins an ongoing conversation, the CTL feature may be used to quickly review the previous media contributions of the unheard\nmessage or messages until catching up to the head of the conversation.  At this point, the rendering of the media seamlessly merges from the time-shifted mode to the real-time mode.\nBy using the render control options, the user may seamlessly transfer a conversation from the time-shifted mode to the real-time mode and vice versa.  For example, the user may use the pause or jump backward render options to seamlessly shift a\nconversation from the real-time to time-shifted modes or the play, jump forward, or CTL options to seamlessly transition from the time-shifted to real-time modes.\nIt should be noted that the user interface 40 is merely exemplary.  It is just one of many possible implementations for providing a user interface for client communication devices 12.  It should be understood that the features and functionality\nas described herein may be implemented in a wide variety of different ways.  Thus the specific interface illustrated herein should not be construed as limiting in any regard.\n<BR><BR>Web Communities\nWith the Internet and world-wide-web becoming pervasive, web sites that create or define communities are become exceedingly popular.  For example, Internet users with a common interest tend to aggregate at select web sites where they can\nconverse and interact with others.  Social networking sites like Facebook.com, online dating sites like match.com, video game sites like addictivegames.com, and other forums, such as stock trading, hobbies, etc., have all become very popular.  Up to now,\nmembers of these various web sites could communicate with each other by either email or instant messaging style interactions.  Some sites support the creation of voice and video messaging, and other sites support live voice and video communication. \nNone, however, allow members to participate in conversations either synchronously in the real-time mode or asynchronously in the time-shifted mode or provide the ability to seamlessly transition communication between the two modes.\nBy embedding the user interface 40 in one or more web pages of a web site, the members of a web community may participate in conversations with one another.  In FIGS. 5A through 5D for example, the user interface 40 is shown embedded in a social\nnetworking site, an online video gaming site, an online dating site, a stock trading forum respectively.  When users of client communication devices 12 access these or similar web sites, they may conduct conversations with other members, in either the\nreal-time mode, the time-shifted mode, and have the ability to seamlessly shift between the modes, as described in detail herein.\nReferring to FIG. 6A, a diagram of a browser-enabled display on a mobile client communication device 12 according to the present invention is shown.  In this example, the user interface 40 is provided within the browser-enabled display of a\nmobile client communication device 12, such as a mobile phone or radio.  FIG. 6B is a diagram of the mobile client communication device 12 with a keyboard 85 superimposed onto the browser display.  With the keyboard 85, the user may create text messages\nduring participation in conversations.\nAlthough a number of popular web-based communities have been mentioned herein, it should be understood that this list is not exhaustive.  The number of web sites is virtually unlimited and there are far too many web sites to list herein.  In\neach case, the members of the web community may communicate with one another through the user interface 40 or a similar interface as described herein.\n<BR><BR>Real-Time Communication Protocols\nIn various embodiments, the store and stream module 24 of the communication application 20 may rely on a number of real-time communication protocols.\nIn one optional embodiment, the store and stream module 24 may use the Cooperative Transmission Protocol (CTP) for near real-time communication, as described in U.S.  application Ser.  Nos.  12/192,890 and 12/192,899 (U.S Patent Publication Nos. 2009/0103521 and 2009/0103560), all incorporated by reference herein for all purposes.\nIn another optional embodiment, a synchronization protocol may be used that maintains the synchronization of time-based media between a sending and receiving client communication devices 12, as well as any intermediate server 10 hops on the\nnetwork 14.  See for example U.S.  application Ser.  Nos.  12/253,833 and 12/253,837, both incorporated by reference herein for all purposes, for more details.\nIn various other embodiments, the communication application 20 may rely on other real-time transmission protocols, including for example SIP, RTP, Skype, UDP and TCP.  For details on using both UDP and TCP, see U.S.  application Ser.  Nos. \n12/792,680 and 12/792,668 both filed on Jun.  2, 2010 and both incorporated by reference herein.\n<BR><BR>Addressing\nIf the user of a client 12 wishes to communicate with a particular recipient, the user will either select the recipient from their list of contacts or reply to an already received message from the intended recipient.  In either case, an\nidentifier associated with the recipient is defined.  Alternatively, the user may manually enter an identifier identifying a recipient.  In some embodiments, a globally unique identifier, such as a telephone number, email address, may be used.  In other\nembodiments, non-global identifiers may be used.  Within an online web community for example, such as a social networking website, a unique identifier may be issued to each member within the community.  This unique identifier may be used for both\nauthentication and the routing of media among members of the web community.  Such identifiers are generally not global because they cannot be used to address the recipient outside of the web community.  Accordingly the term \"identifier\" as used herein is\nintended to be broadly construed and mean both globally and non-globally unique identifiers.\n<BR><BR>Early and Late Binding\nIn early-binding embodiments, the recipient(s) of conversations and messages may be addressed using telephone numbers and Session Internet Protocol (SIP) for setting up and tearing down communication sessions between client communication devices\n12 over the network 14.  In various other optional embodiments, the SIP protocol is used to create, modify and terminate either IP unicasts or multicast sessions.  The modifications may include changing addresses or ports, inviting or deleting\nparticipants, or adding or deleting media streams.  As the SIP protocol and telephony over the Internet and other packet-based networks, and the interface between the VoIP and conventional telephones using the PSTN are all well known, a detailed\nexplanation is not provided herein.  In yet another embodiment, SIP can be used to set up sessions between client communication devices 12 using the CTP protocol mentioned above.\nIn alternative late-binding embodiments, the communication application 20 may be progressively transmit voice and other time-based media as it is created and as soon as a recipient is identified, without having to first wait for a complete\ndiscovery path to the recipient to be fully discovered.  The communication application 20 implements late binding by discovering the route for delivering the media associated with a message as soon as the unique identifier used to identify the recipient\nis defined.  The route is typically discovered by a lookup result of the identifier as soon as it is defined.  The result can be either an actual lookup or a cached result from a previous lookup.  At substantially the same time, the user may begin\ncreating time-based media, for example, by speaking into the microphone, generating video, or both.  The time-based media is then simultaneously and progressively transmitted across one or more server 10 hop(s) over the network 14 to the addressed\nrecipient, using any real-time transmission protocol.  At each hop, the route to the next hop is immediately discovered either before or as the media arrives, allowing the media to be streamed to the next hop without delay and without the need to wait\nfor a complete route to the recipient to be discovered.\nFor all practical purposes, the above-described late-binding steps occur at substantially the same time.  A user may select a contact and then immediately begin speaking.  As the media is created, the real-time protocol progressively and\nsimultaneously transmits the media across the network 14 to the recipient, without any perceptible delay.  Late binding thus solves the problems with current communication systems, including the (i) waiting for a circuit connection to be established\nbefore \"live\" communication may take place, with either the recipient or a voice mail system associated with the recipient, as required with conventional telephony or (ii) waiting for an email to be composed in its entirety before the email may be sent.\n<BR><BR>Progressive Emails\nIn one non-exclusive late-binding embodiment, the communication application 20 may rely on \"progressive emails\" to support real-time communication.  With this embodiment, a sender defines the email address of a recipient in the header of a\nmessage (i.e., either the \"To\", \"CC, or \"BCC\" field).  As soon as the email address is defined, it is provided to a server 10, where a delivery route to the recipient is discovered from a DNS lookup result.  Time-based media of the message may then be\nprogressively transmitted, from hop to hop to the recipient, as the media is created and the delivery path is discovered.  The time-based media of a \"progressive email\" can be delivered progressively, as it is being created, using standard SMTP or other\nproprietary or non-proprietary email protocols.  Conventional email is typically delivered to user devices through an access protocol like POP or IMAP.  These protocols do not support the progressive delivery of messages as they are arriving.  However,\nby making simple modifications to these access protocols, the media of a progressive email may be progressively delivered to a recipient as the media of the message is arriving over the network.  Such modifications include the removal of the current\nrequirement that the email server know the full size of the email message before the message can be downloaded to the client communication device 12.  By removing this restriction, the time-based media of a \"progressive email\" may be rendered as the\ntime-based media of the email message is received.  For more details on the above-described embodiments including late-binding and using identifiers, email addresses, DNS, and the existing email infrastructure, see co-pending U.S.  application Ser.  Nos. 12/419,861, 12/552,979 and 12/857,486, each commonly assigned to the assignee of the present invention and each incorporated herein by reference for all purposes.\n<BR><BR>Full and Half Duplex Communication\nThe communication application 20, regardless of the real-time protocol, addressing scheme, early or late binding, or if progressive emails are used, is capable of both transmitting and receiving voice and other media at the same time or at times\nwithin relative close proximity to one another.  Consequently, the communication application is capable of supporting full-duplex communication, providing a user experience similar to a conventional telephone conversation.  Alternatively, the\ncommunication application is also capable of sending and receiving messages at discrete times, similar to a messaging or half-duplex communication system.\nWhile the invention has been particularly shown and described with reference to specific embodiments thereof, it will be understood by those skilled in the art that changes in the form and details of the disclosed embodiments may be made without\ndeparting from the spirit or scope of the invention.  For example, embodiments of the invention may be employed with a variety of components and methods and should not be restricted to the ones mentioned above.  It is therefore intended that the\ninvention be interpreted to include all variations and equivalents that fall within the true spirit and scope of the invention.", "application_number": "15615406", "abstract": " A method of enabling communication over a network by maintaining a server\n     on a network and receiving a request at the server from a user of a\n     communication device. In response to the request, a communication\n     application is downloading over the network to the communication device.\n     The communication application enabling the user to participate in a\n     conversation on the communication device in either (i) a real-time mode\n     or (ii) a time-shifted mode and (iii) to seamlessly transition the\n     conversation between the two modes (i) and (ii).\n", "citations": ["4807224", "5117422", "5128932", "5283818", "5375018", "5390236", "5487167", "5524140", "5572576", "5651054", "5692213", "5734963", "5737011", "5889764", "5918158", "5958005", "5963551", "5970122", "6031896", "6037932", "6092120", "6104757", "6175619", "6212535", "6233389", "6262994", "6335966", "6378035", "6411685", "6480783", "6507586", "6564261", "6577599", "6580694", "6594693", "6671732", "6690654", "6700902", "6717925", "6721703", "6721784", "6791949", "6792085", "6807565", "6807578", "6829473", "6834039", "6850965", "6907447", "6912544", "6931114", "6970926", "6973309", "6993009", "6996624", "7002913", "7002973", "7039040", "7039675", "7039761", "7047030", "7058392", "7082164", "7111044", "7113767", "7117521", "7133900", "7139371", "7171491", "7180944", "7187941", "7218709", "7228359", "7233589", "7236738", "7240105", "7277453", "7283809", "7304951", "7305438", "7313593", "7339937", "7349871", "7382881", "7403775", "7415284", "7415291", "7426191", "7444306", "7483899", "7613773", "7626951", "7634652", "7636327", "7656836", "7719975", "7730142", "7742429", "7809388", "7899045", "7908389", "7913053", "7957363", "7970918", "7996553", "8027276", "8045682", "8086222", "8094647", "8099512", "8121270", "8130921", "8175234", "8180029", "8180030", "8243894", "8271003", "8296366", "8311050", "8315377", "8345836", "8401582", "8401583", "8463927", "8526456", "8532270", "8533611", "8559319", "8565149", "8645477", "8670531", "8687779", "8688789", "8693647", "8705714", "8744050", "8817955", "8825772", "8832299", "8849927", "8902749", "8924593", "8948354", "9154628", "9178916", "9338113", "9456087", "9608947", "9621491", "9634969", "9674122", "9742712", "9762861", "20010000540", "20010025377", "20010038610", "20010043602", "20010049745", "20010052019", "20020006802", "20020016818", "20020032799", "20020067739", "20020091848", "20020120666", "20020126817", "20020128029", "20020143959", "20020150094", "20020154745", "20020159600", "20020184368", "20030027566", "20030028632", "20030040301", "20030163580", "20030084106", "20030099198", "20030126162", "20030172116", "20030172233", "20030186722", "20030208543", "20030210265", "20040017905", "20040019539", "20040039839", "20040045036", "20040052218", "20040074448", "20040090959", "20040095900", "20040117722", "20040125816", "20040127279", "20040151158", "20040170158", "20040179092", "20040192353", "20040192378", "20040203670", "20040207724", "20040207870", "20040213211", "20040252679", "20040255148", "20040258220", "20050020246", "20050021819", "20050025308", "20050030932", "20050037706", "20050053033", "20050058070", "20050076084", "20050086311", "20050102358", "20050135333", "20050144247", "20050160345", "20050202807", "20050207487", "20050210394", "20050215228", "20050220137", "20050237999", "20050259682", "20050288101", "20060007943", "20060023969", "20060041815", "20060045038", "20060046758", "20060059199", "20060059267", "20060059342", "20060062215", "20060080704", "20060093304", "20060107285", "20060111130", "20060116167", "20060121925", "20060146822", "20060160522", "20060168003", "20060172754", "20060176902", "20060187897", "20060189305", "20060203802", "20060211411", "20060212582", "20060212592", "20060221174", "20060221995", "20060224748", "20060229934", "20060232663", "20060244588", "20060245367", "20060248149", "20060251167", "20060253599", "20060268750", "20060274698", "20060274721", "20060276714", "20060282544", "20060288391", "20060294259", "20070001869", "20070002832", "20070006021", "20070008884", "20070067407", "20070081622", "20070110046", "20070123267", "20070123284", "20070147263", "20070150555", "20070155346", "20070168863", "20070177549", "20070180032", "20070182819", "20070184868", "20070189327", "20070190987", "20070192427", "20070195742", "20070207782", "20070207785", "20070226804", "20070238472", "20070263575", "20070268887", "20070271331", "20070288574", "20070294333", "20070300007", "20080000979", "20080002621", "20080002691", "20080025300", "20080031250", "20080031448", "20080037721", "20080062246", "20080086700", "20080091804", "20080091839", "20080095173", "20080095338", "20080109865", "20080115087", "20080117901", "20080119210", "20080123623", "20080123628", "20080134054", "20080147910", "20080155032", "20080163312", "20080165707", "20080165791", "20080168173", "20080178251", "20080189295", "20080205444", "20080231684", "20080256255", "20080267377", "20080273077", "20080288989", "20090003544", "20090003545", "20090003547", "20090003560", "20090003563", "20090006609", "20090024743", "20090037541", "20090049140", "20090063698", "20090103689", "20090124238", "20090175211", "20090175425", "20100005168", "20100030864", "20100046535", "20100199133", "20100255818", "20100279663", "20110010459", "20110019662", "20120114108", "20150350270", "20160352666", "20160352792", "20170237695", "20180013704"], "related": ["12883116", "12857486", "12561089", "12552980", "12419861", "12028400", "61232627", "61148885", "60999619", "60937552"]}, {"id": "20170339471", "patent_code": "10375449", "patent_name": "Discovering and displaying media content related to an electronic document", "year": "2019", "inventor_and_country_data": " Inventors: \nFisher; Dave Scott (Mountain View, CA), Dureau; Vincent (Palo Alto, CA), Schilit; William Noah (Mountain View, CA)  ", "description": "<BR><BR>TECHNICAL FIELD\nThe disclosed implementations relate generally to discovering and displaying media content related to an electronic document.\n<BR><BR>BACKGROUND\nWhile viewing an electronic document, such as a web site, a user often becomes interested in searching and viewing other information related to the electronic document.  For example, when browsing a movie rating website, a user might want to\nwatch movies or TV programs mentioned by the website on his or her TV.  Typically, however, to view on a TV media content related to a document displayed by a computer, a user would need to first suspend their browsing activity on the computer, and then\nsearch for the related media content on the TV.  During this process, users are inconvenienced because they are required to take actions outside the browsing experience.  Furthermore, users may also become frustrated by the additional TV search process\nand abandon their effort to view the TV content altogether.\n<BR><BR>SUMMARY\nSystems, methods, and non-transitory computer readable storage medium (server side and client side) for discovering and displaying media content related to an electronic document are hereby disclosed.\nIn one aspect, a method for discovering and displaying media content related to an electronic document comprises: at a first computing device (e.g., a client device) having one or more processors and memory storing one or more programs for\nexecution by the one or more processors: obtaining document identifying information for an electronic document, wherein the document identifying information is one of: at least a portion of a network identifier or source code for the electronic document;\nobtaining from a server one or more media entities associated with the electronic document, wherein the media entities correspond to media content including scheduled TV programming, streaming media or locally stored media; displaying, on the first\ncomputing device, a representation of at least a subset of the one or more media entities; obtaining a user selection from the representation of a respective media entity from the at least a subset of the one or more media entities; and in response to\nthe user selection, sending selection information identifying the respective media entity to a second computing device, the second computing device being configured to access and display respective media content corresponding to the respective media\nentity based on the selection information.  In some implementations, the above-described steps are performed by a browser plug-in.\nIn some implementations, the electronic document is an electronic document opened in a document application on the first computing device.  In some implementations, the document application is a web browser and the electronic document is a web\npage.\nIn some implementations, the method further includes: when the document identifying information includes at least a portion of a media content link that is readily identifiable by the first computing device, directly obtaining at the first\ncomputing device media entity associated with the at least a portion of the media content link, without involvement of the server.\nIn some implementations, when the document identifying information is the at least a portion of a network identifier, the one or more obtained media entities include media entities recognized by a search engine while previously indexing the web\npage.\nIn some implementations, when the document identifying information is the source code of the electronic document, the one or more obtained media entities include entities obtained by parsing the source code of the electronic document.\nIn some implementations, the one or more media entities are obtained based at least in part on one or more filtering criteria.\nIn some implementations, the method also includes: obtaining a media entity that is related to the one or more media entities; and displaying, on the first computing device, a representation of the related media entity.\nIn some implementations, the sending of the selection information is done via a local connection, from the first computing device, to the second computing device.\nIn some implementations, the method further includes: during a display of the respective media content on the second computing device: receiving, at the first computing device, a second user selection, and sending a remote control command\nmodifying the display of the respective media content, in accordance with the second user selection, to the second computing device.\nIn some implementations, obtaining one or more media entities associated with the electronic document includes: sending a request to the server for media entities associated with the electronic document, the request including the document\nidentifying information; and receiving the one or more media entities from the server.\nIn addition, a client system and a non-transitory computer readable storage medium storing one or more programs, which when executed by a computer, cause the computer to perform one or more steps of the above-described methods are also\ndisclosed.\nIn another aspect, a method for discovering and displaying media content related to an electronic document comprises: at a server system (e.g., a server) having one or more processors and memory storing one or more programs for execution by the\none or more processors: receiving, from a first computing device, document identifying information for an electronic document, wherein the document identifying information is one of: at least a portion of a network identifier, or source code for the\nelectronic document; in response to receiving the document identifying information: identifying one or more media entities associated with the electronic document, wherein the media entities correspond to media content including scheduled TV programming,\nstreaming media or locally stored media; and sending at least a subset of the one or more entities to the first computing device for displaying a presentation thereof.\nIn some implementations, the electronic document is an electronic document opened in a document application on the first computing device.  In some implementations, the document application is a web browser and the electronic document is a web\npage.\nIn some implementations, when the document identifying information is the at least a portion of a network identifier, the one or more identified media entities include media entities recognized by a search engine while previously indexing the\nweb page.\nIn some implementations, when the document identifying information is the source code of the electronic document, the one or more identified media entities include entities obtained by parsing the source code of the electronic document.\nIn some implementations, the one or more media entities are identified based at least in part on one or more filtering criteria.\nIn some implementations, the method also includes: identifying a media entity that is related to the one or more media entities; and sending the related media entity to the first computing device for displaying a presentation thereof.\nIn addition, a server system and a non-transitory computer readable storage medium storing one or more programs, which when executed by a computer, cause the computer to perform one or more steps of the above-described methods are also\ndisclosed. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe implementations disclosed herein are illustrated by way of example, and not by way of limitation, in the figures of the accompanying drawings.  Like reference numerals refer to corresponding parts throughout the drawings.\nFIG. 1 is a block diagram illustrating a distributed client-server system, in accordance with some implementations.\nFIG. 2A is a block diagram illustrating a client, in accordance with some implementations.\nFIG. 2B is a block diagram illustrating an example of media entities, in accordance with some implementations.\nFIG. 3 is a block diagram illustrating a server (or a backend), in accordance with some implementations.\nFIG. 4 is a flow diagram illustrating a method of discovering and displaying media content related to an electronic document in a client-server system, in accordance with some implementations.\nFIG. 5A is a flow chart illustrating a method of discovering and displaying media content related to an electronic document at a client, in accordance with some implementations.\nFIG. 5B is a flow chart illustrating a method of discovering and displaying media content related to an electronic document at a server, in accordance with some implementations.\nFIG. 6 is a block diagram illustrating an example of filtering media content related to an electronic document, in accordance with some implementations.\nFIGS. 7A-7E are screen images illustrating example portions of a user interface for discovering and displaying media content related to an electronic document, in accordance with some implementations.\n<BR><BR>DETAILED DESCRIPTION\nThe implementations described herein provide techniques for discovering, using a first device media content related to an electronic document, such as a web page, text file or word processor document, and then playing selected items of the\ndiscovered media content on a different device, such as an Internet-connected TV or set-top box.\nIn some implementations, a client module obtains document identifying information for an electronic document that is being displayed on a first computing device.  For example, the first computing device might be a laptop or tablet computer, and\nthe electronic document might be a web page about a subject that is associated with one or more items of media content.  For example, the web page might relate to a historical figure, such as Abraham Lincoln.  In some implementations, the document\nidentifying information is at least a portion of a network identifier (such as a web page URL) or source code for the electronic document (e.g., the HTML code for a web page).\nThe first computing device interacts with a server/backend system to obtain one or more media entities associated with the electronic document.  In some implementations, the first computing device initiates the process of obtaining the media\nentities by sending (either automatically, without user interaction, or upon user request) the server/backend system the document identifying information.  The server/backend system then identifies the media entities by processing the electronic document\nidentified by the document identifying information.  In some implementations, the media entities correspond to media content, including scheduled TV programming, streaming media or locally stored media.  For example, in the case of a web page about\nAbraham Lincoln, each of the media entities would relate to TV programs or movies (including movies that are streamed from Internet sites) about Abraham Lincoln, or subjects related to Abraham Lincoln discussed on the same web page.\nNext, in some implementations, a representation of at least a subset of the one or more media entities is displayed by a program module executing on the first computing device.  For example, referring again to the web page about Abraham Lincoln,\nthe program module would display a list of media items corresponding to one or more of the media entities about Abraham Lincoln returned by the server/backend system.  The program module would then enable a user to select a particular media entity from\nthe representation/list for playback on a TV or second computing device different from the first computing device.  In some implementations, the second computing device might be an Internet-connected TV, set-top box or a media player device with\nassociated software that enables playback of network or Internet-based media content.  In some implementations, the first and second computing devices are connected to the same local area network, over which they exchange information, such as information\non selected TV content.  In response to the user selection, a program module on the first computing device sends selection information identifying the particular media entity selected by the user to the second computing device that is configured to\naccess and display the particular media content.  In this way, a user is able to seamlessly identify, from a first computing device, media content associated with a web page or other electronic document being accessed from that device and then play that\ncontent on a second computing device, such as a connected TV, or set top box that outputs content to a local TV or display.  Additional details of implementations are now described in relation to the Figures.\nFIG. 1 is a block diagram illustrating a distributed client-server system 100 for discovering and displaying media content related to an electronic document.  In some implementations, the distributed client-server system 100 includes the client\nsystem 102 (referred to herein as \"client 102\"), the cloud 104, the connected TV and/or Set-top box 106 (referred to herein as \"TV and/or STB 106\"), the local connection 108, the backend systems 110 (referred to herein as \"backend 110\"), and, optionally,\nother local media content (NAS/DVR) 170.\nIn some implementations, the client 102 includes a document application 150 (e.g., a web browser), which opens and displays an electronic document for a user to view, or loads the electronic document in the background.  In some implementations,\nthe document application 150 is a general purpose Internet browser (sometimes called a Web browser) having a browser window used for viewing web pages, and a representation of media entities associated with the web pages.  In some implementations, the\ndocument application 150 is a document editor or viewer (e.g., Microsoft Word, or Google Docs).  In some implementations, the client 102 sends information identifying the electronic document (e.g., the document identifier 141, such as, at least a portion\nof a network identifier or source code of the electronic document) to the backend 110, via the cloud 104, and receives one or more media entities 143 associated with the electronic document.  In some implementations, the client 102 also displays a\nrepresentation of the received media entities.\nIn some implementations, the document application 150 also includes a TV remote control module 152.  In some implementations, after displaying the representation of media entities, the client 102, through the TV remote control module 152,\nreceives a user selection of a respective entity (e.g., the selected media entity 145), and sends selection information to the TV and/or STB 106, where media content (e.g., the media content 149) corresponding to the selected respective media entity is\ndisplayed.  In some implementations, the TV remote control module 152 also sends user selection information that modifies the display of media content on the TV and/or STB 106.  In some implementations, the client 102 is also connected to a display 154,\nwhich displays the electronic document (e.g., a web page) for a user to view.  In some embodiment, the display 154 is a computer monitor or display.\nIn some implementations, the TV and/or STB 106 includes a TV and/or a set-top box connected thereto.  In some implementations, the TV and/or STB 106 is a Google TV.  In some implementations, the TV and/or STB 106 includes a TV control module 160\nand, optionally, local media content 162.  In some implementations, the TV control module 160 receives user selection of a respective media entity, or control command (e.g., remote control command) from the client 102, via local connection 108, and\nretrieves and displays media content in accordance therewith.  In some implementations, the TV control module 160 retrieves media content (e.g., using the remote content request 147) from the backend 110 via the cloud 104 (e.g., the Internet).\nIn some implementations, the TV control module 160 also retrieves media content locally from local media content 162, e.g., when the TV and/or STB 106 is not connected to the cloud 104, or when the media content selected by the user is stored\nlocally.  In some implementations, the local media content 162 includes media content stored locally.  In some implementations, the local media content 162 includes streaming media previously downloaded, TV programming previously recorded on a local DVR,\nor media content created and saved locally by a user (e.g., home-made audio and/or video clips).  In some implementations, the TV control module 160 also access other local media content (NAS/DVR) 170, which is not included in the TV and/or STB 106.  For\nexample, in some implementations, the other local media content (NAS/DVR) 170 includes home-made audio and/or video clips saved on a plug-and-play storage medium.  In some implementations, the TV and/or STB 106 is also connected to a display 164, which\ndisplays media content, in accordance with user selection.  In some embodiment, the display 154 is a TV monitor or display.\nIn some implementations, the backend 110 includes a content identification module 130, an entity database 132 (optionally), an entity retriever (or WebRef) 134, a content filter 136, and filtering criteria 138.  In some implementations, the\ncontent identification module 130 receives document identifying information (which identifies an electronic document e.g., the document identifier 141), and identifies entities--media entities 143 and/or non-media entities--associated with the electronic\ndocument.  In some implementations, the entity database 132 stores entities (media entities and/or non-media entities) associated with electronic documents.  In some implementations, the entity database 132 is local to the backend 110.  In other\nimplementations, the Entity database 132 is remote from, but accessible to, the backend 110.  In some implementations, in accordance with information received from the content identification module 130, the entity retriever (or WebRef) 134 retrieves\nentities from the entity database 132.  In some implementations, entities retrieved by the entity retriever (or WebRef) 134 are filtered by the content filter 136, for example, to filter out non-media entities, or to produce a certain category of media\nentities (e.g., media entities other than TV programming).  In some embodiment, the content filter 136 filters entities by first retrieving one or more filters from the filtering criteria 138, and then selectively identifying one or more entities from\nall the retrieved entities in accordance with the filters.\nIn some implementations, the cloud 104 optionally includes the Internet, one or more local area networks (LANs), one or more wide area networks (WANs), other types of networks, or a combination of such networks.  In some implementations, Local\nconnection 108 optionally includes connections by infrared signals, radio frequency signals, local area networks (LANs), Bluetooth, serial or parallel cable, or a combination of thereof.\nFIG. 2A is a block diagram illustrating a client system 102 in accordance with some implementations.  The client 102 typically includes one or more processing units CPU(s) 202 (also herein referred to as processors), one or more network or other\nCommunication Interfaces 204, memory 206, an user interface 205 comprising a display device and a keyboard, mouse, touchpad, touchscreen or other input device, and one or more communication Buses 208 for interconnecting these components.  The\ncommunication buses 208 optionally include circuitry (sometimes called a chipset) that interconnects and controls communications between system components.  The memory 206 typically includes high-speed random access memory, such as DRAM, SRAM, DDR RAM or\nother random access solid state memory devices; and optionally includes non-volatile memory, such as one or more magnetic disk storage devices, optical disk storage devices, flash memory devices, or other non-volatile solid state storage devices.  The\nmemory 206 optionally includes one or more storage devices remotely located from the CPU(s) 202.  The memory 206, or alternatively the non-volatile memory device(s) within the memory 206, comprises a non-transitory computer readable storage medium.  In\nsome implementations, the memory 206 or alternatively the non-transitory computer readable storage medium stores the following programs, modules and data structures, or a subset thereof: an operating system 210, which includes procedures for handling\nvarious basic system services and for performing hardware dependent tasks; a network communication module (or instructions) 212 for connecting client 102 with other devices (e.g., backend 110 or TV and/or STB 106) via one or more network interfaces 204\n(wired or wireless), the local connection 108 (FIG. 1), and the cloud 104 (FIG. 1); a document application 150 for loading, opening or displaying electronic documents--e.g., web pages, Word documents or Google documents--on the client 102, including in\nsome implementations: a TV remote control module 152--optionally implemented as an extension to document application 150, such as a browser plug-in--for sending document identifying information to backend 110, and receiving associated media entities\ntherefrom.  In some implementations, TV remote control module 152 also receives a user selection of a respective media entity, and sends user selection information to TV and/or STB 106; data 214 stored on the client 102, which includes: media entities\n216 for including media entity information retrieved from the entity database 132 (returned by the backend 110) or media entities identified by the client 102, including: media entities i (e.g., media entities 218-1 through 218-n) for including\ninformation concerning media content corresponding to one or more media entities, each including one or more of the following media characteristics (this information being used by the TV remote control module 152 to populate a displayed list of media\nentities associated with an electronic document opened in the document application 150): a content title 220-i for including title of the media content corresponding to the media entity 1; a type 222-i for including information indicating one or more\ncategory to which the corresponding media content belongs; a year 224-i for identifying a year in which the corresponding media content was published, created or broadcasted; a rating 226-i for identifying a rating associated with the corresponding media\ncontent; a thumbnail (or link) 228-i corresponding to a thumbnail (e.g., a logo or an audio/video preview) of the corresponding media content, or a link thereto.  a description 229-i for including a description of (e.g., a synopsis, a condensed or full\nreview) the corresponding media content, or a link thereto.  a selected media entity identifier 232 for uniquely identifying an media entity selected by a user associated with the client 102 (the corresponding data for the selected media entity being\ntransmitted to the connected TV 106, enabling the TV control module 160 to identify and initiate playback of the media content corresponding to selected media entity); and related media entities for including media entities related to the media entities\n216.\nIn some implementations, one or more of the above identified elements are stored in one or more of the previously mentioned memory devices, and correspond to a set of instructions for performing a function described above.  The above identified\nmodules or programs (e.g., sets of instructions) need not be implemented as separate software programs, procedures or modules, and thus various subsets of these modules may be combined or otherwise re-arranged in various implementations.  In some\nimplementations, the memory 206 optionally stores a subset of the modules and data structures identified above.  Furthermore, the memory 206 may store additional modules and data structures not described above.\nFIG. 2B is a block diagram illustrating an example of media entities (e.g., the media entities 216 in FIG. 2A), in accordance with some implementations.  In some implementations, media entities includes data returned from the server/backend\nsystem 110 following discovery by the server of media entities related to a web page being displayed at the client system 102.  In some implementations, media entities include data formatted consistently with the JSON object model.  In some\nimplementations, media entities include content title, type, year, rating, thumbnail (or link), and description.  In some implementations, the content title includes title information of a media entity, such as \"Dictator\" for the movie entity 252.  In\nsome implementations, the type identifies a category to which a media entity belongs, such as movie, TV programming, local media content.  In some implementations, the year identifies a year in which the media content corresponding to a media entity is\nbroadcasted or created, and the rating includes a rating associated with a media entity.  In some implementations, thumbnail (or link) includes information for displaying a thumbnail (e.g., a logo or an audio/video preview) of the media content\ncorresponding to a media entity, or a link to the thumbnail, and Description includes a synopsis of the corresponding media content.  Generally, this information is used to populate a list of media entities associated with a particular electronic\ndocument opened in the document application 150 (the list being displayed by the remote control module 152), from which list a particular associated media entity can be selected for transmission to the connected TV.  This media entity information is used\nby the connected TV or set-top box (including a Google TV device) to identify instances of media content 149 (FIG. 1) corresponding to the transmitted selected media entity information 145 (FIG. 1) to be displayed at the display 164.\nFIG. 3 is a block diagram illustrating the backend system 110 (\"backend 110,\" also called a server), in accordance with some implementations.  Backend 110 typically includes one or more processing units CPU(s) 302 (also herein referred to as\nprocessors), one or more network or other communications interfaces 308, the memory 306, and one or more communication buses 308 for interconnecting these components.  The communication buses 308 optionally include circuitry (sometimes called a chipset)\nthat interconnects and controls communications between system components.  The memory 306 includes high-speed random access memory, such as DRAM, SRAM, DDR RAM or other random access solid state memory devices; and optionally includes non-volatile\nmemory, such as one or more magnetic disk storage devices, optical disk storage devices, flash memory devices, or other non-volatile solid state storage devices.  The memory 306 optionally includes one or more storage devices remotely located from CPU(s)\n302.  The memory 306, or alternatively the non-volatile memory device(s) within the memory 306, comprises a non-transitory computer readable storage medium.  In some implementations, the memory 306 or alternatively the non-transitory computer readable\nstorage medium stores the following programs, modules and data structures, or a subset thereof: data 315 stored on the backend 110, which includes: media entities 316 for including media entity information retrieved from the entity database 132,\nincluding: media entities i (e.g., media entities 318-1 through 318-n) for including information concerning media content corresponding to one or more media entities, each including one or more of the following media characteristics (this information\nbeing used by the TV remote control module 152 to populate a displayed list of media entities associated with an electronic document opened in the document application 150): a content title 320-i for including title of the media content corresponding to\nthe media entity 1; a type 322-i for including information indicating one or more category to which the corresponding media content belongs; a year 324-i for identifying a year in which the corresponding media content was published, created or\nbroadcasted; a rating 326-i for identifying a rating associated with the corresponding media content; a thumbnail (or link) 328-i corresponding to a thumbnail (e.g., a logo or an audio/video preview) of the corresponding media content, or a link thereto. a description 329-i for including a description of (e.g., a synopsis, a condensed or full review) the corresponding media content, or a link thereto.  the selected media entity identifier 232 for uniquely identifying an media entity selected by a user\nassociated with the client 102 (the corresponding data for the selected media entity being transmitted to the connected TV 106, enabling the TV control module 160 to identify and initiate playback of the media content corresponding to selected media\nentity); and the related media entities for including media entities related to the media entities 216.\nIn some implementations, data 315 further includes entities other than media entities 316.  For example, data 315, in some implementations, includes both media entities and non-media entities (FIG. 6).  In some implementations, entities--media\nentities and/or non-media entities--returned from the entity database 132 is a superset of media entities sent to the client 102 (e.g., the media entities 143), because entities returned from the entity database 132, in some situations includes non-media\nentities, or entities that do not meet filtering criteria applied by the content filter 136.\nIn some implementations, one or more of the above identified elements are stored in one or more of the previously mentioned memory devices, and corresponds to a set of instructions for performing a function described above.  The above identified\nmodules or programs (e.g., sets of instructions) need not be implemented as separate software programs, procedures or modules, and thus various subsets of these modules may be combined or otherwise re-arranged in various implementations.  In some\nimplementations, the memory 306 optionally stores a subset of the modules and data structures identified above.  Furthermore, Memory 306 optionally stores additional modules and data structures not described above.\nAlthough FIG. 3 shows a \"backend 110,\" also referred to as a server, FIG. 3 is intended more as functional description of the various features which may be present in backend system than as a structural schematic of the implementations described\nherein.  In practice, and as recognized by those of ordinary skill in the art, items shown separately could be combined and some items could be separated.\nFIG. 4 includes a flowchart illustrating a method for discovering and displaying media content related to an electronic document, in accordance with some implementations.\nIn some implementations, the client 102 first connects (402) to TV and/or STB 106, by sending log-in information (e.g., 702 in FIG. 7A) to TV and/or STB 106.  After the log-in information is verified, TV and/or STB 106 is connected (404) to the\nclient 102 through a communication session.\nIn some implementations, after the client 102 and the TV and/or STB 106 are connected, an electronic document is opened (406) or loaded on the client 102.  In some implementations, the electronic document is displayed in foreground on the client\n102 (e.g., visible to a user).  In other implementations, the electronic document is loaded or opened in background (e.g., not visible to the user).  In other implementations, the electronic document was opened or loaded on the client 102 before\nconnection between the client 102 and the TV and/or STB 106 was established.\nIn some implementations, after the electronic document is opened, the client 102 sends (408) document indentifying information that identifies the electronic document to Backend 110.  In some implementations, document indentifying information is\none of: at least a portion of a network identifier (e.g., a portion of a URL) or source code for the electronic document (e.g., web page source code).  In some implementations, a portion of a network identifier is sent to Backend 110, when the electronic\ndocument is also accessible to Backend 110, e.g., a public web page.  In other implementations, source code for the electronic document is sent to Backend 110, when the electronic document is not accessible to Backend 110, e.g., a password protected web\npage, or an encrypted document.\nAfter receiving (410) document identifying information from the client 102, the backend 110 identifies (412) a plurality of entities--media entities and/or non-media entities--associated with the electronic document (e.g., a movie, a person or a\nplace mentioned in the web page displayed on the client 102).  In some implementations, the backend 110 optionally applies (412) one or more filtering criteria (e.g., data from the filtering criteria 138) to the plurality of entities, e.g., to identify\n(416) one or more media entities from the plurality of entities.  After one or more media entities are identified, the backend 110 sends (418) at least a subset of the one or more media entities along with presentation information, to the client 102,\nwhere a presentation of the subset of media entities are displayed (420).  In some implementations, an media entity include information identifying the media entity, such as, a network address at which media content corresponding to the entity can be\nretrieved, or a channel on which media content is being broadcasted (e.g., the channel information 324 in FIG. 2).\nIn some implementations, the client 102 obtains (422) a user selection of a respective entity from the subset of media entities, and sends (424) selection information identifying the respective entity to the TV and/or STB 106.  Based on the\nselection information, the TV and/or STB 106 accesses and displays (426) respective media content corresponding to the respective media entity.\nFIG. 5A is a flow chart illustrating a method of discovering and displaying media content related to an electronic document at a client, in accordance with some implementations.  In some implementations, at a first computing device (e.g., the\nclient 102), document identifying information for an electronic document is obtained (502).  In some implementations, the document identifying information is (504) one of: at least a portion of a network identifier or source code for the electronic\ndocument.  In some implementations, the electronic document is an electronic document opened in a document application on the first computing device.  In some implementations, the document application is a web browser and the electronic document is a web\npage.\nIn some implementations, in accordance with the document identifying information, one or more media entities associated with the electronic document are obtained (506) from a server (e.g., the backend 110).  In some implementations, the media\nentities correspond to media content include (508) scheduled TV programming, streaming media or locally stored media.  In some implementations, the first computing device (e.g., the client 102) obtains the one or more media entities by first sending a\nrequest to the server for media entities associated with the electronic document, the request including the document identifying information; and then receiving the one or more media entities from the server.\nIn some implementations, when the document identifying information is the at least a portion of a network identifier (e.g., a portion of a URL), the one or more obtained media entities include media entities recognized by a search engine while\npreviously indexing the web page.  For example, when the electronic document is also accessible to the server (e.g., a public web page accessible to the backend 110), and the electronic document (e.g., the public web page) has previously been indexed by\na search engine (e.g., Google search engine), entities associated with the electronic document have already been identified and stored in the entity database 132.  As such, after a portion of a URL is sent to the server, the server identifies one or more\nentities based on the portion of the URL by using the entity retriever (WebRef) 134 to retrieve entities associated with the electronic document from Entity database 132.\nIn other implementations, when the document identifying information is the source code of the electronic document, the one or more obtained media entities include entities obtained by parsing the source code of the electronic document.  For\nexample, when the electronic document is not accessible to the server by URL (e.g., a password protected document or web page), the source code of the electronic document (e.g., web page or document source code) is sent to the server, which identifies\nentities by first parsing the source code, using the content parser 314, and then retrieving entities from the entity database 132.\nIn still other implementations, when the document identifying information includes at least a portion of a media content link that is readily identifiable by the first computing device (e.g., a YouTube that is readily identifiable by the client\n102), the first computing device directly obtains media entity associated with the at least a portion of the media content link, without involvement of the server.  For example, when the a web page being viewed by a user includes YouTube links, because\nthe syntax of these links are generally known or readily identifiable by the client 102, the client 102 identifies media entities (e.g., YouTube clip) on the web page, without sending the YouTube link to the server.\nIn some implementations, the first computing device then obtains (506) one or more media entities associated with the electronic document, for example, from the backend 110.  In some implementations, the one or more media entities are obtained\nbased at least in part on one or more filtering criteria.  For example, in some situations, entities--both media entities and non-media entities--are first obtained, non-media entities are then filtered out, and media entities are sent to the client 102. In other situations, filters, such as top-ten viewed media content, are applied to filter out media entities that do not meet filtering criteria.  Filters are applied so that the entities are selectively presented to the client 102, by type or by rating,\nso as to reduce burdens on a user when selecting media content she desires.\nIn some implementations, a representation of at least a subset of the one or more media entities is displayed (510) on the first computing device (e.g., 708, 710, and 712 in FIGS. 7D-7E).\nIn some implementations, a presentation of media entity related to the one or more media entities and also displayed on the first computing device.  For example, in some situations, entities related to media entities associated with the\nelectronic document are also displayed.  For example, when a user is viewing a web page on a thriller movie, in addition to media entity corresponding to the thriller movie, media entity corresponding to a love movie that shares the same director with\nthe thriller movie is also identified and presented for display on the client 102.  This approach allows related content to be automatically displayed to a user, thus reducing burden on the user to search for the related content.\nIn some implementations, the first computing device then obtains (512) a user selection from the representation of a respective media entity from the at least a subset of the one or more media entities.  For example, a user associated with the\nclient 102 selects a movie entity (712 in FIG. 7) from the one or more media entities.\nIn some implementations, in response to the user selection, the first computing device (e.g., the client 102) sends (512) selection information identifying the respective media entity to a second computing device (e.g., the TV and/or STB 106). \nIn some implementations, the selection information is sent via a local connection (e.g., the local connection 108), from the first computing device, to the second computing device.  The second computing device is (516) configured to access and display\nrespective media content corresponding to the respective media entity based on the selection information.\nIn some implementations, respective media content corresponding is then displayed on the second computing device.  For example, after a user selects a YouTube clip (710 in FIG. 7D), the YouTube clip is then displayed on the TV and/or STB 106. \nIn some implementations, during the display of the respective media content on the second computing device, the first computing device receives a second user selection, and sends a remote control command modifying the display of the respective media\ncontent, in accordance with the second user selection, to the second computing device.  For example, during the display of the YouTube clip, a user presses a fast-forward button on a control panel (e.g., 704 and 706 in FIGS. 7B-7C) displayed on the\nclient 102, and a fast-forward command is then sent to the TV and/or STB 106, which fast-forwards the YouTube clip.  This approach provides to user control over the media content being displayed on the TV and/or STB 160, in addition to the initial user\nselection of media entity.\nIn some implementations, the method describe above is performed at the first computing device, by a plug-in (e.g., a browser plug-in, such as the TV remote control module 152 in FIG. 1).\nFIG. 5B is a flow chart illustrating a method of discovering and displaying media content related to an electronic document at a server, in accordance with some implementations.  In some implementations, at a server, document identifying\ninformation for an electronic document is received (552) from a first computing device.  The document identifying information is (554) one of: at least a portion of a network identifier, or source code for the electronic document.\nIn some implementations, in response (556) to receiving the document identifying information: the server identifies (558) one or more media entities associated with the electronic document, and sends at least a subset of the one or more entities\nto the first computing device for displaying a presentation thereof.  The media entities correspond (560) to media content including scheduled TV programming, streaming media or locally stored media.\nIn some implementations, the electronic document is an electronic document opened in a document application on the first computing device.  In some implementations, the document application is a web browser and the electronic document is a web\npage.\nIn some implementations, when the document identifying information is the source code of the electronic document, the one or more identified media entities include entities obtained by parsing the source code of the electronic document.  In\nother implementations, when the document identifying information is the at least a portion of a network identifier, the one or more identified media entities include media entities recognized by a search engine while previously indexing the web page.\nIn some implementations, the one or more media entities are identified based at least in part on one or more filtering criteria.\nIn some implementations, after one or more media entities associated with the electronic document are identified, the server (e.g., the backend 110) sends (562) at least a subset of the one or more media entities to the first computing device\n(e.g., the client 102) for displaying a presentation of the subset of the one or more media entities.\nIn some implementations, the server additional identifies a media entity that is related to the one or more media entities; and sends the related media entity to the first computing device for displaying a presentation thereof.\nFIG. 6 is a block diagram illustrating an example of filtering media content related to an electronic document, in accordance with some implementations.\nAs shown in FIG. 6, after receiving the document identifying information 602 (e.g., a portion of a network identifier or source code), which identifies an electronic document, the entity retriever (WebRef) 134 retrieves A plurality of entities\n604 associated with the electronic document, from the entity database 132.\nIn some implementations, entities retrieved from the entity database 132 include both media entity and non-media entity.  As shown in FIG. 6, A plurality of entities 604 includes both media entities (e.g., the TV programming 630, the streaming\nmedia 632, the local media content 636, and the TV programming 642), and non-media entities (e.g., the book 634, the place 637, and the person 640).\nIn some implementations, after a plurality of entities 604 are retrieved, the content filter 136 retrieves one or more entity filters from the filtering criteria 138, and applies the filters to A plurality of entities 604.  In some\nimplementations, filters are applied to filter out non-media entities from a plurality of entities 604.  In other implementations, filters are applied to filter out certain type of media entities from a plurality of entities 604, for example to filter\nout all local media content, because TV programmings and Streaming media are more likely carry live or recent content.\nIn some implementations, after filters are applied, one or more media entities 606 are selected from a plurality of entities 604.  As shown in FIG. 6, in some implementations, the one or more media entities 606 include media content (e.g., TV\nprogramming 630, streaming media 632, local media content 636, and TV programming 642), and does not include non-media content.\nFIGS. 7A-7E are screen images illustrating example portions of a user interface for discovering and displaying media content related to an electronic document, in accordance with some implementations.\nIn FIG. 7A, the log-in panel 702 is an example of a user interface, through which a user associated with Client 102 connects the client 102 with the TV and/or STB 106.  For example, this is done by a user entering a pairing code \"1234\" that\nestablishes a link, typically over a local area network, between the client 102 and the TV or STB 106, through which the client 102, among other operations, can control the TV and/or forward information on selected media content for playback by the TV.\nIn FIG. 7B, the control panel 704 is an example of a user control panel, through which a user selection of a media entity, and a remote control command modifying display of media content, are sent from the client 102 to the TV and/or STB 106. \nFor example, the control panel 704 allows a user to control the paired TV using conventional remote control functionality, use connected features of the STB or TV (such as features provided by Google TV, including searching for content and accessing the\nGoogle TV home screen).  The control panel 704 also enables a user to send an item of media content (selected from the media entities returned by the server) to the connected TV or STB for display through use of a \"Send page to TV\" button.\nIn FIG. 7C, the track pad 706 is example of a user control panel, through which additional user control commands are sent from the client 102 to the TV and/or STB 106.  For example, the track pad 706 allows a user to control the paired TV with\ngreater granularity, such as fast-forwarding or fast-backwarding media content displayed on the TV and/or STB 106, based on user hand gestures on the track pad 706.  For example, a fast user swipe on the track pad 706 fast-forwards the media content\ndisplayed on the TV and/or STB 106 faster than a slow user swipe.\nIn FIG. 7D, several media entities are displayed after an along with the control panel 704.  For example, when a user is viewing a web page concerning Google TV, media entities (e.g., 708, 710, and 712) related to the Google TV web page are\ndisplayed as a lit.  Media entities are displayed based on their relevancy (e.g., as indicated by the description) or popularity (e.g., as indicated by the number of views).\nIn FIG. 7E, after media entity 708 is selected by a user, media entity 708 disappears from the list of media entities (e.g., the list is updated).  In some implementations, new media entities are also added, so that the number of media entities\non the list does not diminish.\nThe methods illustrated in FIGS. 4, 5A and 5B may be governed by instructions that are stored in a computer readable storage medium and that are executed by at least one processor of at least one host device (or at least one server).  Each of\nthe operations shown in FIGS. 4, 5A and 5B may correspond to instructions stored in a non-transitory computer memory or computer readable storage medium.  In various implementations, the non-transitory computer readable storage medium includes a magnetic\nor optical disk storage device, solid state storage devices such as Flash memory, or other non-volatile memory device or devices.  The computer readable instructions stored on the non-transitory computer readable storage medium may be in source code,\nassembly language code, object code, or other instruction format that is interpreted and/or executable by one or more processors.\nPlural instances may be provided for components, operations or structures described herein as a single instance.  Finally, boundaries between various components, operations, and data stores are somewhat arbitrary, and particular operations are\nillustrated in the context of specific illustrative configurations.  Other allocations of functionality are envisioned and may fall within the scope of the implementation(s).  In general, structures and functionality presented as separate components in\nthe example configurations may be implemented as a combined structure or component.  Similarly, structures and functionality presented as a single component may be implemented as separate components.  These and other variations, modifications, additions,\nand improvements fall within the scope of the implementation(s).\nIt will also be understood that, although the terms \"first,\" \"second,\" etc. may be used herein to describe various elements, these elements should not be limited by these terms.  These terms are only used to distinguish one element from another. For example, a first computing device could be termed a second computing device, and, similarly, a second computing device could be termed a first computing device, which changing the meaning of the description, so long as all occurrences of the \"first\ncomputing device\" are renamed consistently and all occurrences of the \"second computing device\" are renamed consistently.  The first computing device, and the second computing device are both computing devices, but they are not the same computing device.\nThe terminology used herein is for the purpose of describing particular implementations only and is not intended to be limiting of the claims.  As used in the description of the implementations and the appended claims, the singular forms \"a\",\n\"an\" and \"the\" are intended to include the plural forms as well, unless the context clearly indicates otherwise.  It will also be understood that the term \"and/or\" as used herein refers to and encompasses any and all possible combinations of one or more\nof the associated listed items.  It will be further understood that the terms \"comprises\" and/or \"comprising,\" when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and/or components, but do not\npreclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof.\nAs used herein, the term \"if\" may be construed to mean \"when\" or \"upon\" or \"in response to determining\" or \"in accordance with a determination\" or \"in response to detecting,\" that a stated condition precedent is true, depending on the context. \nSimilarly, the phrase \"if it is determined (that a stated condition precedent is true)\" or \"if (a stated condition precedent is true)\" or \"when (a stated condition precedent is true)\" may be construed to mean \"upon determining\" or \"in response to\ndetermining\" or \"in accordance with a determination\" or \"upon detecting\" or \"in response to detecting\" that the stated condition precedent is true, depending on the context.\nThe foregoing description included example systems, methods, techniques, instruction sequences, and computing machine program products that embody illustrative implementations.  For purposes of explanation, numerous specific details were set\nforth in order to provide an understanding of various implementations of the inventive subject matter.  It will be evident, however, to those skilled in the art that implementations of the inventive subject matter may be practiced without these specific\ndetails.  In general, well-known instruction instances, protocols, structures and techniques have not been shown in detail.\nThe foregoing description, for purpose of explanation, has been described with reference to specific implementations.  However, the illustrative discussions above are not intended to be exhaustive or to limit the implementations to the precise\nforms disclosed.  Many modifications and variations are possible in view of the above teachings.  The implementations were chosen and described in order to best explain the principles and their practical applications, to thereby enable others skilled in\nthe art to best utilize the implementations and various implementations with various modifications as are suited to the particular use contemplated.", "application_number": "15614449", "abstract": " Systems, computer-readable storage mediums, and methods for discovering\n     and displaying media content related to an electronic document. At a\n     first computing device, document identifying information--which is one\n     of: at least a portion of a network identifier or source code for the\n     electronic document--for the electronic document is obtained. One or more\n     media entities associated with the electronic document are then obtained\n     from a server. The media entities correspond to media content including\n     scheduled TV programming, streaming media or locally stored media. A\n     representation of a subset of the media entities is displayed, and a user\n     selection from the representation of a respective media entity from the\n     subset is obtained. In response, selection information identifying the\n     respective media entity is sent to a second computing device, which is\n     configured to access and display respective media content corresponding\n     to the respective media entity based on the selection information.\n", "citations": ["6757707", "6760043", "8667540", "8736764", "8776118", "20010038392", "20020010932", "20020145621", "20040117821", "20040117831", "20080028336", "20080098433", "20090198504", "20100138761", "20100205643", "20110041071", "20110099263", "20110302603", "20120084149", "20120204093", "20120257110", "20120303629", "20130117677"], "related": ["13728985", "61665259"]}, {"id": "20170364596", "patent_code": "10373075", "patent_name": "Smart suggestions for query refinements", "year": "2019", "inventor_and_country_data": " Inventors: \nWu; Xianren (Santa Clara, CA), Kanduri; Satya Pradeep (Mountain View, CA), Dialani; Vijay (Fremont, CA), Xu; Ye (Hanover, NH), Yan; Yan (San Jose, CA), Thuc Ha; Viet (Milpitas, CA), Gupta; Abhishek (San Francisco, CA), Sinha; Shakti Dhirendraji (Sunnyvale, CA)  ", "description": "<BR><BR>TECHNICAL FIELD\nThe present disclosure generally relates to computer technology for solving technical challenges in search queries to data sources.  More specifically, the present disclosure relates to smart suggestions for query refinements.\n<BR><BR>BACKGROUND\nThe rise of the Internet has occasioned two disparate phenomena: the increase in the presence of social networks, with their corresponding member profiles visible to large numbers of people, and the increase in use of social networks for job\nsearches, both by applicants and by employers.  Employers, or at least recruiters attempting to connect applicants and employers, often perform searches on social networks to identify candidates who have qualifications that make them good candidates for\nwhatever job opening they are attempting to fill.  The employers or recruiters can then contact these candidates to see if they are interested in applying for the job opening.\nTraditional querying of social networks for candidates involves the employer or recruiter entering one or more search terms to manually create the query.  A key challenge in talent search is to translate the criteria of a hiring position into a\nsearch query that leads to desired candidates.  To fulfill this goal, the searcher has to understand which skills are typically required for the position, what are the alternatives, which companies are likely to have such candidates, which schools the\ncandidates are most likely to graduate from, etc. Moreover, the knowledge varies over time.  As a result, it is not surmising that even for experienced recruiters, it often requires many searching trials in order to obtain a satisfactory query.  During\nthese searching trials, the searcher may provide various refinements to previous queries, such as adding, removing, or altering search terms, or adding, removing, or altering search filters. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nSome embodiments of the technology are illustrated, by way of example and not limitation, in the figures of the accompanying drawings.\nFIG. 1 is a block diagram illustrating a client-server system, in accordance with an example embodiment.\nFIG. 2 is a block diagram showing the functional components of a social networking service, including a data processing module referred to herein as a search engine, for use in generating and providing search results for a search query,\nconsistent with some embodiments of the present disclosure.\nFIG. 3 is a block diagram illustrating an application server module of FIG. 2 in more detail, in accordance with an example embodiment.\nFIG. 4 is a flow diagram illustrating a method for automatic generation of smart suggestions of facets to add to a query, in accordance with an example embodiment.\nFIG. 5 is a block diagram illustrating a skills generator in more detail, in accordance with an example embodiment.\nFIG. 6 is a diagram illustrating an offline process to estimate expertise scores, in accordance with another example embodiment.\nFIG. 7 is a block diagram illustrating a search results ranker in more detail, in accordance with an example embodiment.\nFIG. 8 is a block diagram illustrating a search results ranker in more detail, in accordance with another example embodiment.\nFIG. 9 is a flow diagram illustrating a method for performing an ideal candidate-based search, in accordance with an example embodiment.\nFIG. 10 is a flow diagram illustrating generating a search query based on extracted one or more attributes, in accordance with an example embodiment.\nFIG. 11 is a flow diagram illustrating a method of ranking search results using ideal candidates, in accordance with an example embodiment.\nFIG. 12 is a flow diagram illustrating a method for generating labels for sample ideal candidate member profiles, in accordance with an example embodiment.\nFIG. 13 is a flow diagram illustrating a method of dynamically training weights of a machine learning algorithm model, in accordance with an example embodiment.\nFIG. 14 is a screen capture illustrating a first screen of a user interface for performing an ideal candidate-based search in accordance with an example embodiment.\nFIG. 15 is a screen capture illustrating a second screen of the user interface for performing an ideal candidate-based search, in accordance with an example embodiment.\nFIG. 16 is a block diagram illustrating a representative software architecture, which may be used in conjunction with various hardware architectures herein described.\nFIG. 17 is a block diagram illustrating components of a machine, according to some example embodiments, able to read instructions from a machine-readable medium (e.g., a machine-readable storage medium) and perform any one or more of the\nmethodologies discussed herein.\n<BR><BR>DETAILED DESCRIPTION\nOverview\nThe present disclosure describes, among other things, methods, systems, and computer program products that individually provide various functionality.  In the following description, for purposes of explanation, numerous specific details are set\nforth in order to provide a thorough understanding of the various aspects of different embodiments of the present disclosure.  It will be evident, however, to one skilled in the art, that the present disclosure may be practiced without all of the\nspecific details.\nIn an example embodiment, a system is provided whereby refinements to a search query are automatically suggested to a searcher.  More particularly, the system is able to make these \"smart suggestions\" based on multiple facets (e.g., title,\ncompany, industry, school, location, etc.) of the search results simultaneously.\nIn some example embodiments, the smart suggestions may be utilized in conjunction with a search query that was automatically generated based on a set of input \"ideal\" candidates.  The query is then used to retrieve and/or rank results.  In this\nmanner, a searcher may list one or several examples of good candidates for a given position.  For instance, hiring managers or recruiters can utilize profiles of existing members of the team for which the position pertains.  In this new paradigm, instead\nof specifying a complex query capturing the position requirements, the searcher can simply pick up a small set of ideal candidates for the position.  The system then builds a query automatically extracted from the input candidates and searches for result\ncandidates based on this built query.  It should be noted that the embodiments in which the smart suggestions are utilized in conjunction with a search query that was automatically generated based on a set of input \"ideal\" candidates are only examples,\nand nothing in this disclosure shall be interpreted as limiting the embodiments of the smart suggestions for use with such an ideal candidate search.  Indeed, example embodiments are foreseen wherein the smart suggestions are utilized in conjunction with\na text-based search or other types of searches.\nIt should be noted that the term \"ideal\", as used throughout the present disclosure, is not intended to be any sort of measurement of desirability of a candidate.  Rather, a candidate is simply labeled as \"ideal\" if a searcher has specified the\ncandidate as a basis for the search.  In other words, if the searcher feels that the candidate is ideal enough to specify as a basis for the search, then that is enough for the candidate to be considered ideal for the systems and methods described\nherein.  There is no necessity that that the candidate actually \"be\" ideal, nor any measurement of how ideal a candidate is.\nFIG. 1 is a block diagram illustrating a client-server system 100, in accordance with an example embodiment.  A networked system 102 provides server-side functionality via a network 104 (e.g., the Internet or a wide area network (WAN)) to one or\nmore clients.  FIG. 1 illustrates, for example, a web client 106 (e.g., a browser) and a programmatic client 108 executing on respective client machines 110 and 112.\nAn application program interface (API) server 114 and a web server 116 are coupled to, and provide programmatic and web interfaces respectively to, one or more application servers 118.  The application server(s) 118 host one or more applications\n120.  The application server(s) 118 are, in turn, shown to be coupled to one or more database servers 124 that facilitate access to one or more databases 126.  While the application(s) 120 are shown in FIG. 1 to form part of the networked system 102, it\nwill be appreciated that, in alternative embodiments, the application(s) 120 may form part of a service that is separate and distinct from the networked system 102.\nFurther, while the client-server system 100 shown in FIG. 1 employs a client-server architecture, the present disclosure is, of course, not limited to such an architecture, and could equally well find application in a distributed, or\npeer-to-peer, architecture system, for example.  The various applications 120 could also be implemented as standalone software programs, which do not necessarily have networking capabilities.\nThe web client 106 accesses the various applications 120 via the web interface supported by the web server 116.  Similarly, the programmatic client 108 accesses the various services and functions provided by the application(s) 120 via the\nprogrammatic interface provided by the API server 114.\nFIG. 1 also illustrates a third party application 128, executing on a third party server 130, as having programmatic access to the networked system 102 via the programmatic interface provided by the API server 114.  For example, the third party\napplication 128 may, utilizing information retrieved from the networked system 102, support one or more features or functions on a website hosted by a third party.  The third party website may, for example, provide one or more functions that are\nsupported by the relevant applications 120 of the networked system 102.\nIn some embodiments, any website referred to herein may comprise online content that may be rendered on a variety of devices including, but not limited to, a desktop personal computer (PC), a laptop, and a mobile device (e.g., a tablet computer,\nsmartphone, etc.).  In this respect, any of these devices may be employed by a user to use the features of the present disclosure.  In some embodiments, a user can use a mobile app on a mobile device (any of the machines 110, 112 and the third party\nserver 130 may be a mobile device) to access and browse online content, such as any of the online content disclosed herein.  A mobile server (e.g., API server 114) may communicate with the mobile app and the application server(s) 118 in order to make the\nfeatures of the present disclosure available on the mobile device.\nIn some embodiments, the networked system 102 may comprise functional components of a social networking service.  FIG. 2 is a block diagram showing the functional components of a social networking service, including a data processing module\nreferred to herein as a search engine 216, for use in generating and providing search results for a search query, consistent with some embodiments of the present disclosure.  In some embodiments, the search engine 216 may reside on the application\nserver(s) 118 in FIG. 1.  However, it is contemplated that other configurations are also within the scope of the present disclosure.\nAs shown in FIG. 2, a front end may comprise a user interface module (e.g., a web server 116) 212, which receives requests from various client computing devices, and communicates appropriate responses to the requesting client devices.  For\nexample, the user interface module(s) 212 may receive requests in the form of Hypertext Transfer Protocol (HTTP) requests or other web-based API requests.  In addition, a member interaction detection module 213 may be provided to detect various\ninteractions that members have with different applications 120, services, and content presented.  As shown in FIG. 2, upon detecting a particular interaction, the member interaction detection module 213 logs the interaction, including the type of\ninteraction and any metadata relating to the interaction, in a member activity and behavior database 222.\nAn application logic layer may include one or more various application server modules 214, which, in conjunction with the user interface module(s) 212, generate various user interfaces (e.g., web pages) with data retrieved from various data\nsources in a data layer.  In some embodiments, individual application server modules 214 are used to implement the functionality associated with various applications 120 and/or services provided by the social networking service.\nAs shown in FIG. 2, the data layer may include several databases 126, such as a profile database 218 for storing profile data, including both member profile data and profile data for various organizations (e.g., companies, schools, etc.). \nConsistent with some embodiments, when a person initially registers to become a member of the social networking service, the person will be prompted to provide some personal information, such as his or her name, age (e.g., birthdate), gender, interests,\ncontact information, home town, address, spouse's and/or family members' names, educational background (e.g., schools, majors, matriculation and/or graduation dates, etc.), employment history, skills, professional organizations, and so on.  This\ninformation is stored, for example, in the profile database 218.  Similarly, when a representative of an organization initially registers the organization with the social networking service, the representative may be prompted to provide certain\ninformation about the organization.  This information may be stored, for example, in the profile database 218, or another database (not shown).  In some embodiments, the profile data may be processed (e.g., in the background or offline) to generate\nvarious derived profile data.  For example, if a member has provided information about various job titles that the member has held with the same organization or different organizations, and for how long, this information can be used to infer or derive a\nmember profile attribute indicating the member's overall seniority level, or seniority level within a particular organization.  In some embodiments, importing or otherwise accessing data from one or more externally hosted data sources may enrich profile\ndata for both members and organizations.  For instance, with organizations in particular, financial data may be imported from one or more external data sources and made part of an organization's profile.  This importation of organization data and\nenrichment of the data will be described in more detail later in this document.\nOnce registered, a member may invite other members, or be invited by other members, to connect via the social networking service.  A \"connection\" may constitute a bilateral agreement by the members, such that both members acknowledge the\nestablishment of the connection.  Similarly, in some embodiments, a member may elect to \"follow\" another member.  In contrast to establishing a connection, the concept of \"following\" another member typically is a unilateral operation and, at least in\nsome embodiments, does not require acknowledgement or approval by the member that is being followed.  When one member follows another, the member who is following may receive status updates (e.g., in an activity or content stream) or other messages\npublished by the member being followed, or relating to various activities undertaken by the member being followed.  Similarly, when a member follows an organization, the member becomes eligible to receive messages or status updates published on behalf of\nthe organization.  For instance, messages or status updates published on behalf of an organization that a member is following will appear in the member's personalized data feed, commonly referred to as an activity stream or content stream.  In any case,\nthe various associations and relationships that the members establish with other members, or with other entities and objects, are stored and maintained within a social graph in a social graph database 220.\nAs members interact with the various applications 120, services, and content made available via the social networking service, the members' interactions and behavior (e.g., content viewed, links or buttons selected, messages responded to, etc.)\nmay be tracked, and information concerning the members' activities and behavior may be logged or stored, for example, as indicated in FIG. 2, by the member activity and behavior database 222.  This logged activity information may then be used by the\nsearch engine 216 to determine search results for a search query.\nIn some embodiments, the databases 218, 220, and 222 may be incorporated into the database(s) 126 in FIG. 1.  However, other configurations are also within the scope of the present disclosure.\nAlthough not shown, in some embodiments, the social networking system 210 provides an API module via which applications 120 and services can access various data and services provided or maintained by the social networking service.  For example,\nusing an API, an application 120 may be able to request and/or receive one or more navigation recommendations.  Such applications 120 may be browser-based applications 120, or may be operating system-specific.  In particular, some applications 120 may\nreside and execute (at least partially) on one or more mobile devices (e.g., phone or tablet computing devices) with a mobile operating system.  Furthermore, while in many cases, the applications 120 or services that leverage the API may be applications\n120 and services that are developed and maintained by the entity operating the social networking service, nothing other than data privacy concerns prevents the API from being provided to the public or to certain third parties under special arrangements,\nthereby making the navigation recommendations available to third party applications 128 and services.\nAlthough the search engine 216 is referred to herein as being used in the context of a social networking service, it is contemplated that it may also be employed in the context of any website or online services.  Additionally, although features\nof the present disclosure are referred to herein as being used or presented in the context of a web page, it is contemplated that any user interface view (e.g., a user interface on a mobile device or on desktop software) is within the scope of the\npresent disclosure.\nIn an example embodiment, when member profiles are indexed, forward search indexes are created and stored.  The search engine 216 facilitates the indexing and searching for content within the social networking service, such as the indexing and\nsearching for data or information contained in the data layer, such as profile data (stored, e.g., in the profile database 218), social graph data (stored, e.g., in the social graph database 220), and member activity and behavior data (stored, e.g., in\nthe member activity and behavior database 222).  The search engine 216 may collect, parse, and/or store data in an index or other similar structure to facilitate the identification and retrieval of information in response to received queries for\ninformation.  This may include, but is not limited to, forward search indexes, inverted indexes, N-gram indexes, and so on.\nFIG. 3 is a block diagram illustrating the application server module 214 of FIG. 2 in more detail.  While in many embodiments the application server module 214 will contain many subcomponents used to perform various different actions within the\nsocial networking system 210, in FIG. 3 only those components that are relevant to the present disclosure are depicted.  Here, a server profile search component 300 works in conjunction with a client profile search component 302 to perform one or more\nsearches on member profiles stored in, for example, profile database 218.  The server profile search component 300 may be, for example, part of a larger software service that provides various functionality to employers or recruiters.  The client profile\nsearch component 302 may include a user interface and may be located on a client device.  For example, the client profile search component 302 may be located on a searcher's mobile device or desktop/laptop computer.  In some example embodiments, the\nclient profile search component 302 may itself be, or may be a part of, a stand-alone software application 120 on the client device.  In other example embodiments, the client profile search component 302 is a web page and/or web scripts that are executed\ninside a web browser on the client device.  Regardless, the client profile search component 302 is designed to accept input from the searcher and to provide visual output to the searcher.\nIn an example embodiment, the input from the client profile search component 302 includes an identification of one or more ideal candidates for a job opening.  This identification may be accomplished in many ways.  In some example embodiments,\nthe input may be an explicit identification of one or more member profiles stored in the profile database 218.  This explicit identification may be determined by the searcher, for example, browsing or otherwise locating specific profiles that the\nsearcher feels are ideal.  For example, the searcher may know the identity of individuals on a team in which the open position is available, and may navigate to and select the profiles associated with those team individuals.  In another example\nembodiment, the searcher may create one or more hypothetical \"ideal candidate\" profiles and use those as the input.  In another example embodiment, the searcher may browse or search profiles in the profile database 218 using traditional browsing or\nsearching techniques.  In some example embodiments, the explicit identification may be provided by the job poster.\nThe server profile search component 300 may contain an attribute extractor 304.  The attribute extractor 304 extracts raw attributes, including, for example, skills, companies, titles, schools, industries, etc., from the profiles of the one or\nmore ideal candidates.  These raw attributes are then passed to a query builder 306.  For each attribute type, the query builder 306 aggregates the raw attributes across the input candidates, expands them to similar attributes, and finally selects the\ntop attributes that best represent the ideal candidates.\nAfter the query is generated, in an example embodiment, the generated query may be shown to the searcher via the client profile search component 302 and the searcher may have the opportunity to edit the generated query.  This may include adding\nto or removing some attributes, such as skills and companies, in the query.  As part of this operation, a query processor 308 may perform a search on the query and present raw results to the searcher via the client profile search component 302.  These\nraw results may be useful to the searcher in determining how to edit the generated query.\nIn an example embodiment, a machine learning model is trained to make \"smart suggestions\" to the searcher as to how to modify the generated query.  The model may be trained to output suggestions based on any number of different facets, such as\ntitle, company, industry, location, school, and skill.\nUsage data can be gathered regarding actions taken by searchers when facing a suggestion--(1) add the suggestion, (2) delete the suggestion, or (3) ignore the suggestion.  Intuitively, if a searcher adds a suggestion it is probably a desired one\nand thus can be considered a positive training sample.  If the searcher deletes the suggestion it is probably not a desired one, and thus can be considered a negative training sample.  For ignored suggestions, if the suggestion is positioned lower than\nan added suggestion (e.g. \"Santa Clara University\" is positioned lower than added \"University of California, Santa Cruz\"), then it is not certain whether the suggestion is really ignored by searchers or useless in the setting of the query.  Thus, this\ndata can be ignored.  If, however, the ignored suggestion is positioned higher than an added suggestion, it can be treated as negative data.\nIn an example embodiment, various features may be analyzed to make a smart suggestion for one or more facets.  A faceted classification system classifies each information element along multiple dimensions, called facets, enabling the\nclassifications to be accessed and ordered in multiple ways rather than in a single, predetermined, taxonomic order.  Facets correspond to properties of the information elements.  They are often derived from analysis of an information element using\nentity extraction techniques or from pre-existing fields in a database 126.\nThe various features analyzed for each facet may include, for example, conditional entropy, entity affinity score, homophily score, region match, country match, and entity prior.\nReferring to conditional entropy, as discussed earlier, the smart suggestions may be made based on cross-facet suggestions.  Thus, signals from other facets may be utilized when suggesting possible values in one particular facet.  Conditional\nentropy quantifies the amount of information needed to describe the outcome of one random variable given the value of another random variable.\nIn order to utilize conditional entropy in cross-facet suggestions, a distinction is made between an exclusive facet and an inclusive facet.  For exclusive facets, such as title, the values exclude each other within the facet.  As such, the\nsystem would not be suggesting alternative values within the facet but would be suggesting values in other facets (cross-facet suggestions).  In an example embodiment, a min max framework is employed to measure the significance of adding a particular\nvalue in one of these other facets.  Here, a maximum conditional entropy gain for each of these other facets is computed by adding each value in each of these other facets.  Next, the minimum maximum gain over all of these other facets is computed.\nSpecifically: signal=min.sub.f.sub.i{max.sub.v.sub.j.sub..di-elect cons.VCondEntropyGain(v,v.sub.j)}\nHere, f.sub.i is the facet, v.sub.j is the particular value in facet f.sub.i, CondEntropyGain(v,v.sub.j) is the gain for conditional entropy by adding the value v.sub.j into the facet.  Conditional entropy may be defined as follows:\n.function..function..function..function..function..function..function..fu- nction.  ##EQU00001##\nwhere N(v.sub.i), N(v.sub.j) and N(v.sub.i, v.sub.j) are the number of members with value v.sub.i, the number of members with value v.sub.j and the number of members with both v.sub.i and v.sub.j, respectively.  Thus, the conditional entropy\ncalculation is based on the co-occurrence of values v.sub.i and v.sub.j in member profiles.\nFor inclusive facets, such as skills, the values are inclusive of others within the facet.  Therefore, for cross facet conditional entropy, the min max framework used for exclusive facets can be used, but for within-facet conditional entropy,\nthe minimum value gain across all values within the facet may be used.\nTurning now to entity affinity score, the co-occurrence signal across different facets can be an important indicator when aiming for cross-facet suggestion.  This co-occurrence between values from two facets may be termed the entity affinity\nscore feature.  For example, if \"software engineer\" is taken in a title facet and \"San Francisco Bay area\" is taken in a location facet as an example, first the co-occurrences between \"software engineer\" and \"San Francisco Bay area\" in member profiles\nare counted.  Then the count is normalized by the maximum count of \"software engineer\" and \"San Francisco Bay Area\" as the feature value.\nSpecifically, given facet value Qi, the entity affinity between facet value Qi and potential facet value recommendation Rj can be measured as\n.function..times..intg..times..intg..times..function..times..function..ti- mes..function..times..function.  .times..intg..times..intg..times..function..times..function..times..funct- ion..fwdarw..times..function..times..function.  ##EQU00002##\nWhere I(ml, Qi) is the identity function defined as member ml having the specific attribute Qi.\nTurning now to homophily score, this score takes into account the location of searchers in candidate discovery.  Thus, this score is an indicator of how close the searcher's location is to each possible search result's location.\nRegion match is an indicator as to whether a region specified in a current query matches regions of possible search result locations.  Likewise, country match is an indicator as to whether the countries specified in a query match countries of\npossible search result locations.\nEntity prior is a measure of usage of various candidate search results.  Specifically, if a potential search result does not have a lot of prior hits from previous displays of the potential search result, there may be a bias towards not\nincluding facets related to that search result as smart suggestions.\nFIG. 4 is a flow diagram illustrating a method 400 for automatic generation of smart suggestions of facets to add to a query, in accordance with an example embodiment.  At operation 402, the query is executed on a database 126, producing a first\nplurality of search results.  At operation 404, the first plurality of search results is expanded to include entries in the database 126 that are similar to the first plurality of search results.  Similarity may be based on a number of different\ncriteria.  In one example embodiment, a result is similar if it shares at least some minimum number of threshold facet values with a search result in the first plurality of search results.  For example, if the search results are candidate profiles,\nsimilar profiles may be identified by looking for profiles sharing at least 3 facet values with any candidate profile in the first plurality of search results.\nThen a loop is begun to extract features from the query and from the expanded first plurality of search results.  The loop cycles through each potential value (v.sub.j) in each potential facet (f.sub.j) that can be suggested based on each value\n(v.sub.i) for each facet (f.sub.i) in the query.  Thus, the loop begins with a first potential value value (v.sub.j) for a first potential facet (f.sub.j) that can be suggested based on a first value (v.sub.i) in a first facet (f.sub.i) in the query.  At\noperation 406, the conditional entropy gain for v.sub.i, v.sub.j is calculated.  This calculation was described in more detail earlier.\nAt operation 408, co-occurrences between v.sub.i, v.sub.j are calculated and normalized to an entity affinity score.  At operation 410, it is determined if the potential value v.sub.j is a location.  If so, then at operation 412, it is\ncalculated how closely this location matches searcher location (homophily score), a region of value v.sub.i, and a country of value v.sub.i.\nAt operation 414, entity prior is calculated based on whether potential results having potential value v.sub.j in potential facet f.sub.j have historically very few clicks.\nAt operation 416, it is determined if there are any more potential values in the potential facet.  If so, then the method 400 loops back to operation 406 for the next potential value v.sub.j in potential facet f.sub.j.  If not, then at operation\n418, the potential value v.sub.j in potential facet f.sub.j that has the highest conditional entropy gain is determined.  At operation 420, it is determined if there are any more potential facets to suggest based on each value (v.sub.i) for each facet\n(f.sub.i) in the query.  If so, then the method 400 loops back to operation 406 for the first potential value in the next potential facet.  If not, then at operation 422, the potential facet with the minimum maximum conditional entropy gain of the facets\nto suggest based on each value (v.sub.i) for each facet (f.sub.i) in the query is determined.\nAt operation 424, the minimum maximum conditional entropy gain, and features calculated based on the entity affinity score, homophily score, matching region of value, matching country of value, and entity prior for each combination of v.sub.i\nare v.sub.j are used as input to a machine learning model trained to suggest a facet to add to the query based on this input.\nIt should be noted that, in some example embodiments, if multiple smart suggestions are to be suggested, a separate machine learning model may be used to determine the ranking of how these smart suggestions are to be presented to the searcher\nfor possible inclusion in the query.  The features used for this separate machine learning model may include, for example, some of the features for the machine learning model used to make the smart suggestions, but may include additional features either\nin lieu of or in addition to those previous features, including school affinity score, company affinity score, industry affinity score, title affinity score, geographical affinity score, skill affinity score, and similarity of industry type.\nReferring back to FIG. 3, after the query is modified, the query processor 308 may refresh the search results.  A search results ranker 310 may act to rank the search results, taking into account both the query (including potentially the\ngenerated query and the modified generated query) as well as the input ideal candidates when ranking the search results.\nReferring back to the query builder 306, given the raw attributes from the profiles of the ideal candidates, the query builder 306 generates a query containing skills, companies, titles, etc. that best represents the ideal candidates.\nThe query builder 306 may comprise a skills generator 312 designed to generate skills to be added to the generated query.  The social networking service may allow members to add skills to their profiles.  Typical examples of skills that, for\nexample, an information technology (IT) recruiter might search could be \"search,\" \"information retrieval,\" \"machine learning,\" etc. Members may also endorse skills of other members in their network 104 by, for example asserting that the member does\nindeed have the specified skills.  Thus, skills may be an important part of members' profiles that showcase their professional expertise.  A technical challenge encountered, however, is that ideal candidates may not explicitly list all of the skills they\nhave on their profiles.  Additionally, some of their skills may not be relevant to their core expertise.  For example, an IT professional may list \"nonprofit fundraising\" as a skill.\nTo overcome these challenges, expertise scores for the ideal candidate may be estimated based on explicit skills (skills the member has explicitly listed) as well as implicit skills (skills the member is likely to have, but has not explicitly\nlinked).\nFIG. 5 is a block diagram illustrating the skills generator 312 in more detail, in accordance with an example embodiment.  As shown in FIG. 5, a scoring apparatus 500 may calculate a set of expertise scores 502 using a statistical model 504 and\na set of features 506-408 for candidate member profiles.  Features 506-508 may be aggregated into a data repository 510 from the member profiles and/or user actions.  For example, features 506-508 may be received from a number of servers and/or data\ncenters associated with the websites and/or applications 120 and stored in a relational database 126 for subsequent retrieval and use.\nPrior to calculating expertise scores 502 on actual member profiles, a training apparatus 512 may obtain training data for statistical model 504, which includes a positive class 514 and a negative class 516.  Positive class 514 may include data\nassociated with items of a particular category (e.g., trait, attribute, dimension, etc.), while negative class 516 may include data associated with items that do not belong in the category.\nFor example, statistical model 504 may be a logistic regression model that classifies each member profile as either an expert or a non-expert in a corresponding skill.  Positive class 514 may thus include a subset of features 506-508 associated\nwith members with known expertise in one or more skills.  Such \"expert\" members may be identified based on publications, speeches, awards, and/or contributions of the users in their respective fields.  On the other hand, negative class 516 may include a\nsubset of features 506-508 associated with members who are not recognized as experts in their respective fields, such as random members who list a given skill in their profiles.  Because far fewer users belong in positive class 514 than negative class\n516, positive class 514 may be oversampled to produce a roughly class-balanced set of training data for statistical model 504.\nNext, training apparatus 512 may use positive class 514 and negative class 516 to train statistical model 504.  For example, training apparatus 512 may use maximum-likelihood estimation (MLE) and/or another estimation technique to estimate the\nparameters of a logistic regression model for calculating expertise scores 502.  After training of the logistic regression model is complete, the parameters may be set so that the logistic regression model outputs values close to 1 for training data in\npositive class 514 and values close to 0 for training data in negative class 516.\nThe trained statistical model 504 may be provided to scoring apparatus 500, which calculates expertise scores 502 for member profiles not included in the training data (such as ideal member profiles supplied by the searcher) by applying\nstatistical model 504 to features (e.g., features 506-508) for each of the items.  For example, a feature vector may be generated for each item from a subset of features 506-508 in data repository 510, and statistical model 504 may be applied to the\nfeature vector to calculate an expertise score for the item with respect to a dimension of the member profile.\nFeatures 506-508 used in the calculation of expertise scores 502 may include demographic features, social features, and behavioral features.  Demographic features may include data related to a user's location, age, experience, education, and/or\nbackground; social features may include features related to the behavior of other users with respect to the user; and behavioral features may include features related to the user's actions or behavior with the online professional network and/or related\nwebsites or applications 120.\nFIG. 6 is a diagram illustrating an offline process 600 to estimate expertise scores 502, in accordance with another example embodiment.  A supervised machine learning algorithm combines various signals 602, such as skill-endorsement graph page\nrank, skill-profile textual similarity, member seniority, etc. to estimate the expertise score.  After this step, a formed expertise matrix 604 is very sparse since only a small percentage of the pairs can be predicted with any degree of certainty. \nExpertise matrix 604 may be factored into member matrix 606 and skill matrix 608 in K-dimensional latent space.  Then, the dot-product of the expertise matrix 604 and skill matrix 608 is computed to fill in the \"unknown\" cells.  The intuition is that the\nmore members who list two particular skills in their corresponding member profiles (called co-occurrence of skills), the more likely it is that a member only listing one of those skills also has the other skill as a latent skill.  Since the dot-product\nresults in a large number of non-zero scores of each member on the skills, the scores can then be thresholded such that if the member's score on a skill is less than a particular threshold, the member is assumed not to know the skill and is assigned a\nzero expertise score on the skill.  Thus, the final expertise matrix 610 is still sparse, but relatively much denser than formed expertise matrix 604.\nReferring back to FIG. 3, at run time, given a set of input ideal candidates IC, the skills generator 312 ranks the skills for the group of ideal candidates using the formula:\n.function..di-elect cons..times..times..function.  ##EQU00003##\nThe top N skills are then selected to represent the ideal candidates.  Expertise scores 502 of an ideal candidate on outlier skills are zero or very low, thus these skills are unlikely to be selected.  Moreover, by taking the sum over all\ncandidates, the skills which many candidates have are boosted, thus representing the commonality of the skill set among all ideal candidates.\nTurning now to companies, given the ideal candidate profiles, beside their own companies, the query builder 306 can generate a set of companies which are likely to have candidates similar to the ideal candidates in the ideal candidate profiles. \nIn order to accomplish this, the query builder 306 contains a company generator 314, which uses collaborative filtering to find company relationships.  Specifically, a company browse map using co-viewing relationships (people who view company A and also\nview company B) may be utilized.  Intuitively, companies co-viewed by highly overlapped sets of people are likely to be similar.  Thus, activity and/or usage information for searchers/browsers within the social networking service may be retrieved and\nmined to construct the company browse map, and this browse map may then be used to find the company relationships by the company generator 314.  Other information may be used either in conjunction with or in lieu of the company browse map.  For example,\nthe social networking service may keep track of candidates that apply to a given company.  Therefore, it may deduce that if a user who applied to company B also applied to company A, then company A and company B are similar.  This similarity relationship\nmay be used like the browse map is used to generate companies related to companies identified in profiles of ideal candidates.  Another signal that may be used is company movement, meaning that if a large number of people who left company A went to work\nfor company B, this might imply that company A and company B are somewhat similar.\nSimilar strategies can be used for other facets of a query.  For example, title, industry, locations, and schools can all be expanded from those facets in the idea candidate profiles by finding similar facets using, for example, browse maps.\nOnce the query builder 306 completes building the query based on the techniques described above, the query may be submitted to a search engine 216 to return search results.  The hope, of course, is that the search results represent candidates\nwho are similar in some ways to the ideal candidates submitted by the searcher, thus alleviating the searcher of the burden of composing the query.  Once the results are returned, a search results ranker 310 may rank the search results according to one\nor more ranking algorithms.  A subset of the top ranked search results may then be displayed to the searcher using a results display component 316.  In an example embodiment, the results display component 316 interacts with the client profile search\ncomponent 302 to facilitate such a display.  The number of top ranked search results displayed may vary based on, for example, current size of a display window, font size, user preferences, etc.\nWhile any ranking algorithms may be used by the search results ranker 310 to rank the search results, in an example embodiment, a machine learning algorithm is used to train a ranking model specifically to be used with searches generated by\nsearchers providing ideal candidates in lieu of text-based keywords.  Given the significant difference between a search by ideal candidates and a traditional query-based search, this algorithm helps provide rankings that accommodate this new type of\nsearch.\nFIG. 7 is a block diagram illustrating the search results ranker 310 in more detail, in accordance with an example embodiment.  The search query that produced the search results, as well as the search results, may be fed to a query-based feature\nproducer 700, which produces a set of query-based features 702 of the results.  Query-based features 702 include search engine features such as term frequency-inverse document frequency (TF-IDF), term location in document, bag-of-words, etc. These\nquery-based features 702 may be fed to a query-based ranking model 704, which returns scores for each of the query/result pairs.\nSeparately, an ideal candidate (IC)-based feature producer 706 receives as input the specified ideal candidate(s) and the search results from the query generated by the ideal candidate(s).  The ideal candidate (IC)-based feature producer 706\nthen produces a set of ideal candidate-based features 708 of the results.  Ideal candidate-based features 708 include features that are based on a comparison of ideal candidates and the search results (each feature measures one ideal candidate/search\nresult pair).  Example candidate-based features include similar career path, skill similarity, headline matching, headline similarity, and browse map similarity.\nSimilar career path is a measure of a trajectory similarity between the positions held by the ideal candidate and the search result.  Thus, for example, if the ideal candidate started as an intern, was promoted to a staff engineer, and then was\npromoted to project manager, a search result having a similar progression of the trajectory of their career path would rank higher in this feature than, for example, a search result who started off at the top (e.g., as a project manager).  To capture the\ntrajectory information, each member profile may be modeled as a sequence of nodes, each of which records all information within a particular position of a member's career, such as company, title, industry, time duration, and keyword summary.\nAt the node (position) level, similarity can then be ascertained by using a generalized linear model, although in other embodiments other approaches could be substituted.  Then, at the sequence (profile) level, a sequence alignment method may be\nemployed to find an optimal or near-optimal alignment between pairs of nodes from the two career paths.\nVarious schemes may be used to model the node corresponding to a job position, including sequence of positions and sequence of compositions.  In the sequence of positions scheme, each node represents one particular position of the member's\nprofessional experience.  In the sequence of compositions scheme, for each node, in addition to using position information, transition information is also incorporated between the given position and the previous one.  In other words, the position\ninformation, along with transition-related information, together comprise the node.  Transition information, such as whether title changes in this transition, whether company changes, how the seniority changes, and the time in this transition, enhances\nthe representation of this scheme by further disclosing information of the changing trend between a previous and a given position.\nWhen evaluating the similarity between two career paths, each node is a representation of one particular work experience.  In order to compute the overall similarity between two career sequences, the score may be decomposed into the sum of the\nsimilarity between several pairs of aligned nodes from the two sequences respectively.  A sequence alignment algorithm may be used to measure the sequence level similarity by calculating the sum of the optimal alignment of node pairs.  The two sequences\nmay be aligned incrementally.  The sequence alignment scheme can be formulated as a dynamic programming procedure.\nSuppose there are two career sequences P1=[X1;X2; ;Xm] and P2=[Y1;Y2; ;Yn].  (Xi and Yj are position/composition nodes from two career sequences respectively.) Further, a step of aligning subsequences P1[1:i-1] and subsequence P2[1: j-1] may be\nencountered.  (In other words, shorter subsequences have been aligned previously.) The subsequences P1[1:i] and P2[1:j] can be aligned in three ways according to the following cases:\n(1) The node Xi is similar to node Yj.  This leads to this pair of positions being aligned and results in an overall increase in sequence similarity score as contributed by this node similarity value.  Here, P1[1:i] represents the subsequence\nX1, X2, .  . . , Xi from career sequence P1.\n(2) The node Xi is not very similar to node Yj.  Thus, Xi will be skipped.  Note that although a node is allowed to be skipped during sequence alignment, contiguous alignment may be desirable for the purpose of career path completeness. \nTherefore, a gap penalty may be imposed on sequence level similarity score when skipping a node.\n(3) And vice versa: if the node Xi is not very similar to node Yj, the same gap penalty may be imposed.\nIt should be noted that the position-level similarity function employed may be symmetric.  Hence, S.sup.node(X_i, Y_j) is the same as S.sup.node(Y_j, X_i).  More formally, given the above two career sequences P1 and P2, the similarity between\ntwo career sequences can be solved using the following scheme:\n.function..function..times..times..times..function..times..times..times..- times..function..function..times..times..times..function..times..times..ti- mes..function..function..function..times..times..times..function..times..t-\nimes..times..lamda..function..function..times..times..times..function..tim- es..times..times..lamda.  ##EQU00004## Therein, S.sup.seq is the similarity function at the career sequence level, S.sup.node is the similarity function at the\nposition/composition node level, and .lamda.  is the gap penalty parameter.\nA similarity model may be learned at the node level by using, for example, a logistic regression model.  Features relevant to this model may include, for example, current title, current company, current company size, current industry, current\nfunctions, job seniority, current position summary, title similarity, company similarity, industry similarity, duration difference between positions, whether two transitions were within the same company, whether two transitions were in the same industry,\nwhether seniority changed, whether the title changed, and duration of time between the two transitions.\nSkill similarity is a measure of similarity of the skill set of the ideal candidate and the skill set of the search result.  It should be noted that skill sets may include skills that are explicit (e.g., specified by the member in their member\nprofile) or implicit (e.g., skills that are similar to skills specified by the member in their member profile, but not explicitly listed).\nHeadline matching is a measure of the similarity between the query and the headline of each result.  Notably, this is based on a text-based comparison, and is not strictly ideal candidate-based.  A headline is one or more visible fields (along\nwith name) displayed as a search result snippet for a search result.  While the concept of creating snippets for each search result is a topic that is beyond the scope of the present disclosure, such snippets often include a headline that helps explain\nwhy the result is relevant and likely to trigger actions from the searcher.  The headline matching feature, therefore, measures the similarity between the query and this headline from the search result's snippet.\nHeadline similarity is a measure of the similarity between a headline of the ideal candidate and the headline of the search result.  This similarity calculation may be performed with or without considering word semantics.  In example embodiments\nwhere word semantics are not considered, a word2vec algorithm may be utilized.  Word2vec is a group of related models used to produce word-embeddings.  The word-embeddings are shallow, two-layer neural networks that are trained to reconstruct linguistic\ncontexts of words.  The neural network is shown a word and guesses which words occurred in adjacent position in an input text.  After training, word2vec models can be used to map each word to a vector of typically several hundred elements, which\nrepresent that word's relation to other words.\nBrowse map similarity is a measure of whether and how much other members/searchers/browsers visited both the ideal candidate's profile and the search result's profile in the same browsing session.  The intuition is that if previous\nmembers/searchers/browsers viewed both profiles in the same session, then there is a higher likelihood that the profiles are similar, and thus that the underlying ideal candidate and search result are similar.\nThe ideal candidate-based features 708 may be fed along with the scores from the query-based ranking model 704 to a machine learning algorithm 710.  The machine learning algorithm 710 is designed to train a combined ranking model 712 that is\ncapable of determining a ranking score for a search result at runtime.  This training may use labels supplied for training data (e.g., training ideal candidates and training search results along with labeled scores for each).  The training may involve\nthe machine learning algorithm 710 learning which features/scores are more or less relevant to the ranking scores, and appropriately weighting such features and scores for runtime computations.  At runtime, a feature extractor 714 extracts both\nquery-based and ideal candidate-based features from the query, search results, and ideal candidates and feeds these features to the combined ranking model 712, which produces the scores as per its model.  A ranker 716 then uses these ranking scores to\nrank the search results for display to the searcher.\nIt should be noted that since searching by ideal candidates is a new concept, it is difficult to generate labeled data directly from a log of previous search systems, as would typically be done to generate labeled data.  Instead, in an example\nembodiment, labeled data is generated from the log of a query-based search.  One such log is a log of electronic communications performed after the search.  For example, if a searcher sees 20 results to a query-based search for candidates, and sends\nemail communications to 8 candidates from the 20 results, then it may be assumed that these 8 candidates are similar enough to be considered for the same job, and thus if a profile for one or more of those 8 candidates had been submitted for a search by\nideal candidate, the other candidates could be considered likely top results.  In an example embodiment, other actions taken with respect to previous search results may be logged and similarly used to determine ideal candidate matches.  For example,\nwhile communication with a candidate may be considered as strongly indicative of a match for the underlying position (and thus a match with other candidates also emailed for the same position) and assigned a high relevance score, clicking on a candidate\n(without an email) may be considered to be a partial match and may be assigned a moderate relevance score, while skipped results might be considered a low relevance score.  The relevance scores may be used as the labels for the sample data.\nThus, in an example embodiment, communications between searchers and members of the social network service are monitored and logged and these communications are used to derive a label score for each sample search result/ideal candidate pair (the\nsample search results may simply be the search results presented in response to previous queries).  The label score may be generated using various combinations of the metrics described above.  For example, if the same searcher communicated with both\ncandidates A and B in response to the same search query, then candidate B is assigned a score of 5 (on a scale of 1 to 5, 5 being most relevant) for an ideal candidate A and candidate A is assigned a score of 5 for an ideal candidate B. Actions such as\nclicking on a candidate that indicate a moderate relevance may be assigned a score of 3 and no actions may be assigned a score of 1.  Scores for various log entries can then be combined and averaged.  The result is profile pairs that have been assigned a\nscore of between 1 and 5 based on previous actions or inactions by previous searchers.  These label scores may then be used as labels for hypothetical ideal candidate/search result pairs for those same member profiles.\nIn an example embodiment, a dynamic weight trainer is introduced into the architecture of FIG. 7 in order to dynamically alter the weights assigned to the IC-based features 708.  Specifically, a search query need not be limited to a single query\nand then the search is complete.  Often the searcher may interact with the original query and search result to provide additional refinements of the original search.  This is true not only with traditional text-based searches but also can be true with\nideal candidate-based searches as well.  This may be accomplished by the searcher applying additional filters and or making text-based additions to the initial ideal candidate-based search to refine the results.  The result is that the role of the ideal\ncandidate-based features, which directly measure the similarity between the ideal candidate(s) and the search results, become less and less important as the search is refined.\nAt the same time, as the search session continues, the confidence of the remaining attributes e.g., query-based attributes) increases in usefulness.\nFIG. 8 is a block diagram illustrating the search results ranker 310 in more detail, in accordance with another example embodiment.  FIG. 8 is identical to FIG. 7 with the exception of the addition of a dynamic weight trainer 800.  The purpose\nof the dynamic weight trainer 800 is to dynamically alter the weights of the features extracted to favor the query-based features 702 over the ideal candidate-based features 708 over time.  This may be performed by applying a decay function, defined on\nsome measure of session length, such as the number of query refinements, to gradually reduce the weights of the ideal candidate-based features 708 and/or increase the weights of the query-based features 702.  This function controls the dynamic balance\nbetween the impacts of the ideal input candidates and the query on the result ranking.\nFIG. 9 is a flow diagram illustrating a method 900 for performing an ideal candidate-based search, in accordance with an example embodiment.  At operation 902, one or more ideal candidate documents may be obtained.  In an example embodiment,\nthese documents are member profiles in a social networking service and they are obtained by a searcher specifying the corresponding members and the member profiles being retrieved from a database 126 based on the searcher's specified members.  However,\nimplementations are possible where the documents obtained are not member profiles.\nAt operation 904, one or more attributes are extracted from the one or more ideal candidate documents.  At operation 906, a search query is generated based on the extracted one or more attributes.  At operation 908, a search is performed on\ndocuments using the generated search query, returning one or more result documents.  Like with the ideal candidate documents, the result documents may also be member profiles in a social networking service.\nFIG. 10 is a flow diagram illustrating generating a search query based on extracted one or more attributes, in accordance with an example embodiment.  FIG. 10 corresponds to operation 906 of FIG. 9 in more detail.  At operation 1000, the one or\nmore attributes are aggregated across the one or more ideal candidate documents.  At operation 1002, the aggregated one or more attributes are expanded to include similar attributes.  At operation 1004, top attributes most similar to attributes of all of\nthe one or more ideal candidate documents are selected.  At operation 1006, a set of expertise scores 502 are calculated using a statistical model 504 and a set of features regarding skills of the one or more candidate documents.  The statistical model\n504 may be a logistic regression model trained using a machine learning algorithm 710.  At operation 1008 the expertise scores 502 are used to rank skills of the one or more ideal candidate member profiles, using the top attributes.  At operation 1010,\none or more top ranked skills are added to the search query.\nAt operation 1012, a browse map is referenced.  At operation 1014, one or more companies are added to the search query, the companies being ones who have been co-viewed during the same browsing session as a company identified in one or more of\nthe ideal candidate documents, by using the browse map.\nFIG. 11 is a flow diagram illustrating a method 1100 of ranking search results using ideal candidates in accordance with an example embodiment.  At operation 1102, one or more ideal candidate documents may be obtained.  In an example embodiment,\nthese documents are member profiles in a social networking service and they are obtained by a searcher specifying the corresponding members and the member profiles being retrieved from a database 126 based on the searcher's specified members.  However,\nimplementations are possible where the documents obtained are not member profiles.\nAt operation 1104, a search is performed using a search query, resulting one or more result documents.  Like with the ideal candidate documents, the result documents may be member profiles in an example embodiment.  In one example embodiment,\noperation 1104 can be performed using some of the operations described above with respect to FIGS. 9 and 10.\nAt operation 1106, one or more query-based features 702 are produced from the one or more result documents using the search query.  As described above, this may include features such as TF-IDF.\nAt operation 1108, one or more ideal candidate-based features may be produced from the one or more result documents using the one or more ideal candidate documents.  As described above, the ideal candidate-based features may include similar\ncareer path, skill similarity, headline matching, headline similarity, and/or browse map similarity.\nAt operation 1110, the one or more query-based features 702 and the one or more ideal candidate-based features are input to a combined ranking model 712, outputting ranking scores for each of the one or more result member profiles.  The combined\nranking model 712 may be trained using similar query-based and ideal candidate-based features from sample result documents as well as sample search queries and labels.\nAt operation 1112, the one or more result documents are ranked based on the ranking score.  At operation 1114, display of the one or more top ranked result documents on a computer display is caused.\nFIG. 12 is a flow diagram illustrating a method 1200 for generating labels for sample ideal candidate member profiles, in accordance with an example embodiment.  At operation 1202, one or more sample ideal candidate member profiles in a social\nnetworking service are obtained.  At operation 1204, one or more sample search result member profiles in the social networking service are obtained.  At operation 1206, for each unique pair of sample ideal candidate member profile and sample search\nresult member profile, a label is generated using a score generated from log information of the social networking service.  The log information includes records of communications between a searcher and members of the social networking service, the score\nbeing higher if the searcher communicated with both the member corresponding to the sample ideal candidate member profile and the member corresponding to the sample search result member profile in a same search session.  The log information may further\ninclude records of user input by the searcher, the user input causing interaction with member profiles in the social networking service but not resulting in communications between the searcher and the member of the social networking service corresponding\nto both the sample ideal candidate member profile and the sample search result member profile in the same search session.  An example would include clicking on member profiles and viewing the member profiles but not emailing the corresponding members.  A\nsearch session may be defined in a number of different ways.  In one example embodiment, a search session is the same as a browsing session (e.g., as long as the searcher is logged in to the social networking service).  In another example embodiment, the\nsearch session is limited to a period of time between a searcher initiating a search and the searcher submitting an unrelated search or logging off the social networking service.\nAt operation 1208, the generated labels are fed into a machine learning algorithm 710 to train a combined ranking model 712 used to output ranking scores for search result member profiles.\nFIG. 13 is a flow diagram illustrating a method 1300 of dynamically training weights of a machine learning algorithm model in accordance with an example embodiment.  At operation 1302, one or more ideal candidate documents are obtained.  At\noperation 1304, a search is performed using a search query, returning one or more result documents.  This search query may or may not have been generated using the one or more ideal candidate documents.\nAt operation 1306, one or more query-based features 702 are produced from the one or more result documents using the search query.  At operation 1308, one or more ideal candidate-based features are produced from the one or more result documents\nusing the one or more ideal candidate documents.  At operation 1310, the one or more query-based features 702 and the one or more ideal candidate-based features are input to a combined ranking model 712.  The combined ranking model 712 is trained by a\nmachine learning algorithm 710 to output a ranking score for each of the one or more result documents.  The combined ranking model 712 includes weights assigned to each of the one or more query-based features 702 and each of the one or more ideal\ncandidate-based features.\nAt operation 1312, the one or more result documents are ranked based on the ranking scores.  At operation 1314, display of one or more top ranked documents on a computer display is caused.  At operation 1316, one or more refinements to the\nsearch are received.  At operation 1318, the weights assigned to each of the one or more query-based features 702 are dynamically trained to increase as more refinements are received, and the weights assigned to each of the one or more ideal\ncandidate-based features are dynamically trained to decrease as more refinements are received.  This dynamic training may utilize a decay function based on, for example, time or number of refinements.\nFIG. 14 is a screen capture illustrating a first screen 1400 of a user interface for performing an ideal candidate-based search, in accordance with an example embodiment.  The first screen 1400 includes an area 1402 where a searcher can specify\none or more ideal candidates for the search.\nFIG. 15 is a screen capture illustrating a second screen 1500 of the user interface for performing an ideal candidate-based search in accordance with an example embodiment.  The second screen 1500 presents results 1502 of the search, as well as\ndisplaying the query generated using the specified ideal candidates, the query used for the search.  The query may be displayed by highlighting terms of the query in various categories.  For example, \"software engineer\" 1504 is a job title that was\ngenerated for the query, \"python\" 1506 is a skill that was generated for the query, and \"Internet\" 1508 is an industry that was generated for the query.  The searcher can then easily modify the query by adding additional terms to the query and/or\nremoving some of the identified terms that had been previously generated.\nModules, Components, and Logic\nCertain embodiments are described herein as including logic or a number of components, modules, or mechanisms.  Modules may constitute either software modules (e.g., code embodied on a machine-readable medium) or hardware modules.  A \"hardware\nmodule\" is a tangible unit capable of performing certain operations and may be configured or arranged in a certain physical manner.  In various example embodiments, one or more computer systems (e.g., a standalone computer system, a client computer\nsystem, or a server computer system) or one or more hardware modules of a computer system (e.g., a processor or a group of processors) may be configured by software (e.g., an application 120 or application portion) as a hardware module that operates to\nperform certain operations as described herein.\nIn some embodiments, a hardware module may be implemented mechanically, electronically, or any suitable combination thereof.  For example, a hardware module may include dedicated circuitry or logic that is permanently configured to perform\ncertain operations.  For example, a hardware module may be a special-purpose processor, such as a field-programmable gate array (FPGA) or an application specific integrated circuit (ASIC).  A hardware module may also include programmable logic or\ncircuitry that is temporarily configured by software to perform certain operations.  For example, a hardware module may include software executed by a general-purpose processor or other programmable processor.  Once configured by such software, hardware\nmodules become specific machines 1700 (or specific components of a machine 1700) uniquely tailored to perform the configured functions and are no longer general-purpose processors.  It will be appreciated that the decision to implement a hardware module\nmechanically, in dedicated and permanently configured circuitry, or in temporarily configured circuitry (e.g., configured by software) may be driven by cost and time considerations.\nAccordingly, the phrase \"hardware module\" should be understood to encompass a tangible entity, be that an entity that is physically constructed, permanently configured (e.g., hardwired), or temporarily configured (e.g., programmed) to operate in\na certain manner or to perform certain operations described herein.  As used herein, \"hardware-implemented module\" refers to a hardware module.  Considering embodiments in which hardware modules are temporarily configured (e.g., programmed), each of the\nhardware modules need not be configured or instantiated at any one instance in time.  For example, where a hardware module comprises a general-purpose processor configured by software to become a special-purpose processor, the general-purpose processor\nmay be configured as respectively different special-purpose processors (e.g., comprising different hardware modules) at different times.  Software accordingly configures a particular processor or processors, for example, to constitute a particular\nhardware module at one instance of time and to constitute a different hardware module at a different instance of time.\nHardware modules can provide information to, and receive information from, other hardware modules.  Accordingly, the described hardware modules may be regarded as being communicatively coupled.  Where multiple hardware modules exist\ncontemporaneously, communications may be achieved through signal transmission (e.g., over appropriate circuits and buses) between or among two or more of the hardware modules.  In embodiments in which multiple hardware modules are configured or\ninstantiated at different times, communications between such hardware modules may be achieved, for example, through the storage and retrieval of information in memory structures to which the multiple hardware modules have access.  For example, one\nhardware module may perform an operation and store the output of that operation in a memory device to which it is communicatively coupled.  A further hardware module may then, at a later time, access the memory device to retrieve and process the stored\noutput.  Hardware modules may also initiate communications with input or output devices, and can operate on a resource (e.g., a collection of information).\nThe various operations of example methods described herein may be performed, at least partially, by one or more processors that are temporarily configured (e.g., by software) or permanently configured to perform the relevant operations.  Whether\ntemporarily or permanently configured, such processors may constitute processor-implemented modules that operate to perform one or more operations or functions described herein.  As used herein, \"processor-implemented module\" refers to a hardware module\nimplemented using one or more processors.\nSimilarly, the methods described herein may be at least partially processor-implemented, with a particular processor or processors being an example of hardware.  For example, at least some of the operations of a method may be performed by one or\nmore processors or processor-implemented modules.  Moreover, the one or more processors may also operate to support performance of the relevant operations in a \"cloud computing\" environment or as a \"software as a service\" (SaaS).  For example, at least\nsome of the operations may be performed by a group of computers (as examples of machines including processors), with these operations being accessible via a network 104 (e.g., the Internet) and via one or more appropriate interfaces (e.g., an API).\nThe performance of certain of the operations may be distributed among the processors, not only residing within a single machine, but deployed across a number of machines.  In some example embodiments, the processors or processor-implemented\nmodules may be located in a single geographic location (e.g., within a home environment, an office environment, or a server farm).  In other example embodiments, the processors or processor-implemented modules may be distributed across a number of\ngeographic locations.\nMachine and Software Architecture\nThe modules, methods, applications 120, and so forth described in conjunction with FIGS. 1-15 are implemented in some embodiments in the context of a machine and an associated software architecture.  The sections below describe representative\nsoftware architecture(s) and machine (e.g., hardware) architecture(s) that are suitable for use with the disclosed embodiments.\nSoftware architectures are used in conjunction with hardware architectures to create devices and machines tailored to particular purposes.  For example, a particular hardware architecture coupled with a particular software architecture will\ncreate a mobile device, such as a mobile phone, tablet device, or so forth.  A slightly different hardware and software architecture may yield a smart device for use in the \"internet of things,\" while yet another combination produces a server computer\nfor use within a cloud computing architecture.  Not all combinations of such software and hardware architectures are presented here, as those of skill in the art can readily understand how to implement the inventive subject matter in different contexts\nfrom the disclosure contained herein.\nSoftware Architecture\nFIG. 16 is a block diagram 1600 illustrating a representative software architecture 1602, which may be used in conjunction with various hardware architectures herein described.  FIG. 16 is merely a non-limiting example of a software\narchitecture, and it will be appreciated that many other architectures may be implemented to facilitate the functionality described herein.  The software architecture 1602 may be executing on hardware such as a machine 1700 of FIG. 17 that includes,\namong other things, processors 1710, memory/storage 1730, and I/O components 1750.  A representative hardware layer 1604 is illustrated and can represent, for example, the machine 1700 of FIG. 17.  The representative hardware layer 1604 comprises one or\nmore processing units 1606 having associated executable instructions 1608.  The executable instructions 1608 represent the executable instructions of the software architecture 1602, including implementation of the methods, modules, and so forth of FIGS.\n1-15.  The hardware layer 1604 also includes memory and/or storage modules 1610, which also have the executable instructions 1608.  The hardware layer 1604 may also comprise other hardware 1612, which represents any other hardware of the hardware layer\n1604, such as the other hardware illustrated as part of the machine 1700.\nIn the example architecture of FIG. 16, the software architecture 1602 may be conceptualized as a stack of layers where each layer provides particular functionality.  For example, the software architecture 1602 may include layers such as an\noperating system 1614, libraries 1616, frameworks/middleware 1618, applications 1620, and a presentation layer 1644.  Operationally, the applications 1620 and/or other components within the layers may invoke API calls 1624 through the software stack and\nreceive responses, returned values, and so forth, illustrated as messages 1626, in response to the API calls 1624.  The layers illustrated are representative in nature and not all software architectures have all layers.  For example, some mobile or\nspecial purpose operating systems 1614 may not provide a layer of frameworks/middleware 1618, while others may provide such a layer.  Other software architectures may include additional or different layers.\nThe operating system 1614 may manage hardware resources and provide common services.  The operating system 1614 may include, for example, a kernel 1628, services 1630, and drivers 1632.  The kernel 1628 may act as an abstraction layer between\nthe hardware and the other software layers.  For example, the kernel 1628 may be responsible for memory management, processor management (e.g., scheduling), component management, networking, security settings, and so on.  The services 1630 may provide\nother common services for the other software layers.  The drivers 1632 may be responsible for controlling or interfacing with the underlying hardware.  For instance, the drivers 1632 may include display drivers, camera drivers, Bluetooth.RTM.  drivers,\nflash memory drivers, serial communication drivers (e.g., Universal Serial Bus (USB) drivers), Wi-Fi.RTM.  drivers, audio drivers, power management drivers, and so forth depending on the hardware configuration.\nThe libraries 1616 may provide a common infrastructure that may be utilized by the applications 1620 and/or other components and/or layers.  The libraries 1616 typically provide functionality that allows other software modules to perform tasks\nin an easier fashion than by interfacing directly with the underlying operating system 1614 functionality (e.g., kernel 1628, services 1630, and/or drivers 1632).  The libraries 1616 may include system libraries 1634 (e.g., C standard library) that may\nprovide functions such as memory allocation functions, string manipulation functions, mathematic functions, and the like.  In addition, the libraries 1616 may include API libraries 1636 such as media libraries (e.g., libraries to support presentation and\nmanipulation of various media formats such as MPEG4, H.264, MP3, AAC, AMR, JPG, PNG), graphics libraries (e.g., an OpenGL framework that may be used to render 2D and 3D graphic content on a display), database libraries (e.g., SQLite that may provide\nvarious relational database functions), web libraries (e.g. WebKit that may provide web browsing functionality), and the like.  The libraries 1616 may also include a wide variety of other libraries 1638 to provide many other APIs to the applications 1620\nand other software components/modules.\nThe frameworks 1618 (also sometimes referred to as middleware) may provide a higher-level common infrastructure that may be utilized by the applications 1620 and/or other software components/modules.  For example, the frameworks 1618 may provide\nvarious graphic user interface (GUI) functions, high-level resource management, high-level location services, and so forth.  The frameworks 1618 may provide a broad spectrum of other APIs that may be utilized by the applications 1620 and/or other\nsoftware components/modules, some of which may be specific to a particular operating system 1614 or platform.\nThe applications 1620 include built-in applications 1640 and/or third party applications 1642.  Examples of representative built-in applications 1640 may include, but are not limited to, a contacts application, a browser application, a book\nreader application, a location application, a media application, a messaging application, and/or a game application.  The third party applications 1642 may include any of the built-in applications 1640 as well as a broad assortment of other applications. In a specific example, the third party application 1642 (e.g., an application developed using the Android.TM.  or iOS.TM.  software development kit (SDK) by an entity other than the vendor of the particular platform) may be mobile software running on a\nmobile operating system such as iOS.TM., Android.TM., Windows.RTM.  Phone, or other mobile operating systems.  In this example, the third party application 1642 may invoke the API calls 1624 provided by the mobile operating system such as the operating\nsystem 1614 to facilitate functionality described herein.\nThe applications 1620 may utilize built-in operating system 1614 functions (e.g., kernel 1628, services 1630, and/or drivers 1632), libraries 1616 (e.g., system libraries 1634, API libraries 1636, and other libraries 1638), and\nframeworks/middleware 1618 to create user interfaces to interact with users of the system.  Alternatively, or additionally, in some systems, interactions with a user may occur through a presentation layer, such as the presentation layer 1644.  In these\nsystems, the application/module \"logic\" can be separated from the aspects of the application/module that interact with a user.\nSome software architectures utilize virtual machines.  In the example of FIG. 16, this is illustrated by a virtual machine 1648.  A virtual machine 1648 creates a software environment where applications/modules can execute as if they were\nexecuting on a hardware machine (such as the machine 1700 of FIG. 17, for example).  A virtual machine 1648 is hosted by a host operating system (e.g., operating system 1614 in FIG. 16) and typically, although not always, has a virtual machine monitor\n1646, which manages the operation of the virtual machine 1648 as well as the interface with the host operating system (e.g., operating system 1614).  A software architecture executes within the virtual machine 1648, such as an operating system 1660,\nlibraries 1662, frameworks/middleware 1664, applications 1666, and/or a presentation layer 1668.  These layers of software architecture executing within the virtual machine 1648 can be the same as corresponding layers previously described or may be\ndifferent.\nExample Machine Architecture and Machine-Readable Medium\nFIG. 17 is a block diagram illustrating components of a machine 1700, according to some example embodiments, able to read instructions 1608 from a machine-readable medium (e.g., a machine-readable storage medium) and perform any one or more of\nthe methodologies discussed herein.  Specifically, FIG. 17 shows a diagrammatic representation of the machine 1700 in the example form of a computer system, within which instructions 1716 (e.g., software, a program, an application 1620, an applet, an\napp, or other executable code) for causing the machine 1700 to perform any one or more of the methodologies discussed herein may be executed.  The instructions 1716 transform the general, non-programmed machine 1700 into a particular machine programmed\nto carry out the described and illustrated functions in the manner described.  In alternative embodiments, the machine 1700 operates as a standalone device or may be coupled (e.g., networked) to other machines.  In a networked deployment, the machine\n1700 may operate in the capacity of a server machine or a client machine in a server-client network environment, or as a peer machine in a peer-to-peer (or distributed) network environment.  The machine 1700 may comprise, but not be limited to, a server\ncomputer, a client computer, a PC, a tablet computer, a laptop computer, a netbook, a set-top box (STB), a personal digital assistant (PDA), an entertainment media system, a cellular telephone, a smart phone, a mobile device, a wearable device (e.g., a\nsmart watch), a smart home device (e.g., a smart appliance), other smart devices, a web appliance, a network router, a network switch, a network bridge, or any machine capable of executing the instructions 1716, sequentially or otherwise, that specify\nactions to be taken by the machine 1700.  Further, while only a single machine 1700 is illustrated, the term \"machine\" shall also be taken to include a collection of machines 1700 that individually or jointly execute the instructions 1716 to perform any\none or more of the methodologies discussed herein.\nThe machine 1700 may include processors 1710, memory/storage 1730, and I/O components 1760, which may be configured to communicate with each other such as via a bus 1702.  In an example embodiment, the processors 1710 (e.g., a central processing\nunit (CPU), a reduced instruction set computing (RISC) processor, a complex instruction set computing (CISC) processor, a graphics processing unit (GPU), a digital signal processor (DSP), an ASIC, a radio-frequency integrated circuit (RFIC), another\nprocessor, or any suitable combination thereof) may include, for example, a processor 1712 and a processor 1714 that may execute the instructions 1716.  The term \"processor\" is intended to include multi-core processors 1710 that may comprise two or more\nindependent processors 1710 (sometimes referred to as \"cores\") that may execute instructions 1716 contemporaneously.  Although FIG. 17 shows multiple processors 1710, the machine 1700 may include a single processor 1712 with a single core, a single\nprocessor 1712 with multiple cores (e.g., a multi-core processor 1712), multiple processors 1710 with a single core, multiple processors 1710 with multiples cores, or any combination thereof.\nThe memory/storage 1730 may include a memory 1732, such as a main memory, or other memory storage, and a storage unit 1736, both accessible to the processors 1710 such as via the bus 1702.  The storage unit 1736 and memory 1732 store the\ninstructions 1716 embodying any one or more of the methodologies or functions described herein.  The instructions 1716 may also reside, completely or partially, within the memory 1732, within the storage unit 1736, within at least one of the processors\n1710 (e.g., within the processor's cache memory), or any suitable combination thereof, during execution thereof by the machine 1700.  Accordingly, the memory 1732, the storage unit 1736, and the memory of the processors 1710 are examples of\nmachine-readable media.\nAs used herein, \"machine-readable medium\" means a device able to store instructions 1716 and data temporarily or permanently and may include, but is not limited to, random-access memory (RAM), read-only memory (ROM), buffer memory, flash memory,\noptical media, magnetic media, cache memory, other types of storage (e.g., erasable programmable read-only memory (EEPROM)), and/or any suitable combination thereof.  The term \"machine-readable medium\" should be taken to include a single medium or\nmultiple media (e.g., a centralized or distributed database 126, or associated caches and servers) able to store the instructions 1716.  The term \"machine-readable medium\" shall also be taken to include any medium, or combination of multiple media, that\nis capable of storing instructions (e.g., instructions 1716) for execution by a machine (e.g., machine 1700), such that the instructions 1716, when executed by one or more processors of the machine (e.g., processors 1710), cause the machine 1700 to\nperform any one or more of the methodologies described herein.  Accordingly, a \"machine-readable medium\" refers to a single storage apparatus or device, as well as \"cloud-based\" storage systems or storage networks that include multiple storage apparatus\nor devices.  The term \"machine-readable medium\" excludes signals per se.\nThe I/O components 1750 may include a wide variety of components to receive input, provide output, produce output, transmit information, exchange information, capture measurements, and so on.  The specific I/O components 1760 that are included\nin a particular machine 1700 will depend on the type of machine 1700.  For example, portable machines such as mobile phones will likely include a touch input device or other such input mechanisms, while a headless server machine will likely not include\nsuch a touch input device.  It will be appreciated that the I/O components 1760 may include many other components that are not shown in FIG. 17.  The I/O components 1760 are grouped according to functionality merely for simplifying the following\ndiscussion and the grouping is in no way limiting.  In various example embodiments, the I/O components 1760 may include output components 1762 and input components 1764.  The output components 1762 may include visual components (e.g., a display such as a\nplasma display panel (PDP), a light emitting diode (LED) display, a liquid crystal display (LCD), a projector, or a cathode ray tube (CRT)), acoustic components (e.g., speakers), haptic components (e.g., a vibratory motor, resistance mechanisms), other\nsignal generators, and so forth.  The input components 1764 may include alphanumeric input components (e.g., a keyboard, a touch screen configured to receive alphanumeric input, a photo-optical keyboard, or other alphanumeric input components), point\nbased input components (e.g., a mouse, a touchpad, a trackball, a joystick, a motion sensor, or another pointing instrument), tactile input components (e.g., a physical button, a touch screen that provides location and/or force of touches or touch\ngestures, or other tactile input components), audio input components (e.g., a microphone), and the like.\nIn further example embodiments, the I/O components 1760 may include biometric components 1766, motion components 1768, environmental components 1760, or position components 1762, among a wide array of other components.  For example, the\nbiometric components 1766 may include components to detect expressions (e.g., hand expressions, facial expressions, vocal expressions, body gestures, or eye tracking), measure biosignals (e.g., blood pressure, heart rate, body temperature, perspiration,\nor brain waves), identify a person (e.g., voice identification, retinal identification, facial identification, fingerprint identification, or electroencephalogram based identification), and the like.  The motion components 1768 may include acceleration\nsensor components (e.g., accelerometer), gravitation sensor components, rotation sensor components (e.g., gyroscope), and so forth.  The environmental components 1760 may include, for example, illumination sensor components (e.g., photometer),\ntemperature sensor components (e.g., one or more thermometers that detect ambient temperature), humidity sensor components, pressure sensor components (e.g., barometer), acoustic sensor components (e.g., one or more microphones that detect background\nnoise), proximity sensor components (e.g., infrared sensors that detect nearby objects), gas sensors (e.g., gas detection sensors to detect concentrations of hazardous gases for safety or to measure pollutants in the atmosphere), or other components that\nmay provide indications, measurements, or signals corresponding to a surrounding physical environment.  The position components 1762 may include location sensor components (e.g., a Global Position System (GPS) receiver component), altitude sensor\ncomponents (e.g., altimeters or barometers that detect air pressure from which altitude may be derived), orientation sensor components (e.g., magnetometers), and the like.\nCommunication may be implemented using a wide variety of technologies.  The I/O components 1760 may include communication components 1764 operable to couple the machine 1700 to a network 1780 or devices 1770 via a coupling 1782 and a coupling\n1772, respectively.  For example, the communication components 1764 may include a network interface component or other suitable device to interface with the network 1780.  In further examples, the communication components 1764 may include wired\ncommunication components, wireless communication components, cellular communication components, near field vommunication (NFC) components, Bluetooth.RTM.  components (e.g., Bluetooth.RTM.  Low Energy), Wi-Fi.RTM.  components, and other communication\ncomponents to provide communication via other modalities.  The devices 1770 may be another machine or any of a wide variety of peripheral devices (e.g., a peripheral device coupled via a USB).\nMoreover, the communication components 1764 may detect identifiers or include components operable to detect identifiers.  For example, the communication components 1764 may include radio frequency identification (RFID) tag reader components, NFC\nsmart tag detection components, optical reader components (e.g., an optical sensor to detect one-dimensional bar codes such as Universal Product Code (UPC) bar code, multi-dimensional bar codes such as Quick Response (QR) code, Aztec code, Data Matrix,\nDataglyph, MaxiCode, PDF417, Ultra Code, UCC RSS-2D bar code, and other optical codes), or acoustic detection components (e.g., microphones to identify tagged audio signals).  In addition, a variety of information may be derived via the communication\ncomponents 1764, such as location via Internet Protocol (IP) geolocation, location via Wi-Fi.RTM.  signal triangulation, location via detecting an NFC beacon signal that may indicate a particular location, and so forth.\nTransmission Medium\nIn various example embodiments, one or more portions of the network 1780 may be an ad hoc network, an intranet, an extranet, a virtual private network (VPN), a local area network (LAN), a wireless LAN (WLAN), a WAN, a wireless WAN (WWAN), a\nmetropolitan area network (MAN), the Internet, a portion of the Internet, a portion of the Public Switched Telephone Network (PSTN), a plain old telephone service (POTS) network, a cellular telephone network, a wireless network, a Wi-Fi.RTM.  network,\nanother type of network, or a combination of two or more such networks.  For example, the network 1780 or a portion of the network 1780 may include a wireless or cellular network and the coupling 1782 may be a Code Division Multiple Access (CDMA)\nconnection, a Global System for Mobile communications (GSM) connection, or another type of cellular or wireless coupling.  In this example, the coupling 1782 may implement any of a variety of types of data transfer technology, such as Single Carrier\nRadio Transmission Technology (1.times.RTT), Evolution-Data Optimized (EVDO) technology, General Packet Radio Service (GPRS) technology, Enhanced Data rates for GSM Evolution (EDGE) technology, third Generation Partnership Project (3GPP) including 3G,\nfourth generation wireless (4G) networks, Universal Mobile Telecommunications System (UMTS), High Speed Packet Access (HSPA), Worldwide Interoperability for Microwave Access (WiMAX), Long Term Evolution (LTE) standard, others defined by various\nstandard-setting organizations, other long range protocols, or other data transfer technology.\nThe instructions 1716 may be transmitted or received over the network 1780 using a transmission medium via a network interface device (e.g., a network interface component included in the communication components 1764) and utilizing any one of a\nnumber of well-known transfer protocols (e.g., HTTP).  Similarly, the instructions 1716 may be transmitted or received using a transmission medium via the coupling 1772 (e.g., a peer-to-peer coupling) to the devices 1770.  The term \"transmission medium\"\nshall be taken to include any intangible medium that is capable of storing, encoding, or carrying the instructions 1716 for execution by the machine 1700, and includes digital or analog communications signals or other intangible media to facilitate\ncommunication of such software.\nLanguage\nThroughout this specification, plural instances may implement components, operations, or structures described as a single instance.  Although individual operations of one or more methods are illustrated and described as separate operations, one\nor more of the individual operations may be performed concurrently, and nothing requires that the operations be performed in the order illustrated.  Structures and functionality presented as separate components in example configurations may be\nimplemented as a combined structure or component.  Similarly, structures and functionality presented as a single component may be implemented as separate components.  These and other variations, modifications, additions, and improvements fall within the\nscope of the subject matter herein.\nAlthough an overview of the inventive subject matter has been described with reference to specific example embodiments, various modifications and changes may be made to these embodiments without departing from the broader scope of embodiments of\nthe present disclosure.  Such embodiments of the inventive subject matter may be referred to herein, individually or collectively, by the term \"invention\" merely for convenience and without intending to voluntarily limit the scope of this application to\nany single disclosure or inventive concept if more than one is, in fact, disclosed.\nThe embodiments illustrated herein are described in sufficient detail to enable those skilled in the art to practice the teachings disclosed.  Other embodiments may be used and derived therefrom, such that structural and logical substitutions\nand changes may be made without departing from the scope of this disclosure.  The Detailed Description, therefore, is not to be taken in a limiting sense, and the scope of various embodiments is defined only by the appended claims, along with the full\nrange of equivalents to which such claims are entitled.\nAs used herein, the term \"or\" may be construed in either an inclusive or exclusive sense.  Moreover, plural instances may be provided for resources, operations, or structures described herein as a single instance.  Additionally, boundaries\nbetween various resources, operations, modules, engines, and data stores are somewhat arbitrary, and particular operations are illustrated in a context of specific illustrative configurations.  Other allocations of functionality are envisioned and may\nfall within a scope of various embodiments of the present disclosure.  In general, structures and functionality presented as separate resources in the example configurations may be implemented as a combined structure or resource.  Similarly, structures\nand functionality presented as a single resource may be implemented as separate resources.  These and other variations, modifications, additions, and improvements fall within a scope of embodiments of the present disclosure as represented by the appended\nclaims.  The specification and drawings are, accordingly, to be regarded in an illustrative rather than a restrictive sense.", "application_number": "15188590", "abstract": " In an example embodiment, a query for search results is received, the\n     query including at least one value for one facet, a facet defining a\n     categorical dimension for the search results. It is then determined that\n     the facet in the query is exclusive. In response to the determination\n     that the facet is exclusive: for each potential facet different from the\n     facet in the query: for each potential value in the potential facet:\n     conditional entropy gain of the value in the query and the potential\n     value is determined. The potential value in the potential facet that has\n     the highest conditional entropy gain is determined, as is the potential\n     facet with the minimum maximum conditional entropy gain. Then the\n     potential facet with the minimum maximum is input into a machine learning\n     model, causing the machine learning model to output one or more suggested\n     facets to add to the query.\n", "citations": ["6501855", "8935263", "9135573", "9286569", "20080027747", "20080262902", "20110264656", "20120023031", "20120078902", "20120226681", "20150331948", "20170344554", "20170344555", "20170344556", "20170344954"], "related": []}, {"id": "20180005267", "patent_code": "10373201", "patent_name": "System for providing mobile advertisement actions", "year": "2019", "inventor_and_country_data": " Inventors: \nGupta; Arvind (San Carlos, CA), Tiwari; Ashutosh (San Francisco, CA), Venkatraman; Gopalakrishnan (Irvine, CA), Cheung; Dominic (South Pasadena, CA), Bennett; Stacy R. (West Hollywood, CA), Koen; Douglas B. (Mountain View, CA)  ", "description": "<BR><BR>TECHNICAL FIELD\nThe present description relates generally to a system and method for providing mobile advertisement actions, and more particularly, but not exclusively, to providing mobile advertisement actions with a mobile advertisement to a user on a mobile\ndevice.\n<BR><BR>BACKGROUND\nThe mobile phone is increasingly more important as an information and content access device.  Currently there may be twice as many mobile communication devices as personal computers.  Mobile operators are increasingly looking to high value data\nservices as a way to overcome the continuing decline in average per user voice revenue.  Billions of dollars are being spent globally on wireless licenses with billions more in investments in the pipeline for development of infrastructure and services by\nwireless service and content providers.  Carriers may be introducing new data, content and multimedia services as a means of generating new revenue streams, reversing negative ARPU trends, retaining and attracting customers as well as increasing returns\non investment, and extending and differentiating their service offering to consumers.  The emergence of these wireless technologies create unique opportunities for wireless carriers, advertisers and publishers to generate additional revenue streams\nthrough new and existing customers.  As consumer adoption of wireless technology continues to increase, marketing via mobile devices becomes an important part of all integrated data communications strategies.\nHowever, usage patterns for mobile search and Web search may differ, as well as the expectations of the users and the advertisers.  Combined with a completely different user experience, these may change the value of clicks and lead\nopportunities.  Current mobile devices may have limited browser capabilities that do not support the rich feature set of the Web.  Handset capabilities may impact the search behavior of mobile users, where the limitations of numeric-pad keyed entry\nnarrow the searched for terms.  The small screen size on mobile devices may have an impact on the performance of the search implementations.  The size of screens on mobile handsets may limit the output that may be displayed per listing, and the number of\nlistings per screen.  The absence of a mouse or joystick increases the time necessary for a mobile user to browse a webpage and click on links.  Network broadcast speeds and bandwidths may limit the amount of data that a user receives at a time. \nAdditionally, mobile users travel with their mobile devices and may need to obtain information much quicker than Web users.  Current online marketing systems may not account for differences between mobile handsets and computers.\n<BR><BR>SUMMARY\nA system is disclosed for providing mobile advertisement actions.  The system receives a request from a mobile device.  The system then identifies an advertisement targeted to the request where the advertisement includes at least one mobile\nadvertisement action.  The system then communicates the advertisement and the at least one mobile advertisement action to the mobile device.\nThe system for providing mobile advertisement actions may include a memory to store a request, mobile carrier data, mobile advertisement data, and mobile advertisement action data.  The system may include an interface operatively connected to\nthe memory to communicate with a mobile device.  The system may include a processor operatively connected to the memory and the interface.  The processor may receive information and a request from the mobile device via the interface and may determine the\nmobile carrier data relating to a mobile carrier associated with the mobile device.  The processor may identify the mobile advertisement data and the mobile advertisement action targeted to the request and the mobile carrier data.  The processor may\nappend the mobile advertisement action data to the mobile advertisement data.  The processor may provide the mobile advertisement data with the appended mobile advertisement action data to the mobile device via the interface.\nOther systems, methods, features and advantages are, or will become, apparent to one with skill in the art upon examination of the following figures and detailed description.  It is intended that all such additional systems, methods, features\nand advantages be included within this description, be within the scope of the embodiments, and be protected by the following claims and be defined by the following claims.  Further aspects and advantages are discussed below in conjunction with the\ndescription. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe system and/or method may be better understood with reference to the following drawings and description.  Non-limiting and non-exhaustive descriptions are described with reference to the following drawings.  The components in the figures are\nnot necessarily to scale, emphasis instead being placed upon illustrating principles.  In the figures, like referenced numerals may refer to like parts throughout the different figures unless otherwise specified.\nFIG. 1 is a block diagram of a general overview of a system for providing mobile advertisement actions.\nFIG. 2 is block diagram of a simplified view of a network environment implementing a system for providing mobile advertisement actions.\nFIG. 3 is a flowchart illustrating operations of providing mobile advertisements with mobile advertisement actions in the systems of FIG. 1 and FIG. 2, or other systems for providing mobile advertisement actions.\nFIG. 4 is a flowchart illustrating operations of mobile advertisement actions in the systems of FIG. 1 and FIG. 2, or other systems for providing mobile advertisement actions.\nFIG. 5 is a flowchart illustrating steps that may be taken by a revenue generator in the systems of FIG. 1 and FIG. 2, or other systems for providing mobile advertisement actions.\nFIG. 6 is a flowchart illustrating the operations of creating mobile advertisement actions in the systems of FIG. 1 and FIG. 2, or other systems for providing mobile advertisement actions.\nFIG. 7 is a block diagram illustrating several mobile advertisement actions of an advertisement served to a user in the systems of FIG. 1 and FIG. 2, or other systems for providing mobile advertisement actions.\nFIG. 8 is an illustration of an exemplary mobile device displaying a mobile web page containing advertisements with mobile advertisement actions.\nFIG. 9 is an illustration of an exemplary mobile device displaying an email message containing an advertisement with mobile advertisement actions.\nFIG. 10 is an illustration of an exemplary mobile device displaying an offer landing page including an advertisement with mobile advertisement actions.\nFIG. 11 is an illustration a general computer system that may be used in a system for providing mobile advertisement actions.\n<BR><BR>DETAILED DESCRIPTION\nThe present description relates generally to a system and method, generally referred to as a system, for providing mobile advertisement actions, and more particularly, but not exclusively, to providing mobile advertisement actions with a mobile\nadvertisement to a user on a mobile device.\nThe system may allow advertisers to engage mobile users by providing users with a simple interface for accessing multi-step mobile actions.  The mobile advertisement actions may provide the users with a one click interface to a variety of\ncomplex mobile actions, such as making phone calls, sending text message, purchasing products, making reservations, or generally any action that can be performed with a mobile phone.  Accomplishing these actions in the prior art may have required a user\nto perform several clicks and/or switch applications and/or manually entering data with their mobile device.  It may be burdensome for a user to perform those tasks with a mobile device.  The mobile advertisement actions allow the users to accomplish\nthese actions with one interaction with their mobile device, such as a click or selection.\nFIG. 1 provides a general overview of a system 100 for providing mobile advertisement actions.  Not all of the depicted components may be required, however, and some implementations may include additional components.  Variations in the\narrangement and type of the components may be made without departing from the spirit or scope of the claims as set forth herein.  Additional, different or fewer components may be provided.\nThe system 100 may include one or more revenue generators 110A-N, such as mobile advertisers, a service provider 130, such as a portal or an advertising service provider, one or more mobile network operators (\"MNOs\") 115A-N, more commonly\nreferred to as mobile carriers, or simply carriers, and one or more users 120AA-NN, such as mobile subscribers or consumers.  The service provider 130 may implement a mobile advertising campaign management system incorporating an auction based and/or\nnon-auction based advertisement serving system.\nThe mobile advertising campaign management system may support targeting advertisements to the users 120AA-NN through a variety of mobile advertising tactics, such as search targeting, content match targeting and behavioral profile targeting. \nSearch targeting may refer to targeting advertisements, at least in part, to mobile keywords provided by the users, content match targeting may refer to targeting advertisements, at least in part, to the content of a particular mobile page, and\nbehavioral profile targeting may refer to targeting advertisements, at least in part, to the mobile behavior of the users 120AA-NN.  The service provider 130 may share revenue with the MNOs 115A-N for displaying advertisements of the revenue generators\n110A-N on their mobile networks.  Alternatively or in addition the service provider 130 may share revenue with individual mobile publishers for displaying advertisements of the revenue generators 110A-N on their mobile sites.\nThe revenue generators 110A-N may pay the service provider 130 to serve, or display, advertisements of their goods or services, such as on-line or mobile advertisements, to the users 120AA-NN, such as over mobile messaging, mobile web, the\nInternet, or generally any mode of displaying advertisements.  The advertisements may include sponsored listings, banners ads, popup advertisements, mobile messages, or generally any way of attracting the users 120AA-NN to the web site or mobile site of\nthe revenue generators 110A-N. The revenue generators 110A-N may bid on specific MNOs 115A-N to target their advertisements to.  For example, the revenue generator A 110A may bid on one or more keywords searched for via the MNO A 115A.  In this case the\nmobile advertisements of the revenue generator A 110A may only be displayed to the users 120AA-NA when the users 120AA-NA search for the keywords through the MNO A 115A.\nThe MNOs 115A-N may provide a mobile network to the users 120AA-NN which may provide a variety of services to the users 120AA-NN, such as the ability to send and receive phone calls, send and receive mobile messages, to access the internet\nand/or the mobile web, or generally any service that may be implemented on a mobile device.  The MNOs 115A-N may store data describing the users 120AA-NN, such as billing addresses, call histories, messaging histories, or generally any data regarding the\nusers 120AA-NN that may be available to the MNOs 115A-N.\nThe users 120AA-NN interacting with the service provider 130 on a mobile device may want to click on fewer links than a Web user before obtaining the information that they seek.  A mobile advertisement action is provided that allows users\n120AA-NN to directly click to a specified action in a single step, without the need to navigate through multiple steps to accomplish the action.  Such a mobile advertisement action also allows increased functionality for revenue generators 110A-N who do\nnot have their own mobile webpage set up but who nonetheless want to provide extra functionality to users 120AA-NN.\nA mobile advertisement action may be any functional object, item, applet, program, plug-in, or other device that allows a user AA 120AA to click directly to an action associated with the mobile advertisement.  A mobile advertisement action may\nbe associated with a voice command so that a user AA 120AA can activate the action by a voice command while browsing a mobile web page.  A mobile device may include other functionality allowing accessibility by any other interaction with a mobile webpage\nthat is available.\nUnlike clicking a hyperlink, which only directs a user AA 120AA to the webpage address encoded in the hyperlink, mobile advertisement actions may provide a wide range of functionality when clicked on by a user AA 120AA.  Instead of simply\ndirecting a user AA 120AA to another webpage, a mobile advertisement action may execute encoded or programmed functions.  The mobile advertisement action itself may be a set of instructions or computer code.  Alternatively or in addition the mobile\nadvertisement action may be a link that provides the instructions or code from the service provider 130.\nFor example, if a user AA 120AA clicks on a mobile advertisement action of an advertisement associated with a revenue generator A 110A, the mobile advertisement action may cause the mobile device to dial the phone number of the revenue generator\nA 110A, make a reservation with the revenue generator A 110A, display a map or directions to the physical location of the revenue generator A 110A, program the address of the revenue generator A 110A into a navigation system of a user AA 120AA, purchase\na product from the revenue generator A 110A, or may perform any other function associated with the mobile device and the revenue generator A 110A.  Several examples of specific mobile advertisement actions are provided in FIG. 7.\nThe users 120AA-NN may be consumers of goods or services who may be searching for a business such as the business of one of the revenue generators 110A-N. The users 120AA-NN may communicate with the service provider 130 through the MNOs 115A-N.\nThe users 120AA-NN may supply information describing themselves to the service provider 130, such as the location, gender, mailing address, credit card information, or age of the users 120AA-NN, or generally any information that may be required for the\nusers 120AA-NN to utilize the services provided by the service provider 130.  Alternatively or in addition the service provider 130 may obtain information about the users 120AA-NN from the MNOs 115A-N.\nIn system 100, the revenue generators 110A-N may interact with the service provider 130, such as via a web application.  The revenue generators 110A-N may send information, such as billing, website or mobile site and advertisement information,\nto the service provider 130 via the web application.  The web application may include a web browser or other application such as any application capable of displaying web content.  The application may be implemented with a processor such as a personal\ncomputer, personal digital assistant, mobile phone, or any other machine capable of implementing a web application.\nThe users 120AA-NN may also interact individually with the service provider 130, through the MNOs 115A-N, such as via a mobile phone or any device capable of communicating with the MNOs 115A-N. The users 120AA-NN may interact with the service\nprovider 130 via a mobile web based application, a mobile standalone application, or any application capable of running on a mobile device.  The service provider 130 may communicate data to the revenue generators 110A-N over a network and to the users\n120AA-NN over a network via the MNOs 115A-N. The following examples may refer to a revenue generator A 110A as an online advertiser or mobile advertiser; however the system 100 may apply to any revenue generators 110A-N who may desire to serve\nadvertisements over mobile devices.\nIn operation, one of the revenue generators 110A-N, such as revenue generator A 110A, may provide information to the service provider 130.  This information may relate to the transaction taking place between the revenue generator A 110A and the\nservice provider 130, or may relate to an account the revenue generator A 110A maintains with the service provider 130.  In the case of a revenue generator A 110A who is a mobile advertiser, the revenue generator A 110A may provide initial information\nnecessary to open an account with the service provider 130.\nA revenue generator A 110A who is a mobile advertiser may maintain one or more accounts with the service provider 130.  For each account the revenue generator A 110A may maintain one or more campaigns.  For each campaign the revenue generator A\n110A may maintain one or more ad groups.  An ad group may be associated with one or more MNOs 115A-N, and may include one or more keywords, or categories, and one or more mobile advertisements.  Each advertisement may be associated with one or more\nmobile advertisement actions.  A mobile advertisement may be provided with associated mobile advertisement actions to a user AA 120AA when the user AA 120AA interacts with the service provider 130 through an MNO A 115A associated with the mobile\nadvertisement.  The service provider 130 may verify that the mobile device of the user AA 120AA is capable of performing the mobile advertisement actions.  If the mobile device is not capable of performing a mobile advertisement action, the mobile\nadvertisement action may not be provided to the user AA 120AA.  The revenue generator A 110A may be able to associate different mobile advertisement actions for each of the MNOS 115A-N associated with the mobile advertisement.\nA mobile advertisement action may be a clickable interactive object embedded in an advertisement, such as a button or a link.  The mobile advertisement action may represent one or more operations that may be performed by a mobile device, such as\nemail, text messaging, phone calling, or generally any operation that may be performed by a mobile device and may provide may provide enhanced advertising capabilities to a revenue generator A 110A.  The mobile advertisement action may include data\nspecific to the revenue generator A 110A, such as the phone number, address, or generally any information specific to the revenue generator A 110A.  The mobile advertisement action may include a description such as \"Click to call,\" \"Click to SMS,\" \"Click\nto coupon,\" \"Click to download,\" \"Click to survey,\" \"Click to make reservation,\" \"Click to attend event,\" \"Click to buy,\" or generally any description that may describe the functionality provided by them mobile advertisement action.\nWhen the user AA 120AA clicks on a mobile advertisement action of a revenue generator A 110A, the mobile advertisement action may cause the mobile device of the user AA 120AA to perform the action associated with the mobile advertisement action\nin accordance with the data of the revenue generator A 110A.  For example, if the mobile advertisement action is a mobile call action, then, when clicked on, the mobile advertisement action may cause the mobile device of the user AA 120AA to call a phone\nnumber associated with the revenue generator A 110A.  Alternatively or in addition, when clicked on, the mobile advertisement action may communicate with the service provider 130 and the service provider 130 may cause the mobile device of the user AA\n120AA to perform the specified action, such as by providing instructions to the mobile device and causing the instructions to be executed.\nAlternatively or in addition if a revenue generator A 110A does not have a mobile site URL for an MNO A 115A the service provider 130 may dynamically create a \"WAP ad.\" The \"WAP ad\" may be an offer landing page containing detailed information\nabout the revenue generator A 110A.  The detailed information may include the phone number of the revenue generator A 110A, a logo of the revenue generator A 110A, an address of the revenue generator A 110A, offers and coupons from the revenue generator\nA 110A, an SMS number, software or programs for download, survey materials, marketing materials, mobile advertisement actions, and any other relevant or business information about the revenue generator A 110A.  When a user AA 120AA clicks on the\nadvertisement of the revenue generator A 110A who does not have a mobile site, the user AA 120AA may be taken to the offer landing page showing the phone number, any mobile advertisement actions, and/or logo of the revenue generator A 110A.\nThe keywords associated with an ad group may represent one or more search terms that the revenue generator A 110A wishes to associate with their advertisement generally, or with one or more of the advertisement's mobile advertisement actions\nspecifically.  When a user AA 120AA searches for a search keyword via an MNO A 115A associated with the ad group, the mobile advertisement and the associated mobile advertisement actions of the revenue generator A 110A may be displayed on the search\nresults page.  The service provider 130 may also implement directory search implementations, where the user AA 120AA may click through directories of families of related data.  In this instance the search keyword may be the name of the directory the user\nAA 120AA clicks on.  Alternatively or in addition the user AA 120AA may interact with the service provider 130 through an SMS search service or a voice search service.\nFor example, a revenue generator A 110A, such as MCDONALDS, may desire to target a mobile advertisement for a MCDONALDS BIG MAC to users 120AA-NA on MNO A 115A searching for the keywords \"BIG MAC.\" MCDONALDS may place a bid with the service\nprovider 130 for the keyword \"BIG MAC\" on MNO A 115A.  The revenue generator A 110A may also provide any variety of information to the service provider 130 including the worldwide locations of its businesses, phone numbers of each business locations,\ncoupons for one or more of its products, or any other relevant information or files.  The mobile advertisement of the revenue generator A 110A may be displayed when one of the users 120AA-NA, such as the user AA 120AA, interacts with the service provider\n130 via the MNO A 115A.  The user AA 120AA may search for the keyword \"BIG MAC\" or may click through a directory named \"BIG MAC.\" In addition to displaying a description of a \"BIG MAC,\" the advertisement may also display any of a number of mobile\nadvertisement actions, such as a button to get a coupon for a \"BIG MAC,\" a button to call MCDONALDS, a button to get directions to the MCDONALDS most proximate to the location of the user AA 120AA, or any other relevant mobile advertisement actions. \nAlternatively or in addition the service provider 130 may display the advertisement for a \"BIG MAC,\" and the associated mobile advertisement actions, when the user AA 120AA is located near a MCDONALDS location.\nWhen one of the users 120AA-NN, such as the user AA 120AA, interacts with the service provider 130, such as by searching for a keyword, the service provider 130 may retain data describing the interaction with the user AA 120AA.  The retained\ndata may include the keyword searched for, the geographic location of the user AA 120AA, and the date/time the user AA 120AA interacted with the service provider 130.  The data may also generally include any data available to the service provider 130\nthat may assist in describing the interaction with the user AA 120AA, or describing the user AA 120AA.  The service provider 130 may also store data that indicates whether a mobile advertisement of one of the revenue generators 110A-N, such as the\nrevenue generator A 110A was displayed to the user AA 120AA, and whether the user AA 120AA clicked on the mobile advertisement or one or more of its mobile advertisement actions.\nThe service provider 130 may already have information relating to the geographic location of the user AA 120AA and other information describing the user AA 120AA, such as gender, age, etc. This information may have been previously supplied to\nthe service provider 130 by the user AA 120AA.  Alternatively or in addition the service provider 130 may obtain the location of the user AA 120AA based on the IP address of the user AA 120AA, a location of a cell phone tower in communication with the\nuser, or a positioning system, such as a global positioning system (GPS), in communication with the mobile device.  The service provider 130 may use a current date/time stamp to store the date/time when the user AA 120AA interacted with the service\nprovider 130.  The service provider 130 may use any of the information describing the user or the keyword searched for by the user to evaluate the relevancy of an advertisement or one or more of its mobile advertisement actions to the search.\nFurthermore, the service provider 130 may generate reports based on the data collected from the users 120AA-NN and may communicate the reports to the revenue generators 110A-N to assist the revenue generators 110A-N in measuring the\neffectiveness of their mobile advertising campaigns.  The reports may indicate the number of times the users 120AA-NN viewed a mobile advertisement and/or mobile advertisement action of the revenue generators 110A-N, the number of times a mobile\nadvertisement or mobile advertisement action of the revenue generators 110A-N was clicked on by the users 120AA-NN, or generally any information useful to the revenue generators 110A-N.\nMore detail regarding the aspects of a mobile advertising auction-based systems, as well as the structure, function and operation of the service provider 130 as a mobile advertising provider, as mentioned above, can be found in commonly owned\nU.S.  patent application Ser.  No. 11/712,276, filed on Feb.  28, 2007, entitled, \"SYSTEM FOR SERVING ADVERTISEMENTS OVER MOBILE DEVICES,\" which is hereby incorporated herein by reference in its entirety.  The systems and methods herein associated with\nmobile advertising campaign management may be practiced in combination with methods and systems described in the above-identified patent application incorporated by reference.\nFIG. 2 provides a simplified view of a network environment implementing a system 200 for serving advertisements including mobile advertisement actions over mobile devices.  Not all of the depicted components may be required, however, and some\nimplementations may include additional components not shown in the figure.  Variations in the arrangement and type of the components may be made without departing from the spirit or scope of the claims as set forth herein.  Additional, different or fewer\ncomponents may be provided.\nThe system 200 may include one or more web applications, standalone applications and mobile applications 210A-N, which may be collectively or individually referred to as client applications of the revenue generators 110A-N. The system 200 may\nalso include one or more mobile applications, or mobile apps 220AA-NN, which may collectively be referred to as client applications of the users 120AA-NN, or individually as a user client application.  The system 200 may also include one or more MNO\ngateway servers 215A-N, a network 230, a network 235, the service provider server 240, a third party server 250, and an advertising services server 260.\nSome or all of the advertisement services server 260, service provider server 240, and third-party server 250 may be in communication with each other by way of network 235 and may be the system or components described below in FIG. 11.  The\nadvertisement services server 260, third-party server 250 and service provider server 240 may each represent multiple linked computing devices.  Multiple distinct third party servers, such as the third-party server 250, may be included in the system 200. The third-party server 250 may be an MNO gateway server 215A-N or a server associated with, or in communication with an MNO gateway server 215A-N.\nThe networks 230, 235 may include wide area networks (WAN), such as the Internet, local area networks (LAN), campus area networks, metropolitan area networks, or any other networks that may allow for data communication.  The network 230 may\ninclude the Internet and may include all or part of network 235; network 235 may include all or part of network 230.  The networks 230, 235 may be divided into sub-networks.  The sub-networks may allow access to all of the other components connected to\nthe networks 230, 235 in the system 200, or the sub-networks may restrict access between the components connected to the networks 230, 235.  The network 235 may be regarded as a public or private network connection and may include, for example, a virtual\nprivate network or an encryption or other security mechanism employed over the public Internet, or the like.\nThe revenue generators 110A-N may use a web application 210A, standalone application 210B, or a mobile application 210N, or any combination thereof, to communicate to the service provider server 240, such as via the networks 230, 235.  The\nservice provider server 240 may communicate to the revenue generators 110A-N via the networks 230, 235, through the web applications, standalone applications or mobile applications 210A-N.\nThe users 120AA-NN may use a mobile application 220AA-220NN, such as a mobile web browser, to communicate with the service provider server 240, via the MNO gateway servers 215A-N and the networks 230, 235.  The service provider server 240 may\ncommunicate to the users 120AA-NN via the networks 230, 235 and the MNOs 215A-N, through the mobile applications 220AA-NN.\nThe web applications, standalone applications and mobile applications 210A-N, 220AA-NN may be connected to the network 230 in any configuration that supports data transfer.  This may include a data connection to the network 230 that may be wired\nor wireless.  Any of the web applications, standalone applications and mobile applications 210A-N, 220AA-NN may individually be referred to as a client application.  The web application 210A may run on any platform that supports web content, such as a\nweb browser or a computer, a mobile phone, personal digital assistant (PDA), pager, network-enabled television, digital video recorder, such as TIVO.RTM., automobile and/or any appliance capable of data communications.\nThe standalone applications 210B may run on a machine that may have a processor, memory, a display, a user interface and a communication interface.  The processor may be operatively connected to the memory, display, and the interfaces and may\nperform tasks at the request of the standalone applications 210B or the underlying operating system.  The memory may be capable of storing data.  The display may be operatively connected to the memory and the processor and may be capable of displaying\ninformation to the revenue generator B 110B.  The user interface may be operatively connected to the memory, the processor, and the display and may be capable of interacting with a revenue generator A 110A.  The communication interface may be operatively\nconnected to the memory, and the processor, and may be capable of communicating through the networks 230, 235 with the service provider server 240, third party server 250 and advertising services server 260.  The standalone applications 210B may be\nprogrammed in any programming language that supports communication protocols.  These languages may include: SUN JAVA, C++, C#, ASP, SUN JAVASCRIPT, asynchronous SUN JAVASCRIPT, or ADOBE FLASH ACTIONSCRIPT, amongst others.\nThe mobile applications 210N, 220AA-NN may run on any mobile device which may have a data connection.  The mobile applications 210N, 220AA-NN may be a web application 210A, a standalone application 210B, or a mobile browser.  The mobile device\nmay be one of a broad range of electronic devices which may include mobile phones, PDAs, and laptops and notebook computers.  The mobile device may have a reduced feature set, such as a smaller keyboard and/or screen, and may be incapable of supporting a\ntraditional web search.\nThe data connection of the mobile device may be a cellular connection, such as a GSM/GPRS/WCDMA connection, a wireless data connection, an internet connection, an infra-red connection, a Bluetooth connection, or any other connection capable of\ntransmitting data.  The data connection may be used to connect directly to the network 230, or to connect to the network 230 through the MNO gateway servers 215A-N. The MNO gateway servers 215A-N may control the access the mobile applications 210AA-NN\nmay have to the network.  The MNO gateway servers 215A-N may also control the technology supporting the respective mobile applications 220AA-NN.  This may affect all aspects of the user experience, such as signal strength and availability, speed and\nbilling mechanisms.  For example, the MNO gateway server A 215A may only allow the users 120AA-NA access to content provided by partners of the MNO A 115A.  Furthermore, the MNO gateway servers 215A-N may only allow users 120AA-NN access to data in a\nspecific format, such as WML, XHTML, NTT DOCOMO IMODE HTML, or cHTML.  Alternatively or in addition, the mobile applications 220AA-NN may only support one of the aforementioned formats.\nThe service provider server 240 may include one or more of the following: an application server, a data source, such as a database server, a middleware server, and an advertising services server.  One middleware server may be a mobile commerce\nplatform, such as the YAHOO! SUSHI platform, which may properly encode data, such as mobile pages or mobile advertisements, to the formats specific to the MNO gateway servers 215A-N. The service provider server 240 may co-exist on one machine or may be\nrunning in a distributed configuration on one or more machines.  The service provider server 240 may collectively be referred to as the server.  The service provider server 240 may receive requests from the users 120AA-NN and the revenue generators\n110A-N and may serve mobile pages to the users 120AA-NN and web pages and/or mobile pages to the revenue generators 110A-N based on their requests.\nThe third party server 250 may include one or more of the following: an application server, a data source, such as a database server, a middleware server, and an advertising services server.  The third party server 250 may co-exist on one\nmachine or may be running in a distributed configuration on one or more machines.  The advertising services server 260 may provide a platform for the inclusion of advertisements with mobile advertisement actions in pages, such as web pages or mobile\npages.  The advertisement services server 260 may be used for providing mobile advertisements and/or mobile advertisement actions that may be displayed to the users 120AA-NN.\nThe service provider server 240, the third party server 250 and the advertising services server 260 may be one or more computing devices of various kinds, such as the computing device in FIG. 11.  Such computing devices may generally include any\ndevice that may be configured to perform computation and that may be capable of sending and receiving data communications by way of one or more wired and/or wireless communication interfaces.  Such devices may be configured to communicate in accordance\nwith any of a variety of network protocols, including but not limited to protocols within the Transmission Control Protocol/Internet Protocol (TCP/IP) protocol suite.  For example, the web application 210A may employ HTTP to request information, such as\na web page, from a web server, which may be a process executing on the service provider server 240 or the third-party server 250.\nThere may be several configurations of database servers, application servers, middleware servers and advertising services servers included in the service provider server 240 or the third party server 250.  Database servers may include MICROSOFT\nSQL SERVER, ORACLE, IBM DB2 or any other database software, relational or otherwise.  The application server may be APACHE TOMCAT, MICROSOFT IIS, ADOBE COLDFUSION, YAPACHE or any other application server that supports communication protocols.  The\nmiddleware server may be any middleware that connects software components or applications.  The application server on the service provider server 240 or the third party server 250 may serve pages, such as web pages to the users 120AA-NN and the revenue\ngenerators 110A-N. The advertising services server 260 may provide a platform for the inclusion of advertisements with mobile advertisement actions in pages, such as web pages.  The advertising services server 260 may also exist independent of the\nservice provider server 240 and the third party server 250.\nThe networks 230, 235 may be configured to couple one computing device to another computing device to enable communication of data between the devices.  The networks 230, 235 may generally be enabled to employ any form of machine-readable media\nfor communicating information from one device to another.  Each of networks 230, 235 may include one or more of a wireless network, a wired network, a local area network (LAN), a wide area network (WAN), a direct connection such as through a Universal\nSerial Bus (USB) port, and the like, and may include the set of interconnected networks that make up the Internet.  The networks 230, 235 may include any communication method by which information may travel between computing devices.  Other systems and\nmethods such as the system and methods of U.S.  patent application Ser.  No. 11/712,276, incorporated herein by reference, may be used.\nFIG. 3 is a flowchart illustrating operations of serving mobile advertisements including mobile advertisement actions in the systems of FIG. 1 and FIG. 2, or other systems for providing mobile advertisement actions.  At block 305 the service\nprovider 130 may receive a request for a mobile advertisement.  The mobile advertisement request may be related to a request from the user AA 120AA, such as a search request.  Alternatively or in addition the mobile advertisement request may be related\nto content being viewed by the user AA 120AA, or may be related to the behavior of the user AA 120AA.  The service provider 130 may also identify other associated data about the mobile device of the user AA 120AA, such as the type of device making the\nrequest, the bandwidth available, availability of Internet service, or any other necessary parameters.\nAlternatively or in addition the system 100 may check whether a location for the mobile device of the user AA 120AA is available.  Location may be determined by data sent from a positioning system locator in communication with the mobile device\nof the user AA 120AA.  Alternatively, location of the mobile device of the user AA 120AA may be determined by data sent to the service provider 130 such as the IP address of the requesting user AA 120AA or an identifying code of a location of a cell\nphone tower in communication with mobile device of user AA 120AA.  Alternatively, the user AA 120AA may provide their own location such as by entering an address or a landmark on their mobile device.  Depending on the data available, the location of user\nAA 120AA may thus be determined to any of varying degrees of preciseness.\nAt block 310 the service provider 130 may analyze the request and the associated data to determine the mobile carrier associated with the mobile device of the user AA 120AA, such as the MNO A 115A.  At block 315 the service provider 130 may\ndetermine a mobile advertisement associated with the MNO A 115A, and/or the associated data, such as the request from the user AA 120AA.\nAt block 320 the service provider 130 may determine whether any mobile advertisement actions are associated with the mobile advertisement for the mobile carrier A 115A.  If there are mobile advertisement actions associated with the mobile\nadvertisement for the mobile carrier A 115A, the system 100 may move to block 325.  At block 325 the service provider 130 may determine the functionality supported by the mobile device of the user AA 120AA.  The service provider 130 may determine the\nfunctionality supported by the mobile device based on data associated with the request, based on data previously submitted by the user AA 120AA, based on data received from the MNO A 115A, or generally based on any data that may be indicative of the\nfunctions supported by the mobile device.\nAt block 330 the service provider 130 may determine which of the mobile advertisement actions associated with the mobile advertisement and the MNO A 115A to provide to the user AA 120AA.  The service provider 130 may only provide mobile\nadvertisement actions that provide functionality supported by the mobile device of the user AA 120AA.  The service provider 130 may use the functionality determined at block 325 to determine which mobile advertisement actions to provide to the user AA\n120AA.  For example, if the mobile device of the user AA 120AA does not support text messaging, the service provider 130 may not provide a mobile advertisement action for text messaging.\nAt block 335 the service provider 130 may retrieve data associated with the revenue generator A 110A related to the mobile advertisement actions.  For example, if one of the mobile advertisement actions is a call action, the service provider 130\nmay retrieve the phone number of the revenue generator A 110A.  At block 340 the service provider 130 may generate the mobile advertisement actions based on the type of mobile advertisement action, the technology supported by the mobile device of the\nuser AA 120AA, and the data associated with the revenue generator A 110A.  Alternatively or in addition the service provider 130 may generate a link that communicates a request to the service provider 130 identifying the mobile advertisement action. \nWhen the user AA 120AA clicks on the mobile advertisement action the link may communicate a request to the service provider 130 for the data implementing the mobile advertisement action.  The service provider 130 may then communicate the data\nimplementing the mobile advertisement action to the mobile device of the user AA 120AA.  When the user AA 120AA has limited bandwidth it may be beneficial to only communicate links requesting the data implementing the actions.  The mobile call actions\nmay be implemented through data, code, or instructions.\nAt block 345 the service provider 130 may append the generated mobile advertisement actions to the mobile advertisement.  The mobile advertisement actions may be represented by a link or a button in the advertisement.  The mobile advertisement\nactions may be located anywhere on the advertisement, such as the bottom of the advertisement.  At block 350 the service provider 130 may provide the mobile advertisement, including the appended mobile advertisement actions, to the user AA 120AA.\nIf, at block 320, there are no mobile advertisement actions associated with the mobile advertisement and the MNO A 115A, the system 100 may move to block 350.  At block 350 the service provider 130 may provide the mobile advertisement, without\nany mobile advertisement actions, to the user AA 120AA.\nFIG. 4 is a flowchart illustrating operations of mobile advertisement actions in the systems of FIG. 1 and FIG. 2, or other systems for providing mobile advertisement actions.  At block 402 the service provider 130 may receive a request for a\nmobile advertisement action associated with a mobile advertisement of the revenue generator A 110A.  The request may originate from a mobile advertisement action provided to the user AA 120AA with a link referencing the service provider 130.  The link\nmay request the service provider 130 to provide the additional data necessary to perform the mobile advertisement action.\nAt block 404 the service provider 130 may determine the mobile carrier associated with the mobile device requesting the mobile advertisement action, such as the MNO A 115A.  At block 406 the service provider 130 may determine the type of mobile\nadvertisement action requested.  The type of mobile advertisement action may be identifiable through the request.  At block 410 the service provider 130 may determine whether the mobile advertisement action is a call ad action.  If the mobile\nadvertisement action is a call ad action, the system 100 may move to block 412.  At block 412 the service provider 130 may determine the phone number associated with the revenue generator A 110A.  At block 414 the service provider 130 may cause the\nmobile device of the user AA 120AA to call the number of the revenue generator A 110A, such as by generating code capable of causing the mobile device to call the number, communicating the code to the mobile device and causing the code to be executed on\nthe mobile device.\nIf, at block 410, the action type is not a call action, the system 100 may move to block 420.  At block 420 the service provider 130 may determine whether the requested action is a message action.  If the requested action is a message action the\nsystem 100 may move to block 422.  At block 422 the service provider 130 may determine a messaging protocol supported by the mobile device of the user AA 120AA, such as email, text messaging, or generally any messaging protocol implemented on mobile\ndevices.  If the mobile device supports more than one messaging protocol, the service provider 130 may attempt to determine a default or preferred protocol.  At block 425 the service provider 130 may determine the messaging address of the revenue\ngenerator A 110A for the determined messaging protocol, such as an email address, a text messaging number, or generally any messaging address.  At block 426 the service provider 130 may cause the mobile device of the user AA 120AA to create a message\naddressed to the revenue generator A 110A, such as by generating code capable of causing the mobile device to generate the message, communicating the code to the mobile device and causing the code to be executed on the mobile device.  Alternatively or in\naddition the service provider 130 may cause the mobile device to generate the message and send the message to the revenue generator A 110A, such as for a vote over text messaging.\nIf, at block 420 the mobile advertisement action is not a message action, the system 100 may move to block 430.  At block 430 the service provider 130 may determine whether the mobile advertisement action is a chat action.  If, at block 420, the\nmobile advertisement action is a chat action, the system 100 may move to block 432.  At block 432 the system 100 may determine the chat address of the revenue generator A 110A.  The chat address may be a phone number, an Internet-based address, a voice\nover internet protocol address, an instant messenger address, or generally any address capable of supporting a chat session on the mobile device of the user AA 120AA.  At block 434 the service provider 130 may cause the mobile device of the user AA 120AA\nto initiate a chat session with the revenue generator A 110A, such as by generating code capable of causing the mobile device to initiate the chat session, communicating the code to the mobile device, and causing the code to be executed on the mobile\ndevice.\nIf, at block 430, the mobile advertisement action is not a chat action, the system 100 may move to block 440.  At block 440 the system 100 may determine whether the mobile advertisement action is a mapping action.  If, at block 440, the mobile\nadvertisement action is a mapping action, the system 100 may move to block 442.  At block 442 the service provider 130 may determine whether the location of the user AA 120AA can be automatically determined.  The location of the user AA 120AA may be\nautomatically determined through a positioning system, such as GPS, through the triangulation of a cell phone signal, through information provided by the user AA 120AA, or generally through any means of determining the location of a mobile user.  If, at\nblock 442, the service provider 130 can not determine the location of the user AA 120AA, the system 100 may move to block 444.  At block 444 the service provider 130 may request the user AA 120AA provide their location.  At block 445 the service provider\n130 may receive the location of the user AA 120AA from the user AA 120AA.\nIf, at block 442, the service provider 130 is able to determine the location of the user AA 120AA, the system 100 may move to block 446.  At block 446 the service provider 130 may determine the address of the revenue generator A 110A.  At block\n447 the service provider 130 may determine the directions from the location of the user AA 120AA to the address associated with the revenue generator A 110A.  The service provider 130 may operate a mapping engine, or the third party server 250 may\nprovide mapping functionality.  At block 448 the service provider 130 may cause the mobile device of the user AA 120AA to display the directions to the location of the revenue generator A 110A, such as by generating code capable of causing the mobile\ndevice to display the directions, communicating the code to the mobile device, and causing the code to be executed on the mobile device.\nIf, at block 440, the mobile advertisement action is not a mapping action, the system 100 may move to block 450.  At block 450 the service provider 130 may determine whether the mobile advertisement action is a purchase action.  If, at block\n450, the mobile advertisement action is a purchase action, the system 100 may move to block 452.  At block 452 the service provider 130 may determine the product of the revenue generator A 110A that is advertised in the advertisement associated with the\nmobile advertisement action.  At block 454 the service provider may cause the mobile device of the user AA 120AA to display a purchase screen for the product, such as by generating code capable of causing the mobile device to display the purchase screen,\ncommunicating the code to the mobile device, and causing the code to be executed on the mobile device.  Alternatively or in addition the service provider 130 may cause the mobile device of the user AA 120AA to purchase the product.  The user AA 120AA may\nneed to associate a form of payment with their mobile device, such as a credit card.  Alternatively or in addition the charges for the purchase may be applied to the phone bill of the user AA 120AA.\nIf, at block 450, the mobile advertisement action is not a purchase action, the system 100 may move to block 460.  At block 460 the service provider 130 may determine whether the mobile advertisement action is some other type of mobile\nadvertisement action.  If, at block 460, the service provider 130 determines that the mobile advertisement action is some other type of mobile advertisement action, the system 100 may move to block 462.  At block 462 the service provider 130 may\ndetermine data relevant to the mobile advertisement action, such as data associated with the revenue generator A 110A.  At block 464 the service provider 130 may cause the mobile device of the user AA 120AA to perform the mobile advertisement action,\nsuch as by generating code capable of causing the mobile device to perform the action, communicating the code to the mobile device, and causing the code to be executed on the mobile device.\nIf, at block 460, the service provider 130 determines that the action type of the requested mobile advertisement action is not a known action type, the service provider 130 may communicate an error message to the user AA 120AA.  Alternatively or\nin addition if the action type is not supported by the mobile device of the user AA 120AA, the service provider 130 may provide an error message to the user AA 120AA.\nFIG. 5 is a flowchart illustrating steps that may be taken by a revenue generator in the systems of FIG. 1 and FIG. 2, or other systems for providing mobile advertisement actions.  At block 510 one of the revenue generators 110A-N, such as the\nrevenue generator A 110A, may interact with the service provider 130, such as logging into a web site provided by the service provider 130.  At block 520 the revenue generator A 110A may provide a mobile advertisement to the service provider 130, such as\na mobile advertisement to associate with an ad group.  At block 530 the revenue generator A 110A may associate the mobile advertisement with one or more MNOs 115A-N, such as the MNO A 115A.  At block 535 the revenue generator A 110A may indicate whether\nthey would like to associate mobile advertisement actions with the mobile advertisement.\nIf, at block 535, the revenue generator A 110A would like to associate mobile advertisement actions with the mobile advertisement, the system 100 may move to block 540.  At block 540 the revenue generator A 110A may select one or more types of\nmobile advertisement actions to associate with the mobile advertisement.  At block 545 the service provider 130 may request any additional information required to implement the selected mobile advertisement actions, such as the phone number of the\nrevenue generator A 110A for a call action.  If, at block 545, additional data is requested by the service provider 130, the system 100 may move to block 550.  At block 550 the revenue generator A 110A may provide the additional data requested by the\nservice provider 130.  The service provider 130 may then associate the mobile advertisement actions with the mobile advertisement and the MNO A 115A.\nIf, at block 535, the revenue generator A 110A does not wish to associate any mobile call actions with the mobile advertisement and the MNO A 115A, the system 100 may move to block 555.  If, at block 545, the service provider 130 does not\nrequest any additional data the system 100 may move to block 555.  At block 555 the revenue generator A 110A may indicate whether they would like to associate another one of the MNOs 115B-N with the mobile advertisement.  If, at block 555 the revenue\ngenerator A 110A indicates that they would like to associate another one of the MNOs 115B-N with the mobile advertisement the system 100 may move to block 530 and repeat the process.\nIf, at block 555, the revenue generator A 110A indicates that they do not wish to associate another one of the MNOs 115B-N with the mobile advertisement, the system 100 may move to block 565.  At block 565 the revenue generator A 110A may\nindicate whether they wish to provide another mobile advertisement to the service provider 130.  If, at block 565, the revenue generator A 110A indicates they would like to provide an additional mobile advertisement to the service provider 130 the system\n100 may move to block 520.  At block 520 the revenue generator A 110A may provide the additional advertisement to the service provider 130 and repeat the process.\nIf, at block 565, the revenue generator A 110A indicates that they do not wish to provide any other mobile advertisements to the service provider 130, the system 100 may move to block 570.  At block 570 the revenue generator A 110A may logout of\nthe web site of the service provider 130.\nFIG. 6 is a flowchart illustrating the operations of creating mobile advertisement actions in the systems of FIG. 1 and FIG. 2, or other systems for providing mobile advertisement actions.  At block 610 the service provider 130 may receive a\nmobile advertisement from one of the revenue generators 110A-N. The mobile advertisement may be received in the process of creating, or modifying, an ad group.  At block 620 the service provider 130 may receive an association of one of the MNOs 115A-N,\nsuch as the MNO A 115A, with the mobile advertisement from the revenue generator A 110A.  Alternatively or in addition the revenue generator A 110A may have previously associated one or more MNOs 115A-N with the ad group.  At block 625 the service\nprovider 130 may receive an indication from the revenue generator A 110A as to whether the revenue generator A 110A would like to associate mobile advertisement actions with the mobile advertisement for the MNO A 115A.\nIf, at block 625, the service provider 130 receives an indication that the revenue generator A 110A would like to associate mobile advertisement actions with the mobile advertisement for the MNO A 115A the system 100 may move to block 630.  At\nblock 630 the service provider 130 may receive a selection of the types of mobile advertisement actions to associate with the mobile advertisement for the MNO A 115A.  At block 640 the service provider 130 may determine whether the selected types of\nmobile advertisement actions require additional information from the revenue generator A 110A.  For example, a call action may require a phone number of the revenue generator A 110A, while a mapping action may require an address of the revenue generator\nA 110A.  The service provider 130 may be able to retrieve the required information from data associated with the account of the revenue generator A 110A stored in the data store 245.  If the required data is not stored in the data store 245 then the\nservice provider 130 may need to request the additional data from the revenue generator A 110A.\nIf, at block 645, the service provider 130 needs to request additional data from the revenue generator A 110A, then the system 100 may move to block 650.  At block 650 the service provider 130 may request additional data from the revenue\ngenerator A 110A.  At block 660 the service provider 130 may store the additional data provided by the revenue generator A 110A, such as in the data store 245.  If, at block 645, no additional information is necessary from the revenue generator A 110A,\nthen the system 100 may move to block 665.  At block 665 the service provider 130 may generate the mobile advertisement actions based on the data associated with the revenue generator A 110A and the MNO A 115A.  At block 670 the service provider 130 may\nstore the selected mobile advertisement actions.\nIf, at block 625, the revenue generator A 110A does not indicate that they would like to associate mobile advertisement actions with the mobile advertisement, and the MNO A 115A, the system 100 may move to block 680.  At block 680 the service\nprovider 130 may store the mobile advertisement and the association between the mobile advertisement, the MNO A 115A, and any selected mobile advertisement actions.\nFIG. 7 is a block diagram illustrating descriptions of several mobile advertisement actions 700 which may be served to users 120AA-NN as described in FIGS. 1-6 or in other systems for providing mobile advertisement actions.  The mobile\nadvertisement actions may be any of a plurality of action items presented as text, images, buttons, or any other suitable device to provide the users 120AA-NN with a direct link to any number of specified actions.  Mobile advertisement actions may\nprovide the users 120AA-NN with direct access to an action without having to navigate through multiple screens, applications, or various webpages to otherwise accomplish the action.  The mobile advertisement actions may reduce the amount of input the\nusers 120AA-NN must provide to accomplish a mobile action.  The revenue generator A 110A may select any number of mobile advertisement actions to be communicated to the user AA 120AA with the mobile advertisement.\nRevenue generator A 110A may provide relevant information to service provider 130 to enable mobile advertisement actions on an advertisement.  This may be accomplished during an initial set-up process or at a later time.  Mobile advertisement\nactions may be set to be static or dynamic.  Static mobile advertisement actions may be displayed with the advertisement every time the advertisement is displayed on a mobile device.  The phone number of revenue generator A 110A, which only has one\ncontact phone number, may be one such static mobile advertisement action.  In this case the phone number may be presented with the advertisement at all times allowing the user to click on a \"click to call\" mobile advertisement action 705 which directs\nthe mobile device of the user AA 120AA to dial the revenue generator A 110A who provided the advertisement.  Alternatively or in addition a dynamic mobile advertisement action may vary when displayed to a mobile device with the advertisement.  Revenue\ngenerator A 110A may set the mobile advertisement actions to vary based on time, date, day of the week, location of the user or other parameters.  For example, where a revenue generator A 110A is a chain of stores, the \"click to call\" action 705 may dial\na store of the revenue generator A 110A that is located closest to the location of the mobile device of the user AA 120AA.  The call actions described below have been given descriptions to better describe their functions.  However, it is to be noted that\ncall actions may be named or described by any suitable term even though they may have similar functions.\nOne mobile action may include the previously mentioned \"click to call\" action 705.  Clicking a \"click to call\" action 705 allows user AA 120AA to initiate a direct call to a phone number associated with the advertisement.  The mobile device of\nuser AA 120AA may dial the mobile advertisement action phone number directly without further input from user AA 120AA.  Revenue generator A 110A may need to provide a contact phone number during the set-up process to enable \"click to call\" action 705. \nAlternatively, a \"click to call\" action 705 may submit the phone number of user AA 120AA to the revenue generator A 110A for later callback from the revenue generator A 110A.  Revenue generator A 110A may provide several phone numbers to correspond with\nits various locations.\nAnother mobile action may include a \"click to message\" action 710.  Clicking a \"click to message\" action 710 allows user AA 120AA to send a message directly to the contact phone number or email of revenue generator A 110A.  The message may be\nany of the available messaging systems such as short message service (SMS) or Multimedia Messaging Service (MMS).  Additionally, the message may be sent from the email account of user AA 120AA.  The message may be any message such as a question or\nresponse to the advertisement from the user AA 120AA to the revenue generator A 110A.  The message may also be a message with an integrated picture, sound, or file that the user AA 120AA wishes to provide to the revenue generator A 110A.  Revenue\ngenerator A 110A may need to provide a phone number, email address, or other contact information to receive the message from user AA 120AA.\nAnother mobile action may include a \"click to chat\" action 720.  Clicking a \"click to chat\" action 720 allows user AA 120AA to initiate a chat session, such as a text chat, with revenue generator A 110A or another designated chattee.  The chat\nmay be conducted via a chat room, an instant messaging service, or any other available chat service.  Revenue generator A 110A may need to provide a contact chat service name such as for example its user name for YAHOO! MESSENGER.  If the mobile device\nof user AA 120AA has video transmission capability, the video transmission may be integrated with the \"click to chat\" action 720.\nAnother mobile advertisement action may include a \"click to location\" action 730.  A revenue generator A 110A wishing to direct a user AA 120AA to a physical location may communicate its location or locations to the service provider 130. \nClicking a \"click to location\" action 730 allows the user AA 120AA to be directed via a map, audio commands, text commands, or email directions, to the location provided by revenue generator A 110A.  Where the location of user AA 120AA is known from a\npositioning system locator, an IP address, a pre-set location, a user-provided location, a cell phone tower address, or other means, directions directly from the location of the user AA 120AA may also be provided.  If a revenue generator A 110A has more\nthan one designated physical location, the location most proximal to the location of user AA 120AA, or a pre-determined location, may be provided.  A user AA 120AA may also select which location of revenue generator A 110A they are interested in when\nthey select a \"click to call\" action 730.\nAnother mobile advertisement action may include a \"click to coupon\" action 740.  Clicking a \"click to coupon\" action 740 allows user AA 120AA to obtain a coupon or discount to the business of the revenue generator A 110A.  The revenue generator\nA 110A may provide the information for the coupons or discounts during the set-up process and/or update this information at a later time.  The revenue generator A 110A may change the coupons daily, weekly, or at any other selected time intervals.  After\nclicking the \"click to coupon\" action 740 the coupon may be communicated to user AA 120AA via SMS, MMS, email, or any other method of communication.  The coupons provided to user AA 120AA may be static and pre-selected by revenue generator A 110A before\nthe user AA 120AA submits any search requests.  Alternatively, the coupons may be dynamically selected and may be selected based on submitted user or request parameters.\nAnother mobile advertisement action may include a \"click to download\" action 750.  Clicking a \"click to download\" action 750 allows the user AA 120AA to download information or programs provided by revenue generator A 110A and communicated by\nservice provider 130.  A service provider 130 may, for example, provide a downloadable menu to a restaurant, a ringtone, a software program, or any other downloadable item.  The user AA 120AA may then download these items by clicking on the \"click to\ndownload\" action 750 that is displayed as part of an advertisement.\nAnother mobile advertisement action may include a \"click to make reservation\" action 760.  A revenue generator A 110A may provide information allowing the user AA 120AA to make a reservation at its business.  For example, if revenue generator A\n110A is a restaurant, the user AA 120AA may submit a reservation for dinner via a \"click to make reservation\" action 760.  In one instance a \"click to make reservation\" action 760 may be integrated with the \"click to call\" action 705, so that the call is\ndirected to the reservation section of the restaurant.  In another instance a \"click to make reservation\" action 760 may be integrated with a \"click to message\" action 710, so that the user AA 120AA sends a message to the restaurant with the number of\npeople or time of arrival to the restaurant.  The \"click to make reservation\" action 760 can be integrated with various other call actions.\nAnother mobile advertisement action may include a \"click to survey\" action 770.  A revenue generator A 110A may provide a survey to user AA 120AA.  Clicking the \"click to survey\" action 770 may direct the user AA 120AA to a mobile webpage with a\nsurvey.  Alternatively or in addition the survey may be delivered to user AA 120AA via an SMS or series of SMS, or through an associated email address.  Alternatively or in addition the user AA 120AA may be directed to the phone number associated with\nthe survey and may conduct the survey over voice communications.  In this way, a \"click to survey\" action 770 may function similarly to a \"click to call\" action 705.\nAnother mobile advertisement action may include a \"click to purchase\" action 780.  A revenue generator A 110A may put an image, description, video, or audio of a product into the advertisement.  Clicking a \"click to purchase\" action 780 may take\nthe user AA 120AA directly to a checkout page for the revenue generator A 110A, without the need to click any other links.  A user AA 120AA may pre-submit their credit card information and address to MNO A 115A or service provider 130 or revenue\ngenerator A 110A, allowing the \"click to purchase\" action 780 to act as an instant checkout feature from the revenue generator A 110A.\nAnother mobile advertisement action may include an \"email marketing\" action 790.  A revenue generator A 110A may provide email marketing materials that are delivered to the email of user AA 120AA on the activation of the \"email marketing\" action\n790.  Marketing materials may include newsletters, product information, lists or any other relevant materials.\nThese mobile advertisement actions are only several of those that are available with the integration of system 100.  These descriptions are non-limiting and non-exhaustive and are meant to describe only some of the available possibilities of\nsystem 100.\nFIG. 8 is an illustration 800 of an exemplary mobile device used in the systems of FIG. 1 and FIG. 2, or other systems for providing mobile advertisement actions.  The mobile web page 805 contains advertisements 830, 840 with mobile\nadvertisement actions 835, 836, 837, 838, 845, 846, 847, 848.  User AA 120AA may submit a search request to a search engine such as by typing a search term into a field on the webpage 805.  In this example, the user AA 120AA submits the keyword \"shoes\"\nvia a mobile web search engine.  The system 100 communicates the search results 810 to the mobile application 220AA of user AA 120AA.  The search results 810 are displayed to the mobile device via the mobile webpage 805.  The webpage 805 may contain the\nweb address of the resulting search page 820.  System 100 may also communicate the mobile advertisements 830, 840 of one or more of the revenue generators 110A-N, such as the revenue generator A 110A.  The mobile advertisement 830 may include a mobile\ncall action 835, a mobile mapping action 836, a mobile coupon action 837 and a mobile messaging action 838.  The mobile advertisement 840 may include a mobile call action 845, a mobile mapping action 846, a mobile email action 847 and a mobile purchase\naction 848.  Other mobile advertisement actions are possible and can be created to accommodate the needs of revenue generator A 110A.\nThe mobile advertisements 830, 840 and the mobile advertisement actions 835, 836, 837, 838, 845, 846, 847, 848 may be targeted to the keyword searched for by the user AA 120AA.  Other call actions may include sending the revenue generator A 110A\na message, such as an instant message, or providing a direct link to discounted merchandise or a coupon of the revenue generator or others.  The advertisements 830, 840 may have a title 832, 842 and short description 833, 843 explaining the\nadvertisement.  Title 832, 842 may be clickable and direct the user to a web page or a mobile landing page of the revenue generator A 110A as explained earlier.  Mobile advertisement actions 835, 836, 837, 838, 845, 846, 847, 848 may be clickable\ndirectly from the mobile webpage 805 without clicking on any intermediary webpages.  Clicking any of the mobile advertisement actions 835, 836, 837, 838, 845, 846, 847, 848 may cause the mobile device of the user AA 120AA to execute the functionality of\nthe mobile advertisement action.  Alternatively, clicking on the titles 832, 842 or any area of the advertisements 830, 840 other than the mobile advertisement actions 835, 836, 837, 838, 845, 846, 847, 848 may directs the user AA 120AA to a new webpage\nwhere the advertisement is enlarged and displayed by itself on the screen of the mobile device, as shown in FIG. 10.  Alternatively, clicking on the titles 832, 842 or any area of the advertisements 830, 840 other than the mobile advertisement actions\n835, 836, 837, 838, 845, 846, 847, 848 may direct the user AA 120AA to a mobile web page of the revenue generator A 110A.\nRevenue generator A 110A may choose which of the mobile advertisement actions 705, 710, 720, 730, 740, 750, 760, 770, 780, 790 are displayed in the advertisements 830, 840.  Alternatively, the mobile advertisement actions 835, 836, 837, 838,\n845, 846, 847, 848 may be dynamically selected based on submitted user and request parameters.  Two different mobile advertisements 830, 840 may therefore have different mobile advertisement actions associated with them as shown in FIG. 8.\nFIG. 9 is an illustration 900 of an exemplary mobile device displaying an email message 905.  The email message 905 may have a header line 910, a message line 920, and may contain one or more mobile advertisements 930 with a mobile call action\n937, a mobile mapping action 935, and a mobile reservation action 936.  Other mobile advertisement actions may also be included, such as the mobile advertisement actions 605, 610, 620, 630, 640, 650, 660, 670, 680, 690 described in FIG. 6.  Mobile\napplications 220AA-NN may receive and process emails.  Advertisements may be selected and delivered via email similar to their selection and delivery through a mobile Web search.  For example, service provider 130 may automatically content match email\ntext 920 or header text 910 with keywords.  The resulting targeted advertisement 930 with mobile advertisement actions 935, 936, 937.  The mobile advertisement actions 935, 936, 937 associated with the advertisement 930 are clickable and allow a user AA\n120AA to click on any of the mobile actions 935, 936, 937 if they are interested in the advertisement 930.\nAdditionally, an advertisement 930 with mobile advertisement actions 935, 936, 937 displayed in the email 905 may have several advantages.  For example, it may allow a user AA 120AA to utilize mobile advertisement actions 935, 936, 937 without\nhaving to connect to the Internet.  This may be advantageous for users 120AA-NN who download their email at one time period, but want to click on mobile advertisement actions 935, 936, 937 when they do not have, or want, Internet access.  For example,\nthe mobile call action 937 may have the phone number of the revenue generator A 110A delivered with the email.  A user AA 120AA clicking on the mobile call action 936 may thus connect directly to the revenue generator A 110A without being connected to\nthe Internet.  Additional or alternative call actions may be included such as a call action to receive a follow-up call from a revenue generator A 110A or a call action to submit a packet of information.\nFIG. 10 is an illustration 1000 of a mobile device displaying a mobile landing page 1005 for a mobile advertisement 1030 of a revenue generator A 110A.  The mobile advertisement 1030 may include a title 1032, a description 1033, a mobile call\naction 1041, a mobile mapping action 1042, a mobile purchase action 1043, a mobile coupon action 1044, a mobile question action 1045, a mobile call action 1046, a mobile receive action 1047, a mobile survey action 1048, and a mobile download action 1049. The mobile advertisement actions 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049 may be any of the mobile advertisement actions 705,710, 720,730,740,750,760,770,780,790 or other mobile advertisement actions.  The mobile landing page 1005 may provide\nadditional information 1020 about the revenue generator A 110A, such as a description of the revenue generator, a website, a telephone number and/or an address.  Each mobile advertisement action 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049 to be\ndisplayed may be individually selected by revenue generator A 110A and may be changed dynamically to match a search keyword.  Alternatively, it may be selected by the service provider 130.  Because of the increased screen space allocated to the\nadvertisement, more mobile advertisement actions 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049 may be displayed in the advertisement 1030 than if less space was included, such as in the mobile advertisement 930.  The mobile call actions 1041,\n1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049 may provide the user AA 120AA with the actions of receiving coupons, sending questions to the revenue generator, having the revenue generator A 110A call the user AA 120AA or another, requesting to receive a\nnewsletter, taking a survey of the revenue generator, and downloading wallpaper or other information to display.\nThe service provider 130 may provide the mobile landing page 1005 to the user AA 120AA if the user AA 120AA clicks on the advertisement 840 without clicking on one of the mobile advertisement actions 845, 846, 846, 848.  On the mobile landing\npage 1005 more mobile actions may be displayed to the user AA 120AA than may be displayed on the mobile search page 805.  Additionally, information describing the revenue generator A 110A, such as the homepage address, phone number, and logo may continue\nto be displayed.  Alternatively or in addition clicking on the mobile advertisement 840 may direct the user AA 120AA to the homepage of the revenue generator A 110A.\nFIG. 11 illustrates a general computer system 1100, which may represent a service provider server 240, a third party server 250, an advertising services server 260, a mobile device or any of the other computing devices referenced herein.  The\ncomputer system 1100 may include a set of instructions 1124 that may be executed to cause the computer system 1100 to perform any one or more of the methods or computer based functions disclosed herein.  The computer system 1100 may operate as a\nstandalone device or may be connected, e.g., using a network, to other computer systems or peripheral devices.\nIn a networked deployment, the computer system may operate in the capacity of a server or as a client user computer in a server-client user network environment, or as a peer computer system in a peer-to-peer (or distributed) network environment. The computer system 1100 may also be implemented as or incorporated into various devices, such as a personal computer (PC), a tablet PC, a set-top box (STB), a personal digital assistant (PDA), a mobile device, a palmtop computer, a laptop computer, a\ndesktop computer, a communications device, a wireless telephone, a land-line telephone, a control system, a camera, a scanner, a facsimile machine, a printer, a pager, a personal trusted device, a web appliance, a network router, switch or bridge, or any\nother machine capable of executing a set of instructions 1124 (sequential or otherwise) that specify actions to be taken by that machine.  In a particular embodiment, the computer system 1100 may be implemented using electronic devices that provide\nvoice, video or data communication.  Further, while a single computer system 1100 may be illustrated, the term \"system\" shall also be taken to include any collection of systems or sub-systems that individually or jointly execute a set, or multiple sets,\nof instructions to perform one or more computer functions.\nAs illustrated in FIG. 11, the computer system 1100 may include a processor 1102, such as, a central processing unit (CPU), a graphics processing unit (GPU), or both.  The processor 1102 may be a component in a variety of systems.  For example,\nthe processor 1102 may be part of a standard personal computer or a workstation.  The processor 1102 may be one or more general processors, digital signal processors, application specific integrated circuits, field programmable gate arrays, servers,\nnetworks, digital circuits, analog circuits, combinations thereof, or other now known or later developed devices for analyzing and processing data.  The processor 1102 may implement a software program, such as code generated manually (i.e., programmed).\nThe computer system 1100 may include a memory 1104 that can communicate via a bus 1108.  The memory 1104 may be a main memory, a static memory, or a dynamic memory.  The memory 1104 may include, but may not be limited to computer readable\nstorage media such as various types of volatile and non-volatile storage media, including but not limited to random access memory, read-only memory, programmable read-only memory, electrically programmable read-only memory, electrically erasable\nread-only memory, flash memory, magnetic tape or disk, optical media and the like.  In one case, the memory 1104 may include a cache or random access memory for the processor 1102.  Alternatively or in addition, the memory 1104 may be separate from the\nprocessor 1102, such as a cache memory of a processor, the system memory, or other memory.  The memory 1104 may be an external storage device or database for storing data.  Examples may include a hard drive, compact disc (\"CD\"), digital video disc\n(\"DVD\"), memory card, memory stick, floppy disc, universal serial bus (\"USB\") memory device, or any other device operative to store data.  The memory 1104 may be operable to store instructions 1124 executable by the processor 1102.  The functions, acts\nor tasks illustrated in the figures or described herein may be performed by the programmed processor 1102 executing the instructions 1124 stored in the memory 1104.  The functions, acts or tasks may be independent of the particular type of instructions\nset, storage media, processor or processing strategy and may be performed by software, hardware, integrated circuits, firm-ware, micro-code and the like, operating alone or in combination.  Likewise, processing strategies may include multiprocessing,\nmultitasking, parallel processing and the like.\nThe computer system 1100 may further include a display 1114, such as a liquid crystal display (LCD), an organic light emitting diode (OLED), a flat panel display, a solid state display, a cathode ray tube (CRT), a projector, a printer or other\nnow known or later developed display device for outputting determined information.  The display 1114 may act as an interface for the user to see the functioning of the processor 1102, or specifically as an interface with the software stored in the memory\n1104 or in the drive unit 1106.\nAdditionally, the computer system 1100 may include an input device 1112 configured to allow a user to interact with any of the components of system 1100.  The input device 1112 may be a number pad, a keyboard, or a cursor control device, such as\na mouse, or a joystick, touch screen display, remote control or any other device operative to interact with the system 1100.\nThe computer system 1100 may also include a disk or optical drive unit 1106.  The disk drive unit 1106 may include a computer-readable medium 1122 in which one or more sets of instructions 1124, e.g. software, can be embedded.  Further, the\ninstructions 1124 may perform one or more of the methods or logic as described herein.  The instructions 1124 may reside completely, or at least partially, within the memory 1104 and/or within the processor 1102 during execution by the computer system\n1100.  The memory 1104 and the processor 1102 also may include computer-readable media as discussed above.\nThe present disclosure contemplates a computer-readable medium 1122 that includes instructions 1124 or receives and executes instructions 1124 responsive to a propagated signal; so that a device connected to a network 235 may communicate voice,\nvideo, audio, images or any other data over the network 235.  The instructions 1124 may be implemented with hardware, software and/or firmware, or any combination thereof.  Further, the instructions 1124 may be transmitted or received over the network\n235 via a communication interface 1118.  The communication interface 1118 may be a part of the processor 1102 or may be a separate component.  The communication interface 1118 may be created in software or may be a physical connection in hardware.  The\ncommunication interface 1118 may be configured to connect with a network 235, external media, the display 1114, or any other components in system 1100, or combinations thereof.  The connection with the network 235 may be a physical connection, such as a\nwired Ethernet connection or may be established wirelessly as discussed below.  Likewise, the additional connections with other components of the system 1100 may be physical connections or may be established wirelessly.  In the case of a service provider\nserver 240, a third party server 250, an advertising services server 260, the servers may communicate with users 120AA-NN and the revenue generators 110A-N through the communication interface 1118.\nThe network 235 may include wired networks, wireless networks, or combinations thereof.  The wireless network may be a cellular telephone network, an 802.11, 802.16, 802.20, or WiMax network.  Further, the network 235 may be a public network,\nsuch as the Internet, a private network, such as an intranet, or combinations thereof, and may utilize a variety of networking protocols now available or later developed including, but not limited to TCP/IP based networking protocols.\nThe computer-readable medium 1122 may be a single medium, or the computer-readable medium 1122 may be a single medium or multiple media, such as a centralized or distributed database, and/or associated caches and servers that store one or more\nsets of instructions.  The term \"computer-readable medium\" may also include any medium that may be capable of storing, encoding or carrying a set of instructions for execution by a processor or that may cause a computer system to perform any one or more\nof the methods or operations disclosed herein.\nThe computer-readable medium 1122 may include a solid-state memory such as a memory card or other package that houses one or more non-volatile read-only memories.  The computer-readable medium 1122 also may be a random access memory or other\nvolatile re-writable memory.  Additionally, the computer-readable medium 1122 may include a magneto-optical or optical medium, such as a disk or tapes or other storage device to capture carrier wave signals such as a signal communicated over a\ntransmission medium.  A digital file attachment to an e-mail or other self-contained information archive or set of archives may be considered a distribution medium that may be a tangible storage medium.  Accordingly, the disclosure may be considered to\ninclude any one or more of a computer-readable medium or a distribution medium and other equivalents and successor media, in which data or instructions may be stored.\nAlternatively or in addition, dedicated hardware implementations, such as application specific integrated circuits, programmable logic arrays and other hardware devices, may be constructed to implement one or more of the methods described\nherein.  Applications that may include the apparatus and systems of various embodiments may broadly include a variety of electronic and computer systems.  One or more embodiments described herein may implement functions using two or more specific\ninterconnected hardware modules or devices with related control and data signals that may be communicated between and through the modules, or as portions of an application-specific integrated circuit.  Accordingly, the present system may encompass\nsoftware, firmware, and hardware implementations.\nThe illustrations described herein are intended to provide a general understanding of the structure of various embodiments.  The illustrations are not intended to serve as a complete description of all of the elements and features of apparatus,\nprocessors, and systems that utilize the structures or methods described herein.  Many other embodiments may be apparent to those of skill in the art upon reviewing the disclosure.  Other embodiments may be utilized and derived from the disclosure, such\nthat structural and logical substitutions and changes may be made without departing from the scope of the disclosure.  Additionally, the illustrations are merely representational and may not be drawn to scale.  Certain proportions within the\nillustrations may be exaggerated, while other proportions may be minimized.  Accordingly, the disclosure and the figures are to be regarded as illustrative rather than restrictive.\nAlthough embodiments have been illustrated and described herein, it should be appreciated that any subsequent arrangement designed to achieve the same or similar purpose may be substituted for the specific embodiments shown.  This disclosure is\nintended to cover any and all subsequent adaptations or variations of various embodiments.  Combinations of the above embodiments, and other embodiments not specifically described herein, may be apparent to those of skill in the art upon reviewing the\ndescription.\nThe Abstract is provided with the understanding that it will not be used to interpret or limit the scope or meaning of the claims.  In addition, in the foregoing Detailed Description, various features may be grouped together or described in a\nsingle embodiment for the purpose of streamlining the disclosure.  This disclosure is not to be interpreted as reflecting an intention that the claimed embodiments require more features than are expressly recited in each claim.  Rather, as the following\nclaims reflect, inventive subject matter may be directed to less than all of the features of any of the disclosed embodiments.  Thus, the following claims are incorporated into the Detailed Description, with each claim standing on its own as defining\nseparately claimed subject matter.\nThe above disclosed subject matter is to be considered illustrative, and not restrictive, and the appended claims are intended to cover all such modifications, enhancements, and other embodiments, which fall within the true spirit and scope of\nthe description.  Thus, to the maximum extent allowed by law, the scope is to be determined by the broadest permissible interpretation of the following claims and their equivalents, and shall not be restricted or limited by the foregoing detailed\ndescription.", "application_number": "15697753", "abstract": " A system for providing mobile advertisement actions may include a memory\n     to store a request, mobile carrier data, mobile advertisement data, and\n     mobile advertisement action data. The system may include an interface\n     operatively connected to the memory to communicate with a mobile device.\n     The system may include a processor operatively connected to the memory\n     and the interface. The processor may receive information and a request\n     from the mobile device via the interface and may determine the mobile\n     carrier data relating to a mobile carrier associated with the mobile\n     device. The processor may identify the mobile advertisement data and the\n     mobile advertisement action targeted to the request and the mobile\n     carrier data. The processor may append the mobile advertisement action\n     data to the mobile advertisement data. The processor may provide the\n     mobile advertisement data with the appended mobile advertisement action\n     data to the mobile device via the interface.\n", "citations": ["6009409", "6026368", "6047310", "6199045", "6205193", "6298330", "6334108", "6343317", "6377793", "6381465", "6401075", "6480713", "6510515", "6654725", "6681107", "6714975", "6795710", "6822663", "6826572", "6907566", "6928615", "6968175", "6985742", "7043483", "7120235", "7136661", "7200853", "7277718", "7283974", "7363024", "7487112", "7593721", "7606918", "7657520", "7660581", "7729945", "7801892", "8788344", "8838079", "20020010759", "20020046259", "20020073034", "20020077897", "20020078101", "20020083441", "20020083442", "20020128908", "20020129137", "20020161791", "20020164004", "20030046161", "20030055725", "20030149938", "20030154446", "20040039733", "20040044571", "20040054576", "20040088212", "20040093327", "20040186776", "20040194130", "20050010477", "20050154746", "20050174975", "20050228797", "20050289113", "20060004627", "20060026069", "20060041638", "20060085419", "20060100928", "20060116926", "20060149630", "20060172697", "20060184512", "20060194572", "20060194595", "20060242013", "20060271524", "20060282408", "20060288000", "20070018952", "20070027852", "20070042754", "20070050253", "20070061334", "20070088801", "20070112739", "20070121846", "20070174258", "20070174490", "20070190941", "20070192318", "20070198339", "20070213069", "20070214043", "20070214048", "20070233565", "20070233566", "20070264987", "20070288318", "20070294725", "20080032703", "20080221983", "20080256050", "20090049090", "20100086107"], "related": ["14143872", "12059460"]}, {"id": "20180012285", "patent_code": "10373232", "patent_name": "System and method for coordinating and monitoring a plurality of websites", "year": "2019", "inventor_and_country_data": " Inventors: \nKilloran, Jr.; John P. (Albuquerque, NM)  ", "description": "<BR><BR>BACKGROUND\nThe internet has brought unprecedented amounts of information to the fingertips of users across the globe.  As the quality of search engine algorithms has continued to improve, it has been easier for users to receive search results that are\ncloser to their expectations.  Accordingly, access to large volumes of information has provided many benefits to internet users, such as providing a more solid foundation upon which users may base their decisions.\nThere has been no greater effect by the internet on preexisting businesses than that on retail stores and their business models.  Shoppers were once relegated to searching physical stores within the vicinity of their homes.  Shoppers now have\naccess to retail and virtual stores around the globe, and can compare and contrast products, prices and information regarding all manner of products and services from those retailers.  This has given shoppers unparalleled power to shop for the best price\nand the specific product for which they are looking.\nHowever, there has been a downside to this proliferation of information.  There are now so many retail websites, with so much information on each website, that the shopping experience has begun to lead to information overload and shopping\nfatigue.  It becomes difficult for shoppers to keep track of which website(s) on which they have found a particular item, or which items on a particular website they have been interested.  The shopper has become frustrated, and the shopping experience is\ngreatly diminished as a result.\nThe designers of retail websites understand this frustration and have instituted measures to help shoppers in this regard.  Most retail websites now allow a user to put things in a \"shopping cart\" for later purchase, or even permit a shopper to\ncreate a list of items in which they are interested.  However, shoppers have to establish the shopping cart or list for each website.  This typically includes logging in to each website and entering personal information.  This process is inconvenient and\ntime-consuming for the shopper and becomes yet another source of frustration.  Additionally, such shopping carts are static, and provide no further guidance to shoppers.\nA system for accessing large amounts of shopping data across multiple websites and providing dynamically portions of data to shoppers in a user selective format is greatly needed.\n<BR><BR>SUMMARY\nA system for coordinating information from a plurality of websites and using this information to create a dynamic watchlist is provided.  The system selectively provides this information to a user.  In one embodiment, the system permits a user\nto access a plurality of websites and identify particular items of those websites that are of interest.  The system stores the identified items on a watchlist for later retrieval, review, transmission or action by the user.  The system may periodically\ninform the user of any changes to the items of interest. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nA more detailed understanding may be had from the following description, given by way of example in conjunction with the accompanying drawings wherein:\nFIG. 1 shows a block diagram of a system architecture for creating a watchlist in accordance with the present invention;\nFIG. 2 shows a plurality of items for sale on a retail website;\nFIG. 3 shows one of the plurality of items for sale on the retail website with a button permitting the user to add the item to a watchlist;\nFIG. 4 shows an item as added to a watchlist;\nFIG. 5 shows a plurality of items added to the watchlist of FIG. 4;\nFIG. 6 shows a watchlist parameter configuration screen;\nFIG. 7 shows an updated watchlist of FIG. 4 as transmitted to a user showing any updates that were made since the last transmission of the watchlist;\nFIG. 8 is a flow diagram of a method in accordance with the present invention;\nFIG. 9 shows a second embodiment for adding items to a watchlist from a plurality of websites.\nFIG. 10 shows a third embodiment for adding items to a watchlist via widget on a website (WOW) functionality implemented on at least one website;\nFIG. 11 shows a fourth embodiment for adding items to a watchlist in response to an email; and\nFIG. 12 shows an example system that may be used to implement features described herein with reference to FIGS. 1-11.\n<BR><BR>DETAILED DESCRIPTION\nFIG. 1 shows an example logical architecture 114 for email-based e-commerce and e-donation solicitations and collection.  The example architecture 114 an e-commerce system 100, a payment processing system 136, a vendor interface system 137, a\nvendor order fulfillment system 138, and a product delivery system 139, and may interface with a plurality of user devices 120a-120n and a plurality of vendor websites 130a-130n.\nThe e-commerce system 100 may include an account management module 102, a database module 104, an e-commerce database 106, a website module 107, an order execution module 108, a product delivery module 109, a message processing module 110, a web\nbrowser module 135 and an email interface module 112.\nAlthough not explicitly shown in FIG. 1A for simplicity, one or more of these modules may be functionally linked together as appropriate.  As those of skill in the art would appreciate, the modules may be embodied as physical units that are\nphysically linked together via one or more electronic bus structures, may be functional units that are functionally linked together via one or more communication protocols, or there may be a combination of physical or functional units.  They may be\nprogrammed via software or firmware in a plurality of physically separate units, or may be incorporated into a single programmable unit.  Accordingly, although the e-commerce system 100 may be described herein after as distinct and specific units or\nmodules, this is for convenience of explanation, and these modules, systems or units may be variously combined into one or a plurality of units without departing from the spirit and scope of the present invention.  Likewise, although the systems that\ninterface with the e-commerce system 100 (for example the vend or interface system 137, the payment processing system 136, the order fulfillment system 138 and the product delivery system 139), are described as being separate and distained from each\nother and the e-commerce system 100, they may, in whole or in part, be incorporated into the e-commerce system 100 or part of one or more third-party systems.\nAs will be described in further detail hereinafter, the e-commerce system 100 and user devices 120a-120n may communicate to initiate and manage transactions such the searching, processing, tracking and purchase of goods and or/donations.  As\nwill also be described in further detail hereinafter, the e-commerce system 100 may communicate with the payment processing system 136, the vendor interface system 137, the vendor order fulfillment system 138 and the product delivery system 139 to\nexecute the transactions.  Additionally, one or more of such modules may be optional, for example, in an e-commerce configuration utilized for certain donations, the order execution module 108 and the product delivery module 109, and the order\nfulfillment system 138 and the product delivery system 139 may not be necessary.\nThe account management module 102 may manage data related to accounts for users and vendors that participate in commerce via the e-commerce system 100.  The account management module 102 may be or include, for example, a web application.\nVendors may interact with the account management module 102 via the vendor interface system 137, which may comprise a web browser or a proprietary closed system.  As one example, a vendor may provide information via the vendor interface system\n137 to the account management module 102 such as: product, pricing or donation information to be used for email advertisements to be sent to users in email campaigns; periodic and/or aperiodic updates regarding product, pricing and donation information\nto be sent to users as selected by the users; email formatting information for email advertisements to be sent to users; financial information related to bank accounts and/or other types of financial accounts, (such as e-payment accounts such as PayPal\naccounts), for receiving payments from users of the e-commerce system 100, such as account numbers and/or other identifying information; and/or other information.\nUsers may interact with the account management module 102 via the web browser module 135.  The web browser module 135 provides functionality for user access to the e-commerce system 100.  Users may register with the e-commerce system 100 by\ninteracting with the web browser module 135 via their web browser.  The web browser module 135, in turn, interfaces with the account management module 102.  This permits the account management module 102 and the web browser module 135 to support\nindividualized functionality for each user.  It should be noted that the web browser module 135 and the vendor interface system 137 may comprise a single module.\nAs will be described in detail hereinafter, the account management module 102 may track the shopping trends or statistics of the user including items browsed, items put on a watch list, items that were eventually purchased either for the user,\n(or as gifts for other people), or, if the user inputs such information, a history gifts browsed or purchased for specific individuals.  The trends or statistics may include the length of time that a user spends on in item or certain category of items,\nthe number of \"clicks\" on an item or certain category of items, (such as reviewing other user reviews, comparing several items or the like), the amount of time spent on viewing the item or type of item, or the number of times a user \"returns\" to an item\nor type of item during one or more shopping experiences.  This allows the tracking and statistical determination of the likes and dislikes of each user.  The account management module 102 may retain certain information regarding each user that the user\ndirectly inputs; for example, in response to one, or a series of website product questionnaires or surveys.  All of this user-specific information is collected and stored.\nA user of the user client device 120 may provide information to the account management module 102 via the web browser module 135 such as: an email address associated with the user; financial information associated with the user, for example a\ncredit card information (such as a credit card number and expiration date), and/or other information related to bank accounts and/or other types of financial accounts (such as e-payment accounts) that may be used to make payments to vendors via the\ne-commerce system 100; shipping address information; billing address information; preferences regarding which vendors or non-profit organizations the user would like to receive email advertisements or e-donation solicitations from; and/or other\ninformation.\nIn a separate embodiment, the financial information of a user, such as bank account asset or credit card information, may be supplied to the account management module 102 for analysis of user preferences.  This permits the account management\nmodule 102 to analyze all of the information for user preferences and to suggest items, services or non-profit organizations, in which the user may be interested.  These suggestions may be emailed to the user for adding to a watchlist as will be\ndescribed in detail hereinafter.\nThe account management module 102 may, either alone or via the database module 104, store information received from the user device 120 and/or the vendor website 130a-130n in the e-commerce database 106.  The account management module 102 may\nalso add information to the e-commerce database 106 when users and vendors register with the e-commerce system 100.  This information may include one or more user identifiers, vendor identifiers and other identifying information.\nThe message processing module 110, (in conjunction with the email interface module 112), may generate advertisement or product update email messages, or e-donation solicitations for users that are registered with the e-commerce system 100, such\nas the users of the user devices 120a-120n.  The advertisement, product update or e-donation email messages may be HyperText Markup Language (HTML) email messages, Rich Text Format (RTF) email messages, and/or may be defined according to any other\ncurrent or future appropriate format.  The advertisement, product update or e-donation email messages may include Uniform Resource Identifiers (URIs) or hyperlinks that are defined according to the mailto URI scheme.  Each mailto URI or hyperlink may\ndescribe an email message that may be generated by an email client module (such as the email client module 122) when that URI or hyperlink is selected.\nThe generated email message may include a number of parameters that indicate, for example, a product or fundraising campaign that is described in the email message that the user wishes to purchase (or donate to).  The generated message may\nalternatively include a number of parameters that indicate, for example, updated information regarding a product or fundraising campaign that a user has been tracking for possible future purchase or donation.  The generated message may then be sent from\nthe e-commerce system 100 to users from the email interface module 112.\nThe email interface module 112 may be configured to use one or more email accounts that are associated with the e-commerce system 100, and to send and receive messages associated with the one or more email accounts.  The email interface module\n112 and/or the email client module 122 in the user client device 120 may communicate email messages using technologies such as Simple Mail Transfer Protocol (SMTP), Post Office Protocol (POP) technology, Internet Message Access Protocol (IMAP), Remote\nProcedure Call (RPC) technology, HyperText Transfer Protocol (HTTP), and/or other current or future appropriate technologies.  The interface module 112 and/or the email client module 122 may use these technologies to transmit and/or receive email\nmessages via one or more email servers (not depicted), which may be directly interface with the e-commerce system 100, or may be a third party service such as constant contact or mail chimp.\nThe email client module 122 may be, or include, an email client such as Microsoft Outlook, Thunderbird, a web browser application, or any other current or future client application for the communication of email messages.  In an instance where\nthe email client module 122, is or includes, a web browser application, the email client module 122 may be the same web browser described hereinbefore that may be used to communicate with the account management module 102.  Alternatively, the email\nclient module 122 and the web browser described above that may be used to communicate with the account management module 102 may be different.  As will be described in detail hereinafter, through the email client 122 on the user device 120a, the user may\ntake one or more actions, such as replying to the email message, selecting one or more reply-to links in the email message, adding one or more items from the email message to a watchlist or the like.\nThe payment processing system 136 handles financial transactions associated with the purchase of an item or service, or a donation.  The payment processing system 136 may be, as one example, a payment gateway operated by a financial institution. In an instance where the payment processing system 136 is a payment gateway, the payment processing system 136 may have a connection to one or more banking networks (not depicted) that it may use to process payments.  The order execution module 108 may\ncommunicate with the payment processing system 136 using technology such as Transport Layer Security (TLS) or Secure Sockets Layer (SSL) technology.  The vendor order fulfillment system 138 may be an order management system (OMS), Enterprise Resource\nPlanning (ERP), supply chain management, electronic purchasing system, inventory control system, or any other appropriate system for fulfilling orders.\nThe e-commerce database 106 may store information such as information that describes and/or comprises email campaigns, email advertisements or fundraising campaigns that may be sent to users, user information, vendor information, product\ninformation, product information updates and history, donation-related information, order status information, and/or other information.  Further, the e-commerce database 106 may store information that indicates correspondences between different email\ncampaigns, advertisements, fundraising campaigns, users, vendors, products, donations, information related to order statuses, and/or other information.  For each transmitted email, the e-commerce database 106 may store information that includes an\nidentifier of the vendor associated with the particular email campaign, identifiers of the products or fundraising campaign associated with the email campaign, and/or other information.  For each order that is placed with the e-commerce system 100, the\ne-commerce database 106 may store information such as an identifier of the user that placed the order or e-donation, when the order or e-donation was placed, an identifier of the vendor or non-profit organization associated with the order or the\ne-donation, and/or other information.\nFor each product described or type of e-donation in the e-commerce database 106, the e-commerce database 106 may store information that includes an identifier of the product or e-donation, a description of the product or e-donation, a title, an\nidentifier of the vendor or non-profit organization associated with the product or e-donation, a cost of the product or amount of the e-donation, and/or other information.  According to one approach, data that describes donations may be handled within\nthe e-commerce database 106 in a similar matter to the data that describes products.\nAs one example for a product, the e-commerce database 106 may store information that indicates that a type of wine (e.g., \"Wine One,\" with a product identifier of \"0005\") is sold by a vendor (e.g., \"The Wine Shop,\" with a vendor identifier of\n\"0163\") for $15.00.  In an e-donation example, the e-commerce database 106 may store information that indicates a non-profit organization (e.g., \"Charitable Organization,\" with a vendor identifier of \"1043\") may receive donations for $5.00, $10.00,\n$25.00, $50.00, and $100.00.  Each of the different donation amounts may be stored in the e-commerce database 106, (similar to different \"products\").  For example, the $100.00 donation may have a product identifier (or \"donation identifier\") of \"0099.\"\nFurther according to this example, the e-commerce database 106 may store information that indicates that the product with identifier \"0099\" indicates a donation of $100.00 to a vendor with the identifier of \"1043.\" Alternatively or additionally, the\ne-commerce database 106 may store information (e.g., one or more flags or other fields) that indicates, for each product/donation, whether the product/donation relates to a donation or to a purchase of goods.  It should be noted that the identifiers\nshown above are set forth by way of example only.  Preferably, these identifiers randomized and/or encrypted for heighted security when sent in on email message.\nThe e-commerce database 106 may be spread across one or any number of computer-readable storage media (not depicted).  For example, a portion of the e-commerce database 106 (i.e. sensitive data related to members) may reside on a vendor's or\nnon-profit organization's website and may be accessed through the vendor interface system 137.  The e-commerce database 106 may be or include, for example, a relational database, a hierarchical database, an object-oriented database, a flat file, a\nspreadsheet, or a structured file.  The database module 104 may interface with a database management system (not depicted) in order to add data to, modify data in, or obtain data from the e-commerce database 106.  Alternatively or additionally, the\ndatabase module 104 may perform database driver and/or database client functionality to interact with the database management system.  The database management system may be based on a technology such as Microsoft SQL Server, Microsoft Access, MySQL,\nPostgreSQL, Oracle Relational Database Management System (RDBMS), Not Only SQL (NoSQL), or any other appropriate technology.\nThe website module 107 provides all of the functionality to support a working website.  These functions include displaying a plurality of web pages to a user and providing the standard functionality of a webpage such as graphics and video, links\nto different parts of the website, user login and authentication, support for purchases, security features and an interface to user support.  The website module 107 provides the graphical interface and user experience to the web user, and supports the\nfunctionality as will be described in detail hereinafter.  It should be noted that the web-site module 107 may be provided by a third party which hosts websites.\nFIG. 2 shows a website displaying a plurality of consumer items for sale.  Although in this example, the consumer items are men's winter coats, they may be any type of item, service or gift (including a donation) available for selection and\npurchase.  Each item is associated with a plurality of attributes, (which are not shown in FIG. 2).  The attributes may be one or more words, numbers or other identifiers that may be used to describe, or may be associated with, the item.  In one example,\nthis may include brand, size, price, type (for example winter, men's, coat, hoodie, waterproof), color, model, etc.\nAs those of skill in the art would understand, there may also be many attributes associated with each consumer item that are not intended for the users to utilize, but rather they are for the website proprietor or vendors to utilize.  Any, or\nall, of these attributes may be used for tracking and evaluation by the e-commerce system 100.  For example, a \"program\" attribute may be used by a vendor to track the success of a vendor's current promotional effort to sell a certain line of clothing,\nor grouping of clothing lines and accessories.  In one example, the \"program\" attribute may identify certain coats as from the Arcteryx company.\nBy further example, a \"linking\" attribute may be used by the website proprietor to track the success of their effort to \"upsell\" to the user, for example from a less expensive to a more expensive item, or to have the user buy accessories to the\nitem that the user has selected.  The linking attribute will associate two or more products together; most likely similar products that increase in price.  These attributes will be described in greater detail hereinafter.\nReferring back to FIG. 2, the user may browse through the items, comparing features among different items.  Having looked at all of the attributes, the user may view a particular item and, concluding that this item is the item they were looking\nfor, select the item, (such as a specific coat).  They may select the item as indicated by placing the cursor over the picture of the item or the select button and clicking on it.  The selection in this example is a men's yellow hoodie pullover coat as\nindicated by the cursor 200.\nThis selection brings the user to another web page with more detailed information regarding the item as shown in FIG. 3.  For example, the user may read other user reviews, manufacturer information and warranty information.  However, the price\nof $259 may be greater than what the user is willing to pay at this time.  Accordingly, instead of putting the item into their shopping basket and proceeding to purchase using the \"Purchase\" button 302, the user may select the watch list button 304,\nindicated in FIG. 3 as \"Watch\".\nSelection to put the item on the watch list opens up a user's watch list 400, which is shown in FIG. 4.  The watchlist 400 keeps track of the selected item and all of the relevant associated information at the time of selection by the user.  It\nshould be noted that the amount and type of information that is displayed on the watchlist 400 is configurable by the user.  Therefore, more or less information, than shown in FIG. 4 as desired by the user may be displayed or stored.  In the example\nshown in FIG. 4, the picture 402, the manufacturer 404, the type 406, the price 408, the website 412 from which the item was added and the watchlist \"add\" date are all stored.  The user may create a more extensive watchlist 400 by repeating the method\ndescribed with reference to FIGS. 2-4 and adding more items.  This more extensive watchlist 500 is shown in FIG. 5, by way of example.\nReferring to FIG. 5 additional items are shown that the user has put on the watchlist 500, including performance socks and an extreme weather backpack.  All of these items and their associated information are stored for later use by the user or\nthe vendor (or the non-profit organization as appropriate) as will be described in detail hereinafter.  The user may exit the website by selecting the \"Exit Website\" button 502, or my resume shopping by selecting the \"Resume Shopping\" button 504.\nIn an optional embodiment, the first time that a user creates (or the e-commerce system 100 automatically creates) a watchlist, they may be taken to a \"watchlist parameter\" screen 600 as shown in FIG. 6.  This watchlist parameter screen 600\npermits the user to configure the features of the watchlist to suit their needs.  The user must first enter the email address 602 to which they want the watchlist to be sent.  Alternatively, the system 100 may \"remember\" the users website, and autofill\nthat particular email address (and the remainder of this information to be described herein with reference to FIG. 6).  Next, the user selects a name 604 for the watchlist (for example \"Christmas\", or \"grandmom's birthday\").  The user then selects an\nactivation time 606 specifying when they want to watchlist to be emailed.  This activation time 606 may be a specific date and/or time, or may be periodic for example hourly, daily, weekly monthly or upon changes.  Once a watch list is generated, for\nexample the one shown is FIG. 5, it may be stored in stored in the e-commerce database 106.  As will be discussed in greater detail hereinafter, the system 100 uses the watch list, and optionally the user's preferences, to generate an active watch list.\nIf the user selects an activation time 606 of hourly, (for example, in the case of an auction), daily, weekly, or monthly, the watchlist is emailed to them on the selected periodic basis.  In contrast, the \"upon changes\" selection permits the\nuser to receive an emailed watchlist when some aspect of the watchlist has changed.  This is shown in FIG. 7.  The \"other\" option permits the user to select when the want to be notified, including a specific periodicity, selected dates, or specific times\nor occasions.\nReferring to FIG. 7, the watchlist 700 as shown is emailed in response to the \"upon changes\" selection, and the watchlist indicates two changes: a change to the price of the first item (the Beta AR Jacket) 702, and an additional available color\nof the second item (the performance socks) 704.  The new price may also be shown 706.  The user may take action by buying one of the items on the watchlist or deleting one of the items, or may take no action.\nIt should be noted that the user may create a plurality of different watchlists.  For example, the user may create a Christmas watchlist for dad, a Hanukkah watchlist for mom, and/or a birthday watchlist for grandmom.  All of these watchlists\nmay be independently created, configured and emailed to the user.  For ease of use, the watchlists may be combined into create a single watchlist.\nThe \"auto search\" 608 feature shown in FIG. 6 permits the system 100 to automatically search website (or optionally, multiple websites) for items that are similar to the items that the user selects for placing on their watchlist.  The user may\nbe notified separately of these additional items via email or via a separate section of the watchlist.  The system 100 suggests other similar alternatives to the user and asks the user for permission to add these items on their watchlist.\nThe \"auto add\" 610 feature shown in FIG. 6 goes one step further than the auto search 608 feature, in that it automatically adds the similar items to the website without prior authorization from the user.  As would be understood by those of\nskill in the art, additional limitations may be set in place as shown in FIG. 6 to ensure that too many additional items may not be added.  By way of illustration, a limitation of 2 additional similar \"auto-added\" items per selected item, or 5 additional\nauto-added items overall may be set by the user.\nThe method 800 of this embodiment is shown with reference to FIG. 8.  The user browses the selected website and searches for desired items 802.  The user selects the desired item(s) to put on their watchlist 804.  A determination 806 is made to\nsee if a watchlist was already previously created.  If a watchlist has not been created, a watchlist is created 808.  The user configures the watchlist in accordance with their preferences 810, and the item(s) are put on the watchlist.  It should be\nnoted that steps 806-810 may be optional in that a default setting may be used to automatically create a \"standard\" or \"base\" watchlist as shown in FIGS. 4, 5 and 7.  Therefore, in this alternative embodiment, the watchlist is automatically created by\nthe e-commerce system 100 when a first watchlist item is added.\nIf the watchlist was already created, the method 800 skips to the step of putting items on the watchlist 812.  The watchlist is then stored 814.  The watchlist is then emailed 816 to the user on the frequency selected during the user's\nconfiguration of their watchlist, or on a default frequency or setting.\nIt should be understood by those of skill in the art that the presently inventive e-commerce system 100 is flexible to utilize in a variety of different manners.\nIn one example, a user may put one or more items that have a limited quantity or limited availability duration.  If a website provides quantity or availability duration information along with the other information provided about an item, the\nuser may also put such an item on their watchlist for tracking.  One such example may be movie tickets, (or likewise concert or event tickets) for a particular event at a particular time.  The user may put such an item on their watchlist, and as either\nthe event time approaches or the tickets are within a predetermined number of tickets from becoming sold out, the user may be notified by receiving the transmitted watchlist with the updated information.  This same notification may work for items for\nwhich the stock is running out.\nLikewise, a user may put items on their watchlist that are not yet available, such as show, concert or sporting event tickets.  Once these tickets become available the user may be notified via the parameters set forth on their watchlist.\nReferring to FIG. 9, a second embodiment is shown.  This embodiment is similar to the aforedescribed embodiment of FIGS. 1-7, except that the watchlist of this embodiment may be generated from items on a plurality of websites.  Each of the\nwatchlist items may be from different websites as is graphically illustrated in FIG. 9.  The user may browse a plurality of websites, which are illustrated by example as Amazon.com, LLBean.com and Models.com.  As the user browses the websites, and\nselects one or more items, these items are placed in their one or more watchlists.\nThis functionality provides the user with exceptional convenience in tracking a plurality of items over a plurality of websites in a single, central location, while receiving updates regarding those items in a manner in which the user has\nselected.\nThere are at least two alternatives for permitting items from multiple websites to be added to a watchlist; a first alternative captures the information of an item that the user wants to place on their watchlist from the particular website, and\nstores this information in the e-commerce system 100; and the second alternative utilizes functionality (called WOW, for \"widget on a website\") at each website to communicate with the e-commerce system 100 and transmit this information and any updated\ninformation to the e-commerce system 100.  These two alternatives will be explained in detail hereinafter.\nIn the first alternative, the user browses one or more websites, and when they have identified an item that they want to add to their watchlist, they perform a \"screen capture\", on the particular item.  This information must be saved, and then\nlater pasted into an email or a screen of the e-commerce system 100.  The screen is scrubbed for all relevant information and added to the watchlist.  The e-commerce system 100 may then periodically send out one or more web crawlers to update this\ninformation on a basis as set forth by the user.  One of skill in the art would appreciate that this alternative may meet with mixed results depending upon the information that is available on each website.\nIn the second alternative, shown in FIG. 10, the WOW functionality is included on each \"participating\" website.  A participating website is a website that has incorporated WOW functionality, that will automatically transmit the desired\ninformation directly to the e-commerce system 100.  This permits the information for adding to the watchlist to be seamlessly added and also seamlessly updated as selected by the user.\nReferring to FIG. 10, a typical retail web page is shown.  The WOW functionality can be seen in the \"Add To Watchlist\" button at the upper right-hand corner of the web page.  The user may purchase an item as on any typical website by selecting\nthe \"Purchase\" button.  However, if the user alternatively selects the \"Add To Watchlist\" button, all relevant information associated with that item is automatically added to the user's watchlist.  If the user has a plurality of watchlists, a menu of the\nuser's watchlists will be displayed in response to selecting the Add To Watchlist button.  The user then selects the appropriate watchlist and the item is thereby added.  As will be understood by those of skill in the art, this scheme has tremendous\nadvantages over current systems.\nIn a third embodiment, items may be added to a user's watchlist in response to an email sent from the e-commerce system 100.  Such an embodiment is illustrated in FIG. 11.  Referring to the example set forth in FIG. 11, an email sent from the\ne-commerce system 100 is shown.  The email is branded as part of a fundraiser campaign from the American Red Cross.  This email solicits the user to buy flowers, whereupon the American Red Cross receives 33% of the proceeds.  As shown below each item,\n(shown as a flower arrangement), the user has the option of purchasing the item or adding the item to their watchlist, by selecting the appropriate button.\nIn a fourth embodiment, using the WOW functionality, if the user selects the \"Auto Add\" feature (as shown in FIG. 6), the e-commerce system 100 searches other WOW-affiliated websites and adds similar items (the ones selected for placing on the\nwatchlist by the user), automatically to the watchlist.  Since the WOW functionality permits all of the websites to communicate directly with the e-commerce system 100, the e-commerce system 100 is able to provide other relevant selections.  It should be\nnoted that this also provides the WOW-affiliated websites an tremendous opportunity to cross-sell to users that are actively searching for a particular product.\nIn another embodiment, the e-commerce system 100 may conduct a reverse auction among the one or more WOW-affiliated websites for the item(s) on the watchlist.  This permits the user to receive the best possible price on the watchlist item(s).\nFIG. 12 shows an example system 1300 that may be used to implement features described above with respect to FIGS. 1-11.  The example system 1300 includes an e-commerce server 1350, a client device 1370, and one or more networks 1380.\nThe e-commerce server 1350 may include at least one processor 1352, memory device 1354, network interface 1356, and storage device 1358.  The client device 1370 may include at least one processor 1372, memory device 1374, network interface 1376,\ninput device interface 1375, display device interface 1377, and storage device 1378.\nThe e-commerce server 1350 may be configured to perform any feature or combination of features described above with reference to FIGS. 1-11 as performed by the account management module 102, database module 104, order execution module 108,\nmessage processing module 110, and/or email interface module 112.  The client device 1370 may be configured to perform any feature or combination of features described above with reference to FIGS. 1-11 as performed by the email client module 122 and/or\nthe web browser module in the customer client device 120.  The client device 1370 may be, for example, a desktop computer, a laptop computer, a netbook, a tablet computer, a personal digital assistant (PDA), a cellular phone, or any other appropriate\ndevice.\nEach or any of the memory devices 1354, 1374 may be or include a device such as a Dynamic Random Access Memory (D-RAM), Static RAM (S-RAM), or other RAM or a flash memory.  Each or any of the storage devices 1358, 1378 may be or include a hard\ndisk, a magneto-optical medium, an optical medium such as a CD-ROM, a digital versatile disk (DVDs), or Blu-Ray disc (BD), or other type of device for electronic data storage.  The storage device 1358 in the e-commerce server 1350 may store the\ninformation or any subset of the information described above with reference to FIGS. 1-11 as stored in the e-commerce database 106.\nEach or any of the network interfaces 1356, 1376 may be, for example, a communications port, a wired transceiver, or a wireless transceiver.  Each or any of the network interfaces 1356, 1376 may be capable of communicating using technologies\nsuch as Ethernet, fiber optics, microwave, xDSL (Digital Subscriber Line), Wireless Local Area Network (WLAN) technology, wireless cellular technology, and/or any other appropriate technology.  The network interfaces 1356, 1376 may be used by the\ne-commerce server 1350 and/or the client device 1370 to communicate via the one or more networks 1380.  The network interface in the e-commerce server 1350 may be configured to communicate any of the messages and/or other information described above with\nreference to FIGS. 1-11 as communicated by the account management module 102, database module 104, order execution module 108, message processing module 110, and/or email interface module 112.  The network interface 1376 in the client device 1370 may be\nconfigured to communicate any of the messages and/or other information described above with reference to FIGS. 1-12 as communicated by the email client module 122 and/or by the web browser module in the customer client device 120 used for communicating\nwith the account management module 102.\nThe one or more networks 1380 may include one or more private networks and/or one or more public networks such as the Internet.  The one or more networks 1380 may be based on wired and/or wireless networking technologies.\nThe input device interface 1375 in the client device 1370 may be an interface configured to receive input from an input device such as a keyboard, a mouse, a trackball, a scanner, a touch screen, a touch pad, a stylus pad, and/or other device. \nThe input device interface 1375 may operate using a technology such as Universal Serial Bus (USB), PS/2, Bluetooth, infrared, and/or other appropriate technology.  The input device interface 1375 may be configured to receive any or any combination of the\nuser input described above with reference to FIGS. 1-11 as received by the by the email client module 122 and/or by the web browser module in the customer client device 120 used for communicating with the account management module 102.\nThe display device interface 1377 may be an interface configured to communicate data to a display device (not depicted).  The display device interface 1377 may operate using technology such as Video Graphics Array (VGA), Super VGA (S-VGA),\nDigital Visual Interface (DVI), High-Definition Multimedia Interface (HDMI), or other appropriate technology.  The client device 1370 may include or be connected to a display device (not depicted) via the display device interface 1377.  The display\ndevice may be, for example, a monitor or television display, a plasma display, a liquid crystal display (LCD), and/or a display based on a technology such as front or rear projection, light emitting diodes (LEDs), organic light-emitting diodes (OLEDs),\nor Digital Light Processing (DLP).  The display device may be configured to display, based on data received via the display device interface 1377, any display elements described above with reference to FIGS. 1-11 as displayed by the email client module\n122 and/or by the web browser module in the customer client device.\nThe memory device 1354 and/or the storage device 1358 of the e-commerce server 1350 may store instructions which, when executed by the at least one processor 1352, cause the at least one processor 1352 to perform any feature or combination of\nfeatures described above with reference to FIGS. 1-11 as performed by the account management module 102, database module 104, order execution module 108, message processing module 110, and/or email interface module 112.  The memory device 1374 and/or the\nstorage device 1358 of the client device 1370 may store instructions which, when executed by the at least one processor 1372, cause the at least one processor 1372 to perform any feature or combination of features described above with reference to FIGS.\n1-11 as performed by the email client module 122 and/or by the web browser module in the customer client device 120 used for communicating with the account management module 102.\nAlthough FIG. 12 shows a single e-commerce server 1350 and a single client device 1370, the functionality described above with reference to FIG. 12 as performed by the e-commerce serve 1350 and/or the client device 1370 may be distributed across\nany number of devices that possesses similar characteristics and/or that include similar components 1352, 1354, 1356, 1358, 1372, 1374, 11375, 1376, 1377 as the e-commerce server 1350 and/or the client device 1370.\nWhile examples are provided above with respect to the Figures which includes the use of email communications, the features described above with respect to the Figures may also be implemented using different types of communications technology. \nFor example, the features described above with reference to the Figures may also be implemented, mutatis mutantis, using technologies that include any one or any combination of: email; instant messaging; enterprise messaging; Short Message Service (SMS);\nMultimedia Messaging Service (MMS); and/or any other appropriate technology for the electronic communication of data.\nAs use herein, the term \"vendor\" broadly refers to and is not limited to a business, a non-profit organization, any other type of organization, and/or an individual person.  One example of a business is an online retailer.  Examples of\nnon-profit organizations include charitable organizations, educational institutions such as schools and universities, arts organizations, and recreational organizations.  Examples of recreational organizations include historical or preservation\nsocieties, local recreational sports leagues.\nAs used herein, the term \"processor\" broadly refers to and is not limited to a single- or multi-core general purpose processor, a special purpose processor, a conventional processor, a digital signal processor (DSP), a plurality of\nmicroprocessors, one or more microprocessors in association with a DSP core, a controller, a microcontroller, one or more Application Specific Integrated Circuits (ASICs), one or more Field Programmable Gate Array (FPGA) circuits, any other type of\nintegrated circuit (IC), a system-on-a-chip (SOC), and/or a state machine.\nAs used to herein, the term \"computer-readable storage medium\" broadly refers to and is not limited to a register, a cache memory, a ROM, a semiconductor memory device (such as a D-RAM, S-RAM, or other RAM), a magnetic medium such as a flash\nmemory, a hard disk, a magneto-optical medium, an optical medium such as a CD-ROM, a DVDs, or BD, or other type of device for electronic data storage.\nAlthough features and elements are described above in particular combinations, each feature or element can be used alone or in any combination with or without the other features and elements.  For example, each feature or element as described\nabove with reference to the Figures may be used alone without the other features and elements or in various combinations with or without other features and elements.  Sub-elements of the methods and features described above with reference to the Figures\nmay be performed in any arbitrary order (including concurrently), in any combination or sub-combination.", "application_number": "15709673", "abstract": " A system comprises at least one first processor and at least one second\n     processor. The at least one first processor is of a vendor website and is\n     configured to produce a web site including vendor product information and\n     a widget on the web site. The widget on the web site, in response to a\n     user input, has a vendor product added to a watchlist of a non-vendor\n     third party. The widget on the web site updates the watchlist of the\n     non-vendor third party. The at least one second processor of the\n     non-vendor third party adds the vendor product to the watchlist in\n     response to the widget on the web site. The at least one second processor\n     updates the watch list in response to the widget on the web site. The at\n     least one second processor sends the watchlist to a user.\n", "citations": ["5426781", "5664110", "5694546", "5706442", "5710887", "5758328", "5793972", "5794206", "5799157", "5809242", "5825881", "5826242", "5826269", "5838790", "5848397", "5855008", "5864823", "5870717", "5890138", "5897622", "5899980", "5905973", "5970469", "6101485", "6246996", "6690407", "6938048", "6954737", "6993559", "7533064", "7577587", "7912910", "8156012", "20020010746", "20020065828", "20020103752", "20020120581", "20020152200", "20020178360", "20030172005", "20030182263", "20030217107", "20040024655", "20050044003", "20050251146", "20070022007", "20090254447", "20090276345", "20100010886", "20100131523", "20110202615", "20110264555", "20110295749", "20120109781", "20120276868"], "related": ["13456997", "61524898"]}, {"id": "20180034755", "patent_code": "10374996", "patent_name": "Intelligent processing and contextual retrieval of short message data", "year": "2019", "inventor_and_country_data": " Inventors: \nSaoji; Govind (Hyderabad, IN), Vangala; Vipindeep (Hyderabad, IN), Gill; Deepinder (Hyderabad, IN)  ", "description": "<BR><BR>BACKGROUND\nBusinesses have increasingly moved their marketing, advertising and other direct-to-consumer communications away from paper delivery and towards digital media communications.  As an effect of this transition to electronic content delivery, the\nlow cost of making electronic content delivery and the ease of delivering electronic content to large groups of individuals, there has been an increase in the number of spam messages and other malicious content that individuals receive on a daily basis.\nIt is with respect to this general technical environment that aspects of the present technology disclosed herein have been contemplated.  Furthermore, although a general environment has been discussed, it should be understood that the examples\ndescribed herein should not be limited to the general environment identified in the background.\n<BR><BR>SUMMARY\nThis summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description section.  This summary is not intended to identify key features or essential features of the claimed\nsubject matter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter.\nNon-limiting examples of the present disclosure describe systems, methods and devices for providing contextualized short message service (\"SMS\") data from one or more SMS messages to a client.  SMS data from a client may be received.  The SMS\ndata may be filtered with one or more filters.  Relevant information from the filtered SMS data may be extracted.  The extracted information may be categorized into one or more contextual categories in a tiered contextual content hierarchy, the\ncategorizing comprising analyzing world knowledge related to the extracted information.  Searchable context metadata may be associated with the categorized information.  An indication to provide feedback related to the received SMS data may be received\nfrom the client and feedback related to the received SMS data may be provided to the client based on the associated context metadata. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nNon-limiting and non-exhaustive examples are described with reference to the following figures:\nFIG. 1 is a schematic diagram illustrating an example distributed computing environment for providing contextualized SMS data from one or more SMS messages to a client.\nFIG. 2 illustrates an example scenario whereby an SMS message received in a first context is contextualized and the contextualized SMS data is provided to a user during a second context based on a reactive indication to provide feedback received\nby an SMS data processing service.\nFIG. 3 illustrates an example scenario whereby an SMS message received in a first context is contextualized and the contextualized SMS data is provided to a user during a second context based on a proactive indication to provide feedback\nreceived by an SMS data processing service.\nFIG. 4 illustrates an example scenario whereby an SMS message received in a first context is contextualized and the contextualized SMS data is provided to a user during a second context based on a proactive indication to provide feedback\nreceived by an SMS data processing service.\nFIG. 5 illustrates an example scenario whereby three SMS messages received from three separate contexts are contextualized and the contextualized SMS data is provided to a user during a fourth context based on a reactive indication to provide\nfeedback received by an SMS data processing service.\nFIG. 6 illustrates hierarchical categorization and contextualization of SMS messages by an SMS data processing service.\nFIG. 7 is an exemplary method for providing contextualized short message service data from one or more SMS messages to a client.\nFIG. 8 illustrates a computing device for executing one or more aspects of the present disclosure.\nFIG. 9 is a simplified block diagram of a computing device with which aspects of the present disclosure may be practiced.\nFIG. 10 is a block diagram illustrating physical components (e.g., hardware) of a computing device 1000 with which aspects of the present disclosure may be practiced.\nFIG. 11 is a schematic diagram illustrating an example distributed computing environment for providing contextualized short message service data from one or more SMS messages to a client.\n<BR><BR>DETAILED DESCRIPTION\nVarious embodiments will be described in detail with reference to the drawings, wherein like reference numerals represent like parts and assemblies throughout the several views.  Reference to various embodiments does not limit the scope of the\nclaims attached hereto.  Additionally, any examples set forth in this specification are not intended to be limiting and merely set forth some of the many possible embodiments for the appended claims.\nThe various embodiments and examples described above are provided by way of illustration only and should not be construed to limit the claims attached hereto.  Those skilled in the art will readily recognize various modifications and changes\nthat may be made without following the example embodiments and applications illustrated and described herein, and without departing from the true spirit and scope of the claims.\nGenerally, the present disclosure is directed to extracting SMS messages from a client device (e.g., a smart phone, tablet computing device, smart watch, etc.), filtering those messages for spam, malicious content and other non-relevant\ninformation, categorizing the remaining SMS data using world knowledge to supplement the SMS data, and tagging the SMS data with searchable metadata such that SMS data feedback can be sent to a user in a proactive or reactive manner at an appropriate\ntime to help with task completion.\nAccording to examples, a user may receive SMS messages such as text messages, WhatsApp messages, Skype messages, or other short messages from friends, family, colleagues, restaurants, retail establishments, financial institutions, travel\nservices and other entities.  One or more of those messages may contain content and/or provide a link to content that would be helpful to a user in performing a task.  Examples of such content include dates, times and locations (e.g., travel\nreservations), bus, train and plane tickets and purchase confirmations, event tickets and purchase confirmations, receipts (e.g., dining and retail receipts), bill reminders, bill due dates, bill payment confirmations, banking receipts, cell phone data\nplan use updates and phone minute plane utilization updates, and coupons, among others.\nWhile the amount of useful content received via SMS messages is vast, there is also a large amount of content received in those messages that is irrelevant to task completion (e.g., spam messages, malicious messages, various personal\nconversations, etc.).  Examples described herein relate to filtering irrelevant messaging information out, and categorizing and contextualizing relevant SMS content such that it can be provided to a user reactively (e.g., when a user provides the systems\ndescribed herein with a query related to information received via SMS message) and proactively (e.g., when time, location or other indicators received from a user's client device indicate that SMS content is likely to be helpful in completing a task).\nOne or more standardized schema, such as those found on www.schema.org may be used to filter out irrelevant messaging information, as well as spam content and malicious content.  As used herein, schema refers to a set of standardized formats for\nstructured data on the Internet, on web pages, SMS messages and email messages, which can be used with many different encodings, including RDFa, Microdata and JSON-LD.  These vocabularies, examples of which are available at the website schema.org,\ndescribe entities, relationships between entities and actions, and can easily be extended through a well-documented extension model.\nCommonly used schemas provide standardized formats for identifying and parsing the entities and objects described thereby, including: creative works, embedded non-text objects, events, health and medical types, organizations, persons, places,\nlocal businesses, business type, products, offers, reviews, ratings and actions, among others.\nEmbodiments are related to filtering mechanisms that compare the format of data extracted from an SMS messages and determine whether they meet criteria specified in one or more known schema formats.  If a determination is made that extracted\ndata does not meet a known schema format the message containing that data may be filtered out completely or the SMS message may be tagged as \"unknown\" or with another property or object that denotes that the message does not contain a recognized schema\nformat and it may be passed along for further processing for a determination of whether relevant information may nonetheless be included in the message.\nIn addition to using schemas for filtering SMS content, whitelists and blacklists may also be utilized.  For example, a whitelist containing the names of one or more trusted senders or metadata associated with one or more trusted senders may be\nused to determine whether SMS data from an extracted message should be filtered out.  Similarly, a blacklist containing the names of one or more known spam entities or metadata associated with one or more spam entities may be used to determine whether\nSMS data from an extracted message should be filtered out.  Blacklists containing content that has been identified as spam or as likely being malicious (e.g., phishing schemes or other virus-related content) may also be created and dynamically updated\nsuch that they contain an up to date list of irrelevant and potentially harmful content that may be used to determine whether SMS data from an extracted message should be filtered out.  These lists may also be supplemented with key words, phrases,\nmetadata and language patterns that have been identified as likely being associated with spam or malicious content.\nIn addition to filtering out SMS messaging content after it has been extracted by the systems described herein, a user may configure one or more of their computing devices or an account associated one or more of those devices such that SMS\nmessages and certain information included in those messages is filtered prior to being extracted by the systems described herein.  For example, a user may access a settings feature related to a personal user account such as a cloud based account and\nmodify privacy settings that relate to the type of content that can be accessed by one or more cloud-based services.  Examples of information that a user may prevent cloud-based services from accessing may include information such as passwords, pin\nnumbers, salary information, social security numbers, age, gender, date of birth, location, web browsing history, application use history, and other personal information that a user may wish to keep confidential.  In addition to providing a means for\npersonalizing data security, this feature may also be used to minimize the volume of data that is processed and stored by the SMS data processing systems described herein.\nThe systems and methods described herein may utilize a variety of mechanisms for determining what relevant information should be extracted from received SMS data.  One such mechanism involves identification of key value pairs in SMS data. \nExamples of key value pairs that may be identified in SMS data for extraction include: a coupon name, a coupon code, items, services or events for which a coupon is valid, a coupon's expiration time and date, a bill due date, a value of the amount due\nfor a bill, a travel location such as an airport or train station, a ticket number, a plane or train departure time and a plane or train arrival time, among others.  Upon identifying one or more key value pairs in an SMS message that information can be\nextracted and used for categorization as more fully described below.\nUpon extracting relevant information from filtered SMS data the extracted information may be categorized into one or more contextual categories in a tiered contextual content hierarchy.  The tiered contextual content hierarchy may be arranged\nsuch that extracted SMS data and associated supplemental data can be readily provided to a user as feedback upon receiving an indication to provide feedback to a user.  First level categorization in the tiered hierarchy may include general categories\nsuch as: coupons, retail, dining, travel, finance, events, etc.\nUpon categorizing extracted SMS data to one or more first level categories, further categorization into deeper tiers in the assigned categories may then be made.  For example, if extracted SMS data has been categorize at the first level into a\ntravel category, it may then be further categorized into lower tiers such as method of transportation (e.g., bus, train, plane, etc.), business or personal, international or domestic, etc. Upon assigning extracted SMS data to one or more categories that\ndata may be tagged with searchable metadata indicating it is associated with one or more categories in a tiered hierarchical index such that it can be readily surfaced to a user as feedback upon the SMS data processing service/system receiving an\nindication to provide such feedback to the user.\nCategorization as described above may employ various techniques in determining which categories should be associated with extracted SMS data.  One such technique involves the application of natural language processing.  For example, one or more\nnatural language processing engines may receive text from extracted SMS data and evaluate the context of the received text.\nAnother technique that may be used in categorizing extracted SMS data involves machine learning.  Machine learning may identify patterns in SMS messages, such as language patterns, previous categorization of similar messages from which data has\nbeen extracted as well as user feedback in order to accurately identify appropriate context of extracted SMS data and appropriate categorization of such data.\nYet another technique that may be used in categorizing extracted SMS data involves evaluation of schema associated with SMS messages.  For example, a determination may be made that the schema of an SMS message indicates that the message relates\nto a restaurant, a travel reservation, a financial institution, etc., and the message may be categorized accordingly.  Any of one or more of the above discussed categorization techniques may be used in categorization of extracted SMS data into the\ncontextual content hierarchy.\nAn indication to provide feedback may be received in the way of a reactive natural language query from a user, such as a user providing a personal digital assistant (e.g., Cortana, Siri Alexa, etc.) with a query.  For example, a user may provide\na personal digital assistant with a spoken or typed query such as \"when is my flight to Seattle?\" or \"do I have any coupons for Starbucks?\".  Natural language processing may be used to contextualize the query and identify one or more user intents related\nto the query in order to provide relevant feedback to the user.\nAlternatively, an indication to provide feedback may be received in a proactive manner such as receiving longitude and latitude coordinates from a user's mobile computing device and/or a temporal criteria associated with one or more pieces of\nextracted SMS data providing an indication that feedback is likely to be useful in completing a user's task.  For example, rather than processing a query from a personal digital assistant as described above, received longitude and latitude coordinates\nmay be received from a user's mobile computing device indicating that the user is at or in a threshold vicinity of an airport and a determination may be made that extracted SMS data indicates that the user has a flight reservation leaving within three\nhours of the time that those coordinates are received.  As such, information related to the user's flight, such as a flight confirmation number, a ticket, a gate number for their flight, etc., may be proactively provided to the user at that time.\nIn addition to providing feedback to a user consisting of information that has been extracted from SMS messages, world knowledge may be used to augment the SMS data for categorization purposes and for surfacing as feedback at a later time.  For\nexample, in the scenario that extracted SMS data includes information such as the name of a business, a web service may be used to identify what kind of business that information corresponds to (e.g., restaurant, retail service, airport, etc.) as well as\nother information such as hours of operation, products or services offered at that location, reviews, whether the business has coupons or discounted rates available online, etc. The systems described herein may also perform additional processing of\nextracted SMS data prior to, during and/or after categorization of that data such as calculating the average monthly payments for bills, evaluating the weekly, monthly and yearly amounts of money spent on purchases corresponding to various categories\n(e.g., restaurants, retail, etc.), evaluating the cell phone data usage of user and determining whether a recommendation for a service plan change may be helpful, etc.\nUpon categorizing extracted SMS data in the tiered contextual content hierarchy, the categorized data may be stored in a personal data repository which can be queried for providing user feedback at a later time.  The personal data repository may\nbe analyzed periodically to determine whether one or more pieces of categorized data should be purged from it.  For example, stored data may be analyzed to determine whether a temporal criteria associated with stored data has expired, therefore making\nthe storage of that data unnecessary (e.g., coupon expiration, travel date has passed, etc.).\nFIG. 1 is a schematic diagram illustrating an example distributed computing environment 100 for providing contextualized SMS data from one or more SMS messages to a client.  Distributed computing environment 100 includes exemplary SMS message\nsending entities including financial services entity 102 (e.g., credit card companies, debit card companies, banks, etc.), retail services entity 104 (e.g., department stores, local retail stores, online retail services, etc.), travel services entity 106\n(e.g., train companies, airline companies, online travel booking companies, etc.) and dining establishments 108 (e.g., coffee shops, restaurants, bars, etc.).  Example distributed computing environment 100 further includes SMS message reception context\n112, including mobile computing device 114, SMS data processing service 116, including one or more SMS data processing servers 118 and personal data store 120, SMS feedback context 112, including mobile computing device 124 and 126, and network 110.\nAny of exemplary SMS message sending entities 102, 104, 106 and 108 may send one or more SMS messages to mobile computing device 114 via network 110.  For example, financial services entity may send SMS messages to mobile computing device 114\nrelated to credit card due dates, payment confirmations, ATM transactions, etc. Retail services entity 104 may send SMS messages to mobile computing device 114 related to upcoming sales, coupons that may be used at a retail location, receipts for\npurchases, etc. Travel services entity 106 may send one or more SMS messages to mobile computing device 114 related to ticket confirmations, travel alerts (e.g., flight gate changes, departure delays, etc.), available travel upgrades, etc. Dining\nestablishments 108 may send SMS messages to mobile computing device 114 related to coupons available for use at a dining establishment, upcoming events at a dining establishment, purchases made at a dining establishment, etc.\nUpon receiving one or more SMS messages, SMS data may be extracted from mobile computing device 114 by SMS data processing servers 118 via network 110.  SMS data processing servers 118 may extract every received SMS message from client device\n114, specific data pieces from SMS messages received by client device 114 and/or only SMS messages or portions of SMS messages that have not been filtered due to a user's privacy settings.\nUpon receiving extracted SMS data from mobile device 114, SMS data processing servers 118 may apply one or more filters to the extracted SMS data which are used to identify relevant information in the extracted SMS data.  SMS data processing\nservers 118 may filter the extracted SMS data utilizing standardized schema, whitelists and blacklists and key value pairs.  For example, standardized schema, such as schemas described at www.schema.org may be used to filter out irrelevant messaging\ninformation, as well as spam content and malicious content such as content distributed by entities that are involved with distributing viruses to access personal information from users.\nWhitelists and blacklists may be used as filters to identify and classify relevant information in extracted SMS data.\nWhitelists containing the names of one or more trusted senders or metadata associated with one or more trusted senders may be used to determine that SMS data from an extracted message is unlikely to be associated with spam or malicious content\nand should thus be further processed by SMS data processing servers 118.\nBlacklists containing content that has been identified as spam or as likely being malicious (e.g., content that is identified as involving personal information phishing schemes and/or being associated with other virus-related content) may be\nused to determine that SMS data from an extracted message is likely to be associated with spam or malicious content and should thus not be further processed by SMS data processing servers 118 (i.e., the content should be filtered out).\nAdditional filters may also be used in relation to extracted SMS message data by SMS data processing service 116.  For example, SMS data processing servers 118 may filter information because a schema associated with extracted SMS data does not\nmeet a recognized schema format or pattern.  Exemplary filters may also determine that extracted SMS message data should not be further processed by SMS data processing servers 118 because it does not contain a key value pair such as an entity domain, a\ntemporal domain and or a location domain.  According to examples such filters may not be applied until extracted SMS data has been analyzed and supplemented with world knowledge metadata.  For example, a linked website in extracted SMS data may contain\none of an entity, temporal or location domain which are not part of extracted SMS data but that that can be determined by accessing a link provided in extracted SMS data.\nSMS data processing servers 118 may run extracted verbal or text data received from mobile computing device 114 through one or more natural language processing engines such that relevant and irrelevant data can be filtered in or filtered out\nfrom further processing and categorization in a tiered contextual content hierarchy.  SMS data processing servers 118 may additionally or alternatively determine, based on language-based processing of extracted SMS data, that one or more keywords,\nphrases or language patterns are implicated and that the identified SMS data should or should not be further processed and contextually classified by SMS data processing servers 118.\nSMS data processing servers 118 may receive filtered SMS data and categorize it based on schema recognition, natural language processing, pattern recognition, machine learning, user feedback, etc. Categorization may involve natural language\nprocessing of the received SMS data by SMS data processing servers 118 and performing search engine queries related to extracted SMS data by SMS data processing servers 118\nIdentification of relevant content and categorization of that content may be aided by data augmentation and/or metadata attachment related to automated/dynamic search engine queries performed by SMS data processing servers 118 that are based on\nnatural language extraction of SMS messages and extracted SMS data (e.g., categorization of a name provided in an SMS message such as \"Avanti,\" and phrases such as \"20% off your dinner or drinks at Avanti,\" etc.) which can be associated with\nhierarchically relevant classification criteria such as: restaurant and bar, hours of operation, review history, menu, and popular customer times.\nSMS data processing service 116 also includes personal data store 120 which may comprise one or more servers and/or data storage units that store categorized SMS data and associated searchable metadata such that feedback related to a user query\nor task may be reactively and proactively provided.  Personal data store 120 may be analyzed periodically to determine whether storage utilized in an inefficient manner.  For example an analysis may be made as to whether content stored in personal data\nstore 120 is no longer relevant and should thus be purged or otherwise deleted to eliminate hardware space requirements and associated processing costs.  Such analysis may include identifying SMS data and/or associated augmented content in relation to\nextracted SMS data that has been categorized with SMS data such as, for example, a coupon that is no longer valid because of associated temporal data, a ticket for travel (e.g., bus, train, plane) that indicates that the departure time has expired,\npattern recognition that indicates that an account such as a credit card account is no longer in service, etc.\nFIG. 2 illustrates an example scenario 200 in which an SMS message received by mobile computing device 204 is processed, categorized and stored, via network 212 and SMS data processing service 214.  Example scenario 200 shows feedback of\ncategorized SMS data as provided to user 208 via network 212 (via mobile computing device 210 which may or may not be the same device as mobile computing device 204), during reactive SMS feedback context 206 based on an indication to provide feedback\nprovided by mobile computing device 210.\nSpecifically, SMS message reception context 202 shows mobile computing device 204 which has received an SMS message that has a standardize schema for a credit card company indicating a credit card payment has been made for that company in the\namount of $300.00 on Aug.  1, 2016 at 4:43 pm.  Data from that SMS message may be extracted by SMS data processing service 214, filtered to obtain relevant information, augmented with related information, and stored in personal data store 218.\nReactive SMS feedback context shows user 208 providing a computing device such as mobile computing device 210 with the query \"How much was my last CREDIT CARD COMPANY payment?\" via a digital personal assistant.  SMS feedback context 206 shows\nmobile computing device 210, which may or may not be the same computing device as mobile computing device 204, providing feedback related to the users 208's query, providing that the users 208's last CREDIT CARD COMPANY payment was in the amount of\n$300.00 and that the users 208's last payment of $300.00 was posted to their CREDIT CARD COMPANY account on Aug.  1, 2016.\nFIG. 3 illustrates an example scenario 300 in which an SMS message received by mobile computing device 304 is processed, categorized and stored, via network 312 and SMS data processing service 314.  Example scenario 300 shows feedback of\ncategorized SMS data as provided to user 208 via network 312 (via mobile computing device 310), which may or may not be the same device as mobile computing device 304, during proactive SMS feedback context 306 based on an indication to provide feedback,\nthe indication being provided by mobile computing device 310.\nSMS message reception context 302 shows mobile computing device 304 which has received an SMS message that has a standardize schema for flight reservations, the SMS message being received on Oct.  16, 2016 at 1:15 pm.  Data from that SMS message\nmay be extracted by SMS data processing service 314, filtered to obtain relevant information, augmented with related information, and stored in personal data store 318.\nProactive SMS feedback context 306 shows a user 308, that has provided SMS data processing service 314, with an indication, via time and/or GPS coordinates (i.e., longitude and latitude coordinates), that feedback related to user 308's trip is\nlikely to be helpful in travel task completion.  Proactive SMS feedback context 306 shows proactive feedback being provided to user 308 via mobile computing device 310, as provided by SMS data processing service 314 and the augmented SMS content in\npersonal data store 318.\nThe proactive feedback sent by SMS data processing service 314 to user 308's mobile computing device 310 includes a barcode to scan at their flight departure gate to board their flight (extracted from data received from the SMS message received\nin SMS message reception context 302), the current gate at which their flight will depart (information supplemented by augmenting stored SMS content in personal data store 318 with world knowledge), and a reminder that their flight to Seattle departs in\n57 minutes (based on the proactive SMS feedback context indicating that there is a current time of 1:03 pm and that user 308's mobile computing device 310 is at a geographic location relating to the airport of departure).\nFIG. 4 illustrates an example scenario 400 in which an SMS message received by mobile computing device 404 is processed, categorized and stored, via network 412 and SMS data processing service 414.  Example scenario 400 shows feedback of\ncategorized SMS data as provided to user 408 via network 412 (via mobile computing device 410 which may or may not be the same device as mobile computing device 404), during proactive SMS feedback context 406 based on an indication to provide feedback,\nthe indication being provided by mobile computing device 410.\nSpecifically, SMS message reception context 402 shows mobile computing device 404 which has a received an SMS message that has a standardize schema for a retail store and a coupon for that retail store, the SMS message being received on November\n27 at 9:18 pm.  Data from that SMS message may be extracted by SMS data processing service 414, filtered to obtain relevant information, augmented with related information, and stored in personal data store 418.\nProactive SMS feedback context 406 shows a user 408, that has provided SMS data processing service 414 with an indication, via time and/or GPS coordinates (i.e., longitude and latitude coordinates), that feedback related to extracted SMS retail\ncontent (e.g., a coupon valid at a retail entity) is likely to be helpful in retail task completion.  For example, data provided to SMS data processing service 414 via mobile computing device 410 may indicate that user 408 is within a threshold distance\nof the retail store that sent a link to a coupon in an SMS message to user 408 during SMS message reception context 402.\nProactive SMS feedback context 406 shows proactive feedback being provided to user 408 via mobile computing device 410, as provided by SMS data processing service 414 and the augmented SMS content in personal data store 418.\nThe proactive feedback sent by SMS data processing service 414 to user 408's mobile computing device 410 includes a barcode which can scanned to receive the discount provided by a retail coupon, the amount of the discount available to user 408\n(extracted from data received from the SMS message received in SMS message reception context 402) and the fact that the coupon will expire on the date that the feedback is provided during proactive SMS feedback context 406 (based on the proactive SMS\nfeedback context indicating that there is a current date of Nov.  18, 2016 and analysis by SMS data processing servers 416 of content in the SMS message provided during SMS message reception context 402 and/or augmentation of that content via world\nknowledge).\nFIG. 5 illustrates an example scenario 500 in which three SMS messages are received from three separate contexts are contextualized by SMS data processing service 518 and the contextualized SMS data is provided to a user 514 during reactive SMS\nfeedback context 508 based on a reactive indication to provide feedback received by SMS data processing service 518.\nA first context shows a mobile computing device 502 receiving an SMS receipt for user 514's purchases at Joe's Pizza on Aug.  5, 2016.  A second context shows a mobile computing device 504 receiving an SMS receipt for user 514's purchase at\nBurger Works on Aug.  10, 2016.  A third context shows a mobile computing device 506 receiving an SMS receipt for user 514's purchase at Angelos Fine Dining.\nData from the SMS receipts is extracted by SMS data processing service 518 via network 516, analyzed by SMS data processing servers 520 and stored in personal data store 522.  In addition to extracting, categorizing and storing the receipts, SMS\ndata processing service 518 may augment the SMS data by utilizing the categorization of data from SMS messages, including the receipts into a monthly dining expense category and calculating a value for the amount of money that user 514 spent on dining\nduring the month of August 2016.\nSMS feedback context 508 shows user 514 providing, via network 516, SMS data processing service 518 with a query asking \"How much did I spend at restaurants last month?\".  SMS data processing servers 520 may use natural language processing to\ndetermine what information associated with content stored in personal data store 522 should be provided to user 514 as relevant feedback in relation to user 514's query in reactive SMS feedback context 508.\nMobile computing device 512 shows a response, provided via a personal digital assistant, to user 514's query comprised of augmented data received from user 514's SMS dining receipts for the month of August 2016, indicating that user 514 spent\n$55.27 at restaurants in the month of August and that user 514's individual receipts for each of user 514's restaurant transactions during the month of August 2016 may be viewed if desired.\nFIG. 6 illustrates a simplified diagram 600 of a hierarchical categorization and contextualization of SMS messages by an SMS data processing service.  Extracted SMS messages 602 may be processed by one or more SMS data processing servers.  Upon\nextraction of those SMS messages categorization of those messages and the content therein may be made by SMS data processing servers.  For example, natural language processing, key value pair evaluation, machine learning processing, SMS data augmentation\nand schema analysis may be utilized to assign the extracted SMS data to one or more categories in a tiered contextual content hierarchy.\nAt first level domain classification 604 the above techniques may be used to classify extracted SMS data into exemplary categories such as finance, travel, dining, retail, and event.  Upon determining that that a first level domain\nclassification 604 of finance is to be associated with extracted SMS content a further classification may made at second level domain classification 606 as to whether the extracted SMS content should be classified in second tiered categories in finance\nrelated to credit cards, debit cards, home loans and student loans, for example.\nUpon determining that a second level domain classification 606 of credit cards is to be associated with extracted SMS content a further classification may be made at a third level domain classification 608 as to whether the extracted SMS content\nshould be classified in third tiered categories in finance related to AmEx, Capital One, Chase, Bank of America and Wells Fargo, for example.\nUpon determining that a third level domain classification 608 of AmEx is to be associated with extracted SMS content a further classification may be made at a fourth level domain classification 610 as to whether the extracted SMS content should\nbe classified in fourth tiered categories in finance related to recurring payment, retail transaction, due date and reminder, for example.\nUpon determining that fourth level domain classification 610 of recurring payment is to be associated with extracted SMS content a further classification may be made at N levels of domain classifications 612 until a lowest tier of the contextual\ncontent hierarchy of the finance hierarchy is reached.\nUpon categorization of the extracted SMS data into the lowest level of the finance hierarchy, additional augmentation during metadata augmentation 614 of that data may be made.  For example, metadata associated with each tier of the hierarchy\nmay be associated with the extracted SMS data such that queries related to those categories may be surfaced as relevant feedback to a user for task completion.  Augmentation may include analyzing world knowledge associated with the extracted SMS data\nprovided by automated web searches implemented in relation to natural language processing of extracted SMS messages and the categorization of the data in those extracted SMS messages.\nThe augmented, categorized SMS data may be stored in a personal data store at storage 616.  The stored SMS data may be analyzed periodically to determine whether one or more stored pieces of data should be purged from storage 616 because of an\nexpiration date or another indication that the stored SMS data is unlikely to be accessed by a user at a later time.\nFIG. 7 is an exemplary method 700 for providing contextualized short message service data from one or more SMS messages to a client.  At operation 702 SMS data is extracted from a client.  A client may be one or more user computing device such\nas a smart phone, a tablet, a laptop or other computing device by which a user receives SMS messages.  Extracted SMS data may or may not include personal information for a user, depending on user privacy settings.\nUpon extracting SMS data from the client flow continues to operation 704 where SMS data from the extracted SMS data is filtered.  Filtering may include techniques such as analyzing standardized schemas, comparison of the extracted SMS data\nagainst whitelists and blacklists, key value pair analysis, and keyword recognition, among other techniques.\nMoving to operation 706 relevant information from SMS data is extracted.  Second level extraction of SMS data as implemented at operation 706 may involve ontology analysis of extracted data to determine whether natural language analysis and\nlanguage pattern recognition can be used to determine whether extracted SMS data relates to one or more categories in a tiered contextual content hierarchy.\nAt operation 708 evaluation of world knowledge to determine the context of extracted SMS information and an analysis of whether patterns in the extracted information relate to one or more categories in a tiered contextual content hierarchy is\nperformed.  For example, an entity name from extracted SMS data may be provided to web search engine to determine whether it relates to one or more categories in a tiered contextual content hierarchy (e.g., financial entity, travel service entity, retail\nentity, etc.).\nMoving to operation 710 and 712 the analysis performed at operations 706 and 708 is utilized to categorize extracted SMS data into one or more tiered contextual categories and that categorization is associated with the categorized SMS data using\nmetadata.\nAt operation 714 an indication to provide SMS feedback to a user is received.  The indication to provide feedback may be provided reactively or proactively.  In the reactive scenario the indication may be received due to a user providing a query\nrelated to categorized SMS data and in the proactive scenario the indication may be received due to a user's computing device sending locational coordinates and further processing of those coordinates to determine an entity type, in addition to a\ntemporal criteria, to one or more SMS data processing servers.\nAt operation 716 SMS feedback is provided to a client related to the indication to provide feedback at operation 714.  The SMS feedback may be reactive or proactive feedback as described above.  Such feedback may be provided to a user by a\npersonal digital assistant or other means such as a task completion application and may be specifically tailored to a current scenario or to analysis of extracted SMS data such as calculations related to user patterns over time.\nFIG. 8 and FIG. 9 illustrate computing device 800, for example, a mobile telephone, a smart phone, a tablet personal computer, a laptop computer, and the like, with which embodiments of the disclosure may be practiced.  With reference to FIG. 8,\nan exemplary mobile computing device 800 for implementing the embodiments is illustrated.  In a basic configuration, the mobile computing device 800 is a handheld computer having both input elements and output elements.  The mobile computing device 800\ntypically includes a display 805 and one or more input buttons 810 that allow the user to enter information into the computing device 800.  The display 805 of the mobile computing device 800 may also function as an input device (e.g., a touch screen\ndisplay).  If included, an optional side input element 815 allows further user input.  The side input element 815 may be a rotary switch, a button, or any other type of manual input element.\nIn alternative embodiments, mobile computing device 800 may incorporate more or less input elements.  For example, the display 805 may not be a touch screen in some embodiments.  In yet another alternative embodiment, the mobile computing device\n800 is a portable phone system, such as a cellular phone.  The mobile computing device 800 may also include an optional keypad 835.  Optional keypad 835 may be a physical keypad or a \"soft\" keypad generated on the touch screen display.\nIn various embodiments, the output elements include the display 805 for showing a graphical user interface (GUI), a visual indicator 820 (e.g., a light emitting diode) and/or an audio transducer 825 (e.g., a speaker).  In some embodiments, the\nmobile computing device 800 incorporates a vibration transducer for providing the user with tactile feedback.  In yet another embodiments, the mobile computing device 800 incorporates input and/or output ports, such as an audio input (e.g., a microphone\njack), an audio output (e.g., a headphone jack), and a video output (e.g., a HDMI port) for sending signals to or receiving signals from an external device.  In embodiments, the word processing application may be displayed on the display 805.\nFIG. 9 is a block diagram illustrating the architecture of one embodiment of a mobile computing device.  That is, the mobile computing device 900 can incorporate a system (i.e., an architecture) 902 to implement some aspects of the disclosure. \nIn one aspect the system 902 is implemented as a \"smart phone\" capable of running one or more applications (e.g., browser, e-mail, calendaring, contact managers, messaging clients, games, and media clients/players).  In some aspects, the system 902 is\nintegrated as a computing device, such as an integrated personal digital assistant (PDA) and a wireless phone.\nOne or more application programs 966 may be loaded into the memory 962 and run on or in association with the operating system 964.  Examples of the application programs include phone dialer programs, e-mail programs, personal information\nmanagement (PIM) programs, word processing programs, spreadsheet programs, Internet browser programs, messaging programs, diagramming applications, and so forth.  The system 902 also includes a non-volatile storage area 968 within the memory 962.  The\nnon-volatile storage area 968 may be used to store persistent information that should not be lost if the system 902 is powered down.  The application programs 966 may use and store information in the non-volatile storage area 968, such as e-mail or other\nmessages used by an e-mail application, and the like.\nA synchronization application (not shown) also resides on the system 902 and is programmed to interact with a corresponding synchronization application resident on a host computer to keep the information stored in the non-volatile storage area\n968 synchronized with corresponding information stored in the host computer.  As should be appreciated, other applications may be loaded into the memory 962 and run on the mobile computing device 900, including steps and methods for providing\ncontextualized short message service data from one or more SMS messages to a client.\nThe system 902 has a power supply 970, which may be implemented as one or more batteries.  The power supply 970 might further include an external power source, such as an AC adapter or a powered docking cradle that supplements or recharges the\nbatteries.\nThe system 902 may also include a radio 972 that performs the functions of transmitting and receiving radio frequency communications.  The radio 972 facilitates wireless connectivity between the system 902 and the \"outside world,\" via a\ncommunications carrier or service provider.  Transmissions to and from the radio 972 are conducted under control of the operating system 964.  In other words, communications received by the radio 972 may be disseminated to the application programs 966\nvia the operating system 964, and vice versa.  The radio 972 allows the system 902 to communicate with other computing devices such as over a network.  The radio 972 is one example of communication media.  Communication media may typically be embodied by\ncomputer readable instructions, data structures, program modules, or other data in a modulated data signal, such as a carrier wave or other transport mechanism, and includes any information deliver media.  The term \"modulated data signal\" means a signal\nthat has one or more of its characteristics set or changed in such a manner as to encode information in the signal.  By way of example, not limitation, communication media includes wired media such as a wired network or direct-wired connection, and\nwireless media such as acoustic, RF infrared and other wireless media.  The term computer readable media is used herein includes both storage media and communication media.\nThis embodiment of the system 902 provides notifications using the visual indicator 820 that can be used to provide visual notifications and/or an audio interface 974 producing audible notifications via the audio transducer 825.  In the\nillustrated embodiment, the visual indicator 820 is a light emitting diode (LED) and the audio transducer 825 is a speaker.  These devices may be directly coupled to the power supply 970 so that when activated, they remain on for a duration dictated by\nthe notification mechanism even though the processor 960 and other components might shut down for conserving battery power.  The LED may be programmed to remain on indefinitely until the user takes action to indicate the powered-on status of the device. \nThe audio interface 974 is used to provide audible signals to and receive audible signals from the user.  For example, in addition to being coupled to the audio transducer 825, the audio interface 974 may also be coupled to a microphone to receive\naudible input, such as to facilitate a telephone conversation.  In accordance with embodiments of the present invention, the microphone may also serve as an audio sensor to facilitate control of notifications, as will be described below.  The system 902\nmay further include a video interface 976 that enables an operation of an on-board camera 830 to record still images, video stream, and the like.\nA mobile computing device 900 implementing the system 902 may have additional features or functionality.  For example, the mobile computing device 900 may also include additional data storage devices (removable and/or non-removable) such as,\nmagnetic disks, optical disks, or tape.  Such additional storage is illustrated in FIG. 9 by the non-volatile storage area 968.  Computer storage media may include volatile and nonvolatile, removable and non-removable media implemented in any method or\ntechnology for storage of information, such as computer readable instructions, data structures, program modules, or other data.\nData/information generated or captured by the mobile computing device 900 and stored via the system 902 may be stored locally on the mobile computing device 900, as described above, or the data may be stored on any number of storage media that\nmay be accessed by the device via the radio 972 or via a wired connection between the mobile computing device 900 and a separate computing device associated with the mobile computing device 900, for example, a server computer in a distributed computing\nnetwork, such as the Internet.  As should be appreciated such data/information may be accessed via the mobile computing device 900 via the radio 972 or via a distributed computing network.  Similarly, such data/information may be readily transferred\nbetween computing devices for storage and use according to well-known data/information transfer and storage means, including electronic mail and collaborative data/information sharing systems.\nOne of skill in the art will appreciate that the scale of systems such as system 902 may vary and may include more or fewer components than those described in FIG. 9.  In some examples, interfacing between components of the system 902 may occur\nremotely, for example where components of system 902 may be spread across one or more devices of a distributed network.  In examples, one or more data stores/storages or other memory are associated with system 902.  For example, a component of system 902\nmay have one or more data storages/memories/stores associated therewith.  Data associated with a component of system 902 may be stored thereon as well as processing operations/instructions executed by a component of system 902.\nFIG. 10 is a block diagram illustrating physical components (e.g., hardware) of a computing device 1000 with which aspects of the disclosure may be practiced.  The computing device components described below may have computer executable\ninstructions for assisting with SMS data contextualization including receiving SMS data from a client; filtering the SMS data with one or more content filters; extracting relevant information iinto one or more contextual categories in a tiered contextual\ncontent hierarchy, the categorizing comprising analyzing world knowledge related to the extracted information; associating searchable context metadata with the categorized information; receiving an indication to provide feedback related to the received\nSMS data from the client; and providing, based on the associated context metadata, feedback related to the received SMS data to the client, including computer executable instructions for SMS data processing application 1020 that can be executed to employ\nthe methods disclosed herein.\nIn a basic configuration, the computing device 1000 may include at least one processing unit 1002 and a system memory 1004.  Depending on the configuration and type of computing device, the system memory 1004 may comprise, but is not limited to,\nvolatile storage (e.g., random access memory), non-volatile storage (e.g., read-only memory), flash memory, or any combination of such memories.  The system memory 1004 may include an operating system 1005 and one or more program modules 1006 suitable\nfor SMS data processing application 1020, such as one or more components in regards to FIG. 10 and, in particular, SMS data filter 1011, SMS data extraction engine 1013, metadata augmentation module 1015 and context categorization engine 1017.\nThe operating system 1005, for example, may be suitable for controlling the operation of the computing device 1000.  Furthermore, aspects of the disclosure may be practiced in conjunction with a graphics library, other operating systems, or any\nother application program and is not limited to any particular application or system.  This basic configuration is illustrated in FIG. 10 by those components within a dashed line 1008.  The computing device 1000 may have additional features or\nfunctionality.  For example, the computing device 1000 may also include additional data storage devices (removable and/or non-removable) such as, for example, magnetic disks, optical disks, or tape.  Such additional storage is illustrated in FIG. 10 by a\nremovable storage device 1009 and a non-removable storage device 1010.\nAs stated above, a number of program modules and data files may be stored in the system memory 1004.  While executing on the processing unit 1002, the program modules 1006 (e.g., SMS data processing application 1020) may perform processes\nincluding, but not limited to, the aspects, as described herein.  Other program modules that may be used in accordance with aspects of the present disclosure, and in particular may include a spam identification module, a malicious content module, an\nintent analysis engine, a key value pair identification engine, etc.\nFurthermore, aspects of the disclosure may be practiced in an electrical circuit comprising discrete electronic elements, packaged or integrated electronic chips containing logic gates, a circuit utilizing a microprocessor, or on a single chip\ncontaining electronic elements or microprocessors.  For example, aspects of the disclosure may be practiced via a system-on-a-chip (SOC) where each or many of the components illustrated in FIG. 10 may be integrated onto a single integrated circuit.  Such\nan SOC device may include one or more processing units, graphics units, communications units, system virtualization units and various application functionality all of which are integrated (or \"burned\") onto the chip substrate as a single integrated\ncircuit.  When operating via an SOC, the functionality, described herein, with respect to the capability of client to switch protocols may be operated via application-specific logic integrated with other components of the computing device 900 on the\nsingle integrated circuit (chip).  Embodiments of the disclosure may also be practiced using other technologies capable of performing logical operations such as, for example, AND, OR, and NOT, including but not limited to mechanical, optical, fluidic,\nand quantum technologies.  In addition, embodiments of the disclosure may be practiced within a general purpose computer or in any other circuits or systems.\nThe computing device 1000 may also have one or more input device(s) 1012 such as a keyboard, a mouse, a pen, a sound or voice input device, a touch or swipe input device, etc. The output device(s) 1014 such as a display, speakers, a printer,\netc. may also be included.  The aforementioned devices are examples and others may be used.  The computing device 1000 may include one or more communication connections 1016 allowing communications with other computing devices 1050.  Examples of suitable\ncommunication connections 1016 include, but are not limited to, radio frequency (RF) transmitter, receiver, and/or transceiver circuitry; universal serial bus (USB), parallel, and/or serial ports.\nThe term computer readable media as used herein may include computer storage media.  Computer storage media may include volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of\ninformation, such as computer readable instructions, data structures, or program modules.  The system memory 1004, the removable storage device 1009, and the non-removable storage device 1010 are all computer storage media examples (e.g., memory\nstorage).  Computer storage media may include RAM, ROM, electrically erasable read-only memory (EEPROM), flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic\ndisk storage or other magnetic storage devices, or any other article of manufacture which can be used to store information and which can be accessed by the computing device 1000.  Any such computer storage media may be part of the computing device 1000. \nComputer storage media does not include a carrier wave or other propagated or modulated data signal.\nCommunication media may be embodied by computer readable instructions, data structures, program modules, or other data in a modulated data signal, such as a carrier wave or other transport mechanism, and includes any information delivery media. \nThe term \"modulated data signal\" may describe a signal that has one or more characteristics set or changed in such a manner as to encode information in the signal.  By way of example, and not limitation, communication media may include wired media such\nas a wired network or direct-wired connection, and wireless media such as acoustic, radio frequency (RF), infrared, and other wireless media.\nThe different aspects described herein may be employed using software, hardware, or a combination of software and hardware to implement and perform the systems and methods disclosed herein.  Although specific devices have been recited throughout\nthe disclosure as performing specific functions, one of skill in the art will appreciate that these devices are provided for illustrative purposes, and other devices may be employed to perform the functionality disclosed herein without departing from the\nscope of the disclosure.\nAs stated above, a number of program modules and data files may be stored in the system memory 1004.  While executing on processing unit 1002, program modules (e.g., applications, Input/Output (I/O) management, and other utilities) may perform\nprocesses including, but not limited to, one or more of the operational stages of the methods described herein.\nFIG. 11 illustrates one example of the architecture of a system for providing an application assists with SMS data contextualization as described above.  SMS data may be accessed, interacted with, or edited in association with programming\nmodules, SMS data processing application 1006 and storage/memory which may be stored in different communication channels or other storage types.  For example, various documents may be stored using a directory service 1122, a web portal 1124, a mailbox\nservice 1126, an instant messaging store 1128, or a social networking site 1130, application 1006, an 10 manager, other utilities and storage systems may use any of these types of systems or the like for enabling data utilization, as described herein.  A\nserver 1102 may provide a storage system for use by a client operating on a general computing device 1104 and mobile computing devices 1106 through network 1115.  By way of example, network 1115 may comprise the Internet or any other type of local or\nwide area network, and client nodes may be implemented as a computing device embodied in a personal computer, a tablet computing device 1106, and/or by a mobile computing device 1108 (e.g., mobile processing device).  Any of these examples of the\ncomputing devices described herein may obtain content from the store 1116.\nReference has been made throughout this specification to \"one example\" or \"an example,\" meaning that a particular described feature, structure, or characteristic is included in at least one example.  Thus, usage of such phrases may refer to more\nthan just one example.  Furthermore, the described features, structures, or characteristics may be combined in any suitable manner in one or more examples.\nOne skilled in the relevant art may recognize, however, that the examples may be practiced without one or more of the specific details, or with other methods, resources, materials, etc. In other instances, well known structures, resources, or\noperations have not been shown or described in detail merely to observe obscuring aspects of the examples.\nWhile examples and applications have been illustrated and described, it is to be understood that the examples are not limited to the precise configuration and resources described above.  Various modifications, changes, and variations apparent to\nthose skilled in the art may be made in the arrangement, operation, and details of the methods and systems disclosed herein without departing from the scope of the claimed examples.", "application_number": "15220917", "abstract": " In non-limiting examples of the present disclosure, systems, methods and\n     devices for providing contextualized SMS data from one or more SMS\n     messages to a client are presented. SMS data from SMS messages may be\n     filtered and relevant information from the filtered SMS data may be\n     extracted. The extracted information from the SMS data may be categorized\n     into one or more contextual categories in a tiered contextual content\n     hierarchy. The categorization of the SMS data may include analyzing world\n     knowledge related to the extracted information and associating searchable\n     context metadata with the categorized information. An indication to\n     provide feedback related to the SMS data may be received and feedback\n     related to the received SMS data may then be provided to the client.\n", "citations": ["8380697", "8832205", "9176945", "9232373", "9280520", "20060123083", "20090089352", "20090307313", "20100042470", "20110320562", "20120054135", "20130212190", "20150281920", "20160119268", "20160149849"], "related": []}, {"id": "20180040004", "patent_code": "10373179", "patent_name": "Determining streaming content user consumption", "year": "2019", "inventor_and_country_data": " Inventors: \nGreenberger; Jeremy A. (Research Triangle Park, NC), Greenberger; Zachary M. (Research Triangle Partk, NC)  ", "description": "<BR><BR>BACKGROUND\nThis invention relates generally to methods and systems for determining, with a level of confidence, whether a user of a mobile device is attentive of streaming content such as a television broadcast, an online video, a radio broadcast, or an\nonline audio channel.\nIt is valuable information to know if a user is actually attentive to a streaming media, such as a television station, online broadcast, or audio channel, or if the device receiving the streaming media is simply unattended.  Proximity of the\nuser to the receiving device is not entirely indicative of the user paying attention to the media, whereas the media may be being played to a window on a screen which is minimized or covered by another window, and/or the audio may be muted or played at a\nminimal volume.\nThe knowledge of what a user actually views or hears versus what he or she ignores is critical to many producers of media so that they can improve their products and services for increased consumer affinity.\n<BR><BR>SUMMARY OF THE INVENTION\nEmbodiments of the present invention determine a degree of confidence of attentiveness of a consumer of a media object on a media playback device is estimated by monitoring for a user action performed on a mobile computing device while the media\nobjects being played back; responsive to a user action, retrieving descriptors for the media object; computing a relevance of the detected user action and the descriptors using correlation; if sufficiently correlated, determining a degree of confidence\nthat the user is attentive to the media object; and recording the one user action, the media object descriptors, and the degree of confidence into digital results for reporting and usage by other devices and processes to improve viewership, readership,\nand listenership measurements. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe figures presented herein, when considered in light of this description, form a complete disclosure of one or more embodiments of the invention, wherein like reference numbers in the figures represent similar or same elements or steps.\nFIG. 1 illustrates an exemplary constellation of computers, processes, and data communications networks according to at least one embodiment of the present invention.\nFIG. 2 illustrates an exemplary embodiment of a user action correlator according to at least one embodiment of the present invention.\nFIG. 3 illustrates an exemplary embodiment of correlation processes according to at least one embodiment of the present invention.\nFIG. 4 depicts a computer-based embodiment according to the present invention.\n<BR><BR>DETAILED DESCRIPTION OF EMBODIMENT(S) OF THE INVENTION\nProblems Recognized\nThe inventors have recognized that, today, there are very few solutions to gain insight as to what a person is watching on a television at any given moment, or if they are watching at all.  Just because a TV is on in a person's house, does not\nmean they are paying attention to the content or are interested.  Accurately determining a person's viewing habits and preferences will enable development of immersive television viewing experiences.  The embodiments of the present invention disclosed\nherein provide valuable improvements to the accuracy of several technologies used cooperatively during the determination of a consumer's viewing and listening patterns, habits, and preferences, such that the technologies themselves are improved through\nembodiment of the present invention.\nGeneral Principles of Operation\nA First Class of Embodiments.  The present inventors have realized that today's viewers and listeners to streaming media, including content transmitted over wired networks and wireless networks, alike, often interact electronically with other\nusers, with other digital resources, or both, while actively watching or listening to the streaming media.  For example, the inventors recognized that a user may send an instant message, such as a Twitter.TM.  Tweet.TM., post a comment on a social media\naccount, such as a Facebook.TM.  account, or access a website or perform search for information, such as using Google.TM.  or Wikipedia.TM..  According to the inventors' realization, the nature of these user actions may or may not be related to the\ncontent of the current or recently streamed media, but if the nature of a user action can be determined to be related to the recently streamed media, then that is a strong indication that the user is paying attention to the streaming media.  Further, if\nlocation determination such as Global Positioning System (GPS) or proximity beacons (e.g., Bluetooth Low Energy beacons) are incorporated to the system, additional confidence can be obtained about whether or not the user is actually paying attention to\nthe streaming media.\nAs such, the present inventors have devised a solution to meet the unsatisfied needs in the art to determine not only whether or not a user is likely attentive to a streaming media, but also a confidence factor for that determination.  The\nfollowing several embodiments fall within the scope of the present invention, and as such, they are illustrations of various embodiments but do not limit the scope of the invention itself.\nGenerally speaking, embodiments according to the present invention monitor sources of media streams such as a television stations, an on-demand video source servers, a radio stations, and audio streaming service servers.  Methods of monitoring\nthe streaming content at such sources are well known, and are hereby incorporated into this disclosure.  Many embodiments according to the invention will operate solely from metadata from the sources describing the media objects they are streaming or\nbroadcasting (now or recently).\nIt is also possible in some embodiments to further confirm what is being received by users' streaming players, set top TV decoder boxes, etc. For example, some set-top cable television boxes can report or be polled as to their status (on or off)\nand what channel they are tuned to.  Similarly, digital radios, such as some in-dash car radios, are capable of reporting the same status and tuning information, and many applications for mobile devices and computers for receiving personalized play lists\nand on-demand video source can report what is currently playing through the application, and some extensions and helper applications for web browsers can also report such connections and receiving status information.\nHowever, just because a particular receiver reports that it is receiving a particular media object, it is still unknown if the user is actually attentive to the media object being played or rendered at the receiving device (e.g., many cable TV\ncustomers leave their cable decoder box turned on all the time and just turn of the TV).\nEmbodiments of the invention may utilize these and other available methods to gain insight into what media is currently being streamed, broadcasted, received (recently and presently).\nA mobile computing device, such as a smart cell phone or tablet computer, which is associated with a user of the media playback device, is monitored for time-relevant user actions, such as current actions and/or recent actions taken by the user. Actions of interest include, but are not limited to, submitting search criteria to search engine websites and applications (\"apps\"), creating events in a locally-stored or remotely-stored electronic calendar, launching applications on the mobile device\n(e.g., related games, related retail sales applications, related poll taking applications, etc.), authoring new electronic messages or responding to received electronic messages (e.g., Short Message Service \"text messages\", multi-media messages, short\nmessages such as \"Tweets\" .TM.  and Instagrams .TM., and streaming messages such as Periscope .TM.  and web chats), creating or responding to social media posts on social media servers or using social media apps, casting an electronic vote, visiting or\nnavigating to a website or Uniform Resource Locator (URL), accessing another media object (e.g., video, music, app download), and initiating a telephone call.\nThere are a number of known mechanisms which can be employed by embodiments of the present invention to accomplish this user action monitoring, such as application programming interfaces (APIs) provided by some online servers to access user\ninformation in near real-time, similar APIs provided by mobile device operating systems that provide access to user activity, having the user add a monitoring portion of the embodiment to their \"friends\" list, and sharing the user's log in credentials\nwith the monitoring portion of the embodiment.  For example, at least two of the currently most popular social media online services provide application programs with a REpresentational State Transfer (REST) API for obtaining copies in near real-time of\nsocial media posts and/or instant messages authored, responded to, forwarded and read by a user.  Various embodiments of the present invention may utilize any of these methods, as well as others which may become available or are currently available, in\ngreater or less combination to accomplish the task of detecting user actions while the streaming media is being received by the media playback device.\nNext, when the embodiment of the invention detects that a user action has been performed on the mobile device, the embodiment determines from the ascertainable descriptors of the currently-received media object at the media playback device if\nthe action was relevant to a currently or recently transmitted or broadcast media object according to the source, and, optionally, confirmed as a received media object.  This can be done in the first order by comparing meta-data descriptors for the media\nobject to the meta-data and data of the user action, such as comparing the title and the names of the actors, singers or speakers of the media object to the website URL or content of a social media posting or message created by the user (or forwarded, or\nresponded to, etc.).  In a second order, more information can be obtained about the media object by the embodiment of the invention posting a query to an API for another computer service to provide more information, such as searching an internet movie\ndatabase using some known meta-data descriptors about the media object to obtain additional descriptors about the media object (e.g., date of release, producer name, studio name, genre, etc.).  In a third order of relevancy determination, embodiments of\nthe invention may also perform advanced analysis or request advanced analysis to be performed on the media object to generate more descriptors, such as text-to-speech recognition on an audio stream or audio track of a movie, image recognition of faces,\nbuildings, and object in frames of a video stream, etc. Embodiments of the present invention may utilize any or all of these and other available methods and technologies to obtain shallow to deep descriptive data about the media object being received (or\nrecently received) by the media playback device.\nOnce a threshold of relevancy has been determined by one or more orders of comparison of descriptors for the media object with the one or more monitored and detected user actions on the mobile device, it is still not known with certainty if the\nuser is actually paying attention to the received media object, or just potentially coincidentally performing some user actions which have some commonality (keywords, favorite websites, etc.).  It is therefore an improvement beyond the presently\navailable media consumer monitoring technology to determine a degree of confidence whether or not the user is actually paying attention to the current or recent media object on the media playback device.\nAccording to the inventors' solution, if a user action is determined to be at least minimally relevant to the currently-received or recently-received media object, the embodiment of the invention proceeds to perform analytic analysis on various\navailable metrics to produce a degree of confidence if the device's owner is attending to the data stream.  For example, such metrics which can be employed in the analytic analysis can include but are not limited to an amount of time elapsed between when\nthe media object (or a point within the media object) is transmitted to or received by the media playback device; the strength (strong, weak, neutral) of the relevancy of the user action to the descriptors of the media object (e.g., use of specific\nbrand, actor or place names; use of specific movie, show, episode or song titles, etc.); a number of times a relevant user action has been performed; and how many other known associates (e.g., social media friends, the user's messaging contact list\nentries, etc.) are concurrently performing relevant user actions to the media object.\nThis determination of relevancy and degree of confidence is then provided to one or more reports, other client devices, and/or other client application programs, thereby enabling additional improvements and enhancements such as, but not limited\nto creating push notifications relative to the channel a person is watching, suggesting favorite types of shows/channels a person likes to watch, calculating how much television or radio a person consumes in a day, determining when a user typically\nwatches television or consumes media objects, generating statistics on viewer demographics for companies that are tailoring advertisements to specific groups of users; and determining if nearby users are consuming the same media objects at the same time.\nIt should be noted that embodiments of the present invention can operate across many data streams (channels), such as a user switching between channels (serial consumption) or having multiple channels and media objects being played\nsimultaneously (parallel consumption).  The embodiment of the invention may provide some measure of the degree of confidence as to which of the several media objects are getting any or most of the user's attention, presuming that the media objects\ndifferentiate from each other in their descriptors.\nAn additional improvement to other services and technologies which are enabled by embodiments of the present invention is that the confidence degree measurement can assist other client devices and client applications to determine which\nposition(s) within the media objects (movies, songs, lectures, discussions, advertisements, etc.) is/are most effective at generating user response and user action, which can lead to better insight as to which markets will create the best return on\ninvestment for brands, which arguments will generate the most persuasive response to users, and which scenes or chapters will become iconic of a media object.\nA Second Class of Embodiments.  Still further improvements in the technologies of measuring and determining viewership, readership and listener affinity and habits can be obtained in a second class of embodiments which utilize some or all of the\nfunctions and features of the foregoing first class of embodiments, but which also use location based services to further increase the confidence factor determination.\nFor example, if both the media playback device and the mobile device are the same physical device, then the locations of the two are known to be exactly the same.  However, in many realistic scenarios, the media playback device (e.g., a cable\nset-top box or a digital radio) and the mobile device (e.g., a smart phone or a tablet computer) are not physically the same device, and one or both of the two devices may be relocated from time to time.\nSo, if the two devices are determined to be outside of proximity to each other, such as the user is away from home with his or her smart phone but left his or her cable box on and tuned to a particular channel back at their home, then the degree\nof confidence that the user is actually paying attention to the received media content on the cable TV receiver is lessened.  Conversely, if the two devices are determined to be within a certain proximity, physical or geographical, of each other, then\nthe confidence factor can be suitably increased accordingly.\nA number of available technologies can be incorporated into embodiments of the invention in this second class to achieve this proximity determination and additional adjustment to the rendered confidence factor.  For example, a global positioning\nposition (GPS) reading for the user's mobile device can be compared to the geographical address known to be associated with user's cable television account where the cable TV receive set-top box is likely located.  Or, internet protocol (IP) addresses\nand subnet addresses on a home, office, public or hotel network can be compared for both devices to see if they are connected to the same local area network or the same intranet.  Other location based services technologies, such as proximity to a\nBluetooth Low Energy beacon (BLE), can be utilized to see if both devices are within physical proximity of each other.\nUsing these location determination technologies, and optionally others, embodiments of the present invent may determine if the two devices are within a maximum proximity to each other, and if so, increase the degree of confidence value such as\nincrementing it or multiplying it by a location factor.  Conversely, if the locations of the two devices are determined to be outside a minimal physical distance to each other, the embodiment of the invention may decrease the degree of confidence value\nsuch as decrementing it or dividing it by a location factor.\nA Third Class of Embodiments.  In still another group of embodiments according to the present invention, aggregation of actions across multiple users and comparison of those actions in real-time to known broadcast and streaming media metadata\ncan yield viewership certainty on a group basis.  For example, if a group of users who are demographically of an age group \"young teens\" are detected making one or more user actions (texting, posting, commenting, etc.) regarding a particular television\nshow, then an embodiment of the invention may search metadata describing currently (and/or recently) broadcast and streamed media objects to find a correlation between the aggregation of the user actions and the metadata.  In this class of embodiments,\nagain, no actual data connections to the users' streaming players or broadcast receivers are necessary because additional certainty regarding the estimation of viewership is obtained by using a sufficiently large sample of users (e.g., a threshold of\ngroup size taking relevant user actions).\nSpecific Example Embodiments\nReferring now to FIG. 1, a particular example embodiment of coordinated systems, methods and processes is shown.  While the user's media object receiver (102) such as a cable TV box, a digital radio, or a streaming media device or app is\nreceiving audio and/or video media content from a streaming server or broadcast station (201, 202), the user's mobile device (101) such as a smart phone or tablet computer is monitored for a user action by a user action correlator (501).  Monitored and\ndetected user actions can include any combination, but are not limited to, launching of one or more local application programs (101a) on the user's mobile device (101); creating, accepting, editing or modifying one or more events in a local or remotely\nmaintained electronic calendar (301); sending, receiving, replying to, forwarding, or tagging an electronic message (302); submitting search criteria to a search engine (303) or database; creating, responding to, commenting on, reposting, forwarding,\nblocking, reporting, or deleting an entry in a social media account on a social media server (304) or through a local social media app; navigating to a particular website (305) or URL by direct browser address entry or selection of a hyperlink; and/or\ncasting an electronic vote to a voting server (306) or via a local voting app.\nWhen a user action such as these is detected during or soon after reception of a media object, the user action correlator (501) determines the relevancy of the user actions to the received media object as previously described, and if minimally\nrelevant, then additional analytics (502) are performed as previously described using one or more additional metrics (503) to increase or decrease a confidence factor that the user is (or was recently) actually paying attention to the media object being\nplayed or rendered by the user's media playback device (102).\nThe results of the analytics are then posted by a confidence server (503) as one or more reports (505), such as machine-readable and/or human-readable outputs to printers, files, and displays.  And, embodiments of the invention may provide\nelectronic information via messages, database entries, or API responses to one or more client application programs (504) and/or one or more client devices (506) for further enhanced analysis as previously described.\nReferring now to FIG. 2, additional details of an exemplary embodiment of a user action correlator (501) are shown, in which the monitored and detected user actions (301-306) are aggregated (5012), including optionally notices (5011) from the\nuser's mobile devices operating system of launched application programs, and in which descriptors of the currently-streaming or recently-streamed media object(s) (201, 202) are also aggregated.  These subjected to one or more correlation processes (5013)\nto each in a time-based manner, such as the methods previously described, and when a minimal correlation of at least one user action and one media object is detected, corresponding information about the relevant user action(s) and media object(s) is/are\nforwarded to the analytics portion (502) of the embodiment of the invention.\nNow referring to FIG. 3, further details of an exemplary embodiment of the correlation processes (5013) is shown, in which the detected actions from the user action aggregator (5012) and the media object descriptor aggregator (5014) are received\ninto the process (600, 601), and the initial relevance is determined (602) as previously described.  The confidence level regarding the user's attention or inattention to the media object(s) is initialized (603), and it may be increased and decreased\n(604, 605) using one or more of the additional analytical phases or steps as previously described, including, optionally, consideration of user actions by additional known associate users (e.g., friends list entries, etc.) (605) and/or the location\nproximity analysis, also previously described.\nFinally, reports (505) and/or output data structures such as messages, database entries, and push notifications to client devices (506) and/or application programs (504) are generated (606), including one or more pairs of correlated user actions\nand media objects with an associated degree of confidence for each pair.\nIt should be noted that data communications shown in FIGS. 1, 2 and 3 may include local area networks, intranets, an Internet, and one or more wireless networks, as appropriate for each specific embodiment, and that each embodiment uses the\ncoordinated computer functions to improve the performance of the aforementioned viewership analysis systems and solves certain problems directly relevant to computer networks as described herein.\nHardware Product Embodiments.  Embodiments according to the present invention may be realized in part or whole through electronic hardware circuitry, such as microprocessor-based circuitry, microcontroller-based circuitry, programmable logic\narrays, custom circuits, and combinations thereof.  Additionally, in the several figures, references to a \"server\" are to a hardware computer running one or more application programs which, together, perform the functions of a server computer.  The\nseveral servers may be several distinctly separate computers running separate server application programs, or they may be combined into fewer hardware computer(s) which are capable of executing the functionality of multiple server application programs.\nReferring to FIG. 4, a computer-based embodiment according to the present invention is illustrated, in which one or more data and network interfaces (4015) (SCSI, PCI, IDE, Ethernet, WiFi, USB, cellular data, etc.) received detections of user\nactions and media object descriptions into system memory devices (4010, 4011, respectively), optionally enabled by a memory management unit (4013) (DMA controller, memory handler processor, etc.).  One or more processors (4012) with program memory (4014)\nstoring one or more program instructions coupled with optional functions committed to custom integrated circuits and/or programmable logic devices (PALs, PLDs, etc.) (4014) are configured to perform the logical functions described in the foregoing\nparagraphs.  The results of the analytics performed by the processor (4012) and/or the logical functions (4014) are then transmitted to one or more client processes, client devices and/or reports via the data and network interface(s) (4015).  As such, a\nspecialized, processor-based implementation embodiment accomplishes the several objectives of the present invention.\nComputer Program Product Embodiments.  The present invention may be a system, a method, and/or a computer program product at any possible technical detail level of integration.  The computer program product may include a computer readable\nstorage medium (or media) having computer readable program instructions thereon for causing a processor to carry out aspects of the present invention.\nThe computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device.  The computer readable storage medium may be, for example, but is not limited to, an electronic\nstorage device, a magnetic storage device, an optical storage device, an electromagnetic storage device, a semiconductor storage device, or any suitable combination of the foregoing.  A non-exhaustive list of more specific examples of the computer\nreadable storage medium includes the following: a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a static random access memory (SRAM), a\nportable compact disc read-only memory (CD-ROM), a digital versatile disk (DVD), a memory stick, a floppy disk, a mechanically encoded device such as punch-cards or raised structures in a groove having instructions recorded thereon, and any suitable\ncombination of the foregoing.  A computer readable storage medium, as used herein, is not to be construed as being transitory signals per se, such as radio waves or other freely propagating electromagnetic waves, electromagnetic waves propagating through\na waveguide or other transmission media (e.g., light pulses passing through a fiber-optic cable), or electrical signals transmitted through a wire.\nComputer readable program instructions described herein can be downloaded to respective computing/processing devices from a computer readable storage medium or to an external computer or external storage device via a network, for example, the\nInternet, a local area network, a wide area network and/or a wireless network.  The network may comprise copper transmission cables, optical transmission fibers, wireless transmission, routers, firewalls, switches, gateway computers and/or edge servers. \nA network adapter card or network interface in each computing/processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium\nwithin the respective computing/processing device.\nComputer readable program instructions for carrying out operations of the present invention may be assembler instructions, instruction-set-architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, firmware\ninstructions, state-setting data, configuration data for integrated circuitry, or either source code or object code written in any combination of one or more programming languages, including an object oriented programming language such as Smalltalk, C++,\nor the like, and procedural programming languages, such as the \"C\" programming language or similar programming languages.  The computer readable program instructions may execute entirely on the user's computer, partly on the user's computer, as a\nstand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server.  In the latter scenario, the remote computer may be connected to the user's computer through any type of network,\nincluding a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).  In some embodiments, electronic circuitry including, for\nexample, programmable logic circuitry, field-programmable gate arrays (FPGA), or programmable logic arrays (PLA) may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to\npersonalize the electronic circuitry, in order to perform aspects of the present invention.\nAspects of the present invention are described herein with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems), and computer program products according to embodiments of the invention.  It will be\nunderstood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer readable program instructions.\nThese computer readable program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute\nvia the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.  These computer readable program instructions may also be\nstored in a computer readable storage medium that can direct a computer, a programmable data processing apparatus, and/or other devices to function in a particular manner, such that the computer readable storage medium having instructions stored therein\ncomprises an article of manufacture including instructions which implement aspects of the function/act specified in the flowchart and/or block diagram block or blocks.\nThe computer readable program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other device to cause a series of operational steps to be performed on the computer, other programmable apparatus or\nother device to produce a computer implemented process, such that the instructions which execute on the computer, other programmable apparatus, or other device implement the functions/acts specified in the flowchart and/or block diagram block or blocks.\nThe flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present invention. \nIn this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of instructions, which comprises one or more executable instructions for implementing the specified logical function(s).  In some alternative\nimplementations, the functions noted in the blocks may occur out of the order noted in the Figures.  For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse\norder, depending upon the functionality involved.  It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special\npurpose hardware-based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.\nConclusion.  The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention.  As used herein, the singular forms \"a\", \"an\" and \"the\" are intended to include the plural\nforms as well, unless the context clearly indicates otherwise.  It will be further understood that the terms \"comprises\" and/or \"comprising,\" when used in this specification, specify the presence of stated features, steps, operations, elements, and/or\ncomponents, but do not preclude the presence or addition of one or more other features, steps, operations, elements, components, and/or groups thereof, unless specifically stated otherwise.\nThe corresponding structures, materials, acts, and equivalents of all means or step plus function elements in the claims below are intended to include any structure, material, or act for performing the function in combination with other claimed\nelements as specifically claimed.  The description of the present invention has been presented for purposes of illustration and description, but is not intended to be exhaustive or limited to the invention in the form disclosed.  Many modifications and\nvariations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the invention.  The embodiment was chosen and described in order to best explain the principles of the invention and the practical\napplication, and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated.\nIt should also be recognized by those skilled in the art that certain embodiments utilizing a microprocessor executing a logical process may also be realized through customized electronic circuitry performing the same logical process or\nprocesses.\nIt will be readily recognized by those skilled in the art that the foregoing example embodiments do not define the extent or scope of the present invention, but instead are provided as illustrations of how to make and use at least one embodiment\nof the invention.  The following claims define the extent and scope of at least one invention disclosed herein.", "application_number": "15231410", "abstract": " Confidence of attentiveness of a consumer of a media object on a media\n     playback device is estimated by monitoring for a user action performed on\n     a mobile computing device while the media objects are being played back;\n     responsive to a user action, retrieving descriptors for the media object;\n     computing a relevance of the detected user action and the descriptors\n     using correlation; if sufficiently correlated, determining a degree of\n     confidence that the user is attentive to the media object; and recording\n     the one user action, the media object descriptors, and the degree of\n     confidence into digital results for reporting and usage by other devices\n     and processes to improve viewership, readership, and listenership\n     measurements.\n", "citations": ["7813822", "20110161513", "20110202947", "20130110565", "20130113993", "20130308818", "20140181851", "20150245090", "20150286464", "20150382075"], "related": []}, {"id": "20180068576", "patent_code": "10373511", "patent_name": "Automatic learning curriculum generation", "year": "2019", "inventor_and_country_data": " Inventors: \nCalmon; Flavio D. (White Plains, NY), Kokku; Ravindranath (Yorktown Heights, NY), Vempaty; Aditya (Elmsford, NY)  ", "description": "<BR><BR>BACKGROUND\n<BR><BR>Technical Field\nThe present disclosure generally relates to automatic generation of curricula for a class of students, and more particularly, to automatic generation of blended-learning course contents adapted to students with different interests and\nbackgrounds.\n<BR><BR>Background\nA traditional setting of education, typically involving an instructor teaching a class of students in a classroom on a particular subject, generally allows interactions between the instructor and the students.  Such interactions are salient in\nseveral ways.  The instructor can monitor the responses of students in the classroom and adjust the level of lecturing accordingly.  The students can communicate to the instructor to reflect their level of understanding or seek help for various aspects\nof learning.  The efficiency and quality of learning is the ultimate goal of the traditional setting of education.  However, challenges of selecting an appropriate course content or defining proper course objectives arise for a class of students with\ndiverse backgrounds and different interests.  It is also rather difficult for a human teacher to search, select, and organize the best content from massive educational materials available on-line or from other digital resources without a computer-aided\nmethod.  The traditional setting of education hence cannot span wide enough to cover the needs of all the students owing to such a fixed style of teaching and lack of variability of the course content.  Students on either end of a performance spectrum,\nmeasured by testing students on the course content, are often ignored or forced to follow avenues inadequate to their levels in the traditional setting.  Corrections have been attempted to solve this problem with a variety of one-on-one teaching methods\nbut are either not efficient or limited to a very small scale.  The inception of web-based learning in recent years has catalyzed some methodologies towards improving the learning experience of individual student with a free-form style and adaptive\ncontent, but this web-based format is not suitable for subjects, which require collaborative learning and communicative teacher-student relationships, and in settings that can require in-person interaction between students or between the student and the\nteacher.\n<BR><BR>SUMMARY\nIn the present disclosure, an approach of blended learning, which integrates the teaching of traditional classroom setting and self-learning of web-based format, is provided.  This approach takes into consideration the diverse backgrounds and\ndifferent interests of a class of students and utilizes publically and potentially privately available educational materials to automatically generate curricula for the students in a class.  The curricula provide two different regimes of learning; one is\na common course content shared by all the students in the class, and the other is a private course content that better attends to the background and interest of each student.  The curricula are further optimized with respect to a set of constraints to\nfit the course content length into a predetermined number of sessions.\nAccording to one embodiment of the present disclosure, a method of automatic learning curriculum generation is provided for a class of students.  The input data from users including a class of students, one or more instructors, one or more\neducational institutions, is received by one or more processors.  The input data describes a variety of aspects concerning a course and attributes of students attending the course.  The users also provide a set of instructions for the processing of the\nrespective input data.  A course boundary is constructed by one or more processors based on the input data.  The course boundary comprises a plurality of topics for the class of students to define the scope of the course.  A plurality of blended-learning\ncontents pertaining to the course boundary are determined by the one or more processors to provide a set of common topics for the class as a whole and a plurality of sets of private topics for each of the sub-groups respectively, wherein each of the\nsub-groups comprises at least one of the students in the class.  The curricula based on the blended-learning contents for each of the sub-groups of the class are further optimized by the one or more processors with respect to a set of constraints.\nAccording to another embodiment of the present disclosure, an apparatus is provided.  The apparatus comprises one or more processers configured to receive the input data and a plurality of instructions and automatically generate curricula, the\none or more processors comprising: (1) an assessor unit configured to integrate input data from a group of users and store the input data, wherein the group of users comprise one or more instructors, a class of students attending a course, and one or\nmore educational institutions; (2) a searcher unit configured to search for a collection of course materials pertaining to the course from a plurality of sources, wherein the sources comprise a collection of substances from the one or more instructors, a\nplurality of institutional repositories, and a collection of educational databases publicly available; (3) an analyzer unit configured to determine a plurality of blended-learning contents for a plurality of sub-groups of the class respectively, wherein\nthe plurality of blended-learning contents comprise a plurality of common topics for the class as a whole and a plurality of private topics for each of the sub-groups in the class from output of the searcher, and wherein each of the sub-groups comprises\nat least one of the students in the class; and (4) an optimizer unit configured to output a plurality of curricula based on the blended-learning contents for each of the sub-groups of the class, wherein the curricula are optimized with respect to a set\nof constraints. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe drawings are of illustrative embodiments.  They do not illustrate all embodiments.  Other embodiments may be used in addition or instead.  Details that may be apparent or unnecessary may be omitted to save space or for more effective\nillustration.  Some embodiments may be practiced with additional components or steps and/or without all of the components or steps that are illustrated.  When the same numeral appears in different drawings, it refers to the same or like components or\nsteps.\nFIG. 1 is a block diagram of an example scheme in which various embodiments in accordance with the present disclosure may be implemented.\nFIG. 2 is a block diagram of an example process in accordance with an embodiment of the present disclosure.\nFIG. 3 is a block diagram of an example apparatus that may provide curricula for a class of students, consistent with an embodiment of the present disclosure.\nFIG. 4 is a block diagram of an example scenario of generation of curricula for a class of students utilizing the example apparatus, consistent with an embodiment of the present disclosure.\nFIG. 5 is a diagram of an example scenario of constructing a course boundary in accordance with an embodiment of the present disclosure.\nFIG. 6 is a diagram of an example scenario of determining a plurality of blended-learning contents based on the course boundary in accordance with an embodiment of the present disclosure.\nFIG. 7 illustrates an example process that may generate optimized curricula for a class of students based on the input data received from students and instructors in accordance with an embodiment of the present disclosure.\n<BR><BR>DETAILED DESCRIPTION\nIn the following detailed description, numerous specific details are set forth by way of examples in order to provide a thorough understanding of the relevant teachings.  However, it should be apparent that the present teachings may be practiced\nwithout such details.  In other instances, well-known methods, procedures, components, and/or circuitry have been described at a relatively high-level, without detail, in order to avoid unnecessarily obscuring aspects of the present teachings.\nThe present disclosure generally relates to a method and apparatus of automatic generation of curricula for a class of students, and more particularly, to the creation of adaptive course contents suitable for students with different interests\nand backgrounds.  A method according to various embodiments of the present disclosure automatically integrates course materials from public educational domain regarding attributes and requirements from students and instructor(s) to generate curricula\nthat accommodate a wide range of diversity of students with adaptive blended-learning contents.  The proposed approach of the present disclosure is based on blended learning with certain contents taught in groups and certain contents taught individually. What is taught in groups may be adapted based on a variety of factors such as, for example and not limited to, the interests and profiles of a set of students registered for a given course, current job market requirements, and contents available in\nmultiple other forms for consumption.  The proposed approach may automatically integrate data from multiple sources, including information about the students in the class, in creating an improved learning curriculum for a given subject at hand.  What can\nbe achieved by the proposed approach of the present disclosure could not be achieved before since large-scale, organized and automated access to education and job market information (outside of information available in a classroom), as well as teaching\nmaterials, was not available.\n<BR><BR>Example Schemes\nFIG. 1 depicts an example scheme 100 in which various embodiments in accordance with the present disclosure may be implemented.  The example scheme 100 may include one or more operations, actions, or functions as represented by one or more of\nblocks 101-106, 111, 112 and 121-123.  Although illustrated as discrete blocks, various blocks of scheme 100 may be divided into additional blocks, combined into fewer blocks, or eliminated, depending on the desired implementation.\nBlocks 101-106 may be referred to as input data blocks, with each block representing a type of input data.  Input data blocks in the example shown in FIG. 1 include the following: core topics block 101, prerequisites block 102, student profiles\nblock 103, preliminary reference materials block 104, objectives of course block 105, and constraints information block 106.  Core topics block 101 may represent the fundamental substances of a course.  Prerequisites block 102 may represent subjects\nrequired to be known or otherwise learned by the students before attending the course.  Student profiles block 103 may represent information related to the backgrounds, grades, and interests of students.  Preliminary reference materials block 104 may\nrepresent previous lecture notes, textbooks, and articles related to a course.  Objectives of course block 105 may represent career goals of students.  Constraints information block 106 may represent availability of faculties, teaching assistants,\nclassrooms, number of sessions, or a combination thereof, for a course.  Blocks 121-123 may be processing blocks, with each block processing input data and performing particular tasks.  Topic dependency and organization block 121 may organize dependency\nof preliminary course topics.  Course direction and outcome alignment block 122 may search additional materials to decide course topics and knowledge covered in a course according to the objectives of course block 105.  Reference mapping and scheduling\nblock 123 may map different course materials to the respective student(s) and optimize the class schedule with respect to constraint information.  Blocks 111 and 112 may be database blocks, with block 111 representing publicly available materials on the\ninternet, and block 112 representing social media or employment market information.  Curricula Block 110 may represent a number of curricula for a class of students.  Block 150 is a legend block.\nThe example scheme 100 may utilize a number of processing blocks 121-123 to retrieve course materials from database blocks 111 and 112 by analyzing input data blocks 101-105.  Each of the processing blocks 121-123 may perform particular tasks\nindividually but may be also be able to exchange algorithms and information with one another to collaboratively accomplish a targeted task.  The legend block 150 depicts various symbols representing different behaviors of communications between blocks. \nFor instance, the topic dependency and organization block 121 may organize a number of preliminary course topics and dependency between preliminary course topics based on both the input data blocks 101-104 and search results from database block 111.  The\ncourse direction and outcome alignment block 122 may align course materials retrieved from database block 111 and database block 112 to the objectives from the objectives of course block 105.  The topic dependency and organization block 121 and course\ndirection and outcome alignment block 122 may exchange algorithms and information for the determination of a course boundary 131 that refines the preliminary course topics and related course materials, taking information from input data 101-105 into\naccount.  The reference mapping and scheduling block 123 may interact with course direction and outcome alignment block 122 to determine course contents and perform optimization on the course contents with respect to the constraint information block 106. The reference mapping and scheduling block 123 may output curricula 110 for a class of students, with each curriculum of curricula 110 including a blended-learning content adapted to the objectives of the students and instructor(s).  In some embodiments,\nthe generated curricula 110 may be used as input data for topic dependency and organization block 121 to generate a series of curricula for a sequence of courses.\nReferring to FIG. 1, various forms of input data blocks 101-104 from students and instructor(s) may be utilized in the assessment of various aspects of students to determine preliminary course topics pertaining to the objectives of the course. \nThe preliminary course topics may include core topics necessary for the course and non-core topics of relevant interests or prerequisites.  In one embodiment, the core topics 101 may be indicated by the instructor(s) as fundamental substances of the\ncourse, and the prerequisites 102 may also be indicated by the instructor(s) as requirements for the course.  For illustrative purposes and without limitation, the student profiles 103 may include a variety of attributes associated with the interest of\nstudents, prior courses taken by the students, test performance of the students, and responses to questionnaires of course-related subjects.  In some embodiments, the preliminary reference materials 104 may be generated as input for the topic dependency\nand organization block 121 using Natural Language Processing of previous textbooks or lecture notes, provided by the instructor(s) or automatically selected from database block 111.\nIn some embodiments, the input data 101-103 may be analyzed and integrated by the topic dependency and organization block 121 to determine a number of preliminary course topics and dependency between preliminary course topics.  Test performance\nof the students may be ranked to determine a level of difficulty of the course materials.  The number of students interested in a particular topic may be calculated to provide a course direction.  Course materials relevant to the preliminary course\ntopics may be gathered in topic dependency and organization block 121 by performing searches from preliminary reference materials block 104 and database block 111.  The topic dependency and organization block 121 may also perform clustering of students\ninto sub-groups according to a particular attribute of student profiles 103.  In one embodiment, students with a similarity of interest and having taken the same prerequisites may be grouped together.  Such a clustering instruction may be provided by the\ninstructor(s).\nThe objectives of course block 105 may include the learning objectives of the students or outcomes expected by the instructor(s).  Students may wish to learn more relevant materials associated with the core topics for preparing an advanced level\nof graduate study or simply more practical tutorials of the non-core topics that may increase opportunities in the employment market.  The instructor(s) may expect students to pass certain tests or obtain certificates as the outcome of the course.  The\ncourse direction and outcome alignment block 122 may analyze the input data 105 and retrieve pertinent information from database block 111 and database block 112 that is aligned to the objectives of course block 105.\nThe database block 111 may refer to public educational materials available on the internet.  There are a number of public databases providing educational contents such as Wikipedia.TM.  and Massive Open Online Courses (MOOCs).  Other sources may\ninclude repositories of educational institutions, journals of academic societies, conference proceedings in certain disciplines, and white papers from corporate publication.  The public educational materials mentioned above is for illustration purposes,\nand is not intended to be exhaustive or limited to the present disclosure.  The database block 112 may generally relate to emerging views or trending topics of the employment market, status of economy, news, or social media that may have indicated new\nskills required for a given profession or a promising new field of research.\nThe constraints information block 106 may include information regarding time constraints and teaching resource constraints.  A time constraint may be a limit of the length of the course, which may be defined as a product of a predetermined\nnumber of sessions and a predetermined number of hours per session.  The length of the course may be further divided into a common course length and a private course length with a predetermined ratio.  In some embodiments, the predetermined numbers for\nsessions, hours per session, and ratio of the common course length to the private course length may be configurable.  The common course length may refer to the number of scheduled sessions available for the class as a whole, while the private course\nlength is the number of the sessions available for self-learning.  The teaching resource constraints may involve the number of faculties, the number of teaching assistants, and classroom facilities that are available for the course, or internet access\nthat is available for the students.\nThe search results of the course materials within the course boundary 131 may be further filtered with respect to the constraints information 106.  The reference mapping and scheduling block 123 may perform filtering and optimization among the\nsearch results to select appropriate contents that fit into the time and teaching resource constraints.  In some embodiments, the reference mapping and scheduling block 123 may also collaborate with course direction and outcome alignment block 122 for\nthe selection of the non-core topics and mapping of non-core topics to the sub-groups of students, subject to the relevancy between the non-core topics and the objectives of the sub-groups of students.  The output of the reference mapping and scheduling\nblock 123 may be curricula 110, which may include the common topics for the class as a whole and sets of private topics for each of the sub-groups of the class.\n<BR><BR>Example Process\nFIG. 2 depicts an example process 200 in accordance with an embodiment of the present disclosure.  Process 200 may include one or more operations, actions, or functions as represented by one or more of blocks 201, 202, 203, 204, 205, 206, 207\nand 208.  Although illustrated as discrete blocks, various blocks of process 200 may be divided into additional blocks, combined into fewer blocks, or eliminated, depending on the desired implementation.\nProcess 200 may start at 201, where users may enter data as input for the following steps.  The users herein may refer to students attending a course, instructor(s) (may include the teaching assistant) teaching the course, and school(s)\nproviding information of students.  At 201, data may be entered manually or retrieved automatically with a set of instructions specified by the students or instructor(s).  The input data may include a variety of attributes of students such as the\ninterest of students, prior courses taken by the students, test performance of the students, learning objectives of the students, and responses to questionnaires of course-related subjects.  In one embodiment, the instructor(s) may set the requirements\nof prerequisites for the course, and provide core topics and possible topics of interest for the students.  Course materials like previous lecture notes, video sessions, reference books, or constraints information of the course may serve as input data at\n201.  The input data 201 may include a set of instructions for how to process the input data.\nAt 202, the input data 201 may be analyzed to establish a concept graph 203.  The concept graph 203, which defines directions and dependency of course topics, may include a number of preliminary course topics.  In one embodiment, the preliminary\ncourse topics in the concept graph 203 may be provided by the instructor(s), and may include core topics necessary for the course and non-core topics such as prerequisites or relevant topics of interest.  The input data 201 may be further assessed to\ndetermine a number of sub-groups of the students at 204.  The clustering of students may be determined by a similarity of student background, a similarity of test performance, a similarity of interest, or a similarity in career goals from instructions of\nthe input data 201, or a mixture of these criteria.  In another embodiment, students may also be clustered, for example, by the .kappa.-means algorithm on a numeric vector where each entry represents an observation.  The entry may be 1 if the student has\nthat observation, and 0 otherwise.  Other clustering algorithms based on a similarity of a particular attribute of the students may also be applied.  Alternatively, the sub-groups may be pre-defined by the instructor(s) or by the students themselves. \nEach of the sub-groups may include at least one or more students of the class.\nOnce the concept graph 203 is established and the students are divided into sub-groups based on a similarity of an attribute from the input data of students, process 200 may proceed to 205 to construct a course boundary.  At 205, process 200 may\nexamine the preliminary course topics against the input data 201 about whether or not to exclude one or more of the non-core topics or to include additional relevant topics.  For instance, in some embodiments, a prerequisite may be excluded if 80% of\nstudents in the class have taken similar subject of that prerequisite.  As another example, a relevant topic may be added to the course boundary 205 if 33% of students in the class are interested.  The threshold used for excluding or including a topic\nmay be predetermined in the instructions received from the instructor(s).  The course boundary 205 may be, therefore, a confined scope of the course materials for the students after taking into consideration the input data 201.\nIn some embodiments, the course materials pertaining to the course boundary may be collected from multiple sources at 206 with a default web-search engine or search API (Application Programming Interface).  Various forms of course materials,\nsuch as lecture notes, slides, theses, white papers, journal articles, or video sessions, may exist in Wikipedia.TM., MOOCs, and other educational databases.  A set of keywords may be generated automatically from the instructions of input data, such as\ndescriptions and objectives of the course, to serve as search criteria.  The search results may be ranked according to the meta-data such as on-line reviews, number of views per video, number of citations, number of references, or rating of the\npublisher.  The search results may be tagged with traits indicating the ranking, the length, and the type of the course materials.  The ranking method is described above.  The length of the course materials may be the number of slides, the number of\nweb-pages or pages, or the duration of a video.  In one embodiment, the types of the course materials may have two varieties, with one being the common topics for the class as a whole and the other being the private topics for the sub-groups.  In another\nembodiment, the types may be research-oriented (for students preparing for advanced levels) or skill-oriented (for students preparing for career opportunities).  In another embodiment, the types may be static or dynamic, indicating that the course\nmaterials are either documents or slides (static), or videos (dynamic).  For instance, the course materials related to the core topics and some of the non-core topics with shared interest of students may be categorized as the first type as the common\ntopics for the whole class.  The rest of the course materials pertaining to the non-core topics may be the second type as private topics for self-learning by the students.\nAt 207, a predetermined number of the course materials for the common topics and the private topics, respectively, may be determined or otherwise chosen from the search results (output of 206) after further relevancy analysis of the search\nresults with respect to the course boundary.  The predetermined number of the course materials chosen for either the common topics or the private topics may be configured in the instructions from the instructor(s).  Mapping of sets of private topics to\neach of sub-groups according to the attributes of the input data 201 may also be performed at 208 in optimizing the curricula.  For example, students preparing for an advanced graduate-level course may be associated with course materials with a\nresearch-oriented tag, whereas students looking for employment opportunities may be associated with course materials with a skill-oriented tag.  Process 200 may then proceed to optimize the curricula with respect to the time constraint of input data 201\nto fit the chosen course materials to the common course length and the private course length respectively.  If more than one of the course materials may satisfy the constraints, the lower ranked course materials may be dropped.  Ultimately, if no clear\ncourse material is better, any of the course materials may be dropped at random.  In one embodiment, a given curriculum may be generated by combining the common course schedule, the private course schedule, a predetermined number of assignments, a\npredetermined number of projects, and a predetermined number of exams.\n<BR><BR>Example Apparatus and Graphs\nFIG. 3 depicts an example apparatus 300 that may provide curricula for a class of students, consistent with an embodiment of the present disclosure.  Apparatus 300 may be implemented in the form of one or more computing devices including, for\nexample and not limited to, one or more servers, one or more desktop computers, one or more laptop computers, one or more tablet computers, one or more smartphones, one or more portable devices, one or more wearable devices, or any combination thereof. \nApparatus 300 may include one or more processors 310.  The processor(s) 310 may further include an assessor unit 320, a searcher unit 330, an analyzer unit 340, and an optimizer unit 350.  In some embodiments, the assessor unit 320, the searcher unit\n330, the analyzer unit 340, and the optimizer unit 350, may be implemented as an individual processor by itself.\nProcessor(s) 310 may receive input data and instructions from users, where the users refer to students attending a course and instructor(s) teaching the course.  The processor(s) 310 may also retrieve a set of information about students from\nschool(s) where the students attend, a set of information about the classroom facilities, or a set of employment information related to the skills learned in the course upon receiving the instructions for extraction.  In some embodiments, the\ninstructor(s) may draft up questionnaires concerning particular aspects of a subject and enter questionnaires to the processor(s) 310, the processor(s) 310 may deliver questionnaires to the email boxes of students and collect the responses\nelectronically.  In still another embodiment, the processor(s) 310 may take previously generated curricula of a prerequisite as input to automatically generate course content for the next course in a sequence of related courses.\nThe assessor unit 320 may communicate to processor(s) 310 to process the input data according to the instructions from users.  The assessor unit may analyze the input data to classify, rank and calculate various attributes of the students.  For\ninstance, the assessor unit 320 may rank the test performance of the students to determine a level of difficulty of the course materials.  The assessor unit 320 may also classify the career goals of the students as preparing for advanced level or looking\nfor employment opportunities.\nThe assessor unit 320 may also perform assembly of a concept graph to organize directions and dependency of preliminary course topics.  In some embodiments, the preliminary course topics may be provided by the instructor(s) entering as an input. But the assessor unit 320 may also recommend additional topics in the concept graph based on analysis of the attributes of the students from the input data.  For instance, a relevant topic may be added to the concept graph if 33% of students in the class\nare interested.  The assessor unit 320 may also decide to remove one or more preliminary topics from the concept graph, e.g., a prerequisite may be excluded if 80% of students in the class have taken similar subject of that prerequisite.  A course\nboundary may be established after refining the concept graph at assessor unit 320.\nIn one embodiment, the students may be assessed and clustered by the assessor unit 320 based on similarities of attributes of the students from the input data.  For instance, the students may be grouped into sub-groups according to a similarity\nof background or test performance.  In another embodiment, students may be divided into sub-groups based on the entries to questionnaires by the .kappa.-means algorithm on a numeric vector where each entry may represent an observation, the entry may be 1\nif the student has that observation, and 0 otherwise.\nThe searcher unit 330 may communicate with the assessor unit 320 and the analyzer unit 340 to receive instructions for performing searches.  In some embodiments, the searches may be performed with a set of keywords generated automatically from\nthe instructions of the input data, such as descriptions and objectives of the course, to serve as search criteria.\nThe search results may be ranked and categorized by the analyzer unit 340.  In some embodiments, the ranking may be determined by the meta-data such as on-line reviews, number of views per video, number of citations, number of references, or\nrating of the publisher.  The search results may be tagged by the analyzer unit 340 with traits indicating the ranking, the length, and the type of the course materials.  In some embodiments, the analyzer unit 340 may further choose a predetermined\nnumber of the course materials from the search results for optimization with respect to the constraints.\nThe optimizer 350 may determine mapping of sets of the private course topics to each of the sub-groups according to attributes of the students from the instructions of the input data.  For instance, course materials for a prerequisite may be\nincluded in the curriculum of a sub-group of students who indicate lacking the background of the prerequisite in the respective attribute.  In some embodiments, the optimization may be an iterative process by successively fitting the course materials to\na predetermined common course length and a predetermined private course length, respectively.  The optimizer 350 may decide to drop course materials randomly if more than one course materials satisfy the constraints and none is clearly better than the\nother.  In some embodiments, the instructor(s) may allocate a predetermined portion of the course length for a specific type, e.g., the dynamic course materials (videos), to adjust optimization results.  The optimizer 350 may deliver optimized curricula\nto the students electronically.\nFIG. 4 depicts an example scenario 400 of generation of curricula for a class of students utilizing the example apparatus, consistent with an embodiment of the present disclosure.  The one or more processors 410 may be an example implementation\nof the one or more processors 310.  Therefore, detailed description above with respect to the one or more processors 310 applies to the one or more processors 410.  Accordingly, in the interest of brevity, a detailed description of the one or more\nprocessors 410 is not provided to avoid redundancy.\nFIG. 4 depicts an example graph for automatic generation of a plurality of curricula for a class of students 401 utilizing processor(s) 410 to integrate data from multiple sources, such as web-based databases, institutional repositories, and\ninput data of users.  The users may include a class of students 401 (Student 1, Student 2, .  . . , Student K, with K a positive integer greater than 1) attending a course, an instructor 402 (Teacher 1) teaching the course, and school 403 (School 1)\nwhere the students attend the course.  The processor(s) may receive the input data of users.  The processor(s) 410 may further analyze the input data of users and may deliver questionnaires to the users based on the analysis results.  In one embodiment,\nthe processor(s) 410 may perform clustering of the students into sub-groups (N groups with N a positive integer) based on similarities of attributes from the input data of students 401.  A class of students 401 may be divided into N sub-groups with each\nsub-group having at least one or more students as illustrated by block 420.  The process(s) 410 may also perform searches for information related to the course from web-based sources 404-409 such as MOOCs, Wikipedia.TM., and social media.  The\nprocessor(s) 410 may further perform ranking, tagging, and optimization of the search results to generate a plurality of curricula 421-4XX for each of the sub-groups respectively.  The optimization may be performed with respect to constraints.  The\nconstraints may be entered as input data by the instructor 402 or may be retrieved from school 403.\n<BR><BR>Example Scenarios\nFIG. 5 depicts an example scenario 500 of constructing a course boundary in accordance with an embodiment of the present disclosure.  The example scenario 500 may start with a concept graph 501 including preliminary course topics.  In one\nembodiment, the concept graph 501 may be provided by the instructor(s).  The preliminary course topics in the concept graph 501 may include core topics 503 (Signal Processing and Digital Communications) and non-core topics.  The non-core topics may be\nfurther categorized as prerequisites 502 (Fourier Transform, Linear Algebra, and Calculus) and topics of interest 504 (Mobile Communication).  The core topics 503 are indicated by solid black dots, the prerequisites 502 are indicated by hollow circles,\nand the topics of interest are indicated by grey dots.  The dependency between the preliminary course topics is indicated by the arrows with the dependent (prerequisite) at the origin of an arrow.  For example, the two prerequisites of Digital\nCommunications are Signal Processing and Calculus, whereas Digital Communications is a prerequisite of Mobile Communication.\nThe concept graph 501 serves as a basis for the scope of the course to offer.  The scope may be adjusted depending on the attributes of students.  At 511, the backgrounds of the students may be analyzed to determine whether to include or exclude\neach of the non-core topics.  In one embodiment, the analysis may be done with a set of questionnaires.  For instance, at 512, the Mobile Communication may be included once the students interested in the Mobile Communication have exceeded a predetermined\nthreshold.  In another embodiment, the students may be classified by the knowledge of prerequisites 502 according to the background data.  As indicated at 513, Linear Algebra and Calculus are known by most of the students (exceeding a predetermined\nthreshold) and are excluded from the concept graph 501.  Also at 513, Fourier Transform is not known by most of the students (not exceeding a predetermined threshold) and is not excluded.  The scope of the concept graph 501 may also be adjusted according\nto the objectives of course 520 from the students.  In still another embodiment, the objective of the course 520 from the students may include the following: (1) increase a chance for employment 521, and (2) prepare students for graduate-level courses\n522.  The processor(s) (not shown in FIG. 5) work in the background to perform searches 523 and 524 for the respective objectives 521 and 522.  The searchable domains may include internet search 525 or syllabi database 526.  The internet search 525 for\nthe objective 521 may be performed by using Natural Language Processing techniques to determine emerging fields or by trending keywords in job requirements based on the scope of the concept graph 501.  As illustrated in FIG. 5 concerning the internet\nsearch 525 for the objective 521, 5G and MIMO are the two course topics that may be most related to the technical field of Digital Communications and Mobile Communications.  The syllabi database search 526 may be focused on the dependency of\ngraduate-level course materials in the syllabi databases.  For the objective 522, Probability may be identified to be essential for a number of graduate-level courses, thereby, Probability may be defined as a new prerequisite for students preparing for\ngraduate-level courses.  Combining actions at 514, 515, and 530, a course boundary 540 may be constructed after taking into consideration of background profiles 511 and objectives of the course 520 from students.  The course boundary 540 now includes\ndifferent contents compared to those in the concept graph 501.  The course boundary 540 may include new non-core topics 542, 544, and the same core topics 503 (Signal Processing and Digital Communication).  At 542, a new prerequisite (Probability) may be\nadded, Linear Algebra and Calculus may be removed.  At 544, two new topics of interest MIMO and 5G may be added.\nFIG. 6 depicts an example scenario 600 of determining a plurality of blended-learning contents based on the course boundary in accordance with an embodiment of the present disclosure.  Example scenario 600 may start with the course boundary 540\nconstructed according to one embodiment of the present disclosure.  The course boundary 540 has been described extensively in the previous paragraph.  Once the course boundary 540 is defined, the course materials pertaining to the course boundary may be\ncollected by actions performed in block 605, which searches multiple sources such as MOOCs 609, books and references 608 (which may be stored in institutional repositories), and Wikipedia.TM.  607 out of a non-comprehensive list of internet-accessible\nsources.  The course materials may include static forms like lecture slides, theses, white papers, journal articles, or dynamic forms like video sessions.  The search results of block 605 may be ranked according to meta-data such as on-line reviews,\nnumber of views per video, number of citations, number of references, or rating of the publisher.  The search results of block 605 may be tagged with traits indicating the ranking, the length, and the type of the course materials.  A predetermined number\nof the course materials may be chosen from the search results of block 605 after further relevancy analysis of the search results with respect to the course boundary.  Block 606 depicts an example of summarized traits of the course materials collected\nafter actions performed by block 605.  Two course topics in the boundary graph are concerned at 606, Fourier Transform and 5G.  The first course material for the definition of Fourier Transform may be found from a Wikipedia.TM.  link, which points to\nchapter 1 of Book A, and it may be a video lecture with a duration of 30 minutes.  The second course material for Fourier Transform regarding to Parseval's theorem is located on chapter 2 of Book B, also a video lecture with a duration of 20 minutes. \nSimilar observations may be made for the course materials collected for the course topic 5G at 606, and will not repeat here for brevity.\nBlock 602 depicts an example list of constraints information.  In one embodiment, the constraints information may be provided by the instructor(s).  At 603, optimization may be performed to arrange the course materials at 606 to satisfy the\nconstraints at 602.  The optimization procedure of this type may involve solving a system of equations with variables being constrained to integer values.  In some embodiments, the optimization may be performed separately to generate a blended-learning\ncurriculum 610 including a schedule 611 for the common topics and a schedule 612 for the private topics respectively.  In some embodiments, the teacher 601 may allocate a portion of the course schedule for specific topics to increase flexibility.  At\n604, the teacher 601 may specify 10% of the course schedule for review, 20% of the schedule for Signal Processing, 70% of the schedule for Digital Communications and all other topics.  Output information of block 604 may be taken into consideration by\nblock 603 for further optimizing the course content to fit the adjusted schedule in addition to the constraints information of block 602.  Thus the teacher 601 may change a portion of course schedule and quickly generate an optimized course content for\ndifferent course objectives or learning outcomes.\n<BR><BR>Example Process\nFIG. 7 depicts an example process of automatic generation of a blended-learning curricula for a class of students.  Process 700 may include one or more operations, actions, or functions as represented by one or more of blocks 710, 720, 730, and\n740.  Although illustrated as discrete blocks, various blocks of process 700 may be divided into additional blocks, combined into fewer blocks, or eliminated, depending on the desired implementation.  Process 700 may be implemented by apparatus 300 and\nany variations and/or derivatives thereof.  Process 700 may be an example implementation of process 200, whether partially or completely.  For illustrative purposes and without limitation, process 700 is described below in the context of apparatus 300. \nProcess 700 may begin at block 710.\nAt 710, process 700 may involve assessor unit 320 of apparatus 300 integrating the input data according to the instructions received from the users.  Process 700 may proceed from 710 to 720.\nAt 720, process 700 may involve assessor unit 320 constructing a course boundary to define the scope of the course.  Alternatively, process 700 may involve assessor unit 320 and searcher unit 330 of apparatus 300 constructing a course boundary\nfeaturing publicly available materials obtained by searcher unit 330.  Process 700 may proceed from 720 to 730.\nAt 730, process 700 may involve searcher unit 330 and analyzer unit 340 of apparatus 300 determining blended-learning course contents within the course boundary by searching multiple sources that are publically available.  Process 700 may\nproceed from 730 to 740.\nAt 740, process 700 may involve analyzer unit 340 and optimizer unit 350 of apparatus 300 generating a plurality of curricula for each sub-group of students by optimizing the blended-learning course contents with respect to constraints.  In some\nembodiments, the constraints may include two or more of the following: a number of instructors, a number of teaching assistants, a common course length, a private course length, and a number of office hours from the instructors and the teaching\nassistants.  The common course length may be defined as a product of a number of scheduled common sessions and a number of planned hours per common session.  The private course length may be defined as a product of a number of scheduled private sessions\nand a number of planned hours per private session.\nIn some embodiments, in integrating the input data from the group of users, process 700 may involve assessor unit 320 performing a number of operations.  For instance, assessor unit 320 may receive the input data and instructions from the users,\nclassify the input data according to the instructions, rank the input data according to the instructions, calculate the input data according to the instructions, and divide the students into the plurality of sub-groups according to the instructions.\nIn some embodiments, in integrating the input data according to the instructions received from the users, process 700 may involve assessor unit 320 classifying students according to an attribute of students (e.g. knowledge of a prerequisite). \nAnother attribute (e.g., test performance) of students may be ranked according to the instructions.  A different attribute (e.g., number of students interested in a particular topic) of the input data may be calculated for providing relevant course\ndirections.  The students may be divided into a number of sub-groups based on the classification or ranking results.\nIn some embodiments, in constructing the course boundary according to the input data of the users, process 700 may involve assessor unit 320 performing a number of operations.  For instance, process 700 may involve assessor unit 320 assembling a\nconcept graph of the course.  The concept graph may include one or more core topics and a plurality of non-core topics provided by the one or more instructors.  The non-core topics may include one or more prerequisites and one or more possible topics of\ninterest.  Moreover, for each of the non-core topics, process 700 may involve assessor unit 320 determining whether to exclude the respective non-core topic by analyzing the input data.  The input data may include a collection of background profiles of\nthe students and a set of requirements of the course.  Furthermore, process 700 may involve assessor unit 320 determining whether to add one or more relevant topics to the concept graph by analyzing the input data.  The input data may further include a\nset of objectives of the course.  The one or more relevant topics may be associated with the objectives of the course.\nIn some embodiments, in constructing a course boundary to define the scope of the course, process 700 may involve assessor unit 320 assembling a concept graph including core topics and non-core topics.  The core topics may be provided by the\ninstructor(s).  The non-core topics including prerequisite and topics of interest may also be provided by the instructor(s).  Process 700 may also involve searcher unit 330 searching for non-core topics related to the attributes of students such as\nbackgrounds, interests, or career goals.\nMoreover, in constructing a course boundary to define the scope of the course, process 700 may further involve assessor unit 320 determining whether to exclude a non-core topic initially provided by the instructor(s) or search results performed\naccording to some attributes of students.  A questionnaire concerning each of the non-core topics may be received by the students.  The responses to the questionnaire may be analyzed by the assessor unit 320.  Predetermined thresholds for the respective\nnon-core topics may be used to exclude a particular non-core topic if the corresponding threshold for that particular non-core topic is not met.\nAlternatively or additionally, in constructing a course boundary to define the scope of the course, process 700 may further involve assessor unit 320 determining whether to include a relevant topic of interest from search results performed\naccording to some attributes of students.  A questionnaire concerning each of the relevant topic of interest may be received by the students.  The responses to the questionnaire may be analyzed by the assessor unit 320.  Predetermined thresholds for the\nrespective relevant topic of interest may be used to include a particular topic if the corresponding threshold for that particular topic is exceeded.\nIn some embodiments, in determining whether to exclude the respective non-core topic by analyzing the input data, process 700 may involve assessor unit 320 performing a number of operations.  For instance, process 700 may involve assessor unit\n320 receiving a plurality of responses from the students concerning each of the non-core topics.  Additionally, process 700 may involve assessor unit 320 assigning a plurality of thresholds for each of the responses concerning each of the non-core\ntopics.  Moreover, process 700 may involve assessor unit 320 comparing each of the thresholds with a corresponding response for the respective non-core topic to include the respective non-core topic when the threshold is exceeded or to exclude the\nrespective non-core topic when the threshold is not exceeded.\nAlternatively or additionally, in determining whether to add the one or more relevant topics to the concept graph by analyzing the input data, process 700 may involve assessor unit 320 performing a number of operations.  For instance, process\n700 may involve assessor unit 320 receiving a plurality of responses from the students concerning each of the objectives of the course.  Additionally, process 700 may involve assessor unit 320 assigning a plurality of thresholds for each of the responses\nconcerning each of the objectives of the course.  Furthermore, process 700 may involve assessor unit 320 comparing each of the thresholds with a corresponding response for the respective objective of the course to include the respective relevant topic\nwhen the threshold is exceeded or to exclude the respective relevant topic when the threshold is not exceeded.\nIn some embodiments, in determining the plurality of blended-learning contents pertaining to the course boundary for the plurality of sub-groups of the class, respectively, process 700 may involve assessor unit 320 performing a number of\noperations.  For instance, process 700 may involve assessor unit 320 amassing information pertaining to the course boundary by searching for a collection of course materials from a plurality of sources.  The searching for the course materials from the\nsources may be performed using a set of keywords relevant to the one or more core topics and the non-core topics in the course boundary.  Additionally, process 700 may involve assessor unit 320 determining a first set of the common topics from the\namassed information pertaining to the course boundary.  The first set of the common topics may be correlated with the one or more core topics.  Moreover, process 700 may involve assessor unit 320 determining a plurality of first sets of the private\ntopics from the amassed information pertaining to the course boundary for each of the sub-groups of the class.  The plurality of the first sets of private topics may be correlated with the non-core topics.  In some embodiments, the sources may include a\ncollection of substances from the one or more instructors, a plurality of institutional repositories, and a collection of educational databases publicly available.\nIn some embodiments, in determining blended-learning course contents within the course boundary by searching multiple sources that are publically available, process 700 may involve searcher unit 330 searching multiple sources such as a\ncollection of substances from instructor(s), a plurality of institutional repositories, and a collection of educational databases publicly available.\nAlternatively or additionally, in determining blended-learning course contents within the course boundary by searching multiple sources that are publically available, process 700 may involve analyzer unit 340 determining a first set of common\ntopics from the search results performed according to the scope of the course boundary.  The first set of the common topics may be correlated with core topics and may be shared by all the students in a class.\nAlternatively or additionally, in determining blended-learning course contents within the course boundary by searching multiple sources that are publically available, process 700 may involve analyzer unit 340 determining first sets of private\ntopics for each of the sub-groups from the search results performed according to the scope of the course boundary.  Each of the sets of the private topics may be correlated with non-core topics for the respective sub-group.\nIn some embodiments, in optimizing the plurality of curricula based on the blended-learning contents for the plurality of the sub-groups of the class with respect to the constraints, process 700 may involve optimizer unit 350 choosing a second\nset of the common topics from the first set of the common topics to fit within a common course length.  Alternatively or additionally, process 700 may involve optimizer unit 350 choosing a second sets of the private topics from the first sets of the\nprivate topics to fit within a private course length.  Alternatively or additionally, process 700 may involve optimizer unit 350 associating each of the second sets of the private topics to each of the students in the class.  Alternatively or\nadditionally, process 700 may involve optimizer unit 350 organizing a common class schedule based on the second set of the common topics for the class as a whole.  Alternatively or additionally, process 700 may involve optimizer unit 350 organizing a\nplurality of private class schedules based on the second sets of the private topics for each of the sub-groups of the class.  Alternatively or additionally, process 700 may involve optimizer unit 350 generating a plurality of curricula, each of which for\na respective sub-group of the sub-groups of the class, by combining the common class schedule, each of the private class schedules, a number of assignments, a number of projects, and a number of exams.\nThe foregoing one or more embodiments facilitate the optimization of online learning techniques within a computer infrastructure by searching for a collection of course materials on-line from a plurality of sources publically available,\nclassifying the search results according to the meta-data associated with the respective course materials, and determining the course contents with matched keywords via Natural Language Processing techniques.  The computer-aided methods involved in these\nembodiments automatically search, select, and organize the appropriate course contents for students with diverse backgrounds or different needs to achieve their respective course objectives.  Such tasks were not done by humans or machines before.\n<BR><BR>CONCLUSION\nThe descriptions of the various embodiments of the present teachings have been presented for purposes of illustration, but are not intended to be exhaustive or limited to the embodiments disclosed.  Many modifications and variations will be\napparent to those of ordinary skill in the art without departing from the scope and spirit of the described embodiments.  The terminology used herein was chosen to best explain the principles of the embodiments, the practical application or technical\nimprovement over technologies found in the marketplace, or to enable others of ordinary skill in the art to understand the embodiments disclosed herein.\nWhile the foregoing has described what are considered to be the best state and/or other examples, it is understood that various modifications may be made therein and that the subject matter disclosed herein may be implemented in various forms\nand examples, and that the teachings may be applied in numerous applications, only some of which have been described herein.  It is intended by the following claims to claim any and all applications, modifications and variations that fall within the true\nscope of the present teachings.\nThe components, steps, features, objects, benefits and advantages that have been discussed herein are merely illustrative.  None of them, nor the discussions relating to them, are intended to limit the scope of the present teachings.  While\nvarious advantages have been discussed herein, it will be understood that not all embodiments necessarily include all advantages.  Unless otherwise stated, all measurements, values, ratings, positions, magnitudes, sizes, and other specifications that are\nset forth in this specification, including in the claims that follow, are approximate, not exact.  They are intended to have a reasonable range that is consistent with the functions to which they relate and with what is customary in the art to which they\npertain.\nNumerous other embodiments are also contemplated.  These include embodiments that have fewer, additional, and/or different components, steps, features, objects, benefits and advantages.  These also include embodiments in which the components\nand/or steps are arranged and/or ordered differently.\nAspects of the present disclosure are described herein with reference to a flowchart illustration and/or block diagram of a method, apparatus (systems), and computer program products according to embodiments of the present disclosure.  It will\nbe understood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer readable program instructions.\nThese computer readable program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute\nvia the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.  These computer readable program instructions may also be\nstored in a computer readable storage medium that can direct a computer, a programmable data processing apparatus, and/or other devices to function in a particular manner, such that the computer readable storage medium having instructions stored therein\ncomprises an article of manufacture including instructions which implement aspects of the function/act specified in the flowchart and/or block diagram block or blocks.\nThe computer readable program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other device to cause a series of operational steps to be performed on the computer, other programmable apparatus or\nother device to produce a computer implemented process, such that the instructions which execute on the computer, other programmable apparatus, or other device implement the functions/acts specified in the flowchart and/or block diagram block or blocks.\nThe flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present disclosure. \nIn this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of instructions, which comprises one or more executable instructions for implementing the specified logical function(s).  In some alternative\nimplementations, the functions noted in the blocks may occur out of the order noted in the Figures.  For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse\norder, depending upon the functionality involved.  It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special\npurpose hardware-based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.\nWhile the foregoing has been described in conjunction with exemplary embodiments, it is understood that the term \"exemplary\" is merely meant as an example, rather than the best or optimal.  Except as stated immediately above, nothing that has\nbeen stated or illustrated is intended or should be interpreted to cause a dedication of any component, step, feature, object, benefit, advantage, or equivalent to the public, regardless of whether it is or is not recited in the claims.\nIt will be understood that the terms and expressions used herein have the ordinary meaning as is accorded to such terms and expressions with respect to their corresponding respective areas of inquiry and study except where specific meanings have\notherwise been set forth herein.  Relational terms such as first and second and the like may be used solely to distinguish one entity or action from another without necessarily requiring or implying any actual such relationship or order between such\nentities or actions.  The terms \"comprises,\" \"comprising,\" or any other variation thereof, are intended to cover a non-exclusive inclusion, such that a process, method, article, or apparatus that comprises a list of elements does not include only those\nelements but may include other elements not expressly listed or inherent to such process, method, article, or apparatus.  An element proceeded by \"a\" or \"an\" does not, without further constraints, preclude the existence of additional identical elements\nin the process, method, article, or apparatus that comprises the element.\nThe Abstract of the Disclosure is provided to allow the reader to quickly ascertain the nature of the technical disclosure.  It is submitted with the understanding that it will not be used to interpret or limit the scope or meaning of the\nclaims.  In addition, in the foregoing Detailed Description, it can be seen that various features are grouped together in various embodiments for the purpose of streamlining the disclosure.  This method of disclosure is not to be interpreted as\nreflecting an intention that the claimed embodiments have more features than are expressly recited in each claim.  Rather, as the following claims reflect, inventive subject matter lies in less than all features of a single disclosed embodiment.  Thus\nthe following claims are hereby incorporated into the Detailed Description, with each claim standing on its own as a separately claimed subject matter.", "application_number": "15257843", "abstract": " Techniques pertaining to automating generation of curricula for a class\n     of students with blended-learning contents are presented. The\n     blended-leaning contents provide common topics shared by all the students\n     in a class as well as self-learning materials for sub-groups of the\n     class. This approach creates curricula optimally attending to students\n     with different interests and backgrounds. The course materials are\n     selected by searching multiple external sources publically available\n     while filtering the search results through inputs of the students and\n     requirements set by the instructors. The course contents and schedules of\n     the curricula are further optimized by organizing the filtered search\n     results with respect to a set of constraints.\n", "citations": ["7677896", "8506304", "8699939", "20020098468", "20070099161", "20080038708", "20080138786", "20100075289", "20100159438", "20110117534", "20130011821", "20130280690", "20140024009", "20140099626", "20140193795", "20140272889", "20140342325"], "related": []}, {"id": "20180114329", "patent_code": "10375266", "patent_name": "Systems and methods for selecting an action based on a detected person", "year": "2019", "inventor_and_country_data": " Inventors: \nWexler; Yonatan (Jerusalem, IL), Shashua; Amnon (Mevaseret Zion, IL)  ", "description": "<BR><BR>BACKGROUND\nTechnical Field\nThis disclosure generally relates to devices and methods for capturing and processing images from an environment of a user, and using information derived from captured images.  More particularly, this disclosure relates to devices and methods\nfor selection of a device action based on a detected person.\nBackground Information\nToday, technological advancements make it possible for wearable devices to automatically capture images and store information that is associated with the captured images.  Certain devices have been used to digitally record aspects and personal\nexperiences of one's life in an exercise typically called \"lifelogging.\" Some individuals log their life so they can retrieve moments from past activities, for example, social events, trips, etc. Lifelogging may also have significant benefits in other\nfields (e.g., business, fitness and healthcare, and social research).  Lifelogging devices, while useful for tracking daily activities, may be improved with capability to enhance one's interaction in his environment with feedback and other advanced\nfunctionality based on the analysis of captured image data.\nEven though users can capture images with their smartphones and some smartphone applications can process the captured images, smartphones may not be the best platform for serving as lifelogging apparatuses in view of their size and design. \nLifelogging apparatuses should be small and light, so they can be easily worn.  Moreover, with improvements in image capture devices, including wearable apparatuses, additional functionality may be provided to assist users in navigating in and around an\nenvironment, identifying persons and objects they encounter, and providing feedback to the users about their surroundings and activities.  Therefore, there is a need for apparatuses and methods for automatically capturing and processing images to provide\nuseful information to users of the apparatuses, and for systems and methods to process and leverage information gathered by the apparatuses.\n<BR><BR>SUMMARY\nEmbodiments consistent with the present disclosure provide devices and methods for automatically capturing and processing images from an environment of a user, and systems and methods for processing information related to images captured from\nthe environment of the user.\nIn one embodiment, a wearable apparatus for causing an action to be executed based on a person being present in an environment of a user of the wearable apparatus may comprise a wearable image sensor configured to capture a plurality of images\nfrom the environment of the user of the wearable apparatus and at least one processing device.  The at least one processing device may be programmed to analyze at least one of the plurality of images to detect the person; analyze at least one of the\nplurality of images to identify an attribute of the detected person; and select at least one category for the detected person based on the identified attribute.  The at least one processing device may be further programmed to select at least one action\nbased on the at least one category and cause the at least one selected action to be executed.\nIn one embodiment, a method is provided for causing an action to be executed based on a person being present in an environment of a user of a wearable apparatus.  The method includes receiving a plurality of images captured by an image sensor of\nthe wearable apparatus from the environment of the user of the wearable apparatus; analyzing at least one of the plurality of images to detect the person; analyzing at least one of the plurality of images to identify an attribute of the detected person;\nselecting at least one category for the detected person based on the identified attribute; selecting at least one action based on the at least one category; and causing the at least one selected action to be executed.\nIn one embodiment, a wearable apparatus for causing an action to be executed based on whether a person is physically present in an environment of a user of the wearable apparatus may comprise a wearable image sensor configured to capture a\nplurality of images from the environment of the user of the wearable apparatus and at least one processing device.  The at least one processing device may be programmed to analyze at least one of the plurality of images to detect the person and analyze\nat least one of the plurality of images to determine whether the detected person is physically present in the environment of the user.  The at least one processing device may be further programmed to select at least one action based on whether the\ndetected person is physically present in the environment of the user and cause the selected at least one action to be executed.\nIn one embodiment, a wearable apparatus for causing an action to be executed based on whether a person is visible on a display of a device in an environment of a user of the wearable apparatus may comprise a wearable image sensor configured to\ncapture a plurality of images from the environment of the user of the wearable apparatus and at least one processing device.  The at least one processing device may be programmed to analyze at least one of the plurality of images to detect the person and\nanalyze at least one of the plurality of images to determine whether the detected person is visible on the display of the device.  The at least one processing device may be further programmed to select at least one action based on whether the detected\nperson is visible on the display of the device and cause the at least one action to be executed.\nIn one embodiment, a wearable apparatus for causing an action to be executed based on whether a person is physically present in an environment of a user of the wearable apparatus may comprise a wearable image sensor configured to capture a\nplurality of images from the environment of the user of the wearable apparatus and at least one processing device.\nThe at least one processing device may be programmed to analyze at least one of the plurality of images to detect the person and analyze at least one of the plurality of images to determine whether the detected person is physically present in\nthe environment of the user or whether a graphical representation of the detected person appears in the environment of the user.  The at least one processing device may be further programmed to select a first action after the determination is made that\nthe detected person is physically present in the environment of the user, select a second action different from the first action after the determination is made that the graphical representation of the detected person appears in the environment of the\nuser, and cause the first action or the second action to be executed.\nIn one embodiment, a method for causing an action to be executed based on whether a person is physically present in an environment of a user of the wearable apparatus may comprise receiving a plurality of images of the environment of the user\nfrom an image sensor of the wearable apparatus, analyzing at least one of the plurality of images to detect the person, and analyzing at least one of the plurality of images to determine whether the detected person is physically present in the\nenvironment of the user.  The method may further comprise selecting at least one action based on whether the detected person is physically present in the environment of the user and causing the selected at least one action to be executed.\nIn one embodiment, a method for causing an action to be executed based on whether a person is visible on a display of a device in an environment of a user of the wearable apparatus may comprise receiving a plurality of images of the environment\nof the user from an image sensor of the wearable apparatus, analyzing at least one of the plurality of images to detect the person, and analyzing at least one of the plurality of images to determine whether the detected person is visible on the display\nof the device.  The method may further comprise selecting at least one action based on whether the detected person is visible on the display of the device and causing the at least one action to be executed.\nIn one embodiment, a method for causing an action to be executed based on whether a person is physically present in an environment of a user of the wearable apparatus may receiving a plurality of images of the environment of the user from an\nimage sensor of the wearable apparatus, analyzing at least one of the plurality of images to detect the person, and analyzing at least one of the plurality of images to determine whether the detected person is physically present in the environment of the\nuser or whether a graphical representation of the detected person appears in the environment of the user.  The method may further comprise selecting a first action after the determination is made that the detected person is physically present in the\nenvironment of the user, selecting a second action different from the first action after the determination is made that the graphical representation of the detected person appears in the environment of the user, and causing the first action or the second\naction to be executed.\nIn one embodiment, a system for updating profile information based on data collected by a wearable apparatus may comprise a database storing a plurality of profiles and at least one processing device.  The at least one processing device may be\nprogrammed to obtain identification information associated with a person detected in one or more images captured by a wearable image sensor included in the wearable apparatus and obtain, from the wearable apparatus, auxiliary information associated with\nthe detected person.  The at least one processing device may be further programmed to identify, in the database, a profile associated with the detected person based on the identification information and update the identified profile based on the\nauxiliary information.\nIn one embodiment, a method is provided for updating profile information based on data collected by a wearable apparatus.  The method may comprise obtaining identification information associated with a person detected in one or more images\ncaptured by a wearable image sensor included in the wearable apparatus and obtaining, from the wearable apparatus, auxiliary information associated with the detected person.  The method may further comprise identifying, in a database storing a plurality\nof profiles, a profile associated with the detected person based on the identification information and updating the identified profile based on the auxiliary information.\nIn one embodiment, a system is provided for providing information to a user of a wearable apparatus.  The system includes at least one processing device programmed to identify a person represented in at least one image captured by a wearable\nimage sensor included in the wearable apparatus, obtain information associated with the person represented in the at least one captured image, and obtain at least one affinity measurement representing a degree of a relationship between the user and the\nperson.  The at least one processing device is further programmed to determine, based on the at least one affinity measurement, an information level to be disclosed to the user of the wearable apparatus and provide, to the user of the wearable apparatus,\nthe information based on the information associated with the person and on the information level.\nIn one embodiment, a method is provided for providing information to a user of a wearable apparatus.  The method includes identifying a person represented in at least one image captured by a wearable image sensor included in the wearable\napparatus, obtaining information associated with the person represented in the at least one captured image, and obtaining at least one affinity measurement representing a degree of a relationship between the user and the person.  The method further\nincludes determining, based on the at least one affinity measurement, an information level to be disclosed to the user of the wearable apparatus and providing, to the user of the wearable apparatus, the information based on the information associated\nwith the person and on the information level.\nIn one embodiment, a wearable apparatus is provided for registering a verbal contract.  The wearable apparatus includes at least one image sensor configured to capture a plurality of images from an environment of a user of the wearable apparatus\nand at least one audio sensor configured to capture audio data from the environment of the user of the wearable apparatus.  The wearable apparatus also includes at least one processing device programmed to analyze the plurality of images to detect a\nperson in the environment of the user of the wearable apparatus, obtain identification information associated with the detected person, analyze at least a portion of the audio data to identify one or more words associated with the verbal contract and\nspoken by the user of the wearable apparatus or the detected person, and obtain at least one profile of the user.  The at least one processing device is also programmed to authenticate an identity of the user based on the at least one profile of the user\nand, based on the authentication of the identity of the user, register the verbal contract and the identification information associated with the detected person.\nIn another embodiment, a method is provided for registering a verbal contract.  The method includes analyzing a plurality of images captured by at least one image sensor from an environment of a user of a wearable apparatus to detect a person in\nthe environment of the user of the wearable apparatus, obtaining identification information associated with the detected person, and analyzing at least a portion of audio data captured by at least one audio sensor from the environment of the user of the\nwearable apparatus to identify one or more words associated with the verbal contract and spoken by the user of the wearable apparatus or the detected person.  The method also includes obtaining at least one profile of the user, authenticating an identity\nof the user based on the at least one profile of the user and, based on the authentication of the identity of the user, registering the verbal contract and the identification information associated with the detected person.\nIn one embodiment, a wearable apparatus for providing information to a user of the wearable apparatus is disclosed.  The apparatus may include at least one image sensor configured to capture a plurality of images from an environment of the user\nof the wearable apparatus, at least one communication device, and at least one processing device.  The processing device may be programmed to analyze at least one of the plurality of images to detect an object in the environment of the user of the\nwearable apparatus, determine a measurement of an estimated physical distance from the user to the object, and transmit, based on the measurement and using the at least one communication device, information related to the detected object.\nIn one embodiment, a method provides information to a user of a wearable apparatus.  The method may be performed by at least one image sensor, at least one communication device, and at least one processing device.  The method may include\ncapturing, via the at least one image sensor, a plurality of images from an environment of the user of the wearable apparatus, analyzing, via the at least one processing device, at least one of the plurality of images to detect an object in the\nenvironment of the user of the wearable apparatus, determining a measurement of an estimated physical distance from the user to the object, and transmitting based on the measurement and using the at least one communication device, information related to\nthe detected object.\nIn one embodiment, a system for providing recommendations based on images captured by a wearable apparatus is disclosed.  The system may include a wearable image sensor and at least one processing device.  The processing device may be programmed\nto analyze at least one image captured by the wearable image sensor included in the wearable apparatus from an environment of a user of the wearable apparatus, obtain information based on a result of the analysis of the at least one captured image,\ngenerate at least one contact recommendation for at least one new social network contact based on the obtained information, and provide the at least one contact recommendation to at least one of the user and at least one other person.\nIn one embodiment, a method provides recommendations based on images captured by a wearable apparatus.  The method may be performed by a wearable image sensor and at least one processing device.  The method may include analyzing at least one\nimage captured by the wearable image sensor included in the wearable apparatus from an environment of a user of the wearable apparatus, obtaining information based on a result of the analysis of the at least one captured image, generating at least one\ncontact recommendation for at least one new social network contact based on the obtained information, and providing the at least one contact recommendation to at least one of the user and at least one other person.\nIn one embodiment, a wearable apparatus is disclosed.  The apparatus may include at least one image sensor configured to capture a plurality of images from an environment of a user of the wearable apparatus, and at least one projector configured\nto emit a light pattern configured to visually indicate to the user of the wearable apparatus an active field of view of the at least one image sensor.\nIn one embodiment, a method provides visual feedback to a user of a wearable apparatus.  The method may include capturing, via at least one image sensor included in the wearable apparatus, a plurality of images from an environment of the user of\nthe wearable apparatus, activating at least one projector included in the wearable apparatus based at least on a visual trigger appearing in at least one of the plurality of images, and emitting, via the at least one projector, a light pattern configured\nto visually indicate to the user of the wearable apparatus an active field of view of the at least one image sensor.\nConsistent with other disclosed embodiments, non-transitory computer-readable storage media may store program instructions, which are executed by at least one processor and perform any of the methods described herein.\nThe foregoing general description and the following detailed description are exemplary and explanatory only and are not restrictive of the claims. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe accompanying drawings, which are incorporated in and constitute a part of this disclosure, illustrate various disclosed embodiments.  In the drawings:\nFIG. 1A is a schematic illustration of an example of a user wearing a wearable apparatus according to a disclosed embodiment.\nFIG. 1B is a schematic illustration of an example of the user wearing a wearable apparatus according to a disclosed embodiment.\nFIG. 1C is a schematic illustration of an example of the user wearing a wearable apparatus according to a disclosed embodiment.\nFIG. 1D is a schematic illustration of an example of the user wearing a wearable apparatus according to a disclosed embodiment.\nFIG. 2 is a schematic illustration of an example system consistent with the disclosed embodiments.\nFIG. 3A is a schematic illustration of an example of the wearable apparatus shown in FIG. 1A.\nFIG. 3B is an exploded view of the example of the wearable apparatus shown in FIG. 3A.\nFIG. 4A is a schematic illustration of an example of the wearable apparatus shown in FIG. 1B from a first viewpoint.\nFIG. 4B is a schematic illustration of the example of the wearable apparatus shown in FIG. 1B from a second viewpoint.\nFIG. 5A is a block diagram illustrating an example of the components of a wearable apparatus according to a first embodiment.\nFIG. 5B is a block diagram illustrating an example of the components of a wearable apparatus according to a second embodiment.\nFIG. 5C is a block diagram illustrating an example of the components of a wearable apparatus according to a third embodiment.\nFIG. 6 illustrates an exemplary embodiment of a memory containing software modules consistent with the present disclosure.\nFIG. 7 is a schematic illustration of an embodiment of a wearable apparatus including an orientable image capture unit.\nFIG. 8 is a schematic illustration of an embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.\nFIG. 9 is a schematic illustration of a user wearing a wearable apparatus consistent with an embodiment of the present disclosure.\nFIG. 10 is a schematic illustration of an embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.\nFIG. 11 is a schematic illustration of an embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.\nFIG. 12 is a schematic illustration of an embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.\nFIG. 13 is a schematic illustration of an embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.\nFIG. 14 is a schematic illustration of an embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.\nFIG. 15 is a schematic illustration of an embodiment of a wearable apparatus power unit including a power source.\nFIG. 16 is a schematic illustration of an exemplary embodiment of a wearable apparatus including protective circuitry.\nFIG. 17 illustrates an exemplary embodiment of a memory containing software modules for selecting an action based on a detected person consistent with the present disclosure.\nFIG. 18A is a schematic illustration of an example image captured by a wearable apparatus consistent with an embodiment of the present disclosure.\nFIG. 18B is a schematic illustration of another example image captured by a wearable apparatus consistent with an embodiment of the present disclosure.\nFIG. 19 is a flowchart of an example method for selecting an action based on a detected person consistent with the present disclosure.\nFIG. 20 illustrates an exemplary embodiment of a memory containing software modules for selecting an action based on a detected person consistent with the present disclosure.\nFIG. 21A is a schematic illustration of an example image captured by a wearable apparatus consistent with an embodiment of the present disclosure.\nFIG. 21B is a schematic illustration of another example image captured by a wearable apparatus consistent with an embodiment of the present disclosure.\nFIG. 22A is a flowchart of an example method for causing execution of an action based on physical presence of a detected person consistent with the present disclosure.\nFIG. 22B is a flowchart of an example method for causing execution of an action based on whether a detected person is visible on a display consistent with the present disclosure.\nFIG. 22C is a flowchart of another example method for causing execution of an action based on physical presence of a detected person consistent with the present disclosure.\nFIG. 23 illustrates an exemplary embodiment of a memory containing software modules for updating profile information based on data collected by a wearable apparatus consistent with the present disclosure.\nFIG. 24 is a schematic illustration of a profile stored in a database, consistent with an embodiment of the present disclosure.\nFIG. 25 is a flowchart of an example method for updating profile information based on data collected by a wearable apparatus consistent with the present disclosure.\nFIG. 26 illustrates an exemplary embodiment of a memory containing software modules consistent with the present disclosure.\nFIG. 27 shows an example environment including a wearable apparatus for capturing and processing images.\nFIG. 28A is a flowchart illustrating an exemplary method for identifying a person and information associated with the person.\nFIG. 28B is a flowchart illustrating an exemplary method for determining a level of detail of information relating to the identified person and provided to the user.\nFIG. 29 illustrates an exemplary embodiment of a memory containing software modules consistent with the present disclosure.\nFIG. 30 shows an example environment including a wearable apparatus for capturing and processing images and audio data.\nFIG. 31A is a flowchart illustrating an exemplary method for analyzing image and audio data captured by a wearable device.\nFIG. 31B is a flowchart illustrating an exemplary method for registering a verbal contract based on an analysis of captured image and audio data.\nFIG. 31C is a flowchart illustrating an exemplary method for registering information related to a witness to a verbal contract.\nFIG. 32 illustrates an exemplary embodiment of a memory containing software modules consistent with the present disclosure.\nFIG. 33A is a schematic illustration of an example of a user wearing a wearable apparatus and capturing an image of a person according to a disclosed embodiment.\nFIG. 33B is a schematic illustration of an example of an update of a social network profile according to a disclosed embodiment.\nFIG. 33C is a schematic illustration of an example of a user wearing a wearable apparatus and capturing an image of an object according to a disclosed embodiment.\nFIG. 33D is a schematic illustration of an example of an update of a social network profile according to a disclosed embodiment.\nFIG. 34 is a flowchart of an example of a method for providing information to a user of a wearable apparatus.\nFIG. 35 illustrates an exemplary embodiment of a memory containing software modules consistent with the present disclosure.\nFIG. 36A is a schematic illustration of an example of a user wearing a wearable apparatus capturing an image of a person according to a disclosed embodiment.\nFIG. 36B is a schematic illustration of an example of a contact recommendation according to a disclosed embodiment.\nFIG. 36C is a schematic illustration of an example of a user wearing a wearable apparatus capturing an image of a person according to a disclosed embodiment.\nFIG. 36D is a schematic illustration of an example of a contact recommendation according to a disclosed embodiment.\nFIG. 37 is a flowchart of an example of a method for providing contact recommendations based on captured images to a user of a wearable apparatus.\nFIG. 38A is a diagrammatic view of an apparatus including a light projector.\nFIG. 38B is a diagrammatic view of a wearable apparatus securable to an article of clothing that includes a light projector consistent with the present disclosure.\nFIG. 39 is a diagrammatic illustration of one example of a type of visual feedback that the light projector shown in FIG. 38A may provide to a user.\nFIGS. 40A-40H are examples of various patterns that can be generated by the light projector of FIG. 38A and/or by the light projector of FIG. 38B.\nFIG. 41 is a flowchart of an example of a method for providing visual feedback to a user of a wearable apparatus.\n<BR><BR>DETAILED DESCRIPTION\nThe following detailed description refers to the accompanying drawings.  Wherever possible, the same reference numbers are used in the drawings and the following description to refer to the same or similar parts.  While several illustrative\nembodiments are described herein, modifications, adaptations and other implementations are possible.  For example, substitutions, additions or modifications may be made to the components illustrated in the drawings, and the illustrative methods described\nherein may be modified by substituting, reordering, removing, or adding steps to the disclosed methods.  Accordingly, the following detailed description is not limited to the disclosed embodiments and examples.  Instead, the proper scope is defined by\nthe appended claims.\nFIG. 1A illustrates a user 100 wearing an apparatus 110 that is physically connected (or integral) to glasses 130, consistent with the disclosed embodiments.  Glasses 130 may be prescription glasses, magnifying glasses, non-prescription glasses,\nsafety glasses, sunglasses, etc. Additionally, in some embodiments, glasses 130 may include parts of a frame and earpieces, nosepieces, etc., and one or no lenses.  Thus, in some embodiments, glasses 130 may function primarily to support apparatus 110,\nand/or an augmented reality display device or other optical display device.  In some embodiments, apparatus 110 may include an image sensor (not shown in FIG. 1A) for capturing real-time image data of the field-of-view of user 100.  The term \"image data\"\nincludes any form of data retrieved from optical signals in the near-infrared, infrared, visible, and ultraviolet spectrums.  The image data may include video clips and/or photographs.\nIn some embodiments, apparatus 110 may communicate wirelessly or via a wire with a computing device 120.  In some embodiments, computing device 120 may include, for example, a smartphone, or a tablet, or a dedicated processing unit, which may be\nportable (e.g., can be carried in a pocket of user 100).  Although shown in FIG. 1A as an external device, in some embodiments, computing device 120 may be provided as part of wearable apparatus 110 or glasses 130, whether integral thereto or mounted\nthereon.  In some embodiments, computing device 120 may be included in an augmented reality display device or optical head mounted display provided integrally or mounted to glasses 130.  In other embodiments, computing device 120 may be provided as part\nof another wearable or portable apparatus of user 100 including a wrist-strap, a multifunctional watch, a button, a clip-on, etc. And in other embodiments, computing device 120 may be provided as part of another system, such as an on-board automobile\ncomputing or navigation system.  A person skilled in the art can appreciate that different types of computing devices and arrangements of devices may implement the functionality of the disclosed embodiments.  Accordingly, in other implementations,\ncomputing device 120 may include a Personal Computer (PC), laptop, an Internet server, etc.\nFIG. 1B illustrates user 100 wearing apparatus 110 that is physically connected to a necklace 140, consistent with a disclosed embodiment.  Such a configuration of apparatus 110 may be suitable for users that do not wear glasses some or all of\nthe time.  In this embodiment, user 100 can easily wear apparatus 110, and take it off.\nFIG. 1C illustrates user 100 wearing apparatus 110 that is physically connected to a belt 150, consistent with a disclosed embodiment.  Such a configuration of apparatus 110 may be designed as a belt buckle.  Alternatively, apparatus 110 may\ninclude a clip for attaching to various clothing articles, such as belt 150, or a vest, a pocket, a collar, a cap or hat or other portion of a clothing article.\nFIG. 1D illustrates user 100 wearing apparatus 110 that is physically connected to a wrist strap 160, consistent with a disclosed embodiment.  Although the aiming direction of apparatus 110, according to this embodiment, may not match the\nfield-of-view of user 100, apparatus 110 may include the ability to identify a hand-related trigger based on the tracked eye movement of a user 100 indicating that user 100 is looking in the direction of the wrist strap 160.  Wrist strap 160 may also\ninclude an accelerometer, a gyroscope, or other sensor for determining movement or orientation of a user's 100 hand for identifying a hand-related trigger.\nFIG. 2 is a schematic illustration of an exemplary system 200 including a wearable apparatus 110, worn by user 100, and an optional computing device 120 and/or a server 250 capable of communicating with apparatus 110 via a network 240,\nconsistent with disclosed embodiments.  In some embodiments, apparatus 110 may capture and analyze image data, identify a hand-related trigger present in the image data, and perform an action and/or provide feedback to a user 100, based at least in part\non the identification of the hand-related trigger.  In some embodiments, optional computing device 120 and/or server 250 may provide additional functionality to enhance interactions of user 100 with his or her environment, as described in greater detail\nbelow.\nAccording to the disclosed embodiments, apparatus 110 may include an image sensor system 220 for capturing real-time image data of the field-of-view of user 100.  In some embodiments, apparatus 110 may also include a processing unit 210 for\ncontrolling and performing the disclosed functionality of apparatus 110, such as to control the capture of image data, analyze the image data, and perform an action and/or output a feedback based on a hand-related trigger identified in the image data. \nAccording to the disclosed embodiments, a hand-related trigger may include a gesture performed by user 100 involving a portion of a hand of user 100.  Further, consistent with some embodiments, a hand-related trigger may include a wrist-related trigger. \nAdditionally, in some embodiments, apparatus 110 may include a feedback outputting unit 230 for producing an output of information to user 100.\nAs discussed above, apparatus 110 may include an image sensor 220 for capturing image data.  The term \"image sensor\" refers to a device capable of detecting and converting optical signals in the near-infrared, infrared, visible, and ultraviolet\nspectrums into electrical signals.  The electrical signals may be used to form an image or a video stream (i.e. image data) based on the detected signal.  The term \"image data\" includes any form of data retrieved from optical signals in the\nnear-infrared, infrared, visible, and ultraviolet spectrums.  Examples of image sensors may include semiconductor charge-coupled devices (CCD), active pixel sensors in complementary metal-oxide-semiconductor (CMOS), or N-type metal-oxide-semiconductor\n(NMOS, Live MOS).  In some cases, image sensor 220 may be part of a camera included in apparatus 110.\nApparatus 110 may also include a processor 210 for controlling image sensor 220 to capture image data and for analyzing the image data according to the disclosed embodiments.  As discussed in further detail below with respect to FIG. 5A,\nprocessor 210 may include a \"processing device\" for performing logic operations on one or more inputs of image data and other data according to stored or accessible software instructions providing desired functionality.  In some embodiments, processor\n210 may also control feedback outputting unit 230 to provide feedback to user 100 including information based on the analyzed image data and the stored software instructions.  As the term is used herein, a \"processing device\" may access memory where\nexecutable instructions are stored or, in some embodiments, a \"processing device\" itself may include executable instructions (e.g., stored in memory included in the processing device).\nIn some embodiments, the information or feedback information provided to user 100 may include time information.  The time information may include any information related to a current time of day and, as described further below, may be presented\nin any sensory perceptive manner.  In some embodiments, time information may include a current time of day in a preconfigured format (e.g., 2:30 pm or 14:30).  Time information may include the time in the user's current time zone (e.g., based on a\ndetermined location of user 100), as well as an indication of the time zone and/or a time of day in another desired location.  In some embodiments, time information may include a number of hours or minutes relative to one or more predetermined times of\nday.  For example, in some embodiments, time information may include an indication that three hours and fifteen minutes remain until a particular hour (e.g., until 6:00 pm), or some other predetermined time.  Time information may also include a duration\nof time passed since the beginning of a particular activity, such as the start of a meeting or the start of a jog, or any other activity.  In some embodiments, the activity may be determined based on analyzed image data.  In other embodiments, time\ninformation may also include additional information related to a current time and one or more other routine, periodic, or scheduled events.  For example, time information may include an indication of the number of minutes remaining until the next\nscheduled event, as may be determined from a calendar function or other information retrieved from computing device 120 or server 250, as discussed in further detail below.\nFeedback outputting unit 230 may include one or more feedback systems for providing the output of information to user 100.  In the disclosed embodiments, the audible or visual feedback may be provided via any type of connected audible or visual\nsystem or both.  Feedback of information according to the disclosed embodiments may include audible feedback to user 100 (e.g., using a Bluetooth.TM.  or other wired or wirelessly connected speaker, or a bone conduction headphone).  Feedback outputting\nunit 230 of some embodiments may additionally or alternatively produce a visible output of information to user 100, for example, as part of an augmented reality display projected onto a lens of glasses 130 or provided via a separate heads up display in\ncommunication with apparatus 110, such as a display 260 provided as part of computing device 120, which may include an onboard automobile heads up display, an augmented reality device, a virtual reality device, a smartphone, PC, table, etc.\nThe term \"computing device\" refers to a device including a processing unit and having computing capabilities.  Some examples of computing device 120 include a PC, laptop, tablet, or other computing systems such as an on-board computing system of\nan automobile, for example, each configured to communicate directly with apparatus 110 or server 250 over network 240.  Another example of computing device 120 includes a smartphone having a display 260.  In some embodiments, computing device 120 may be\na computing system configured particularly for apparatus 110, and may be provided integral to apparatus 110 or tethered thereto.  Apparatus 110 can also connect to computing device 120 over network 240 via any known wireless standard (e.g., Wi-Fi,\nBluetooth.RTM., etc.), as well as near-filed capacitive coupling, and other short range wireless techniques, or via a wired connection.  In an embodiment in which computing device 120 is a smartphone, computing device 120 may have a dedicated application\ninstalled therein.  For example, user 100 may view on display 260 data (e.g., images, video clips, extracted information, feedback information, etc.) that originate from or are triggered by apparatus 110.  In addition, user 100 may select part of the\ndata for storage in server 250.\nNetwork 240 may be a shared, public, or private network, may encompass a wide area or local area, and may be implemented through any suitable combination of wired and/or wireless communication networks.  Network 240 may further comprise an\nintranet or the Internet.  In some embodiments, network 240 may include short range or near-field wireless communication systems for enabling communication between apparatus 110 and computing device 120 provided in close proximity to each other, such as\non or near a user's person, for example.  Apparatus 110 may establish a connection to network 240 autonomously, for example, using a wireless module (e.g., Wi-Fi, cellular).  In some embodiments, apparatus 110 may use the wireless module when being\nconnected to an external power source, to prolong battery life.  Further, communication between apparatus 110 and server 250 may be accomplished through any suitable communication channels, such as, for example, a telephone network, an extranet, an\nintranet, the Internet, satellite communications, off-line communications, wireless communications, transponder communications, a local area network (LAN), a wide area network (WAN), and a virtual private network (VPN).\nAs shown in FIG. 2, apparatus 110 may transfer or receive data to/from server 250 via network 240.  In the disclosed embodiments, the data being received from server 250 and/or computing device 120 may include numerous different types of\ninformation based on the analyzed image data, including information related to a commercial product, or a person's identity, an identified landmark, and any other information capable of being stored in or accessed by server 250.  In some embodiments,\ndata may be received and transferred via computing device 120.  Server 250 and/or computing device 120 may retrieve information from different data sources (e.g., a user specific database or a user's social network account or other account, the Internet,\nand other managed or accessible databases) and provide information to apparatus 110 related to the analyzed image data and a recognized trigger according to the disclosed embodiments.  In some embodiments, calendar-related information retrieved from the\ndifferent data sources may be analyzed to provide certain time information or a time-based context for providing certain information based on the analyzed image data.\nAn example of wearable apparatus 110 incorporated with glasses 130 according to some embodiments (as discussed in connection with FIG. 1A) is shown in greater detail in FIG. 3A.  In some embodiments, apparatus 110 may be associated with a\nstructure (not shown in FIG. 3A) that enables easy detaching and reattaching of apparatus 110 to glasses 130.  In some embodiments, when apparatus 110 attaches to glasses 130, image sensor 220 acquires a set aiming direction without the need for\ndirectional calibration.  The set aiming direction of image sensor 220 may substantially coincide with the field-of-view of user 100.  For example, a camera associated with image sensor 220 may be installed within apparatus 110 in a predetermined angle\nin a position facing slightly downwards (e.g., 5-15 degrees from the horizon).  Accordingly, the set aiming direction of image sensor 220 may substantially match the field-of-view of user 100.\nFIG. 3B is an exploded view of the components of the embodiment discussed regarding FIG. 3A.  Attaching apparatus 110 to glasses 130 may take place in the following way.  Initially, a support 310 may be mounted on glasses 130 using a screw 320,\nin the side of support 310.  Then, apparatus 110 may be clipped on support 310 such that it is aligned with the field-of-view of user 100.  The term \"support\" includes any device or structure that enables detaching and reattaching of a device including a\ncamera to a pair of glasses or to another object (e.g., a helmet).  Support 310 may be made from plastic (e.g., polycarbonate), metal (e.g., aluminum), or a combination of plastic and metal (e.g., carbon fiber graphite).  Support 310 may be mounted on\nany kind of glasses (e.g., eyeglasses, sunglasses, 3D glasses, safety glasses, etc.) using screws, bolts, snaps, or any fastening means used in the art.\nIn some embodiments, support 310 may include a quick release mechanism for disengaging and reengaging apparatus 110.  For example, support 310 and apparatus 110 may include magnetic elements.  As an alternative example, support 310 may include a\nmale latch member and apparatus 110 may include a female receptacle.  In other embodiments, support 310 can be an integral part of a pair of glasses, or sold separately and installed by an optometrist.  For example, support 310 may be configured for\nmounting on the arms of glasses 130 near the frame front, but before the hinge.  Alternatively, support 310 may be configured for mounting on the bridge of glasses 130.\nIn some embodiments, apparatus 110 may be provided as part of a glasses frame 130, with or without lenses.  Additionally, in some embodiments, apparatus 110 may be configured to provide an augmented reality display projected onto a lens of\nglasses 130 (if provided), or alternatively, may include a display for projecting time information, for example, according to the disclosed embodiments.  Apparatus 110 may include the additional display or alternatively, may be in communication with a\nseparately provided display system that may or may not be attached to glasses 130.\nIn some embodiments, apparatus 110 may be implemented in a form other than wearable glasses, as described above with respect to FIGS. 1B-1D, for example.  FIG. 4A is a schematic illustration of an example of an additional embodiment of apparatus\n110 from a first viewpoint.  The viewpoint shown in FIG. 4A is from the front of apparatus 110.  Apparatus 110 includes an image sensor 220, a clip (not shown), a function button (not shown) and a hanging ring 410 for attaching apparatus 110 to, for\nexample, necklace 140, as shown in FIG. 1B.  When apparatus 110 hangs on necklace 140, the aiming direction of image sensor 220 may not fully coincide with the field-of-view of user 100, but the aiming direction would still correlate with the\nfield-of-view of user 100.\nFIG. 4B is a schematic illustration of the example of a second embodiment of apparatus 110, from a second viewpoint.  The viewpoint shown in FIG. 4B is from a side orientation of apparatus 110.  In addition to hanging ring 410, as shown in FIG.\n4B, apparatus 110 may further include a clip 420.  User 100 can use clip 420 to attach apparatus 110 to a shirt or belt 150, as illustrated in FIG. 1C.  Clip 420 may provide an easy mechanism for disengaging and reengaging apparatus 110 from different\narticles of clothing.  In other embodiments, apparatus 110 may include a female receptacle for connecting with a male latch of a car mount or universal stand.\nIn some embodiments, apparatus 110 includes a function button 430 for enabling user 100 to provide input to apparatus 110.  Function button 430 may accept different types of tactile input (e.g., a tap, a click, a double-click, a long press, a\nright-to-left slide, a left-to-right slide).  In some embodiments, each type of input may be associated with a different action.  For example, a tap may be associated with the function of taking a picture, while a right-to-left slide may be associated\nwith the function of recording a video.\nThe example embodiments discussed above with respect to FIGS. 3A, 3B, 4A, and 4B are not limiting.  In some embodiments, apparatus 110 may be implemented in any suitable configuration for performing the disclosed methods.  For example, referring\nback to FIG. 2, the disclosed embodiments may implement an apparatus 110 according to any configuration including an image sensor 220 and a processor unit 210 to perform image analysis and for communicating with a feedback unit 230.\nFIG. 5A is a block diagram illustrating the components of apparatus 110 according to an example embodiment.  As shown in FIG. 5A, and as similarly discussed above, apparatus 110 includes an image sensor 220, a memory 550, a processor 210, a\nfeedback outputting unit 230, a wireless transceiver 530, and a mobile power source 520.  In other embodiments, apparatus 110 may also include buttons, other sensors such as a microphone, and inertial measurements devices such as accelerometers,\ngyroscopes, magnetometers, temperature sensors, color sensors, light sensors, etc. Apparatus 110 may further include a data port 570 and a power connection 510 with suitable interfaces for connecting with an external power source or an external device\n(not shown).\nProcessor 210, depicted in FIG. 5A, may include any suitable processing device.  The term \"processing device\" includes any physical device having an electric circuit that performs a logic operation on input or inputs.  For example, processing\ndevice may include one or more integrated circuits, microchips, microcontrollers, microprocessors, all or part of a central processing unit (CPU), graphics processing unit (GPU), digital signal processor (DSP), field-programmable gate array (FPGA), or\nother circuits suitable for executing instructions or performing logic operations.  The instructions executed by the processing device may, for example, be pre-loaded into a memory integrated with or embedded into the processing device or may be stored\nin a separate memory (e.g., memory 550).  Memory 550 may comprise a Random Access Memory (RAM), a Read-Only Memory (ROM), a hard disk, an optical disk, a magnetic medium, a flash memory, other permanent, fixed, or volatile memory, or any other mechanism\ncapable of storing instructions.\nAlthough, in the embodiment illustrated in FIG. 5A, apparatus 110 includes one processing device (e.g., processor 210), apparatus 110 may include more than one processing device.  Each processing device may have a similar construction, or the\nprocessing devices may be of differing constructions that are electrically connected or disconnected from each other.  For example, the processing devices may be separate circuits or integrated in a single circuit.  When more than one processing device\nis used, the processing devices may be configured to operate independently or collaboratively.  The processing devices may be coupled electrically, magnetically, optically, acoustically, mechanically or by other means that permit them to interact.\nIn some embodiments, processor 210 may process a plurality of images captured from the environment of user 100 to determine different parameters related to capturing subsequent images.  For example, processor 210 can determine, based on\ninformation derived from captured image data, a value for at least one of the following: an image resolution, a compression ratio, a cropping parameter, frame rate, a focus point, an exposure time, an aperture size, and a light sensitivity.  The\ndetermined value may be used in capturing at least one subsequent image.  Additionally, processor 210 can detect images including at least one hand-related trigger in the environment of the user and perform an action and/or provide an output of\ninformation to a user via feedback outputting unit 230.\nIn another embodiment, processor 210 can change the aiming direction of image sensor 220.  For example, when apparatus 110 is attached with clip 420, the aiming direction of image sensor 220 may not coincide with the field-of-view of user 100. \nProcessor 210 may recognize certain situations from the analyzed image data and adjust the aiming direction of image sensor 220 to capture relevant image data.  For example, in one embodiment, processor 210 may detect an interaction with another\nindividual and sense that the individual is not fully in view, because image sensor 220 is tilted down.  Responsive thereto, processor 210 may adjust the aiming direction of image sensor 220 to capture image data of the individual.  Other scenarios are\nalso contemplated where processor 210 may recognize the need to adjust an aiming direction of image sensor 220.\nIn some embodiments, processor 210 may communicate data to feedback-outputting unit 230, which may include any device configured to provide information to a user 100.  Feedback outputting unit 230 may be provided as part of apparatus 110 (as\nshown) or may be provided external to apparatus 110 and communicatively coupled thereto.  Feedback-outputting unit 230 may be configured to output visual or nonvisual feedback based on signals received from processor 210, such as when processor 210\nrecognizes a hand-related trigger in the analyzed image data.\nThe term \"feedback\" refers to any output or information provided in response to processing at least one image in an environment.  In some embodiments, as similarly described above, feedback may include an audible or visible indication of time\ninformation, detected text or numerals, the value of currency, a branded product, a person's identity, the identity of a landmark or other environmental situation or condition including the street names at an intersection or the color of a traffic light,\netc., as well as other information associated with each of these.  For example, in some embodiments, feedback may include additional information regarding the amount of currency still needed to complete a transaction, information regarding the identified\nperson, historical information or times and prices of admission etc. of a detected landmark etc. In some embodiments, feedback may include an audible tone, a tactile response, and/or information previously recorded by user 100.  Feedback-outputting unit\n230 may comprise appropriate components for outputting acoustical and tactile feedback.  For example, feedback-outputting unit 230 may comprise audio headphones, a hearing aid type device, a speaker, a bone conduction headphone, interfaces that provide\ntactile cues, vibrotactile stimulators, etc. In some embodiments, processor 210 may communicate signals with an external feedback outputting unit 230 via a wireless transceiver 530, a wired connection, or some other communication interface.  In some\nembodiments, feedback outputting unit 230 may also include any suitable display device for visually displaying information to user 100.\nAs shown in FIG. 5A, apparatus 110 includes memory 550.  Memory 550 may include one or more sets of instructions accessible to processor 210 to perform the disclosed methods, including instructions for recognizing a hand-related trigger in the\nimage data.  In some embodiments memory 550 may store image data (e.g., images, videos) captured from the environment of user 100.  In addition, memory 550 may store information specific to user 100, such as image representations of known individuals,\nfavorite products, personal items, and calendar or appointment information, etc. In some embodiments, processor 210 may determine, for example, which type of image data to store based on available storage space in memory 550.  In another embodiment,\nprocessor 210 may extract information from the image data stored in memory 550.\nAs further shown in FIG. 5A, apparatus 110 includes mobile power source 520.  The term \"mobile power source\" includes any device capable of providing electrical power, which can be easily carried by hand (e.g., mobile power source 520 may weigh\nless than a pound).  The mobility of the power source enables user 100 to use apparatus 110 in a variety of situations.  In some embodiments, mobile power source 520 may include one or more batteries (e.g., nickel-cadmium batteries, nickel-metal hydride\nbatteries, and lithium-ion batteries) or any other type of electrical power supply.  In other embodiments, mobile power source 520 may be rechargeable and contained within a casing that holds apparatus 110.  In yet other embodiments, mobile power source\n520 may include one or more energy harvesting devices for converting ambient energy into electrical energy (e.g., portable solar power units, human vibration units, etc.).\nMobile power source 520 may power one or more wireless transceivers (e.g., wireless transceiver 530 in FIG. 5A).  The term \"wireless transceiver\" refers to any device configured to exchange transmissions over an air interface by use of radio\nfrequency, infrared frequency, magnetic field, or electric field.  Wireless transceiver 530 may use any known standard to transmit and/or receive data (e.g., Wi-Fi, Bluetooth.RTM., Bluetooth Smart, 802.15.4, or ZigBee).  In some embodiments, wireless\ntransceiver 530 may transmit data (e.g., raw image data, processed image data, extracted information) from apparatus 110 to computing device 120 and/or server 250.  Wireless transceiver 530 may also receive data from computing device 120 and/or server\n250.  In other embodiments, wireless transceiver 530 may transmit data and instructions to an external feedback outputting unit 230.\nFIG. 5B is a block diagram illustrating the components of apparatus 110 according to another example embodiment.  In some embodiments, apparatus 110 includes a first image sensor 220a, a second image sensor 220b, a memory 550, a first processor\n210a, a second processor 210b, a feedback outputting unit 230, a wireless transceiver 530, a mobile power source 520, and a power connector 510.  In the arrangement shown in FIG. 5B, each of the image sensors may provide images in a different image\nresolution, or face a different direction.  Alternatively, each image sensor may be associated with a different camera (e.g., a wide angle camera, a narrow angle camera, an IR camera, etc.).  In some embodiments, apparatus 110 can select which image\nsensor to use based on various factors.  For example, processor 210a may determine, based on available storage space in memory 550, to capture subsequent images in a certain resolution.\nApparatus 110 may operate in a first processing-mode and in a second processing-mode, such that the first processing-mode may consume less power than the second processing-mode.  For example, in the first processing-mode, apparatus 110 may\ncapture images and process the captured images to make real-time decisions based on an identifying hand-related trigger, for example.  In the second processing-mode, apparatus 110 may extract information from stored images in memory 550 and delete images\nfrom memory 550.  In some embodiments, mobile power source 520 may provide more than fifteen hours of processing in the first processing-mode and about three hours of processing in the second processing-mode.  Accordingly, different processing-modes may\nallow mobile power source 520 to produce sufficient power for powering apparatus 110 for various time periods (e.g., more than two hours, more than four hours, more than ten hours, etc.).\nIn some embodiments, apparatus 110 may use first processor 210a in the first processing-mode when powered by mobile power source 520, and second processor 210b in the second processing-mode when powered by external power source 580 that is\nconnectable via power connector 510.  In other embodiments, apparatus 110 may determine, based on predefined conditions, which processors or which processing modes to use.  Apparatus 110 may operate in the second processing-mode even when apparatus 110\nis not powered by external power source 580.  For example, apparatus 110 may determine that it should operate in the second processing-mode when apparatus 110 is not powered by external power source 580, if the available storage space in memory 550 for\nstoring new image data is lower than a predefined threshold.\nAlthough one wireless transceiver is depicted in FIG. 5B, apparatus 110 may include more than one wireless transceiver (e.g., two wireless transceivers).  In an arrangement with more than one wireless transceiver, each of the wireless\ntransceivers may use a different standard to transmit and/or receive data.  In some embodiments, a first wireless transceiver may communicate with server 250 or computing device 120 using a cellular standard (e.g., LTE or GSM), and a second wireless\ntransceiver may communicate with server 250 or computing device 120 using a short-range standard (e.g., Wi-Fi or Bluetooth.RTM.).  In some embodiments, apparatus 110 may use the first wireless transceiver when the wearable apparatus is powered by a\nmobile power source included in the wearable apparatus, and use the second wireless transceiver when the wearable apparatus is powered by an external power source.\nFIG. 5C is a block diagram illustrating the components of apparatus 110 according to another example embodiment including computing device 120.  In this embodiment, apparatus 110 includes an image sensor 220, a memory 550a, a first processor\n210, a feedback-outputting unit 230, a wireless transceiver 530a, a mobile power source 520, and a power connector 510.  As further shown in FIG. 5C, computing device 120 includes a processor 540, a feedback-outputting unit 545, a memory 550b, a wireless\ntransceiver 530b, and a display 260.  One example of computing device 120 is a smartphone or tablet having a dedicated application installed therein.  In other embodiments, computing device 120 may include any configuration such as an on-board automobile\ncomputing system, a PC, a laptop, and any other system consistent with the disclosed embodiments.  In this example, user 100 may view feedback output in response to identification of a hand-related trigger on display 260.  Additionally, user 100 may view\nother data (e.g., images, video clips, object information, schedule information, extracted information, etc.) on display 260.  In addition, user 100 may communicate with server 250 via computing device 120.\nIn some embodiments, processor 210 and processor 540 are configured to extract information from captured image data.  The term \"extracting information\" includes any process by which information associated with objects, individuals, locations,\nevents, etc., is identified in the captured image data by any means known to those of ordinary skill in the art.  In some embodiments, apparatus 110 may use the extracted information to send feedback or other real-time indications to feedback outputting\nunit 230 or to computing device 120.  In some embodiments, processor 210 may identify in the image data the individual standing in front of user 100, and send computing device 120 the name of the individual and the last time user 100 met the individual. \nIn another embodiment, processor 210 may identify in the image data, one or more visible triggers, including a hand-related trigger, and determine whether the trigger is associated with a person other than the user of the wearable apparatus to\nselectively determine whether to perform an action associated with the trigger.  One such action may be to provide a feedback to user 100 via feedback-outputting unit 230 provided as part of (or in communication with) apparatus 110 or via a feedback unit\n545 provided as part of computing device 120.  For example, feedback-outputting unit 545 may be in communication with display 260 to cause the display 260 to visibly output information.  In some embodiments, processor 210 may identify in the image data a\nhand-related trigger and send computing device 120 an indication of the trigger.  Processor 540 may then process the received trigger information and provide an output via feedback outputting unit 545 or display 260 based on the hand-related trigger.  In\nother embodiments, processor 540 may determine a hand-related trigger and provide suitable feedback similar to the above, based on image data received from apparatus 110.  In some embodiments, processor 540 may provide instructions or other information,\nsuch as environmental information to apparatus 110 based on an identified hand-related trigger.\nIn some embodiments, processor 210 may identify other environmental information in the analyzed images, such as an individual standing in front user 100, and send computing device 120 information related to the analyzed information such as the\nname of the individual and the last time user 100 met the individual.  In a different embodiment, processor 540 may extract statistical information from captured image data and forward the statistical information to server 250.  For example, certain\ninformation regarding the types of items a user purchases, or the frequency a user patronizes a particular merchant, etc. may be determined by processor 540.  Based on this information, server 250 may send computing device 120 coupons and discounts\nassociated with the user's preferences.\nWhen apparatus 110 is connected or wirelessly connected to computing device 120, apparatus 110 may transmit at least part of the image data stored in memory 550a for storage in memory 550b.  In some embodiments, after computing device 120\nconfirms that transferring the part of image data was successful, processor 540 may delete the part of the image data.  The term \"delete\" means that the image is marked as `deleted` and other image data may be stored instead of it, but does not\nnecessarily mean that the image data was physically removed from the memory.\nAs will be appreciated by a person skilled in the art having the benefit of this disclosure, numerous variations and/or modifications may be made to the disclosed embodiments.  Not all components are essential for the operation of apparatus 110. Any component may be located in any appropriate apparatus and the components may be rearranged into a variety of configurations while providing the functionality of the disclosed embodiments.  For example, in some embodiments, apparatus 110 may include a\ncamera, a processor, and a wireless transceiver for sending data to another device.  Therefore, the foregoing configurations are examples and, regardless of the configurations discussed above, apparatus 110 can capture, store, and/or process images.\nFurther, the foregoing and following description refers to storing and/or processing images or image data.  In the embodiments disclosed herein, the stored and/or processed images or image data may comprise a representation of one or more images\ncaptured by image sensor 220.  As the term is used herein, a \"representation\" of an image (or image data) may include an entire image or a portion of an image.  A representation of an image (or image data) may have the same resolution or a lower\nresolution as the image (or image data), and/or a representation of an image (or image data) may be altered in some respect (e.g., be compressed, have a lower resolution, have one or more colors that are altered, etc.).\nFor example, apparatus 110 may capture an image and store a representation of the image that is compressed as a .JPG file.  As another example, apparatus 110 may capture an image in color, but store a black-and-white representation of the color\nimage.  As yet another example, apparatus 110 may capture an image and store a different representation of the image (e.g., a portion of the image).  For example, apparatus 110 may store a portion of an image that includes a face of a person who appears\nin the image, but that does not substantially include the environment surrounding the person.  Similarly, apparatus 110 may, for example, store a portion of an image that includes a product that appears in the image, but does not substantially include\nthe environment surrounding the product.  As yet another example, apparatus 110 may store a representation of an image at a reduced resolution (i.e., at a resolution that is of a lower value than that of the captured image).  Storing representations of\nimages may allow apparatus 110 to save storage space in memory 550.  Furthermore, processing representations of images may allow apparatus 110 to improve processing efficiency and/or help to preserve battery life.\nIn addition to the above, in some embodiments, any one of apparatus 110 or computing device 120, via processor 210 or 540, may further process the captured image data to provide additional functionality to recognize objects and/or gestures\nand/or other information in the captured image data.  In some embodiments, actions may be taken based on the identified objects, gestures, or other information.  In some embodiments, processor 210 or 540 may identify in the image data, one or more\nvisible triggers, including a hand-related trigger, and determine whether the trigger is associated with a person other than the user to determine whether to perform an action associated with the trigger.\nSome embodiments of the present disclosure may include an apparatus securable to an article of clothing of a user.  Such an apparatus may include two portions, connectable by a connector.  A capturing unit may be designed to be worn on the\noutside of a user's clothing, and may include an image sensor for capturing images of a user's environment.  The capturing unit may be connected to or connectable to a power unit, which may be configured to house a power source and a processing device. \nThe capturing unit may be a small device including a camera or other device for capturing images.  The capturing unit may be designed to be inconspicuous and unobtrusive, and may be configured to communicate with a power unit concealed by a user's\nclothing.  The power unit may include bulkier aspects of the system, such as transceiver antennas, at least one battery, a processing device, etc. In some embodiments, communication between the capturing unit and the power unit may be provided by a data\ncable included in the connector, while in other embodiments, communication may be wirelessly achieved between the capturing unit and the power unit.  Some embodiments may permit alteration of the orientation of an image sensor of the capture unit, for\nexample to better capture images of interest.\nFIG. 6 illustrates an exemplary embodiment of a memory containing software modules consistent with the present disclosure.  Included in memory 550 are orientation identification module 601, orientation adjustment module 602, and motion tracking\nmodule 603.  Modules 601, 602, 603 may contain software instructions for execution by at least one processing device, e.g., processor 210, included with a wearable apparatus.  Orientation identification module 601, orientation adjustment module 602, and\nmotion tracking module 603 may cooperate to provide orientation adjustment for a capturing unit incorporated into wireless apparatus 110.\nFIG. 7 illustrates an exemplary capturing unit 710 including an orientation adjustment unit 705.  Orientation adjustment unit 705 may be configured to permit the adjustment of image sensor 220.  As illustrated in FIG. 7, orientation adjustment\nunit 705 may include an eye-ball type adjustment mechanism.  In alternative embodiments, orientation adjustment unit 705 may include gimbals, adjustable stalks, pivotable mounts, and any other suitable unit for adjusting an orientation of image sensor\n220.\nImage sensor 220 may be configured to be movable with the head of user 100 in such a manner that an aiming direction of image sensor 220 substantially coincides with a field of view of user 100.  For example, as described above, a camera\nassociated with image sensor 220 may be installed within capturing unit 710 at a predetermined angle in a position facing slightly upwards or downwards, depending on an intended location of capturing unit 710.  Accordingly, the set aiming direction of\nimage sensor 220 may match the field-of-view of user 100.  In some embodiments, processor 210 may change the orientation of image sensor 220 using image data provided from image sensor 220.  For example, processor 210 may recognize that a user is reading\na book and determine that the aiming direction of image sensor 220 is offset from the text.  That is, because the words in the beginning of each line of text are not fully in view, processor 210 may determine that image sensor 220 is tilted in the wrong\ndirection.  Responsive thereto, processor 210 may adjust the aiming direction of image sensor 220.\nOrientation identification module 601 may be configured to identify an orientation of an image sensor 220 of capturing unit 710.  An orientation of an image sensor 220 may be identified, for example, by analysis of images captured by image\nsensor 220 of capturing unit 710, by tilt or attitude sensing devices within capturing unit 710, and by measuring a relative direction of orientation adjustment unit 705 with respect to the remainder of capturing unit 710.\nOrientation adjustment module 602 may be configured to adjust an orientation of image sensor 220 of capturing unit 710.  As discussed above, image sensor 220 may be mounted on an orientation adjustment unit 705 configured for movement. \nOrientation adjustment unit 705 may be configured for rotational and/or lateral movement in response to commands from orientation adjustment module 602.  In some embodiments orientation adjustment unit 705 may be adjust an orientation of image sensor 220\nvia motors, electromagnets, permanent magnets, and/or any suitable combination thereof.\nIn some embodiments, monitoring module 603 may be provided for continuous monitoring.  Such continuous monitoring may include tracking a movement of at least a portion of an object included in one or more images captured by the image sensor. \nFor example, in one embodiment, apparatus 110 may track an object as long as the object remains substantially within the field-of-view of image sensor 220.  In additional embodiments, monitoring module 603 may engage orientation adjustment module 602 to\ninstruct orientation adjustment unit 705 to continually orient image sensor 220 towards an object of interest.  For example, in one embodiment, monitoring module 603 may cause image sensor 220 to adjust an orientation to ensure that a certain designated\nobject, for example, the face of a particular person, remains within the field-of view of image sensor 220, even as that designated object moves about.  In another embodiment, monitoring module 603 may continuously monitor an area of interest included in\none or more images captured by the image sensor.  For example, a user may be occupied by a certain task, for example, typing on a laptop, while image sensor 220 remains oriented in a particular direction and continuously monitors a portion of each image\nfrom a series of images to detect a trigger or other event.  For example, image sensor 210 may be oriented towards a piece of laboratory equipment and monitoring module 603 may be configured to monitor a status light on the laboratory equipment for a\nchange in status, while the user's attention is otherwise occupied.\nIn some embodiments consistent with the present disclosure, capturing unit 710 may include a plurality of image sensors 220.  The plurality of image sensors 220 may each be configured to capture different image data.  For example, when a\nplurality of image sensors 220 are provided, the image sensors 220 may capture images having different resolutions, may capture wider or narrower fields of view, and may have different levels of magnification.  Image sensors 220 may be provided with\nvarying lenses to permit these different configurations.  In some embodiments, a plurality of image sensors 220 may include image sensors 220 having different orientations.  Thus, each of the plurality of image sensors 220 may be pointed in a different\ndirection to capture different images.  The fields of view of image sensors 220 may be overlapping in some embodiments.  The plurality of image sensors 220 may each be configured for orientation adjustment, for example, by being paired with an image\nadjustment unit 705.  In some embodiments, monitoring module 603, or another module associated with memory 550, may be configured to individually adjust the orientations of the plurality of image sensors 220 as well as to turn each of the plurality of\nimage sensors 220 on or off as may be required.  In some embodiments, monitoring an object or person captured by an image sensor 220 may include tracking movement of the object across the fields of view of the plurality of image sensors 220.\nEmbodiments consistent with the present disclosure may include connectors configured to connect a capturing unit and a power unit of a wearable apparatus.  Capturing units consistent with the present disclosure may include least one image sensor\nconfigured to capture images of an environment of a user.  Power units consistent with the present disclosure may be configured to house a power source and/or at least one processing device.  Connectors consistent with the present disclosure may be\nconfigured to connect the capturing unit and the power unit, and may be configured to secure the apparatus to an article of clothing such that the capturing unit is positioned over an outer surface of the article of clothing and the power unit is\npositioned under an inner surface of the article of clothing.  Exemplary embodiments of capturing units, connectors, and power units consistent with the disclosure are discussed in further detail with respect to FIGS. 8-14.\nFIG. 8 is a schematic illustration of an embodiment of wearable apparatus 110 securable to an article of clothing consistent with the present disclosure.  As illustrated in FIG. 8, capturing unit 710 and power unit 720 may be connected by a\nconnector 730 such that capturing unit 710 is positioned on one side of an article of clothing 750 and power unit 720 is positioned on the opposite side of the clothing 750.  In some embodiments, capturing unit 710 may be positioned over an outer surface\nof the article of clothing 750 and power unit 720 may be located under an inner surface of the article of clothing 750.  The power unit 720 may be configured to be placed against the skin of a user.\nCapturing unit 710 may include an image sensor 220 and an orientation adjustment unit 705 (as illustrated in FIG. 7).  Power unit 720 may include mobile power source 520 and processor 210.  Power unit 720 may further include any combination of\nelements previously discussed that may be a part of wearable apparatus 110, including, but not limited to, wireless transceiver 530, feedback outputting unit 230, memory 550, and data port 570.\nConnector 730 may include a clip 715 or other mechanical connection designed to clip or attach capturing unit 710 and power unit 720 to an article of clothing 750 as illustrated in FIG. 8.  As illustrated, clip 715 may connect to each of\ncapturing unit 710 and power unit 720 at a perimeter thereof, and may wrap around an edge of the article of clothing 750 to affix the capturing unit 710 and power unit 720 in place.  Connector 730 may further include a power cable 760 and a data cable\n770.  Power cable 760 may be capable of conveying power from mobile power source 520 to image sensor 220 of capturing unit 710.  Power cable 760 may also be configured to provide power to any other elements of capturing unit 710, e.g., orientation\nadjustment unit 705.  Data cable 770 may be capable of conveying captured image data from image sensor 220 in capturing unit 710 to processor 800 in the power unit 720.  Data cable 770 may be further capable of conveying additional data between capturing\nunit 710 and processor 800, e.g., control instructions for orientation adjustment unit 705.\nFIG. 9 is a schematic illustration of a user 100 wearing a wearable apparatus 110 consistent with an embodiment of the present disclosure.  As illustrated in FIG. 9, capturing unit 710 is located on an exterior surface of the clothing 750 of\nuser 100.  Capturing unit 710 is connected to power unit 720 (not seen in this illustration) via connector 730, which wraps around an edge of clothing 750.\nIn some embodiments, connector 730 may include a flexible printed circuit board (PCB).  FIG. 10 illustrates an exemplary embodiment wherein connector 730 includes a flexible printed circuit board 765.  Flexible printed circuit board 765 may\ninclude data connections and power connections between capturing unit 710 and power unit 720.  Thus, in some embodiments, flexible printed circuit board 765 may serve to replace power cable 760 and data cable 770.  In alternative embodiments, flexible\nprinted circuit board 765 may be included in addition to at least one of power cable 760 and data cable 770.  In various embodiments discussed herein, flexible printed circuit board 765 may be substituted for, or included in addition to, power cable 760\nand data cable 770.\nFIG. 11 is a schematic illustration of another embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.  As illustrated in FIG. 11, connector 730 may be centrally located with respect to\ncapturing unit 710 and power unit 720.  Central location of connector 730 may facilitate affixing apparatus 110 to clothing 750 through a hole in clothing 750 such as, for example, a button-hole in an existing article of clothing 750 or a specialty hole\nin an article of clothing 750 designed to accommodate wearable apparatus 110.\nFIG. 12 is a schematic illustration of still another embodiment of wearable apparatus 110 securable to an article of clothing.  As illustrated in FIG. 12, connector 730 may include a first magnet 731 and a second magnet 732.  First magnet 731\nand second magnet 732 may secure capturing unit 710 to power unit 720 with the article of clothing positioned between first magnet 731 and second magnet 732.  In embodiments including first magnet 731 and second magnet 732, power cable 760 and data cable\n770 may also be included.  In these embodiments, power cable 760 and data cable 770 may be of any length, and may provide a flexible power and data connection between capturing unit 710 and power unit 720.  Embodiments including first magnet 731 and\nsecond magnet 732 may further include a flexible PCB 765 connection in addition to or instead of power cable 760 and/or data cable 770.  In some embodiments, first magnet 731 or second magnet 732 may be replaced by an object comprising a metal material.\nFIG. 13 is a schematic illustration of yet another embodiment of a wearable apparatus 110 securable to an article of clothing.  FIG. 13 illustrates an embodiment wherein power and data may be wirelessly transferred between capturing unit 710 and\npower unit 720.  As illustrated in FIG. 13, first magnet 731 and second magnet 732 may be provided as connector 730 to secure capturing unit 710 and power unit 720 to an article of clothing 750.  Power and/or data may be transferred between capturing\nunit 710 and power unit 720 via any suitable wireless technology, for example, magnetic and/or capacitive coupling, near field communication technologies, radiofrequency transfer, and any other wireless technology suitable for transferring data and/or\npower across short distances.\nFIG. 14 illustrates still another embodiment of wearable apparatus 110 securable to an article of clothing 750 of a user.  As illustrated in FIG. 14, connector 730 may include features designed for a contact fit.  For example, capturing unit 710\nmay include a ring 733 with a hollow center having a diameter slightly larger than a disk-shaped protrusion 734 located on power unit 720.  When pressed together with fabric of an article of clothing 750 between them, disk-shaped protrusion 734 may fit\ntightly inside ring 733, securing capturing unit 710 to power unit 720.  FIG. 14 illustrates an embodiment that does not include any cabling or other physical connection between capturing unit 710 and power unit 720.  In this embodiment, capturing unit\n710 and power unit 720 may transfer power and data wirelessly.  In alternative embodiments, capturing unit 710 and power unit 720 may transfer power and data via at least one of cable 760, data cable 770, and flexible printed circuit board 765.\nFIG. 15 illustrates another aspect of power unit 720 consistent with embodiments described herein.  Power unit 720 may be configured to be positioned directly against the user's skin.  To facilitate such positioning, power unit 720 may further\ninclude at least one surface coated with a biocompatible material 740.  Biocompatible materials 740 may include materials that will not negatively react with the skin of the user when worn against the skin for extended periods of time.  Such materials\nmay include, for example, silicone, PTFE, kapton, polyimide, titanium, nitinol, platinum, and others.  Also as illustrated in FIG. 15, power unit 720 may be sized such that an inner volume of the power unit is substantially filled by mobile power source\n520.  That is, in some embodiments, the inner volume of power unit 720 may be such that the volume does not accommodate any additional components except for mobile power source 520.  In some embodiments, mobile power source 520 may take advantage of its\nclose proximity to the skin of user's skin.  For example, mobile power source 520 may use the Peltier effect to produce power and/or charge the power source.\nIn further embodiments, an apparatus securable to an article of clothing may further include protective circuitry associated with power source 520 housed in in power unit 720.  FIG. 16 illustrates an exemplary embodiment including protective\ncircuitry 775.  As illustrated in FIG. 16, protective circuitry 775 may be located remotely with respect to power unit 720.  In alternative embodiments, protective circuitry 775 may also be located in capturing unit 710, on flexible printed circuit board\n765, or in power unit 720.\nProtective circuitry 775 may be configured to protect image sensor 220 and/or other elements of capturing unit 710 from potentially dangerous currents and/or voltages produced by mobile power source 520.  Protective circuitry 775 may include\npassive components such as capacitors, resistors, diodes, inductors, etc., to provide protection to elements of capturing unit 710.  In some embodiments, protective circuitry 775 may also include active components, such as transistors, to provide\nprotection to elements of capturing unit 710.  For example, in some embodiments, protective circuitry 775 may comprise one or more resistors serving as fuses.  Each fuse may comprise a wire or strip that melts (thereby braking a connection between\ncircuitry of image capturing unit 710 and circuitry of power unit 720) when current flowing through the fuse exceeds a predetermined limit (e.g., 500 milliamps, 900 milliamps, 1 amp, 1.1 amps, 2 amp, 2.1 amps, 3 amps, etc.) Any or all of the previously\ndescribed embodiments may incorporate protective circuitry 775.\nIn some embodiments, the wearable apparatus may transmit data to a computing device (e.g., a smartphone, tablet, watch, computer, etc.) over one or more networks via any known wireless standard (e.g., cellular, Wi-Fi, Bluetooth.RTM., etc.), or\nvia near-filed capacitive coupling, other short range wireless techniques, or via a wired connection.  Similarly, the wearable apparatus may receive data from the computing device over one or more networks via any known wireless standard (e.g., cellular,\nWi-Fi, Bluetooth.RTM., etc.), or via near-filed capacitive coupling, other short range wireless techniques, or via a wired connection.  The data transmitted to the wearable apparatus and/or received by the wireless apparatus may include images, portions\nof images, identifiers related to information appearing in analyzed images or associated with analyzed audio, or any other data representing image and/or audio data.  For example, an image may be analyzed and an identifier related to an activity\noccurring in the image may be transmitted to the computing device (e.g., the \"paired device\").  In the embodiments described herein, the wearable apparatus may process images and/or audio locally (on board the wearable apparatus) and/or remotely (via a\ncomputing device).  Further, in the embodiments described herein, the wearable apparatus may transmit data related to the analysis of images and/or audio to a computing device for further analysis, display, and/or transmission to another device (e.g., a\npaired device).  Further, a paired device may execute one or more applications (apps) to process, display, and/or analyze data (e.g., identifiers, text, images, audio, etc.) received from the wearable apparatus.\nSome of the disclosed embodiments may involve systems, devices, methods, and software products for determining at least one keyword.  For example, at least one keyword may be determined based on data collected by apparatus 110.  At least one\nsearch query may be determined based on the at least one keyword.  The at least one search query may be transmitted to a search engine.\nIn some embodiments, at least one keyword may be determined based on at least one or more images captured by image sensor 220.  In some cases, the at least one keyword may be selected from a keywords pool stored in memory.  In some cases,\noptical character recognition (OCR) may be performed on at least one image captured by image sensor 220, and the at least one keyword may be determined based on the OCR result.  In some cases, at least one image captured by image sensor 220 may be\nanalyzed to recognize: a person, an object, a location, a scene, and so forth.  Further, the at least one keyword may be determined based on the recognized person, object, location, scene, etc. For example, the at least one keyword may comprise: a\nperson's name, an object's name, a place's name, a date, a sport team's name, a movie's name, a book's name, and so forth.\nIn some embodiments, at least one keyword may be determined based on the user's behavior.  The user's behavior may be determined based on an analysis of the one or more images captured by image sensor 220.  In some embodiments, at least one\nkeyword may be determined based on activities of a user and/or other person.  The one or more images captured by image sensor 220 may be analyzed to identify the activities of the user and/or the other person who appears in one or more images captured by\nimage sensor 220.  In some embodiments, at least one keyword may be determined based on at least one or more audio segments captured by apparatus 110.  In some embodiments, at least one keyword may be determined based on at least GPS information\nassociated with the user.  In some embodiments, at least one keyword may be determined based on at least the current time and/or date.\nIn some embodiments, at least one search query may be determined based on at least one keyword.  In some cases, the at least one search query may comprise the at least one keyword.  In some cases, the at least one search query may comprise the\nat least one keyword and additional keywords provided by the user.  In some cases, the at least one search query may comprise the at least one keyword and one or more images, such as images captured by image sensor 220.  In some cases, the at least one\nsearch query may comprise the at least one keyword and one or more audio segments, such as audio segments captured by apparatus 110.\nIn some embodiments, the at least one search query may be transmitted to a search engine.  In some embodiments, search results provided by the search engine in response to the at least one search query may be provided to the user.  In some\nembodiments, the at least one search query may be used to access a database.\nFor example, in one embodiment, the keywords may include a name of a type of food, such as quinoa, or a brand name of a food product; and the search will output information related to desirable quantities of consumption, facts about the\nnutritional profile, and so forth.  In another example, in one embodiment, the keywords may include a name of a restaurant, and the search will output information related to the restaurant, such as a menu, opening hours, reviews, and so forth.  The name\nof the restaurant may be obtained using OCR on an image of signage, using GPS information, and so forth.  In another example, in one embodiment, the keywords may include a name of a person, and the search will provide information from a social network\nprofile of the person.  The name of the person may be obtained using OCR on an image of a name tag attached to the person's shirt, using face recognition algorithms, and so forth.  In another example, in one embodiment, the keywords may include a name of\na book, and the search will output information related to the book, such as reviews, sales statistics, information regarding the author of the book, and so forth.  In another example, in one embodiment, the keywords may include a name of a movie, and the\nsearch will output information related to the movie, such as reviews, box office statistics, information regarding the cast of the movie, show times, and so forth.  In another example, in one embodiment, the keywords may include a name of a sport team,\nand the search will output information related to the sport team, such as statistics, latest results, future schedule, information regarding the players of the sport team, and so forth.  For example, the name of the sport team may be obtained using audio\nrecognition algorithms.\nSelecting Actions Based on a Detected Person\nIn some embodiments, wearable apparatus 110 may execute a variety of actions, such as identifying persons in captured images, uploading images of persons (e.g., to one or more social networks, to one or more cloud storage folders, etc.), tagging\nimages of persons, sending images of persons (e.g., via email, text message, or the like), updating a Gnatt chart or a calendar, sending information to one or more matchmaking services, updating one or more social network profiles, providing one or more\nstatistics, or the like.  The wearable apparatus 110 may select one or more actions to perform based on one or more attributes of a detected person, such as age, gender, weight, height, relationship with a wearer of the device (e.g., social, family,\nbusiness, etc.).  In doing so, embodiments consistent with the present disclosure may address the technical problem of extracting information from an environment of the wearer of the wearable apparatus that is relevant to the wearer and then determining\nhow to use that information in a way that is useful to the wearer and/or according to the user's preferences.  For example, the wearer may wish to track encounters with certain people who are related to them or persons with whom the wearer is associated\nwith at work, but may have different preferences as to the kinds of information that the wearer would like to store regarding different persons.  Embodiments of the present disclosure may address this problem through techniques for categorizing\ninformation extracted or determined from images of the wearer's environment and executing appropriate actions related to the extracted information.\nFIG. 17 illustrates an exemplary embodiment of a memory 1700 containing software modules consistent with the present disclosure.  Memory 1700 may be included in apparatus 110 in lieu of or in combination with memory 550.  In some embodiments,\nthe software modules of memory 1700 may be combined with one or more software modules of memory 550 into one or more memories.  Memory 1700 may store more or fewer modules than those shown in FIG. 17.\nAs illustrated in FIG. 17, included in memory 1700 are software instructions to execute a person detection module 1701, an attribute identification module 1702, a categorization module 1703, and an action module 1704.  Modules 1701, 1702, 1703,\nand 1704 may contain software instructions for execution by at least one processing device, e.g., processor 210, included in a wearable apparatus, e.g., wearable apparatus 110.  In some embodiments, person detection module 1701, attribute identification\nmodule 1702, categorization module 1703, and action module 1704 may cooperate to execute method 1900 (or a variant thereof) of FIG. 19.\nPerson detection module 1701 may be configured to analyze one or more images captured from a wearable apparatus to detect at least one person within the images.  For example, person detection module 1701 may be configured to identify a subset of\nthe captured data that includes at least one person.  In some embodiments, person detection module 1701 may be configured to receive a plurality of images and to identify one or more of the plurality of images that include at least one person.  For\nexample, module 1701 may receive a plurality of images of an environment surrounding a user wearing the wearable apparatus 110 and identify which of the plurality of images include at least one person.\nIn some embodiments, such an analysis may be performed by employing a facial recognition algorithm designed to detect facial features (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person. \nIn other embodiments, the at least one person may be identified using a thermal signature algorithm designed to detect the presence of at least one person based on the heat generated by the at least one person.  In such embodiments, the wearable\napparatus 110 may capture thermal images, either alone or in combination with visual images, for processing by the thermal signature algorithm.  Thermal recognition of the at least one person may be desirable in implementations in which the wearable\ndevice 110 is operating in reduced lighting situations.  In some embodiments, the at least one person may be identified through the application of one or more image classification techniques.  For example, at least one image classification technique may\nbe used to classify at least one feature of an image.  In some embodiments, an image classification technique may include one or more of image enhancement, edge detection, image analysis, and data extraction.  Specific examples of the methods for\nidentifying at least one person are exemplary only, and a person of ordinary skill in the art will recognize other methods for identifying the at least one person that remain consistent with the present disclosure.  In some examples, the at least one\nperson may be detected using a facial detection algorithm, using a neural network trained to detect faces and/or persons in images, and so forth.  In some examples, the at least one person may be identified using a facial recognition algorithm, using a\nneural network trained to identify people in images, and so forth.\nAttribute identification module 1702 may be configured to receive one or more images of detected persons and further analyze the one or more images to determine one or more attributes (e.g., age, gender, weight, height, relationship with a\nwearer of the wearable apparatus) associated with the identified persons.  In some embodiments, attribute identification module 1702 may determine more than one attribute for each detected person.\nIn some embodiments, at least one attribute of a detected person may be determined based on analysis of one or more images of the detected person.  For example, one or more algorithms may analyze one or more of detected facial features (e.g.,\nmouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person to estimate or determine information such as a the detected person's age, gender, weight, height, etc. For example, a neural network trained to\nidentify attributes of a person from an image may be used.\nIn some embodiments, attribute identification module 1702 may determine an identity of a detected person (e.g., through facial recognition), and then access one or more databases (stored, e.g., locally in a memory of wearable apparatus 110\nand/or accessible remotely over a network, e.g., such as by accessing server 250) to retrieve at least one attribute of the detected person.  For example, after identifying a detected person as a particular individual (e.g., determining a match based on\nfacial recognition to a known person's image), attribute identification module 1702 may access a database to retrieve information about the detected person, such as the detected person's age, family members, etc.\nCategorization module 1703 may be configured to use associated attributes to categorize identified persons.  For example, categorization module 1703 may classify a detected person as an adult, a teenager, a child, a brother of the wearer, a\nmother-in-law of the wearer, a tall (e.g., above median) person, a short (e.g., below median) person, a male, a female, or the like.\nIn some embodiments, based on an attribute of a particular individual, categorization module 1703 may associate the detected person with one or more relevant categories.  For example, if an attribute of a person indicates he or she is more than\na predetermined height (e.g., six feet or taller), categorization module 1703 may associate the detected person a category of \"tall.\" Similarly, if an attribute of a person indicates he or she is less than a predetermined age (e.g., 18 years),\ncategorization module 1703 may associate the detected person a category of \"child.\" Categorization module 1703 may further associate more than one category with a detected person.  For example, a detected person who has an attribute of an age of fifteen\nyears old may be associated with both a category of \"child\" and a category of \"teenager.\"\nIn some embodiments, categorization module 1703 may be configured to receive one or more images of detected persons and further analyze the one or more images to categorize identified persons.  In some examples, the detected person may be\nclassified to one of a number of predefined categorizes using an image classifier that assign a category to a person based on images of a person.  The image classifier may be a result of a machine learning algorithm trained on a set of examples, where an\nexample may include images of a person along with the desired category for the person.  In some examples, a neural network trained to assign one or more categories to a person based on images of a person may be used.\nIn some embodiments, categorization module 1703 may be configured to receive one or more images and analyze the one or more images to detect persons of a select category.  In some examples, the one or more images may be analyzed using a detector\nconfigured to detect females, males, children, elderly persons, business persons, and so forth.  For example, the detector may comprise a classifier that classifies images and/or portion of images as ones that contain and ones that do not contain a\nperson matching the selected category.  The classifier may be a result of a machine learning algorithm trained using training examples, where a training example may comprise images and a desired answer.  For example, the detector may comprise a neural\nnetwork trained to detect persons that match the selected category in images.\nAction module 1704 may be configured to select one or more actions based on the categories.  For example, action module 1704 may identify the detected person, when the detected person is categorized as business contact of the wearer.  By way of\nfurther example, action module 1704 may upload and tag an image (e.g., to a cloud storage service, a social network, etc.) when, for example, the detected person is categorized as a friend or family of the wearer.  In another example, action module 1704\nmay update a timeline, calendar, Gnatt chart, or the like, when the detected person is categorized as a coworker or significant other of the wearer.  In yet another example, action module 1704 may provide information related to a potential dating match,\ne.g., when the detected person is categorized in one or more categories matching dating preferences of the wearer (e.g., gender, height, hair color, etc.).\nAlternatively or concurrently to performing one or more actions such as those above, action module 1704 may update one or more statistics based on the categorization.  In some embodiments, the statistics may be accumulated over geographical\nregions and/or time frames.\nIn some embodiments, action module 1704 may cause the selected one or more actions to be executed by providing information (e.g., one or more instructions and/or one or more parameters to an instruction) to a device paired with the wearable\napparatus (e.g., a smartphone) and/or by providing information (e.g., one more instructions and/or one or more parameters to an instruction) to a remote server (e.g., server 250) over a network.  For example, action module 1704 may transmit one or more\ninstructions to a paired device to update a calendar displayed on a screen of the paired device to indicate that the user had a meeting with a co-worker on a particular day.\nModules 1701-1704 may be implemented in software, hardware, firmware, a mix of any of those, or the like.  For example, if the modules are implemented in software, they may be stored, for example, in memory 550.  However, in some embodiments,\nany one or more of modules 1701-1704 may, for example, be stored in processor 540 and/or located on server 250, which may include one or more processing devices.  Processing devices of server 250 may be configured to execute the instructions of modules\n1701-1704.  In some embodiments, aspects of modules 1701-1704 may include software, hardware, or firmware instructions (or a combination thereof) executable by one or more processors, alone or in various combinations with each other.  For example,\nmodules 1701-1704 may be configured to interact with each other and/or other modules of server 250 and/or a wearable camera system to perform functions consistent with disclosed embodiments.  In some embodiments, any of the disclosed modules may each\ninclude dedicated sensors (e.g., IR, image sensors, etc.) and/or dedicated application processing devices to perform the functionality associated with each module.\nFIG. 18A is an illustration of an example image 1800 captured by a wearable apparatus, such as wearable apparatus 110.  In the example of FIG. 18A, image 1800 contains a person 1810.  In the example of FIG. 18A, wearable apparatus 110 may\nclassify person 1810 as a brother of the wearer of apparatus 110.  The classification may be determined using any one or more of the techniques discussed above in connection with FIG. 17.  Based on this classification, wearable apparatus 110 may take one\nor more actions.  For example, apparatus 110 may identify person 1810 as \"brother\" and/or by the name \"Ted,\" may upload and tag an image of person 1810 (e.g., to a social network account associated with the wearer), may update a social network account\nassociated with the wearer to indicate that the wearer was with person 1810, or the like.\nFIG. 18B is an illustration of another example image 1850 captured by a wearable apparatus such as wearable apparatus 110.  In the example of FIG. 18B, image 1850 contains a person 1860.  In the example of FIG. 18B, wearable apparatus 110 may\nclassify person 1860 as a child (e.g., based on a determined age of person 1860).  Based on this classification, apparatus 110 may take one or more actions.  For example, as depicted in FIG. 18B, wearable apparatus 110 may transmit an amber alert based\non the classification of person 1860 as a child and/or based on the absence of other persons, such as adults, nearby.  Alternatively or concurrently, as depicted in FIG. 18B, wearable apparatus 110 may transmit an update to a database providing\ninformation related to missing persons.  Such an update may verify if person 1860 is in the missing persons database and, if so, transmit a time and/or a location of the capture of image 1850 to the database.  In some examples, wearable apparatus 110 may\nforgo certain actions based on the category of the person, for example in FIG. 18B the apparatus may forgo uploading images of person 1860 to a social network or a public photo album based on the categorization of person 1860 as a child.\nFIGS. 18A and 18B are examples of persons being detected and categorized by wearable apparatus 110.  As would be understood by one of ordinary skill in the art, wearable apparatus 110 may capture images throughout the user's day at a variety of\nlocations as the environment surrounding the user changes.  For example, images may be captured when the user visits a restaurant for dinner, commutes to and from work, attends social events, etc. In this way, wearable apparatus 110 may be configured to\nmonitor the environment surrounding user 100 throughout the user's activities to identify exposure to one or more persons throughout the time user wears wearable apparatus 110, and then determine attributes of detected persons, base categories on the\ndetermined attributes, and take appropriate actions based on the categories.\nFIG. 19 illustrates a flowchart of an example method 1900 for selecting an action based on a detected person.  Method 1900 may be implemented by at least one processing device (e.g., processor 210 of wearable apparatus 110) and by a wearable\nimage sensor (e.g., image sensor 220 of wearable apparatus 110) configured to capture a plurality of images from the environment of the user of the wearable apparatus.\nAt step 1910, the processing device may analyze at least one of the plurality of images to detect the person.  For example, received image data may be processed by software steps executed by person detection module 1701.  In some embodiments,\nthe processing device may compare one or more regions of the at least one image against a database of known patterns and/or images to determine whether a person is included in the one or more regions.  Alternatively or concurrently, the processing device\nmay apply one or more classifiers to the one or more regions, and the one or more classifiers may output whether a person is detected in the one or more regions.  Still further, the processing device may execute one or more algorithms to detect facial\nfeatures (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person.\nIn some embodiments, the detection may be binary.  In such embodiments, the comparison and/or classifiers may output true if a person is detected and false if a person is not detected.  In other embodiments, the detection may be threshold-based. In such embodiments, the comparison and/or classifiers may output a percentage or other measurement of confidence that the one or more regions include a person.  For example, the comparison may output a likelihood (e.g., a 58% likelihood) that the one or\nmore regions include a person.  By way of further example, the classifiers may output a score (e.g., a score of 8 out of 10) that the one or more regions include a person.  In such embodiments, the processing device may use a threshold (e.g., at least\n55%, at least 7 out of 10, etc.) to convert the output to a binary detection.\nAt step 1920, the processing device may analyze at least one of the plurality of images to identify an attribute of the detected person.  For example, received image data may be processed by software steps executed by attribute identification\nmodule 1702.  Similar to step 1910, the processing device may compare one or more regions of the at least one image against a database of known patterns and/or images to determine the attribute.  Alternatively or concurrently, the processing device may\napply one or more classifiers to the one or more regions, and the one or more classifiers may output the attribute.  For example, as discussed earlier, one or more algorithms may analyze one or more of detected facial features (e.g., mouth, eyes, etc.),\nfacial contours, body shape, or any other suitable identifying feature of a person to estimate or determine information such as a the detected person's age, gender, weight, height, etc. In some embodiments, as discussed earlier, attribute identification\nmodule 1702 may determine an identity of a detected person (e.g., through facial recognition), and then access one or more databases (stored, e.g., locally in a memory of wearable apparatus 110 and/or accessible remotely over a network, e.g., such as by\naccessing server 250) to retrieve at least one attribute of the detected person.\nIn some embodiments, a confidence score or the like may be associated with the attribute.  For example, the comparison may output an attribute with a 64% confidence score.  By way of further example, the classifiers may output an attribute with\na confidence score of 4 (out of 10).  In such embodiments, the comparison and/or classifiers may output a plurality of attributes, and the processing device may select the attribute with the highest confidence score as the associated attribute.\nAs one example, analyzing at least one of the plurality of images to identify an attribute of the detected person may include analyzing the at least one of the plurality of images to estimate an age of the detected person.  As a second example,\nanalyzing at least one of the plurality of images to identify an attribute of the detected person may include analyzing the at least one of the plurality of images to estimate a height of the detected person.  In a third example, analyzing at least one\nof the plurality of images to identify an attribute of the detected person may include analyzing the at least one of the plurality of images to estimate a weight of the detected person.  As a fourth example, analyzing at least one of the plurality of\nimages to identify an attribute of the detected person may include analyzing the at least one of the plurality of images to estimate a gender of the detected person.\nIn some embodiments, analyzing at least one of the plurality of images to identify an attribute of the detected person may include analyzing the at least one of the plurality of images to determine an identity of the detected person.  For\nexample, an identity of the detected person may comprise a name, a job title, a phone number, or other identifier of the detected person.  In some embodiments, the processing device may compare one or more regions of the at least one image against a\ndatabase of known patterns to identify the detected person.  Alternatively or concurrently, the processing device may apply one or more classifiers to the one or more regions, and the one or more classifiers may output an identity of the detected person. In some embodiments, a confidence score or the like may be associated with the identity.  For example, the classifiers may output an identity with a confidence score (e.g., a 43% confidence score).  By way of further example, the comparison may output an\nidentity with a confidence score (e.g., a confidence score of 9 out of 10).  In such embodiments, the comparison and/or classifiers may output a plurality of identities, and the processing device may select the identity with the highest confidence score\nas the associated identity.\nBased on the identity of the detected person, the processing device may determine a type of a relationship between the user and the detected person, as depicted in the example of FIG. 18A.  For example, the relationship may comprise a category\nof relationship (such as family, friend, social, business, etc.), a specific relationship (such as brother, cousin, coworker, client, etc.), or the like.  In some embodiments, the processing device may determine multiple types of relationships (e.g., the\ndetected person is a family member and, more specifically, is the wearer's brother).\nAt step 1930, the processing device may select at least one category for the detected person based on the identified attribute.  Selecting the at least one category may be facilitated by software steps executed by categorization module 1704.  In\nsome embodiments, the at least one category may be selected from a plurality of attribute categories.  For example, the processing device may categorize the detected person into the at least one category based on the estimated age of the detected person\n(such as adult, child, teenager, senior citizen, etc.).  In a second example, the processing device may categorize the detected person into the at least one category based on the estimated height of the detected person (such as tall, average, short,\netc.).  As a third example, the processing device may categorize the detected person into the at least one category based on the estimated weight of the detected person (such as heavy, average, thin, etc.).  In a fourth example, the processing device may\ncategorize the detected person into the at least one category based on the estimated gender of the detected person (such as male, female, etc.).\nIn embodiments where the processing device has determined an identity of the detected person and determined a type of a relationship between the user and the detected person, the processing device may categorize the detected person into the at\nleast one category based on the type of the relationship between the user and the detected person (such as family, friend, social, business, sibling, coworker, etc.).\nIn some embodiments, the at least one category may include a statistical category.  For example, the processing device may retain a collection of statistics on the age, height, weight, and/or gender of persons detected in images captured by the\nwearable apparatus.  Statistical categories may thus be based on the collection of statistics, for example, medians, modes, means, deciles, quartiles, quintiles, or the like.  In some embodiments, the statistical category may be associated with a\ngeographical region and/or associated with a time period.  For example, the median may be measured only for a portion of the collection associated with North America.  By way of further example, the quartiles may be measured only for a portion of the\ncollection associated with the previous week.\nIn some embodiments, method 1900 may skip steps 1910 and 1920, and step 1930 may detect persons that match a selected category in images, for example using categorization module 1704.\nAt step 1940, the processing device may select at least one action based on the at least one category.  Selection of the at least one action may be facilitated via action module 1704.  For example, the at least one action may include identifying\nthe person, at least one of uploading an image and tagging an image, at least one of updating a log and updating a timeline or a Gantt chart, providing information related to a potential dating match, updating at least one of a social graph and a social\nnetwork profile, or the like.  The at least one action may be based on the at least one category.  For example, the processing device may select updating a timeline or a Gnatt chart (or other calendar or task list) if the detected person is categorized\nas a coworker or client.  By way of further example, the processing device may provide information related to a potential dating match if the detected person is categorized in one or more categories that match a dating profile of the user.  In another\nexample, the processing device may update a social graph and/or a social network profile if the detected person is categorized as a friend.\nIn some embodiments, the at least one action may be selected from a plurality of alternative actions associated with the at least one category.  For example, the category of friend may be associated with at least one of uploading an image and\ntagging an image and/or updating at least one of a social graph and a social network profile.  In such an example, the processing device may select at least one of uploading an image and tagging an image and/or updating at least one of a social graph and\na social network profile when the detected person is categorized as a friend.  In another example, the category of coworker may be associated with at least one of updating a log and updating a timeline or a Gantt chart and/or providing information\nrelated to a potential dating match.  In such an example, the processing device may select at least one of updating a log and updating a timeline or a Gantt chart and/or providing information related to a potential dating match when the detected person\nis categorized as a coworker.\nIn embodiments with a plurality of categories, the processing device may use a second category to select at least one action from a plurality of alternative actions associated with the first category.  Accordingly, in an example where the\ncategory of friend is associated with at least one of uploading an image and tagging an image and/or providing information related to a potential dating match, the processing device may select providing information related to a potential dating match\nwhen the gender of the detected person matches a preferred gender in a dating profile of the user.\nIn embodiments where the at least one category includes a statistical category, the at least one action may include updating information related to the statistical category.  For example, updating the information related to the statistical\ncategory includes updating a count of unique persons associated with the statistical category.  Accordingly, the processing device may keep track of a total number of friends, coworkers, males, females, short persons, tall persons, heavy persons, thin\npersons, or the like that are detected by the wearable apparatus.  As explained above, this count may be associated with a geographical region and/or associated with a time period.\nAt step 1950, the processing device may cause the at least one selected action to be executed.  In some embodiments, causing the at least one selected action to be executed may include sending information to a device paired with the wearable\napparatus.  A device paired with the wearable apparatus may include a smartphone, a tablet, a laptop, or the like.  Accordingly, the processing device may send information to the paired device using a wired connection (such as a universal serial bus\n(USB), or the like), a direct wireless connection (such as Bluetooth.RTM., or the like), and/or an indirect wireless connection (such as WiFi, 4G, or the like).\nThe transmitted information may include at least one of the plurality of images, one or more regions of at least one image (e.g., one or more regions used to detect a person and/or identify an attribute), one or more attributes of detected\npersons in the plurality of images, one or more categories for detected persons, one or more identities or detected persons, or the like.  Furthermore, the transmitted information may allow the paired device to executed one or more selected actions.  For\nexample, the processing device may transmit information to the paired device such that the paired device may identify the person, upload an image, tag an image, update a log, update a timeline or a Gantt chart, provide information related to a potential\ndating match, update at least one of a social graph and a social network profile, or the like.\nMethod 1900 may further include additional steps.  For example, the processing device may analyze at least one of the plurality of images to detect a second person.  The detection of the second person may be performed as the detection of the\nfirst person in step 1910.  Furthermore, method 1900 may further include analyzing at least one of the plurality of images to identify an attribute of the second person and selecting at least one category for the second person based on the identified\nattribute of the second person.\nIn some embodiments, one or more steps regarding the second person may be performed after steps 1910 to 1950 regarding the first person.  In other embodiments, one or more steps regarding the second person may be interspersed with steps 1910 to\n1950.  For example, the processing device may detect the second person after detecting the first person but before identifying an attribute of the first person and/or selecting at least one category for the first person.  By way of further example, the\nprocessing device may identify an attribute of the second person concurrently with identifying an attribute of the first person and/or may select at least one category for the second person concurrently with selecting at least one category for the first\nperson.\nIn some embodiments, method 1900 may base the selection of the at least one action on the at least one category selected for the second person.  For example, if a first detected person is categorized as a child and a second detected person is\ncategorized as an adult, the processing device may verify if the first detected person has been reported as abducted or kidnapped by the second detected person using a database of missing persons, a database of police reports, or the like.  By way of\nfurther example, if a first detected person is categorized as a coworker and a second detected person is categorized as a client, the processing device may update a timeline or a Gantt chart (or a calendar or task list) with a log of the meeting with the\nidentified coworker and the identified client.\nIn some embodiments, method 1900 may detect and categorize a plurality of persons appearing in one or more images, for example in a fashion similar to the one described above.  In some examples, step 1940 may base the selection of the at least\none action on the categories of the plurality of persons.  For example, the selection of the at least one action may be based on a distribution of the categories of a group of persons.  For example, the entropy of the distribution of the categories of a\ngroup of persons may be calculated, and the selection of the at least one action may be based on the calculated entropy.\nExecuting Actions Based on Physical Presence of a Detected Person\nAs explained above, in some embodiments, wearable apparatus 110 of the present disclosure may execute a variety of actions.  Some actions may be associated with a physical location of the apparatus, such as transmitting information associated\nwith a physical location of the wearable apparatus, updating at least one of a database and a social network profile based on information associated with a physical location of the wearable apparatus, determining one or more statistics based, at least in\npart, on information associated with a physical location of the wearable apparatus, or the like.  Other actions may be associated with a time and date (e.g., a time and date of capture of one or more images), such as transmitting information associated\nwith at least one of a time and a date, updating a database or a social network based on information associated with at least one of a time and a date, identifying one or more statistics based, at least in part, on information associated with at least\none of a time and a date, or the like.\nIn some embodiments, wearable apparatus 110 may determine one or more actions to take based on whether a detected person is physically present in an environment of a user of the wearable apparatus or visible on a display of a device in an\nenvironment of a user of the wearable apparatus.  In doing so, embodiments consistent with the present disclosure may address the technical problem of extracting and using information from an environment of the wearer of the wearable apparatus when the\nwearer is interacting with other people and not with images or screens.  For example, the wearer may wish to track encounters with certain people but not with any images of people in picture frames and/or on television screens, tablet screens, smartphone\nscreens, or the like.  Alternatively, the wearer may wish to track encounters with images separately from tracking physical encounters with other people.  Embodiments of the present disclosure may address this problem through techniques for assessing\nphysical presence of others based on images of the wearer's environment and executing appropriate actions based on the assessment.\nFIG. 20 illustrates an exemplary embodiment of a memory 2000 containing software modules consistent with the present disclosure.  Memory 200 may be included in apparatus 110 in lieu of or in combination with memory 550.  In some embodiments, the\nsoftware modules of memory 2000 may be combined with one or more software modules of memory 550 into one or more memories.  Memory 2000 may store more or fewer modules than those shown in FIG. 20.\nAs illustrated in FIG. 20, included in memory 2000 are software instructions to execute a person detection module 2001, a physical presence identification module 2002, and action module 2003.  Modules 2001, 2002, and 2003 may contain software\ninstructions for execution by at least one processing device, e.g., processor 210, included in a wearable apparatus, e.g., wearable apparatus 110.  In some embodiments, person detection module 2001, physical presence identification module 2002, and\naction module 2003 may cooperate to execute method 2200 of FIG. 22A, method 2230 of FIG. 22B, and/or method 2260 of FIG. 22C (or variants or combinations thereof).\nPerson detection module 2001 may be configured to analyze one or more images captured from a wearable apparatus to detect at least one person within the images.  For example, person detection module 2001 may be configured to identify a subset of\nthe captured data that includes at least one person.  In some embodiments, person detection module 2001 may be configured to receive a plurality of images and to identify one or more of the plurality of images that include at least one person.  For\nexample, module 2001 may receive a plurality of images of an environment surrounding a user wearing the wearable apparatus 110 and identify which of the plurality of images, if any, include at least one person.\nIn some embodiments, such an analysis may be performed by employing a facial recognition algorithm designed to detect facial features (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person. \nIn other embodiments, the at least one person may be identified using a thermal signature algorithm designed to detect the presence of at least one person based on the heat generated by the at least one person.  In such embodiments, the wearable\napparatus 110 may capture thermal images, either alone or in combination with visual images, for processing by the thermal signature algorithm.  In such embodiments, module 2001 (or another module not depicted) may align the thermal images with the\nvisual images such that at least one person detected on a thermal image may then be identified on a corresponding visual image.  Thermal recognition of the at least one person may be desirable in implementations in which the wearable device 110 is\noperating in reduced lighting situations.\nIn some embodiments, the at least one person may be identified through the application of one or more image classification techniques.  For example, at least one image classification technique may be used to classify at least one feature of an\nimage.  In some embodiments, an image classification technique may include one or more of image enhancement, edge detection, image analysis, and data extraction.  In some examples, the at least one person may be detected using a facial detection\nalgorithm, using a neural network trained to detect faces and/or persons in images, and so forth.  Specific examples of the methods for identifying at least one person are exemplary only, and a person of ordinary skill in the art will recognize other\nmethods for identifying the at least one person that remain consistent with the present disclosure.\nPhysical presence identification module 2002 may be configured to receive one or more images of detected persons and further analyze the one or more images to determine whether the detected persons are physically present or visible on a display\nof a device.  For example, module 2002 may receive one or more images in which person detection module 2001 has determined include at least one person and may identify if one or more persons of the at least one person are physically present and/or\nvisible on a display of a device.\nIn some embodiments, physical presence and/or visibility on a screen of a detected person may be determined based on analysis of one or more images of the detected person.  For example, one or more algorithms may analyze and/or compare one or\nmore of detected facial features (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person to determine whether the detected person was physically present in the one or more images or was visible on a\ndisplay in the one or more images.  For example, visibility on a display may cause one or more proportions of the detected person in the one or more images to differ from one or more expected proportions if the detected person were physically present. \nIn some examples, visibility on a display may cause pixilation and/or other aberrations of the detected person in the one or more images to exceed one or more expected values of such aberrations if the detected person were physically present.\nIn some embodiments, physical presence and/or visibility on a screen of a detected person may be determined based on analysis of areas adjacent to or surrounding the detected person and/or in the background of the detected person.  Such analysis\nmay include determining whether the detected person is bordered by a frame or other surface, such as the edge of a display screen.  In such instances, the analysis may include determining that certain features or parts (e.g., arms, legs, hands, feet,\netc.) of the detected person do not continue beyond a surface or edge depicted and detected in one or more images captured by the wearable apparatus, such as for example, when only a person's face is shown on display screen, but the person's body is not\npresent on the display.  Such analysis may further or alternatively include comparing features or parts of the detected person to other areas of one or more captured images to determine whether the detected person is not fully visible in the captured\nimages.  In some examples, such analysis may indicate that a detected person is interacting with items known to be the environment of a wearer of the wearable apparatus (e.g., an item previously identified in the wearer's environment, and now associated\nwith a detected person).  For example, the wearer may have picked up a particular item (e.g., a cup of coffee) and held the item in his or her hand previously before setting the item down (e.g., on a table) or handing it to another person. \nIdentification of the item in the hand of a detected person (or near the detected person) may contribute to or constitute a determination to that the detected person is physically present in the wearer's environment.  Such a determination may further\ntaken into an amount of time that has elapsed since the wearer or detected person held the item, interacted with the item, or otherwise encountered the item in his or her environment.  For example, if the wearer held the item within a predetermined time\nperiod (e.g., within 5 seconds, within 10 seconds, within 30 seconds, within 45 seconds, within 1 minute, etc.) before or after the item was identified in association with a detected person, then physical presence identification module 2002 may determine\nthat the detected person is in fact in the environment of the wearer.\nIn some embodiments, physical presence and/or visibility on a screen of a detected person may be determined based on analysis of one or more 3D images of the detected person.  For example, a person visible on a screen may correspond to a flat\nsurface in the 3D images, while a person which is physically present may correspond to a surface with convex and/or concave curves typical to a physical human body and/or face.\nIn some embodiments, physical presence identification module 2002 may be configured to receive one or more images, and analyze the one or more images using a detector configured to detect persons that are physically present and not to detect\npersons that are visible on a screen and/or in a photo.  For example, the detector may comprise using a classifier that classifies images and/or regions of images as depicting people that are physically present.  The classifier may be a result of a\nmachine learning algorithm trained on training examples, where a training example may comprise images and a label of the desired result.  For example, the classifier may comprise a neural network trained to detect persons that are physically present in\nimages.\nIn some embodiments, physical presence identification module 2002 may be configured to receive one or more images, and analyze the one or more images using a detector configured to detect persons that are visible on a screen and/or in a photo\nand not to detect persons that are physically present.  For example, the detector may comprise using a classifier that classifies images and/or regions of images as depicting people that are visible on a screen and/or in a photo.  The classifier may be a\nresult of a machine learning algorithm trained on training examples, where a training example may comprise images and a label of the desired result.  For example, the classifier may comprise a neural network trained to detect persons that are visible on\na screen and/or in a photo in images.\nIn some embodiments, physical presence and/or visibility on a screen of a detected person may be determined based on analysis of one or more images depicting the detected person.  For example, regions of the images containing the detected person\nmay be compared with regions of the images that do not contain the detected person, and the determination may be made based on the comparison results.  For example, shadows may be detected in regions of the images containing the detected person and in\nregions of the images that do not contain the detected person, properties of the shadows, such as the angle of the shadow, may be compared, and the determination may be made based on the comparison results, for example determining that the detected\nperson is physically present if and only if the angles match.  For example, statistics about colors and/or edges may be collected from regions of the images containing the detected person and from regions of the images that do not contain the detected\nperson, the statistics may be compared, and the determination may be made based on the comparison results.\nIn some embodiments, a determination as to whether or not a detected person is physically present may be based on a scoring or weighting approach in which certain cues (e.g., indicators tending to establish or not establish physical presence)\nare considered to determine whether or not a detected person is physically present or is depicted in an image on a display screen.  As such, weights or scores may be assigned to one or more cues determined from image analysis, and physical presence\nidentification module 2002 may use a cumulative weight value or score value for the one or more cues to make a determination (e.g., the cumulative weight value or score value exceeding a predetermined threshold may result in a determination that a\ndetected person is or is not physically present with the wearer).  Any combination of weighting and or scoring any one or more cues is contemplated and consistent with the disclosed embodiments.\nIn addition or in combination, certain rules or predetermined rule combinations may contribute to such determinations by physical presence identification module 2002.  For example, if a detected person's hands and/or feet are visible in one or\nmore captured images, and at least a portion of a background of the detected person includes an item known to be in the environment of the wearer of the wearable apparatus, physical presence identification module 2002 may conclude that the detected\nperson is physically present in the wearer's environment.  In contrast, if a detected person's hands and/or feet are not visible in one or more captured images, and at least a portion of a background of the detected person has no known commonality with\nthe environment of the wearer of the wearable apparatus (e.g., no items known to be in the environment of the wearer), physical presence identification module 2002 may conclude that the detected person is not physically present in the wearer's\nenvironment and, instead, is visible on a display or in photograph.  In some embodiments, any one of these example cues individually (e.g., that the detected person's hands and/or feet are or are not visible) may constitute a sufficient determination to\narrive at a conclusion as to physical presence or a lack of physical presence.\nSpecific examples of the methods for determining whether a detected person is physically present or visible on a display are exemplary only, and a person of ordinary skill in the art will recognize other methods for identifying the at least one\nperson that remain consistent with the present disclosure.  In some examples, physical presence may be determined using an algorithm, using a neural network trained to detect physical presence in images, and so forth.\nAction module 2003 may be configured to select one or more actions based on whether the detected person is physically present in the environment of the user and/or based on whether the detected person is visible on the display of the device. \nFor example, action module 2003 may update at least one of a database and a social network profile based on information associated with a physical location of the wearable apparatus when the detected person is physically present.  In such an example, a\nsocial network profile of the wearer may be updated with a new status indicating that the wearer interacted with the detected person at the physical location.  By way of further example, action module 2003 may update a database or a social network based\non information associated with at least one of a time and a date (e.g., a time or a date of capture of one or more images) when the detected person is visible on a display.  In such an example, a social network profile of the wearer may be updated with a\nnew status indicating that the wearer watched a particular television show, movie, or other source of the detected person at the time and/or the date.\nAlternatively or concurrently to performing one or more actions such as those above, action module 2003 may identify, determine, and/or update one or more statistics based, at least in part, on whether the detected person is physically present\nin the environment of the user or visible on the display of the device.  The one or more statistics may, alternatively or concurrently, be based, at least in part, on information associated with at least one of a time and a date and/or associated with a\nphysical location of the wearable apparatus.  Accordingly, in some embodiments, the statistics may be accumulated over physical locations and/or time frames.\nIn some embodiments, action module 2003 may cause the selected one or more actions to be executed by providing information (e.g., one or more instructions) to a device paired with the wearable apparatus (e.g., a smartphone) and/or by providing\ninformation (e.g., one more instructions) to a remote server (e.g., server 250) over a network.  For example, action module 1704 may transmit one or more instructions to a paired device to update a social network profile displayed on a screen of the\npaired device to indicate that the user interacted with the detected person, watched a particular television show, or the like.\nModules 2001, 2002, and 2003 may be implemented in software, hardware, firmware, a mix of any of those, or the like.  For example, if the modules are implemented in software, they may be stored, for example, in memory 550.  However, in some\nembodiments, any one or more of modules 2001, 2002, and 2003 may, for example, be stored in processor 540 and/or located on server 250, which may include one or more processing devices.  Processing devices of server 250 may be configured to execute the\ninstructions of modules 2001, 2002, and 2003.  In some embodiments, aspects of modules 2001, 2002, and 2003 may include software, hardware, or firmware instructions (or a combination thereof) executable by one or more processors, alone or in various\ncombinations with each other.  For example, modules 2001, 2002, and 2003 may be configured to interact with each other and/or other modules of server 250 and/or a wearable camera system to perform functions consistent with disclosed embodiments.  In some\nembodiments, any of the disclosed modules may each include dedicated sensors (e.g., IR, image sensors, etc.) and/or dedicated application processing devices to perform the functionality associated with each module.\nFIG. 21A is an illustration of an example image 2100 captured by a wearable apparatus, such as wearable apparatus 110.  In the example of FIG. 21A, image 2100 contains a person 2110.  In the example of FIG. 21A, wearable apparatus 110 may\ndetermine that person 2110 is physically present in the environment of the wearer of apparatus 110.  This determination may be made using any one or more of the techniques discussed above in connection with FIG. 20.  Based on this determination, wearable\napparatus 110 may take one or more actions.  For example, apparatus 110 may update a social network account associated with the wearer to indicate that the wearer was with person 2110, or the like.  In such an example, apparatus 110 may include a\nphysical location of apparatus 110 in the update (e.g., to indicate that the wearer was with person 2110 at the Starbucks on K St NW in Washington, D.C.).\nFIG. 21B is an illustration of another example image 2150 captured by a wearable apparatus, such as wearable apparatus 110.  In the example of FIG. 21B, image 2150 contains a person 2160.  In the example of FIG. 21B, wearable apparatus 110 may\ndetermine that person 2160 is visible on a display of a device in the environment of the wearer of apparatus 110.  This determination may be made using any one or more of the techniques discussed above in connection with FIG. 20.  For example, as\ndepicted in FIG. 21B, person 2160 may be visible on a display of a television.  In other examples, person 2160 may be visible on a display of a tablet, a display of a laptop, a display of a smartphone, or the like.  In the example of FIG. 21B, wearable\napparatus 110 may determine that person 2160 is visible in a photo.\nBased on this determination, wearable apparatus 110 may take one or more actions.  For example, apparatus 110 may update a social network account associated with the wearer to indicate that the wearer was watching person 2110 (or a television\nshow or movie including person 211), or the like.  In such an example, apparatus 110 may include at least one of a time and date in the update (e.g., to indicate that the wearer was watching Casablanca at 9:05 pm on Monday, October 16).\nFIGS. 21A and 21B are examples of persons being detected and determined to be either physically present or visible on a display by wearable apparatus 110.  As would be understood by one of ordinary skill in the art, wearable apparatus 110 may\ncapture images throughout the user's day at a variety of locations as the environment surrounding the user changes.  For example, images may be captured when the user visits a coffee shop to meet a friend, commutes to and from work, relaxes in a living\nroom of the user's house, etc. In this way, wearable apparatus 110 may be configured to monitor the environment surrounding user 100 throughout the user's activities to identify exposure to one or more persons throughout the time user wears wearable\napparatus 110, and then determine whether detected persons are physically present or visible on a display and take appropriate actions based on the determinations.\nFIG. 22A illustrates a flowchart of an example method 2200 for causing execution of an action based on physical presence of a detected person.  Method 2200 may be implemented by at least one processing device (e.g., processor 210 of wearable\napparatus 110) and by a wearable image sensor (e.g., image sensor 220 of wearable apparatus 110) configured to capture a plurality of images from the environment of the user of the wearable apparatus.\nAt step 2205, the processing device may analyze at least one of the plurality of images to detect the person.  For example, received image data may be processed by software steps executed by person detection module 2001.  In some embodiments,\nthe processing device may compare one or more regions of the at least one image against a database of known patterns and/or to determine whether a person is included in the one or more regions.  Alternatively or concurrently, the processing device may\napply one or more classifiers to the one or more regions, and the one or more classifiers may output whether a person is detected in the one or more regions.  Still further, the processing device may execute one or more algorithms to detect facial\nfeatures (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person.\nIn some embodiments, the detection may be binary.  In such embodiments, the comparison and/or classifiers may output true if a person is detected and false if a person is not detected.  In other embodiments, the detection may be threshold-based. In such embodiments, the comparison and/or classifiers may output a percentage or other measurement of confidence that the one or more regions include a person.  For example, the comparison may output a likelihood (e.g., a 58% likelihood) that the one or\nmore regions include a person.  By way of further example, the classifiers may output a score (e.g., a score of 8 out of 10) that the one or more regions include a person.  In such embodiments, the processing device may use a threshold (e.g., at least\n55%, at least 7 out of 10, etc.) to convert the output to a binary detection.\nAt step 2210, the processing device may analyze at least one of the plurality of images to determine whether the detected person is physically present in the environment of the user.  For example, received image data may be processed by software\nsteps executed by physical presence identification module 2002 executing any one or more of the techniques discussed above.  Similar to step 2205, the processing device may compare one or more regions of the at least one image against a database of known\npatterns and/or images to make the determination.  Alternatively or concurrently, the processing device may apply one or more classifiers to the one or more regions, and the one or more classifiers may output the determination.  Accordingly, in some\nembodiments, the analysis may include selecting, in the at least one of the plurality of images, one or more regions located within a threshold distance from the detected person and analyzing the selected one or more regions.  Additionally or\nalternatively, analyzing the at least one of the plurality of images to determine whether the person is physically present in the environment of the user may include analyzing whether the person is visible on a display of a device in the environment of\nthe user of the wearable apparatus.\nIn some embodiments, a confidence score or the like may be associated with the determination.  For example, the comparison may output the determination with a confidence score (e.g., a 64% confidence score).  By way of further example, the\nclassifiers may output the determination with a confidence score (e.g., a score of 4 out of 10).  In such embodiments, the processing device may use a threshold (e.g., at least 55%, at least 7 out of 10, etc.) to finalize the determination. \nAlternatively or concurrently, the comparison and/or classifiers may output a first confidence score or the like associated with a determination that the detected person is physically present in the environment of the user and a second confidence score\nor the like associated with a determination that the detected person is visible on a display of a device in the environment of the user of the wearable apparatus.  In such embodiments, the processing device may select the determination with the highest\nconfidence score.\nAt step 2215, the processing device may select at least one action based on whether the detected person is physically present in the environment of the user.  Selection of the at least one action may be facilitated via action module 2003.  For\nexample, the at least one action may include transmitting information associated with a physical location of the wearable apparatus, updating at least one of a database and a social network profile based on information associated with a physical location\nof the wearable apparatus; determining one or more statistics based, at least in part, on information associated with a physical location of the wearable apparatus; or the like.  As an example, when the detected person is physically present in the\nenvironment of the user, the processing device may update a database and/or a social network profile with a record of the interaction between the user and the detected person.  In such an example, the update may include a physical location of the\nwearable apparatus.  Alternatively or concurrently, the update may include a time and/or a date of the interaction.  In embodiments where the at least one action include determining one or more statistics, the processing device may update a count of\npersons associated with a physical location of the wearable apparatus and/or associated with a time and/or date of the interaction.\nAt step 2220, the processing device may cause the at least one selected action to be executed.  In some embodiments, causing the at least one selected action to be executed may include sending information to a device paired with the wearable\napparatus.  A device paired with the wearable apparatus may include a smartphone, a tablet, a laptop, or the like.  Accordingly, the processing device may send information to the paired device using a wired connection (such as a universal serial bus\n(USB), or the like), a direct wireless connection (such as Bluetooth.RTM., or the like), and/or an indirect wireless connection (such as WiFi, 4G, or the like).\nThe transmitted information may include at least one of the plurality of images, one or more regions of at least one image (e.g., one or more regions used to detect a person and/or identify an attribute), one or more determinations of whether\ndetected persons in the plurality of images are physically present or visible on a display, a physical location of the wearable apparatus, a time and/or a date of capture of the images, or the like.  Furthermore, the transmitted information may allow the\npaired device to execute one or more selected actions.  For example, the processing device may update at least one of a database and a social network profile based on information associated with a physical location of the wearable apparatus; determine\none or more statistics based, at least in part, on information associated with a physical location of the wearable apparatus; or the like.\nMethod 2200 may further include additional steps.  For example, the processing device may analyze at least one of the plurality of images to detect a second person.  The detection of the second person may be performed as the detection of the\nfirst person in step 2205.  Furthermore, method 2200 may further include analyzing at least one of the plurality of images to determine whether the second person is visible on a display of a device.\nIn some embodiments, one or more steps regarding the second person may be performed after steps 2205 to 2220 regarding the first person.  In other embodiments, one or more steps regarding the second person may be interspersed with steps 2205 to\n2220.  For example, the processing device may detect the second person after detecting the first person but before determining whether the first person is physically present.  By way of further example, the processing device may determine whether the\nsecond person is visible on a display of a device concurrently with determining whether the first person is physically present.\nIn some embodiments, method 2200 may include further basing the selection of the at least one action on whether the second person is visible on the display of the device.  For example, if a first detected person is determined as physically\npresent and a second detected person is determined as visible on a display, the processing device may update a database and/or a social network profile and include the first detected person in the update and omit the second detected person from the\nupdate, or the like.  By way of further example, the processing device may update a first statistic based on the determination that the first detected person is physically present and update a second statistic based on the determination that the second\ndetected person is visible on a display.  In another example, based on the determination that the first detected person is physically present and the second detected person is visible on a display, the processing device may update a database and/or a\nsocial network profile with information regarding the interaction of wearer and the first detected person with the second person, for example indicating that the wearer and the first detected person are in a video conference with the second detected\nperson.\nFIG. 22B illustrates a flowchart of an example method 2230 for causing execution of an action based on whether a detected person is visible on a display of a device.  Method 2230 may be implemented by at least one processing device (e.g.,\nprocessor 210 of wearable apparatus 110) and by a wearable image sensor (e.g., image sensor 220 of wearable apparatus 110) configured to capture a plurality of images from the environment of the user of the wearable apparatus.\nAt step 2235, the processing device may analyze at least one of the plurality of images to detect the person.  For example, received image data may be processed by software steps executed by person detection module 2001.  In some embodiments,\nthe processing device may compare one or more regions of the at least one image against a database of known patterns and/or to determine whether a person is included in the one or more regions.  Alternatively or concurrently, the processing device may\napply one or more classifiers to the one or more regions, and the one or more classifiers may output whether a person is detected in the one or more regions.  Still further, the processing device may execute one or more algorithms to detect facial\nfeatures (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person.\nIn some embodiments, the detection may be binary.  In such embodiments, the comparison and/or classifiers may output true if a person is detected and false if a person is not detected.  In other embodiments, the detection may be threshold-based. In such embodiments, the comparison and/or classifiers may output a percentage or other measurement of confidence that the one or more regions include a person.  For example, the comparison may output a likelihood (e.g., a 58% likelihood) that the one or\nmore regions include a person.  By way of further example, the classifiers may output a score (e.g., a score of 8 out of 10) that the one or more regions include a person.  In such embodiments, the processing device may use a threshold (e.g., at least\n55%, at least 7 out of 10, etc.) to convert the output to a binary detection.\nAt step 2240, the processing device may analyze at least one of the plurality of images to determine whether the detected person is visible on the display of the device.  For example, received image data may be processed by software steps\nexecuted by physical presence identification module 2002 executing any one or more of the techniques discussed above.  Similar to step 2235, the processing device may compare one or more regions of the at least one image against a database of known\npatterns and/or images to make the determination.  Alternatively or concurrently, the processing device may apply one or more classifiers to the one or more regions, and the one or more classifiers may output the determination.  Accordingly, in some\nembodiments, the analysis may include selecting, in the at least one of the plurality of images, one or more regions located within a threshold distance from the detected person and analyzing the selected one or more regions.  Additionally or\nalternatively, analyzing the at least one of the plurality of images to determine whether the detected person is visible on the display of the device may include analyzing whether the detected person is physically present in the environment of the user\nof the wearable apparatus.\nIn some embodiments, a confidence score or the like may be associated with the determination.  For example, the comparison may output the determination with a confidence score (e.g., a 64% confidence score).  By way of further example, the\nclassifiers may output the determination with a confidence score (e.g., a score of 4 out of 10).  In such embodiments, the processing device may use a threshold (e.g., at least 55%, at least 7 out of 10, etc.) to finalize the determination. \nAlternatively or concurrently, the comparison and/or classifiers may output a first confidence score or the like associated with a determination that the detected person is visible on the display of the device and a second confidence score or the like\nassociated with a determination that the detected person is physically present in the environment of the user of the wearable apparatus.  In such embodiments, the processing device may select the determination with the highest confidence score.\nAt step 2245, the processing device may select at least one action based on whether the detected person is visible on the display of the device.  Selection of the at least one action may be facilitated via action module 2003.  For example, the\nat least one action may include transmitting information associated with at least one of a time and a date (e.g., a time and a date of capture of at least one of the plurality of images), updating a database or a social network based on information\nassociated with at least one of a time and a date (e.g., a time and a date of capture of at least one of the plurality of images); identifying one or more statistics based, at least in part, on information associated with at least one of a time and a\ndate (e.g., a time and a date of capture of at least one of the plurality of images); or the like.  As an example, when the detected person is visible on the display of the device, the processing device may update a database and/or a social network\nprofile with a television show, movie, or other media in which the detected person appears.  In such an example, the update may include a time and/or a date of capture of the at least one image with the detected person.  Alternatively or concurrently,\nthe update may include a physical location associated with the wearable apparatus.  In embodiments where the at least one action include identifying one or more statistics, the processing device may update a count of displays (or television shows or\nmovies or the like) associated with a time and/or a date of capture of the at least one image with the detected person and/or associated with a physical location of the wearable apparatus.\nAt step 2250, the processing device may cause the at least one selected action to be executed.  In some embodiments, causing the at least one selected action to be executed may include sending information to a device paired with the wearable\napparatus.  A device paired with the wearable apparatus may include a smartphone, a tablet, a laptop, or the like.  Accordingly, the processing device may send information to the paired device using a wired connection (such as a universal serial bus\n(USB), or the like), a direct wireless connection (such as Bluetooth.RTM., or the like), and/or an indirect wireless connection (such as WiFi, 4G, or the like).\nThe transmitted information may include at least one of the plurality of images, one or more regions of at least one image (e.g., one or more regions used to detect a person and/or identify an attribute), one or more determinations of whether\ndetected persons in the plurality of images are visible on the display or physically present, a physical location of the wearable apparatus, a time and/or a date of capture of the images, or the like.  Furthermore, the transmitted information may allow\nthe paired device to execute one or more selected actions.  For example, the processing device may update at least one of a database and a social network profile based on information associated with a time and/or a date; identify one or more statistics\nbased, at least in part, on information associated with a time and/or a date; or the like.\nMethod 2230 may further include additional steps.  For example, the processing device may analyze at least one of the plurality of images to determine that the detected person is visible on the display device and is taking part in a video\nconference and further base the selection of the at least one action on the determination that the detected person is visible on the display device and is taking part in the video conference.  In such an example, the processing device may update a\ndatabase and/or a social network profile with a record of the video conference in which the detected person appears.  In such an example, the update may include a time and/or a date of the videoconference.\nBy way of additional example, the processing device may analyze at least one of the plurality of images to determine that the detected person is included in at least one of a video image and a still image and further base the selection of the at\nleast one action on the determination that the detected person is included in at least one of a video image and a still image.  For example, if the detected person is included in a video image, the processing device may then analyze at least one of the\nplurality of images to determine that the detected person is visible on the display device and is taking part in a video conference.  Alternatively or concurrently, the processing device may update a database and/or a social network profile with a\ntelevision show, movie, or other media in which the detected person appears.  In such an example, the update may include a time and/or a date of capture of the at least one image with the detected person.  In other embodiments, if the detected person is\nincluded in a still image, the processing device may update a database and/or a social network profile with a record of the image in which the detected person appears.  Alternatively, the processing device may take no action.\nIn another example, the processing device may analyze at least one of the plurality of images to detect a second person.  The detection of the second person may be performed as the detection of the first person in step 2235.  Furthermore, method\n2230 may further include analyzing at least one of the plurality of images to determine whether the second person is physically present in the environment of the user.\nIn some embodiments, one or more steps regarding the second person may be performed after steps 2235 to 2250 regarding the first person.  In other embodiments, one or more steps regarding the second person may be interspersed with steps 2235 to\n2250.  For example, the processing device may detect the second person after detecting the first person but before determining whether the first person is visible on the display of the device.  By way of further example, the processing device may\ndetermine whether the second person is physically present in the environment of the user concurrently with determining whether the first person is visible on the display of the device.\nIn some embodiments, method 2230 may include further basing the selection of the at least one action on whether the second person is physically present in the environment of the user.  For example, if a first detected person is determined as\nvisible on the display and a second detected person is determined as physically present, the processing device may update a database and/or a social network profile and include the second detected person in the update and omit the first detected person\nfrom the update, or the like.  By way of further example, the processing device may update a first statistic based on the determination that the first detected person is visible on the display and update a second statistic based on the determination that\nthe second detected person is physically present.\nFIG. 22C illustrates a flowchart of an example method 2260 for causing execution of an action based on physical presence of a detected person.  Method 2260 may be implemented by at least one processing device (e.g., processor 210 of wearable\napparatus 110) and by a wearable image sensor (e.g., image sensor 220 of wearable apparatus 110) configured to capture a plurality of images from the environment of the user of the wearable apparatus.\nAt step 2265, the processing device may analyze at least one of the plurality of images to detect the person.  For example, received image data may be processed by software steps executed by person detection module 2001.  In some embodiments,\nthe processing device may compare one or more regions of the at least one image against a database of known patterns and/or to determine whether a person is included in the one or more regions.  Alternatively or concurrently, the processing device may\napply one or more classifiers to the one or more regions, and the one or more classifiers may output whether a person is detected in the one or more regions.  Still further, the processing device may execute one or more algorithms to detect facial\nfeatures (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person.\nIn some embodiments, the detection may be binary.  In such embodiments, the comparison and/or classifiers may output true if a person is detected and false if a person is not detected.  In other embodiments, the detection may be threshold-based. In such embodiments, the comparison and/or classifiers may output a percentage or other measurement of confidence that the one or more regions include a person.  For example, the comparison may output a likelihood (e.g., a 58% likelihood) that the one or\nmore regions include a person.  By way of further example, the classifiers may output a score (e.g., a score of 8 out of 10) that the one or more regions include a person.  In such embodiments, the processing device may use a threshold (e.g., at least\n55%, at least 7 out of 10, etc.) to convert the output to a binary detection.\nAt step 2270, the processing device may analyze at least one of the plurality of images to determine whether the detected person is physically present in the environment of the user or whether a graphical representation of the detected person\nappears in the environment of the user.  For example, received image data may be processed by software steps executed by physical presence identification module 2002 executing any one or more of the techniques discussed above.  Similar to step 2265, the\nprocessing device may compare one or more regions of the at least one image against a database of known patterns and/or images to make the determination.  Alternatively or concurrently, the processing device may apply one or more classifiers to the one\nor more regions, and the one or more classifiers may output the determination.  Accordingly, in some embodiments, the analysis may include selecting, in the at least one of the plurality of images, one or more regions located within a threshold distance\nfrom the detected person and analyzing the selected one or more regions.\nIn some embodiments, a confidence score or the like may be associated with the determination.  For example, the comparison may output the determination with a confidence score (e.g., a 64% confidence score).  By way of further example, the\nclassifiers may output the determination with a confidence score (e.g., a score of 4 out of 10).  In such embodiments, the processing device may use a threshold (e.g., at least 55%, at least 7 out of 10, etc.) to finalize the determination. \nAlternatively or concurrently, the comparison and/or classifiers may output a first confidence score or the like associated with a determination that the detected person is physically present in the environment of the user and a second confidence score\nor the like associated with a determination that a graphical representation of the detected person appears in the environment of the user.  In such embodiments, the processing device may select the determination with the highest confidence score.\nAt step 2275, the processing device may select a first action after the determination is made that the detected person is physically present in the environment of the user.  Selection of the first action may be facilitated via action module\n2003.  For example, the first action may include transmitting information associated with a physical location of the wearable apparatus, updating at least one of a database and a social network profile based on information associated with a physical\nlocation of the wearable apparatus; determining one or more statistics based, at least in part, on information associated with a physical location of the wearable apparatus; or the like.  As an example, when the detected person is physically present in\nthe environment of the user, the processing device may update a database and/or a social network profile with a record of the interaction between the user and the detected person.  In such an example, the update may include a physical location of the\nwearable apparatus.  Alternatively or concurrently, the update may include a time and/or a date of the interaction.  In embodiments where the first action include determining one or more statistics, the processing device may update a count of persons\nassociated with a physical location of the wearable apparatus and/or associated with a time and/or date of the interaction.\nAt step 2280, the processing device may select a second action different from the first action after the determination is made that the graphical representation of the detected person appears in the environment of the user.  Selection of the\nsecond action may be facilitated via action module 2003.  For example, the second action may include transmitting information associated with at least one of a time and a date (e.g., a time and a date of capture of at least one of the plurality of\nimages), updating a database or a social network based on information associated with at least one of a time and a date (e.g., a time and a date of capture of at least one of the plurality of images); identifying one or more statistics based, at least in\npart, on information associated with at least one of a time and a date (e.g., a time and a date of capture of at least one of the plurality of images); or the like.  As an example, when the graphical representation of the detected person appears in the\nenvironment of the user, the processing device may update a database and/or a social network profile with a television show, movie, or other media in which the graphical representation appears.  In such an example, the update may include a time and/or a\ndate of capture of the at least one image with the detected person.  Alternatively or concurrently, the update may include a physical location associated with the wearable apparatus.  In embodiments where the second action include identifying one or more\nstatistics, the processing device may update a count of displays (or television shows or movies or the like) associated with a time and/or a date of capture of the at least one image with the detected person and/or associated with a physical location of\nthe wearable apparatus.\nIn addition, the first action and/or the second action may include taking no action.  Accordingly, the wearable apparatus 110 may take no action if the detected person is physically present in the environment of the user and/or if a graphical\nrepresentation of the detected person appears in the environment of the user.  Accordingly, wearable apparatus may only update at least one of a database and a social network profile, determine or identify one or more statistics or the like when the\ndetected person is physically present in the environment of the user and/or if a graphical representation of the detected person appears in the environment of the user.\nAt step 2285, the processing device may cause the first action or the second action to be executed.  In some embodiments, causing the first action or the second action to be executed may include sending information to a device paired with the\nwearable apparatus.  A device paired with the wearable apparatus may include a smartphone, a tablet, a laptop, or the like.  Accordingly, the processing device may send information to the paired device using a wired connection (such as a universal serial\nbus (USB), or the like), a direct wireless connection (such as Bluetooth.RTM., or the like), and/or an indirect wireless connection (such as WiFi, 4G, or the like).\nThe transmitted information may include at least one of the plurality of images, one or more regions of at least one image (e.g., one or more regions used to detect a person and/or identify an attribute), one or more determinations of whether\ndetected persons in the plurality of images are visible on the display or physically present, a physical location of the wearable apparatus, a time and/or a date of capture of the images, or the like.  Furthermore, the transmitted information may allow\nthe paired device to execute one or more selected actions.  For example, the processing device may update at least one of a database and a social network profile based on information associated with a time and/or a date; identify one or more statistics\nbased, at least in part, on information associated with a time and/or a date; or the like.\nMethod 2260 may further include additional steps.  For example, any of the additional steps of method 2230 and/or method 2200 may be included in method 2260.\nUpdating Profile Information for a Person Viewed by Multiple Wearable Apparatuses\nIn some embodiments, a plurality of persons may use a plurality of wearable apparatuses, such as wearable apparatus 110, and a profile associated with a particular person may be updated using information from one or more of the plurality of\nwearable apparatuses.  In some embodiments in which the profile is updated based on information from a plurality of wearable apparatuses, the profile may be considered to be a joint profile as the profile may reflect information collected and/or updated\nfrom the plurality of wearable apparatuses.\nIn certain aspects, the profile may be stored and updated on a server and/or on a shared storage and/or on a shared database to which the information from the one or more wearable apparatuses is sent.  In doing so, embodiments consistent with\nthe present disclosure may address the technical problem of extracting, collating, and indexing information from a plurality of wearable apparatuses and keeping the information updated such that it remains useful rather than stale, out-of-date, and/or\ninaccurate.  Further, collecting and leveraging data from multiple wearable apparatuses may contribute to an approved accuracy of a particular person's profile.  For example, updates to the profile may be more frequent and therefore are more likely to be\naccurate and complete.  Moreover, the improved accuracy and completeness of the profile may provide for the delivery or relevant news, advertisements, or the like to the person associated with the profile.  Embodiments of the present disclosure may\naddress this problem through techniques for managing information extracted or determined from images of multiple wearable devices and properly indexing and storing the extracted or determined information in a database of profiles.\nAlthough discussed herein in association with a profile related to a particular person, the disclosed systems and methods may equally apply to updating and/or managing a profile related to an object or a place.\nFIG. 23 illustrates an exemplary embodiment of a memory 2300 containing software modules consistent with the present disclosure.  Memory 2300 may be included in apparatus 110 in lieu of or in combination with memory 550.  In some embodiments,\none or more software modules of memory 2300 may be stored in a remote server, e.g., server 250.  Memory 2300 may store more or fewer modules than those shown in FIG. 23.\nAs illustrated in FIG. 23, included in memory 2300 are software instructions to execute a person detection module 2301, an identification information module 2302, an auxiliary information module 2303, and a profile module 2304.  Modules 2301,\n2302, 2303, and 2304 may contain software instructions for execution by at least one processing device, e.g., one or more processes, included in a remote server, e.g., server 250.  In some embodiments, person detection module 2301, identification\ninformation module 2302, auxiliary information module 2303, and profile module 2304 may cooperate to execute method 2500 (or a variant thereof) of FIG. 25.\nPerson detection module 2301 may be configured to analyze one or more images captured from a wearable apparatus to detect at least one person within the images.  For example, person detection module 2301 may be configured to identify a subset of\nthe captured data that includes at least one person.  In some embodiments, person detection module 2301 may be configured to receive a plurality of images and to identify one or more of the plurality of images that include at least one person.  For\nexample, module 2301 may receive a plurality of images of an environment surrounding a user wearing wearable apparatus 110 and identify which of the plurality of images include at least one person.\nIn some embodiments, such an analysis may be performed by employing a facial recognition algorithm designed to detect facial features (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person. \nIn other embodiments, the at least one person may be identified using a thermal signature algorithm designed to detect the presence of at least one person based on the heat generated by the at least one person.  In such embodiments, the wearable\napparatus 110 may capture thermal images, either alone or in combination with visual images, for processing by the thermal signature algorithm.  Thermal recognition of the at least one person may be desirable in implementations in which the wearable\ndevice 110 is operating in reduced lighting situations.  In some embodiments, the at least one person may be identified through the application of one or more image classification techniques.  For example, at least one image classification technique may\nbe used to classify at least one feature of an image.  In some embodiments, an image classification technique may include one or more of image enhancement, edge detection, image analysis, and data extraction.  Specific examples of the methods for\nidentifying at least one person are exemplary only, and a person of ordinary skill in the art will recognize other methods for identifying the at least one person that remain consistent with the present disclosure.  In some examples, the at least one\nperson may be detected using a facial detection algorithm, using a neural network trained to detect faces and/or persons in images, and so forth.\nIdentification information module 2302 may be configured to obtain identification information associated with the detected persons.  For example, identification information may include a name, birthday, or other real life indicator of the\ndetected person and/or may include an identification number, a username, or other artificial (optionally anonymous) indicator of the detected person.  In some embodiments, the identification information may comprise a plurality of indicators.\nIn some embodiments, identification information of a detected person may be obtained based on analysis of one or more images of the detected person.  For example, one or more algorithms may analyze one or more of detected facial features (e.g.,\nmouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person to determine one or more indicators associated with the detected person.  In another example, identification information module 2302 may obtain\nidentification information through facial recognition.  Identification information module 2302 may then access a database included, for example, in remote server 250 and/or a database accessible over a network in order to search for and retrieve\ninformation related to the detected person.\nIn some embodiments, identification information module 2302 may apply at least one hashing function on at least part of at least one image of the detected person captured by the wearable image sensor to obtain a hash value of the detected\nperson.  Accordingly, obtaining the identification information may include obtaining a hash value associated with the detected person and accessing a plurality of hash values to determine the existence of a hash value associated with the detected person\nbased on the obtained hash value.  In some embodiments, then, a plurality of hash values may be stored in a lookup database, which may be separate from or form a part of the database of profiles.\nIn some examples, the at least part of at least one image of the detected person may comprise at least one image of a face of the detected person.  By hashing the face (or any other portion of at least one image) of the detected person,\nidentification information module 2302 may allow for indexing and lookup of the detected person in the database without compromising the anonymity of the detected person.  Accordingly, in such embodiments, even if the hash value of the detected person\nwere intercepted, for example, while traveling across a computer network, the identity of the detected person may remain hidden from the intercepting party.\nIn some examples, the at least one hashing function may comprise obtaining an identifier of a person (such as a unique identifier, a nearly unique identifier, a name, a login name, a user name from a social network, and so forth), for example\nusing a person recognition algorithm and/or facial recognition algorithm, and evaluating a hash function on the unique identifier.  Examples of such hash functions may include a cryptographic hash function, a perfect hash function, a nonlinear table\nlookup hash function, and so forth.  In some examples, the at least one hashing function may comprise projecting the image of the detected person (or of the face of the detected person) to an n-dimensional space to obtain an n-dimensional vector,\nquantizing the n-dimensional vector, and using the resulting quantized n-dimensional vector as a hash value and/or evaluating a hash function on the quantized n-dimensional vector.  In some cases, the projection function may be the result of training a\nmachine learning dimensional reduction algorithm on training examples, where a training example may comprise an image of a person and/or a face with a desired it-dimensional vector and/or indication of desired distance and/or proximity to other\ndata-points in the n-dimensional space.\nAuxiliary information module 2303 may be configured to obtain auxiliary information associated with the detected person.  For example, auxiliary information module 2303 may obtain position information, a capture time of at least one of the one\nor more images, information associated with a user of the wearable image sensor (e.g., identification information associated with the user, one or more images of the user, or the like).\nAlternatively or concurrently, auxiliary information may be obtained from analysis of the one or more images of the detected person.  For example, auxiliary information module 2303 may analyze one or more of detected facial features, facial\ncontours, body position, or the like to determine an emotional state of the detected person and/or a facial expression of the detected person.  In another example, auxiliary information module 2303 may analyze the one or more images to determine\ninformation associated with a distance (e.g., a measurement of the distance, a direction of the distance, etc.) of the wearable image sensor to the detected person.  In some embodiments, auxiliary information may be obtained from analysis of one or more\nimages of other detected persons.  Accordingly, the auxiliary information may include information related to a second person appearing in the one or more images with the detected person.  For example, auxiliary information module 2303 may analyze one or\nmore of detected facial features, facial contours, body position, or the like to obtain information associated with at least one other person.  In certain aspects, the detected person and the at least one other detected person may be detected in the same\nimage.\nAlternatively or concurrently, auxiliary information may be obtained from input to a wearable apparatus.  For example, auxiliary information module 2303 may receive at least one of audio topics and video topics from the wearable apparatus (or\nfrom user input into the wearable apparatus).  Similarly, an emotional state of the detected person, a facial expression of the detected person, and/or information associated with at least one other person may be obtained from user input into the\nwearable apparatus.\nMoreover, the auxiliary information may directly include at least part of an image.  For example, at least part of an image of the one or more images of the detected person may be included in the auxiliary information.  Alternatively or\nconcurrently, at least part of an image of the one or more images of other detected persons may be included in the auxiliary information.\nIn some embodiments, auxiliary information module 2303 may be configured to receive one or more images of detected persons and further analyze the one or more images to obtain at least one property associated with the detected person (e.g., age,\ngender, weight, height, facial expression, emotional state, etc.) associated with the identified persons.\nIn some embodiments, the at least one property of a detected person may be determined based on further analysis of one or more images of the detected person.  For example, one or more algorithms may analyze one or more of detected facial\nfeatures (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person to estimate or determine information such as a the detected person's age, gender, weight, height, facial expression, emotional state,\netc.\nIn some embodiments, auxiliary information module 2303 may determine an identity of a detected person (e.g., through facial recognition) or receive identification information from identification information module 2302, and then access one or\nmore databases (stored, e.g., locally in a memory of wearable apparatus 110 and/or accessible remotely over a network, e.g., such as by accessing server 250) to retrieve at least one property of the detected person.  For example, after identifying a\ndetected person as a particular individual (e.g., determining a match based on facial recognition to a known person's image), auxiliary information module 2303 may access a database to retrieve information about the detected person, such as the detected\nperson's age, gender, family members, friends, etc.\nIn some embodiments, the auxiliary information may include information related to at least one item associated with the detected person.  For example, auxiliary information module 2303 may analyze one or more regions of the one or more images to\nidentify the at least one item.  In some embodiments, the regions may include regions within a particular distance of the detected person.  In some embodiments, the at least one item may include a product appearing in the one or more images with the\ndetected person.  For example, auxiliary information module 2303 may compare at least a portion of the one or more regions to known patterns and/or images to identify the product.  Alternatively or concurrently, auxiliary information module 2303 may use\none or more classifiers on the one or more regions to identify the product.\nIn some embodiments, the auxiliary information may include information related to an action performed by the detected person.  For example, using one or more comparisons to known patterns and/or images and/or using one or more classifiers,\nauxiliary information module 2303 may identify an action being performed by the detected person.  In such an example, auxiliary information module 2303 may identify the detected person as cheering if a facial expression of the detected person matches\nknown images and/or patterns of people cheering and/or is identified as an image of cheering by one or more classifiers trained to recognize people cheering.\nProfile module 2304 may be configured to identify, in a database storing a plurality of profiles, a profile associated with the detected person based on the identification information and update the identified profile based on the auxiliary\ninformation.  For example, profile module 2304 may identify the profile using at least a portion of the identification information.  The profile may be included in a database indexed by identification information.  Accordingly, in some embodiments,\nprofile module 2304 may use an indicator determined by identification information module 2302 and query an index of indicators to identify the profile associated with the detected person.  Alternatively or concurrently, profile module 2304 may query the\nidentification information against the profiles directly (e.g., by performing a fuzzy search of images in the profiles against at least a portion of the one or more images of the detected person) to identify the profile associated with the detected\nperson.  Alternatively or concurrently, profile module 2304 may use a hash value, such as a hash value calculated by identification information module 2302, to select a profile.\nIn some embodiments, profile module 2304 may update the profile by adding at least a portion of the auxiliary information to the profile.  Additionally or alternatively, profile module 2304 may update the profile by adding at least a portion of\nthe identification information to the profile.  For example, profile module 2304 may store at least a portion of the one or more images of the detected person and/or at least a portion of one or more images of other detected persons in the profile.  In\nanother example, profile module 2304 may store an emotional state of the detected person, a facial expression of the detected person, information associated with at least one other person, position information, a capture time of at least one of the one\nor more images, information associated with a user of the wearable image sensor, or other portions of the auxiliary information (and/or identification information) in the profile.\nAlternatively or additionally, profile module 2304 may update the profile based on the auxiliary information (and/or the identification information).  For example, information associated with a user of the wearable image sensor may be used to\nupdate a network of social connections of the detected person stored within the profile.  By way of further example, an emotional state of the detected person and/or a facial expression of the detected person may be used to update an emotional timeline\nof the detected person stored within the profile.\nIn some embodiments, profile module 2304 may also provide information based on the identified profile.  For example, profile module 2304 may provide the information to a device paired with the wearable apparatus (e.g., a smartphone or tablet)\nand/or may provide the information to a remote server (e.g., server 250) over a network.  For example, profile module 2304 may transmit information from the identified profile to a paired device to alert a user of the wearable apparatus of the name of\nthe detected person.  In another example, profile module 2304 may transmit a determined emotional state of the detected person to alert the user that the detected person is feeling sad and may desire comforting.\nModules 2301, 2302, 2303, and 2304 may be implemented in software, hardware, firmware, a mix of any of those, or the like.  For example, if the modules are implemented in software, they may be stored, for example, in memory 550.  However, in\nsome embodiments, any one or more of modules 2301, 2302, 2303, and 2304 may, for example, be stored in processor 540 and/or located on server 250, which may include one or more processing devices.  Processing devices of server 250 may be configured to\nexecute the instructions of modules 2301, 2302, 2303, and 2304.  In some embodiments, aspects of modules 2301, 2302, 2303, and 2304 may include software, hardware, or firmware instructions (or a combination thereof) executable by one or more processors,\nalone or in various combinations with each other.  For example, modules 2301, 2302, 2303, and 2304 may be configured to interact with each other and/or other modules of server 250 and/or a wearable camera system to perform functions consistent with\ndisclosed embodiments.  In some embodiments, any of the disclosed modules may each include dedicated sensors (e.g., IR, image sensors, etc.) and/or dedicated application processing devices to perform the functionality associated with each module.\nFIG. 24 is an illustration of an example profile 2400 associated with a person.  In some embodiments, at least some of the information stored in profile 2400 may include or be based upon information collected by one or more wearable apparatuses,\nsuch as wearable apparatus 110.\nAs would be understood by one of ordinary skill in the art, wearable apparatus 110 may capture images throughout a user's day at a variety of locations as the environment surrounding the user changes.  In this way, wearable apparatus 110 may be\nconfigured to capture information related to the user and/or various detected persons.  Disclosed embodiments may use the captured information to construct profiles associated with the user and/or detected persons and index those profiles within one or\nmore databases.\nIn the example of FIG. 24, profile 2400 includes position information 2401.  For example, position information 2401 may include indicators of locations in which a person was detected by one or more wearable apparatuses.  For example, position\ninformation 2401 may include indicators that the person was detected in zip code 20001; at the Starbucks on New York Ave NW; at particular GPS coordinates; at a street address; in a particular country, city, state, and/or county; at a particular\nlandmark; or the like.\nAs further illustrated in FIG. 24, profile 2400 may include times and/or distances 2402 related to the capture of one or more images of the person.  For example, profile 2400 may include information such as the person being imaged at a\nparticular date and/or time, such as at 4:30 pm on Tuesday, November 9; at 21:16 on Halloween 2018; or the like.  Further, as other examples, such information may include distances between persons or between person and objects as determined from captured\nimages.\nProfile 2400 may further include emotional states 2403 and/or facial expressions 2404 of the person.  For example, emotional states may include information and/or images reflected observed emotional states (e.g., happy, sad, etc.) determined\nfrom analysis of one or more images.  In some embodiments, such information may be correlated with dates and/or times at which the emotional state was observed.  Similarly, facial expressions may include information and/or images reflecting observed\nfacial expressions (e.g., smiling, frowning, etc.) determined from analysis of one or more images.  In some embodiments, such information may be correlated with dates and/or times at which the facial expression was observed.\nAs further illustrated in FIG. 24, profile 2400 may include audio topics and/or video topics 2405 associated with the person.  In some embodiments, audio topics may have been determined through analysis of audio data recorded by a microphone\nincluded a wearable apparatus.  Similarly, in some embodiments, video topics may have been determined through analysis of video data captured by an image sensor included a wearable apparatus.\nIn addition, profile 2400 may include information about nearby items 2406.  Such information may be determined through analysis of one or more images captured by a wearable apparatus.  As explained above in reference to auxiliary information\nmodule 2303, the nearby items may include products within a vicinity of a person included in an image and/or a user of a wearable apparatus.\nIn some embodiments, profile 2400 may include demographic information 2407.  For example, estimates of age, height, weight, gender, socioeconomic status, or the like may be included in profile 2400.\nAlthough profile 2400 include the foregoing exemplary information, profile 2400 may include any information collected related to a particular person, object, or location.  In some embodiments, at least some of the information stored in profile\n2400 includes and/or is based upon information collected by one or more wearable apparatus.  Such collected information may include any combination of images, video, and audio, and/or or any information derived from analysis of any combination of images,\nvideo, and audio.\nIn some embodiments, profile 2400 may be stored in a server, in a shared storage device, in a distributed database, in a blockchain, and so forth.\nFIG. 25 illustrates a flowchart of an example method 2500 for updating profile information based on data collected by a wearable apparatus.  Method 2500 may be implemented by at least one processing device (e.g., one or more processors of server\n250) receiving information from one or more wearable image sensors (e.g., image sensor 220 of wearable apparatus 110) configured to capture a plurality of images from the environment of the user(s) of the wearable apparatus(es).\nAt step 2510, the processing device may obtain identification information associated with a person detected in one or more images captured by a wearable image sensor included in the wearable apparatus.  For example, received image data may be\nprocessed by software steps executed by person detection module 2301 and/or identification information module 2302.  In some embodiments, the detection may include comparing one or more regions of the at least one image against a database of known\npatterns and/or images to determine whether a person is included in the one or more regions.  Alternatively or concurrently, the detection may include applying one or more classifiers to the one or more regions, and the one or more classifiers may output\nwhether a person is detected in the one or more regions.  Still further, the detection may include using one or more algorithms to detect facial features (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of\na person.  In some embodiments, the detection may be performed by a separate processing device.  For example, the detection may be performed by processor 210 of wearable apparatus 110, and obtaining identification information associated with the detected\nperson may be performed by one or more processors of server 250.  In some embodiments, detecting and person and obtaining identification information associated with the detected person may be performed in a single device, for example by processor 210 of\nwearable apparatus 110, by server 250, and so forth.\nIn some embodiments, the identification information may include a name, birthday, or other real life indicator of the detected person and/or may include an identification number, a username, or other artificial (optionally anonymous) indicator\nof the detected person.  In some embodiments, the identification information may comprise a plurality of indicators.  In some embodiments, the identification information may comprise a hash value, such as a hash value calculated by identification\ninformation module 2302.\nIn some embodiments, identification information of a detected person may be obtained based on analysis of one or more images of the detected person.  Similar to the detection, the processing device may compare one or more regions of the at least\none image against a database of known patterns and/or images to obtain the identification information.  Alternatively or concurrently, the processing device may apply one or more classifiers to the one or more regions, and the one or more classifiers may\noutput the identification information.  For example, as discussed earlier, one or more algorithms may extract one or more of detected facial features (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a\nperson as the identification information.\nIn some embodiments, obtaining the identification information may include searching a lookup database, which may be separate from or form a part of a database of profiles, for identification information using one or more extracted features of\nthe one or more images, as described above.  In such embodiments, obtaining the identification information may include obtaining a hash value associated with the detected person and accessing a plurality of hash values to determine the existence of a\nhash value associated with the detected person based on the obtained hash value.  Accordingly, the lookup database may include and/or be indexed by a plurality of hash values.  In one example, the at least part of at least one image of the detected\nperson may comprise at least one image of a face of the detected person.\nAt step 2520, the processing device may obtain, for example from the wearable apparatus, auxiliary information associated with the detected person.  For example, received image data may be processed by software steps executed by auxiliary\ninformation module 2303 to produce the auxiliary information.  In another example, step 2520 may comprise receiving the auxiliary information produced by auxiliary information module 2303.\nIn some embodiments, the auxiliary information may include at least one of position information, a capture time of at least one of the one or more images, and information associated with a user of the wearable image sensor (e.g., identification\ninformation associated with the user, one or more images or the user, or the like).  Alternatively or concurrently, the auxiliary information may be obtained from analysis of the one or more images of the detected person.  For example, the auxiliary\ninformation may include at least one of an emotional state of the detected person and a facial expression of the detected person.  In another example, the auxiliary information may include information associated with a distance (e.g., a measurement of\nthe distance, a qualitative label of the distance such as \"far\" or \"near,\" or the like) of the wearable image sensor to the detected person.\nIn some embodiments, the auxiliary information may be obtained from analysis of one or more images of other detected persons.  For example, the auxiliary information may include information associated with at least one other person detected in\nthe one or more images and/or information related to a second person appearing in the one or more images with the detected person (e.g., identification information associated with the at least one other person and/or the second person, one or more images\nof the at least one other person and/or the second person, or the like).  In some examples, the detected person and the at least one other detected person may be detected in the same image.\nAlternatively or concurrently, the auxiliary information may be obtained from input to the wearable apparatus.  For example, the processing device may receive at least one of audio topics and video topics from the wearable apparatus (or from\nuser input into the wearable apparatus).  Similarly, an emotional state of the detected person, a facial expression of the detected person, information associated with at least one other person, and/or information related to the second person may be\nobtained from user input into the wearable apparatus.\nMoreover, the auxiliary information may directly include at least part of an image.  For example, at least part of an image of the one or more images of the detected person may be included in the auxiliary information.  Alternatively or\nconcurrently, at least part of an image of one or more images of the at least one other person and/or the second person may be included in the auxiliary information.\nIn some embodiments, obtaining auxiliary information may include receiving one or more images of a detected person and further analyzing the one or more images to obtain at least one property associated with the detected person (e.g., age,\ngender, weight, height, facial expression, emotional state, etc.).  In some embodiments, this analysis may be performed by a separate processing device.  For example, the analysis may be performed by processor 210 of wearable apparatus 110, and the\noutput may be received (and thus obtained as the auxiliary information) by one or more processors of server 250.\nIn some embodiments, the analysis may include applying one or more algorithms to analyze one or more of detected facial features (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person to\nestimate or determine information such as a the detected person's age, gender, weight, height, facial expression, emotional state, etc. In some embodiments, the detected person be identified (e.g., through facial recognition), and the auxiliary\ninformation including at least one property may be obtained from one or more databases (stored, e.g., locally in a memory of wearable apparatus 110 and/or accessible remotely over a network, e.g., such as by accessing server 250).\nIn some embodiments, obtaining auxiliary information may include receiving one or more images of a detected person and further analyzing the one or more images to identify the at least one item associated with the detected person.  For example,\nthe analysis may include analysis of one or more regions of the one or more images to identify the at least one item.  The regions may, for example, include regions within a particular distance of the detected person.  In some embodiments, the analysis\nmay further include identifying the at least one item as a product.  For example, the analysis may include compare at least a portion of the one or more regions to known patterns and/or images to identify the product.  Alternatively or concurrently, one\nor more classifiers may be used on the one or more regions to identify the product.  In some embodiments, these analyses may be performed by a separate processing device.  For example, the analysis to identify the at least one item and/or the analysis to\nidentify the at least one item as a product may be performed by processor 210 of wearable apparatus 110, and the output may be received (and thus obtained as the auxiliary information) by one or more processors of server 250.\nIn some embodiments, the auxiliary information may include information related to an action performed by the detected person.  For example, obtaining the auxiliary informing may include using one or more comparisons to known patterns and/or\nimages and/or using one or more classifiers to identify an action being performed by the detected person.  In some embodiments, this analysis may be performed by a separate processing device.  For example, the analysis may be performed by processor 210\nof wearable apparatus 110, and the output may be received (and thus obtained as the auxiliary information) by one or more processors of server 250.\nIn some embodiments, the auxiliary information may include at least one property associated with the detected person.  Similar to step 2510, the processing device may compare one or more regions of the at least one image against a database of\nknown patterns and/or images to determine the at least one property.  Alternatively or concurrently, the processing device may apply one or more classifiers to the one or more regions, and the one or more classifiers may output the attribute.  For\nexample, as discussed earlier, one or more algorithms may analyze one or more of detected facial features (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person to estimate or determine information\nsuch as a the detected person's age, gender, weight, height, facial expression, emotional state, etc. In some embodiments, this analysis may be performed by a separate processing device.  For example, the analysis may be performed by processor 210 of\nwearable apparatus 110, and the output may be received (and thus obtained as the auxiliary information) by one or more processors of server 250.\nAt step 2530, the processing device may identify, in the database, a profile associated with the detected person based on the identification information.  Identifying the profile may be facilitated by software steps executed by profile module\n2304.  For example, the processing device may identify the profile using an indicator included in the identification information and query an index of indicators to identify the profile associated with the detected person.  Alternatively or concurrently,\nthe processing device may query the identification information against the profiles directly (e.g., by performing a fuzzy search of names in the profiles against one or more names and/or partial names included in the obtained identification information)\nto identify the profile associated with the detected person.\nAt step 2540, the processing device may update the identified profile based on the auxiliary information.  For example, the processing device may update the profile by adding at least a portion of the auxiliary information to the profile. \nAdditionally or alternatively, the processing device may update the profile by adding at least a portion of the identification information to the profile.  For example, the processing device may store at least a portion of one or more names associated\nwith the detected person in the profile.  In another example, the processing device may store an emotional state of the detected person, a facial expression of the detected person, information associated with a second person, position information, a\ncapture time of at least one of the one or more images, information associated with a user of the wearable image sensor, or other portions of the auxiliary information (and/or identification information) in the profile.\nAlternatively or additionally, the processing device may update the profile based on the auxiliary information (and/or the identification information).  For example, information associated with at least one other detected person may be used to\nupdate a network of social connections of the detected person stored within the profile.  By way of further example, information associated with at least one other detected person may be used to update a calendar of social interactions, business\ninteractions, or the like stored within profile.\nIn some embodiments, the processing device may provide information based on the identified profile.  For example, the processing device may provide the information to a device (e.g., a server, a computer, a smartphone, a tablet, etc.) or may\ntransmit the information to an address (e.g., an email address) or a network address.  In some embodiments, the device to which the information is provided may be paired with a wearable apparatus.  A device paired with the wearable apparatus may include\na smartphone, a tablet, a laptop, or the like.  Accordingly, the processing device may send information to a device using a wired connection (such as a universal serial bus (USB), or the like), a direct wireless connection (such as Bluetooth.RTM., or the\nlike), and/or an indirect wireless connection (such as WiFi, 4G, or the like).\nIn some embodiments, the transmitted information may include at least a portion of the identification information, at least a portion of the auxiliary information, at least a portion of the profile, or the like.\nMethod 2500 may further include additional steps.  For example, the processing device may obtain, from a second wearable apparatus, additional auxiliary information associated with the detected person, the additional auxiliary information being\nbased on a second set of one or more images captured by the second wearable image sensor.  The obtaining of additional auxiliary information associated with the detected person may be performed similar to the obtaining of the auxiliary information in\nstep 2520.  Furthermore, method 2500 may further include updating the profile of the detected person based on the additional auxiliary information.  The updating of the profile based on the additional auxiliary information may be performed similar to the\nupdating of the profile in step 2540.\nDetermining a Level of Information Detail Provided to a User\nIn some embodiments, wearable apparatus 110 may collect information related to at least one person detected in an environment of the user of the wearable apparatus 110.  The wearable apparatus 101 may then determine what level of detail (e.g.,\nhow much) of information to provide to the user about the detected person.  Some existing wearable device systems may encounter the technical problem of how to process the information collected by the wearable device and use that information to provide\nuseful feedback to the user.  For example, certain existing systems may capture images that include people in the user's environment, but given the increasing quantity of publicly available data about individuals and the likelihood that the majority of\nthe available data is not of interest to the user, fail to provide information pertinent to the user (e.g., information that the user finds useful or of interest).  Some of the presently disclosed embodiments, on the other hand, may address this problem\nby assigning affinity measurements to a degree of a relationship between the identified person and the user to determine what information and/or level of information to provide to the user.\nFor example, in one embodiment, the affinity measurement may be based on a relationship in a social network between the user of the wearable apparatus 110 and the detected person in the captured image.  As a further example, in one embodiment,\nthe user may not receive any information related to a stranger with whom the user has no social relationship, but may receive a name and affiliation for a person in a common social group, and a name and common friends for second degree connections.  In\nthis way, presently disclosed embodiments may address the technical problems associated with parsing out information useful to a user in view of the large quantity of data acquired by the wearable device and publicly available.\nFIG. 26 is a diagram illustrating an example of memory 550 storing a plurality of modules, consistent with the disclosed embodiments.  The modules may be executable by at least one processing device to perform various methods and processes\ndisclosed herein.  Memory 550 may store more or fewer modules than those shown in FIG. 26.\nAs illustrated in FIG. 26, memory 550 may store software instructions to execute a data capture module 2601, a person identification module 2602, an action execution module 2603, a database access module 2604, and may also include database(s)\n2605.  Data capture module 2601 may include software instructions for receiving data from wearable apparatus 110.  Person identification module 2602 may include software instructions for analyzing data obtained by wearable apparatus 110 to identify\nsubsets of the captured data including at least one person and information associated with the at least one person.  Action execution module 2603 may include software instructions to cause the occurrence of an action based on the information associated\nwith the at least one person identified in the acquired data.  Database access module 2604 may include software instructions executable to interact with database(s) 2605, to store and/or retrieve information.\nData capture module 2601 may include software instructions for receiving data from a wearable apparatus, such as a wearable camera system.  Data received from a wearable camera system may include audio and image data, captured, by, for example,\nan image sensor or microphone associated with the wearable camera system.  Image data may include raw images and may include image data that has been processed.  Raw images may be provided, for example, in the form of still images and video data, either\nwith or without embedded metadata.  In some embodiments, image data and audio data may be preprocessed prior to capture by data capture module 2601.  Preprocessing may include, for example, noise reduction, artifact removal, compression, and other image\npre-processing techniques.\nPerson identification module 2602 may be configured to analyze data captured by data capture module 2601 to detect or identify a subset of the captured data that includes at least one person.  In some embodiments, module 2602 may be configured\nto receive a plurality of images and to identify one or more of the plurality of images that include at least one person.  For example, module 2602 may receive a plurality of images of an environment surrounding a user wearing the wearable device 110 and\nidentify which of the plurality of images include at least one person.\nIn some embodiments, such an analysis may be performed by employing a facial recognition algorithm designed to detect facial features (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person. \nIn other embodiments, the at least one person may be identified using a thermal signature algorithm designed to detect the presence of at least one person based on the heat generated by the at least one person.  In such embodiments, the wearable device\n110 may capture thermal images, either alone or in combination with visual images, for processing by the thermal signature algorithm.  Thermal recognition of the at least one person may be desirable in implementations in which the wearable device 110 is\noperating in reduced lighting situations.  In some embodiments, the at least one person may be identified through the application of one or more image classification techniques.  For example, at least one image classification technique may be used to\nclassify at least one feature of an image.  In some embodiments, an image classification technique may include one or more of image enhancement, edge detection, image analysis, and data extraction.  Specific examples of the methods for identifying at\nleast one person are exemplary only, and a person of ordinary skill in the art will recognize other methods for identifying the at least one person that remain consistent with the present disclosure.  In some examples, the at least one person may be\ndetected using a face detection algorithm, using a neural network trained to detect faces and/or persons in images, and so forth.  In some examples, the at least one person may be identified using a face recognition algorithm, using a neural network\ntrained to identify people in images, and so forth.\nPerson identification module 2602 may further be configured to determine or obtain information associated with the at least one person identified in the image(s).  The information associated with the at least one person may include a name,\nnickname, social security number, account number, or any other identifier of the at least one person.  Once identifying information is obtained for the at least one person, the person identification module 2602 may obtain or determine at least one\naffinity measurement representing a degree of a relationship between the user of the wearable device 110 and the identified at least one person.\nThe affinity measurement may be represented in any suitable manner.  For example, the affinity measurement may be a numerical value assigned on a given scale, such as 0-1 or 1-100, with larger numbers representing a higher or closer degree of\nrelationship between the user and the at least one person and lower numbers representing a lower or more distant degree of relationship between the user and the at least one person (or vice versa, with lower numbers indicating a higher degree).  In other\nembodiments, the affinity measurement may be a level selected from a finite number of levels, for example with each level representing a range capturing the number of years the user has been associated with the at least one person.  For example, a\nchildhood friend whom the user has known for 15 years may be an affinity 1, which is assigned to people known by the user for 15 or more years.  In this example, a friend for two months may be assigned an affinity 3, which is assigned to people known by\nthe user for less than a year.\nIn some embodiments, the affinity measurement may be based on a category that indicates a relationship degree.  For example, some affinity categories may include family, friends, acquaintances, co-workers, strangers, etc. In other embodiments,\nthe affinity measurement may capture the emotional and/or social closeness of the user to the at least one person.  For example, close friends and family (e.g., childhood friends and siblings) may be assigned a similar affinity measurement, while more\ndistant friends and family (e.g., co-workers and second cousins) may be assigned a different affinity measurement.  Indeed, the affinity measurement may be assigned in any suitable manner, not limited to the examples herein, depending on\nimplementation-specific considerations.\nIn some embodiments, the affinity measurement may be based on a social network.  In some examples, the distance between two persons in social network may calculated as the minimal number of social network connections required to connect the two\npersons, and the affinity measurement may be based on the distance.  For example, let the number of social network connections required to connect the two persons be N, the affinity measure may be calculated as 1/N, as exp(-(N-A)/S) for some constants A\nand S, as f(N) for some monotonically decreasing function f, and so forth.  In some examples, the affinity measurement of two persons may be calculated based on the number of shared connections the two persons have in the social network.  For example,\nlet the number of shared connections the two persons have in the social network be M, the affinity measure may be calculated as M, log(A), as exp((M-A)/S) for some constants A and S, as f(M) for some monotonically increasing function f, and so forth. \nFor example, let the number of shared connections the two persons have in the social network be M, and let the number of connections the first person and second person have in the social network be M1 and M2 correspondingly, the affinity measure may be\ncalculated as M/sqrt(M1*M2), as f(M,M1,M2) for some function f that is monotonically increasing in M and monotonically decreasing in M1 and M2, and so forth.  In some examples, the affinity measurement of two persons may be calculated based on the number\nand/or length of interaction (e.g., conversations) the two persons conducted in the social network.  For example, let the overall length of interaction the two persons conducted in the social network be M, the affinity measure may be calculated as M,\nlog(M), exp((M-A)/S) for some constants A and S, f(M) for some monotonically increasing function f, and so forth.\nAction execution module 2603 may be configured to perform a specific action in response to the identification of one or more images including the at least one person.  For example, action execution module 2603 may determine, based on the at\nleast one affinity measurement, an information level to be disclosed to the user of the wearable apparatus.  In certain embodiments, the information level may be an amount, quantity, extent, type, and/or quality of information.  For example, a higher\ninformation level may provide the user a greater amount of information (e.g., name, athletic interests, and job title) than a lower information level (e.g., only name).  In other embodiments, the information level may provide matchmaking information if\nthe affinity measurement indicates a non-familial connection between the user and the at least one person.\nAction execution module 2603 may further be configured to provide information to the user of the wearable apparatus based on the information associated with the at least one person identified in the captured images and the determined information\nlevel.  The information provided to the user may include, for example, a person's name, job title, gender, interests, hobbies, political affiliation, work related information (e.g., whether the user and the at least one person have worked together in the\npast), leisure related information (e.g., whether the user and the at least one person have played sports together in the past, whether the user and the at least one person are predicted to be a successful match, whether the at least one person is\nsingle, etc.), matchmaking information (e.g., whether the user and the at least one person have dated in the past), or any other information about the at least one person that is available to the wearable device 110.\nDatabase 2605 may be configured to store any type of information of use to modules 2601-2604, depending on implementation-specific considerations.  For example, in embodiments in which action execution module 2603 is configured to provide the\ninformation about the identified at least one person to the user of the wearable apparatus 110, database 2605 may store prior-collected information about the user's social, familial, or other contacts.  Further, the database 2605 may store the metadata\nassociated with the captured images.  In some embodiments, database 2605 may store the one or more images of the plurality of captured images that include the at least one person.  Indeed, database 2605 may be configured to store any information\nassociated with the functions of modules 2601-2604.\nModules 2601-2604 may be implemented in software, hardware, firmware, a mix of any of those, or the like.  For example, if the modules are implemented in software, they may be stored in memory 550, as shown in FIG. 26.  However, in some\nembodiments, any one or more of modules 2601-2604 and data associated with database 2605, may, for example, be stored in processor 540 and/or located on server 250, which may include one or more processing devices.  Processing devices of server 250 may\nbe configured to execute the instructions of modules 2601-2604.  In some embodiments, aspects of modules 2601-2604 may include software, hardware, or firmware instructions (or a combination thereof) executable by one or more processors, alone or in\nvarious combinations with each other.  For example, modules 2601-2604 may be configured to interact with each other and/or other modules of server 250 and/or a wearable camera system to perform functions consistent with disclosed embodiments.  In some\nembodiments, any of the disclosed modules may each include dedicated sensors (e.g., IR, image sensors, etc.) and/or dedicated application processing devices to perform the functionality associated with each module.\nFIG. 27 shows an example environment including wearable apparatus 110 for capturing and processing images.  In the depicted embodiment, user 100 may wear wearable apparatus 110 on his or her neck.  However, in other embodiments, wearable\napparatus 110 may be differently positioned in any suitable location to enable capture of images of the user's environment, such as the locations explained in detail above.  User 100 may be in any location and engaging in any interaction encountered\nduring user's daily activities.  For example, user 100 may be at a convenience store, grocery store, sports event, social event, work-related event, movie theater, concert, etc. Wearable apparatus 110 may capture a plurality of images depicting the\nenvironment to which the user is exposed while user 100 is engaging in his/her chosen activity.  For example, in the illustrated embodiment, wearable apparatus 110 may capture images that include a first person 2702 and/or a second person 2704\ninteracting with user 100.  Further, wearable apparatus 110 may also capture, for example, when user 100 turns, one or more additional persons 2706 located at a distance from the area in which the conversation with persons 2702 and 2704 is occurring.  As\nsuch, the images may show that the user 100 is exposed to persons 2702, 2704, and 2706.  The images depicting the exposure of user 100 to particular persons 2702, 2704, and 2706 may be included in a log or otherwise saved in database 2605.\nFIG. 27 shows user 100 being exposed to persons 2702, 2704, and 2706 while standing.  However, as would be understood by one of ordinary skill in the art, wearable apparatus 110 may capture images throughout the user's day at a variety of\nlocations with the user in a variety of positions as the environment surrounding the user changes.  For example, images may be captured when the user visits a restaurant for dinner, commutes to and from work, attends social events, etc. In this way,\nwearable apparatus 110 may be configured to monitor the environment surrounding user 100 throughout the user's activities to identify exposure to one or more persons throughout the time user wears wearable apparatus 110.\nIn some embodiments, the one or more affinity measurements assigned to the identified persons 2702, 2704, and/or 2706 may depend on a type of interaction of the user 100 with the given identified person.  For example, in the illustrated\nembodiment, the user 100 is shaking the hand of person 2702 while standing in a conversational position with respect to persons 2702 and 2704.  As such, the user 100 and the person 2704 may be previously acquainted, while the user 100 and the person 2702\nmay have just met.  Based on this interaction, a higher affinity measurement may be assigned to person 2704 than person 2702.  For further example, while persons 2706 may be captured in images, user 100 is at a distance from persons 2706, thus indicating\nlack of a current interaction.  As such, a lower affinity measurement may be assigned to persons 2706 than persons 2702 and 2704.\nFurther, in some embodiments, the affinity measurement(s) assigned to the identified person(s) may be based in whole or in part on a physical distance between user 100 and the person(s) identified.  For example, in the illustrated embodiment,\nuser 100 may be at a distance 2708 from person 2702.  The distance 2708 may be determined, for example, by analyzing the size and/or location of the features of the captured person(s), using 3D and/or range imaging, and so forth.  For example, let the\nphysical distance between two persons be L, the affinity measure may be calculated as 1/L, as exp(-(L-A)/S) for some constants A and S, as f(L) for some monotonically decreasing function f, and so forth.  Still further, in some embodiments, wearable\ndevice 110 may be programmed to determine the distance 2708 or interaction between, for example, user 100 and person 2702, by analyzing the captured image.  For example, wearable device 110 may recognize that the chest of person 2702 is in the direct\nline of capture of the camera in wearable device 110, which may in turn indicate the relative position between user 100 and person 2702.\nFIG. 28A illustrates a flowchart of an exemplary method 2800 for identifying exposure to at least one person, consistent with embodiments of the present disclosure.  The method 2800 may be carried out, for example, by a processing device\nintegrated with and/or associated with wearable apparatus 110.  In such an embodiment, wearable apparatus 110 may include a wearable image sensor, e.g., image sensor 220, configured to capture a plurality of images from the environment of the user.  For\nexemplary purposes only, method 2800 for identifying exposure to at least one person is described herein with respect to processing device 210 cooperating with memory 550 to execute modules 2601-2604.\nIn accordance with method 2800, processor 210 may receive image data captured by a wearable image sensor at block 2802.  Block 2802 may be facilitated by software instructions of data capture module 2601.  Data capture module 2601 may be\nconfigured to execute instructions to receive image data from a wearable image sensor, and may also be configured to execute instructions to control the wearable image sensor.  Controlling the wearable image sensor may include issuing commands to record\nimages and/or videos, and may also include issuing commands to control an orientation or direction of viewing of the image sensor.\nReceived image data may be processed via software steps executed by person identification module 2602.  For example, at block 2804, person identification module 2602 may identify one or more images including the at least one person from a\nplurality of captured images.  For example, the at least one person may be person 2702, and the module 2602 may analyze the plurality of images to identify a subset of the captured images that include features sized, shaped, or otherwise resembling a\nperson.  Further, at block 2806, the one or more identified images may be processed to identify the particular person(s) depicted in the images flagged as including person(s).  In another example, at block 2804 and/or 2806, person identification module\n2602 may identify unique instances of the at least one person appearing in the plurality of captured images.\nAt block 2808, person identification module 2602 may further analyze the subset of the captured images including the at least one person to determine information associated with the at least one person.  The information associated with the at\nleast one person may include a name, nickname, hobby, interest, like, dislike, places frequently visited, sexual orientation, gender, political affiliation, nationality, sporting events attended, or any other identifier or information available about the\ngiven person.  Further, the information about the at least one person may be sourced from any desired location.  For example, the information may be sourced from prior-sourced information stored in database(s) 2605 via database access module 2604.  For\nfurther example, the information may be sourced from one or more social media accounts.  In such embodiments, wearable device 110 may source the information publicly available via a web browser and/or through a private account of user 100.  In other\nembodiments, the information may be sourced from records of email, text, voicemail, or telephone communications between user 100 and the at least one identified person.  Indeed, the information may be sourced from any suitable location, not limited to\nthose described herein, depending on implementation-specific considerations.\nAt block 2810, person identification module 2602 may determine or obtain at least one affinity measurement representing a degree of a relationship between the user 100 and the at least one identified person.  The at least one affinity\nmeasurement may take any of the forms described above.  Further, the at least one affinity measurement may be based on one or more factors indicative of the degree of the relationship between the user 100 and the identified at least one person.  For\nexample, in some embodiments, the affinity measurement may be based on a social graph, a social network, a type of interaction between the user 100 and the at least one person, a physical distance between the user 100 and the at least one person, or any\nother suitable factor.  In one embodiment, the affinity measurement may capture the relationship between the user 100 and the at least one person person in a social network (e.g., a list of contacts on a social media site) to ensure that an appropriate\nlevel of information is provided to the user 100.  For example, in one embodiment, a low affinity measurement may ensure that the user 100 would not receive any information related to a stranger, but a higher affinity measurement would ensure that the\nuser 100 would receive a name and affiliation for person in a common group, or a name and common friends for second degree connections.  In some embodiments, an appropriate affinity measurement could ensure that for a detected person that is a first\ndegree connection, even more information could be provided, such as the time of a last meeting, a last email, etc.\nIn the embodiment illustrated in FIG. 28A, at block 2812, the data including the information associated with the at least one person and/or the affinity measurement(s) is transmitted to an external device.  The external device may be a\nsmartphone, tablet, smartwatch, laptop, server, or any other suitable device configured to process the transmitted data.  To that end, the external device and wearable apparatus 110 may include suitable components to enable data transfer between wearable\napparatus 110 and the external device.  For example, in one embodiment, the wearable apparatus may include a transmitter configured to enable wireless pairing with a receiver located in the external device.  In such embodiments, the wearable apparatus\nand the external device may be reversibly or irreversibly paired to enable exclusive data transfer between the two devices.  The pairing may be established by the user of the wearable apparatus and external device, or may be automatically performed when\nthe wearable apparatus and the external device are within a given distance from one another (e.g., within a range such that the transmitter and receiver are capable of exchanging data).\nHowever, in other embodiments, the transfer of data to the external device at block 2812 may be omitted from method 2800, and further processing of the data may be performed by the wearable apparatus 110, for example, in action execution module\n2603 or using blocks 2824, 2826 and 2828.  In the exemplary illustrated embodiment, however, the information associated with the at least one person and the affinity measurement are received by an external device at block 2822 in method 2820.  At block\n2824, method 2820 includes determining an information level to be disclosed to the user 100 of the wearable apparatus 110.\nIn some embodiments, the information level may be selected from a plurality of alternate information levels, as discussed above.  For example, a first information level may be for a close social connection, and a second information level may be\nfor a more distant connection.  More specifically, with respect to FIG. 27, the first information level may be assigned to person 2704, with whom user 100 is already acquainted, and the second information level may be assigned to person 2702, whom the\nuser 100 just met.  In one embodiment, selection of the first information level may result in the shared information including the name of the person 2704, and selection of the second information level may result in the provided information not including\nthe name of the person 2702.\nThe method 2820 further includes accessing stored data reflecting the determined information level for the at least one person at block 2826 and providing the information to the user 100 at block 2828 in accordance with the assigned information\nlevel(s).  For example, in one embodiment, a given information level may correspond to a potential romantic match, and the information provided to the user 100 may be information relevant to whether or not the at least one person is a romantic match.  In\nsuch an embodiment, the information provided to the user about the at least one person may include, for example, a name, age, gender, school(s) attended, mutual friends, common interests, dietary preferences, hair color, eye color, weight, height, etc.\nIn such an embodiment, another information level may correspond to someone who is not a potential romantic match, for example, because the gender of the at least one person does not match the gender preference of the user 100, the age gap between the\nuser 100 and the at least one person is too large, the at least one person is a family member, etc.\nIn some embodiments, the at least one person may include one or more persons.  For example, in some embodiments, separate affinity measurement(s) and/or information level(s) may be assigned to different persons identified in the captured images. More particularly, in one embodiment, a first person identified in the captured images may be assigned a first affinity measurement representing a degree of relationship between the user 100 and the first person.  A second person may also be identified\nin the captured images, and a second affinity measurement may be assigned to the second person.  The determination of the information level provided to the user may be based on both the first affinity measurement and the second affinity measurement.\nIdentifying a Verbal Contract\nIn some embodiments, wearable apparatus 110 may collect information related to an interaction between the user of the wearable apparatus 110 and at least one person detected in an environment of the user of the wearable apparatus 110.  For\nexample, in some embodiments, the wearable apparatus 110 may identify when the user and another person detected in the user's environment enter into a verbal contract.\nSome existing wearable device systems may encounter the technical problem of how to process the large amount of information collected by the wearable apparatus 110 and to use that information to provide useful feedback and/or services to the\nuser.  Some of the presently disclosed embodiments may address this problem by collecting visual and audio information, and using the collected information to determine which frames of the collected information to store for the user.  For example, in\nsome embodiments, the wearable apparatus 110 may store the collected information when the video and/or the audio information include features associated with a verbal contract.\nStill further, when a verbal contract has been detected, presently disclosed embodiments may address the problem of authenticating the identity of the user of the wearable apparatus 110 and/or the other party to the verbal contract.  For\nexample, when using automatically collected information regarding a verbal contract, it may be desirable to the user of the wearable apparatus 110 and/or the other party to the contract to register or log the existence of the contract.  However, given\nthe possibility of identity fraud, it may be desirable to log only contracts for which one or both of the parties are authenticated.  Presently disclosed embodiments may address this problem by using digital signatures or other electronic verifications\nof the identity of one or more of the parties to the contract.\nFIG. 29 is a diagram illustrating an example of memory 550 storing a plurality of modules, consistent with the disclosed embodiments.  The modules may be executable by at least one processing device to perform various methods and processes\ndisclosed herein.  Memory 550 may store more or fewer modules than those shown in FIG. 29.\nAs illustrated in FIG. 29, memory 550 may store software instructions to execute a data capture module 2901, a person identification module 2902, a contract identification module 2903, an action execution module 2904, a database access module\n2905, and may also include database(s) 2906.  Data capture module 2901 may include software instructions for receiving data from wearable apparatus 110.  Person identification module 2902 may include software instructions for analyzing data obtained by\nwearable apparatus 110 to identify subsets of the captured data including at least one person and information associated with the at least one person.  Contract identification module 2903 may include software instructions for analyzing images(s) and/or\naudio data capture by the wearable apparatus 110 to identify the presence of a verbal contract.  Action execution module 2904 may include software instructions to cause the occurrence of an action based on the information that a verbal contract has been\nidentified.  Database access module 2905 may include software instructions executable to interact with database(s) 2906, to store and/or retrieve information.\nData capture module 2901 may include software instructions for receiving data from a wearable apparatus, such as a wearable camera system and/or a wearable audio system.  Data received from a wearable camera system may include audio and image\ndata, captured, by, for example, an image sensor or microphone associated with the wearable camera system.  Image data may include raw images and may include image data that has been processed.  Raw images may be provided, for example, in the form of\nstill images and video data, either with or without embedded metadata.  In some embodiments, image data and audio data may be preprocessed prior to capture by data capture module 2901.  Preprocessing may include, for example, noise reduction, artifact\nremoval, compression, and other image pre-processing techniques.\nPerson identification module 2902 may be configured to analyze data captured by data capture module 2901 to detect or identify a subset of the captured data that includes at least one person.  In some embodiments, module 2902 may be configured\nto receive a plurality of images and to identify one or more of the plurality of images that include at least one person.  For example, module 2902 may receive a plurality of images of an environment surrounding a user wearing the wearable device 110 and\nidentify which of the plurality of images include at least one person, for example using person detection module 1701 described above, using person detection module 2001, using person detection module 2301, and so forth.\nIn some embodiments, such an analysis may be performed by employing a facial recognition algorithm designed to detect facial features (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person. \nIn other embodiments, the at least one person may be identified using a thermal signature algorithm designed to detect the presence of at least one person based on the heat generated by the at least one person.  In such embodiments, the wearable device\n110 may capture thermal images, either alone or in combination with visual images, for processing by the thermal signature algorithm.  Thermal recognition of the at least one person may be desirable in implementations in which the wearable device 110 is\noperating in reduced lighting situations.  In some embodiments, the at least one person may be identified through the application of one or more image classification techniques.  For example, at least one image classification technique may be used to\nclassify at least one feature of an image.  In some embodiments, an image classification technique may include one or more of image enhancement, edge detection, image analysis, and data extraction.  Specific examples of the methods for identifying at\nleast one person are exemplary only, and a person of ordinary skill in the art will recognize other methods for identifying the at least one person that remain consistent with the present disclosure.\nPerson identification module 2902 may further be configured to determine or obtain information associated with the at least one person identified in the image(s).  The information associated with the at least one person may include a name,\nnickname, social security number, account number, or any other identifier of the at least one person.  The identification information of the at least one detected person may be sourced from any suitable location, such as prior-stored information in\ndatabase 2906.  In some embodiments, the identification information may be obtained by matching the captured image(s) of the person with one or more images accessible via the Internet associated with a given identity, and then assigning that identity to\nthe at least one person detected in the image(s).  In some embodiments, the identification information may be obtained using identification information module 2302, using person identification module 2602, and so forth.\nIn some embodiments, person identification module 2902 may further obtain one or more profiles of the at least one person in the image(s).  Based on the one or more profiles of the at least one person, the person identification module 2902 may\nauthenticate the identity of the at least one person.  For example, in one embodiment, person identification module 2902 may access the digital signature of the at least one person to authenticate the at least one person.  In other embodiments, person\nidentification module 2902 may authenticate the identity of the at least one person based on one or more motions of a body part (e.g., a hand) of the at least one person, which may be a signature movement of the at least one person.  In another example,\nperson identification module 2902 may authenticate the identity of the at least one person based on a face image of the person and using a face recognition algorithm.  In yet another example, person identification module 2902 may authenticate the\nidentity of the at least one person using biometric readings captured by wearable apparatus 110 and by comparing the biometric readings with a biometric signature associated with the person, for example as retrieved from database 2906 using database\naccess module 2905.\nContract identification module 2903 may be configured to analyze the plurality of images identified by the person identification module 2902 to determine if the images reflect an action associated with the formation of a verbal contract. \nFurther, contract identification module 2903 may be configured to analyze at least a portion of the collected audio data to identify one or more sounds (e.g., words) associated with the formation of a verbal contract.  In some embodiments, the identified\nsounds may be linked to the user of the wearable apparatus 110 and/or the detected person in the collected image(s).  Still further, in some embodiments, contract identification module 2903 may utilize a combination of actions identified in the analyzed\nimages and sounds identified in the audio data to determine that a verbal contract has occurred between the user of the wearable apparatus 110 and the detected person.\nFor example, in one embodiment, the collected images may show that a handshake occurred between the user of the wearable apparatus 110 and another detected person.  The analysis of the audio data may show that one or more words associated with a\nverbal contract were spoken.  For example, the audio data may detect words such as \"I agree,\" \"yes,\" \"you have a deal,\" \"it's a deal,\" \"we're all set,\" \"I agree to the contract,\" \"those terms are acceptable to me,\" \"we agree,\" or any other words\nindicative of the formation of a verbal contract.  The one or more words indicative of a verbal contract may be used alone or in combination with one or more identified actions associated with a verbal contract in the captured images by the contract\nidentification module 2903 to determine that a verbal contract was agreed upon.\nSimilarly, in some embodiments, the collected images and/or audio data may be analyzed to determine that a verbal contract did not occur.  In such embodiments, the analysis of the collected images may show that no actions indicative of a verbal\ncontract were taken (e.g., no handshake occurred).  Further, the analysis of the audio data may include one or more words indicating the lack of a verbal contract, such as \"I don't agree,\" \"no,\" \"you don't have a deal,\" \"it's not a deal,\" \"there's no\ncontract,\" \"we'll have to keep working on this,\" etc. The one or more words indicating the lack of a verbal contract may be used alone or in combination with the collected images to determine that a contract is likely not to have occurred.\nAction execution module 2904 may be configured to perform a specific action in response to the identification of one or more images including the at least one person and/or the identification of one or more sounds indicative of a verbal\ncontract.  For example, action execution module 2904 may be configured to authenticate the identity of the user of the wearable apparatus 110, the at least one person identified in the collected images, and/or a witness to a verbal contract between the\nuser and the at least one person when a verbal contract has been identified by contract identification module 2903.  For example, in one embodiment, action execution module 2904 may obtain at least one profile of the user of the wearable apparatus 110\nand authenticate the user's identity based on the at least one profile.  The at least one profile may be any representation of identity that is suitable for linking the user and/or detected person to a given identity.  For example, in one embodiment,\nauthenticating the identity of the user may include analyzing one or more of the captured images, for example, to identify features in the image of the user's face that are unique to the user.  In another embodiment, authentication of the user may be\nperformed by analyzing the captured images to identify a motion of a body part (e.g., a hand) of the user and/or ego motion associated with the user.  In some embodiments, authentication of the user may use biometric readings captured by wearable\napparatus 110 and by comparing the biometric readings with a biometric signature associated with the user.\nIn some examples, authenticating the identity of the user may comprise identifying motion of a body part of the user (e.g., a hand, a head, etc.) from images captured by data capture module 2901, analyze the motion of the body part in the images\nto determine a distribution of motions (e.g., based on positions, relative positions, directions, relative directions, velocity, directional velocity, acceleration and/or deceleration, direction acceleration and/or deceleration, etc.), and comparing the\ndistribution to a known past distributions of the user.  For example, the distribution of motions may include a histogram of positions, relative positions, directions, relative directions, velocity, directional velocity, acceleration and/or deceleration,\ndirection acceleration and/or deceleration, and so forth.  In another example, the distribution of motions may include a statistical characteristic (such as mean, variance, entropy, etc.) of positions, relative positions, directions, relative directions,\nvelocity, directional velocity, acceleration and/or deceleration, direction acceleration and/or deceleration, and so forth.  Similarly, authenticating the identity of the at least one person may comprise identifying motion of a body part of the at least\none person (e.g., a hand, a head, etc.) from images captured by data capture module 2901, analyze the motion of the body part in the images to determine a distribution of motions (e.g., based on positions, relative positions, directions, relative\ndirections, velocity, directional velocity, acceleration and/or deceleration, direction acceleration and/or deceleration, etc., as described in details above), and comparing the distribution to a known past distributions of the at least one person.\nIn some embodiments, action module 2904 may authenticate the identity of at least one person identified in the collected images (which may be a side to the verbal contract, a witness, etc.) may include communicating with a device associated to\nthe at least one person (for example, with a wearable device worn by the at least one person), and obtaining identifying information and/or identity proof of the at least one person from the device associated with the at least one person.\nAction execution module 2904 may further be configured to register the verbal contract and the identification information associated with the detected person based on the authentication of the user, the detected person, and/or a witness to the\nverbal contract.  For example, in one embodiment, the verbal contract may be registered upon authentication of the user.  However, in other embodiments, the verbal contract may be registered when the identity of both the user and the detected person are\nidentified.  Still further, in some embodiments, the verbal contract may be registered when the identity of the user, the detected person, and the witness are authenticated.\nRegistration of the verbal contract may take on any of a variety of suitable forms, depending on implementation-specific considerations.  For example, in one embodiment, registering the verbal contract and the identification information of the\ndetected person may include storing at least a portion of the audio data associated with the verbal contract and at least part of the identification information associated with the detected person, for example, in database(s) 2906.  The portion of the\naudio data that is stored may include the audio data reflecting the terms of the contract, the offer made by one of the parties, the acceptance made by another of the parties, and any consideration given for the contract.  The portion of the\nidentification information that is stored may be any portion suitable to identify the person, such as a social security number, full legal name, nickname, etc.\nIn some embodiments, registration of the verbal contract and the identification information of the detected person may include transmitting at least a portion of the audio data associated with the verbal contract and at least part of the\nidentification information associated with the detected person using at least one communication device.  For example, in some embodiments, wearable apparatus 110 may include a communication device, such as one or more wireless transceivers, as discussed\nabove in connection with FIGS. 5A-5C, which may transmit information across network 240 to, for example, computing device 120 and/or server 250 In some embodiments, the registered verbal contract and identification information may be transmitted from the\ncommunication device to a longer term storage location, such as a cloud-based storage facility (e.g., server 250) via network 240.  In some embodiments, wearable apparatus 110 may transmit the information to a paired device (e.g., computing device 120),\nwhich may then in turn transmit the information to another destination (e.g., server 250) via network 240.\nIn some embodiments, registration of the verbal contract and the identification information of the detected person may include posting on a public database and/or a blockchain information based on at least a portion of the audio data associated\nwith the verbal contract and/or at least part of the identification information associated with the detected person.  The posted information may include at least a portion of the audio data associated with the verbal contract, at least part of the\nidentification information associated with the detected person, any other information related to the verbal contract (such as time, place, witnesses, context of the agreement, financial transfer and/or commitment, etc.), an encrypted version of any of\nthe above, a digitally signed version of any of the above, a digital signature of any of the above with or without the signed data, and so forth.  The posted information may also include any other information related to the contract.\nIn some embodiments, registration of the verbal contract and/or identification information may include digitally signing at least a portion of the audio data associated with the verbal contract and/or at least part of the identification\ninformation associated with the detected person.  Digitally signing may refer to any technique used to validate the authenticity and/or integrity of the data being signed.  For example, digitally signing may include applying a verified or previously\nauthenticated digital signature of the user, detected person, or witness to the audio data or identification information.  The digital signature may include a mark assigned to the user, detected person, or witness, for example, by a company that verifies\nthe identity of its customers.  For example, the company may have previously verified its customers' identities based on a review of government-issued documents, such as drivers' licenses, passports, etc.\nIn addition to the verbal contract and identification information, other data may also be registered by action execution module 2904.  For example, at least one clock may be configured to provide time information associated with the audio data,\nfor example, by timestamping the audio data as it is generated and/or collected with a date and/or time.  The time information may be registered with the audio data when the verbal contract is registered.  In other embodiments, additional details\nregarding the time, location, conditions, etc. surround the formation of the verbal contract may be registered.  For example, at least one positioning device may be configured to generate position information associated with the verbal contract.  The\npositioning device may be, for example, an accelerometer in a device paired with the wearable apparatus (e.g., the user's smartphone) configured to track the position of the user relative to the user's environment (e.g., that the user was sitting or\nstanding when the contract was established), a global positioning device configured to obtain the position of the wearable apparatus and/or a device paired with the wearable apparatus, and so forth.\nIn some embodiments, action execution module 2904 may recognize that the verbal contract comprise an obligation and/or a desire of a first entity (such as the user, the at least one person, a third party, etc.) to transfer funds to a second\nentity (such as the user, the at least one person, a third party, etc.), and cause the funds to be transferred from an account of the first entity to an account of the second entity.  In some examples, action execution module 2904 may also inform the\nfirst entity and/or the second entity about the transfer of funds, for example in an audible output, in an email, in a visual manner, and so forth.  In some examples, before transferring the funds, action execution module 2904 may ask the first entity\nexplicit permission, for example using a user interface, an audible user interaction bot, a graphical user interface, an email, etc.\nIn some embodiments, action execution module 2904 may provide information related to the verbal contract to parties involved in the verbal contract (such as the user, the at least one person, a witness, a third party, etc.).  For example, the\nprovided information may include a request to acknowledge and/or ratify the verbal contract, a summary of the verbal contract details, identifying information of parties of the verbal contract and/or witnesses to the verbal contract, a time and/or place\nassociated with the verbal contract, a reminder (for example in the form of a calendar event, a pop-up message, an email, etc.) to perform an action related to the verbal contract (such as an action the party obligated to take in the verbal contract).\nDatabase 2906 may be configured to store any type of information of use to modules 2901-2905, depending on implementation-specific considerations.  For example, in embodiments in which action execution module 2904 is configured to obtain the\nidentification information about the identified at least one person to the user of the wearable apparatus 110, database 2906 may store prior-collected information about the user's identity.  Further, the database 2906 may store the metadata associated\nwith the captured images and/or audio data.  In some embodiments, database 2906 may store the one or more images of the plurality of captured images that include the at least one person.  Database 2906 may further store some or all of the captured audio\ndata indicating formation of the verbal contract.  In other embodiments, database 2906 may be configured to store the profile of the user, detected person, and/or witness to the contract for reference when authenticating the identities of the parties to\nthe verbal contract and/or witnesses to the verbal contract.  For example, database 2906 may store one or more digital signatures.  Indeed, database 2906 may be configured to store any information associated with the functions of modules 2901-2905.\nModules 2901-2905 may be implemented in software, hardware, firmware, a mix of any of those, or the like.  For example, if the modules are implemented in software, they may be stored in memory 550.  However, in some embodiments, any one or more\nof modules 2901-2905 and data associated with database 2906, may, for example, be stored in processor 540 and/or located on server 250, which may include one or more processing devices.  Processing devices of server 250 may be configured to execute the\ninstructions of modules 2901-2905.  In some embodiments, aspects of modules 2901-2905 may include software, hardware, or firmware instructions (or a combination thereof) executable by one or more processors, alone or in various combinations with each\nother.  For example, modules 2901-2905 may be configured to interact with each other and/or other modules of server 250 and/or a wearable camera system to perform functions consistent with disclosed embodiments.  In some embodiments, any of the disclosed\nmodules may each include dedicated sensors (e.g., IR, image sensors, audio sensors, etc.) and/or dedicated application processing devices to perform the functionality associated with each module.\nFIG. 30 shows an example environment including wearable apparatus 110 for capturing and processing images and audio data.  In the depicted embodiment, user 100 may wear wearable apparatus 110 on his or her neck.  However, in other embodiments,\nwearable apparatus 110 may be differently positioned in any suitable location to enable capture of images of the user's environment, such as the locations explained in detail above.  User 100 may be in any location and engaging in any interaction\nencountered during user's daily activities.  For example, user 100 may be at a convenience store, grocery store, sports event, social event, work-related event, office, move theater, concert, etc. Wearable apparatus 110 may capture a plurality of images\ndepicting the environment to which the user is exposed while user 100 is engaging in his/her chosen activity.  Wearable apparatus 110 may also capture audio data via at least one audio sensor (e.g., a microphone) reflecting the sounds occurring in the\nenvironment surrounding the user.  For example, in the illustrated embodiment, wearable apparatus 110 may capture images that include a first person 3002 and/or a second person 3004 interacting with user 100.  As such, the images may show that the user\n100 is exposed to persons 3002 and 3004.  The images depicting the exposure of user 100 to particular persons 3002 and 3004 may be included in a log or otherwise saved in database 2906.  Wearable apparatus 110 may further capture audio sounds 3006 spoken\nby user 100 and/or audio sounds 3008 spoken by first person 3002.\nWearable apparatus 110 may also capture audio data reflecting one or more words spoken by user 100, first person 3002, and/or second person 3004.  As such, at least a portion of the audio data may reflect that a verbal contract has occurred\nbetween user 100 and first person 3002 by detecting one or more words indicative of contract formation.  For example, at least a portion of the audio data at a first timestamp may reflect that first person 3002 made an offer.  Another portion of the\naudio data at a second, later timestamp may indicate that user 100 accepted the offer.  Finally, another portion of the audio data may indicate that consideration was exchanged to support the offer and acceptance.\nStill further, in some embodiments, wearable apparatus 110 may capture images showing that a handshake occurred between user 100 and first person 3002.  This may be used either alone or in combination with the audio data indicating that a verbal\ncontract occurred to determine whether to register the verbal contract.  Additionally, wearable apparatus 110 may capture one or more words spoken by second person 3004, a witness to the verbal contract between user 100 and first person 3002.  For\nexample, witness 3004 may say one or more words indicating that witness 3004 believes a contract was formed, such as \"congratulations,\" \"I'm glad you were able to come to an agreement,\" \"it's wonderful that you agree,\" \"I'm excited that you decided to\nwork together,\" or any other words that indicate the belief on the part of witness 3004 that a contract has occurred.\nFIG. 30 shows user 100 being exposed to persons 3002 and 3004 while standing.  However, as would be understood by one of ordinary skill in the art, wearable apparatus 110 may capture images throughout the user's day at a variety of locations\nwith the user in a variety of positions as the environment surrounding the user changes.  For example, images may be captured when the user visits a restaurant for dinner, commutes to and from work, attends social events, attends work meetings, etc. In\nthis way, wearable apparatus 110 may be configured to monitor the environment surrounding user 100 throughout the user's activities to identify exposure to one or more persons throughout the time user wears wearable apparatus 110 and to capture audio\ndata associated with the user's daily activities.\nIn some embodiments, wearable apparatus 110 may be programmed to selectively collect data from at least one image sensor and at least one audio sensor to reduce the quantity of data collected that is irrelevant to the monitoring of verbal\ncontract formation.  For example, in one embodiment, the at least one image sensor may be activated to collect data, and the collected images may be processed to determine if the images include at least one person.  When at least one person is identified\nin the images, the at least one audio sensor may be triggered to collect data to capture any conversations that occur between user 100 and the at least one person.  In this way, the image and audio sensor(s) may be selectively controlled to address the\ntechnical problems associated with logging and storing large quantities of data that may be acquired by wearable apparatus 110 throughout use by user 100.\nFIG. 31A illustrates a flowchart of an exemplary method 3100 for receiving and analyzing image and/or audio data, consistent with embodiments of the present disclosure.  The method 3100 may be carried out, for example, by a processing device\nintegrated with and/or associated with wearable apparatus 110.  In such an embodiment, wearable apparatus 110 may include a wearable image sensor, e.g., image sensor 220, configured to capture a plurality of images from the environment of the user. \nWearable apparatus 110 may also include a wearable audio sensor configured to capture a plurality of sounds (e.g., one or more words) from the environment of the user.  In some other examples, the entire method 3100 or parts of method 3100 may be\nperformed by a device external to wearable apparatus 110, such as a device paired with wearable apparatus 110 (such as a smartphone, a tablet, etc.), a server communicating with wearable apparatus 110 (such as server 250), and so forth.  For exemplary\npurposes only, method 3100 for image and/or audio data is described herein with respect to processing device 210 cooperating with memory 550 to execute modules 2901-2905.\nIn accordance with method 3100, processor 210 may receive image data captured by a wearable image sensor at block 3102.  Block 3102 may be facilitated by software instructions of data capture module 2901.  Data capture module 2901 may be\nconfigured to execute instructions to receive image data from a wearable image sensor, and may also be configured to execute instructions to control the wearable image sensor.  Controlling the wearable image sensor may include issuing commands to record\nimages and/or videos, and may also include issuing commands to control an orientation or direction of viewing of the image sensor.\nReceived image data may be processed via software steps executed by person identification module 2902.  For example, at block 3104, person identification module 2902 may identify one or more images including the at least one person from a\nplurality of captured images.  For example, the at least one person may be person 3002, and the module 2902 may analyze the plurality of images to identify a subset of the captured images that include features sized, shaped, or otherwise resembling a\nperson.  Further, at block 3106, the one or more identified images may be processed to identify the particular person(s) depicted in the images flagged as including person(s).  In another example, at block 3104 and/or 3106, person identification module\n2902 may identify unique instances of the at least one person appearing in the plurality of captured images.\nAt block 3108, contract identification module 2903 may further analyze the subset of the captured images including the at least one person to identify one or more actions associated with a verbal contract.  For example, contract identification\nmodule 2903 may identify actions such as a handshake, head nod upward and downward, smiling, or any other physical movement cue associated with an offer or acceptance of a contract term.\nAt block 3110, processor 210 may receive audio data captured by a wearable image sensor.  Block 3110 may be facilitated by software instructions of data capture module 2901.  Data capture module 2901 may be configured to execute instructions to\nreceive audio data from a wearable image sensor, and may also be configured to execute instructions to control the wearable image sensor.  Controlling the wearable image sensor may include issuing commands to record audio data, and may also include\nissuing commands to control a collection volume or frequency of the audio sensor.  Received audio data may be processed via software steps executed by contract identification module 2903.  For example, at block 3112, contract identification module 2903\nmay identify at least a portion of the audio data including the at least one sound associated with the formation of a verbal contract.\nFor example, contract identification module 2903 may analyze the captured audio data to identify one or more portions that include one or more words associated with formation of a verbal contract, as discussed in detail above.  Further, in other\nembodiments, the one or more sounds identified in the audio data may be sounds associated with certain actions that support the conclusion that a verbal contract was formed.  For example, certain sound profiles or frequency components may be associated a\nhandshake of user 100 and person 3002 and/or the upward and downward movement of the hand of user 100 during the handshake with person 3002.  To that end, in some embodiments, wearable apparatus 110 may be located in other non-illustrated locations, such\nas proximate the user's hand, to better capture such audio data.\nFIG. 31B illustrates a flowchart of an exemplary method 3120 for authenticating the identity of one or more of the parties to the identified verbal contract, consistent with embodiments of the present disclosure.  The method 3120 may be carried\nout, for example, by a processing device integrated with and/or associated with wearable apparatus 110.  In some other examples, the entire method 3120 or parts of method 3120 may be performed by a device external to wearable apparatus 110, such as a\ndevice paired with wearable apparatus 110 (such as a smartphone, a tablet, etc.), a server communicating with wearable apparatus 110 (such as server 250), and so forth.  At block 3122, action execution module 2904 may obtain at least one profile of the\nuser 100 of wearable apparatus 110.  For example, action execution module 2904 may access a digital signature of user 100 stored in database 2906, as described above.  At block 3124, action execution module 2904 may authenticate an identity of the user\nbased on the accessed at least one profile.  For example, in one embodiment, the identity of user 100 may be authenticated by analyzing the captured images to identify a motion of at least one hand of the user.  In other embodiments, user 100 may be\nauthenticated by identifying ego motion associated with user 100.\nIn the illustrated embodiment, method 3120 further includes obtaining at least one profile of the detected person at block 3126.  For example, a digital signature of the detected person may be located in database 2906.  Based on the at least one\nprofile of the detected person, action execution module 2904 may authenticate the identity of the detected person at block 3128.  The identity of the detected person may be authenticated in a similar way to the authentication of user 100 described above. However, although the embodiment of method 3120 is illustrated with authentication of both user 100 and the detected person, in other embodiments, only one of the parties to the contract may be authenticated prior to registering the verbal contract.  For\nexample, the identity of user 100 may be known such that only the detected person is authenticated prior to contract registration.  In other embodiments, only the identity of user 100 may be authenticated prior to registration of the contract, for\nexample, to reduce or eliminate the likelihood that the individual wearing wearable apparatus 110 is not user 100.\nAt block 3130, the verbal contract and/or identification information of the detected person are registered if one or more registration conditions are met, for example using action execution module 2904 as described in details above.  The\nregistration conditions may be any suitable prerequisite to registration.  For example, the registration conditions may include authentication of user 100, authentication of the other party to the contract, presence of a detected witness, authentication\nof a detected witness, and so forth.  Further, registration may occur in any suitable manner.  For example, registration may include storing the image(s) and audio data evidencing the contract to database 2906, sending a confirmation of the verbal\ncontract to one or more parties, etc.\nFIG. 31C illustrates a flowchart of an exemplary method 3140 for identifying and/or authenticating the identity of one or more witnesses to the identified verbal contract, consistent with embodiments of the present disclosure.  The method 3140\nmay be carried out, for example, by a processing device integrated with and/or associated with wearable apparatus 110.  In some other examples, the entire method 3140 or parts of method 3140 may be performed by a device external to wearable apparatus\n110, such as a device paired with wearable apparatus 110 (such as a smartphone, a tablet, etc.), a server communicating with wearable apparatus 110 (such as server 250), and so forth.  At block 3142, action execution module 2904 may receive image data\ncaptured by a wearable image sensor.  Block 3142 may be facilitated by software instructions of data capture module 2901.  For example, data capture module 2901 may be configured to execute instructions to receive image data from a wearable image sensor.\nAt block 3144, the received image data may be processed via software steps executed by person identification module 2902.  For example, at block 3144, person identification module 2902 may identify one or more images including the at least one\nwitness from a plurality of captured images.  For example, the at least one witness may be person 3004, and the module 2902 may analyze the plurality of images to identify a subset of the captured images that include features sized, shaped, or otherwise\nresembling a person other than person 3002 and user 100.  Further, at block 3146, the one or more identified images may be processed to identify the particular person(s) that are witnesses and depicted in the images flagged as including person(s).\nThe identification information about the at least one witness may be sourced from any desired location.  For example, the information may be sourced from prior-sourced information stored in database(s) 2906 via database access module 2905.  For\nfurther example, the information may be sourced from one or more social media accounts.  In such embodiments, wearable device 110 may source the information publicly available via a web browser and/or through a private account of user 100.  In other\nembodiments, the information may be sourced from records of email, text, voicemail, or telephone communications between user 100 and the at least one identified witness.  Indeed, the information may be sourced from any suitable location, not limited to\nthose described herein, depending on implementation-specific considerations.  Once located, the at least part of the identification information may be registered at block 3148.  For example, the identification of the witness may be registered with the\nregistration of the verbal contract to indicate that the contract is verified.\nTransmitting Information Based on a Physical Distance\nIn some embodiments, wearable apparatus 110 may collect information related to at least one person or object detected in an environment of the user of the wearable apparatus 110.  The wearable apparatus 110 may then transmit information related\nto the at least one person or object based on an estimated physical distance from the user of the wearable apparatus to the at least one person or object.  Some existing wearable device systems may encounter the technical problem of how to process the\ninformation collected by the wearable device and use that information to provide useful feedback to the user.  For example, certain existing systems may capture images that include people or objects in the user's environment, but given the amount of\ncollected data and the likelihood that the majority of the data is not of interest to the user, fail to provide information pertinent to the user (e.g., information that the user finds useful or of interest).  Some of the presently disclosed embodiments,\non the other hand, may address this problem by providing information to the user based on the user's estimated physical distance to a particular person or object.  Such embodiments may make use of the estimated physical distance to determine whether a\nperson or object is likely relevant and/or of interest to the user and then provide information to the user on that basis.\nAs discussed above, system 200 may comprise a wearable apparatus 110, worn by user 100, and an optional computing device 120 and/or a server 250 capable of communicating with wearable apparatus 110 via a network 240.  Consistent with this\ndisclosure, wearable apparatus 110 may analyze image data to detect and identify an object or a person, may determine a distance or estimated distance from the user to the identified object or person, and may transmit information to, for example, update\na social media account, as described in greater detail below.  Wearable apparatus 110 may also transmit information to computing device 120, which may be, for example, a smartphone or tablet having a dedicated application installed therein.  A graphical\nuser interface (GUI) including, for example, a plurality of user-adjustable feature social media settings may be included on display 260 of computing device 120 to visibly output information to an operating user.\nFIG. 32 illustrates an exemplary embodiment of a memory containing software modules consistent with the present disclosure.  In particular, as shown, memory 3200 may include a detection module 3202, a distance module 3204, a transmission module\n3206, a database access module 3208, and a database 3210.  Modules 3202, 3204, 3206, and 3208 may contain software instructions for execution by at least one processing device, e.g., processor 210, included with a wearable apparatus (e.g., wearable\napparatus 110).  Detection module 3202, distance module 3204, and transmission module 3206, database access module 3208, and database 3210 may cooperate to detect an object, determine a distance to the object, and transmit information related to the\ndetected object.  In some embodiments, memory 3200 may be included in, for example, memory 550, discussed above.  Further, in other embodiments, the components of memory 3200 may be distributed over more than one location (e.g. stored in a server 250 in\ncommunication with, for example, network 240).\nIn some embodiments, detection module 3202 may detect a person or an object in the environment of the user of the wearable apparatus.  Detection module 3202 may operate in a manner similar to data capture module 2601 and person identification\nmodule 2602, as illustrated in FIG. 26 and discussed above.  For example, detection module 3202 may include software instructions for receiving data from wearable apparatus 110, such as a wearable camera system, and may include software instructions for\nanalyzing data obtained by wearable apparatus 110 to identify a person or an object associated with at least one person.  Data received from a wearable camera system may include audio and image data, captured, by, for example, an image sensor or a\nmicrophone associated with the wearable camera system and/or related to an audio topic.  Audio data captured by the microphone may identify an audio topic associated with the person.  Image data may include raw images and may include image data that has\nbeen processed.  Raw images may be provided, for example, in the form of still images and video data, either with or without embedded metadata.  In some embodiments, image data and audio data may be preprocessed prior to it being received by detection\nmodule 3202.  Preprocessing may include, for example, noise reduction, artifact removal, compression, and other image pre-processing techniques.\nIn some embodiments, detection module 3202 may detect or identify a subset or portion of the captured data that includes at least one person or object.  In some embodiments, detection module 3202 may be configured to receive a plurality of\nimages that include at least one person or object.  For example, detection module 3202 may receive a plurality of images of an environment surrounding a user wearing the wearable device 110 and may identify which of the plurality of images include at\nleast one person or object.\nIn some embodiments, detection analysis may be performed by executing a facial recognition algorithm designed to detect facial features (e.g., mouth, eyes, etc.), facial contours, paralinguistic indicators such as facial gestures or expressions,\nbody shape, or any other suitable identifying feature of a person.  In other embodiments, the at least one person may be identified using a thermal signature algorithm design to detect the presence of at least one person based on the heat generated by\nthe at least one person.  In such embodiments, the wearable device 110 may capture thermal images, either alone or in combination with visual images, for processing by the thermal signature algorithm.  Thermal recognition of the at least one person may\nbe desirable in implementations in which wearable device 110 is operating in reduced lighting situations.  In some embodiments, the at least one person may be identified through application of one or more classification techniques.  For example, at least\none image classification technique may be used to classify at least one feature of an image.  In some embodiments, an image classification technique may include at least one or more of image enhancement, edge detection, image analysis, and data\nextraction.  Specific examples of the methods for identifying at least one person or at least one object are exemplary only, and a person of ordinary skill in the art will recognize other methods for identifying the at least one person or object that\nremain consistent with present disclosure.\nIn some embodiments, the at least one person may be detected using a face detection algorithm, using a neural network trained to detect faces and/or persons in images, and so forth.  In some examples, the at least one person may be identified\nusing a face recognition algorithm, using a neural network trained to identify people in images, and so forth.  In other examples, the at least one object may be detected using an object detection algorithm, using a neural network trained to detect\nobjects and/or associated characteristics in images, and so forth.  In some examples, the at least one object may be identified using an object recognition algorithm, using a neural network trained to identify objects in images, and so forth.\nDetection module 3202 may be further configured to determine or obtain information associated with the at least one person or object identified in the image(s).  Information associated with the at least one person may include a name, nickname,\nsocial security number, account number, or any other identifier of the at least one person.  Information associated with the at least one object may include a length, width, depth, GPS position of an object, brand of an object, a value or cost of an\nobject, an occupancy of an object, or any other identifier or characteristic indicator of the at least one object.\nIn some embodiments, distance module 3204 may determine a measurement of an estimated physical distance of a detected person or object from the user of wearable apparatus 110.  In some examples, the distance to a detected person and/or object\nmay be estimated using depth imaging, such as: stereo, active stereo, LIDAR, and so forth.  Stereo imaging may include use of spatially separated multiple cameras to form images from different directions.  Depth information may then be extracted from the\ndifferences in the images to determine a measurement of an estimated physical distance.  In some embodiments, active stereo imaging may include a range of pulse techniques to measure a particular distance to a point of a person and/or object, and may\ninclude, for example, laser pulse or laser line scans, radar, and ultrasound.\nIn other embodiments, LIDAR techniques may be employed in accordance with software instructions from distance module 3204 to determine a measurement of an estimated physical distance to a person or an object.  LIDAR relates generally to systems\nand processes for measuring distances to a target person or object by illuminating the target person or object with laser light and detecting the reflection of the light.  For example, a pulsed laser light device, which may be included in wearable\napparatus 110, may emit light incident upon a surface of a person or an object, and pulsed light reflected from the surface of the person or object may be detected at a receiver.  A timer may measure an elapsed time from light being emitted from the\nlaser light device to the reflection reaching the receiver.  Based on a measurement of the elapsed time and the speed of light, processor 210 may be able to calculate the distance to the target person or object.\nIn some embodiments, a receiver in a LIDAR system of distance module 3204 of wearable apparatus 110 may be equipped with sensors such as avalanche photodiodes (APD) to detect reflected light pulses at particular wavelengths.  LIDAR systems may\nalso include a scanning mechanism so that the incident laser may scan over multiple points on the target person or object, and may generate 3-D point clouds that include object distance or depth information.  Mechanical LIDAR systems are well known in\nthe art and include mechanical scanning mechanisms to acquire distance information at multiple points of coverage, and may be incorporated as part of wearable apparatus 110.\nIn other embodiments, wearable apparatus 110 may include a mechanical rotatable LIDAR system that may include an upper scanning mechanism and a fixed lower part to determine a distance in accordance with software instructions from distance\nmodule 3204.  The upper scanning mechanism may include a predetermined number of laser-detector pairs, and may rotate at a fixed frequency to determine an estimated distance to a person or object.  Software instructions from distance module 3204 may\nchange the operation of a number of laser-detector pairs and may change the frequency of rotation in order to capture additional data and provide additional distance measurements.\nIn other embodiments, the distance to a detected person and/or object may be estimated by distance module 3204 based on the size in pixels of the person and/or object in the captured images, the position in the captured images, and/or based on\nan estimation of the physical size of the person and/or object.  For example, if the size in pixels of the person and/or object in the captured images is determined to be large or covering a large pixel area, a short distance to the person and/or object\nmay be estimated.  Conversely, if the size in pixels of the person and/or object in the captured images is determined to be small or covering a small pixel area, a far distance to the person and/or object may be estimated.  Similarly, if a position of\nthe person and/or object in the captured images is determined to be in the foreground, a short distance to the person and/or object may be estimated.  Conversely, if a position of the person and/or object in the captured images is determined to be in the\nbackground, a far distance to the person and/or object may be estimated.  Indeed, the distance measurement may be estimated in any suitable manner relating to a person and/or object, not limited to the examples herein, depending on\nimplementation-specific considerations.\nIn some embodiments, transmission module 3206 may transmit, according to the determined distance measurement, information related to the detected person and/or object.  For example, information may be communicated or transmitted from wearable\napparatus 110 to a paired device, such as computing device 120, or an external server, such as server 250.  In some embodiments, wearable apparatus 110 may include a communication device, such as one or more wireless transceivers, as discussed above in\nconnection with FIGS. 5A-5C, which may transmit information across network 240 to, for example, computing device 120 and/or server 250\nIn some embodiments, transmission module 3206 may determine whether to transmit information based on at least the determined distance estimated by distance module 3204.  For example, transmission module 3206 may determine whether to transmit\ninformation in accordance with a predetermined distance threshold.  If, for example, it is determined that an estimated distance to a person or object exceeds a particular predetermined distance estimate (e.g., greater than 1 meter, greater than 5\nmeters, greater than 10 meters, etc.), information may not be transmitted to the user.  Alternatively, if, for example, it is determined that an estimated distance to a person or object is within a particular predetermined distance estimate (e.g., less\nthan 5 meters, less than 2 meters, less than 1 meter, etc.), information may be transmitted to the user.\nThe information transmitted by transmission module 3206 may include any meaningful data extracted from the image and may include, for example, a person's identifier, name, job title, gender, interests, hobbies, political affiliation (e.g.,\nwhether the user and the at least one person have worked together in the past), leisure related information (e.g., whether the user and the at least one person have played sports together in the past, whether the user and the at least one person are\npredicted to be a successful match, whether the at least one person is single, etc.), matchmaking information (e.g., whether the user and the at least one person have dated in the past), or any other information about the at least one person that is\navailable to the wearable device 110.  The information transmitted by transmission module 3206 may also include, for example, any meaningful data related to a detected object such as a description of the object, value of the object, brand name of the\nobject, and any other information about the at least one object that is available to the wearable device 110.  In some examples, the information transmitted by transmission module 3206 may include images depicting the at least one person and/or the\nobject.  For example, the portions of the image data identified by detection module 3202 as depicting the at least one person and/or the object may be transmitted by transmission module 3206.  In some examples, the information transmitted by transmission\nmodule 3206 may include properties related to the at least one person and/or object identified by analyzing the images.\nIn some embodiments, detection module 3202 may detect multiple persons and/or objects, distance module 3204 may determine measurements of estimated physical distances of the detected persons and/or objects from the user of wearable apparatus\n110, and transmission module 3206 may determine whether to transmit information based on at least the distances estimated by distance module 3204.  For example, transmission module 3206 may determine a threshold based on the person and/or object with the\nsmallest estimated physical distance (for example, twice the estimated physical distance, three times the estimated physical distance, etc.), transmit information related to persons and/or objects corresponding to estimated physical distances smaller\nthan the determined threshold, and withhold transmission of information related to persons and/or objects corresponding to estimated physical distances greater than the determined threshold.  In another example, transmission module 3206 may cluster the\ndistances estimated by distance module 3204, for example using a clustering algorithm, and perform different actions with information related to persons and/or objects corresponding to different clusters, for example the actions may include transmitting\ninformation related to persons and/or objects corresponding to one cluster, providing audible output to a wearer of wearable apparatus 110 about persons and/or objects corresponding to a second cluster, storing information related to persons and/or\nobjects corresponding to a third cluster, ignoring information related to persons and/or objects corresponding to a fourth cluster, and so forth.\nIn some embodiments, database access module 3208 may cooperate with database 3210 to retrieve a plurality of captured images or any type of information.  Database 3210 may be configured to store any type of information of use to modules\n3202-3208, depending on implementation-specific considerations.  For example, in embodiments in which action execution database access module 3208 is configured to provide the information about the identified at least one person or object to the user of\nthe wearable apparatus 110, database 3210 may store prior-collected information about the user's social, familial, or other contacts.  Further, database 3210 may store the metadata associated with the captured images.  In some embodiments, database 3210\nmay store the one or more images of the plurality of captured images that include the at least one person or object.  In some embodiments, database 3210 may store images of known persons, places, or objects, which may be compared with one or more images\ncaptured by wearable apparatus 110.  Indeed, database 3210 may be configured to store any information associated with the functions of modules 3202-3208.\nModules 3202-3208 may be implemented in software, hardware, firmware, a mix of any of those, or the like.  For example, if the modules are implemented in software, they may be stored in memory 550, as shown in FIG. 32.  However, in some\nembodiments, any one or more of modules 3202-3208 and data associated with database 3210, may, for example, be stored in processor 540 and/or located on server 250, which may include one or more processing devices.  Processing devices of server 250 may\nbe configured to execute the instructions of modules 3202-3208.  In some embodiments, aspects of modules 3202-3208 may include software, hardware, or firmware instructions (or a combination thereof) executable by one or more processors, alone or in\nvarious combinations with each other.  For example, modules 3202-3208 may be configured to interact with each other and/or other modules of server 250 and/or a wearable camera system to perform functions consistent with disclosed embodiments.  In some\nembodiments, any of the disclosed modules may each include dedicated sensors (e.g., IR, image sensors, etc.) and/or dedicated application processing devices to perform the functionality associated with each module.\nFIG. 33A is a schematic illustration 3300 of an example of a user wearing a wearable apparatus and capturing an image of a person 3222 according to a disclosed embodiment.  User 100 may wear a wearable apparatus 110 consistent with an embodiment\nof the present disclosure (as shown in FIG. 9).  Capturing unit 710 may be located on an exterior surface of the clothing 750 of user 100.  Capturing unit 710 may also be connected to power unit 720 (not seen in this illustration) via connector 730,\nwhich wraps around an edge of clothing 750.  Wearable apparatus 110 may be differently positioned in any suitable location to enable capture of images of the user's environment, such as the locations explained in detail above.  User 100 may be in any\nlocation and engaging in any interaction encountered during user's daily activities.  For example, user 100 may be at a convenience store, grocery store, sports event, social event, work-related event, movie theater, concert, etc. Wearable apparatus 110\nmay capture a plurality of images depicting the environment to which the user is exposed while user 100 is engaging in his/her chosen activity.  For example, in the illustrated embodiment, wearable apparatus 110 may capture images that include person\n3222.\nA captured image may be analyzed to estimate at least one of: an age of person 3222, a height of person 3222, a weight of person 3222, a gender of person 3222, and so forth.  For example, the analysis may determine the gender of person 3222 is\nmale.  In other embodiments, an image may be analyzed to identify at least one of: an action associated with the person, a product associated with the person, a facial expression of the person, an emotional state of the person, and/or other\nparalinguistic indicators.  For example, as shown in FIG. 33A, the analysis may determine person 3222 is smiling and in a happy emotional state.  A captured image may be analyzed according to any digital processing techniques, as discussed above with\nregard to detection module 3202, to identify person 3222 and to measure an estimated physical distance, using any mechanisms as discussed above with regard to distance module 3204, from the user to person 3222.\nIn some embodiments, capturing unit 710 may capture an image of person 3222 and may estimate a distance D to person 3222.  Distance D to detected person 3222 may be estimated in accordance with instructions from distance module 3204 to implement\nat least one of depth imaging, such as: stereo, active stereo, LIDAR, and so forth.  For example, in one embodiment, wearable apparatus 110 may include at least one LIDAR sensor.  As discussed above, wearable apparatus 110 may be equipped with LIDAR\nsensors such as avalanche photodiodes (APD) to detect reflected light pulses at particular wavelengths.  LIDAR systems may also include a scanning mechanism so that an incident laser may scan over multiple points on the target person, and may generate\n3-D point clouds that include object distance or depth information to provide an estimate of distance D to person 3222.\nIn some examples, distance D to detected person 3222 may be estimated based on the size in pixels of person 3222 in the captured images, the position of person 3222 in the captured images, and possibly on an estimation of the physical size\n(height and weight) of person 3222.  For example, if the size in pixels of the person in the captured images is determined to be large or covering a large pixel area, a short distance to the person may be estimated.  Conversely, if the size in pixels of\nthe person in the captured images is determined to be small or covering a small pixel area, a far distance to the person may be estimated.\nIn some embodiments, an affinity measurement to the person may be determined based, at least in part, on estimated distance D. For example, the degree to which user 100 likes or dislikes person 3222 may be estimated based on distance D. When\nuser 100 likes person 3222, distance D may be small, whereas when user 100 dislikes person 3222, distance D may be large.  A social graph (not shown) may also be updated based on the affinity measurement.  Additional details regarding affinity\nmeasurements are provided above in connection with FIGS. 26-28B.\nFIG. 33B is a schematic illustration of an example of an update of a social network profile according to a disclosed embodiment.  Social network profile 3310 may be displayed as part of a personalized social media page and linked to a user's\nsocial media account.  In some cases, social network profile 3310 may include the name 3320 of user 100, and may further include a captured image of person 3222 during an update as part of the social network profile 3110.  As shown case in FIG. 33B, user\n3320 \"Mike Smith\" is linked to his social network profile 3310, and social network profile 3310 may include features such as a \"News Feed,\" \"Message,\" \"Business,\" \"Shortcuts,\" \"Events,\" \"Groups,\" \"Saved,\" and \"Pages\" functions to customize and control\nsocial network profile 3310.  User 3320 may post textual, photo, and video content, as well as share any emoticons or other paralinguistic indicators expressing a current emotional state of user 3320.\nIn some embodiments, social network profile 3310 may include a notification or alert to other users that user 3320 is in the company of the person 3222.  For example, as shown in FIG. 33B, the notification may include a textual posting 3380 that\n\"Mike Smith is with Joe Johnson\" and may include a pictorial posting of person 3222 or an image of \"Joe Johnson.\" This update may be triggered when it is determined that user 100 is within a predetermined threshold distance D to person 3222.  The updated\nposting may remain until it is determined that user 100 is no longer within a predetermined threshold distance D to person 3222.  This determination may be made based on image analysis of a plurality of images of person 3222 over a selected period of\ntime.  The update may include only one or both of textual and pictorial changes to the social network profile 3310.  In addition, the size of person 3222 as displayed within social network profile 3310 may increase or decrease based on a decrease or\nincrease of distance D from user 100 to person 3222.  For example, the size of person 3222 as displayed may be proportional to the distance D, to log(D), to exp((D-A)/B) for some constants A and B, to f(D) for some monotonically increasing function f,\nand so forth.  The update may further include additional alerts or notifications sent out directly to friends included the social network of user 100, and the alerts or notifications may also be distributed as part of a shareable feed that friends in the\nsocial network of user 100 may be able to subscribe to and/or follow.  Mutual friends, and friends not directly part of the social network of user 100 may also be able to receive alerts or notifications based on adjustable social media settings set by\nuser 100.\nFIG. 33C is a schematic illustration 3330 of an example of a user wearing a wearable apparatus and capturing an image of an object in the environment of the user according to a disclosed embodiment.  In some embodiments, a plurality of images\ncaptured from an environment of a user of a wearable apparatus may be obtained.  The plurality of images may be analyzed to detect an object.  Tent 3332 may be included in a plurality of captured images.\nCapturing unit 710 may capture an image of an object or tent 3332 and may estimate a distance D to tent 3332.  Distance D to detected tent 3332 may be estimated using depth imaging, such as: stereo, active stereo, LIDAR, and so forth.  For\nexample, in one embodiment, wearable apparatus 110 may include at least one LIDAR sensor.  As discussed above, wearable apparatus 110 may be equipped with LIDAR sensors such as avalanche photodiodes (APD) to detect reflected light pulses at particular\nwavelengths.  LIDAR systems may also include a scanning mechanism so that an incident laser may scan over multiple points on the target object, and may generate 3-D point clouds that include object distance or depth information to provide an estimate of\ndistance D to object 3332.  In some examples, distance D to detected tent 3332 may be estimated based on the size in pixels of tent 3332 in the captured images, the position of tent 3332 in the captured images, and possibly on an estimation of the\nphysical size (length, width, and depth) of tent 3332.  A proximity measure to tent 3332 may be based, at least in part, on estimated distance D. For example, the degree to which user 100 is near or far from tent 3332 may be determined based on distance\nD. In addition, information associated with distance D measurement may be stored in memory and/or transmitted to an external device.\nIn some embodiments, captured audio data may be captured by a microphone and analyzed to identify audio topics associated with tent 3332.  For example, captured audio data may include sounds from nature such as rushing river rapids or other\nsounds determined near to tent 3332, and may indicate the location of tent 3332 and/or associated camping grounds.  Consistent with this disclosure, captured lighting data such as intensity or brightness may also be analyzed to determine a time of date,\nsunrise, sunset, and so forth.\nFIG. 33D is a schematic illustration of an example of an update of a social network profile according to a disclosed embodiment.  Social network profile 3340 may be displayed as part of a personalized social media page and linked to a user's\nsocial media account.  In some cases, social network profile 3340 may include the name 3344 of user 100, and may further include an image of tent 3332 during an update as part of social network profile 3340.  As shown in FIG. 33D, user 3344 \"Mike Smith\"\nis linked to his social network profile 3340, and social network profile 3340 may include features such as a \"News Feed,\" \"Message,\" \"Business,\" \"Shortcuts,\" \"Events,\" \"Groups,\" \"Saved,\" and \"Pages\" functions to customize and control social network\nprofile 3340.  User 3344 may post textual, photo, and video content, as well as share any emoticons or other paralinguistic indicators expressing a current emotional state of user 3344 in association with the detection of object or tent 3332.\nIn some embodiments, a social network profile 3340 update may include a notification or alert to other users that user 3344 is positioned in the location of tent 3332.  For example, as shown in FIG. 33D, the notification may include a textual\nposting 3390 that \"Mike Smith is at Local Campgrounds,\" and may include a pictorial posting of tent 3332.  This update may be triggered when it is determined that user 3344 is within a predetermined threshold distance D to tent 3332.  The updated posting\nmay remain until it is determined that user 3344 is no longer within a predetermined threshold distance D to tent 3332.  This determination may be made based on image analysis of a plurality of images of tent 3332 over a period of time.  The update may\ninclude only one or both of textual and pictorial changes to the social network profile 3340.  In addition, the size of tent 3332 as displayed within social network profile 3340 may increase or decrease based on a decrease or increase of distance D from\nuser 100 to tent 3332.  For example, the size of tent 3332 as displayed may be proportional to the distance D, to log(D), to exp((D-A)/B) for some constants A and B, to f(D) for some monotonically increasing function f, and so forth.  In other\nembodiments, images may be analyzed to estimate at least one of: a location of an object, such as tent 3332, a GPS position of an object, a brand of an object, a value or cost of an object, an occupancy status of an object, or other characteristic\nindicators.\nFIG. 34 is a flowchart of an example of a method 3400 for providing information to a user of a wearable apparatus.  Steps of method 3400 may be performed by one or more processors of server 250 and/or memory 550 and memory modules 3200.\nAt step 3402, detection module 3202 may detect a person or an object in the environment of the user of the wearable apparatus.  Detection module 3202 may operate in a manner similar to data capture module 2601 and person identification module\n2602, as illustrated in FIG. 26.  Detection module 3202 may include software instructions for receiving data from wearable apparatus 110, such as a wearable camera system, and may include software instructions for analyzing data obtained by wearable\napparatus 110 to identify a person or an object associated with at least one person.  An object may include, for example, person 3222 or tent 3332.  In some embodiments, detection module 3202 may detect or identify a subset or portion of the captured\ndata that includes person 3222 or object 3332.  In some embodiments, detection module 3202 may be configured to receive a plurality of images that include person 3222 or object 3332.  For example, detection module 3202 may receive a plurality of images\nof an environment surrounding a user wearing the wearable device 110 and may identify which of the plurality of images include person 3222 or object 3332.\nIn some embodiments, detection analysis may be performed by executing a facial recognition algorithm designed to detect facial features (e.g. mouth, eyes, etc.), facial contours, paralinguistic indicators such as facial gestures or expressions,\nbody shape, or any other suitable identifying feature of person 3222.  The plurality of images may be analyzed to estimate an age of person 3222, a height of person 3222, a weight of person 3222, a gender of person 3222, an action associated with person\n3222, a product associated with person 3222, and an action associated with tent 3332 and person 3222 associated with tent 3332.  The analysis may include extraction of meaningful data and may employ digital processing techniques to identify a person or\nan object captured in a digital image.\nAt step 3404, distance module 3204 may determine a measurement of an estimated physical distance from the user to the detected person and/or object.  For example, wearable apparatus 110 may include at least one LIDAR sensor.  As discussed above,\nwearable apparatus 110 may be equipped with LIDAR sensors such as avalanche photodiodes (APD) to detect reflected light pulses at particular wavelengths.  LIDAR systems may also include a scanning mechanism so that an incident laser may scan over\nmultiple points on the target person, and may generate 3-D point clouds that include object distance or depth information to provide an estimate of distance D to person 3222 or tent 3332.  In other examples, the distance to detected person and/or object\nmay be estimated based on the size in pixels of person 3222 or tent 3332 in the captured images, the position in the captured images, and possibly on an estimation of the physical size of person 3222 or tent 3332.  In some embodiments, the plurality of\nimages may depict a second object in the environment of the user of the wearable apparatus, and may determine a second distance measurement for basing a transmission.  The second measurement may include an estimated physical distance from the user to the\nsecond object.\nAt step 3406, transmission module 3206 may transmit, based on the determined physical distance measurement, information related to the detected person and/or object.  As discussed, in some embodiments, transmission module 3206 may determine\nwhether to transmit information in accordance with a predetermined distance threshold (e.g., when the determined distance measurement is within the predetermined distance threshold).  Information may be communicated or transmitted from wearable apparatus\n110 via a communication device (e.g., wireless transceiver 530) to a paired device, such as computing device 120, or an external server, such as server 250.  In some embodiments, transmission module 3206 may provide information to the user of the\nwearable apparatus 110 based on information associated with the at least one person or object identified in the captured images.\nIn some embodiments, information related to the detected object may be transmitted to update at least one of a social graph and a social network profile.  This update may be triggered when it is determined that user 100 is within a predetermined\nthreshold distance to to a person or object.  The update may remain in place until it is determined that user 100 is no longer within a predetermined threshold distance to the person or object, for a selected time period, until another notification\narrives, and so forth.  The update may include a notification of a textual and/or pictorial posting, and may be based on information related to the detected person or object.\nIn some embodiments, step 3406 of method 3400 may be replaced by other steps performing other actions with information related to detected persons and/or objects based on the determined physical distance measurement.  For example, method 3400\nmay provide audible information about detected persons and/or objects when the determined physical distance measurement corresponding to the detected person and/or object is smaller than a selected threshold.  In another example, the level of details\nprovided to a user (for example, as audible and/or visual output) may be determined on the determined physical distance measurement corresponding to the detected person and/or object, for example as described above in connection with FIGS. 26-28B.\nProviding a Social Media Recommendation\nIn some embodiments, wearable apparatus 110 may analyze one or more images captured by a wearable image sensor included in the wearable apparatus, obtain information based on analysis of the one or more captured images, and generate one or more\ncontact recommendations for at least one new social network contact based on the obtained information.  In some embodiments, the one or more contact recommendations may include new social network contact recommendations, and the at least one new social\nnetwork contact may be a member of one or more social networks.\nSome existing wearable device systems may encounter the technical challenge of how to process the information collected by the wearable device and use that information to provide useful feedback to the user.  Some of the presently disclosed\nembodiments may address this problem by providing social media recommendations to the user based on image data captured by a wearable device.  As such, the recommendations may be targeted as they may be based on information related to persons and/or\nobjects encountered by the user of the wearable device.\nFor example, in one embodiment, wearable apparatus (e.g. wearable apparatus 110) may analyze images for providing social media recommendations based on images captured in the environment of a user.  The analysis may include extraction of\nmeaningful data and may employ digital processing techniques to obtain information captured in a digital image that may be relevant to providing a social media recommendation.  In some embodiments, the obtained information may include identity\ninformation of at least one person present in the environment of the user.  At least one contact recommendation of the user may be made based on an interaction with identified people and a length of the interaction with identified people.  The at least\none contact recommendation of the user may also include a new contact in a social network.  In other embodiments, the obtained information may include information related to an activity or object present in the environment of the user of the wearable\napparatus.  For example, the activity may include reading a book and an object may include a book written by an author.  The obtained information may further include information related to the author of the book.\nAs discussed above, system 200 may comprise a wearable apparatus 110, worn by user 100, and an optional computing device 120 and/or a server 250 capable of communicating with wearable apparatus 110 via a network 240.  Consistent with this\ndisclosure, apparatus 110 may analyze image data to obtain information captured in a digital image and may provide a social media recommendation, as described in greater detail below.  Apparatus 110 may also transmit information to computing device 120,\nwhich may be, for example, a smartphone or tablet having a dedicated application installed therein.  A graphical user interface (GUI) including a plurality of user-adjustable feature social media settings may be included on display 260 of computing\ndevice 120 to visibly output social media recommendations to an operating user.  Additionally or alternatively, server 250 may receive information based on image data captured by wearable apparatus 110, server 250 may analyze the received information to\nprovide a social media recommendation, as described in greater detail below, and transmit information to computing device 120 associated with a user of wearable apparatus 110, which may be, for example, a smartphone or tablet having a dedicated\napplication installed therein.\nFIG. 35 illustrates an exemplary embodiment of a memory containing software modules consistent with the present disclosure.  In particular, as shown, memory 3500 may include an analysis module 3502, an information module 3504, a recommendation\nmodule 3506, a database access module 3508, and a database 3510.  Modules 3502, 3504, 3506, and 3508 may contain software instructions for execution by at least one processing device, e.g., processor 210, included with a wearable apparatus (e.g.,\nwearable apparatus 110).  Analysis module 3502, information module 3504, recommendation module 3506, database access module 3508, and database 3510 may cooperate to analyze a captured image, obtain information based on the analysis, and generate and\nprovide a contact recommendation.  The contact recommendation may be generated for a user of a wearable apparatus.  In some embodiments, the contact recommendation may be generated for the user of the wearable apparatus and an other person.  Further, the\ncontact recommendation may be for a new social network contact.  In some embodiments, memory 3500 may be included in, for example, memory 550, discussed above.  Further, in other embodiments, the components of memory 3500 may be distributed over more\nthan one location (e.g. stored in a server 250 in communication with, for example, network 240).\nIn some embodiments, analysis module 3502 may analyze at least one image captured by a wearable image sensor included in the wearable apparatus from an environment of a user of the wearable apparatus.  Analysis module 3502 may operate in a\nmanner similar to detection module 3202, as illustrated in FIG. 32 and discussed above.  Analysis module 3502 may include software instructions for receiving data from wearable apparatus 110, such as a wearable camera system, and may include software\ninstructions for analyzing data obtained by wearable apparatus 110 to identify a person, activity, or an object associated with at least one person.  Data received from a wearable camera system may include audio and image data, captured, by, for example,\nan image sensor or a microphone associated with the wearable camera system and/or related to an audio topic.  Audio data captured by the microphone may identify an audio topic associated with the person.  Image data may include raw images and may include\nimage data that has been processed.  Raw images may be provided, for example, in the form of still images and video data, either with or without embedded metadata.  In some embodiments, image data and audio data may be preprocessed prior to capture by\nanalysis module 3502.  Preprocessing may include, for example, noise reduction, artifact removal, compression, and other image pre-processing techniques.\nIn some embodiments, analysis module 3502 may detect or identify a subset or portion of the captured data that includes at least one person, activity, or object.  In some embodiments, analysis module 3502 may be configured to receive a plurality\nof images that include at least one person or object.  For example, analysis module 3502 may receive a plurality of images of an environment surrounding a user wearing the wearable device 110 and may identify which of the plurality of images include at\nleast one person or object.\nIn some embodiments, analysis may be performed by performing a facial recognition algorithm designed to detect facial features (e.g. mouth, eyes, etc.), facial contours, paralinguistic indicators such as facial gestures or expressions, body\nshape, or any other suitable identifying feature of a person.  In other embodiments, at least one person may be identified using a thermal signature algorithm design to detect the presence of at least one person based on the heat generated by the at\nleast one person.  In such embodiments, the wearable device 110 may capture thermal images, either alone or in combination with visual images, for processing by the thermal signature algorithm.  Thermal recognition of the at least one person may be\ndesirable in implementations in which wearable device 110 is operating in reduced lighting situations.  In some embodiments, at least one person may be identified through application of one or more classification techniques.  For example, at least one\nimage classification technique may be used to classify at least one feature of an image, such as a particular activity of the user, such as reading a book.  In some embodiments, an image classification technique may include at least one or more of image\nenhancement, edge detection, image analysis, and data extraction.\nIn some examples, the at least one person may be detected using a face detection algorithm, using a neural network trained to detect faces and/or persons in images, and so forth.  In some examples, the at least one person may be identified using\na face recognition algorithm, using a neural network trained to identify people in images, and so forth.  In other examples, the at least one object may be detected using an object detection algorithm, using a neural network trained to detect objects\nand/or associated characteristics in images, and so forth.  In some examples, the at least one object may be identified using an object recognition algorithm, using a neural network trained to identify objects in images, and so forth.  Specific examples\nof the methods for identifying at least one person or at least one object, such as a book, are exemplary only, and a person of ordinary skill in the art will recognize other methods for identifying the at least one person or object that remain consistent\nwith present disclosure.\nInformation module 3504 may then obtain information based on a result of the analysis of the at least one captured image.  For example, information module 3504 may be further configured to determine or obtain information associated with the at\nleast one person or object identified in the image(s).  Information module 3504 may, for example, access a local database (e.g., database 3510) and/or one or more remote databases (e.g., available via server 250) to search for information based on the\nanalysis.  Information that may be obtained for the at least one person may include a name, nickname, social security number, account number, or any other identifier of the at least one person.  Information obtained for the at least one object may\ninclude a length, width, depth, GPS position of an object, brand of an object, a value or cost of an object, an occupancy of an object, or any other identifier or characteristic indicator of the at least one object.  In some embodiments, information\nmodule 3504 may obtain or determine at least one activity of the person in relation to the object, such as a person reading a book, and search a local or remote database based on the activity or information related to the activity (e.g., an author of the\nbook being read).\nRecommendation module 3506 may then generate at least one contact recommendation for the user based on the obtained information.  In some embodiments, recommendation module 3506 may generate at least one contact recommendation for at least one\nnew social network contact based on the obtained information.  For example, the at least one contact recommendation may include a recommendation for a new social network contact for the user and/or for a person other than the user.  In some embodiments,\nthe new social network contact and the user and/or the person other than the user may be members of one or more common social networks.\nRecommendation module 3506 may facilitate transmission of obtained information and a corresponding recommendation.  For example, information may be communicated or transmitted from wearable apparatus 110 to a paired device, such as computing\ndevice 120, to a device associated with the user of wearable apparatus 110 and/or a person other than the user, to an external server (such as server 250), and so forth.\nIn some embodiments, recommendation module 3506 may provide a social media recommendation to the user of the wearable apparatus 110 based on information associated with the at least one person or object identified in the captured images.  The at\nleast one other person may also receive a social media recommendation.\nThe at least one contact recommendation may include, for example, any one or more of a person's name, job title, gender, interests, hobbies, political affiliation (e.g., whether the user and the at least one person have worked together in the\npast), leisure related information (e.g., whether the user and the at least one person have played sports together in the past, whether the user and the at least one person are predicted to be a successful match, whether the at least one person is\nsingle, etc.), matchmaking information (e.g., whether the user and the at least one person have dated in the past), etc. The at least one contact recommendation may also include, for example, any meaningful data related to a detected object or activity\nsuch as one or more of a description of the object, value of the object, brand name of the object, etc.\nIn some embodiments, recommendation module 3506 may generate at least one contact recommendation for the user based also on an affinity between the user and the at least one person in a social network, for example as calculated by step 2810\ndescribed above.\nIn some embodiments, database access module 3508 may cooperate with database 3510 to retrieve a plurality of captured images or any type of information.  Database 3510 may be configured to store any type of information of use to modules\n3502-3508, depending on implementation-specific considerations.  For example, in embodiments in which database access module 3508 is configured to provide the information about a detected person, database 3510 may store prior-collected information about\nthe detected person's social, familial, or other contacts.  Further, database 3510 may store the metadata associated with the captured images.  In some embodiments, database 3510 may store the one or more images of the plurality of captured images that\ninclude the at least one person or object.  In some embodiments, database 3510 may store a social graph, such as a social graph of a social network.  Indeed, database 3510 may be configured to store any information associated with the functions of\nmodules 3502-3510.\nModules 3502-3508 may be implemented in software, hardware, firmware, a mix of any of those, or the like.  For example, if the modules are implemented in software, they may be stored in memory 550, as shown in FIG. 35.  However, in some\nembodiments, any one or more of modules 3502-3508 and data associated with database 3510, may, for example, be stored in processor 540 and/or located on server 250, which may include one or more processing devices.  Processing devices of server 250 may\nbe configured to execute the instructions of modules 3502-3508.  In some embodiments, aspects of modules 3502-3508 may include software, hardware, or firmware instructions (or a combination thereof) executable by one or more processors, alone or in\nvarious combinations with each other.  For example, modules 3502-3508 may be configured to interact with each other and/or other modules of server 250 and/or a wearable camera system to perform functions consistent with disclosed embodiments.  In some\nembodiments, any of the disclosed modules may each include dedicated sensors (e.g., IR, image sensors, etc.) and/or dedicated application processing devices to perform the functionality associated with each module.\nFIG. 36A is a schematic illustration of an example 3600 of a user wearing a wearable apparatus and capturing an image of a person according to a disclosed embodiment.  User 100 may wear a wearable apparatus 110 consistent with an embodiment of\nthe present disclosure (as shown in FIG. 9).  Capturing unit 710 may be located on an exterior surface of the clothing 750 of user 100.  Capturing unit 710 may also be connected to power unit 720 (not seen in this illustration) via connector 730, which\nwraps around an edge of clothing 750.\nAs shown, capturing unit 710 may capture an image including person 3622 and a processor may determine an identity of a person 3622 for providing a social media recommendation.  Further, capturing unit 710 may also capture book 3626 and a\nprocessor may determine an author of book 3626.\nWearable apparatus 110 may be differently positioned in any suitable location to enable capture of images of the user's environment, such as the locations explained in detail above.  User 100 may be in any location and engaging in any\ninteraction encountered during user's daily activities.  For example, user 100 may be at a convenience store, grocery store, sports event, social event, work-related event, movie theater, concert, etc. Wearable apparatus 110 may capture a plurality of\nimages depicting the environment to which the user is exposed while user 100 is engaging in his/her chosen activity.  For example, in the illustrated embodiment, wearable apparatus 110 may capture images that include person 3622.\nA captured image may be analyzed to obtain information in accordance with software instructions from information module 3504.  For example, an image 3630 may be analyzed to obtain information including an age of person 3622, a height of person\n3622, a weight of person 3622, a gender of person 3622, facial features of person 3622 suitable for facial recognition analysis, and so forth.  For example, the analysis may determine the gender of person 3622, named \"Veronica\" is female.  In other\nembodiments, an image may be analyzed to identify at least one of: an action or activity associated with the person, a product associated with the person, a facial expression of the person, an emotional state of the person, and/or other paralinguistic\nindicators.  For example, the analysis may determine \"Veronica\" is reading a book.  Information module 3504 may obtain information from image 3630 that may include information related to an activity present in the environment of the user of the wearable\napparatus, information related to an object present in the environment of the of the wearable apparatus, and identity information of person 3622 present in the environment of the user of the wearable apparatus.  For example, information module 3504 may\nobtain information including information related to the author of the book \"Veronica\" is reading.\nFIG. 36B is a schematic illustration of an example of generating a contact recommendation 3610 according to a disclosed embodiment.  For example, in some embodiments, recommendations for new contacts for a user of a wearable apparatus may be\nbased on obtained information from image data captured using the wearable apparatus.  For example, people and objects appearing in captured image data may be identified, and social media recommendations may be made based on their identity.  In some\nembodiments, wearable apparatus 110 may transmit information including social media recommendations across network 240 to computing device 120 and/or server 250, and recommendation module 3506 may provide a social media recommendation to the user of the\nwearable apparatus 110 based on information associated with the at least one person, activity, or object identified in the captured images.\nAs shown in FIG. 36B, a GUI displaying a contact recommendation of a profile image 3630 of person 3622 identified in the captured image as \"Veronica\" may be provided to the user of the wearable image device 110.  Contact recommendation 3610 may\nbe made based on obtained information.  For example, the obtained information may indicate that the book \"Veronica\" is reading is by an author that the user has read.  The contact recommendation may source user author information from database 3510 in\naccordance with the obtained information prior to providing recommendation 3610.  As shown in FIG. 36B, the user may select button 3624 to add \"Veronica\" as a friend to his social network.  The user may also select \"Comments,\" \"Information,\" and\n\"Buddies\" as displayed in the GUI to see comments relating to \"Veronica,\" obtained information leading to contact recommendation 3610, and buddies or other contacts that \"Veronica\" has in her network before deciding to add \"Veronica\" to his social\nnetwork.  In some examples, the system may access a list of favorite authors of the user in database 3510, may determine if the author of the book \"Veronica\" is reading is in the list, and may decide whether to provide contact recommendation 3610 based\non said determination, for example providing contact recommendation 3610 if the author is in the list, and not to provide contact recommendation 3610 or to turn to other decision rules if the author is not in the list.  In some examples, the system may\nmeasure a distance to \"Veronica\", for example using distance module 3204 described above, and may decide whether to provide contact recommendation 3610 based on the measured distance, for example withholding contact recommendation 3610 when the measured\ndistance is larger than a selected distance.  In some examples, the system may determine whether the person is physically present, for example using physical presence identification module 2002 and/or step 2210 described above, and may decide whether to\nprovide contact recommendation 3610 based on whether the person is present.  In some examples, the system may determine whether the person is visible on a display, for example using physical presence identification module 2002 and/or step 2240 described\nabove, and may decide whether to provide contact recommendation 3610 based on whether the person is visible on a display.  In some examples, the system may determine whether the person is visible in a photo, for example using physical presence\nidentification module 2002 described above, and may decide whether to provide contact recommendation 3610 based on whether the person is visible in a photo.\nFIG. 36C is a schematic illustration of an example 3634 of a user wearing a wearable apparatus 110 and capturing an image of person 3520 in the environment of the user according to a disclosed embodiment.  User 100 may wear a wearable apparatus\n110 consistent with an embodiment of the present disclosure (as shown in FIG. 9) and discussed above in connection with FIG. 36A.\nA show in FIG. 36C, capturing unit 710 may capture an image including person 3520 and a processor may determine an identity of a person 3520 for providing a social media recommendation.  As shown in FIG. 36C, an image of \"Sally Cooper\" 3520 is\ncaptured and information from her image may be gathered in the form of a social media recommendation discussed in FIG. 36D.\nFIG. 36D is a schematic illustration of an example of generating a contact recommendation 3640 according to a disclosed embodiment.  In some embodiments, a contact recommendation may be made based on a level or length of interaction with at\nleast one other person.  For example, social media recommendations may be made and further based on the type of interaction with identified people, on the length of the interaction, and so forth.  As shown in FIG. 36D, the user's interaction with person\n3520 may yield a social media recommendations.  As shown in FIG. 36D, a recommendation to add a new contact \"Sally Cooper\" may be provided.\nAs shown, an invite friend window 3642 with an invite friend button 3644 based on a suggestion text 3646, e.g. \"Mike Smith suggests the following friends\" may be displayed in a GUI.  This recommendation may be made to a person other than the\nuser or the person in the captured image.  For example, rather than providing a social media recommendation directly to person 3520, a social media suggestion to add \"Sally Cooper\" is provided to another person (e.g., a contact of the user of the\nwearable apparatus, e.g., Mike Smith).\nIn some embodiments, second information based on a second plurality of images captured by one or more wearable apparatuses may be obtained.  For example, second information may include information relating to the stretching, exercising, or Yoga\nposture associated with person 3520.  Generating the at least one recommendation may also be based on the second information.  For example, since person 3520 likes to exercise, a social recommendation may be made to friends of the user of the wearable\napparatus who also like to exercise according to information stored in database 3510.  In some examples, a plurality of wearable apparatuses may be associated with a plurality of users to generate a plurality of social media content recommendations.  In\nother embodiments, generating the at least one recommendation may also be based on identity of people appearing in the plurality of images.  In some examples, a plurality of wearable apparatuses may be associated with a plurality of users, information\nbased on image data captured by two or more wearable apparatuses may be analyzed (for example by analyzing profile 2400 described above), and generating the at least one recommendation may also be based on the analysis results.\nFIG. 37 is a flowchart of an example of a method 3700 for providing recommendations based on captured images, consistent with disclosed embodiments.  Steps of method 3700 may be performed by one or more processors of server 250 and/or memory 550\nor memory modules 3500.\nAt step 3702, analysis module 3702 may analyze at least one image captured by a wearable image sensor.  For example, as discussed earlier, analysis module 3702 may analyze the at least one image to detect a person, an object, and/or an activity. Analysis of a person included in the at least one image may be performed by performing a facial recognition algorithm designed to detect facial features (e.g. mouth, eyes, etc.), facial contours, paralinguistic indicators such as facial gestures or\nexpressions, body shape, or any other suitable identifying feature of a person.\nAt step 3704, information module 3504 may obtain information based on the result of the analysis of the captured image.  For example, obtained information in accordance with software instructions from information module 3504 may include an age,\na height, a weight, a gender, etc., of a person detected in the at least one image.  In some embodiments, the obtained information may include information related to an activity or an object present in the environment of the user of the wearable\napparatus.  For example, the object may include a book written by an author and the obtained information may include information related to the author (e.g., stored in database 3510).  In some examples, the obtained information may also include identity\ninformation of at least one other person present in the environment of the user of the wearable apparatus 110.\nAt step 3706, recommendation module 3706 may generate at least one contact recommendation of the user of the wearable apparatus 110 and/or a person other than the user.  The other person may be a person who is known to or a new social network\ncontact of the user.  As discussed earlier, the at least one contact recommendation may include a recommendation for a new social network contact (e.g., a person who is a member of a social network of which the user and/or the at least one other person\nmay also be members).\nAt step 3708, recommendation module may provide the contact recommendation to the user and/or the other person.  For example, recommendation module 3506 may provide a social media recommendation to the user of the wearable apparatus 110 based on\ninformation associated with the at least one person or object identified in the captured images.  In some embodiments, the at least one contact recommendation of the user may be based on an interaction with identified people and a length of interaction. \nThe contact recommendation may include a recommendation for the user in a user window to add a new friend as shown in FIG. 36B, and/or based on a detected identify of a user in a captured image.  Alternatively, as shown in FIG. 36D, the contact\nrecommendation may be based on the identity of the user in the captured image and provided to another person or user.  This recommendation may include a mutual friend of the user and the identified person at a person deemed worthy of the friendship of\nthe identified person.  In some embodiments, the contact recommendation of the user may include a new contact in a social network.  The contact recommendation of the user may also include an existing contact in a social network.  The contact\nrecommendation may be provided to one or both of the user and at least one other person.\nProviding Visual Feedback of a Field of View\nIn some embodiments, a wearable apparatus (e.g., wearable apparatus 110) may include at least one image sensor and at least one projector configured to move together, and the projector may be configured to emit a light pattern that shows the\nborders of the field of view of the at least one image sensor.\nIn some examples, the at least one projector may be controlled by a processing unit, such as processing unit 210.  The processing unit may activate and/or deactivate the at least one projector.  In some examples, the at least one projector may\nbe activated and/or deactivated based, at least in part, on a visual trigger appearing in images captured using the at least one image sensor.  In some examples, the at least one projector may be activate and/or deactivated based, at least in part, on\nvisual context associated with images captured using the at least one image sensor.\nIn some embodiments, a wearable apparatus (e.g., wearable apparatus 110) may obtain a plurality of images captured from an environment of a user of the wearable apparatus.  The plurality of images may be analyzed to identify a visual trigger. \nAt least one projector may be configured to project a light pattern, for example based on the identification of the visual trigger.  In some examples, the light pattern may be indicative of a field of view of an image sensor.  In some examples, the light\npattern may comprise two parallel lines; for example, two parallel lines showing two borders of the field of view of the image sensor.  In some examples, the light pattern may comprise two perpendicular lines; for example, two perpendicular lines showing\ntwo borders of the field of view of the image sensor.  In some examples, the light pattern may comprise lines or dashes in a rectangular pattern; for example, a rectangular pattern showing the borders of the field of view of the image sensor.\nAs discussed earlier, the visual trigger that may activate and/or deactivate the at least one light projector may comprise a hand gesture.  In some examples, the visual trigger may comprise a textual document appearing in the plurality of\nimages.  In some examples, a first visual trigger may appear in at least a first one of a plurality of images to activate at least one projector.  The at least one projector may also be deactivated, for example, after being activated for a selected\nduration, after a second visual trigger is identified, and so forth.  The second trigger may appear in at least a second of the plurality of images.\nFIG. 38A provides a diagrammatic view of wearable apparatus 110, including glasses 130 fitted with image sensor system 220, which may include a light projector 3802, such as the light projector described above.  As discussed, light projector\n3802 may provide feedback to the user of wearable apparatus 110 of a field of view associated with one or more image sensors provided on the wearable apparatus.  In such a way, the user may be informed, through a visual guide, of where the image sensor\nor image sensors of the apparatus are aimed, what those sensors \"see,\" and/or what area of text or object that the processing device may effectively analyze through processing of images captured by the image sensor(s).\nFor example, in some embodiments, a wearable apparatus (e.g., wearable apparatus 110) equipped with light projector 3802 may assist a visually impaired user of the wearable apparatus by providing the user with an indication of a field of view of\none or more image sensors included in the wearable apparatus.  Such information may help the user provide input to the wearable apparatus through the use of gestures, which may act as triggers for causing the wearable apparatus to undertake certain\nactions.\nAlthough FIG. 38A depicts light projector 3802 as surrounding image sensor system 220, light projector 3802 may be arranged in an appropriate manner or location on wearable apparatus 110.  For example, light projector 3802 may be located above,\nbelow, or beside image sensor system 220.  In some embodiments, light projector 3802 may be attached to a side surface of wearable apparatus 110 (e.g., positioned on a surface other than the surface including image sensor system 220) or included in a\ncomponent attached to wearable apparatus 110.  Any appropriate location and positioning of light projector 3802 is contemplated.\nIn some embodiments, light projector 3802 and image sensor system 220 may be configured to move with the head of the user, for example by being connected and/or mounted to glasses 130 as depicted in FIG. 38A.\nAlthough the example shown on FIG. 38A shows apparatus 110, including glasses 130 fitted with image sensor system 220, in other embodiments, a light projector (such as light projector 3802) may be included in any of the variations of wearable\napparatus 110 shown in FIGS. 1-16, and in the example shown in FIG. 38B.\nFIG. 38B shows an example of a wearable apparatus that can be secured to an article of clothing, consistent with disclosed embodiments.  In particular, FIG. 38B provides a diagrammatic view of a wearable apparatus 3800, including a capturing\nunit 3804 fitted with image sensor system 220.  Capturing unit 3804 may be located on an exterior surface of the clothing 750 of user 100.  Capturing unit 3804 may be connected to power unit 720 (not seen in this illustration) via a connector, which may\nwrap around an edge of clothing 750.  Capturing unit 3804 may further include a light projector, such as light projector 3802 described above.  Consistent with this disclosure, light projector 3802 may generate light and project light onto a surface,\nobject, text, etc., in an environment of the user.  Although FIG. 38B depicts light projector 3802 as surrounding image sensor system 220, light projector 3802 may be arranged in an appropriate manner or location on wearable apparatus 3800, as discussed\nabove in connection with FIG. 38A.\nLight projector 3802 may be configured to include any component or components capable of generating light and projecting light onto a surface, object, text, etc., in an environment of the user.  In some embodiments, light projector 3802 may\ninclude a light emitting diode (LED).  In some embodiments, light projector 3802 may include an array of LEDs.  The array of LEDs may be positioned in any suitable arrangement, such as around an aperture of an image sensor associated with system 220, for\nexample.  In some cases, light projector 3802 may include one or more light path altering structures.  For example, light projector 3802 may include one or more lenses to direct light from a light source onto a surface or object, etc., in an environment\nof the user along a desired light path.  Light projector 3802 may also include one or more light hoods or shrouds to aid in selectively illuminating only a portion of a surface or object in an environment of the user (e.g., by blocking light or otherwise\nreducing or limiting a light field emitted from one or more light sources of light projector 3802).\nIn addition to LEDs, light projector 3802 may also include one or more solid state lasers.  Such lasers (and/or LEDs) may be used to illuminate a fixed area relative to a surface or object in an environment of the user.  In other embodiments,\nhowever, lasers and/or LEDs may be configured to scan at least a portion of a surface or object in an environment of the user.  For example, such scanning light sources may be scanned over a particular pattern such that portions of the pattern are\nilluminated at different times.  A scan may be associated with a lighting profile comparing light intensity over the scan pattern as a function of time.  At relatively low scan rates (e.g., 30 Hz or below), the scanning of the pattern may be perceptible\nto a user.  At higher scan rates (e.g., 30 Hz or above, 60 Hz or above, or even higher), scanning of the pattern may be more difficult to discern.  For certain light sources and scan rates, the scanned light pattern may appear to a user as a continuously\nilluminated pattern.\nFIG. 39 provides a diagrammatic illustration of one example of a type of visual feedback that light projector 3802 in FIG. 38A may provide to a user of wearable apparatus 110.  For example, light projector 3802 may generate a light projection\npattern 3902 that illuminates one or more surfaces or objects in an environment of the user.  As shown in FIG. 39, light projection pattern 3902 is illuminating a text book 3906 including multiple lines of text 3908 with a light incident pattern 3904. \nAs shown, light incident pattern 3904 includes a series of dashed lines roughly arranged in a rectangular outline pattern.  Light projector 3802 may provide visual feedback to the user to indicate that wearable apparatus 110, including one or more image\nsensors and/or one or more processing devices associated with wearable apparatus 110, is able to capture and/or process images from a field of view at least partially overlapping with an area bounded by light incident pattern 3904.  In some cases, image\ncapture area can be substantially the same as the area bounded by light incident pattern 3904.  In this way, a user may better understand what the apparatus \"sees\" and, therefore, whether the apparatus will be able to provide information relative to an\nobject, etc., within the environment of the user.  If the light incident pattern 3904 does not align with an area of interest to the user, the user can reposition his or her head and facing direction until light incident pattern 3904 surrounds or covers\nan area of interest (e.g., text, bank note, object portion, person, etc.) in an environment of the user.\nIn some embodiments, as discussed above, wearable apparatus 110 may be securable to clothing, such as shown in FIG. 39B.  In such embodiments, wearable apparatus 110 and light projector 3802 may operate in a substantially similar manner as shown\nin FIG. 39 to project a light projection patter that illuminates one or more surfaces or objects in an environment of a user.\nMoreover, light incident pattern 3904 is not limited to the dashed outline pattern shown in FIG. 39.  Rather, any suitable illumination pattern for providing visual feedback to the user may be employed.  FIGS. 40A-40H provide several examples of\nvarious patterns that can be generated by light projector 3802.  For example, light incident pattern 3904 can have a circular or elliptical outline pattern (FIG. 40A), a rectangular/square outline pattern (FIG. 40B), a dashed pattern (FIG. 40C), one or\nmore illuminated dots (FIG. 40D), a rectangular/square solid illumination pattern (FIG. 40E), a circular or elliptical solid illumination pattern (FIG. 40F), two parallel horizontal lines (FIG. 40G), two parallel vertical lines (FIG. 40H), or any other\nsuitable pattern.\nLight incident pattern 3904 may also comprise one or more colors.  In some embodiments, pattern 3904 may be created by white light, red light, green light, blue light, or any other color or combination of colors.\nThere may also be various ways to initiate illumination by light projector 3802.  In some cases, light projector 3802 may be illuminated in response to a user input.  For example, the user may manually activate a switch, button, etc. to change\nillumination states of the light projector 3802 (e.g., from OFF to ON or from ON to OFF).  In some cases, the user may control operation of light projector 3802 through voice commands.  In addition to changes in illumination state, other operational\ncharacteristics of the light projector may also be controlled by the user.  For example, the user may control a brightness level associated with light generated by the light projector, may change colors of one or more portions of light incident pattern\n3904, or may control any other operational characteristic of light projector 3802.  As discussed above, operation of light projector 3802 may occur automatically.  For example, light projector 3802 may be activated, deactivated, dimmed, etc. by at least\none processing device in response to a detected trigger (e.g., a gesture by the user, including pointing, hand wave, or any other gesture), in response to a detected type of object (e.g., text, bank note, etc.), in response to an object detected in the\nuser's hand, or any other type of trigger.  In another example, light projector 3802 may be activated, deactivated, dimmed, etc. by at least one processing device in response to a determination that a text and/or an object of interest is partially\ncaptured by an image sensor, for example as determined by analyzing images captured using the image sensor.  Light projector 3802 may also be controlled based on an amount of time that an object remains present in a field of view of the user.  For\nexample, if an object lingers in a field of view of the user for more than 1 second, 2 seconds (or any other suitable time threshold), then the at least one processing device may determine that the user would like information about the object and may\ntake one or more actions relative to the object, including illuminating at least a portion of the object with an incident light pattern 3904.  As discussed in FIG. 5A, apparatus 110 may also include microphone, which may initiate illumination by light\nprojector 3802.  For example, a processing device may change, based on a voice command captured at the microphone, an illumination state of light projector 3802, a color of the light pattern, or a shape of the light pattern.\nAlignment of light incident pattern 3904 with an active field of view of one or more image sensors associated with wearable apparatus 110 may be accomplished with an adjustment component and/or in various ways.  For example, in some embodiments,\nthe user may be prompted (e.g., by audible signals or voice instructions) generated by at least one processor in response to the processor's analysis of one or more images captured by an image capture device (e.g., camera) associated with apparatus 110. \nSuch an alignment procedure may occur through the processor causing light projector to turn ON and analyzing at least one image captured by a camera associated with apparatus 110 to detect the presence of light incident pattern 3904 on a surface or\nobject represented by the captured image.  If the light incident pattern is fully present in the captured image, then no alignment may be needed.  However, if only a portion of the incident pattern present in the captured image, the processor may\ninstruct the user (e.g., voice commands, audible signals, visible guides, etc.) to manipulate one or more alignment controls (set screws, knobs, etc.) until the incident pattern is sufficiently present in one or more subsequently captured images.  Such\nan alignment process can also be fully automatic.  That is, rather than instructing the user to manipulate one or more adjustments to align light incident pattern 3904 with an operational field of view of the image capture device(s), the processor may\nautomatically control one or more adjusters to align light incident pattern 3904 with the field of view of the image capture device(s).  Such adjustors may include, e.g., micromotors for adjusting set screws, splined or threaded rods (e.g., screw drive\nunits), piezoelectric steppers, or any other type of electromechanical adjustment device.\nFIG. 41 is a flowchart of an example of a method 4100 for providing visual feedback to a user of a wearable apparatus, such as wearable apparatus 110 or wearable apparatus 3800, consistent with disclosed embodiments.  Some of the steps of method\n4100 may be performed by at least one processor, which may execute software instructions stored in, for example, memory 550.\nAt step 4102, the wearable apparatus may capture, via image sensor system 220 included in the wearable apparatus, a plurality of images from an environment of the user of the wearable apparatus.  For example, glasses 130 may be fitted with image\nsensor system 220, as shown in FIG. 38A.  Alternatively, as shown in FIG. 38B, capturing unit 3804 may be fitted with image sensor system 220.  Capturing unit 3804 may be located, for example, on an exterior surface of the clothing 750 of user 100 and as\nsuch may be positioned so that image sensor system 220 may capture images from the environment of user 100.\nAt step 4104, the at least one processor may activate a projector included in the wearable apparatus based on a visual trigger appearing in a plurality of images.  In some embodiments, the activation may occur in response to a user input.  The\nuser input may include a voice command or an input associated with a depressible button or other input device of the wearable apparatus.  In some embodiments, the activation may be based at least one a visual trigger appearing in at least one of the\nplurality of images or based on at least an amount of time that an object remains present in the active field of view of the at least one image sensor.  In some embodiments, the activation may be based on a determination that images captured in step 3802\ninclude only partial view of a text and/or an object of interest.  For example, a determination that images captured in step 3802 include only partial view of a text may be made based on a detection of a partial view of letters at the edge of the images. In another example, a determination that images captured in step 3802 include only partial view of a text may be made based on a natural language processing of textual information obtained by analyzing the images using OCR algorithms.  In a third\nexample, a determination that images captured in step 3802 include only partial view of an object may be made by analyzing the images with a classifier and/or a neural network trained to identify and/or detect partial views of objects.\nAt step 4106, light projector 3802 may emit a light pattern to visually indicate to the user of the wearable apparatus an active field of view of the image sensor.  For example, the light pattern may include a circular or elliptical outline\npattern, a rectangular outline pattern, a square outline pattern, a dashed pattern, one or more illuminated dots, a rectangular solid illumination pattern, a square solid illumination pattern, a circular solid illumination pattern, an elliptical solid\nillumination pattern, two parallel horizontal lines, or two parallel vertical lines.  In some embodiments, the light pattern may coincide with one or more borders of the active field of view of the at least one image sensor.  In some embodiments, the\nlight pattern may also be included with the active field of view of the at least one image sensor or the light pattern may substantially overlap with the active field of view of the at least one image sensor.\nThe foregoing description has been presented for purposes of illustration.  It is not exhaustive and is not limited to the precise forms or embodiments disclosed.  Modifications and adaptations will be apparent to those skilled in the art from\nconsideration of the specification and practice of the disclosed embodiments.  Additionally, although aspects of the disclosed embodiments are described as being stored in memory, one skilled in the art will appreciate that these aspects can also be\nstored on other types of computer readable media, such as secondary storage devices, for example, hard disks or CD ROM, or other forms of RAM or ROM, USB media, DVD, Blu-ray, Ultra HD Blu-ray, or other optical drive media.\nComputer programs based on the written description and disclosed methods are within the skill of an experienced developer.  The various programs or program modules can be created using any of the techniques known to one skilled in the art or can\nbe designed in connection with existing software.  For example, program sections or program modules can be designed in or by means of .Net Framework, .Net Compact Framework (and related languages, such as Visual Basic, C, etc.), Java, C++, Objective-C,\nHTML, HTML/AJAX combinations, XML, or HTML with included Java applets.\nMoreover, while illustrative embodiments have been described herein, the scope of any and all embodiments having equivalent elements, modifications, omissions, combinations (e.g., of aspects across various embodiments), adaptations and/or\nalterations as would be appreciated by those skilled in the art based on the present disclosure.  The limitations in the claims are to be interpreted broadly based on the language employed in the claims and not limited to examples described in the\npresent specification or during the prosecution of the application.  The examples are to be construed as non-exclusive.  Furthermore, the steps of the disclosed methods may be modified in any manner, including by reordering steps and/or inserting or\ndeleting steps.  It is intended, therefore, that the specification and examples be considered as illustrative only, with a true scope and spirit being indicated by the following claims and their full scope of equivalents.", "application_number": "15793370", "abstract": " The present disclosure relates to systems and methods for selecting an\n     action based on a detected person. In one implementation, a wearable\n     apparatus may include a wearable image sensor configured to capture a\n     plurality of images from the environment of the user of the wearable\n     apparatus and at least one processing device. The at least one processing\n     device may be programmed to analyze at least one of the plurality of\n     images to detect the person; analyze at least one of the plurality of\n     images to identify an attribute of the detected person; select at least\n     one category for the detected person based on the identified attribute;\n     select at least one action based on the at least one category; and cause\n     the at least one selected action to be executed.\n", "citations": ["7395507", "20050259035", "20120290950", "20140347265", "20150201181", "20160055499", "20160246378"], "related": ["62413103", "62418296", "62418300", "62439899", "62546141"]}, {"id": "20180115075", "patent_code": "10374316", "patent_name": "System and dielectric antenna with non-uniform dielectric", "year": "2019", "inventor_and_country_data": " Inventors: \nBennett; Robert (Southold, NY), Gerszberg; Irwin (Kendall Park, NJ), Barzegar; Farhad (Branchburg, NJ), Willis, III; Thomas M. (Tinton Falls, NJ), Barnickel; Donald J. (Flemington, NJ), Henry; Paul Shala (Holmdel, NJ)  ", "description": "<BR><BR>FIELD OF THE DISCLOSURE\nThe subject disclosure relates antennas and systems used for wireless communication.\n<BR><BR>BACKGROUND\nAs smart phones and other portable devices increasingly become ubiquitous, and data usage increases, macrocell base station devices and existing wireless infrastructure in turn require higher bandwidth capability in order to address the\nincreased demand.  To provide additional mobile bandwidth, small cell deployment is being pursued, with microcells and picocells providing coverage for much smaller areas than traditional macrocells.\nIn addition, most homes and businesses have grown to rely on broadband data access for services such as voice, video and Internet browsing, etc. Broadband access networks include satellite, 4G or 5G wireless, power line communication, fiber,\ncable, and telephone networks. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nReference will now be made to the accompanying drawings, which are not necessarily drawn to scale, and wherein:\nFIG. 1 is a block diagram illustrating an example, non-limiting embodiment of a guided-wave communications system in accordance with various aspects described herein.\nFIG. 2 is a block diagram illustrating an example, non-limiting embodiment of a transmission device in accordance with various aspects described herein.\nFIG. 3 is a graphical diagram illustrating an example, non-limiting embodiment of an electromagnetic field distribution in accordance with various aspects described herein.\nFIG. 4 is a graphical diagram illustrating an example, non-limiting embodiment of an electromagnetic field distribution in accordance with various aspects described herein.\nFIG. 5A is a graphical diagram illustrating an example, non-limiting embodiment of a frequency response in accordance with various aspects described herein.\nFIG. 5B is a graphical diagram illustrating example, non-limiting embodiments of a longitudinal cross-section of an insulated wire depicting fields of guided electromagnetic waves at various operating frequencies in accordance with various\naspects described herein.\nFIG. 6 is a graphical diagram illustrating an example, non-limiting embodiment of an electromagnetic field distribution in accordance with various aspects described herein.\nFIG. 7 is a block diagram illustrating an example, non-limiting embodiment of an arc coupler in accordance with various aspects described herein.\nFIG. 8 is a block diagram illustrating an example, non-limiting embodiment of an arc coupler in accordance with various aspects described herein.\nFIG. 9A is a block diagram illustrating an example, non-limiting embodiment of a stub coupler in accordance with various aspects described herein.\nFIG. 9B is a diagram illustrating an example, non-limiting embodiment of an electromagnetic distribution in accordance with various aspects described herein.\nFIGS. 10A and 10B are block diagrams illustrating example, non-limiting embodiments of couplers and transceivers in accordance with various aspects described herein.\nFIG. 11 is a block diagram illustrating an example, non-limiting embodiment of a dual stub coupler in accordance with various aspects described herein.\nFIG. 12 is a block diagram illustrating an example, non-limiting embodiment of a repeater system in accordance with various aspects described herein.\nFIG. 13 illustrates a block diagram illustrating an example, non-limiting embodiment of a bidirectional repeater in accordance with various aspects described herein.\nFIG. 14 is a block diagram illustrating an example, non-limiting embodiment of a waveguide system in accordance with various aspects described herein.\nFIG. 15 is a block diagram illustrating an example, non-limiting embodiment of a guided-wave communications system in accordance with various aspects described herein.\nFIGS. 16A and 16B are block diagrams illustrating an example, non-limiting embodiment of a system for managing a communication system in accordance with various aspects described herein.\nFIG. 17A illustrates a flow diagram of an example, non-limiting embodiment of a method for detecting and mitigating disturbances occurring in a communication network of the system of FIGS. 16A and 16B.\nFIG. 17B illustrates a flow diagram of an example, non-limiting embodiment of a method for detecting and mitigating disturbances occurring in a communication network of the system of FIGS. 16A and 16B.\nFIG. 18A is a block diagram illustrating an example, non-limiting embodiment of a communication system in accordance with various aspects described herein.\nFIG. 18B is a block diagram illustrating an example, non-limiting embodiment of a portion of the communication system of FIG. 18A in accordance with various aspects described herein.\nFIGS. 18C-18D are block diagrams illustrating example, non-limiting embodiments of a communication node of the communication system of FIG. 18A in accordance with various aspects described herein.\nFIG. 19A is a graphical diagram illustrating an example, non-limiting embodiment of downlink and uplink communication techniques for enabling a base station to communicate with communication nodes in accordance with various aspects described\nherein.\nFIG. 19B is a block diagram illustrating an example, non-limiting embodiment of a communication node in accordance with various aspects described herein.\nFIG. 19C is a block diagram illustrating an example, non-limiting embodiment of a communication node in accordance with various aspects described herein.\nFIG. 19D is a graphical diagram illustrating an example, non-limiting embodiment of a frequency spectrum in accordance with various aspects described herein.\nFIG. 19E is a graphical diagram illustrating an example, non-limiting embodiment of a frequency spectrum in accordance with various aspects described herein.\nFIG. 19F is a graphical diagram illustrating an example, non-limiting embodiment of a frequency spectrum in accordance with various aspects described herein.\nFIG. 19G is a graphical diagram illustrating an example, non-limiting embodiment of a frequency spectrum in accordance with various aspects described herein.\nFIG. 19H is a block diagram illustrating an example, non-limiting embodiment of a transmitter in accordance with various aspects described herein.\nFIG. 19I is a block diagram illustrating an example, non-limiting embodiment of a receiver in accordance with various aspects described herein.\nFIG. 20A is a block diagram illustrating an example, non-limiting embodiment of an antenna system in accordance with various aspects described herein.\nFIG. 20B is a block diagram illustrating example, non-limiting embodiments of a dielectric antenna in accordance with various aspects described herein.\nFIG. 20C is a diagram illustrating example, non-limiting embodiments of relative permittivity distributions in accordance with various aspects described herein.\nFIG. 21 is a block diagram of an example, non-limiting embodiment of a computing environment in accordance with various aspects described herein.\nFIG. 22 is a block diagram of an example, non-limiting embodiment of a mobile network platform in accordance with various aspects described herein.\nFIG. 23 is a block diagram of an example, non-limiting embodiment of a communication device in accordance with various aspects described herein.\n<BR><BR>DETAILED DESCRIPTION\nOne or more embodiments are now described with reference to the drawings, wherein like reference numerals are used to refer to like elements throughout.  In the following description, for purposes of explanation, numerous details are set forth\nin order to provide a thorough understanding of the various embodiments.  It is evident, however, that the various embodiments can be practiced without these details (and without applying to any particular networked environment or standard).\nIn an embodiment, a guided wave communication system is presented for sending and receiving communication signals such as data or other signaling via guided electromagnetic waves.  The guided electromagnetic waves include, for example, surface\nwaves or other electromagnetic waves that are bound to or guided by a transmission medium.  It will be appreciated that a variety of transmission media can be utilized with guided wave communications without departing from example embodiments.  Examples\nof such transmission media can include one or more of the following, either alone or in one or more combinations: wires, whether insulated or not, and whether single-stranded or multi-stranded; conductors of other shapes or configurations including wire\nbundles, cables, rods, rails, pipes; non-conductors such as dielectric pipes, rods, rails, or other dielectric members; combinations of conductors and dielectric materials; or other guided wave transmission media.\nThe inducement of guided electromagnetic waves on a transmission medium can be independent of any electrical potential, charge or current that is injected or otherwise transmitted through the transmission medium as part of an electrical circuit. For example, in the case where the transmission medium is a wire, it is to be appreciated that while a small current in the wire may be formed in response to the propagation of the guided waves along the wire, this can be due to the propagation of the\nelectromagnetic wave along the wire surface, and is not formed in response to electrical potential, charge or current that is injected into the wire as part of an electrical circuit.  The electromagnetic waves traveling on the wire therefore do not\nrequire a circuit to propagate along the wire surface.  The wire therefore is a single wire transmission line that is not part of a circuit.  Also, in some embodiments, a wire is not necessary, and the electromagnetic waves can propagate along a single\nline transmission medium that is not a wire.\nMore generally, \"guided electromagnetic waves\" or \"guided waves\" as described by the subject disclosure are affected by the presence of a physical object that is at least a part of the transmission medium (e.g., a bare wire or other conductor, a\ndielectric, an insulated wire, a conduit or other hollow element, a bundle of insulated wires that is coated, covered or surrounded by a dielectric or insulator or other wire bundle, or another form of solid, liquid or otherwise non-gaseous transmission\nmedium) so as to be at least partially bound to or guided by the physical object and so as to propagate along a transmission path of the physical object.  Such a physical object can operate as at least a part of a transmission medium that guides, by way\nof an interface of the transmission medium (e.g., an outer surface, inner surface, an interior portion between the outer and the inner surfaces or other boundary between elements of the transmission medium), the propagation of guided electromagnetic\nwaves, which in turn can carry energy, data and/or other signals along the transmission path from a sending device to a receiving device.\nUnlike free space propagation of wireless signals such as unguided (or unbounded) electromagnetic waves that decrease in intensity inversely by the square of the distance traveled by the unguided electromagnetic waves, guided electromagnetic\nwaves can propagate along a transmission medium with less loss in magnitude per unit distance than experienced by unguided electromagnetic waves.\nUnlike electrical signals, guided electromagnetic waves can propagate from a sending device to a receiving device without requiring a separate electrical return path between the sending device and the receiving device.  As a consequence, guided\nelectromagnetic waves can propagate from a sending device to a receiving device along a transmission medium having no conductive components (e.g., a dielectric strip), or via a transmission medium having no more than a single conductor (e.g., a single\nbare wire or insulated wire).  Even if a transmission medium includes one or more conductive components and the guided electromagnetic waves propagating along the transmission medium generate currents that flow in the one or more conductive components in\na direction of the guided electromagnetic waves, such guided electromagnetic waves can propagate along the transmission medium from a sending device to a receiving device without requiring a flow of opposing currents on an electrical return path between\nthe sending device and the receiving device.\nIn a non-limiting illustration, consider electrical systems that transmit and receive electrical signals between sending and receiving devices by way of conductive media.  Such systems generally rely on electrically separate forward and return\npaths.  For instance, consider a coaxial cable having a center conductor and a ground shield that are separated by an insulator.  Typically, in an electrical system a first terminal of a sending (or receiving) device can be connected to the center\nconductor, and a second terminal of the sending (or receiving) device can be connected to the ground shield.  If the sending device injects an electrical signal in the center conductor via the first terminal, the electrical signal will propagate along\nthe center conductor causing forward currents in the center conductor, and return currents in the ground shield.  The same conditions apply for a two terminal receiving device.\nIn contrast, consider a guided wave communication system such as described in the subject disclosure, which can utilize different embodiments of a transmission medium (including among others a coaxial cable) for transmitting and receiving guided\nelectromagnetic waves without an electrical return path.  In one embodiment, for example, the guided wave communication system of the subject disclosure can be configured to induce guided electromagnetic waves that propagate along an outer surface of a\ncoaxial cable.  Although the guided electromagnetic waves will cause forward currents on the ground shield, the guided electromagnetic waves do not require return currents to enable the guided electromagnetic waves to propagate along the outer surface of\nthe coaxial cable.  The same can be said of other transmission media used by a guided wave communication system for the transmission and reception of guided electromagnetic waves.  For example, guided electromagnetic waves induced by the guided wave\ncommunication system on an outer surface of a bare wire, or an insulated wire can propagate along the bare wire or the insulated bare wire without an electrical return path.\nConsequently, electrical systems that require two or more conductors for carrying forward and reverse currents on separate conductors to enable the propagation of electrical signals injected by a sending device are distinct from guided wave\nsystems that induce guided electromagnetic waves on an interface of a transmission medium without the need of an electrical return path to enable the propagation of the guided electromagnetic waves along the interface of the transmission medium.\nIt is further noted that guided electromagnetic waves as described in the subject disclosure can have an electromagnetic field structure that lies primarily or substantially outside of a transmission medium so as to be bound to or guided by the\ntransmission medium and so as to propagate non-trivial distances on or along an outer surface of the transmission medium.  In other embodiments, guided electromagnetic waves can have an electromagnetic field structure that lies primarily or substantially\ninside a transmission medium so as to be bound to or guided by the transmission medium and so as to propagate non-trivial distances within the transmission medium.  In other embodiments, guided electromagnetic waves can have an electromagnetic field\nstructure that lies partially inside and partially outside a transmission medium so as to be bound to or guided by the transmission medium and so as to propagate non-trivial distances along the transmission medium.  The desired electronic field structure\nin an embodiment may vary based upon a variety of factors, including the desired transmission distance, the characteristics of the transmission medium itself, and environmental conditions/characteristics outside of the transmission medium (e.g., presence\nof rain, fog, atmospheric conditions, etc.).\nIt is further noted that guided wave systems as described in the subject disclosure also differ from fiber optical systems.  Guided wave systems of the subject disclosure can induce guided electromagnetic waves on an interface of a transmission\nmedium constructed of an opaque material (e.g., a dielectric cable made of polyethylene) or a material that is otherwise resistive to the transmission of light waves (e.g., a bare conductive wire or an insulated conductive wire) enabling propagation of\nthe guided electromagnetic waves along the interface of the transmission medium over non-trivial distances.  Fiber optic systems in contrast cannot function with a transmission medium that is opaque or other resistive to the transmission of light waves.\nVarious embodiments described herein relate to coupling devices, that can be referred to as \"waveguide coupling devices\", \"waveguide couplers\" or more simply as \"couplers\", \"coupling devices\" or \"launchers\" for launching and/or extracting guided\nelectromagnetic waves to and from a transmission medium at millimeter-wave frequencies (e.g., 30 to 300 GHz), wherein the wavelength can be small compared to one or more dimensions of the coupling device and/or the transmission medium such as the\ncircumference of a wire or other cross sectional dimension, or lower microwave frequencies such as 300 MHz to 30 GHz.  Transmissions can be generated to propagate as waves guided by a coupling device, such as: a strip, arc or other length of dielectric\nmaterial; a horn, monopole, rod, slot or other antenna; an array of antennas; a magnetic resonant cavity, or other resonant coupler; a coil, a strip line, a waveguide or other coupling device.  In operation, the coupling device receives an\nelectromagnetic wave from a transmitter or transmission medium.  The electromagnetic field structure of the electromagnetic wave can be carried inside the coupling device, outside the coupling device or some combination thereof.  When the coupling device\nis in close proximity to a transmission medium, at least a portion of an electromagnetic wave couples to or is bound to the transmission medium, and continues to propagate as guided electromagnetic waves.  In a reciprocal fashion, a coupling device can\nextract guided waves from a transmission medium and transfer these electromagnetic waves to a receiver.\nAccording to an example embodiment, a surface wave is a type of guided wave that is guided by a surface of a transmission medium, such as an exterior or outer surface of the wire, or another surface of the wire that is adjacent to or exposed to\nanother type of medium having different properties (e.g., dielectric properties).  Indeed, in an example embodiment, a surface of the wire that guides a surface wave can represent a transitional surface between two different types of media.  For example,\nin the case of a bare or uninsulated wire, the surface of the wire can be the outer or exterior conductive surface of the bare or uninsulated wire that is exposed to air or free space.  As another example, in the case of insulated wire, the surface of\nthe wire can be the conductive portion of the wire that meets the insulator portion of the wire, or can otherwise be the insulator surface of the wire that is exposed to air or free space, or can otherwise be any material region between the insulator\nsurface of the wire and the conductive portion of the wire that meets the insulator portion of the wire, depending upon the relative differences in the properties (e.g., dielectric properties) of the insulator, air, and/or the conductor and further\ndependent on the frequency and propagation mode or modes of the guided wave.\nAccording to an example embodiment, the term \"about\" a wire or other transmission medium used in conjunction with a guided wave can include fundamental guided wave propagation modes such as a guided waves having a circular or substantially\ncircular field distribution, a symmetrical electromagnetic field distribution (e.g., electric field, magnetic field, electromagnetic field, etc.) or other fundamental mode pattern at least partially around a wire or other transmission medium.  In\naddition, when a guided wave propagates \"about\" a wire or other transmission medium, it can do so according to a guided wave propagation mode that includes not only the fundamental wave propagation modes (e.g., zero order modes), but additionally or\nalternatively non-fundamental wave propagation modes such as higher-order guided wave modes (e.g., 1.sup.st order modes, 2.sup.nd order modes, etc.), asymmetrical modes and/or other guided (e.g., surface) waves that have non-circular field distributions\naround a wire or other transmission medium.  As used herein, the term \"guided wave mode\" refers to a guided wave propagation mode of a transmission medium, coupling device or other system component of a guided wave communication system.\nFor example, such non-circular field distributions can be unilateral or multi-lateral with one or more axial lobes characterized by relatively higher field strength and/or one or more nulls or null regions characterized by relatively low-field\nstrength, zero-field strength or substantially zero-field strength.  Further, the field distribution can otherwise vary as a function of azimuthal orientation around the wire such that one or more angular regions around the wire have an electric or\nmagnetic field strength (or combination thereof) that is higher than one or more other angular regions of azimuthal orientation, according to an example embodiment.  It will be appreciated that the relative orientations or positions of the guided wave\nhigher order modes or asymmetrical modes can vary as the guided wave travels along the wire.\nAs used herein, the term \"millimeter-wave\" can refer to electromagnetic waves/signals that fall within the \"millimeter-wave frequency band\" of 30 GHz to 300 GHz.  The term \"microwave\" can refer to electromagnetic waves/signals that fall within a\n\"microwave frequency band\" of 300 MHz to 300 GHz.  The term \"radio frequency\" or \"RF\" can refer to electromagnetic waves/signals that fall within the \"radio frequency band\" of 10 kHz to 1 THz.  It is appreciated that wireless signals, electrical signals,\nand guided electromagnetic waves as described in the subject disclosure can be configured to operate at any desirable frequency range, such as, for example, at frequencies within, above or below millimeter-wave and/or microwave frequency bands.  In\nparticular, when a coupling device or transmission medium includes a conductive element, the frequency of the guided electromagnetic waves that are carried by the coupling device and/or propagate along the transmission medium can be below the mean\ncollision frequency of the electrons in the conductive element.  Further, the frequency of the guided electromagnetic waves that are carried by the coupling device and/or propagate along the transmission medium can be a non-optical frequency, e.g., a\nradio frequency below the range of optical frequencies that begins at 1 THz.\nAs used herein, the term \"antenna\" can refer to a device that is part of a transmitting or receiving system to transmit/radiate or receive wireless signals.\nIn accordance with one or more embodiments, a communication device includes a dielectric antenna having a non-uniform spatial distribution of relative permittivity.  A cable comprising a dielectric core is coupled to a feed point of the\ndielectric antenna.  A transmitter facilitates a transmission of the electromagnetic waves, the electromagnetic waves guided by the dielectric core to the feed point of the dielectric antenna without an electrical return path.\nIn accordance with one or more embodiments, an antenna structure includes a feed point that facilitates coupling to a dielectric core that supplies electromagnetic waves to the feed point.  A solid dielectric antenna receives the electromagnetic\nwaves, the solid dielectric antenna having a non-uniform spatial distribution of relative permittivity.\nIn accordance with one or more embodiments, an antenna includes three or more dielectric sections each composed of a different dielectric material.\nReferring now to FIG. 1, a block diagram 100 illustrating an example, non-limiting embodiment of a guided wave communications system is shown.  In operation, a transmission device 101 receives one or more communication signals 110 from a\ncommunication network or other communications device that includes data and generates guided waves 120 to convey the data via the transmission medium 125 to the transmission device 102.  The transmission device 102 receives the guided waves 120 and\nconverts them to communication signals 112 that include the data for transmission to a communications network or other communications device.  The guided waves 120 can be modulated to convey data via a modulation technique such as phase shift keying,\nfrequency shift keying, quadrature amplitude modulation, amplitude modulation, multi-carrier modulation such as orthogonal frequency division multiplexing and via multiple access techniques such as frequency division multiplexing, time division\nmultiplexing, code division multiplexing, multiplexing via differing wave propagation modes and via other modulation and access strategies.\nThe communication network or networks can include a wireless communication network such as a mobile data network, a cellular voice and data network, a wireless local area network (e.g., WiFi or an 802.xx network), a satellite communications\nnetwork, a personal area network or other wireless network.  The communication network or networks can also include a wired communication network such as a telephone network, an Ethernet network, a local area network, a wide area network such as the\nInternet, a broadband access network, a cable network, a fiber optic network, or other wired network.  The communication devices can include a network edge device, bridge device or home gateway, a set-top box, broadband modem, telephone adapter, access\npoint, base station, or other fixed communication device, a mobile communication device such as an automotive gateway or automobile, laptop computer, tablet, smartphone, cellular telephone, or other communication device.\nIn an example embodiment, the guided wave communication system 100 can operate in a bi-directional fashion where transmission device 102 receives one or more communication signals 112 from a communication network or device that includes other\ndata and generates guided waves 122 to convey the other data via the transmission medium 125 to the transmission device 101.  In this mode of operation, the transmission device 101 receives the guided waves 122 and converts them to communication signals\n110 that include the other data for transmission to a communications network or device.  The guided waves 122 can be modulated to convey data via a modulation technique such as phase shift keying, frequency shift keying, quadrature amplitude modulation,\namplitude modulation, multi-carrier modulation such as orthogonal frequency division multiplexing and via multiple access techniques such as frequency division multiplexing, time division multiplexing, code division multiplexing, multiplexing via\ndiffering wave propagation modes and via other modulation and access strategies.\nThe transmission medium 125 can include a cable having at least one inner portion surrounded by a dielectric material such as an insulator or other dielectric cover, coating or other dielectric material, the dielectric material having an outer\nsurface and a corresponding circumference.  In an example embodiment, the transmission medium 125 operates as a single-wire transmission line to guide the transmission of an electromagnetic wave.  When the transmission medium 125 is implemented as a\nsingle wire transmission system, it can include a wire.  The wire can be insulated or uninsulated, and single-stranded or multi-stranded (e.g., braided).  In other embodiments, the transmission medium 125 can contain conductors of other shapes or\nconfigurations including wire bundles, cables, rods, rails, pipes.  In addition, the transmission medium 125 can include non-conductors such as dielectric pipes, rods, rails, or other dielectric members; combinations of conductors and dielectric\nmaterials, conductors without dielectric materials or other guided wave transmission media.  It should be noted that the transmission medium 125 can otherwise include any of the transmission media previously discussed.\nFurther, as previously discussed, the guided waves 120 and 122 can be contrasted with radio transmissions over free space/air or conventional propagation of electrical power or signals through the conductor of a wire via an electrical circuit. \nIn addition to the propagation of guided waves 120 and 122, the transmission medium 125 may optionally contain one or more wires that propagate electrical power or other communication signals in a conventional manner as a part of one or more electrical\ncircuits.\nReferring now to FIG. 2, a block diagram 200 illustrating an example, non-limiting embodiment of a transmission device is shown.  The transmission device 101 or 102 includes a communications interface (I/F) 205, a transceiver 210 and a coupler\n220.\nIn an example of operation, the communications interface 205 receives a communication signal 110 or 112 that includes data.  In various embodiments, the communications interface 205 can include a wireless interface for receiving a wireless\ncommunication signal in accordance with a wireless standard protocol such as LTE or other cellular voice and data protocol, WiFi or an 802.11 protocol, WIMAX protocol, Ultra Wideband protocol, Bluetooth protocol, Zigbee protocol, a direct broadcast\nsatellite (DBS) or other satellite communication protocol or other wireless protocol.  In addition or in the alternative, the communications interface 205 includes a wired interface that operates in accordance with an Ethernet protocol, universal serial\nbus (USB) protocol, a data over cable service interface specification (DOCSIS) protocol, a digital subscriber line (DSL) protocol, a Firewire (IEEE 1394) protocol, or other wired protocol.  In additional to standards-based protocols, the communications\ninterface 205 can operate in conjunction with other wired or wireless protocol.  In addition, the communications interface 205 can optionally operate in conjunction with a protocol stack that includes multiple protocol layers including a MAC protocol,\ntransport protocol, application protocol, etc.\nIn an example of operation, the transceiver 210 generates an electromagnetic wave based on the communication signal 110 or 112 to convey the data.  The electromagnetic wave has at least one carrier frequency and at least one corresponding\nwavelength.  The carrier frequency can be within a millimeter-wave frequency band of 30 GHz-300 GHz, such as 60 GHz or a carrier frequency in the range of 30-40 GHz or a lower frequency band of 300 MHz-30 GHz in the microwave frequency range such as\n26-30 GHz, 11 GHz, 6 GHz or 3 GHz, but it will be appreciated that other carrier frequencies are possible in other embodiments.  In one mode of operation, the transceiver 210 merely upconverts the communications signal or signals 110 or 112 for\ntransmission of the electromagnetic signal in the microwave or millimeter-wave band as a guided electromagnetic wave that is guided by or bound to the transmission medium 125.  In another mode of operation, the communications interface 205 either\nconverts the communication signal 110 or 112 to a baseband or near baseband signal or extracts the data from the communication signal 110 or 112 and the transceiver 210 modulates a high-frequency carrier with the data, the baseband or near baseband\nsignal for transmission.  It should be appreciated that the transceiver 210 can modulate the data received via the communication signal 110 or 112 to preserve one or more data communication protocols of the communication signal 110 or 112 either by\nencapsulation in the payload of a different protocol or by simple frequency shifting.  In the alternative, the transceiver 210 can otherwise translate the data received via the communication signal 110 or 112 to a protocol that is different from the data\ncommunication protocol or protocols of the communication signal 110 or 112.\nIn an example of operation, the coupler 220 couples the electromagnetic wave to the transmission medium 125 as a guided electromagnetic wave to convey the communications signal or signals 110 or 112.  While the prior description has focused on\nthe operation of the transceiver 210 as a transmitter, the transceiver 210 can also operate to receive electromagnetic waves that convey other data from the single wire transmission medium via the coupler 220 and to generate communications signals 110 or\n112, via communications interface 205 that includes the other data.  Consider embodiments where an additional guided electromagnetic wave conveys other data that also propagates along the transmission medium 125.  The coupler 220 can also couple this\nadditional electromagnetic wave from the transmission medium 125 to the transceiver 210 for reception.\nThe transmission device 101 or 102 includes an optional training controller 230.  In an example embodiment, the training controller 230 is implemented by a standalone processor or a processor that is shared with one or more other components of\nthe transmission device 101 or 102.  The training controller 230 selects the carrier frequencies, modulation schemes and/or guided wave modes for the guided electromagnetic waves based on feedback data received by the transceiver 210 from at least one\nremote transmission device coupled to receive the guided electromagnetic wave.\nIn an example embodiment, a guided electromagnetic wave transmitted by a remote transmission device 101 or 102 conveys data that also propagates along the transmission medium 125.  The data from the remote transmission device 101 or 102 can be\ngenerated to include the feedback data.  In operation, the coupler 220 also couples the guided electromagnetic wave from the transmission medium 125 and the transceiver receives the electromagnetic wave and processes the electromagnetic wave to extract\nthe feedback data.\nIn an example embodiment, the training controller 230 operates based on the feedback data to evaluate a plurality of candidate frequencies, modulation schemes and/or transmission modes to select a carrier frequency, modulation scheme and/or\ntransmission mode to enhance performance, such as throughput, signal strength, reduce propagation loss, etc.\nConsider the following example: a transmission device 101 begins operation under control of the training controller 230 by sending a plurality of guided waves as test signals such as pilot waves or other test signals at a corresponding plurality\nof candidate frequencies and/or candidate modes directed to a remote transmission device 102 coupled to the transmission medium 125.  The guided waves can include, in addition or in the alternative, test data.  The test data can indicate the particular\ncandidate frequency and/or guide-wave mode of the signal.  In an embodiment, the training controller 230 at the remote transmission device 102 receives the test signals and/or test data from any of the guided waves that were properly received and\ndetermines the best candidate frequency and/or guided wave mode, a set of acceptable candidate frequencies and/or guided wave modes, or a rank ordering of candidate frequencies and/or guided wave modes.  This selection of candidate frequenc(ies) or/and\nguided-mode(s) are generated by the training controller 230 based on one or more optimizing criteria such as received signal strength, bit error rate, packet error rate, signal to noise ratio, propagation loss, etc. The training controller 230 generates\nfeedback data that indicates the selection of candidate frequenc(ies) or/and guided wave mode(s) and sends the feedback data to the transceiver 210 for transmission to the transmission device 101.  The transmission device 101 and 102 can then communicate\ndata with one another based on the selection of candidate frequenc(ies) or/and guided wave mode(s).\nIn other embodiments, the guided electromagnetic waves that contain the test signals and/or test data are reflected back, repeated back or otherwise looped back by the remote transmission device 102 to the transmission device 101 for reception\nand analysis by the training controller 230 of the transmission device 101 that initiated these waves.  For example, the transmission device 101 can send a signal to the remote transmission device 102 to initiate a test mode where a physical reflector is\nswitched on the line, a termination impedance is changed to cause reflections, a loop back mode is switched on to couple electromagnetic waves back to the source transmission device 102, and/or a repeater mode is enabled to amplify and retransmit the\nelectromagnetic waves back to the source transmission device 102.  The training controller 230 at the source transmission device 102 receives the test signals and/or test data from any of the guided waves that were properly received and determines\nselection of candidate frequenc(ies) or/and guided wave mode(s).\nWhile the procedure above has been described in a start-up or initialization mode of operation, each transmission device 101 or 102 can send test signals, evaluate candidate frequencies or guided wave modes via non-test such as normal\ntransmissions or otherwise evaluate candidate frequencies or guided wave modes at other times or continuously as well.  In an example embodiment, the communication protocol between the transmission devices 101 and 102 can include an on-request or\nperiodic test mode where either full testing or more limited testing of a subset of candidate frequencies and guided wave modes are tested and evaluated.  In other modes of operation, the re-entry into such a test mode can be triggered by a degradation\nof performance due to a disturbance, weather conditions, etc. In an example embodiment, the receiver bandwidth of the transceiver 210 is either sufficiently wide or swept to receive all candidate frequencies or can be selectively adjusted by the training\ncontroller 230 to a training mode where the receiver bandwidth of the transceiver 210 is sufficiently wide or swept to receive all candidate frequencies.\nReferring now to FIG. 3, a graphical diagram 300 illustrating an example, non-limiting embodiment of an electromagnetic field distribution is shown.  In this embodiment, a transmission medium 125 in air includes an inner conductor 301 and an\ninsulating jacket 302 of dielectric material, as shown in cross section.  The diagram 300 includes different gray-scales that represent differing electromagnetic field strengths generated by the propagation of the guided wave having an asymmetrical and\nnon-fundamental guided wave mode.\nIn particular, the electromagnetic field distribution corresponds to a modal \"sweet spot\" that enhances guided electromagnetic wave propagation along an insulated transmission medium and reduces end-to-end transmission loss.  In this particular\nmode, electromagnetic waves are guided by the transmission medium 125 to propagate along an outer surface of the transmission medium--in this case, the outer surface of the insulating jacket 302.  Electromagnetic waves are partially embedded in the\ninsulator and partially radiating on the outer surface of the insulator.  In this fashion, electromagnetic waves are \"lightly\" coupled to the insulator so as to enable electromagnetic wave propagation at long distances with low propagation loss.\nAs shown, the guided wave has a field structure that lies primarily or substantially outside of the transmission medium 125 that serves to guide the electromagnetic waves.  The regions inside the conductor 301 have little or no field.  Likewise\nregions inside the insulating jacket 302 have low field strength.  The majority of the electromagnetic field strength is distributed in the lobes 304 at the outer surface of the insulating jacket 302 and in close proximity thereof.  The presence of an\nasymmetric guided wave mode is shown by the high electromagnetic field strengths at the top and bottom of the outer surface of the insulating jacket 302 (in the orientation of the diagram)--as opposed to very small field strengths on the other sides of\nthe insulating jacket 302.\nThe example shown corresponds to a 38 GHz electromagnetic wave guided by a wire with a diameter of 1.1 cm and a dielectric insulation of thickness of 0.36 cm.  Because the electromagnetic wave is guided by the transmission medium 125 and the\nmajority of the field strength is concentrated in the air outside of the insulating jacket 302 within a limited distance of the outer surface, the guided wave can propagate longitudinally down the transmission medium 125 with very low loss.  In the\nexample shown, this \"limited distance\" corresponds to a distance from the outer surface that is less than half the largest cross sectional dimension of the transmission medium 125.  In this case, the largest cross sectional dimension of the wire\ncorresponds to the overall diameter of 1.82 cm, however, this value can vary with the size and shape of the transmission medium 125.  For example, should the transmission medium 125 be of a rectangular shape with a height of 0.3 cm and a width of 0.4 cm,\nthe largest cross sectional dimension would be the diagonal of 0.5 cm and the corresponding limited distance would be 0.25 cm.  The dimensions of the area containing the majority of the field strength also vary with the frequency, and in general,\nincrease as carrier frequencies decrease.\nIt should also be noted that the components of a guided wave communication system, such as couplers and transmission media can have their own cut-off frequencies for each guided wave mode.  The cut-off frequency generally sets forth the lowest\nfrequency that a particular guided wave mode is designed to be supported by that particular component.  In an example embodiment, the particular asymmetric mode of propagation shown is induced on the transmission medium 125 by an electromagnetic wave\nhaving a frequency that falls within a limited range (such as Fc to 2Fc) of the lower cut-off frequency Fc for this particular asymmetric mode.  The lower cut-off frequency Fc is particular to the characteristics of transmission medium 125.  For\nembodiments as shown that include an inner conductor 301 surrounded by an insulating jacket 302, this cutoff frequency can vary based on the dimensions and properties of the insulating jacket 302 and potentially the dimensions and properties of the inner\nconductor 301 and can be determined experimentally to have a desired mode pattern.  It should be noted however, that similar effects can be found for a hollow dielectric or insulator without an inner conductor.  In this case, the cutoff frequency can\nvary based on the dimensions and properties of the hollow dielectric or insulator.\nAt frequencies lower than the lower cut-off frequency, the asymmetric mode is difficult to induce in the transmission medium 125 and fails to propagate for all but trivial distances.  As the frequency increases above the limited range of\nfrequencies about the cut-off frequency, the asymmetric mode shifts more and more inward of the insulating jacket 302.  At frequencies much larger than the cut-off frequency, the field strength is no longer concentrated outside of the insulating jacket,\nbut primarily inside of the insulating jacket 302.  While the transmission medium 125 provides strong guidance to the electromagnetic wave and propagation is still possible, ranges are more limited by increased losses due to propagation within the\ninsulating jacket 302--as opposed to the surrounding air.\nReferring now to FIG. 4, a graphical diagram 400 illustrating an example, non-limiting embodiment of an electromagnetic field distribution is shown.  In particular, a cross section diagram 400, similar to FIG. 3 is shown with common reference\nnumerals used to refer to similar elements.  The example shown corresponds to a 60 GHz wave guided by a wire with a diameter of 1.1 cm and a dielectric insulation of thickness of 0.36 cm.  Because the frequency of the guided wave is above the limited\nrange of the cut-off frequency of this particular asymmetric mode, much of the field strength has shifted inward of the insulating jacket 302.  In particular, the field strength is concentrated primarily inside of the insulating jacket 302.  While the\ntransmission medium 125 provides strong guidance to the electromagnetic wave and propagation is still possible, ranges are more limited when compared with the embodiment of FIG. 3, by increased losses due to propagation within the insulating jacket 302.\nReferring now to FIG. 5A, a graphical diagram illustrating an example, non-limiting embodiment of a frequency response is shown.  In particular, diagram 500 presents a graph of end-to-end loss (in dB) as a function of frequency, overlaid with\nelectromagnetic field distributions 510, 520 and 530 at three points for a 200 cm insulated medium voltage wire.  The boundary between the insulator and the surrounding air is represented by reference numeral 525 in each electromagnetic field\ndistribution.\nAs discussed in conjunction with FIG. 3, an example of a desired asymmetric mode of propagation shown is induced on the transmission medium 125 by an electromagnetic wave having a frequency that falls within a limited range (such as Fc to 2Fc)\nof the lower cut-off frequency Fc of the transmission medium for this particular asymmetric mode.  In particular, the electromagnetic field distribution 520 at 6 GHz falls within this modal \"sweet spot\" that enhances electromagnetic wave propagation\nalong an insulated transmission medium and reduces end-to-end transmission loss.  In this particular mode, guided waves are partially embedded in the insulator and partially radiating on the outer surface of the insulator.  In this fashion, the\nelectromagnetic waves are \"lightly\" coupled to the insulator so as to enable guided electromagnetic wave propagation at long distances with low propagation loss.\nAt lower frequencies represented by the electromagnetic field distribution 510 at 3 GHz, the asymmetric mode radiates more heavily generating higher propagation losses.  At higher frequencies represented by the electromagnetic field distribution\n530 at 9 GHz, the asymmetric mode shifts more and more inward of the insulating jacket providing too much absorption, again generating higher propagation losses.\nReferring now to FIG. 5B, a graphical diagram 550 illustrating example, non-limiting embodiments of a longitudinal cross-section of a transmission medium 125, such as an insulated wire, depicting fields of guided electromagnetic waves at various\noperating frequencies is shown.  As shown in diagram 556, when the guided electromagnetic waves are at approximately the cutoff frequency (f.sub.c) corresponding to the modal \"sweet spot\", the guided electromagnetic waves are loosely coupled to the\ninsulated wire so that absorption is reduced, and the fields of the guided electromagnetic waves are bound sufficiently to reduce the amount radiated into the environment (e.g., air).  Because absorption and radiation of the fields of the guided\nelectromagnetic waves is low, propagation losses are consequently low, enabling the guided electromagnetic waves to propagate for longer distances.\nAs shown in diagram 554, propagation losses increase when an operating frequency of the guide electromagnetic waves increases above about two-times the cutoff frequency (f.sub.c)--or as referred to, above the range of the \"sweet spot\".  More of\nthe field strength of the electromagnetic wave is driven inside the insulating layer, increasing propagation losses.  At frequencies much higher than the cutoff frequency (f.sub.c) the guided electromagnetic waves are strongly bound to the insulated wire\nas a result of the fields emitted by the guided electromagnetic waves being concentrated in the insulation layer of the wire, as shown in diagram 552.  This in turn raises propagation losses further due to absorption of the guided electromagnetic waves\nby the insulation layer.  Similarly, propagation losses increase when the operating frequency of the guided electromagnetic waves is substantially below the cutoff frequency (f.sub.c), as shown in diagram 558.  At frequencies much lower than the cutoff\nfrequency (f.sub.c) the guided electromagnetic waves are weakly (or nominally) bound to the insulated wire and thereby tend to radiate into the environment (e.g., air), which in turn, raises propagation losses due to radiation of the guided\nelectromagnetic waves.\nReferring now to FIG. 6, a graphical diagram 600 illustrating an example, non-limiting embodiment of an electromagnetic field distribution is shown.  In this embodiment, a transmission medium 602 is a bare wire, as shown in cross section.  The\ndiagram 300 includes different gray-scales that represent differing electromagnetic field strengths generated by the propagation of a guided wave having a symmetrical and fundamental guided wave mode at a single carrier frequency.\nIn this particular mode, electromagnetic waves are guided by the transmission medium 602 to propagate along an outer surface of the transmission medium--in this case, the outer surface of the bare wire.  Electromagnetic waves are \"lightly\"\ncoupled to the wire so as to enable electromagnetic wave propagation at long distances with low propagation loss.  As shown, the guided wave has a field structure that lies substantially outside of the transmission medium 602 that serves to guide the\nelectromagnetic waves.  The regions inside the conductor 602 have little or no field.\nReferring now to FIG. 7, a block diagram 700 illustrating an example, non-limiting embodiment of an arc coupler is shown.  In particular a coupling device is presented for use in a transmission device, such as transmission device 101 or 102\npresented in conjunction with FIG. 1.  The coupling device includes an arc coupler 704 coupled to a transmitter circuit 712 and termination or damper 714.  The arc coupler 704 can be made of a dielectric material, or other low-loss insulator (e.g.,\nTeflon, polyethylene, etc.), or made of a conducting (e.g., metallic, non-metallic, etc.) material, or any combination of the foregoing materials.  As shown, the arc coupler 704 operates as a waveguide and has a wave 706 propagating as a guided wave\nabout a waveguide surface of the arc coupler 704.  In the embodiment shown, at least a portion of the arc coupler 704 can be placed near a wire 702 or other transmission medium, (such as transmission medium 125), in order to facilitate coupling between\nthe arc coupler 704 and the wire 702 or other transmission medium, as described herein to launch the guided wave 708 on the wire.  The arc coupler 704 can be placed such that a portion of the curved arc coupler 704 is tangential to, and parallel or\nsubstantially parallel to the wire 702.  The portion of the arc coupler 704 that is parallel to the wire can be an apex of the curve, or any point where a tangent of the curve is parallel to the wire 702.  When the arc coupler 704 is positioned or placed\nthusly, the wave 706 travelling along the arc coupler 704 couples, at least in part, to the wire 702, and propagates as guided wave 708 around or about the wire surface of the wire 702 and longitudinally along the wire 702.  The guided wave 708 can be\ncharacterized as a surface wave or other electromagnetic wave that is guided by or bound to the wire 702 or other transmission medium.\nA portion of the wave 706 that does not couple to the wire 702 propagates as a wave 710 along the arc coupler 704.  It will be appreciated that the arc coupler 704 can be configured and arranged in a variety of positions in relation to the wire\n702 to achieve a desired level of coupling or non-coupling of the wave 706 to the wire 702.  For example, the curvature and/or length of the arc coupler 704 that is parallel or substantially parallel, as well as its separation distance (which can include\nzero separation distance in an embodiment), to the wire 702 can be varied without departing from example embodiments.  Likewise, the arrangement of arc coupler 704 in relation to the wire 702 may be varied based upon considerations of the respective\nintrinsic characteristics (e.g., thickness, composition, electromagnetic properties, etc.) of the wire 702 and the arc coupler 704, as well as the characteristics (e.g., frequency, energy level, etc.) of the waves 706 and 708.\nThe guided wave 708 stays parallel or substantially parallel to the wire 702, even as the wire 702 bends and flexes.  Bends in the wire 702 can increase transmission losses, which are also dependent on wire diameters, frequency, and materials. \nIf the dimensions of the arc coupler 704 are chosen for efficient power transfer, most of the power in the wave 706 is transferred to the wire 702, with little power remaining in wave 710.  It will be appreciated that the guided wave 708 can still be\nmulti-modal in nature (discussed herein), including having modes that are non-fundamental or asymmetric, while traveling along a path that is parallel or substantially parallel to the wire 702, with or without a fundamental transmission mode.  In an\nembodiment, non-fundamental or asymmetric modes can be utilized to minimize transmission losses and/or obtain increased propagation distances.\nIt is noted that the term parallel is generally a geometric construct which often is not exactly achievable in real systems.  Accordingly, the term parallel as utilized in the subject disclosure represents an approximation rather than an exact\nconfiguration when used to describe embodiments disclosed in the subject disclosure.  In an embodiment, substantially parallel can include approximations that are within 30 degrees of true parallel in all dimensions.\nIn an embodiment, the wave 706 can exhibit one or more wave propagation modes.  The arc coupler modes can be dependent on the shape and/or design of the coupler 704.  The one or more arc coupler modes of wave 706 can generate, influence, or\nimpact one or more wave propagation modes of the guided wave 708 propagating along wire 702.  It should be particularly noted however that the guided wave modes present in the guided wave 706 may be the same or different from the guided wave modes of the\nguided wave 708.  In this fashion, one or more guided wave modes of the guided wave 706 may not be transferred to the guided wave 708, and further one or more guided wave modes of guided wave 708 may not have been present in guided wave 706.  It should\nalso be noted that the cut-off frequency of the arc coupler 704 for a particular guided wave mode may be different than the cutoff frequency of the wire 702 or other transmission medium for that same mode.  For example, while the wire 702 or other\ntransmission medium may be operated slightly above its cutoff frequency for a particular guided wave mode, the arc coupler 704 may be operated well above its cut-off frequency for that same mode for low loss, slightly below its cut-off frequency for that\nsame mode to, for example, induce greater coupling and power transfer, or some other point in relation to the arc coupler's cutoff frequency for that mode.\nIn an embodiment, the wave propagation modes on the wire 702 can be similar to the arc coupler modes since both waves 706 and 708 propagate about the outside of the arc coupler 704 and wire 702 respectively.  In some embodiments, as the wave 706\ncouples to the wire 702, the modes can change form, or new modes can be created or generated, due to the coupling between the arc coupler 704 and the wire 702.  For example, differences in size, material, and/or impedances of the arc coupler 704 and wire\n702 may create additional modes not present in the arc coupler modes and/or suppress some of the arc coupler modes.  The wave propagation modes can comprise the fundamental transverse electromagnetic mode (Quasi-TEM.sub.00), where only small electric\nand/or magnetic fields extend in the direction of propagation, and the electric and magnetic fields extend radially outwards while the guided wave propagates along the wire.  This guided wave mode can be donut shaped, where few of the electromagnetic\nfields exist within the arc coupler 704 or wire 702.\nWaves 706 and 708 can comprise a fundamental TEM mode where the fields extend radially outwards, and also comprise other, non-fundamental (e.g., asymmetric, higher-level, etc.) modes.  While particular wave propagation modes are discussed above,\nother wave propagation modes are likewise possible such as transverse electric (TE) and transverse magnetic (TM) modes, based on the frequencies employed, the design of the arc coupler 704, the dimensions and composition of the wire 702, as well as its\nsurface characteristics, its insulation if present, the electromagnetic properties of the surrounding environment, etc. It should be noted that, depending on the frequency, the electrical and physical characteristics of the wire 702 and the particular\nwave propagation modes that are generated, guided wave 708 can travel along the conductive surface of an oxidized uninsulated wire, an unoxidized uninsulated wire, an insulated wire and/or along the insulating surface of an insulated wire.\nIn an embodiment, a diameter of the arc coupler 704 is smaller than the diameter of the wire 702.  For the millimeter-band wavelength being used, the arc coupler 704 supports a single waveguide mode that makes up wave 706.  This single waveguide\nmode can change as it couples to the wire 702 as guided wave 708.  If the arc coupler 704 were larger, more than one waveguide mode can be supported, but these additional waveguide modes may not couple to the wire 702 as efficiently, and higher coupling\nlosses can result.  However, in some alternative embodiments, the diameter of the arc coupler 704 can be equal to or larger than the diameter of the wire 702, for example, where higher coupling losses are desirable or when used in conjunction with other\ntechniques to otherwise reduce coupling losses (e.g., impedance matching with tapering, etc.).\nIn an embodiment, the wavelength of the waves 706 and 708 are comparable in size, or smaller than a circumference of the arc coupler 704 and the wire 702.  In an example, if the wire 702 has a diameter of 0.5 cm, and a corresponding\ncircumference of around 1.5 cm, the wavelength of the transmission is around 1.5 cm or less, corresponding to a frequency of 70 GHz or greater.  In another embodiment, a suitable frequency of the transmission and the carrier-wave signal is in the range\nof 30-100 GHz, perhaps around 30-60 GHz, and around 38 GHz in one example.  In an embodiment, when the circumference of the arc coupler 704 and wire 702 is comparable in size to, or greater, than a wavelength of the transmission, the waves 706 and 708\ncan exhibit multiple wave propagation modes including fundamental and/or non-fundamental (symmetric and/or asymmetric) modes that propagate over sufficient distances to support various communication systems described herein.  The waves 706 and 708 can\ntherefore comprise more than one type of electric and magnetic field configuration.  In an embodiment, as the guided wave 708 propagates down the wire 702, the electrical and magnetic field configurations will remain the same from end to end of the wire\n702.  In other embodiments, as the guided wave 708 encounters interference (distortion or obstructions) or loses energy due to transmission losses or scattering, the electric and magnetic field configurations can change as the guided wave 708 propagates\ndown wire 702.\nIn an embodiment, the arc coupler 704 can be composed of nylon, Teflon, polyethylene, a polyamide, or other plastics.  In other embodiments, other dielectric materials are possible.  The wire surface of wire 702 can be metallic with either a\nbare metallic surface, or can be insulated using plastic, dielectric, insulator or other coating, jacket or sheathing.  In an embodiment, a dielectric or otherwise non-conducting/insulated waveguide can be paired with either a bare/metallic wire or\ninsulated wire.  In other embodiments, a metallic and/or conductive waveguide can be paired with a bare/metallic wire or insulated wire.  In an embodiment, an oxidation layer on the bare metallic surface of the wire 702 (e.g., resulting from exposure of\nthe bare metallic surface to oxygen/air) can also provide insulating or dielectric properties similar to those provided by some insulators or sheathings.\nIt is noted that the graphical representations of waves 706, 708 and 710 are presented merely to illustrate the principles that wave 706 induces or otherwise launches a guided wave 708 on a wire 702 that operates, for example, as a single wire\ntransmission line.  Wave 710 represents the portion of wave 706 that remains on the arc coupler 704 after the generation of guided wave 708.  The actual electric and magnetic fields generated as a result of such wave propagation may vary depending on the\nfrequencies employed, the particular wave propagation mode or modes, the design of the arc coupler 704, the dimensions and composition of the wire 702, as well as its surface characteristics, its optional insulation, the electromagnetic properties of the\nsurrounding environment, etc.\nIt is noted that arc coupler 704 can include a termination circuit or damper 714 at the end of the arc coupler 704 that can absorb leftover radiation or energy from wave 710.  The termination circuit or damper 714 can prevent and/or minimize the\nleftover radiation or energy from wave 710 reflecting back toward transmitter circuit 712.  In an embodiment, the termination circuit or damper 714 can include termination resistors, and/or other components that perform impedance matching to attenuate\nreflection.  In some embodiments, if the coupling efficiencies are high enough, and/or wave 710 is sufficiently small, it may not be necessary to use a termination circuit or damper 714.  For the sake of simplicity, these transmitter 712 and termination\ncircuits or dampers 714 may not be depicted in the other figures, but in those embodiments, transmitter and termination circuits or dampers may possibly be used.\nFurther, while a single arc coupler 704 is presented that generates a single guided wave 708, multiple arc couplers 704 placed at different points along the wire 702 and/or at different azimuthal orientations about the wire can be employed to\ngenerate and receive multiple guided waves 708 at the same or different frequencies, at the same or different phases, at the same or different wave propagation modes.\nFIG. 8, a block diagram 800 illustrating an example, non-limiting embodiment of an arc coupler is shown.  In the embodiment shown, at least a portion of the coupler 704 can be placed near a wire 702 or other transmission medium, (such as\ntransmission medium 125), in order to facilitate coupling between the arc coupler 704 and the wire 702 or other transmission medium, to extract a portion of the guided wave 806 as a guided wave 808 as described herein.  The arc coupler 704 can be placed\nsuch that a portion of the curved arc coupler 704 is tangential to, and parallel or substantially parallel to the wire 702.  The portion of the arc coupler 704 that is parallel to the wire can be an apex of the curve, or any point where a tangent of the\ncurve is parallel to the wire 702.  When the arc coupler 704 is positioned or placed thusly, the wave 806 travelling along the wire 702 couples, at least in part, to the arc coupler 704, and propagates as guided wave 808 along the arc coupler 704 to a\nreceiving device (not expressly shown).  A portion of the wave 806 that does not couple to the arc coupler propagates as wave 810 along the wire 702 or other transmission medium.\nIn an embodiment, the wave 806 can exhibit one or more wave propagation modes.  The arc coupler modes can be dependent on the shape and/or design of the coupler 704.  The one or more modes of guided wave 806 can generate, influence, or impact\none or more guide-wave modes of the guided wave 808 propagating along the arc coupler 704.  It should be particularly noted however that the guided wave modes present in the guided wave 806 may be the same or different from the guided wave modes of the\nguided wave 808.  In this fashion, one or more guided wave modes of the guided wave 806 may not be transferred to the guided wave 808, and further one or more guided wave modes of guided wave 808 may not have been present in guided wave 806.\nReferring now to FIG. 9A, a block diagram 900 illustrating an example, non-limiting embodiment of a stub coupler is shown.  In particular a coupling device that includes stub coupler 904 is presented for use in a transmission device, such as\ntransmission device 101 or 102 presented in conjunction with FIG. 1.  The stub coupler 904 can be made of a dielectric material, or other low-loss insulator (e.g., Teflon, polyethylene and etc.), or made of a conducting (e.g., metallic, non-metallic,\netc.) material, or any combination of the foregoing materials.  As shown, the stub coupler 904 operates as a waveguide and has a wave 906 propagating as a guided wave about a waveguide surface of the stub coupler 904.  In the embodiment shown, at least a\nportion of the stub coupler 904 can be placed near a wire 702 or other transmission medium, (such as transmission medium 125), in order to facilitate coupling between the stub coupler 904 and the wire 702 or other transmission medium, as described herein\nto launch the guided wave 908 on the wire.\nIn an embodiment, the stub coupler 904 is curved, and an end of the stub coupler 904 can be tied, fastened, or otherwise mechanically coupled to a wire 702.  When the end of the stub coupler 904 is fastened to the wire 702, the end of the stub\ncoupler 904 is parallel or substantially parallel to the wire 702.  Alternatively, another portion of the dielectric waveguide beyond an end can be fastened or coupled to wire 702 such that the fastened or coupled portion is parallel or substantially\nparallel to the wire 702.  The fastener 910 can be a nylon cable tie or other type of non-conducting/dielectric material that is either separate from the stub coupler 904 or constructed as an integrated component of the stub coupler 904.  The stub\ncoupler 904 can be adjacent to the wire 702 without surrounding the wire 702.\nLike the arc coupler 704 described in conjunction with FIG. 7, when the stub coupler 904 is placed with the end parallel to the wire 702, the guided wave 906 travelling along the stub coupler 904 couples to the wire 702, and propagates as guided\nwave 908 about the wire surface of the wire 702.  In an example embodiment, the guided wave 908 can be characterized as a surface wave or other electromagnetic wave.\nIt is noted that the graphical representations of waves 906 and 908 are presented merely to illustrate the principles that wave 906 induces or otherwise launches a guided wave 908 on a wire 702 that operates, for example, as a single wire\ntransmission line.  The actual electric and magnetic fields generated as a result of such wave propagation may vary depending on one or more of the shape and/or design of the coupler, the relative position of the dielectric waveguide to the wire, the\nfrequencies employed, the design of the stub coupler 904, the dimensions and composition of the wire 702, as well as its surface characteristics, its optional insulation, the electromagnetic properties of the surrounding environment, etc.\nIn an embodiment, an end of stub coupler 904 can taper towards the wire 702 in order to increase coupling efficiencies.  Indeed, the tapering of the end of the stub coupler 904 can provide impedance matching to the wire 702 and reduce\nreflections, according to an example embodiment of the subject disclosure.  For example, an end of the stub coupler 904 can be gradually tapered in order to obtain a desired level of coupling between waves 906 and 908 as illustrated in FIG. 9A.\nIn an embodiment, the fastener 910 can be placed such that there is a short length of the stub coupler 904 between the fastener 910 and an end of the stub coupler 904.  Maximum coupling efficiencies are realized in this embodiment when the\nlength of the end of the stub coupler 904 that is beyond the fastener 910 is at least several wavelengths long for whatever frequency is being transmitted.\nTurning now to FIG. 9B, a diagram 950 illustrating an example, non-limiting embodiment of an electromagnetic distribution in accordance with various aspects described herein is shown.  In particular, an electromagnetic distribution is presented\nin two dimensions for a transmission device that includes coupler 952, shown in an example stub coupler constructed of a dielectric material.  The coupler 952 couples an electromagnetic wave for propagation as a guided wave along an outer surface of a\nwire 702 or other transmission medium.\nThe coupler 952 guides the electromagnetic wave to a junction at x.sub.0 via a symmetrical guided wave mode.  While some of the energy of the electromagnetic wave that propagates along the coupler 952 is outside of the coupler 952, the majority\nof the energy of this electromagnetic wave is contained within the coupler 952.  The junction at x.sub.0 couples the electromagnetic wave to the wire 702 or other transmission medium at an azimuthal angle corresponding to the bottom of the transmission\nmedium.  This coupling induces an electromagnetic wave that is guided to propagate along the outer surface of the wire 702 or other transmission medium via at least one guided wave mode in direction 956.  The majority of the energy of the guided\nelectromagnetic wave is outside or, but in close proximity to the outer surface of the wire 702 or other transmission medium.  In the example shown, the junction at x.sub.0 forms an electromagnetic wave that propagates via both a symmetrical mode and at\nleast one asymmetrical surface mode, such as the first order mode presented in conjunction with FIG. 3, that skims the surface of the wire 702 or other transmission medium.\nIt is noted that the graphical representations of guided waves are presented merely to illustrate an example of guided wave coupling and propagation.  The actual electric and magnetic fields generated as a result of such wave propagation may\nvary depending on the frequencies employed, the design and/or configuration of the coupler 952, the dimensions and composition of the wire 702 or other transmission medium, as well as its surface characteristics, its insulation if present, the\nelectromagnetic properties of the surrounding environment, etc.\nTurning now to FIG. 10A, illustrated is a block diagram 1000 of an example, non-limiting embodiment of a coupler and transceiver system in accordance with various aspects described herein.  The system is an example of transmission device 101 or\n102.  In particular, the communication interface 1008 is an example of communications interface 205, the stub coupler 1002 is an example of coupler 220, and the transmitter/receiver device 1006, diplexer 1016, power amplifier 1014, low noise amplifier\n1018, frequency mixers 1010 and 1020 and local oscillator 1012 collectively form an example of transceiver 210.\nIn operation, the transmitter/receiver device 1006 launches and receives waves (e.g., guided wave 1004 onto stub coupler 1002).  The guided waves 1004 can be used to transport signals received from and sent to a host device, base station, mobile\ndevices, a building or other device by way of a communications interface 1008.  The communications interface 1008 can be an integral part of system 1000.  Alternatively, the communications interface 1008 can be tethered to system 1000.  The\ncommunications interface 1008 can comprise a wireless interface for interfacing to the host device, base station, mobile devices, a building or other device utilizing any of various wireless signaling protocols (e.g., LTE, WiFi, WiMAX, IEEE 802.xx, etc.)\nincluding an infrared protocol such as an infrared data association (IrDA) protocol or other line of sight optical protocol.  The communications interface 1008 can also comprise a wired interface such as a fiber optic line, coaxial cable, twisted pair,\ncategory 5 (CAT-5) cable or other suitable wired or optical mediums for communicating with the host device, base station, mobile devices, a building or other device via a protocol such as an Ethernet protocol, universal serial bus (USB) protocol, a data\nover cable service interface specification (DOCSIS) protocol, a digital subscriber line (DSL) protocol, a Firewire (IEEE 1394) protocol, or other wired or optical protocol.  For embodiments where system 1000 functions as a repeater, the communications\ninterface 1008 may not be necessary.\nThe output signals (e.g., Tx) of the communications interface 1008 can be combined with a carrier wave (e.g., millimeter-wave carrier wave) generated by a local oscillator 1012 at frequency mixer 1010.  Frequency mixer 1010 can use heterodyning\ntechniques or other frequency shifting techniques to frequency shift the output signals from communications interface 1008.  For example, signals sent to and from the communications interface 1008 can be modulated signals such as orthogonal frequency\ndivision multiplexed (OFDM) signals formatted in accordance with a Long-Term Evolution (LTE) wireless protocol or other wireless 3G, 4G, 5G or higher voice and data protocol, a Zigbee, WIMAX, UltraWideband or IEEE 802.11 wireless protocol; a wired\nprotocol such as an Ethernet protocol, universal serial bus (USB) protocol, a data over cable service interface specification (DOCSIS) protocol, a digital subscriber line (DSL) protocol, a Firewire (IEEE 1394) protocol or other wired or wireless\nprotocol.  In an example embodiment, this frequency conversion can be done in the analog domain, and as a result, the frequency shifting can be done without regard to the type of communications protocol used by a base station, mobile devices, or\nin-building devices.  As new communications technologies are developed, the communications interface 1008 can be upgraded (e.g., updated with software, firmware, and/or hardware) or replaced and the frequency shifting and transmission apparatus can\nremain, simplifying upgrades.  The carrier wave can then be sent to a power amplifier (\"PA\") 1014 and can be transmitted via the transmitter receiver device 1006 via the diplexer 1016.\nSignals received from the transmitter/receiver device 1006 that are directed towards the communications interface 1008 can be separated from other signals via diplexer 1016.  The received signal can then be sent to low noise amplifier (\"LNA\")\n1018 for amplification.  A frequency mixer 1020, with help from local oscillator 1012 can downshift the received signal (which is in the millimeter-wave band or around 38 GHz in some embodiments) to the native frequency.  The communications interface\n1008 can then receive the transmission at an input port (Rx).\nIn an embodiment, transmitter/receiver device 1006 can include a cylindrical or non-cylindrical metal (which, for example, can be hollow in an embodiment, but not necessarily drawn to scale) or other conducting or non-conducting waveguide and an\nend of the stub coupler 1002 can be placed in or in proximity to the waveguide or the transmitter/receiver device 1006 such that when the transmitter/receiver device 1006 generates a transmission, the guided wave couples to stub coupler 1002 and\npropagates as a guided wave 1004 about the waveguide surface of the stub coupler 1002.  In some embodiments, the guided wave 1004 can propagate in part on the outer surface of the stub coupler 1002 and in part inside the stub coupler 1002.  In other\nembodiments, the guided wave 1004 can propagate substantially or completely on the outer surface of the stub coupler 1002.  In yet other embodiments, the guided wave 1004 can propagate substantially or completely inside the stub coupler 1002.  In this\nlatter embodiment, the guided wave 1004 can radiate at an end of the stub coupler 1002 (such as the tapered end shown in FIG. 4) for coupling to a transmission medium such as a wire 702 of FIG. 7.  Similarly, if guided wave 1004 is incoming (coupled to\nthe stub coupler 1002 from a wire 702), guided wave 1004 then enters the transmitter/receiver device 1006 and couples to the cylindrical waveguide or conducting waveguide.  While transmitter/receiver device 1006 is shown to include a separate\nwaveguide--an antenna, cavity resonator, klystron, magnetron, travelling wave tube, or other radiating element can be employed to induce a guided wave on the coupler 1002, with or without the separate waveguide.\nIn an embodiment, stub coupler 1002 can be wholly constructed of a dielectric material (or another suitable insulating material), without any metallic or otherwise conducting materials therein.  Stub coupler 1002 can be composed of nylon,\nTeflon, polyethylene, a polyamide, other plastics, or other materials that are non-conducting and suitable for facilitating transmission of electromagnetic waves at least in part on an outer surface of such materials.  In another embodiment, stub coupler\n1002 can include a core that is conducting/metallic, and have an exterior dielectric surface.  Similarly, a transmission medium that couples to the stub coupler 1002 for propagating electromagnetic waves induced by the stub coupler 1002 or for supplying\nelectromagnetic waves to the stub coupler 1002 can, in addition to being a bare or insulated wire, be wholly constructed of a dielectric material (or another suitable insulating material), without any metallic or otherwise conducting materials therein.\nIt is noted that although FIG. 10A shows that the opening of transmitter receiver device 1006 is much wider than the stub coupler 1002, this is not to scale, and that in other embodiments the width of the stub coupler 1002 is comparable or\nslightly smaller than the opening of the hollow waveguide.  It is also not shown, but in an embodiment, an end of the coupler 1002 that is inserted into the transmitter/receiver device 1006 tapers down in order to reduce reflection and increase coupling\nefficiencies.\nBefore coupling to the stub coupler 1002, the one or more waveguide modes of the guided wave generated by the transmitter/receiver device 1006 can couple to the stub coupler 1002 to induce one or more wave propagation modes of the guided wave\n1004.  The wave propagation modes of the guided wave 1004 can be different than the hollow metal waveguide modes due to the different characteristics of the hollow metal waveguide and the dielectric waveguide.  For instance, wave propagation modes of the\nguided wave 1004 can comprise the fundamental transverse electromagnetic mode (Quasi-TEM.sub.00), where only small electrical and/or magnetic fields extend in the direction of propagation, and the electric and magnetic fields extend radially outwards\nfrom the stub coupler 1002 while the guided waves propagate along the stub coupler 1002.  The fundamental transverse electromagnetic mode wave propagation mode may or may not exist inside a waveguide that is hollow.  Therefore, the hollow metal waveguide\nmodes that are used by transmitter/receiver device 1006 are waveguide modes that can couple effectively and efficiently to wave propagation modes of stub coupler 1002.\nIt will be appreciated that other constructs or combinations of the transmitter/receiver device 1006 and stub coupler 1002 are possible.  For example, a stub coupler 1002' can be placed tangentially or in parallel (with or without a gap) with\nrespect to an outer surface of the hollow metal waveguide of the transmitter/receiver device 1006' (corresponding circuitry not shown) as depicted by reference 1000' of FIG. 10B.  In another embodiment, not shown by reference 1000', the stub coupler\n1002' can be placed inside the hollow metal waveguide of the transmitter/receiver device 1006' without an axis of the stub coupler 1002' being coaxially aligned with an axis of the hollow metal waveguide of the transmitter/receiver device 1006'.  In\neither of these embodiments, the guided wave generated by the transmitter/receiver device 1006' can couple to a surface of the stub coupler 1002' to induce one or more wave propagation modes of the guided wave 1004' on the stub coupler 1002' including a\nfundamental mode (e.g., a symmetric mode) and/or a non-fundamental mode (e.g., asymmetric mode).\nIn one embodiment, the guided wave 1004' can propagate in part on the outer surface of the stub coupler 1002' and in part inside the stub coupler 1002'.  In another embodiment, the guided wave 1004' can propagate substantially or completely on\nthe outer surface of the stub coupler 1002'.  In yet other embodiments, the guided wave 1004' can propagate substantially or completely inside the stub coupler 1002'.  In this latter embodiment, the guided wave 1004' can radiate at an end of the stub\ncoupler 1002' (such as the tapered end shown in FIG. 9) for coupling to a transmission medium such as a wire 702 of FIG. 9.\nIt will be further appreciated that other constructs the transmitter/receiver device 1006 are possible.  For example, a hollow metal waveguide of a transmitter/receiver device 1006'' (corresponding circuitry not shown), depicted in FIG. 10B as\nreference 1000'', can be placed tangentially or in parallel (with or without a gap) with respect to an outer surface of a transmission medium such as the wire 702 of FIG. 4 without the use of the stub coupler 1002.  In this embodiment, the guided wave\ngenerated by the transmitter/receiver device 1006'' can couple to a surface of the wire 702 to induce one or more wave propagation modes of a guided wave 908 on the wire 702 including a fundamental mode (e.g., a symmetric mode) and/or a non-fundamental\nmode (e.g., asymmetric mode).  In another embodiment, the wire 702 can be positioned inside a hollow metal waveguide of a transmitter/receiver device 1006' (corresponding circuitry not shown) so that an axis of the wire 702 is coaxially (or not\ncoaxially) aligned with an axis of the hollow metal waveguide without the use of the stub coupler 1002--see FIG. 10B reference 1000'''.  In this embodiment, the guided wave generated by the transmitter/receiver device 1006''' can couple to a surface of\nthe wire 702 to induce one or more wave propagation modes of a guided wave 908 on the wire including a fundamental mode (e.g., a symmetric mode) and/or a non-fundamental mode (e.g., asymmetric mode).\nIn the embodiments of 1000'' and 1000''', for a wire 702 having an insulated outer surface, the guided wave 908 can propagate in part on the outer surface of the insulator and in part inside the insulator.  In embodiments, the guided wave 908\ncan propagate substantially or completely on the outer surface of the insulator, or substantially or completely inside the insulator.  In the embodiments of 1000'' and 1000''', for a wire 702 that is a bare conductor, the guided wave 908 can propagate in\npart on the outer surface of the conductor and in part inside the conductor.  In another embodiment, the guided wave 908 can propagate substantially or completely on the outer surface of the conductor.\nReferring now to FIG. 11, a block diagram 1100 illustrating an example, non-limiting embodiment of a dual stub coupler is shown.  In particular, a dual coupler design is presented for use in a transmission device, such as transmission device 101\nor 102 presented in conjunction with FIG. 1.  In an embodiment, two or more couplers (such as the stub couplers 1104 and 1106) can be positioned around a wire 1102 in order to receive guided wave 1108.  In an embodiment, one coupler is enough to receive\nthe guided wave 1108.  In that case, guided wave 1108 couples to coupler 1104 and propagates as guided wave 1110.  If the field structure of the guided wave 1108 oscillates or undulates around the wire 1102 due to the particular guided wave mode(s) or\nvarious outside factors, then coupler 1106 can be placed such that guided wave 1108 couples to coupler 1106.  In some embodiments, four or more couplers can be placed around a portion of the wire 1102, e.g., at 90 degrees or another spacing with respect\nto each other, in order to receive guided waves that may oscillate or rotate around the wire 1102, that have been induced at different azimuthal orientations or that have non-fundamental or higher order modes that, for example, have lobes and/or nulls or\nother asymmetries that are orientation dependent.  However, it will be appreciated that there may be less than or more than four couplers placed around a portion of the wire 1102 without departing from example embodiments.\nIt should be noted that while couplers 1106 and 1104 are illustrated as stub couplers, any other of the coupler designs described herein including arc couplers, antenna or horn couplers, magnetic couplers, etc., could likewise be used.  It will\nalso be appreciated that while some example embodiments have presented a plurality of couplers around at least a portion of a wire 1102, this plurality of couplers can also be considered as part of a single coupler system having multiple coupler\nsubcomponents.  For example, two or more couplers can be manufactured as single system that can be installed around a wire in a single installation such that the couplers are either pre-positioned or adjustable relative to each other (either manually or\nautomatically with a controllable mechanism such as a motor or other actuator) in accordance with the single system.\nReceivers coupled to couplers 1106 and 1104 can use diversity combining to combine signals received from both couplers 1106 and 1104 in order to maximize the signal quality.  In other embodiments, if one or the other of the couplers 1104 and\n1106 receive a transmission that is above a predetermined threshold, receivers can use selection diversity when deciding which signal to use.  Further, while reception by a plurality of couplers 1106 and 1104 is illustrated, transmission by couplers 1106\nand 1104 in the same configuration can likewise take place.  In particular, a wide range of multi-input multi-output (MIMO) transmission and reception techniques can be employed for transmissions where a transmission device, such as transmission device\n101 or 102 presented in conjunction with FIG. 1 includes multiple transceivers and multiple couplers.\nIt is noted that the graphical representations of waves 1108 and 1110 are presented merely to illustrate the principles that guided wave 1108 induces or otherwise launches a wave 1110 on a coupler 1104.  The actual electric and magnetic fields\ngenerated as a result of such wave propagation may vary depending on the frequencies employed, the design of the coupler 1104, the dimensions and composition of the wire 1102, as well as its surface characteristics, its insulation if any, the\nelectromagnetic properties of the surrounding environment, etc.\nReferring now to FIG. 12, a block diagram 1200 illustrating an example, non-limiting embodiment of a repeater system is shown.  In particular, a repeater device 1210 is presented for use in a transmission device, such as transmission device 101\nor 102 presented in conjunction with FIG. 1.  In this system, two couplers 1204 and 1214 can be placed near a wire 1202 or other transmission medium such that guided waves 1205 propagating along the wire 1202 are extracted by coupler 1204 as wave 1206\n(e.g. as a guided wave), and then are boosted or repeated by repeater device 1210 and launched as a wave 1216 (e.g. as a guided wave) onto coupler 1214.  The wave 1216 can then be launched on the wire 1202 and continue to propagate along the wire 1202 as\na guided wave 1217.  In an embodiment, the repeater device 1210 can receive at least a portion of the power utilized for boosting or repeating through magnetic coupling with the wire 1202, for example, when the wire 1202 is a power line or otherwise\ncontains a power-carrying conductor.  It should be noted that while couplers 1204 and 1214 are illustrated as stub couplers, any other of the coupler designs described herein including arc couplers, antenna or horn couplers, magnetic couplers, or the\nlike, could likewise be used.\nIn some embodiments, repeater device 1210 can repeat the transmission associated with wave 1206, and in other embodiments, repeater device 1210 can include a communications interface 205 that extracts data or other signals from the wave 1206 for\nsupplying such data or signals to another network and/or one or more other devices as communication signals 110 or 112 and/or receiving communication signals 110 or 112 from another network and/or one or more other devices and launch guided wave 1216\nhaving embedded therein the received communication signals 110 or 112.  In a repeater configuration, receiver waveguide 1208 can receive the wave 1206 from the coupler 1204 and transmitter waveguide 1212 can launch guided wave 1216 onto coupler 1214 as\nguided wave 1217.  Between receiver waveguide 1208 and transmitter waveguide 1212, the signal embedded in guided wave 1206 and/or the guided wave 1216 itself can be amplified to correct for signal loss and other inefficiencies associated with guided wave\ncommunications or the signal can be received and processed to extract the data contained therein and regenerated for transmission.  In an embodiment, the receiver waveguide 1208 can be configured to extract data from the signal, process the data to\ncorrect for data errors utilizing for example error correcting codes, and regenerate an updated signal with the corrected data.  The transmitter waveguide 1212 can then transmit guided wave 1216 with the updated signal embedded therein.  In an\nembodiment, a signal embedded in guided wave 1206 can be extracted from the transmission and processed for communication with another network and/or one or more other devices via communications interface 205 as communication signals 110 or 112. \nSimilarly, communication signals 110 or 112 received by the communications interface 205 can be inserted into a transmission of guided wave 1216 that is generated and launched onto coupler 1214 by transmitter waveguide 1212.\nIt is noted that although FIG. 12 shows guided wave transmissions 1206 and 1216 entering from the left and exiting to the right respectively, this is merely a simplification and is not intended to be limiting.  In other embodiments, receiver\nwaveguide 1208 and transmitter waveguide 1212 can also function as transmitters and receivers respectively, allowing the repeater device 1210 to be bi-directional.\nIn an embodiment, repeater device 1210 can be placed at locations where there are discontinuities or obstacles on the wire 1202 or other transmission medium.  In the case where the wire 1202 is a power line, these obstacles can include\ntransformers, connections, utility poles, and other such power line devices.  The repeater device 1210 can help the guided (e.g., surface) waves jump over these obstacles on the line and boost the transmission power at the same time.  In other\nembodiments, a coupler can be used to jump over the obstacle without the use of a repeater device.  In that embodiment, both ends of the coupler can be tied or fastened to the wire, thus providing a path for the guided wave to travel without being\nblocked by the obstacle.\nTurning now to FIG. 13, illustrated is a block diagram 1300 of an example, non-limiting embodiment of a bidirectional repeater in accordance with various aspects described herein.  In particular, a bidirectional repeater device 1306 is presented\nfor use in a transmission device, such as transmission device 101 or 102 presented in conjunction with FIG. 1.  It should be noted that while the couplers are illustrated as stub couplers, any other of the coupler designs described herein including arc\ncouplers, antenna or horn couplers, magnetic couplers, or the like, could likewise be used.  The bidirectional repeater 1306 can employ diversity paths in the case of when two or more wires or other transmission media are present.  Since guided wave\ntransmissions have different transmission efficiencies and coupling efficiencies for transmission medium of different types such as insulated wires, un-insulated wires or other types of transmission media and further, if exposed to the elements, can be\naffected by weather, and other atmospheric conditions, it can be advantageous to selectively transmit on different transmission media at certain times.  In various embodiments, the various transmission media can be designated as a primary, secondary,\ntertiary, etc. whether or not such designation indicates a preference of one transmission medium over another.\nIn the embodiment shown, the transmission media include an insulated or uninsulated wire 1302 and an insulated or uninsulated wire 1304 (referred to herein as wires 1302 and 1304, respectively).  The repeater device 1306 uses a receiver coupler\n1308 to receive a guided wave traveling along wire 1302 and repeats the transmission using transmitter waveguide 1310 as a guided wave along wire 1304.  In other embodiments, repeater device 1306 can switch from the wire 1304 to the wire 1302, or can\nrepeat the transmissions along the same paths.  Repeater device 1306 can include sensors, or be in communication with sensors (or a network management system 1601 depicted in FIG. 16A) that indicate conditions that can affect the transmission.  Based on\nthe feedback received from the sensors, the repeater device 1306 can make the determination about whether to keep the transmission along the same wire, or transfer the transmission to the other wire.\nTurning now to FIG. 14, illustrated is a block diagram 1400 illustrating an example, non-limiting embodiment of a bidirectional repeater system.  In particular, a bidirectional repeater system is presented for use in a transmission device, such\nas transmission device 101 or 102 presented in conjunction with FIG. 1.  The bidirectional repeater system includes waveguide coupling devices 1402 and 1404 that receive and transmit transmissions from other coupling devices located in a distributed\nantenna system or backhaul system.\nIn various embodiments, waveguide coupling device 1402 can receive a transmission from another waveguide coupling device, wherein the transmission has a plurality of subcarriers.  Diplexer 1406 can separate the transmission from other\ntransmissions, and direct the transmission to low-noise amplifier (\"LNA\") 1408.  A frequency mixer 1428, with help from a local oscillator 1412, can downshift the transmission (which is in the millimeter-wave band or around 38 GHz in some embodiments) to\na lower frequency, such as a cellular band (.about.1.9 GHz) for a distributed antenna system, a native frequency, or other frequency for a backhaul system.  An extractor (or demultiplexer) 1432 can extract the signal on a subcarrier and direct the signal\nto an output component 1422 for optional amplification, buffering or isolation by power amplifier 1424 for coupling to communications interface 205.  The communications interface 205 can further process the signals received from the power amplifier 1424\nor otherwise transmit such signals over a wireless or wired interface to other devices such as a base station, mobile devices, a building, etc. For the signals that are not being extracted at this location, extractor 1432 can redirect them to another\nfrequency mixer 1436, where the signals are used to modulate a carrier wave generated by local oscillator 1414.  The carrier wave, with its subcarriers, is directed to a power amplifier (\"PA\") 1416 and is retransmitted by waveguide coupling device 1404\nto another system, via diplexer 1420.\nAn LNA 1426 can be used to amplify, buffer or isolate signals that are received by the communication interface 205 and then send the signal to a multiplexer 1434 which merges the signal with signals that have been received from waveguide\ncoupling device 1404.  The signals received from coupling device 1404 have been split by diplexer 1420, and then passed through LNA 1418, and downshifted in frequency by frequency mixer 1438.  When the signals are combined by multiplexer 1434, they are\nupshifted in frequency by frequency mixer 1430, and then boosted by PA 1410, and transmitted to another system by waveguide coupling device 1402.  In an embodiment bidirectional repeater system can be merely a repeater without the output device 1422.  In\nthis embodiment, the multiplexer 1434 would not be utilized and signals from LNA 1418 would be directed to mixer 1430 as previously described.  It will be appreciated that in some embodiments, the bidirectional repeater system could also be implemented\nusing two distinct and separate unidirectional repeaters.  In an alternative embodiment, a bidirectional repeater system could also be a booster or otherwise perform retransmissions without downshifting and upshifting.  Indeed in example embodiment, the\nretransmissions can be based upon receiving a signal or guided wave and performing some signal or guided wave processing or reshaping, filtering, and/or amplification, prior to retransmission of the signal or guided wave.\nReferring now to FIG. 15, a block diagram 1500 illustrating an example, non-limiting embodiment of a guided wave communications system is shown.  This diagram depicts an exemplary environment in which a guided wave communication system, such as\nthe guided wave communication system presented in conjunction with FIG. 1, can be used.\nTo provide network connectivity to additional base station devices, a backhaul network that links the communication cells (e.g., macrocells and macrocells) to network devices of a core network correspondingly expands.  Similarly, to provide\nnetwork connectivity to a distributed antenna system, an extended communication system that links base station devices and their distributed antennas is desirable.  A guided wave communication system 1500 such as shown in FIG. 15 can be provided to\nenable alternative, increased or additional network connectivity and a waveguide coupling system can be provided to transmit and/or receive guided wave (e.g., surface wave) communications on a transmission medium such as a wire that operates as a\nsingle-wire transmission line (e.g., a utility line), and that can be used as a waveguide and/or that otherwise operates to guide the transmission of an electromagnetic wave.\nThe guided wave communication system 1500 can comprise a first instance of a distribution system 1550 that includes one or more base station devices (e.g., base station device 1504) that are communicably coupled to a central office 1501 and/or a\nmacrocell site 1502.  Base station device 1504 can be connected by a wired (e.g., fiber and/or cable), or by a wireless (e.g., microwave wireless) connection to the macrocell site 1502 and the central office 1501.  A second instance of the distribution\nsystem 1560 can be used to provide wireless voice and data services to mobile device 1522 and to residential and/or commercial establishments 1542 (herein referred to as establishments 1542).  System 1500 can have additional instances of the distribution\nsystems 1550 and 1560 for providing voice and/or data services to mobile devices 1522-1524 and establishments 1542 as shown in FIG. 15.\nMacrocells such as macrocell site 1502 can have dedicated connections to a mobile network and base station device 1504 or can share and/or otherwise use another connection.  Central office 1501 can be used to distribute media content and/or\nprovide internet service provider (ISP) services to mobile devices 1522-1524 and establishments 1542.  The central office 1501 can receive media content from a constellation of satellites 1530 (one of which is shown in FIG. 15) or other sources of\ncontent, and distribute such content to mobile devices 1522-1524 and establishments 1542 via the first and second instances of the distribution system 1550 and 1560.  The central office 1501 can also be communicatively coupled to the Internet 1503 for\nproviding internet data services to mobile devices 1522-1524 and establishments 1542.\nBase station device 1504 can be mounted on, or attached to, utility pole 1516.  In other embodiments, base station device 1504 can be near transformers and/or other locations situated nearby a power line.  Base station device 1504 can facilitate\nconnectivity to a mobile network for mobile devices 1522 and 1524.  Antennas 1512 and 1514, mounted on or near utility poles 1518 and 1520, respectively, can receive signals from base station device 1504 and transmit those signals to mobile devices 1522\nand 1524 over a much wider area than if the antennas 1512 and 1514 were located at or near base station device 1504.\nIt is noted that FIG. 15 displays three utility poles, in each instance of the distribution systems 1550 and 1560, with one base station device, for purposes of simplicity.  In other embodiments, utility pole 1516 can have more base station\ndevices, and more utility poles with distributed antennas and/or tethered connections to establishments 1542.\nA transmission device 1506, such as transmission device 101 or 102 presented in conjunction with FIG. 1, can transmit a signal from base station device 1504 to antennas 1512 and 1514 via utility or power line(s) that connect the utility poles\n1516, 1518, and 1520.  To transmit the signal, radio source and/or transmission device 1506 upconverts the signal (e.g., via frequency mixing) from base station device 1504 or otherwise converts the signal from the base station device 1504 to a microwave\nband signal and the transmission device 1506 launches a microwave band wave that propagates as a guided wave traveling along the utility line or other wire as described in previous embodiments.  At utility pole 1518, another transmission device 1508\nreceives the guided wave (and optionally can amplify it as needed or desired or operate as a repeater to receive it and regenerate it) and sends it forward as a guided wave on the utility line or other wire.  The transmission device 1508 can also extract\na signal from the microwave band guided wave and shift it down in frequency or otherwise convert it to its original cellular band frequency (e.g., 1.9 GHz or other defined cellular frequency) or another cellular (or non-cellular) band frequency.  An\nantenna 1512 can wireless transmit the downshifted signal to mobile device 1522.  The process can be repeated by transmission device 1510, antenna 1514 and mobile device 1524, as necessary or desirable.\nTransmissions from mobile devices 1522 and 1524 can also be received by antennas 1512 and 1514 respectively.  The transmission devices 1508 and 1510 can upshift or otherwise convert the cellular band signals to microwave band and transmit the\nsignals as guided wave (e.g., surface wave or other electromagnetic wave) transmissions over the power line(s) to base station device 1504.\nMedia content received by the central office 1501 can be supplied to the second instance of the distribution system 1560 via the base station device 1504 for distribution to mobile devices 1522 and establishments 1542.  The transmission device\n1510 can be tethered to the establishments 1542 by one or more wired connections or a wireless interface.  The one or more wired connections may include without limitation, a power line, a coaxial cable, a fiber cable, a twisted pair cable, a guided wave\ntransmission medium or other suitable wired mediums for distribution of media content and/or for providing internet services.  In an example embodiment, the wired connections from the transmission device 1510 can be communicatively coupled to one or more\nvery high bit rate digital subscriber line (VDSL) modems located at one or more corresponding service area interfaces (SAIs--not shown) or pedestals, each SAI or pedestal providing services to a portion of the establishments 1542.  The VDSL modems can be\nused to selectively distribute media content and/or provide internet services to gateways (not shown) located in the establishments 1542.  The SAIs or pedestals can also be communicatively coupled to the establishments 1542 over a wired medium such as a\npower line, a coaxial cable, a fiber cable, a twisted pair cable, a guided wave transmission medium or other suitable wired mediums.  In other example embodiments, the transmission device 1510 can be communicatively coupled directly to establishments\n1542 without intermediate interfaces such as the SAIs or pedestals.\nIn another example embodiment, system 1500 can employ diversity paths, where two or more utility lines or other wires are strung between the utility poles 1516, 1518, and 1520 (e.g., for example, two or more wires between poles 1516 and 1520)\nand redundant transmissions from base station/macrocell site 1502 are transmitted as guided waves down the surface of the utility lines or other wires.  The utility lines or other wires can be either insulated or uninsulated, and depending on the\nenvironmental conditions that cause transmission losses, the coupling devices can selectively receive signals from the insulated or uninsulated utility lines or other wires.  The selection can be based on measurements of the signal-to-noise ratio of the\nwires, or based on determined weather/environmental conditions (e.g., moisture detectors, weather forecasts, etc.).  The use of diversity paths with system 1500 can enable alternate routing capabilities, load balancing, increased load handling,\nconcurrent bi-directional or synchronous communications, spread spectrum communications, etc.\nIt is noted that the use of the transmission devices 1506, 1508, and 1510 in FIG. 15 are by way of example only, and that in other embodiments, other uses are possible.  For instance, transmission devices can be used in a backhaul communication\nsystem, providing network connectivity to base station devices.  Transmission devices 1506, 1508, and 1510 can be used in many circumstances where it is desirable to transmit guided wave communications over a wire, whether insulated or not insulated. \nTransmission devices 1506, 1508, and 1510 are improvements over other coupling devices due to no contact or limited physical and/or electrical contact with the wires that may carry high voltages.  The transmission device can be located away from the wire\n(e.g., spaced apart from the wire) and/or located on the wire so long as it is not electrically in contact with the wire, as the dielectric acts as an insulator, allowing for cheap, easy, and/or less complex installation.  However, as previously noted\nconducting or non-dielectric couplers can be employed, for example in configurations where the wires correspond to a telephone network, cable television network, broadband data service, fiber optic communications system or other network employing low\nvoltages or having insulated transmission lines.\nIt is further noted, that while base station device 1504 and macrocell site 1502 are illustrated in an embodiment, other network configurations are likewise possible.  For example, devices such as access points or other wireless gateways can be\nemployed in a similar fashion to extend the reach of other networks such as a wireless local area network, a wireless personal area network or other wireless network that operates in accordance with a communication protocol such as a 802.11 protocol,\nWIMAX protocol, UltraWideband protocol, Bluetooth protocol, Zigbee protocol or other wireless protocol.\nReferring now to FIGS. 16A & 16B, block diagrams illustrating an example, non-limiting embodiment of a system for managing a power grid communication system are shown.  Considering FIG. 16A, a waveguide system 1602 is presented for use in a\nguided wave communications system, such as the system presented in conjunction with FIG. 15.  The waveguide system 1602 can comprise sensors 1604, a power management system 1605, a transmission device 101 or 102 that includes at least one communication\ninterface 205, transceiver 210 and coupler 220.\nThe waveguide system 1602 can be coupled to a power line 1610 for facilitating guided wave communications in accordance with embodiments described in the subject disclosure.  In an example embodiment, the transmission device 101 or 102 includes\ncoupler 220 for inducing electromagnetic waves on a surface of the power line 1610 that longitudinally propagate along the surface of the power line 1610 as described in the subject disclosure.  The transmission device 101 or 102 can also serve as a\nrepeater for retransmitting electromagnetic waves on the same power line 1610 or for routing electromagnetic waves between power lines 1610 as shown in FIGS. 12-13.\nThe transmission device 101 or 102 includes transceiver 210 configured to, for example, up-convert a signal operating at an original frequency range to electromagnetic waves operating at, exhibiting, or associated with a carrier frequency that\npropagate along a coupler to induce corresponding guided electromagnetic waves that propagate along a surface of the power line 1610.  A carrier frequency can be represented by a center frequency having upper and lower cutoff frequencies that define the\nbandwidth of the electromagnetic waves.  The power line 1610 can be a wire (e.g., single stranded or multi-stranded) having a conducting surface or insulated surface.  The transceiver 210 can also receive signals from the coupler 220 and down-convert the\nelectromagnetic waves operating at a carrier frequency to signals at their original frequency.\nSignals received by the communications interface 205 of transmission device 101 or 102 for up-conversion can include without limitation signals supplied by a central office 1611 over a wired or wireless interface of the communications interface\n205, a base station 1614 over a wired or wireless interface of the communications interface 205, wireless signals transmitted by mobile devices 1620 to the base station 1614 for delivery over the wired or wireless interface of the communications\ninterface 205, signals supplied by in-building communication devices 1618 over the wired or wireless interface of the communications interface 205, and/or wireless signals supplied to the communications interface 205 by mobile devices 1612 roaming in a\nwireless communication range of the communications interface 205.  In embodiments where the waveguide system 1602 functions as a repeater, such as shown in FIGS. 12-13, the communications interface 205 may or may not be included in the waveguide system\n1602.\nThe electromagnetic waves propagating along the surface of the power line 1610 can be modulated and formatted to include packets or frames of data that include a data payload and further include networking information (such as header information\nfor identifying one or more destination waveguide systems 1602).  The networking information may be provided by the waveguide system 1602 or an originating device such as the central office 1611, the base station 1614, mobile devices 1620, or in-building\ndevices 1618, or a combination thereof.  Additionally, the modulated electromagnetic waves can include error correction data for mitigating signal disturbances.  The networking information and error correction data can be used by a destination waveguide\nsystem 1602 for detecting transmissions directed to it, and for down-converting and processing with error correction data transmissions that include voice and/or data signals directed to recipient communication devices communicatively coupled to the\ndestination waveguide system 1602.\nReferring now to the sensors 1604 of the waveguide system 1602, the sensors 1604 can comprise one or more of a temperature sensor 1604a, a disturbance detection sensor 1604b, a loss of energy sensor 1604c, a noise sensor 1604d, a vibration\nsensor 1604e, an environmental (e.g., weather) sensor 1604f, and/or an image sensor 1604g.  The temperature sensor 1604a can be used to measure ambient temperature, a temperature of the transmission device 101 or 102, a temperature of the power line\n1610, temperature differentials (e.g., compared to a setpoint or baseline, between transmission device 101 or 102 and 1610, etc.), or any combination thereof.  In one embodiment, temperature metrics can be collected and reported periodically to a network\nmanagement system 1601 by way of the base station 1614.\nThe disturbance detection sensor 1604b can perform measurements on the power line 1610 to detect disturbances such as signal reflections, which may indicate a presence of a downstream disturbance that may impede the propagation of\nelectromagnetic waves on the power line 1610.  A signal reflection can represent a distortion resulting from, for example, an electromagnetic wave transmitted on the power line 1610 by the transmission device 101 or 102 that reflects in whole or in part\nback to the transmission device 101 or 102 from a disturbance in the power line 1610 located downstream from the transmission device 101 or 102.\nSignal reflections can be caused by obstructions on the power line 1610.  For example, a tree limb may cause electromagnetic wave reflections when the tree limb is lying on the power line 1610, or is in close proximity to the power line 1610\nwhich may cause a corona discharge.  Other obstructions that can cause electromagnetic wave reflections can include without limitation an object that has been entangled on the power line 1610 (e.g., clothing, a shoe wrapped around a power line 1610 with\na shoe string, etc.), a corroded build-up on the power line 1610 or an ice build-up.  Power grid components may also impede or obstruct with the propagation of electromagnetic waves on the surface of power lines 1610.  Illustrations of power grid\ncomponents that may cause signal reflections include without limitation a transformer and a joint for connecting spliced power lines.  A sharp angle on the power line 1610 may also cause electromagnetic wave reflections.\nThe disturbance detection sensor 1604b can comprise a circuit to compare magnitudes of electromagnetic wave reflections to magnitudes of original electromagnetic waves transmitted by the transmission device 101 or 102 to determine how much a\ndownstream disturbance in the power line 1610 attenuates transmissions.  The disturbance detection sensor 1604b can further comprise a spectral analyzer circuit for performing spectral analysis on the reflected waves.  The spectral data generated by the\nspectral analyzer circuit can be compared with spectral profiles via pattern recognition, an expert system, curve fitting, matched filtering or other artificial intelligence, classification or comparison technique to identify a type of disturbance based\non, for example, the spectral profile that most closely matches the spectral data.  The spectral profiles can be stored in a memory of the disturbance detection sensor 1604b or may be remotely accessible by the disturbance detection sensor 1604b.  The\nprofiles can comprise spectral data that models different disturbances that may be encountered on power lines 1610 to enable the disturbance detection sensor 1604b to identify disturbances locally.  An identification of the disturbance if known can be\nreported to the network management system 1601 by way of the base station 1614.  The disturbance detection sensor 1604b can also utilize the transmission device 101 or 102 to transmit electromagnetic waves as test signals to determine a roundtrip time\nfor an electromagnetic wave reflection.  The round trip time measured by the disturbance detection sensor 1604b can be used to calculate a distance traveled by the electromagnetic wave up to a point where the reflection takes place, which enables the\ndisturbance detection sensor 1604b to calculate a distance from the transmission device 101 or 102 to the downstream disturbance on the power line 1610.\nThe distance calculated can be reported to the network management system 1601 by way of the base station 1614.  In one embodiment, the location of the waveguide system 1602 on the power line 1610 may be known to the network management system\n1601, which the network management system 1601 can use to determine a location of the disturbance on the power line 1610 based on a known topology of the power grid.  In another embodiment, the waveguide system 1602 can provide its location to the\nnetwork management system 1601 to assist in the determination of the location of the disturbance on the power line 1610.  The location of the waveguide system 1602 can be obtained by the waveguide system 1602 from a pre-programmed location of the\nwaveguide system 1602 stored in a memory of the waveguide system 1602, or the waveguide system 1602 can determine its location using a GPS receiver (not shown) included in the waveguide system 1602.\nThe power management system 1605 provides energy to the aforementioned components of the waveguide system 1602.  The power management system 1605 can receive energy from solar cells, or from a transformer (not shown) coupled to the power line\n1610, or by inductive coupling to the power line 1610 or another nearby power line.  The power management system 1605 can also include a backup battery and/or a super capacitor or other capacitor circuit for providing the waveguide system 1602 with\ntemporary power.  The loss of energy sensor 1604c can be used to detect when the waveguide system 1602 has a loss of power condition and/or the occurrence of some other malfunction.  For example, the loss of energy sensor 1604c can detect when there is a\nloss of power due to defective solar cells, an obstruction on the solar cells that causes them to malfunction, loss of power on the power line 1610, and/or when the backup power system malfunctions due to expiration of a backup battery, or a detectable\ndefect in a super capacitor.  When a malfunction and/or loss of power occurs, the loss of energy sensor 1604c can notify the network management system 1601 by way of the base station 1614.\nThe noise sensor 1604d can be used to measure noise on the power line 1610 that may adversely affect transmission of electromagnetic waves on the power line 1610.  The noise sensor 1604d can sense unexpected electromagnetic interference, noise\nbursts, or other sources of disturbances that may interrupt reception of modulated electromagnetic waves on a surface of a power line 1610.  A noise burst can be caused by, for example, a corona discharge, or other source of noise.  The noise sensor\n1604d can compare the measured noise to a noise profile obtained by the waveguide system 1602 from an internal database of noise profiles or from a remotely located database that stores noise profiles via pattern recognition, an expert system, curve\nfitting, matched filtering or other artificial intelligence, classification or comparison technique.  From the comparison, the noise sensor 1604d may identify a noise source (e.g., corona discharge or otherwise) based on, for example, the noise profile\nthat provides the closest match to the measured noise.  The noise sensor 1604d can also detect how noise affects transmissions by measuring transmission metrics such as bit error rate, packet loss rate, jitter, packet retransmission requests, etc. The\nnoise sensor 1604d can report to the network management system 1601 by way of the base station 1614 the identity of noise sources, their time of occurrence, and transmission metrics, among other things.\nThe vibration sensor 1604e can include accelerometers and/or gyroscopes to detect 2D or 3D vibrations on the power line 1610.  The vibrations can be compared to vibration profiles that can be stored locally in the waveguide system 1602, or\nobtained by the waveguide system 1602 from a remote database via pattern recognition, an expert system, curve fitting, matched filtering or other artificial intelligence, classification or comparison technique.  Vibration profiles can be used, for\nexample, to distinguish fallen trees from wind gusts based on, for example, the vibration profile that provides the closest match to the measured vibrations.  The results of this analysis can be reported by the vibration sensor 1604e to the network\nmanagement system 1601 by way of the base station 1614.\nThe environmental sensor 1604f can include a barometer for measuring atmospheric pressure, ambient temperature (which can be provided by the temperature sensor 1604a), wind speed, humidity, wind direction, and rainfall, among other things.  The\nenvironmental sensor 1604f can collect raw information and process this information by comparing it to environmental profiles that can be obtained from a memory of the waveguide system 1602 or a remote database to predict weather conditions before they\narise via pattern recognition, an expert system, knowledge-based system or other artificial intelligence, classification or other weather modeling and prediction technique.  The environmental sensor 1604f can report raw data as well as its analysis to\nthe network management system 1601.\nThe image sensor 1604g can be a digital camera (e.g., a charged coupled device or CCD imager, infrared camera, etc.) for capturing images in a vicinity of the waveguide system 1602.  The image sensor 1604g can include an electromechanical\nmechanism to control movement (e.g., actual position or focal points/zooms) of the camera for inspecting the power line 1610 from multiple perspectives (e.g., top surface, bottom surface, left surface, right surface and so on).  Alternatively, the image\nsensor 1604g can be designed such that no electromechanical mechanism is needed in order to obtain the multiple perspectives.  The collection and retrieval of imaging data generated by the image sensor 1604g can be controlled by the network management\nsystem 1601, or can be autonomously collected and reported by the image sensor 1604g to the network management system 1601.\nOther sensors that may be suitable for collecting telemetry information associated with the waveguide system 1602 and/or the power lines 1610 for purposes of detecting, predicting and/or mitigating disturbances that can impede the propagation of\nelectromagnetic wave transmissions on power lines 1610 (or any other form of a transmission medium of electromagnetic waves) may be utilized by the waveguide system 1602.\nReferring now to FIG. 16B, block diagram 1650 illustrates an example, non-limiting embodiment of a system for managing a power grid 1653 and a communication system 1655 embedded therein or associated therewith in accordance with various aspects\ndescribed herein.  The communication system 1655 comprises a plurality of waveguide systems 1602 coupled to power lines 1610 of the power grid 1653.  At least a portion of the waveguide systems 1602 used in the communication system 1655 can be in direct\ncommunication with a base station 1614 and/or the network management system 1601.  Waveguide systems 1602 not directly connected to a base station 1614 or the network management system 1601 can engage in communication sessions with either a base station\n1614 or the network management system 1601 by way of other downstream waveguide systems 1602 connected to a base station 1614 or the network management system 1601.\nThe network management system 1601 can be communicatively coupled to equipment of a utility company 1652 and equipment of a communications service provider 1654 for providing each entity, status information associated with the power grid 1653\nand the communication system 1655, respectively.  The network management system 1601, the equipment of the utility company 1652, and the communications service provider 1654 can access communication devices utilized by utility company personnel 1656\nand/or communication devices utilized by communications service provider personnel 1658 for purposes of providing status information and/or for directing such personnel in the management of the power grid 1653 and/or communication system 1655.\nFIG. 17A illustrates a flow diagram of an example, non-limiting embodiment of a method 1700 for detecting and mitigating disturbances occurring in a communication network of the systems of FIGS. 16A & 16B.  Method 1700 can begin with step 1702\nwhere a waveguide system 1602 transmits and receives messages embedded in, or forming part of, modulated electromagnetic waves or another type of electromagnetic waves traveling along a surface of a power line 1610.  The messages can be voice messages,\nstreaming video, and/or other data/information exchanged between communication devices communicatively coupled to the communication system 1655.  At step 1704 the sensors 1604 of the waveguide system 1602 can collect sensing data.  In an embodiment, the\nsensing data can be collected in step 1704 prior to, during, or after the transmission and/or receipt of messages in step 1702.  At step 1706 the waveguide system 1602 (or the sensors 1604 themselves) can determine from the sensing data an actual or\npredicted occurrence of a disturbance in the communication system 1655 that can affect communications originating from (e.g., transmitted by) or received by the waveguide system 1602.  The waveguide system 1602 (or the sensors 1604) can process\ntemperature data, signal reflection data, loss of energy data, noise data, vibration data, environmental data, or any combination thereof to make this determination.  The waveguide system 1602 (or the sensors 1604) may also detect, identify, estimate, or\npredict the source of the disturbance and/or its location in the communication system 1655.  If a disturbance is neither detected/identified nor predicted/estimated at step 1708, the waveguide system 1602 can proceed to step 1702 where it continues to\ntransmit and receive messages embedded in, or forming part of, modulated electromagnetic waves traveling along a surface of the power line 1610.\nIf at step 1708 a disturbance is detected/identified or predicted/estimated to occur, the waveguide system 1602 proceeds to step 1710 to determine if the disturbance adversely affects (or alternatively, is likely to adversely affect or the\nextent to which it may adversely affect) transmission or reception of messages in the communication system 1655.  In one embodiment, a duration threshold and a frequency of occurrence threshold can be used at step 1710 to determine when a disturbance\nadversely affects communications in the communication system 1655.  For illustration purposes only, assume a duration threshold is set to 500 ms, while a frequency of occurrence threshold is set to 5 disturbances occurring in an observation period of 10\nsec. Thus, a disturbance having a duration greater than 500 ms will trigger the duration threshold.  Additionally, any disturbance occurring more than 5 times in a 10 sec time interval will trigger the frequency of occurrence threshold.\nIn one embodiment, a disturbance may be considered to adversely affect signal integrity in the communication systems 1655 when the duration threshold alone is exceeded.  In another embodiment, a disturbance may be considered as adversely\naffecting signal integrity in the communication systems 1655 when both the duration threshold and the frequency of occurrence threshold are exceeded.  The latter embodiment is thus more conservative than the former embodiment for classifying disturbances\nthat adversely affect signal integrity in the communication system 1655.  It will be appreciated that many other algorithms and associated parameters and thresholds can be utilized for step 1710 in accordance with example embodiments.\nReferring back to method 1700, if at step 1710 the disturbance detected at step 1708 does not meet the condition for adversely affected communications (e.g., neither exceeds the duration threshold nor the frequency of occurrence threshold), the\nwaveguide system 1602 may proceed to step 1702 and continue processing messages.  For instance, if the disturbance detected in step 1708 has a duration of 1 msec with a single occurrence in a 10 sec time period, then neither threshold will be exceeded. \nConsequently, such a disturbance may be considered as having a nominal effect on signal integrity in the communication system 1655 and thus would not be flagged as a disturbance requiring mitigation.  Although not flagged, the occurrence of the\ndisturbance, its time of occurrence, its frequency of occurrence, spectral data, and/or other useful information, may be reported to the network management system 1601 as telemetry data for monitoring purposes.\nReferring back to step 1710, if on the other hand the disturbance satisfies the condition for adversely affected communications (e.g., exceeds either or both thresholds), the waveguide system 1602 can proceed to step 1712 and report the incident\nto the network management system 1601.  The report can include raw sensing data collected by the sensors 1604, a description of the disturbance if known by the waveguide system 1602, a time of occurrence of the disturbance, a frequency of occurrence of\nthe disturbance, a location associated with the disturbance, parameters readings such as bit error rate, packet loss rate, retransmission requests, jitter, latency and so on.  If the disturbance is based on a prediction by one or more sensors of the\nwaveguide system 1602, the report can include a type of disturbance expected, and if predictable, an expected time occurrence of the disturbance, and an expected frequency of occurrence of the predicted disturbance when the prediction is based on\nhistorical sensing data collected by the sensors 1604 of the waveguide system 1602.\nAt step 1714, the network management system 1601 can determine a mitigation, circumvention, or correction technique, which may include directing the waveguide system 1602 to reroute traffic to circumvent the disturbance if the location of the\ndisturbance can be determined.  In one embodiment, the waveguide coupling device 1402 detecting the disturbance may direct a repeater such as the one shown in FIGS. 13-14 to connect the waveguide system 1602 from a primary power line affected by the\ndisturbance to a secondary power line to enable the waveguide system 1602 to reroute traffic to a different transmission medium and avoid the disturbance.  In an embodiment where the waveguide system 1602 is configured as a repeater the waveguide system\n1602 can itself perform the rerouting of traffic from the primary power line to the secondary power line.  It is further noted that for bidirectional communications (e.g., full or half-duplex communications), the repeater can be configured to reroute\ntraffic from the secondary power line back to the primary power line for processing by the waveguide system 1602.\nIn another embodiment, the waveguide system 1602 can redirect traffic by instructing a first repeater situated upstream of the disturbance and a second repeater situated downstream of the disturbance to redirect traffic from a primary power line\ntemporarily to a secondary power line and back to the primary power line in a manner that avoids the disturbance.  It is further noted that for bidirectional communications (e.g., full or half-duplex communications), repeaters can be configured to\nreroute traffic from the secondary power line back to the primary power line.\nTo avoid interrupting existing communication sessions occurring on a secondary power line, the network management system 1601 may direct the waveguide system 1602 to instruct repeater(s) to utilize unused time slot(s) and/or frequency band(s) of\nthe secondary power line for redirecting data and/or voice traffic away from the primary power line to circumvent the disturbance.\nAt step 1716, while traffic is being rerouted to avoid the disturbance, the network management system 1601 can notify equipment of the utility company 1652 and/or equipment of the communications service provider 1654, which in turn may notify\npersonnel of the utility company 1656 and/or personnel of the communications service provider 1658 of the detected disturbance and its location if known.  Field personnel from either party can attend to resolving the disturbance at a determined location\nof the disturbance.  Once the disturbance is removed or otherwise mitigated by personnel of the utility company and/or personnel of the communications service provider, such personnel can notify their respective companies and/or the network management\nsystem 1601 utilizing field equipment (e.g., a laptop computer, smartphone, etc.) communicatively coupled to network management system 1601, and/or equipment of the utility company and/or the communications service provider.  The notification can include\na description of how the disturbance was mitigated and any changes to the power lines 1610 that may change a topology of the communication system 1655.\nOnce the disturbance has been resolved (as determined in decision 1718), the network management system 1601 can direct the waveguide system 1602 at step 1720 to restore the previous routing configuration used by the waveguide system 1602 or\nroute traffic according to a new routing configuration if the restoration strategy used to mitigate the disturbance resulted in a new network topology of the communication system 1655.  In another embodiment, the waveguide system 1602 can be configured\nto monitor mitigation of the disturbance by transmitting test signals on the power line 1610 to determine when the disturbance has been removed.  Once the waveguide system 1602 detects an absence of the disturbance it can autonomously restore its routing\nconfiguration without assistance by the network management system 1601 if it determines the network topology of the communication system 1655 has not changed, or it can utilize a new routing configuration that adapts to a detected new network topology.\nFIG. 17B illustrates a flow diagram of an example, non-limiting embodiment of a method 1750 for detecting and mitigating disturbances occurring in a communication network of the system of FIGS. 16A and 16B.  In one embodiment, method 1750 can\nbegin with step 1752 where a network management system 1601 receives from equipment of the utility company 1652 or equipment of the communications service provider 1654 maintenance information associated with a maintenance schedule.  The network\nmanagement system 1601 can at step 1754 identify from the maintenance information, maintenance activities to be performed during the maintenance schedule.  From these activities, the network management system 1601 can detect a disturbance resulting from\nthe maintenance (e.g., scheduled replacement of a power line 1610, scheduled replacement of a waveguide system 1602 on the power line 1610, scheduled reconfiguration of power lines 1610 in the power grid 1653, etc.).\nIn another embodiment, the network management system 1601 can receive at step 1755 telemetry information from one or more waveguide systems 1602.  The telemetry information can include among other things an identity of each waveguide system 1602\nsubmitting the telemetry information, measurements taken by sensors 1604 of each waveguide system 1602, information relating to predicted, estimated, or actual disturbances detected by the sensors 1604 of each waveguide system 1602, location information\nassociated with each waveguide system 1602, an estimated location of a detected disturbance, an identification of the disturbance, and so on.  The network management system 1601 can determine from the telemetry information a type of disturbance that may\nbe adverse to operations of the waveguide, transmission of the electromagnetic waves along the wire surface, or both.  The network management system 1601 can also use telemetry information from multiple waveguide systems 1602 to isolate and identify the\ndisturbance.  Additionally, the network management system 1601 can request telemetry information from waveguide systems 1602 in a vicinity of an affected waveguide system 1602 to triangulate a location of the disturbance and/or validate an identification\nof the disturbance by receiving similar telemetry information from other waveguide systems 1602.\nIn yet another embodiment, the network management system 1601 can receive at step 1756 an unscheduled activity report from maintenance field personnel.  Unscheduled maintenance may occur as result of field calls that are unplanned or as a result\nof unexpected field issues discovered during field calls or scheduled maintenance activities.  The activity report can identify changes to a topology configuration of the power grid 1653 resulting from field personnel addressing discovered issues in the\ncommunication system 1655 and/or power grid 1653, changes to one or more waveguide systems 1602 (such as replacement or repair thereof), mitigation of disturbances performed if any, and so on.\nAt step 1758, the network management system 1601 can determine from reports received according to steps 1752 through 1756 if a disturbance will occur based on a maintenance schedule, or if a disturbance has occurred or is predicted to occur\nbased on telemetry data, or if a disturbance has occurred due to an unplanned maintenance identified in a field activity report.  From any of these reports, the network management system 1601 can determine whether a detected or predicted disturbance\nrequires rerouting of traffic by the affected waveguide systems 1602 or other waveguide systems 1602 of the communication system 1655.\nWhen a disturbance is detected or predicted at step 1758, the network management system 1601 can proceed to step 1760 where it can direct one or more waveguide systems 1602 to reroute traffic to circumvent the disturbance.  When the disturbance\nis permanent due to a permanent topology change of the power grid 1653, the network management system 1601 can proceed to step 1770 and skip steps 1762, 1764, 1766, and 1772.  At step 1770, the network management system 1601 can direct one or more\nwaveguide systems 1602 to use a new routing configuration that adapts to the new topology.  However, when the disturbance has been detected from telemetry information supplied by one or more waveguide systems 1602, the network management system 1601 can\nnotify maintenance personnel of the utility company 1656 or the communications service provider 1658 of a location of the disturbance, a type of disturbance if known, and related information that may be helpful to such personnel to mitigate the\ndisturbance.  When a disturbance is expected due to maintenance activities, the network management system 1601 can direct one or more waveguide systems 1602 to reconfigure traffic routes at a given schedule (consistent with the maintenance schedule) to\navoid disturbances caused by the maintenance activities during the maintenance schedule.\nReturning back to step 1760 and upon its completion, the process can continue with step 1762.  At step 1762, the network management system 1601 can monitor when the disturbance(s) have been mitigated by field personnel.  Mitigation of a\ndisturbance can be detected at step 1762 by analyzing field reports submitted to the network management system 1601 by field personnel over a communications network (e.g., cellular communication system) utilizing field equipment (e.g., a laptop computer\nor handheld computer/device).  If field personnel have reported that a disturbance has been mitigated, the network management system 1601 can proceed to step 1764 to determine from the field report whether a topology change was required to mitigate the\ndisturbance.  A topology change can include rerouting a power line 1610, reconfiguring a waveguide system 1602 to utilize a different power line 1610, otherwise utilizing an alternative link to bypass the disturbance and so on.  If a topology change has\ntaken place, the network management system 1601 can direct at step 1770 one or more waveguide systems 1602 to use a new routing configuration that adapts to the new topology.\nIf, however, a topology change has not been reported by field personnel, the network management system 1601 can proceed to step 1766 where it can direct one or more waveguide systems 1602 to send test signals to test a routing configuration that\nhad been used prior to the detected disturbance(s).  Test signals can be sent to affected waveguide systems 1602 in a vicinity of the disturbance.  The test signals can be used to determine if signal disturbances (e.g., electromagnetic wave reflections)\nare detected by any of the waveguide systems 1602.  If the test signals confirm that a prior routing configuration is no longer subject to previously detected disturbance(s), then the network management system 1601 can at step 1772 direct the affected\nwaveguide systems 1602 to restore a previous routing configuration.  If, however, test signals analyzed by one or more waveguide coupling device 1402 and reported to the network management system 1601 indicate that the disturbance(s) or new\ndisturbance(s) are present, then the network management system 1601 will proceed to step 1768 and report this information to field personnel to further address field issues.  The network management system 1601 can in this situation continue to monitor\nmitigation of the disturbance(s) at step 1762.\nIn the aforementioned embodiments, the waveguide systems 1602 can be configured to be self-adapting to changes in the power grid 1653 and/or to mitigation of disturbances.  That is, one or more affected waveguide systems 1602 can be configured\nto self-monitor mitigation of disturbances and reconfigure traffic routes without requiring instructions to be sent to them by the network management system 1601.  In this embodiment, the one or more waveguide systems 1602 that are self-configurable can\ninform the network management system 1601 of its routing choices so that the network management system 1601 can maintain a macro-level view of the communication topology of the communication system 1655.\nWhile for purposes of simplicity of explanation, the respective processes are shown and described as a series of blocks in FIGS. 17A and 17B, respectively, it is to be understood and appreciated that the claimed subject matter is not limited by\nthe order of the blocks, as some blocks may occur in different orders and/or concurrently with other blocks from what is depicted and described herein.  Moreover, not all illustrated blocks may be required to implement the methods described herein.\nTurning now to FIG. 18A, a block diagram illustrating an example, non-limiting embodiment of a communication system 1800 in accordance with various aspects of the subject disclosure is shown.  The communication system 1800 can include a macro\nbase station 1802 such as a base station or access point having antennas that covers one or more sectors (e.g., 6 or more sectors).  The macro base station 1802 can be communicatively coupled to a communication node 1804A that serves as a master or\ndistribution node for other communication nodes 1804B-E distributed at differing geographic locations inside or beyond a coverage area of the macro base station 1802.  The communication nodes 1804 operate as a distributed antenna system configured to\nhandle communications traffic associated with client devices such as mobile devices (e.g., cell phones) and/or fixed/stationary devices (e.g., a communication device in a residence, or commercial establishment) that are wirelessly coupled to any of the\ncommunication nodes 1804.  In particular, the wireless resources of the macro base station 1802 can be made available to mobile devices by allowing and/or redirecting certain mobile and/or stationary devices to utilize the wireless resources of a\ncommunication node 1804 in a communication range of the mobile or stationary devices.\nThe communication nodes 1804A-E can be communicatively coupled to each other over an interface 1810.  In one embodiment, the interface 1810 can comprise a wired or tethered interface (e.g., fiber optic cable).  In other embodiments, the\ninterface 1810 can comprise a wireless RF interface forming a radio distributed antenna system.  In various embodiments, the communication nodes 1804A-E can be configured to provide communication services to mobile and stationary devices according to\ninstructions provided by the macro base station 1802.  In other examples of operation however, the communication nodes 1804A-E operate merely as analog repeaters to spread the coverage of the macro base station 1802 throughout the entire range of the\nindividual communication nodes 1804A-E.\nThe micro base stations (depicted as communication nodes 1804) can differ from the macro base station in several ways.  For example, the communication range of the micro base stations can be smaller than the communication range of the macro base\nstation.  Consequently, the power consumed by the micro base stations can be less than the power consumed by the macro base station.  The macro base station optionally directs the micro base stations as to which mobile and/or stationary devices they are\nto communicate with, and which carrier frequency, spectral segment(s) and/or timeslot schedule of such spectral segment(s) are to be used by the micro base stations when communicating with certain mobile or stationary devices.  In these cases, control of\nthe micro base stations by the macro base station can be performed in a master-slave configuration or other suitable control configurations.  Whether operating independently or under the control of the macro base station 1802, the resources of the micro\nbase stations can be simpler and less costly than the resources utilized by the macro base station 1802.\nTurning now to FIG. 18B, a block diagram illustrating an example, non-limiting embodiment of the communication nodes 1804B-E of the communication system 1800 of FIG. 18A is shown.  In this illustration, the communication nodes 1804B-E are placed\non a utility fixture such as a light post.  In other embodiments, some of the communication nodes 1804B-E can be placed on a building or a utility post or pole that is used for distributing power and/or communication lines.  The communication nodes\n1804B-E in these illustrations can be configured to communicate with each other over the interface 1810, which in this illustration is shown as a wireless interface.  The communication nodes 1804B-E can also be configured to communicate with mobile or\nstationary devices 1806A-C over a wireless interface 1811 that conforms to one or more communication protocols (e.g., fourth generation (4G) wireless signals such as LTE signals or other 4G signals, fifth generation (5G) wireless signals, WiMAX, 802.11\nsignals, ultra-wideband signals, etc.).  The communication nodes 1804 can be configured to exchange signals over the interface 1810 at an operating frequency that may be higher (e.g., 28 GHz, 38 GHz, 60 GHz, 80 GHz or higher) than the operating frequency\nused for communicating with the mobile or stationary devices (e.g., 1.9 GHz) over interface 1811.  The high carrier frequency and a wider bandwidth can be used for communicating between the communication nodes 1804 enabling the communication nodes 1804\nto provide communication services to multiple mobile or stationary devices via one or more differing frequency bands, (e.g. a 900 MHz band, 1.9 GHz band, a 2.4 GHz band, and/or a 5.8 GHz band, etc.) and/or one or more differing protocols, as will be\nillustrated by spectral downlink and uplink diagrams of FIG. 19A described below.  In other embodiments, particularly where the interface 1810 is implemented via a guided wave communications system on a wire, a wideband spectrum in a lower frequency\nrange (e.g. in the range of 2-6 GHz, 4-10 GHz, etc.) can be employed.\nTurning now to FIGS. 18C-18D, block diagrams illustrating example, non-limiting embodiments of a communication node 1804 of the communication system 1800 of FIG. 18A is shown.  The communication node 1804 can be attached to a support structure\n1818 of a utility fixture such as a utility post or pole as shown in FIG. 18C.  The communication node 1804 can be affixed to the support structure 1818 with an arm 1820 constructed of plastic or other suitable material that attaches to an end of the\ncommunication node 1804.  The communication node 1804 can further include a plastic housing assembly 1816 that covers components of the communication node 1804.  The communication node 1804 can be powered by a power line 1821 (e.g., 110/220 VAC).  The\npower line 1821 can originate from a light pole or can be coupled to a power line of a utility pole.\nIn an embodiment where the communication nodes 1804 communicate wirelessly with other communication nodes 1804 as shown in FIG. 18B, a top side 1812 of the communication node 1804 (illustrated also in FIG. 18D) can comprise a plurality of\nantennas 1822 (e.g., 16 dielectric antennas devoid of metal surfaces) coupled to one or more transceivers such as, for example, in whole or in part, the transceiver 1400 illustrated in FIG. 14.  Each of the plurality of antennas 1822 of the top side 1812\ncan operate as a sector of the communication node 1804, each sector configured for communicating with at least one communication node 1804 in a communication range of the sector.  Alternatively, or in combination, the interface 1810 between communication\nnodes 1804 can be a tethered interface (e.g., a fiber optic cable, or a power line used for transport of guided electromagnetic waves as previously described).  In other embodiments, the interface 1810 can differ between communication nodes 1804.  That\nis, some communications nodes 1804 may communicate over a wireless interface, while others communicate over a tethered interface.  In yet other embodiments, some communications nodes 1804 may utilize a combined wireless and tethered interface.\nA bottom side 1814 of the communication node 1804 can also comprise a plurality of antennas 1824 for wirelessly communicating with one or more mobile or stationary devices 1806 at a carrier frequency that is suitable for the mobile or stationary\ndevices 1806.  As noted earlier, the carrier frequency used by the communication node 1804 for communicating with the mobile or station devices over the wireless interface 1811 shown in FIG. 18B can be different from the carrier frequency used for\ncommunicating between the communication nodes 1804 over interface 1810.  The plurality of antennas 1824 of the bottom portion 1814 of the communication node 1804 can also utilize a transceiver such as, for example, in whole or in part, the transceiver\n1400 illustrated in FIG. 14.\nTurning now to FIG. 19A, a block diagram illustrating an example, non-limiting embodiment of downlink and uplink communication techniques for enabling a base station to communicate with the communication nodes 1804 of FIG. 18A is shown.  In the\nillustrations of FIG. 19A, downlink signals (i.e., signals directed from the macro base station 1802 to the communication nodes 1804) can be spectrally divided into control channels 1902, downlink spectral segments 1906 each including modulated signals\nwhich can be frequency converted to their original/native frequency band for enabling the communication nodes 1804 to communicate with one or more mobile or stationary devices 1906, and pilot signals 1904 which can be supplied with some or all of the\nspectral segments 1906 for mitigating distortion created between the communication nodes 1904.  The pilot signals 1904 can be processed by the top side 1816 (tethered or wireless) transceivers of downstream communication nodes 1804 to remove distortion\nfrom a receive signal (e.g., phase distortion).  Each downlink spectral segment 1906 can be allotted a bandwidth 1905 sufficiently wide (e.g., 50 MHz) to include a corresponding pilot signal 1904 and one or more downlink modulated signals located in\nfrequency channels (or frequency slots) in the spectral segment 1906.  The modulated signals can represent cellular channels, WLAN channels or other modulated communication signals (e.g., 10-20 MHz), which can be used by the communication nodes 1804 for\ncommunicating with one or more mobile or stationary devices 1806.\nUplink modulated signals generated by mobile or stationary communication devices in their native/original frequency bands can be frequency converted and thereby located in frequency channels (or frequency slots) in the uplink spectral segment\n1910.  The uplink modulated signals can represent cellular channels, WLAN channels or other modulated communication signals.  Each uplink spectral segment 1910 can be allotted a similar or same bandwidth 1905 to include a pilot signal 1908 which can be\nprovided with some or each spectral segment 1910 to enable upstream communication nodes 1804 and/or the macro base station 1802 to remove distortion (e.g., phase error).\nIn the embodiment shown, the downlink and uplink spectral segments 1906 and 1910 each comprise a plurality of frequency channels (or frequency slots), which can be occupied with modulated signals that have been frequency converted from any\nnumber of native/original frequency bands (e.g. a 900 MHz band, 1.9 GHz band, a 2.4 GHz band, and/or a 5.8 GHz band, etc.).  The modulated signals can be up-converted to adjacent frequency channels in downlink and uplink spectral segments 1906 and 1910. \nIn this fashion, while some adjacent frequency channels in a downlink spectral segment 1906 can include modulated signals originally in a same native/original frequency band, other adjacent frequency channels in the downlink spectral segment 1906 can\nalso include modulated signals originally in different native/original frequency bands, but frequency converted to be located in adjacent frequency channels of the downlink spectral segment 1906.  For example, a first modulated signal in a 1.9 GHz band\nand a second modulated signal in the same frequency band (i.e., 1.9 GHz) can be frequency converted and thereby positioned in adjacent frequency channels of a downlink spectral segment 1906.  In another illustration, a first modulated signal in a 1.9 GHz\nband and a second communication signal in a different frequency band (i.e., 2.4 GHz) can be frequency converted and thereby positioned in adjacent frequency channels of a downlink spectral segment 1906.  Accordingly, frequency channels of a downlink\nspectral segment 1906 can be occupied with any combination of modulated signals of the same or differing signaling protocols and of a same or differing native/original frequency bands.\nSimilarly, while some adjacent frequency channels in an uplink spectral segment 1910 can include modulated signals originally in a same frequency band, adjacent frequency channels in the uplink spectral segment 1910 can also include modulated\nsignals originally in different native/original frequency bands, but frequency converted to be located in adjacent frequency channels of an uplink segment 1910.  For example, a first communication signal in a 2.4 GHz band and a second communication\nsignal in the same frequency band (i.e., 2.4 GHz) can be frequency converted and thereby positioned in adjacent frequency channels of an uplink spectral segment 1910.  In another illustration, a first communication signal in a 1.9 GHz band and a second\ncommunication signal in a different frequency band (i.e., 2.4 GHz) can be frequency converted and thereby positioned in adjacent frequency channels of the uplink spectral segment 1906.  Accordingly, frequency channels of an uplink spectral segment 1910\ncan be occupied with any combination of modulated signals of a same or differing signaling protocols and of a same or differing native/original frequency bands.  It should be noted that a downlink spectral segment 1906 and an uplink spectral segment 1910\ncan themselves be adjacent to one another and separated by only a guard band or otherwise separated by a larger frequency spacing, depending on the spectral allocation in place.\nTurning now to FIG. 19B, a block diagram 1920 illustrating an example, non-limiting embodiment of a communication node is shown.  In particular, the communication node device such as communication node 1804A of a radio distributed antenna system\nincludes a base station interface 1922, duplexer/diplexer assembly 1924, and two transceivers 1930 and 1932.  It should be noted however, that when the communication node 1804A is collocated with a base station, such as a macro base station 1802, the\nduplexer/diplexer assembly 1924 and the transceiver 1930 can be omitted and the transceiver 1932 can be directly coupled to the base station interface 1922.\nIn various embodiments, the base station interface 1922 receives a first modulated signal having one or more down link channels in a first spectral segment for transmission to a client device such as one or more mobile communication devices. \nThe first spectral segment represents an original/native frequency band of the first modulated signal.  The first modulated signal can include one or more downlink communication channels conforming to a signaling protocol such as a LTE or other 4G\nwireless protocol, a 5G wireless communication protocol, an ultra-wideband protocol, a WiMAX protocol, a 802.11 or other wireless local area network protocol and/or other communication protocol.  The duplexer/diplexer assembly 1924 transfers the first\nmodulated signal in the first spectral segment to the transceiver 1930 for direct communication with one or more mobile communication devices in range of the communication node 1804A as a free space wireless signal.  In various embodiments, the\ntransceiver 1930 is implemented via analog circuitry that merely provides: filtration to pass the spectrum of the downlink channels and the uplink channels of modulated signals in their original/native frequency bands while attenuating out-of-band\nsignals, power amplification, transmit/receive switching, duplexing, diplexing, and impedance matching to drive one or more antennas that sends and receives the wireless signals of interface 1810.\nIn other embodiments, the transceiver 1932 is configured to perform frequency conversion of the first modulated signal in the first spectral segment to the first modulated signal at a first carrier frequency based on, in various embodiments, an\nanalog signal processing of the first modulated signal without modifying the signaling protocol of the first modulated signal.  The first modulated signal at the first carrier frequency can occupy one or more frequency channels of a downlink spectral\nsegment 1906.  The first carrier frequency can be in a millimeter-wave or microwave frequency band.  As used herein analog signal processing includes filtering, switching, duplexing, diplexing, amplification, frequency up and down conversion, and other\nanalog processing that does not require digital signal processing, such as including without limitation either analog to digital conversion, digital to analog conversion, or digital frequency conversion.  In other embodiments, the transceiver 1932 can be\nconfigured to perform frequency conversion of the first modulated signal in the first spectral segment to the first carrier frequency by applying digital signal processing to the first modulated signal without utilizing any form of analog signal\nprocessing and without modifying the signaling protocol of the first modulated signal.  In yet other embodiments, the transceiver 1932 can be configured to perform frequency conversion of the first modulated signal in the first spectral segment to the\nfirst carrier frequency by applying a combination of digital signal processing and analog processing to the first modulated signal and without modifying the signaling protocol of the first modulated signal.\nThe transceiver 1932 can be further configured to transmit one or more control channels, one or more corresponding reference signals, such as pilot signals or other reference signals, and/or one or more clock signals together with the first\nmodulated signal at the first carrier frequency to a network element of the distributed antenna system, such as one or more downstream communication nodes 1904B-E, for wireless distribution of the first modulated signal to one or more other mobile\ncommunication devices once frequency converted by the network element to the first spectral segment.  In particular, the reference signal enables the network element to reduce a phase error (and/or other forms of signal distortion) during processing of\nthe first modulated signal from the first carrier frequency to the first spectral segment.  The control channel can include instructions to direct the communication node of the distributed antenna system to convert the first modulated signal at the first\ncarrier frequency to the first modulated signal in the first spectral segment, to control frequency selections and reuse patterns, handoff and/or other control signaling.  In embodiments where the instructions transmitted and received via the control\nchannel are digital signals, the transceiver can 1932 can include a digital signal processing component that provides analog to digital conversion, digital to analog conversion and that processes the digital data sent and/or received via the control\nchannel.  The clock signals supplied with the downlink spectral segment 1906 can be utilized to synchronize timing of digital control channel processing by the downstream communication nodes 1904B-E to recover the instructions from the control channel\nand/or to provide other timing signals.\nIn various embodiments, the transceiver 1932 can receive a second modulated signal at a second carrier frequency from a network element such as a communication node 1804B-E. The second modulated signal can include one or more uplink frequency\nchannels occupied by one or more modulated signals conforming to a signaling protocol such as a LTE or other 4G wireless protocol, a 5G wireless communication protocol, an ultra-wideband protocol, a 802.11 or other wireless local area network protocol\nand/or other communication protocol.  In particular, the mobile or stationary communication device generates the second modulated signal in a second spectral segment such as an original/native frequency band and the network element frequency converts the\nsecond modulated signal in the second spectral segment to the second modulated signal at the second carrier frequency and transmits the second modulated signal at the second carrier frequency as received by the communication node 1804A.  The transceiver\n1932 operates to convert the second modulated signal at the second carrier frequency to the second modulated signal in the second spectral segment and sends the second modulated signal in the second spectral segment, via the duplexer/diplexer assembly\n1924 and base station interface 1922, to a base station, such as macro base station 1802, for processing.\nConsider the following examples where the communication node 1804A is implemented in a distributed antenna system.  The uplink frequency channels in an uplink spectral segment 1910 and downlink frequency channels in a downlink spectral segment\n1906 can be occupied with signals modulated and otherwise formatted in accordance with a DOCSIS 2.0 or higher standard protocol, a WiMAX standard protocol, an ultra-wideband protocol, a 802.11 standard protocol, a 4G or 5G voice and data protocol such as\nan LTE protocol and/or other standard communication protocol.  In addition to protocols that conform with current standards, any of these protocols can be modified to operate in conjunction with the system of FIG. 18A.  For example, a 802.11 protocol or\nother protocol can be modified to include additional guidelines and/or a separate data channel to provide collision detection/multiple access over a wider area (e.g. allowing network elements or communication devices communicatively coupled to the\nnetwork elements that are communicating via a particular frequency channel of a downlink spectral segment 1906 or uplink spectral segment 1910 to hear one another).  In various embodiments all of the uplink frequency channels of the uplink spectral\nsegment 1910 and downlink frequency channel of the downlink spectral segment 1906 can all be formatted in accordance with the same communications protocol.  In the alternative however, two or more differing protocols can be employed on both the uplink\nspectral segment 1910 and the downlink spectral segment 1906 to, for example, be compatible with a wider range of client devices and/or operate in different frequency bands.\nWhen two or more differing protocols are employed, a first subset of the downlink frequency channels of the downlink spectral segment 1906 can be modulated in accordance with a first standard protocol and a second subset of the downlink\nfrequency channels of the downlink spectral segment 1906 can be modulated in accordance with a second standard protocol that differs from the first standard protocol.  Likewise a first subset of the uplink frequency channels of the uplink spectral\nsegment 1910 can be received by the system for demodulation in accordance with the first standard protocol and a second subset of the uplink frequency channels of the uplink spectral segment 1910 can be received in accordance with a second standard\nprotocol for demodulation in accordance with the second standard protocol that differs from the first standard protocol.\nIn accordance with these examples, the base station interface 1922 can be configured to receive modulated signals such as one or more downlink channels in their original/native frequency bands from a base station such as macro base station 1802\nor other communications network element.  Similarly, the base station interface 1922 can be configured to supply to a base station modulated signals received from another network element that is frequency converted to modulated signals having one or more\nuplink channels in their original/native frequency bands.  The base station interface 1922 can be implemented via a wired or wireless interface that bidirectionally communicates communication signals such as uplink and downlink channels in their\noriginal/native frequency bands, communication control signals and other network signaling with a macro base station or other network element.  The duplexer/diplexer assembly 1924 is configured to transfer the downlink channels in their original/native\nfrequency bands to the transceiver 1932 which frequency converts the frequency of the downlink channels from their original/native frequency bands into the frequency spectrum of interface 1810--in this case a wireless communication link used to transport\nthe communication signals downstream to one or more other communication nodes 1804B-E of the distributed antenna system in range of the communication device 1804A.\nIn various embodiments, the transceiver 1932 includes an analog radio that frequency converts the downlink channel signals in their original/native frequency bands via mixing or other heterodyne action to generate frequency converted downlink\nchannels signals that occupy downlink frequency channels of the downlink spectral segment 1906.  In this illustration, the downlink spectral segment 1906 is within the downlink frequency band of the interface 1810.  In an embodiment, the downlink channel\nsignals are up-converted from their original/native frequency bands to a 28 GHz, 38 GHz, 60 GHz, 70 GHz or 80 GHz band of the downlink spectral segment 1906 for line-of-sight wireless communications to one or more other communication nodes 1804B-E. It is\nnoted, however, that other frequency bands can likewise be employed for a downlink spectral segment 1906 (e.g., 3 GHz to 5 GHz).  For example, the transceiver 1932 can be configured for down-conversion of one or more downlink channel signals in their\noriginal/native spectral bands in instances where the frequency band of the interface 1810 falls below the original/native spectral bands of the one or more downlink channels signals.\nThe transceiver 1932 can be coupled to multiple individual antennas, such as antennas 1822 presented in conjunction with FIG. 18D, for communicating with the communication nodes 1804B, a phased antenna array or steerable beam or multi-beam\nantenna system for communicating with multiple devices at different locations.  The duplexer/diplexer assembly 1924 can include a duplexer, triplexer, splitter, switch, router and/or other assembly that operates as a \"channel duplexer\" to provide\nbi-directional communications over multiple communication paths via one or more original/native spectral segments of the uplink and downlink channels.\nIn addition to forwarding frequency converted modulated signals downstream to other communication nodes 1804B-E at a carrier frequency that differs from their original/native spectral bands, the communication node 1804A can also communicate all\nor a selected portion of the modulated signals unmodified from their original/native spectral bands to client devices in a wireless communication range of the communication node 1804A via the wireless interface 1811.  The duplexer/diplexer assembly 1924\ntransfers the modulated signals in their original/native spectral bands to the transceiver 1930.  The transceiver 1930 can include a channel selection filter for selecting one or more downlink channels and a power amplifier coupled to one or more\nantennas, such as antennas 1824 presented in conjunction with FIG. 18D, for transmission of the downlink channels via wireless interface 1811 to mobile or fixed wireless devices.\nIn addition to downlink communications destined for client devices, communication node 1804A can operate in a reciprocal fashion to handle uplink communications originating from client devices as well.  In operation, the transceiver 1932\nreceives uplink channels in the uplink spectral segment 1910 from communication nodes 1804B-E via the uplink spectrum of interface 1810.  The uplink frequency channels in the uplink spectral segment 1910 include modulated signals that were frequency\nconverted by communication nodes 1804B-E from their original/native spectral bands to the uplink frequency channels of the uplink spectral segment 1910.  In situations where the interface 1810 operates in a higher frequency band than the native/original\nspectral segments of the modulated signals supplied by the client devices, the transceiver 1932 down-converts the up-converted modulated signals to their original frequency bands.  In situations, however, where the interface 1810 operates in a lower\nfrequency band than the native/original spectral segments of the modulated signals supplied by the client devices, the transceiver 1932 up-converts the down-converted modulated signals to their original frequency bands.  Further, the transceiver 1930\noperates to receive all or selected ones of the modulated signals in their original/native frequency bands from client devices via the wireless interface 1811.  The duplexer/diplexer assembly 1924 transfers the modulated signals in their original/native\nfrequency bands received via the transceiver 1930 to the base station interface 1922 to be sent to the macro base station 1802 or other network element of a communications network.  Similarly, modulated signals occupying uplink frequency channels in an\nuplink spectral segment 1910 that are frequency converted to their original/native frequency bands by the transceiver 1932 are supplied to the duplexer/diplexer assembly 1924 for transfer to the base station interface 1922 to be sent to the macro base\nstation 1802 or other network element of a communications network.\nTurning now to FIG. 19C, a block diagram 1935 illustrating an example, non-limiting embodiment of a communication node is shown.  In particular, the communication node device such as communication node 1804B, 1804C, 1804D or 1804E of a radio\ndistributed antenna system includes transceiver 1933, duplexer/diplexer assembly 1924, an amplifier 1938 and two transceivers 1936A and 1936B.\nIn various embodiments, the transceiver 1936A receives, from a communication node 1804A or an upstream communication node 1804B-E, a first modulated signal at a first carrier frequency corresponding to the placement of the channels of the first\nmodulated signal in the converted spectrum of the distributed antenna system (e.g., frequency channels of one or more downlink spectral segments 1906).  The first modulated signal includes first communications data provided by a base station and directed\nto a mobile communication device.  The transceiver 1936A is further configured to receive, from a communication node 1804A one or more control channels and one or more corresponding reference signals, such as pilot signals or other reference signals,\nand/or one or more clock signals associated with the first modulated signal at the first carrier frequency.  The first modulated signal can include one or more downlink communication channels conforming to a signaling protocol such as a LTE or other 4G\nwireless protocol, a 5G wireless communication protocol, an ultra-wideband protocol, a WiMAX protocol, a 802.11 or other wireless local area network protocol and/or other communication protocol.\nAs previously discussed, the reference signal enables the network element to reduce a phase error (and/or other forms of signal distortion) during processing of the first modulated signal from the first carrier frequency to the first spectral\nsegment (i.e., original/native spectrum).  The control channel includes instructions to direct the communication node of the distributed antenna system to convert the first modulated signal at the first carrier frequency to the first modulated signal in\nthe first spectral segment, to control frequency selections and reuse patterns, handoff and/or other control signaling.  The clock signals can synchronize timing of digital control channel processing by the downstream communication nodes 1804B-E to\nrecover the instructions from the control channel and/or to provide other timing signals.\nThe amplifier 1938 can be a bidirectional amplifier that amplifies the first modulated signal at the first carrier frequency together with the reference signals, control channels and/or clock signals for coupling via the duplexer/diplexer\nassembly 1924 to transceiver 1936B, which in this illustration, serves as a repeater for retransmission of the amplified the first modulated signal at the first carrier frequency together with the reference signals, control channels and/or clock signals\nto one or more others of the communication nodes 1804B-E that are downstream from the communication node 1804B-E that is shown and that operate in a similar fashion.\nThe amplified first modulated signal at the first carrier frequency together with the reference signals, control channels and/or clock signals are also coupled via the duplexer/diplexer assembly 1924 to the transceiver 1933.  The transceiver\n1933 performs digital signal processing on the control channel to recover the instructions, such as in the form of digital data, from the control channel.  The clock signal is used to synchronize timing of the digital control channel processing.  The\ntransceiver 1933 then performs frequency conversion of the first modulated signal at the first carrier frequency to the first modulated signal in the first spectral segment in accordance with the instructions and based on an analog (and/or digital)\nsignal processing of the first modulated signal and utilizing the reference signal to reduce distortion during the converting process.  The transceiver 1933 wirelessly transmits the first modulated signal in the first spectral segment for direct\ncommunication with one or more mobile communication devices in range of the communication node 1804B-E as free space wireless signals.\nIn various embodiments, the transceiver 1936B receives a second modulated signal at a second carrier frequency in an uplink spectral segment 1910 from other network elements such as one or more other communication nodes 1804B-E that are\ndownstream from the communication node 1804B-E that is shown.  The second modulated signal can include one or more uplink communication channels conforming to a signaling protocol such as a LTE or other 4G wireless protocol, a 5G wireless communication\nprotocol, an ultra-wideband protocol, a 802.11 or other wireless local area network protocol and/or other communication protocol.  In particular, one or more mobile communication devices generate the second modulated signal in a second spectral segment\nsuch as an original/native frequency band and the downstream network element performs frequency conversion on the second modulated signal in the second spectral segment to the second modulated signal at the second carrier frequency and transmits the\nsecond modulated signal at the second carrier frequency in an uplink spectral segment 1910 as received by the communication node 1804B-E shown.  The transceiver 1936B operates to send the second modulated signal at the second carrier frequency to\namplifier 1938, via the duplexer/diplexer assembly 1924, for amplification and retransmission via the transceiver 1936A back to the communication node 1804A or upstream communication nodes 1804B-E for further retransmission back to a base station, such\nas macro base station 1802, for processing.\nThe transceiver 1933 may also receive a second modulated signal in the second spectral segment from one or more mobile communication devices in range of the communication node 1804B-E. The transceiver 1933 operates to perform frequency\nconversion on the second modulated signal in the second spectral segment to the second modulated signal at the second carrier frequency, for example, under control of the instructions received via the control channel, inserts the reference signals,\ncontrol channels and/or clock signals for use by communication node 1804A in reconverting the second modulated signal back to the original/native spectral segments and sends the second modulated signal at the second carrier frequency, via the\nduplexer/diplexer assembly 1924 and amplifier 1938, to the transceiver 1936A for amplification and retransmission back to the communication node 1804A or upstream communication nodes 1804B-E for further retransmission back to a base station, such as\nmacro base station 1802, for processing.\nTurning now to FIG. 19D, a graphical diagram 1940 illustrating an example, non-limiting embodiment of a frequency spectrum is shown.  In particular, a spectrum 1942 is shown for a distributed antenna system that conveys modulated signals that\noccupy frequency channels of a downlink segment 1906 or uplink spectral segment 1910 after they have been converted in frequency (e.g. via up-conversion or down-conversion) from one or more original/native spectral segments into the spectrum 1942.\nIn the example presented, the downstream (downlink) channel band 1944 includes a plurality of downstream frequency channels represented by separate downlink spectral segments 1906.  Likewise the upstream (uplink) channel band 1946 includes a\nplurality of upstream frequency channels represented by separate uplink spectral segments 1910.  The spectral shapes of the separate spectral segments are meant to be placeholders for the frequency allocation of each modulated signal along with\nassociated reference signals, control channels and clock signals.  The actual spectral response of each frequency channel in a downlink spectral segment 1906 or uplink spectral segment 1910 will vary based on the protocol and modulation employed and\nfurther as a function of time.\nThe number of the uplink spectral segments 1910 can be less than or greater than the number of the downlink spectral segments 1906 in accordance with an asymmetrical communication system.  In this case, the upstream channel band 1946 can be\nnarrower or wider than the downstream channel band 1944.  In the alternative, the number of the uplink spectral segments 1910 can be equal to the number of the downlink spectral segments 1906 in the case where a symmetrical communication system is\nimplemented.  In this case, the width of the upstream channel band 1946 can be equal to the width of the downstream channel band 1944 and bit stuffing or other data filling techniques can be employed to compensate for variations in upstream traffic. \nWhile the downstream channel band 1944 is shown at a lower frequency than the upstream channel band 1946, in other embodiments, the downstream channel band 1844 can be at a higher frequency than the upstream channel band 1946.  In addition, the number of\nspectral segments and their respective frequency positions in spectrum 1942 can change dynamically over time.  For example, a general control channel can be provided in the spectrum 1942 (not shown) which can indicate to communication nodes 1804 the\nfrequency position of each downlink spectral segment 1906 and each uplink spectral segment 1910.  Depending on traffic conditions, or network requirements necessitating a reallocation of bandwidth, the number of downlink spectral segments 1906 and uplink\nspectral segments 1910 can be changed by way of the general control channel.  Additionally, the downlink spectral segments 1906 and uplink spectral segments 1910 do not have to be grouped separately.  For instance, a general control channel can identify\na downlink spectral segment 1906 being followed by an uplink spectral segment 1910 in an alternating fashion, or in any other combination which may or may not be symmetric.  It is further noted that instead of utilizing a general control channel,\nmultiple control channels can be used, each identifying the frequency position of one or more spectral segments and the type of spectral segment (i.e., uplink or downlink).\nFurther, while the downstream channel band 1944 and upstream channel band 1946 are shown as occupying a single contiguous frequency band, in other embodiments, two or more upstream and/or two or more downstream channel bands can be employed,\ndepending on available spectrum and/or the communication standards employed.  Frequency channels of the uplink spectral segments 1910 and downlink spectral segments 1906 can be occupied by frequency converted signals modulated formatted in accordance\nwith a DOCSIS 2.0 or higher standard protocol, a WiMAX standard protocol, an ultra-wideband protocol, a 802.11 standard protocol, a 4G or 5G voice and data protocol such as an LTE protocol and/or other standard communication protocol.  In addition to\nprotocols that conform with current standards, any of these protocols can be modified to operate in conjunction with the system shown.  For example, a 802.11 protocol or other protocol can be modified to include additional guidelines and/or a separate\ndata channel to provide collision detection/multiple access over a wider area (e.g. allowing devices that are communicating via a particular frequency channel to hear one another).  In various embodiments all of the uplink frequency channels of the\nuplink spectral segments 1910 and downlink frequency channel of the downlink spectral segments 1906 are all formatted in accordance with the same communications protocol.  In the alternative however, two or more differing protocols can be employed on\nboth the uplink frequency channels of one or more uplink spectral segments 1910 and downlink frequency channels of one or more downlink spectral segments 1906 to, for example, be compatible with a wider range of client devices and/or operate in different\nfrequency bands.\nIt should be noted that, the modulated signals can be gathered from differing original/native spectral segments for aggregation into the spectrum 1942.  In this fashion, a first portion of uplink frequency channels of an uplink spectral segment\n1910 may be adjacent to a second portion of uplink frequency channels of the uplink spectral segment 1910 that have been frequency converted from one or more differing original/native spectral segments.  Similarly, a first portion of downlink frequency\nchannels of a downlink spectral segment 1906 may be adjacent to a second portion of downlink frequency channels of the downlink spectral segment 1906 that have been frequency converted from one or more differing original/native spectral segments.  For\nexample, one or more 2.4 GHz 802.11 channels that have been frequency converted may be adjacent to one or more 5.8 GHz 802.11 channels that have also been frequency converted to a spectrum 1942 that is centered at 80 GHz.  It should be noted that each\nspectral segment can have an associated reference signal such as a pilot signal that can be used in generating a local oscillator signal at a frequency and phase that provides the frequency conversion of one or more frequency channels of that spectral\nsegment from its placement in the spectrum 1942 back into it original/native spectral segment.\nTurning now to FIG. 19E, a graphical diagram 1950 illustrating an example, non-limiting embodiment of a frequency spectrum is shown.  In particular a spectral segment selection is presented as discussed in conjunction with signal processing\nperformed on the selected spectral segment by transceivers 1930 of communication node 1840A or transceiver 1932 of communication node 1804B-E. As shown, a particular uplink frequency portion 1958 including one of the uplink spectral segments 1910 of\nuplink frequency channel band 1946 and a particular downlink frequency portion 1956 including one of the downlink spectral segments 1906 of downlink channel frequency band 1944 is selected to be passed by channel selection filtration, with the remaining\nportions of uplink frequency channel band 1946 and downlink channel frequency band 1944 being filtered out--i.e. attenuated so as to mitigate adverse effects of the processing of the desired frequency channels that are passed by the transceiver.  It\nshould be noted that while a single particular uplink spectral segment 1910 and a particular downlink spectral segment 1906 are shown as being selected, two or more uplink and/or downlink spectral segments may be passed in other embodiments.\nWhile the transceivers 1930 and 1932 can operate based on static channel filters with the uplink and downlink frequency portions 1958 and 1956 being fixed, as previously discussed, instructions sent to the transceivers 1930 and 1932 via the\ncontrol channel can be used to dynamically configure the transceivers 1930 and 1932 to a particular frequency selection.  In this fashion, upstream and downstream frequency channels of corresponding spectral segments can be dynamically allocated to\nvarious communication nodes by the macro base station 1802 or other network element of a communication network to optimize performance by the distributed antenna system.\nTurning now to FIG. 19F, a graphical diagram 1960 illustrating an example, non-limiting embodiment of a frequency spectrum is shown.  In particular, a spectrum 1962 is shown for a distributed antenna system that conveys modulated signals\noccupying frequency channels of uplink or downlink spectral segments after they have been converted in frequency (e.g. via up-conversion or down-conversion) from one or more original/native spectral segments into the spectrum 1962.\nAs previously discussed two or more different communication protocols can be employed to communicate upstream and downstream data.  When two or more differing protocols are employed, a first subset of the downlink frequency channels of a\ndownlink spectral segment 1906 can be occupied by frequency converted modulated signals in accordance with a first standard protocol and a second subset of the downlink frequency channels of the same or a different downlink spectral segment 1910 can be\noccupied by frequency converted modulated signals in accordance with a second standard protocol that differs from the first standard protocol.  Likewise a first subset of the uplink frequency channels of an uplink spectral segment 1910 can be received by\nthe system for demodulation in accordance with the first standard protocol and a second subset of the uplink frequency channels of the same or a different uplink spectral segment 1910 can be received in accordance with a second standard protocol for\ndemodulation in accordance with the second standard protocol that differs from the first standard protocol.\nIn the example shown, the downstream channel band 1944 includes a first plurality of downstream spectral segments represented by separate spectral shapes of a first type representing the use of a first communication protocol.  The downstream\nchannel band 1944' includes a second plurality of downstream spectral segments represented by separate spectral shapes of a second type representing the use of a second communication protocol.  Likewise the upstream channel band 1946 includes a first\nplurality of upstream spectral segments represented by separate spectral shapes of the first type representing the use of the first communication protocol.  The upstream channel band 1946' includes a second plurality of upstream spectral segments\nrepresented by separate spectral shapes of the second type representing the use of the second communication protocol.  These separate spectral shapes are meant to be placeholders for the frequency allocation of each individual spectral segment along with\nassociated reference signals, control channels and/or clock signals.  While the individual channel bandwidth is shown as being roughly the same for channels of the first and second type, it should be noted that upstream and downstream channel bands 1944,\n1944', 1946 and 1946' may be of differing bandwidths.  Additionally, the spectral segments in these channel bands of the first and second type may be of differing bandwidths, depending on available spectrum and/or the communication standards employed.\nTurning now to FIG. 19G, a graphical diagram 1970 illustrating an example, non-limiting embodiment of a frequency spectrum is shown.  In particular a portion of the spectrum 1942 or 1962 of FIGS. 19D-19F is shown for a distributed antenna system\nthat conveys modulated signals in the form of channel signals that have been converted in frequency (e.g. via up-conversion or down-conversion) from one or more original/native spectral segments.\nThe portion 1972 includes a portion of a downlink or uplink spectral segment 1906 and 1910 that is represented by a spectral shape and that represents a portion of the bandwidth set aside for a control channel, reference signal, and/or clock\nsignal.  The spectral shape 1974, for example, represents a control channel that is separate from reference signal 1979 and a clock signal 1978.  It should be noted that the clock signal 1978 is shown with a spectral shape representing a sinusoidal\nsignal that may require conditioning into the form of a more traditional clock signal.  In other embodiments however, a traditional clock signal could be sent as a modulated carrier wave such by modulating the reference signal 1979 via amplitude\nmodulation or other modulation technique that preserves the phase of the carrier for use as a phase reference.  In other embodiments, the clock signal could be transmitted by modulating another carrier wave or as another signal.  Further, it is noted\nthat both the clock signal 1978 and the reference signal 1979 are shown as being outside the frequency band of the control channel 1974.\nIn another example, the portion 1975 includes a portion of a downlink or uplink spectral segment 1906 and 1910 that is represented by a portion of a spectral shape that represents a portion of the bandwidth set aside for a control channel,\nreference signal, and/or clock signal.  The spectral shape 1976 represents a control channel having instructions that include digital data that modulates the reference signal, via amplitude modulation, amplitude shift keying or other modulation technique\nthat preserves the phase of the carrier for use as a phase reference.  The clock signal 1978 is shown as being outside the frequency band of the spectral shape 1976.  The reference signal, being modulated by the control channel instructions, is in effect\na subcarrier of the control channel and is in-band to the control channel.  Again, the clock signal 1978 is shown with a spectral shape representing a sinusoidal signal, in other embodiments however, a traditional clock signal could be sent as a\nmodulated carrier wave or other signal.  In this case, the instructions of the control channel can be used to modulate the clock signal 1978 instead of the reference signal.\nConsider the following example, where the control channel 1976 is carried via modulation of a reference signal in the form of a continuous wave (CW) from which the phase distortion in the receiver is corrected during frequency conversion of the\ndownlink or uplink spectral segment 1906 and 1910 back to its original/native spectral segment.  The control channel 1976 can be modulated with a robust modulation such as pulse amplitude modulation, binary phase shift keying, amplitude shift keying or\nother modulation scheme to carry instructions between network elements of the distributed antenna system such as network operations, administration and management traffic and other control data.  In various embodiments, the control data can include\nwithout limitation: Status information that indicates online status, offline status, and network performance parameters of each network element.  Network device information such as module names and addresses, hardware and software versions, device\ncapabilities, etc. Spectral information such as frequency conversion factors, channel spacing, guard bands, uplink/downlink allocations, uplink and downlink channel selections, etc. Environmental measurements such as weather conditions, image data, power\noutage information, line of sight blockages, etc.\nIn a further example, the control channel data can be sent via ultra-wideband (UWB) signaling.  The control channel data can be transmitted by generating radio energy at specific time intervals and occupying a larger bandwidth, via\npulse-position or time modulation, by encoding the polarity or amplitude of the UWB pulses and/or by using orthogonal pulses.  In particular, UWB pulses can be sent sporadically at relatively low pulse rates to support time or position modulation, but\ncan also be sent at rates up to the inverse of the UWB pulse bandwidth.  In this fashion, the control channel can be spread over an UWB spectrum with relatively low power, and without interfering with CW transmissions of the reference signal and/or clock\nsignal that may occupy in-band portions of the UWB spectrum of the control channel.\nTurning now to FIG. 19H, a block diagram 1980 illustrating an example, non-limiting embodiment of a transmitter is shown.  In particular, a transmitter 1982 is shown for use with, for example, a receiver 1981 and a digital control channel\nprocessor 1995 in a transceiver, such as transceiver 1933 presented in conjunction with FIG. 19C.  As shown, the transmitter 1982 includes an analog front-end 1986, clock signal generator 1989, a local oscillator 1992, a mixer 1996, and a transmitter\nfront end 1984.\nThe amplified first modulated signal at the first carrier frequency together with the reference signals, control channels and/or clock signals are coupled from the amplifier 1938 to the analog front-end 1986.  The analog front end 1986 includes\none or more filters or other frequency selection to separate the control channel signal 1987, a clock reference signal 1978, a pilot signal 1991 and one or more selected channels signals 1994.\nThe digital control channel processor 1995 performs digital signal processing on the control channel to recover the instructions, such as via demodulation of digital control channel data, from the control channel signal 1987.  The clock signal\ngenerator 1989 generates the clock signal 1990, from the clock reference signal 1978, to synchronize timing of the digital control channel processing by the digital control channel processor 1995.  In embodiments where the clock reference signal 1978 is\na sinusoid, the clock signal generator 1989 can provide amplification and limiting to create a traditional clock signal or other timing signal from the sinusoid.  In embodiments where the clock reference signal 1978 is a modulated carrier signal, such as\na modulation of the reference or pilot signal or other carrier wave, the clock signal generator 1989 can provide demodulation to create a traditional clock signal or other timing signal.\nIn various embodiments, the control channel signal 1987 can be either a digitally modulated signal in a range of frequencies separate from the pilot signal 1991 and the clock reference 1988 or as modulation of the pilot signal 1991.  In\noperation, the digital control channel processor 1995 provides demodulation of the control channel signal 1987 to extract the instructions contained therein in order to generate a control signal 1993.  In particular, the control signal 1993 generated by\nthe digital control channel processor 1995 in response to instructions received via the control channel can be used to select the particular channel signals 1994 along with the corresponding pilot signal 1991 and/or clock reference 1988 to be used for\nconverting the frequencies of channel signals 1994 for transmission via wireless interface 1811.  It should be noted that in circumstances where the control channel signal 1987 conveys the instructions via modulation of the pilot signal 1991, the pilot\nsignal 1991 can be extracted via the digital control channel processor 1995 rather than the analog front-end 1986 as shown.\nThe digital control channel processor 1995 may be implemented via a processing module such as a microprocessor, micro-controller, digital signal processor, microcomputer, central processing unit, field programmable gate array, programmable logic\ndevice, state machine, logic circuitry, digital circuitry, an analog to digital converter, a digital to analog converter and/or any device that manipulates signals (analog and/or digital) based on hard coding of the circuitry and/or operational\ninstructions.  The processing module may be, or further include, memory and/or an integrated memory element, which may be a single memory device, a plurality of memory devices, and/or embedded circuitry of another processing module, module, processing\ncircuit, and/or processing unit.  Such a memory device may be a read-only memory, random access memory, volatile memory, non-volatile memory, static memory, dynamic memory, flash memory, cache memory, and/or any device that stores digital information. \nNote that if the processing module includes more than one processing device, the processing devices may be centrally located (e.g., directly coupled together via a wired and/or wireless bus structure) or may be distributedly located (e.g., cloud\ncomputing via indirect coupling via a local area network and/or a wide area network).  Further note that the memory and/or memory element storing the corresponding operational instructions may be embedded within, or external to, the microprocessor,\nmicro-controller, digital signal processor, microcomputer, central processing unit, field programmable gate array, programmable logic device, state machine, logic circuitry, digital circuitry, an analog to digital converter, a digital to analog converter\nor other device.  Still further note that, the memory element may store, and the processing module executes, hard coded and/or operational instructions corresponding to at least some of the steps and/or functions described herein and such a memory device\nor memory element can be implemented as an article of manufacture.\nThe local oscillator 1992 generates the local oscillator signal 1997 utilizing the pilot signal 1991 to reduce distortion during the frequency conversion process.  In various embodiments the pilot signal 1991 is at the correct frequency and\nphase of the local oscillator signal 1997 to generate the local oscillator signal 1997 at the proper frequency and phase to convert the channel signals 1994 at the carrier frequency associated with their placement in the spectrum of the distributed\nantenna system to their original/native spectral segments for transmission to fixed or mobile communication devices.  In this case, the local oscillator 1992 can employ bandpass filtration and/or other signal conditioning to generate a sinusoidal local\noscillator signal 1997 that preserves the frequency and phase of the pilot signal 1991.  In other embodiments, the pilot signal 1991 has a frequency and phase that can be used to derive the local oscillator signal 1997.  In this case, the local\noscillator 1992 employs frequency division, frequency multiplication or other frequency synthesis, based on the pilot signal 1991, to generate the local oscillator signal 1997 at the proper frequency and phase to convert the channel signals 1994 at the\ncarrier frequency associated with their placement in the spectrum of the distributed antenna system to their original/native spectral segments for transmission to fixed or mobile communication devices.\nThe mixer 1996 operates based on the local oscillator signal 1997 to shift the channel signals 1994 in frequency to generate frequency converted channel signals 1998 at their corresponding original/native spectral segments.  While a single\nmixing stage is shown, multiple mixing stages can be employed to shift the channel signals to baseband and/or one or more intermediate frequencies as part of the total frequency conversion.  The transmitter (Xmtr) front-end 1984 includes a power\namplifier and impedance matching to wirelessly transmit the frequency converted channel signals 1998 as a free space wireless signals via one or more antennas, such as antennas 1824, to one or more mobile or fixed communication devices in range of the\ncommunication node 1804B-E.\nTurning now to FIG. 19I, a block diagram 1985 illustrating an example, non-limiting embodiment of a receiver is shown.  In particular, a receiver 1981 is shown for use with, for example, transmitter 1982 and digital control channel processor\n1995 in a transceiver, such as transceiver 1933 presented in conjunction with FIG. 19C.  As shown, the receiver 1981 includes an analog receiver (RCVR) front-end 1983, local oscillator 1992, and mixer 1996.  The digital control channel processor 1995\noperates under control of instructions from the control channel to generate the pilot signal 1991, control channel signal 1987 and clock reference signal 1978.\nThe control signal 1993 generated by the digital control channel processor 1995 in response to instructions received via the control channel can also be used to select the particular channel signals 1994 along with the corresponding pilot signal\n1991 and/or clock reference 1988 to be used for converting the frequencies of channel signals 1994 for reception via wireless interface 1811.  The analog receiver front end 1983 includes a low noise amplifier and one or more filters or other frequency\nselection to receive one or more selected channels signals 1994 under control of the control signal 1993.\nThe local oscillator 1992 generates the local oscillator signal 1997 utilizing the pilot signal 1991 to reduce distortion during the frequency conversion process.  In various embodiments the local oscillator employs bandpass filtration and/or\nother signal conditioning, frequency division, frequency multiplication or other frequency synthesis, based on the pilot signal 1991, to generate the local oscillator signal 1997 at the proper frequency and phase to frequency convert the channel signals\n1994, the pilot signal 1991, control channel signal 1987 and clock reference signal 1978 to the spectrum of the distributed antenna system for transmission to other communication nodes 1804A-E. In particular, the mixer 1996 operates based on the local\noscillator signal 1997 to shift the channel signals 1994 in frequency to generate frequency converted channel signals 1998 at the desired placement within spectrum spectral segment of the distributed antenna system for coupling to the amplifier 1938, to\ntransceiver 1936A for amplification and retransmission via the transceiver 1936A back to the communication node 1804A or upstream communication nodes 1804B-E for further retransmission back to a base station, such as macro base station 1802, for\nprocessing.  Again, while a single mixing stage is shown, multiple mixing stages can be employed to shift the channel signals to baseband and/or one or more intermediate frequencies as part of the total frequency conversion.\nTurning now to FIG. 20A, a block diagram is shown illustrating an example, non-limiting embodiment of a dielectric antenna in accordance with various aspects described herein.  In particular, a dielectric horn antenna 2001 is shown having a\nconical structure.  The dielectric horn antenna 2001 is coupled to a feed point 2002, which can also be comprised of a dielectric material.  The dielectric horn antenna 2001 and the feed point 2002 (as well as other embodiments of the dielectric antenna\ndescribed below in the subject disclosure) can be constructed as a geometric solid of dielectric materials such as a polyethylene material, a polyurethane material, Teflon or other suitable dielectric material (e.g., a synthetic resin, other plastics,\netc.).  In various embodiments, the dielectric material can be opaque, i.e., resistant to propagation of electromagnetic waves having an optical frequency range.  The dielectric horn antenna 2001 and the feed point 2002 (as well as other embodiments of\nthe dielectric antenna described below in the subject disclosure) can be adapted to be substantially or entirely devoid of any conductive materials.\nFor example, the external surfaces 2007 of the dielectric horn antenna 2001 and the feed point 2002 can be non-conductive or substantially non-conductive with at least 95% of the external surface area being non-conductive and the dielectric\nmaterials used to construct the dielectric horn antenna 2001 and the feed point 2002 can be such that they substantially do not contain impurities that may be conductive (e.g., such as less than 1 part per thousand) or result in imparting conductive\nproperties.  In other embodiments, however, a limited number of conductive components can be used such as a metallic connector component used at the feed point 2002, one or more screws, rivets or other coupling elements used to bind components to one\nanother, and/or one or more structural elements that do not significantly alter the radiation pattern of the dielectric antenna.  The feed point 2002 can be adapted to couple to a core 2052 such as a dielectric core of a conductorless cable described in\nconjunction with the co-pending U.S.  application Ser.  No. 14/799,272 filed Jul.  14, 2015 by Henry et al., entitled \"Apparatus and Methods for Transmitting Wireless Signals\".  In one embodiment, the feed point 2002 can be coupled to the core 2052\nutilizing a joint (not shown in FIG. 20A) such as a splicing device or other coupling mechanism or via an adhesive.  In an embodiment, the joint can be configured to cause the feed point 2002 to touch an endpoint of the core 2052.  In another embodiment,\nthe joint can create a gap between an end of the feed point 2002 and an end of the core 2052.  In yet another embodiment, the joint can cause the feed point 2002 and the core 2052 to be coaxially aligned or partially misaligned.  Notwithstanding any\ncombination of the foregoing embodiments, electromagnetic waves can in whole or at least in part propagate between the junction of the feed point 2002 and the core 2052.\nThe cable 2050 can be coupled to a communication system, such as a waveguide system, any of the communication nodes 1804A-E previously described, or other transceiver.  For illustration purposes only, the communication node 1804A can include a\nlauncher, such as any of the cable launchers described in the co-pending U.S.  application Ser.  No. 14/799,272 filed Jul.  14, 2015 by Henry et al., entitled \"Apparatus and Methods for Transmitting Wireless Signals\".  In various embodiments, the\nlauncher can be configured to select a wave mode (e.g., non-fundamental wave mode, fundamental wave mode, a hybrid wave mode, or combinations thereof as described earlier) and transmit instances of electromagnetic waves having a millimeter wave or other\nmicrowave operating frequency (e.g., 60 GHz).  The electromagnetic waves can be directed to an interface of the cable 2050.\nThe instances of electromagnetic waves generated by the communication node 1804A can induce a combined electromagnetic wave such as a hybrid mode or combination of modes having the selected wave mode that propagates from the core 2052 to the\nfeed point 2002.  The combined electromagnetic wave can propagate partly inside the core 2052 and partly on an outer surface of the core 2052.  Once the combined electromagnetic wave has propagated through the junction between the core 2052 and the feed\npoint 2002, the combined electromagnetic wave can continue to propagate partly inside the feed point 2002 and partly on an outer surface of the feed point 2002.  In some embodiments, the portion of the combined electromagnetic wave that propagates on the\nouter surface of the core 2052 and the feed point 2002 is small.  In these embodiments, the combined electromagnetic wave can be said to be guided by and tightly coupled to the core 2052 and the feed point 2002 while propagating longitudinally towards\nthe dielectric antenna 2001.\nWhen the combined electromagnetic wave reaches a proximal portion of the dielectric antenna 2001 (at a junction 2002' between the feed point 2002 and the dielectric antenna 2001), the combined electromagnetic wave enters the proximal portion of\nthe dielectric antenna 2001 and propagates longitudinally along an axis of the dielectric antenna 2001 (shown as a dashed line).  By the time the combined electromagnetic wave reaches the operating face 2003, the combined electromagnetic wave has an\nintensity pattern where the electric fields of the combined electromagnetic wave are strongest near a center region of the operating face 2003 and weaker in the outer regions.  In an embodiment, where the wave mode of the electromagnetic waves\npropagating in the dielectric antenna 2001 is a hybrid wave mode (e.g., HE11), the leakage of the electromagnetic waves at the external surfaces 2007 is reduced or in some instances eliminated.  In particular, while the dielectric antenna 2002 is\nconstructed of solid dielectric and may have no physical aperture, the operating face 2003 functions as an aperture of a traditional horn antenna to radiate and receive free space wireless signals such as microwave signals or other RF signaling.\nIn an embodiment, the far-field antenna gain pattern of the dielectric antenna 2001 can be widened by decreasing the operating frequency of the combined electromagnetic wave from a nominal frequency.  Similarly, the gain pattern can be narrowed\nby increasing the operating frequency of the combined electromagnetic wave from the nominal frequency.  Accordingly, a width of a beam of wireless signals emitted by the operating face 2003 can be controlled by configuring the communication node,\nwaveguide system or other transceiver to increase or decrease the operating frequency of the combined electromagnetic wave.\nThe dielectric antenna 2001 of FIG. 20A can also be used for receiving wireless signals, such as free space wireless signals transmitted by either a similar antenna or conventional antenna design.  Wireless signals received by the dielectric\nantenna 2001 at the operating face 2003 induce electromagnetic waves in the dielectric antenna 2001 that propagate towards the feed point 2002.  The electromagnetic waves continue to propagate from the feed point 2002 to the core 2052, and are thereby\ndelivered to the waveguide system, communication node or other transceiver coupled to the cable 2050.  In this configuration, the waveguide system, communication node or transceiver can include both a transmitter and receiver to perform bidirectional\ncommunications utilizing the dielectric antenna 2001.  It is further noted that in some embodiments the core 2052 of the cable 2050 (shown with dashed lines) can be configured to be collinear with the feed point 2002 to avoid a bend shown in FIG. 20A. \nIn some embodiments, a collinear configuration can reduce an alteration or distortion in the propagation of the electromagnetic due to the bend in cable 2050.\nTurning now to FIG. 20B, block diagrams illustrating example, non-limiting embodiments of dielectric antennas 2001 and 2001', each having a non-uniform spatial distribution of relative permittivity.  In particular, the non-uniform spatial\ndistribution of relative permittivity can include at least one spatial region of the dielectric antenna 2001 or 2001' having a higher relative permittivity than at least one other spatial region of the dielectric antenna.  In various embodiments, the\ndielectric antenna can be constructed of one or more dielectric materials such as a polyethylene material, a polyurethane material, Teflon or other suitable dielectric material (e.g., a synthetic resin, other plastics, etc.).  In various embodiments, the\ndielectric materials can be opaque, i.e., resistant to propagation of electromagnetic waves having an optical frequency range.\nIn one example shown, the dielectric antenna 2001 is non-uniformly doped with a dopant of high permittivity material such as rutile or other high permittivity substance such that some regions of the dielectric antenna 2001 have a higher relative\ndopant density than others.  In the embodiment of dielectric antenna 2001 that is shown, the shading of the dielectric antenna varies along a vertical axis 2007 from a light shading at near the top of the antenna and a darker shading at the bottom of the\nantenna to indicate this non-uniform doping.  For example, a high density polyethylene material, a high density polyurethane material, or other suitable dielectric materials can be doped with a rutile suspension or other dielectric materials with a high\npermittivity to increase the dielectric constant as the density of the doping increases.  Displacements along the vertical axis 2007 from a longitudinal axis 2005 of the dielectric antenna 2001 can be represented by\n-.DELTA.V.sub.max.ltoreq..DELTA.V.ltoreq..DELTA.V.sub.max, where the displacements -.DELTA.V.sub.max and .DELTA.V.sub.max represent the outer boundaries of the dielectric antenna along the vertical axis 2007.\nFunction 2076 of FIG. 20C represents the variation in relative permittivity as a function of .DELTA.V for a dielectric antenna such as dielectric antenna 2001.  In the example shown, the relative permittivity varies along a vertical axis of the\ndielectric antenna as a monotonic and continuous function from a value of .epsilon..sub.r1 corresponding to relatively high doping density--to a value of .epsilon..sub.r2 corresponding to relatively lower doping density.  It should be noted that while a\nparticular monotonic and continuous function is presented, other monotonic and continuous functions and other functions could likewise be employed.\nReturning to FIG. 20B, the convex portion 2012 of the dielectric antenna can be composed of a hydrophobic polymer such as a Teflon or other hydrophobic dielectrics.  Alternatively, the convex portion 2012 can include an operating face 2003\nhaving a hydrophobic coating, such as a spray polyamide, Teflon or other polymers or other hydrophobic coatings to reduce accumulation of water or other liquids on the operating face 2003.  In addition to providing weather protection for the dielectric\nantenna, the convex portion can function as a dielectric lens to control the beam pattern of the dielectric antenna 2001 and 2001'.\nIn various embodiments, the non-uniform spatial distribution of relative permittivity also serves to modify the shape of the beam pattern.  While the dielectric antenna 2001 is symmetrical about its longitudinal axis 2005, the dielectric antenna\n2001 generates a far-field beam pattern 2009 in a vertical plane of the dielectric antenna 2001 that is asymmetrical about the longitudinal axis.  In the example shown, the far-field beam pattern 2009 has a major lobe, characterized by a gain G.sub.1 in\nthe maximal direction, that is angularly displaced by an angle .theta.  downward from the longitudinal axis.  Furthermore, the major lobe has a gain G.sub.2 at an upward angular displacement .phi.  from a maximal direction of the major lobe and a gain\nG.sub.3 at a downward angular displacement .phi.  from the maximal direction of the major lobe in the opposite direction along a vertical plane of the antenna.  In the example shown, the beam pattern is angled slightly downward, and\nG.sub.1&gt;G.sub.3&gt;G.sub.2.  This configuration is useful for mounting of the antenna on a pole tower or other structures above ground where less gain is required to reach devices at ground level in close proximity to the antenna when compared with\nother devices at ground level at greater distances.  Furthermore, upward radiation and reception by the antenna is not needed.\nIt should be noted that the far-field beam pattern 2009 is shown in simplified form, merely as an example of the many possible far-field beam patterns that can be implemented via a non-uniform spatial distribution of relative permittivity in the\ndielectric antenna.\nIn the further example of dielectric antenna 2001', the non-uniform spatial distribution of relative permittivity is produced by constructing the dielectric antenna 2001' with four dielectric sections each being a lateral slice of the solid horn\nstructure and each composed of a different dielectric material and/or section with differing relative permittivity.  While a configuration with four different vertical sections is shown, configurations with greater or fewer sections can likewise be\nemployed.  In the embodiment of dielectric antenna 2001' that is shown, the shading of the dielectric antenna varies along a vertical axis 2007 from lighter shading for the top sections of the antenna and a darker shading at the bottom of the antenna to\nindicate the differing relative permittivity of the four sections.\nFunction 2078 of FIG. 20C represents the variation in relative permittivity as a function of .DELTA.V for a dielectric antenna such as dielectric antenna 2001'.  In the example shown, the relative permittivity varies along a vertical axis of the\ndielectric antenna as a monotonic and discrete function from a value of .epsilon..sub.r3 corresponding to relatively high permittivity to a value of .epsilon..sub.r6 corresponding to relatively lower permittivity.  It should be noted that while a\nparticular monotonic and discrete function is presented, other monotonic and discrete functions and other functions could likewise be employed.\nReturning to FIG. 20B, the non-uniform spatial distribution of relative permittivity again serves to modify the shape of the beam pattern.  While the dielectric antenna 2001' is symmetrical about its longitudinal axis 2005, the dielectric\nantenna 2001' generates a far-field beam pattern 2009 in a vertical plane of the dielectric antenna 2001' that is asymmetrical about the longitudinal axis.  It should be noted that the far-field beam pattern 2009 is shown in simplified form and similar\nto the beam produced by antenna 2001 for illustrative purposes, and merely as an example of the many possible the far-field beam patterns that can be implemented via a non-uniform spatial distribution of relative permittivity in the dielectric antenna.\nWhile a conical dielectric antenna configuration is shown with a circular cross section, other shapes including pyramidal shapes, elliptical shapes and other geometric shapes can likewise be implemented.  For example, the convex dielectric\nradome 2012 can have an elliptical shape that results in near-field wireless signals emitted by the operating face of the dielectric antenna having a first beam pattern cross section of first elliptical shape and the far-field wireless signals having a\nsecond beam pattern cross section second elliptical shape that is rotationally offset from the first elliptical shape.  In accordance with this example, the first elliptical shape of the near-field wireless signals can have a first width-to-height ratio\nand the second elliptical shape of the far-field wireless signals can have a second width-to-height ratio.  Furthermore, the first width-to-height ratio of the near-field wireless signals can be inversely proportional to the second width-to-height ratio\nof the far-field wireless signals.\nReferring now to FIG. 21, there is illustrated a block diagram of a computing environment in accordance with various aspects described herein.  In order to provide additional context for various embodiments of the embodiments described herein,\nFIG. 21 and the following discussion are intended to provide a brief, general description of a suitable computing environment 2100 in which the various embodiments of the subject disclosure can be implemented.  While the embodiments have been described\nabove in the general context of computer-executable instructions that can run on one or more computers, those skilled in the art will recognize that the embodiments can be also implemented in combination with other program modules and/or as a combination\nof hardware and software.\nGenerally, program modules comprise routines, programs, components, data structures, etc., that perform particular tasks or implement particular abstract data types.  Moreover, those skilled in the art will appreciate that the inventive methods\ncan be practiced with other computer system configurations, comprising single-processor or multiprocessor computer systems, minicomputers, mainframe computers, as well as personal computers, hand-held computing devices, microprocessor-based or\nprogrammable consumer electronics, and the like, each of which can be operatively coupled to one or more associated devices.\nAs used herein, a processing circuit includes processor as well as other application specific circuits such as an application specific integrated circuit, digital logic circuit, state machine, programmable gate array or other circuit that\nprocesses input signals or data and that produces output signals or data in response thereto.  It should be noted that while any functions and features described herein in association with the operation of a processor could likewise be performed by a\nprocessing circuit.\nThe terms \"first,\" \"second,\" \"third,\" and so forth, as used in the claims, unless otherwise clear by context, is for clarity only and doesn't otherwise indicate or imply any order in time.  For instance, \"a first determination,\" \"a second\ndetermination,\" and \"a third determination,\" does not indicate or imply that the first determination is to be made before the second determination, or vice versa, etc.\nThe illustrated embodiments of the embodiments herein can be also practiced in distributed computing environments where certain tasks are performed by remote processing devices that are linked through a communications network.  In a distributed\ncomputing environment, program modules can be located in both local and remote memory storage devices.\nComputing devices typically comprise a variety of media, which can comprise computer-readable storage media and/or communications media, which two terms are used herein differently from one another as follows.  Computer-readable storage media\ncan be any available storage media that can be accessed by the computer and comprises both volatile and nonvolatile media, removable and non-removable media.  By way of example, and not limitation, computer-readable storage media can be implemented in\nconnection with any method or technology for storage of information such as computer-readable instructions, program modules, structured data or unstructured data.\nComputer-readable storage media can comprise, but are not limited to, random access memory (RAM), read only memory (ROM), electrically erasable programmable read only memory (EEPROM), flash memory or other memory technology, compact disk read\nonly memory (CD-ROM), digital versatile disk (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices or other tangible and/or non-transitory media which can be used to store desired\ninformation.  In this regard, the terms \"tangible\" or \"non-transitory\" herein as applied to storage, memory or computer-readable media, are to be understood to exclude only propagating transitory signals per se as modifiers and do not relinquish rights\nto all standard storage, memory or computer-readable media that are not only propagating transitory signals per se.\nComputer-readable storage media can be accessed by one or more local or remote computing devices, e.g., via access requests, queries or other data retrieval protocols, for a variety of operations with respect to the information stored by the\nmedium.\nCommunications media typically embody computer-readable instructions, data structures, program modules or other structured or unstructured data in a data signal such as a modulated data signal, e.g., a carrier wave or other transport mechanism,\nand comprises any information delivery or transport media.  The term \"modulated data signal\" or signals refers to a signal that has one or more of its characteristics set or changed in such a manner as to encode information in one or more signals.  By\nway of example, and not limitation, communication media comprise wired media, such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media.\nWith reference again to FIG. 21, the example environment 2100 for transmitting and receiving signals via or forming at least part of a base station (e.g., base station devices 1504, macrocell site 1502, or base stations 1614) or central office\n(e.g., central office 1501 or 1611).  At least a portion of the example environment 2100 can also be used for transmission devices 101 or 102.  The example environment can comprise a computer 2102, the computer 2102 comprising a processing unit 2104, a\nsystem memory 2106 and a system bus 2108.  The system bus 2108 couples system components including, but not limited to, the system memory 2106 to the processing unit 2104.  The processing unit 2104 can be any of various commercially available processors. Dual microprocessors and other multiprocessor architectures can also be employed as the processing unit 2104.\nThe system bus 2108 can be any of several types of bus structure that can further interconnect to a memory bus (with or without a memory controller), a peripheral bus, and a local bus using any of a variety of commercially available bus\narchitectures.  The system memory 2106 comprises ROM 2110 and RAM 2112.  A basic input/output system (BIOS) can be stored in a non-volatile memory such as ROM, erasable programmable read only memory (EPROM), EEPROM, which BIOS contains the basic routines\nthat help to transfer information between elements within the computer 2102, such as during startup.  The RAM 2112 can also comprise a high-speed RAM such as static RAM for caching data.\nThe computer 2102 further comprises an internal hard disk drive (HDD) 2114 (e.g., EIDE, SATA), which internal hard disk drive 2114 can also be configured for external use in a suitable chassis (not shown), a magnetic floppy disk drive (FDD)\n2116, (e.g., to read from or write to a removable diskette 2118) and an optical disk drive 2120, (e.g., reading a CD-ROM disk 2122 or, to read from or write to other high capacity optical media such as the DVD).  The hard disk drive 2114, magnetic disk\ndrive 2116 and optical disk drive 2120 can be connected to the system bus 2108 by a hard disk drive interface 2124, a magnetic disk drive interface 2126 and an optical drive interface 2128, respectively.  The interface 2124 for external drive\nimplementations comprises at least one or both of Universal Serial Bus (USB) and Institute of Electrical and Electronics Engineers (IEEE) 1394 interface technologies.  Other external drive connection technologies are within contemplation of the\nembodiments described herein.\nThe drives and their associated computer-readable storage media provide nonvolatile storage of data, data structures, computer-executable instructions, and so forth.  For the computer 2102, the drives and storage media accommodate the storage of\nany data in a suitable digital format.  Although the description of computer-readable storage media above refers to a hard disk drive (HDD), a removable magnetic diskette, and a removable optical media such as a CD or DVD, it should be appreciated by\nthose skilled in the art that other types of storage media which are readable by a computer, such as zip drives, magnetic cassettes, flash memory cards, cartridges, and the like, can also be used in the example operating environment, and further, that\nany such storage media can contain computer-executable instructions for performing the methods described herein.\nA number of program modules can be stored in the drives and RAM 2112, comprising an operating system 2130, one or more application programs 2132, other program modules 2134 and program data 2136.  All or portions of the operating system,\napplications, modules, and/or data can also be cached in the RAM 2112.  The systems and methods described herein can be implemented utilizing various commercially available operating systems or combinations of operating systems.  Examples of application\nprograms 2132 that can be implemented and otherwise executed by processing unit 2104 include the diversity selection determining performed by transmission device 101 or 102.\nA user can enter commands and information into the computer 2102 through one or more wired/wireless input devices, e.g., a keyboard 2138 and a pointing device, such as a mouse 2140.  Other input devices (not shown) can comprise a microphone, an\ninfrared (IR) remote control, a joystick, a game pad, a stylus pen, touch screen or the like.  These and other input devices are often connected to the processing unit 2104 through an input device interface 2142 that can be coupled to the system bus\n2108, but can be connected by other interfaces, such as a parallel port, an IEEE 1394 serial port, a game port, a universal serial bus (USB) port, an IR interface, etc.\nA monitor 2144 or other type of display device can be also connected to the system bus 2108 via an interface, such as a video adapter 2146.  It will also be appreciated that in alternative embodiments, a monitor 2144 can also be any display\ndevice (e.g., another computer having a display, a smart phone, a tablet computer, etc.) for receiving display information associated with computer 2102 via any communication means, including via the Internet and cloud-based networks.  In addition to the\nmonitor 2144, a computer typically comprises other peripheral output devices (not shown), such as speakers, printers, etc.\nThe computer 2102 can operate in a networked environment using logical connections via wired and/or wireless communications to one or more remote computers, such as a remote computer(s) 2148.  The remote computer(s) 2148 can be a workstation, a\nserver computer, a router, a personal computer, portable computer, microprocessor-based entertainment appliance, a peer device or other common network node, and typically comprises many or all of the elements described relative to the computer 2102,\nalthough, for purposes of brevity, only a memory/storage device 2150 is illustrated.  The logical connections depicted comprise wired/wireless connectivity to a local area network (LAN) 2152 and/or larger networks, e.g., a wide area network (WAN) 2154. \nSuch LAN and WAN networking environments are commonplace in offices and companies, and facilitate enterprise-wide computer networks, such as intranets, all of which can connect to a global communications network, e.g., the Internet.\nWhen used in a LAN networking environment, the computer 2102 can be connected to the local network 2152 through a wired and/or wireless communication network interface or adapter 2156.  The adapter 2156 can facilitate wired or wireless\ncommunication to the LAN 2152, which can also comprise a wireless AP disposed thereon for communicating with the wireless adapter 2156.\nWhen used in a WAN networking environment, the computer 2102 can comprise a modem 2158 or can be connected to a communications server on the WAN 2154 or has other means for establishing communications over the WAN 2154, such as by way of the\nInternet.  The modem 2158, which can be internal or external and a wired or wireless device, can be connected to the system bus 2108 via the input device interface 2142.  In a networked environment, program modules depicted relative to the computer 2102\nor portions thereof, can be stored in the remote memory/storage device 2150.  It will be appreciated that the network connections shown are example and other means of establishing a communications link between the computers can be used.\nThe computer 2102 can be operable to communicate with any wireless devices or entities operatively disposed in wireless communication, e.g., a printer, scanner, desktop and/or portable computer, portable data assistant, communications satellite,\nany piece of equipment or location associated with a wirelessly detectable tag (e.g., a kiosk, news stand, restroom), and telephone.  This can comprise Wireless Fidelity (Wi-Fi) and BLUETOOTH.RTM.  wireless technologies.  Thus, the communication can be a\npredefined structure as with a conventional network or simply an ad hoc communication between at least two devices.\nWi-Fi can allow connection to the Internet from a couch at home, a bed in a hotel room or a conference room at work, without wires.  Wi-Fi is a wireless technology similar to that used in a cell phone that enables such devices, e.g., computers,\nto send and receive data indoors and out; anywhere within the range of a base station.  Wi-Fi networks use radio technologies called IEEE 802.11 (a, b, g, n, ac, ag etc.) to provide secure, reliable, fast wireless connectivity.  A Wi-Fi network can be\nused to connect computers to each other, to the Internet, and to wired networks (which can use IEEE 802.3 or Ethernet).  Wi-Fi networks operate in the unlicensed 2.4 and 5 GHz radio bands for example or with products that contain both bands (dual band),\nso the networks can provide real-world performance similar to the basic 10BaseT wired Ethernet networks used in many offices.\nFIG. 22 presents an example embodiment 2200 of a mobile network platform 2210 that can implement and exploit one or more aspects of the disclosed subject matter described herein.  In one or more embodiments, the mobile network platform 2210 can\ngenerate and receive signals transmitted and received by base stations (e.g., base station devices 1504, macrocell site 1502, or base stations 1614), central office (e.g., central office 1501 or 1611), or transmission device 101 or 102 associated with\nthe disclosed subject matter.  Generally, wireless network platform 2210 can comprise components, e.g., nodes, gateways, interfaces, servers, or disparate platforms, that facilitate both packet-switched (PS) (e.g., internet protocol (IP), frame relay,\nasynchronous transfer mode (ATM)) and circuit-switched (CS) traffic (e.g., voice and data), as well as control generation for networked wireless telecommunication.  As a non-limiting example, wireless network platform 2210 can be included in\ntelecommunications carrier networks, and can be considered carrier-side components as discussed elsewhere herein.  Mobile network platform 2210 comprises CS gateway node(s) 2222 which can interface CS traffic received from legacy networks like telephony\nnetwork(s) 2240 (e.g., public switched telephone network (PSTN), or public land mobile network (PLMN)) or a signaling system #7 (SS7) network 2270.  Circuit switched gateway node(s) 2222 can authorize and authenticate traffic (e.g., voice) arising from\nsuch networks.  Additionally, CS gateway node(s) 2222 can access mobility, or roaming, data generated through SS7 network 2270; for instance, mobility data stored in a visited location register (VLR), which can reside in memory 2230.  Moreover, CS\ngateway node(s) 2222 interfaces CS-based traffic and signaling and PS gateway node(s) 2218.  As an example, in a 3GPP UMTS network, CS gateway node(s) 2222 can be realized at least in part in gateway GPRS support node(s) (GGSN).  It should be appreciated\nthat functionality and specific operation of CS gateway node(s) 2222, PS gateway node(s) 2218, and serving node(s) 2216, is provided and dictated by radio technology(ies) utilized by mobile network platform 2210 for telecommunication.\nIn addition to receiving and processing CS-switched traffic and signaling, PS gateway node(s) 2218 can authorize and authenticate PS-based data sessions with served mobile devices.  Data sessions can comprise traffic, or content(s), exchanged\nwith networks external to the wireless network platform 2210, like wide area network(s) (WANs) 2250, enterprise network(s) 2270, and service network(s) 2280, which can be embodied in local area network(s) (LANs), can also be interfaced with mobile\nnetwork platform 2210 through PS gateway node(s) 2218.  It is to be noted that WANs 2250 and enterprise network(s) 2260 can embody, at least in part, a service network(s) like IP multimedia subsystem (IMS).  Based on radio technology layer(s) available\nin technology resource(s) 2217, packet-switched gateway node(s) 2218 can generate packet data protocol contexts when a data session is established; other data structures that facilitate routing of packetized data also can be generated.  To that end, in\nan aspect, PS gateway node(s) 2218 can comprise a tunnel interface (e.g., tunnel termination gateway (TTG) in 3GPP UMTS network(s) (not shown)) which can facilitate packetized communication with disparate wireless network(s), such as Wi-Fi networks.\nIn embodiment 2200, wireless network platform 2210 also comprises serving node(s) 2216 that, based upon available radio technology layer(s) within technology resource(s) 2217, convey the various packetized flows of data streams received through\nPS gateway node(s) 2218.  It is to be noted that for technology resource(s) 2217 that rely primarily on CS communication, server node(s) can deliver traffic without reliance on PS gateway node(s) 2218; for example, server node(s) can embody at least in\npart a mobile switching center.  As an example, in a 3GPP UMTS network, serving node(s) 2216 can be embodied in serving GPRS support node(s) (SGSN).\nFor radio technologies that exploit packetized communication, server(s) 2214 in wireless network platform 2210 can execute numerous applications that can generate multiple disparate packetized data streams or flows, and manage (e.g., schedule,\nqueue, format .  . . ) such flows.  Such application(s) can comprise add-on features to standard services (for example, provisioning, billing, customer support .  . . ) provided by wireless network platform 2210.  Data streams (e.g., content(s) that are\npart of a voice call or data session) can be conveyed to PS gateway node(s) 2218 for authorization/authentication and initiation of a data session, and to serving node(s) 2216 for communication thereafter.  In addition to application server, server(s)\n2214 can comprise utility server(s), a utility server can comprise a provisioning server, an operations and maintenance server, a security server that can implement at least in part a certificate authority and firewalls as well as other security\nmechanisms, and the like.  In an aspect, security server(s) secure communication served through wireless network platform 2210 to ensure network's operation and data integrity in addition to authorization and authentication procedures that CS gateway\nnode(s) 2222 and PS gateway node(s) 2218 can enact.  Moreover, provisioning server(s) can provision services from external network(s) like networks operated by a disparate service provider; for instance, WAN 2250 or Global Positioning System (GPS)\nnetwork(s) (not shown).  Provisioning server(s) can also provision coverage through networks associated to wireless network platform 2210 (e.g., deployed and operated by the same service provider), such as the distributed antennas networks shown in FIG.\n1(s) that enhance wireless service coverage by providing more network coverage.  Repeater devices such as those shown in FIGS. 7, 8, and 9 also improve network coverage in order to enhance subscriber service experience by way of UE 2275.\nIt is to be noted that server(s) 2214 can comprise one or more processors configured to confer at least in part the functionality of macro network platform 2210.  To that end, the one or more processor can execute code instructions stored in\nmemory 2230, for example.  It is should be appreciated that server(s) 2214 can comprise a content manager 2215, which operates in substantially the same manner as described hereinbefore.\nIn example embodiment 2200, memory 2230 can store information related to operation of wireless network platform 2210.  Other operational information can comprise provisioning information of mobile devices served through wireless platform network\n2210, subscriber databases; application intelligence, pricing schemes, e.g., promotional rates, flat-rate programs, couponing campaigns; technical specification(s) consistent with telecommunication protocols for operation of disparate radio, or wireless,\ntechnology layers; and so forth.  Memory 2230 can also store information from at least one of telephony network(s) 2240, WAN 2250, enterprise network(s) 2270, or SS7 network 2260.  In an aspect, memory 2230 can be, for example, accessed as part of a data\nstore component or as a remotely connected memory store.\nIn order to provide a context for the various aspects of the disclosed subject matter, FIG. 22, and the following discussion, are intended to provide a brief, general description of a suitable environment in which the various aspects of the\ndisclosed subject matter can be implemented.  While the subject matter has been described above in the general context of computer-executable instructions of a computer program that runs on a computer and/or computers, those skilled in the art will\nrecognize that the disclosed subject matter also can be implemented in combination with other program modules.  Generally, program modules comprise routines, programs, components, data structures, etc. that perform particular tasks and/or implement\nparticular abstract data types.\nFIG. 23 depicts an illustrative embodiment of a communication device 2300.  The communication device 2300 can serve as an illustrative embodiment of devices such as mobile devices and in-building devices referred to by the subject disclosure\n(e.g., in FIGS. 15, 16A and 16B).\nThe communication device 2300 can comprise a wireline and/or wireless transceiver 2302 (herein transceiver 2302), a user interface (UI) 2304, a power supply 2314, a location receiver 2316, a motion sensor 2318, an orientation sensor 2320, and a\ncontroller 2306 for managing operations thereof.  The transceiver 2302 can support short-range or long-range wireless access technologies such as Bluetooth.RTM., ZigBee.RTM., WiFi, DECT, or cellular communication technologies, just to mention a few\n(Bluetooth.RTM.  and ZigBee.RTM.  are trademarks registered by the Bluetooth.RTM.  Special Interest Group and the ZigBee.RTM.  Alliance, respectively).  Cellular technologies can include, for example, CDMA-1X, UMTS/HSDPA, GSM/GPRS, TDMA/EDGE, EV/DO,\nWiMAX, SDR, LTE, as well as other next generation wireless communication technologies as they arise.  The transceiver 2302 can also be adapted to support circuit-switched wireline access technologies (such as PSTN), packet-switched wireline access\ntechnologies (such as TCP/IP, VoIP, etc.), and combinations thereof.\nThe UI 2304 can include a depressible or touch-sensitive keypad 2308 with a navigation mechanism such as a roller ball, a joystick, a mouse, or a navigation disk for manipulating operations of the communication device 2300.  The keypad 2308 can\nbe an integral part of a housing assembly of the communication device 2300 or an independent device operably coupled thereto by a tethered wireline interface (such as a USB cable) or a wireless interface supporting for example Bluetooth.RTM..  The keypad\n2308 can represent a numeric keypad commonly used by phones, and/or a QWERTY keypad with alphanumeric keys.  The UI 2304 can further include a display 2310 such as monochrome or color LCD (Liquid Crystal Display), OLED (Organic Light Emitting Diode) or\nother suitable display technology for conveying images to an end user of the communication device 2300.  In an embodiment where the display 2310 is touch-sensitive, a portion or all of the keypad 2308 can be presented by way of the display 2310 with\nnavigation features.\nThe display 2310 can use touch screen technology to also serve as a user interface for detecting user input.  As a touch screen display, the communication device 2300 can be adapted to present a user interface having graphical user interface\n(GUI) elements that can be selected by a user with a touch of a finger.  The touch screen display 2310 can be equipped with capacitive, resistive or other forms of sensing technology to detect how much surface area of a user's finger has been placed on a\nportion of the touch screen display.  This sensing information can be used to control the manipulation of the GUI elements or other functions of the user interface.  The display 2310 can be an integral part of the housing assembly of the communication\ndevice 2300 or an independent device communicatively coupled thereto by a tethered wireline interface (such as a cable) or a wireless interface.\nThe UI 2304 can also include an audio system 2312 that utilizes audio technology for conveying low volume audio (such as audio heard in proximity of a human ear) and high volume audio (such as speakerphone for hands free operation).  The audio\nsystem 2312 can further include a microphone for receiving audible signals of an end user.  The audio system 2312 can also be used for voice recognition applications.  The UI 2304 can further include an image sensor 2313 such as a charged coupled device\n(CCD) camera for capturing still or moving images.\nThe power supply 2314 can utilize common power management technologies such as replaceable and rechargeable batteries, supply regulation technologies, and/or charging system technologies for supplying energy to the components of the\ncommunication device 2300 to facilitate long-range or short-range portable communications.  Alternatively, or in combination, the charging system can utilize external power sources such as DC power supplied over a physical interface such as a USB port or\nother suitable tethering technologies.\nThe location receiver 2316 can utilize location technology such as a global positioning system (GPS) receiver capable of assisted GPS for identifying a location of the communication device 2300 based on signals generated by a constellation of\nGPS satellites, which can be used for facilitating location services such as navigation.  The motion sensor 2318 can utilize motion sensing technology such as an accelerometer, a gyroscope, or other suitable motion sensing technology to detect motion of\nthe communication device 2300 in three-dimensional space.  The orientation sensor 2320 can utilize orientation sensing technology such as a magnetometer to detect the orientation of the communication device 2300 (north, south, west, and east, as well as\ncombined orientations in degrees, minutes, or other suitable orientation metrics).\nThe communication device 2300 can use the transceiver 2302 to also determine a proximity to a cellular, WiFi, Bluetooth.RTM., or other wireless access points by sensing techniques such as utilizing a received signal strength indicator (RSSI)\nand/or signal time of arrival (TOA) or time of flight (TOF) measurements.  The controller 2306 can utilize computing technologies such as a microprocessor, a digital signal processor (DSP), programmable gate arrays, application specific integrated\ncircuits, and/or a video processor with associated storage memory such as Flash, ROM, RAM, SRAM, DRAM or other storage technologies for executing computer instructions, controlling, and processing data supplied by the aforementioned components of the\ncommunication device 2300.\nOther components not shown in FIG. 23 can be used in one or more embodiments of the subject disclosure.  For instance, the communication device 2300 can include a slot for adding or removing an identity module such as a Subscriber Identity\nModule (SIM) card or Universal Integrated Circuit Card (UICC).  SIM or UICC cards can be used for identifying subscriber services, executing programs, storing subscriber data, and so on.\nIn the subject specification, terms such as \"store,\" \"storage,\" \"data store,\" data storage,\" \"database,\" and substantially any other information storage component relevant to operation and functionality of a component, refer to \"memory\ncomponents,\" or entities embodied in a \"memory\" or components comprising the memory.  It will be appreciated that the memory components described herein can be either volatile memory or nonvolatile memory, or can comprise both volatile and nonvolatile\nmemory, by way of illustration, and not limitation, volatile memory, non-volatile memory, disk storage, and memory storage.  Further, nonvolatile memory can be included in read only memory (ROM), programmable ROM (PROM), electrically programmable ROM\n(EPROM), electrically erasable ROM (EEPROM), or flash memory.  Volatile memory can comprise random access memory (RAM), which acts as external cache memory.  By way of illustration and not limitation, RAM is available in many forms such as synchronous\nRAM (SRAM), dynamic RAM (DRAM), synchronous DRAM (SDRAM), double data rate SDRAM (DDR SDRAM), enhanced SDRAM (ESDRAM), Synchlink DRAM (SLDRAM), and direct Rambus RAM (DRRAM).  Additionally, the disclosed memory components of systems or methods herein are\nintended to comprise, without being limited to comprising, these and any other suitable types of memory.\nMoreover, it will be noted that the disclosed subject matter can be practiced with other computer system configurations, comprising single-processor or multiprocessor computer systems, mini-computing devices, mainframe computers, as well as\npersonal computers, hand-held computing devices (e.g., PDA, phone, smartphone, watch, tablet computers, netbook computers, etc.), microprocessor-based or programmable consumer or industrial electronics, and the like.  The illustrated aspects can also be\npracticed in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network; however, some if not all aspects of the subject disclosure can be practiced on stand-alone computers. In a distributed computing environment, program modules can be located in both local and remote memory storage devices.\nSome of the embodiments described herein can also employ artificial intelligence (AI) to facilitate automating one or more features described herein.  For example, artificial intelligence can be used in optional training controller 230 evaluate\nand select candidate frequencies, modulation schemes, MIMO modes, and/or guided wave modes in order to maximize transfer efficiency.  The embodiments (e.g., in connection with automatically identifying acquired cell sites that provide a maximum\nvalue/benefit after addition to an existing communication network) can employ various AI-based schemes for carrying out various embodiments thereof.  Moreover, the classifier can be employed to determine a ranking or priority of the each cell site of the\nacquired network.  A classifier is a function that maps an input attribute vector, x=(x1, x2, x3, x4, .  . . , xn), to a confidence that the input belongs to a class, that is, f(x)=confidence (class).  Such classification can employ a probabilistic\nand/or statistical-based analysis (e.g., factoring into the analysis utilities and costs) to prognose or infer an action that a user desires to be automatically performed.  A support vector machine (SVM) is an example of a classifier that can be\nemployed.  The SVM operates by finding a hypersurface in the space of possible inputs, which the hypersurface attempts to split the triggering criteria from the non-triggering events.  Intuitively, this makes the classification correct for testing data\nthat is near, but not identical to training data.  Other directed and undirected model classification approaches comprise, e.g., naive Bayes, Bayesian networks, decision trees, neural networks, fuzzy logic models, and probabilistic classification models\nproviding different patterns of independence can be employed.  Classification as used herein also is inclusive of statistical regression that is utilized to develop models of priority.\nAs will be readily appreciated, one or more of the embodiments can employ classifiers that are explicitly trained (e.g., via a generic training data) as well as implicitly trained (e.g., via observing UE behavior, operator preferences,\nhistorical information, receiving extrinsic information).  For example, SVMs can be configured via a learning or training phase within a classifier constructor and feature selection module.  Thus, the classifier(s) can be used to automatically learn and\nperform a number of functions, including but not limited to determining according to a predetermined criteria which of the acquired cell sites will benefit a maximum number of subscribers and/or which of the acquired cell sites will add minimum value to\nthe existing communication network coverage, etc.\nAs used in some contexts in this application, in some embodiments, the terms \"component,\" \"system\" and the like are intended to refer to, or comprise, a computer-related entity or an entity related to an operational apparatus with one or more\nspecific functionalities, wherein the entity can be either hardware, a combination of hardware and software, software, or software in execution.  As an example, a component may be, but is not limited to being, a process running on a processor, a\nprocessor, an object, an executable, a thread of execution, computer-executable instructions, a program, and/or a computer.  By way of illustration and not limitation, both an application running on a server and the server can be a component.  One or\nmore components may reside within a process and/or thread of execution and a component may be localized on one computer and/or distributed between two or more computers.  In addition, these components can execute from various computer readable media\nhaving various data structures stored thereon.  The components may communicate via local and/or remote processes such as in accordance with a signal having one or more data packets (e.g., data from one component interacting with another component in a\nlocal system, distributed system, and/or across a network such as the Internet with other systems via the signal).  As another example, a component can be an apparatus with specific functionality provided by mechanical parts operated by electric or\nelectronic circuitry, which is operated by a software or firmware application executed by a processor, wherein the processor can be internal or external to the apparatus and executes at least a part of the software or firmware application.  As yet\nanother example, a component can be an apparatus that provides specific functionality through electronic components without mechanical parts, the electronic components can comprise a processor therein to execute software or firmware that confers at least\nin part the functionality of the electronic components.  While various components have been illustrated as separate components, it will be appreciated that multiple components can be implemented as a single component, or a single component can be\nimplemented as multiple components, without departing from example embodiments.\nFurther, the various embodiments can be implemented as a method, apparatus or article of manufacture using standard programming and/or engineering techniques to produce software, firmware, hardware or any combination thereof to control a\ncomputer to implement the disclosed subject matter.  The term \"article of manufacture\" as used herein is intended to encompass a computer program accessible from any computer-readable device or computer-readable storage/communications media.  For\nexample, computer readable storage media can include, but are not limited to, magnetic storage devices (e.g., hard disk, floppy disk, magnetic strips), optical disks (e.g., compact disk (CD), digital versatile disk (DVD)), smart cards, and flash memory\ndevices (e.g., card, stick, key drive).  Of course, those skilled in the art will recognize many modifications can be made to this configuration without departing from the scope or spirit of the various embodiments.\nIn addition, the words \"example\" and \"exemplary\" are used herein to mean serving as an instance or illustration.  Any embodiment or design described herein as \"example\" or \"exemplary\" is not necessarily to be construed as preferred or\nadvantageous over other embodiments or designs.  Rather, use of the word example or exemplary is intended to present concepts in a concrete fashion.  As used in this application, the term \"or\" is intended to mean an inclusive \"or\" rather than an\nexclusive \"or\".  That is, unless specified otherwise or clear from context, \"X employs A or B\" is intended to mean any of the natural inclusive permutations.  That is, if X employs A; X employs B; or X employs both A and B, then \"X employs A or B\" is\nsatisfied under any of the foregoing instances.  In addition, the articles \"a\" and \"an\" as used in this application and the appended claims should generally be construed to mean \"one or more\" unless specified otherwise or clear from context to be\ndirected to a singular form.\nMoreover, terms such as \"user equipment,\" \"mobile station,\" \"mobile,\" subscriber station,\" \"access terminal,\" \"terminal,\" \"handset,\" \"mobile device\" (and/or terms representing similar terminology) can refer to a wireless device utilized by a\nsubscriber or user of a wireless communication service to receive or convey data, control, voice, video, sound, gaming or substantially any data-stream or signaling-stream.  The foregoing terms are utilized interchangeably herein and with reference to\nthe related drawings.\nFurthermore, the terms \"user,\" \"subscriber,\" \"customer,\" \"consumer\" and the like are employed interchangeably throughout, unless context warrants particular distinctions among the terms.  It should be appreciated that such terms can refer to\nhuman entities or automated components supported through artificial intelligence (e.g., a capacity to make inference based, at least, on complex mathematical formalisms), which can provide simulated vision, sound recognition and so forth.\nAs employed herein, the term \"processor\" can refer to substantially any computing processing unit or device comprising, but not limited to comprising, single-core processors; single-processors with software multithread execution capability;\nmulti-core processors; multi-core processors with software multithread execution capability; multi-core processors with hardware multithread technology; parallel platforms; and parallel platforms with distributed shared memory.  Additionally, a processor\ncan refer to an integrated circuit, an application specific integrated circuit (ASIC), a digital signal processor (DSP), a field programmable gate array (FPGA), a programmable logic controller (PLC), a complex programmable logic device (CPLD), a discrete\ngate or transistor logic, discrete hardware components or any combination thereof designed to perform the functions described herein.  Processors can exploit nano-scale architectures such as, but not limited to, molecular and quantum-dot based\ntransistors, switches and gates, in order to optimize space usage or enhance performance of user equipment.  A processor can also be implemented as a combination of computing processing units.\nAs used herein, terms such as \"data storage,\" data storage,\" \"database,\" and substantially any other information storage component relevant to operation and functionality of a component, refer to \"memory components,\" or entities embodied in a\n\"memory\" or components comprising the memory.  It will be appreciated that the memory components or computer-readable storage media, described herein can be either volatile memory or nonvolatile memory or can include both volatile and nonvolatile memory.\nWhat has been described above includes mere examples of various embodiments.  It is, of course, not possible to describe every conceivable combination of components or methodologies for purposes of describing these examples, but one of ordinary\nskill in the art can recognize that many further combinations and permutations of the present embodiments are possible.  Accordingly, the embodiments disclosed and/or claimed herein are intended to embrace all such alterations, modifications and\nvariations that fall within the spirit and scope of the appended claims.  Furthermore, to the extent that the term \"includes\" is used in either the detailed description or the claims, such term is intended to be inclusive in a manner similar to the term\n\"comprising\" as \"comprising\" is interpreted when employed as a transitional word in a claim.\nIn addition, a flow diagram may include a \"start\" and/or \"continue\" indication.  The \"start\" and \"continue\" indications reflect that the steps presented can optionally be incorporated in or otherwise used in conjunction with other routines.  In\nthis context, \"start\" indicates the beginning of the first step presented and may be preceded by other activities not specifically shown.  Further, the \"continue\" indication reflects that the steps presented may be performed multiple times and/or may be\nsucceeded by other activities not specifically shown.  Further, while a flow diagram indicates a particular ordering of steps, other orderings are likewise possible provided that the principles of causality are maintained.\nAs may also be used herein, the term(s) \"operably coupled to\", \"coupled to\", and/or \"coupling\" includes direct coupling between items and/or indirect coupling between items via one or more intervening items.  Such items and intervening items\ninclude, but are not limited to, junctions, communication paths, components, circuit elements, circuits, functional blocks, and/or devices.  As an example of indirect coupling, a signal conveyed from a first item to a second item may be modified by one\nor more intervening items by modifying the form, nature or format of information in a signal, while one or more elements of the information in the signal are nevertheless conveyed in a manner than can be recognized by the second item.  In a further\nexample of indirect coupling, an action in a first item can cause a reaction on the second item, as a result of actions and/or reactions in one or more intervening items.\nAlthough specific embodiments have been illustrated and described herein, it should be appreciated that any arrangement which achieves the same or similar purpose may be substituted for the embodiments described or shown by the subject\ndisclosure.  The subject disclosure is intended to cover any and all adaptations or variations of various embodiments.  Combinations of the above embodiments, and other embodiments not specifically described herein, can be used in the subject disclosure. For instance, one or more features from one or more embodiments can be combined with one or more features of one or more other embodiments.  In one or more embodiments, features that are positively recited can also be negatively recited and excluded from\nthe embodiment with or without replacement by another structural and/or functional feature.  The steps or functions described with respect to the embodiments of the subject disclosure can be performed in any order.  The steps or functions described with\nrespect to the embodiments of the subject disclosure can be performed alone or in combination with other steps or functions of the subject disclosure, as well as from other embodiments or from other steps that have not been described in the subject\ndisclosure.  Further, more than or less than all of the features described with respect to an embodiment can also be utilized.", "application_number": "15299576", "abstract": " Aspects of the subject disclosure may include, for example, a solid\n     dielectric antenna having a non-uniform spatial distribution of relative\n     permittivity.\n", "citations": ["395814", "529290", "1721785", "1798613", "1860123", "2058611", "2129711", "2147717", "2187908", "2199083", "2202380", "2232179", "2283935", "2398095", "2402622", "2405242", "2407068", "2407069", "2411338", "2415089", "2415807", "2419205", "2420007", "2422058", "2432134", "2461005", "2471021", "2488400", "2513205", "2514679", "2519603", "2540839", "2542980", "2557110", "2562281", "2596190", "2599864", "2659817", "2677055", "2685068", "2688732", "2691766", "2706279", "2711514", "2723378", "2727232", "2740826", "2745101", "2748350", "2749545", "2754513", "2761137", "2769147", "2769148", "2794959", "2805415", "2806972", "2810111", "2819451", "2820083", "2825060", "2835871", "2851686", "2852753", "2867776", "2883135", "2883136", "2910261", "2912695", "2914741", "2915270", "2921277", "2925458", "2933701", "2946970", "2949589", "2960670", "2970800", "2972148", "2974297", "2981949", "2990151", "2993205", "3025478", "3028565", "3040278", "3045238", "3047822", "3065945", "3072870", "3077569", "3101472", "3109175", "3116485", "3129356", "3134951", "3146297", "3146453", "3201724", "3205462", "3218384", "3219954", "3234559", "3255454", "3296364", "3296685", "3310808", "3316344", "3316345", "3318561", "3321763", "3329958", "3351947", "3355738", "3369788", "3389394", "3392388", "3392395", "3411112", "3413637", "3413642", "3414903", "3420596", "3427573", "3448455", "3453617", "3459873", "3465346", "3474995", "3482251", "3487158", "3495262", "3500422", "3509463", "3522560", "3524192", "3529205", "3530481", "3531803", "3536800", "3555553", "3557341", "3566317", "3568204", "3569979", "3588754", "3589121", "3594494", "3599219", "3603904", "3603951", "3609247", "3623114", "3638224", "3653622", "3666902", "3668459", "3668574", "3672202", "3686596", "3693922", "3699574", "3703690", "3704001", "3725937", "3753086", "3760127", "3765021", "3772528", "3775769", "3787872", "3796970", "3806931", "3833909", "3835407", "3845426", "3858214", "3877032", "3888446", "3896380", "3911415", "3921949", "3925763", "3935577", "3936836", "3936838", "3952984", "3959794", "3973087", "3973240", "3976358", "3983560", "4010799", "4012743", "4020431", "4026632", "4030048", "4030953", "4031536", "4035054", "4047180", "4079361", "4080600", "4099184", "4115782", "4123759", "4125768", "4129872", "4149170", "4155108", "4156241", "4166669", "4175257", "4188595", "4190137", "4191953", "4195302", "4210357", "4216449", "4220957", "4231042", "4234753", "4238974", "4246584", "4247858", "4250489", "4274097", "4274112", "4278955", "4293833", "4298877", "4300242", "4307938", "4316646", "4319074", "4329690", "4333082", "4335613", "4336719", "4345256", "4366565", "4367446", "4378143", "4384289", "4398058", "4398121", "4413263", "4447811", "4458250", "4463329", "4468672", "4475209", "4477814", "4482899", "4488156", "4491386", "4495498", "4516130", "4525432", "4525693", "4533875", "4541303", "4550271", "4553112", "4556271", "4558325", "4565348", "4566012", "4567401", "4573215", "4589424", "4598262", "4599598", "4604551", "4604624", "4604627", "4618867", "4636753", "4638322", "4641916", "4642651", "4644365", "4647329", "4660050", "4665660", "4672384", "4673943", "4680558", "4694599", "4704611", "4715695", "4717974", "4728910", "4730172", "4730888", "4731810", "4735097", "4743915", "4743916", "4745377", "4746241", "4749244", "4755830", "4757324", "4758962", "4764738", "4772891", "4777457", "4785304", "4786913", "4788553", "4792771", "4792812", "4799031", "4800350", "4801937", "4818963", "4818990", "4821006", "4825221", "4829310", "4829314", "4831346", "4832148", "4835517", "4839659", "4845508", "4847610", "4849611", "4851788", "4855749", "4866454", "4873534", "4879544", "4881028", "4886980", "4897663", "4904996", "4915468", "4916460", "4922180", "4929962", "4931808", "4932620", "4946202", "4956620", "4965856", "4977593", "4977618", "4989011", "4998095", "5003318", "5006846", "5006859", "5015914", "5017936", "5017937", "5018180", "5019832", "5036335", "956", "5042903", "5043538", "5043629", "5044722", "5045820", "5057106", "5065760", "5065969", "5072228", "5082349", "5086467", "5107231", "5109232", "5113197", "5117237", "5121129", "5126750", "5132968", "5134251", "5134423", "5134965", "5142767", "5148509", "5152861", "5153676", "5166698", "5174164", "5175560", "5182427", "5187409", "5193774", "5198823", "5212755", "5214394", "5214438", "5216616", "5218657", "5235662", "5239537", "5241321", "5241701", "5248876", "5254809", "5265266", "5266961", "5276455", "5278687", "5280297", "5291211", "5298911", "5299773", "5304999", "5311596", "5327149", "5329285", "5341088", "5345522", "5347287", "5352984", "5353036", "5359338", "5371623", "5379455", "5380224", "5381160", "5389442", "5400040", "5402140", "5402151", "5404146", "5410318", "5412654", "5428364", "5428818", "5434575", "5440660", "5451969", "5479176", "5481268", "5482525", "5486839", "5488380", "5495546", "5499308", "5499311", "5502392", "5512906", "5513176", "5514965", "5515059", "5519408", "5528208", "5539421", "5543000", "5557283", "5559359", "5566022", "5566196", "5576721", "5586054", "5592183", "5600630", "5603089", "5619015", "5621421", "5627879", "5628050", "5630223", "5637521", "5640168", "5642121", "5646936", "5650788", "5652554", "5663693", "5671304", "5677699", "5677909", "5680139", "5682256", "5684495", "5686930", "5724168", "5726980", "5748153", "5750941", "5757323", "5767807", "5768689", "5769879", "5784033", "5784034", "5784683", "5787673", "5793334", "5800494", "5805983", "5809395", "5812524", "5818390", "5818396", "5818512", "5845391", "5848054", "5850199", "5854608", "5859618", "5861843", "5867763", "5870060", "5872544", "5872547", "5873324", "5889449", "5890055", "5892480", "5898133", "5898830", "5900847", "5903373", "5905438", "5905949", "5910790", "5917977", "5922081", "5926128", "5933422", "5936589", "5948044", "5948108", "5952964", "5952972", "5952984", "5955992", "5959578", "5959590", "5973641", "5977650", "5978738", "5982276", "5986331", "5987099", "5990848", "5994984", "5994998", "6005694", "6005758", "6009124", "6011520", "6011524", "6014110", "6023619", "6026173", "6026208", "6026331", "6031455", "6034638", "6037894", "6038425", "6049647", "6057802", "6061035", "6063234", "6075451", "6075493", "6076044", "6078297", "6088001", "6095820", "6100846", "6103031", "6107897", "6111553", "6114998", "6121885", "6122753", "6140911", "6140976", "6142434", "6146330", "6150612", "6151145", "6154488", "6158383", "6163296", "6166694", "6167055", "6175917", "6177801", "6184828", "6195058", "6195395", "6198440", "6208161", "6208308", "6208903", "6211836", "6211837", "6215443", "6219006", "6222503", "6225960", "6229327", "6236365", "6239377", "6239379", "6239761", "6241045", "6243049", "6246821", "6252553", "6259337", "6266016", "6266025", "6268835", "6271790", "6271799", "6271952", "6278357", "6278370", "6281855", "6282354", "6283425", "6285325", "6292139", "6292143", "6292153", "6300906", "6301420", "6308085", "6311288", "6317028", "6317092", "6320509", "6320553", "6323819", "6329959", "6348683", "6351247", "6356246", "6357709", "6362788", "6362789", "6366238", "6373436", "6373441", "6376824", "6388564", "6396440", "6404773", "6404775", "6421021", "6433736", "6433741", "6436536", "6441723", "6445351", "6445774", "6452467", "6452569", "6452923", "6455769", "6456251", "6462700", "6463295", "6469676", "6473049", "6480168", "6483470", "6489928", "6489931", "6492957", "6501433", "6507573", "6510152", "6515635", "6522305", "6531991", "6532215", "6534996", "6535169", "6542739", "6549106", "6549173", "6552693", "6559811", "6563981", "6567573", "6573803", "6573813", "6580295", "6584084", "6584252", "6587077", "6593893", "6594238", "6596944", "6600456", "6606057", "6606066", "6606077", "6611252", "6614237", "6628859", "6631229", "6634225", "6639484", "6639566", "6642887", "6643254", "6650296", "6653598", "6653848", "6657437", "6659655", "6661391", "6668104", "6670921", "6671824", "6677899", "6680903", "6683580", "6686832", "6686873", "6686875", "6697027", "6697030", "6703981", "6714165", "6720935", "6725035", "6727470", "6727891", "6728439", "6728552", "6731210", "6731649", "6741705", "6747557", "6750827", "6754470", "6755312", "6756538", "6765479", "6768454", "6768456", "6768471", "6768474", "6771216", "6771225", "6771739", "6774859", "6788865", "6788951", "6789119", "6792290", "6806710", "6809633", "6809695", "6812895", "6819744", "6822615", "6839032", "6839160", "6839846", "6842157", "6842430", "6850128", "6853351", "6856273", "6859185", "6859187", "6859590", "6861998", "6864851", "6864853", "6867744", "6868258", "6870465", "6873265", "6885674", "6886065", "6888623", "6901064", "6904218", "6906676", "6906681", "6909893", "6917974", "6920289", "6920315", "6920407", "6922135", "6924732", "6924776", "6933887", "6934655", "6937595", "6943553", "6944555", "6947147", "6947376", "6947635", "6948371", "6950567", "6952143", "6952183", "6956506", "6958729", "6965302", "6965355", "6965784", "6967627", "6970502", "6970682", "6972729", "6980091", "6982611", "6982679", "6983174", "6985118", "6992639", "6999667", "7008120", "7009471", "7012489", "7012572", "7016585", "7019704", "7023400", "7026917", "7027003", "7027454", "7032016", "7038636", "7039048", "7042403", "7042416", "7042420", "7043271", "7054286", "7054376", "7054513", "7055148", "7057558", "7057573", "7058524", "7061370", "7061891", "7068998", "7069163", "7075414", "7075485", "7075496", "7082321", "7084742", "7088221", "7088306", "7098405", "7098773", "7102581", "7106265", "7106270", "7106273", "7109939", "7113002", "7113134", "7119755", "7120338", "7120345", "7122012", "7123191", "7123801", "7125512", "7126557", "7126711", "7130516", "7132950", "7133930", "7134012", "7134135", "7136397", "7136772", "7137605", "7138767", "7138958", "7139328", "7145440", "7145552", "7151497", "7155238", "7161934", "7164354", "7167139", "7171087", "7171308", "7171493", "7176589", "7180459", "7180467", "7183922", "7183991", "7183998", "7193562", "7194528", "7199680", "7200391", "7200658", "7205950", "7212163", "7215220", "7215928", "7218285", "7224170", "7224243", "7224272", "7224320", "7224985", "7228123", "7234413", "7234895", "7239284", "7243610", "7248148", "7250772", "7255821", "7259657", "7260424", "7266154", "7266275", "7272281", "7272362", "7274305", "7274936", "7276990", "7280033", "7280803", "7282922", "7286099", "7289449", "7289704", "7289828", "7292125", "7292196", "7295161", "7297869", "7301440", "7301508", "7307357", "7307596", "7308264", "7308370", "7309873", "7310065", "7310335", "7311605", "7312686", "7313087", "7313312", "7315224", "7315678", "7318564", "7319717", "7321291", "7321707", "7324046", "7324817", "7329815", "7333064", "7333593", "7339466", "7339897", "7340768", "7345623", "7346244", "7346359", "7353293", "7355560", "7358808", "7358921", "7369085", "7369095", "7376191", "7380272", "7381089", "7382232", "7383577", "7388450", "7397422", "7398946", "7400304", "7403169", "7406337", "7408426", "7408507", "7408923", "7410606", "7417587", "7418178", "7420474", "7420525", "7423604", "7426554", "7427927", "7430257", "7430932", "7443334", "7444404", "7446567", "7450000", "7450001", "7453352", "7453393", "7456650", "7459834", "7460834", "7463877", "7465879", "7466225", "7468657", "7477285", "7479776", "7479841", "7486247", "7490275", "7492317", "7496674", "7498822", "7502619", "7504938", "7508834", "7509009", "7509675", "7512090", "7515041", "7516487", "7518529", "7518952", "7519323", "7522115", "7522812", "7525501", "7525504", "7531803", "7532792", "7535867", "7539381", "7541981", "7545818", "7546214", "7548212", "7551921", "7554998", "7555182", "7555186", "7555187", "7557563", "7561025", "7567154", "7567740", "7570137", "7570470", "7577398", "7580643", "7581702", "7583074", "7583233", "7584470", "7589470", "7589630", "7589686", "7590404", "7591020", "7591792", "7593067", "7596222", "7598844", "7602333", "7602815", "7605768", "7620370", "7625131", "7626489", "7626542", "7627300", "7633442", "7634250", "7639201", "7640562", "7640581", "7653363", "41147", "7656167", "7656358", "7660244", "7660252", "7660328", "7664117", "7669049", "7671701", "7671820", "7672271", "7676679", "7680478", "7680516", "7680561", "7683848", "7684383", "7693079", "7693162", "7693939", "7697417", "7701931", "7705747", "7710346", "7714536", "7714709", "7714725", "7715672", "7716660", "7724782", "7728772", "7729285", "7733094", "7734717", "7737903", "7739402", "7743403", "7747356", "7747774", "7750244", "7750763", "7751054", "7760978", "7761079", "7764943", "7773664", "7782156", "7783195", "7786894", "7786945", "7786946", "7791549", "7792016", "7795877", "7795994", "7796025", "7796122", "7796890", "7797367", "7805029", "7808441", "7809223", "7812686", "7812778", "7813344", "7817063", "7825793", "7825867", "7826602", "7827610", "7830228", "7835128", "7835600", "7843375", "7844081", "7848517", "7852752", "7852837", "7853267", "7855612", "7856007", "7869391", "7872610", "7873249", "7876174", "7884285", "7884648", "7885542", "7889129", "7889148", "7889149", "7890053", "7893789", "7894770", "7898480", "7899403", "7903918", "7903972", "7906973", "7907097", "7915980", "7916081", "7929940", "7930750", "7937699", "7940207", "7940731", "7956818", "7958120", "7961710", "7962957", "7965842", "7970365", "7970937", "7971053", "7974387", "7983740", "7986711", "7990146", "7990329", "7991877", "7992014", "7994996", "7994999", "7997546", "8010116", "8013694", "8019288", "8022885", "8022887", "8023410", "8027391", "8036207", "8049576", "8054199", "8059576", "8059593", "8060308", "8063832", "8064744", "8064944", "8065099", "8069483", "8072323", "8072386", "8073810", "8077049", "8077113", "8081854", "8089356", "8089404", "8089952", "8090258", "8090379", "8094081", "8094985", "8095093", "8098198", "8102324", "8102779", "8106749", "8106849", "43163", "8111148", "8112649", "8120488", "8121624", "8125282", "8125399", "8126393", "8129817", "8131125", "8131266", "8132239", "8134424", "8134458", "8140113", "8150311", "8151306", "8156520", "8159316", "8159342", "8159385", "8159394", "8159742", "8159933", "8159955", "8160064", "8160530", "8160825", "8164531", "8171146", "8172173", "8173943", "8175535", "8175649", "8179787", "8180917", "8184015", "8184059", "8184311", "8185062", "8188855", "8199762", "8203501", "8212635", "8212722", "8213758", "8218929", "8222919", "8222977", "8225379", "8233905", "8237617", "8238824", "8238840", "8242358", "8243603", "8249028", "8251307", "8253516", "8255952", "8258743", "8259028", "8264417", "8269583", "8284102", "8287323", "8295301", "8300538", "8300640", "8316228", "8316364", "8324990", "8325034", "8325636", "8325693", "8330259", "8335596", "8343145", "8344829", "8354970", "8359124", "8362775", "8363313", "8369667", "8373095", "8373597", "8374821", "8384600", "8385978", "8386198", "8390402", "8405567", "8406239", "8406593", "8407687", "8412130", "8414326", "8415884", "8428033", "8433168", "8434103", "8437383", "8452555", "8457027", "8458453", "8462063", "8467363", "8468244", "8471513", "8472327", "8484137", "8484511", "8495718", "8497749", "8503845", "8504135", "8505057", "8509114", "8514980", "8515383", "8516129", "8516470", "8516474", "8519892", "8520578", "8520636", "8528059", "8532023", "8532046", "8532492", "8536857", "8537068", "8537705", "8538428", "8539540", "8539569", "8542968", "8545322", "8548294", "8553646", "8561104", "8561181", "8565568", "8566058", "8572247", "8572639", "8572661", "8578076", "8578486", "8582502", "8584195", "8587490", "8587492", "8588567", "8588840", "8588991", "8593238", "8594956", "8595141", "8599150", "8600602", "8604982", "8604999", "8605361", "8605579", "8612550", "8613020", "8615190", "8625547", "8629811", "8639260", "8639390", "8639934", "8644219", "8653906", "8655396", "8656458", "8660526", "8660698", "8665102", "8666553", "8670946", "8674630", "8676186", "8680450", "8680706", "8681463", "8686911", "8687650", "8688153", "8699454", "8699461", "8705925", "8706026", "8707432", "8711538", "8711732", "8711806", "8711857", "8712200", "8719938", "8723730", "8724102", "8729857", "8731358", "8732476", "8736502", "8737793", "8738318", "8743004", "8749449", "8750097", "8750664", "8754852", "8755659", "8760354", "8761792", "8763097", "8766657", "8767071", "8769622", "8773312", "8780012", "8782195", "8786284", "8786514", "8789091", "8792760", "8792933", "8793363", "8793742", "8797207", "8804667", "8806202", "8810404", "8810421", "8810468", "8811278", "8811912", "8812050", "8812154", "8817741", "8824380", "8825239", "8830112", "8831506", "8836503", "8836607", "8839350", "8847840", "8847846", "8856239", "8856530", "8863245", "8866691", "8866695", "8867226", "8872032", "8875224", "8878740", "8880765", "8881588", "8885689", "8886229", "8887212", "8890759", "8893246", "8897215", "8897499", "8897695", "8897697", "8901916", "8903214", "8907222", "8907845", "8908502", "8908573", "8913862", "8917210", "8917215", "8917964", "8918108", "8918135", "8922447", "8925079", "8929841", "8934747", "8937577", "8938144", "8938255", "8941912", "8947258", "8948235", "8948690", "8952678", "8955051", "8955075", "8957818", "8957821", "8958356", "8958665", "8958812", "8963424", "8963790", "8964433", "8966609", "8968287", "8970438", "8984113", "8989788", "8994473", "8994474", "8996188", "8996728", "9000353", "9001689", "9001717", "9003492", "9008208", "9008513", "9009460", "9013361", "9014621", "9015467", "9019164", "9019595", "9019846", "9019892", "9020555", "9021251", "9021575", "45514", "9024831", "9037516", "9042245", "9042812", "9065172", "9065177", "9066224", "9070962", "9070964", "9079349", "9082307", "9083083", "9083581", "9092962", "9092963", "9094407", "9098325", "9099787", "9103864", "9105981", "9106617", "9112281", "9113347", "9119127", "9128941", "9130641", "9134945", "9137485", "9142334", "9143084", "9143196", "9154641", "9158418", "9158427", "9167535", "9171458", "9173217", "9194930", "9219594", "9201556", "9202371", "9203149", "9204418", "9207168", "9209902", "9210192", "9210586", "9219307", "9225396", "9240835", "9244117", "9246231", "9246334", "9253588", "9260244", "9264204", "9265078", "9270013", "9271185", "9276303", "9276304", "9281564", "9282144", "9285461", "9287605", "9288844", "9289177", "9293798", "9293801", "9302770", "9306682", "9312919", "9312929", "9315663", "9319311", "9324020", "9325067", "9325516", "9326316", "9334052", "9338823", "9346560", "9350063", "9351182", "9356358", "9362629", "9363333", "9363690", "9363761", "9366743", "9368275", "9369177", "9379527", "9379556", "9380857", "9391874", "9393683", "9394716", "9397380", "9401863", "9413519", "9414126", "9417731", "9419712", "9421869", "9422139", "9432478", "9432865", "9439092", "9443417", "9458974", "9459746", "9461706", "9465397", "9467219", "9467870", "9476932", "9478865", "9479241", "9479535", "9490869", "9490913", "9495037", "9496921", "9497572", "9503170", "9503189", "9509415", "9510203", "9520945", "9525524", "9544006", "9564947", "9577306", "9608692", "9608740", "9615269", "9627768", "9628116", "9640850", "9653770", "9680670", "9692101", "9705561", "9705571", "9742462", "9748626", "9749053", "9722318", "9768833", "9769020", "9780834", "9793951", "9793954", "9847566", "9853342", "9860075", "9865911", "9866309", "9871282", "9871283", "9876264", "9876570", "9876605", "9882257", "9893795", "9912381", "9917341", "9991580", "9997819", "9998172", "9998870", "9999038", "10003364", "10009063", "10009065", "10009901", "10027397", "10027427", "10033107", "10033108", "10044409", "10051483", "10051488", "10062970", "10069535", "10079661", "10090606", "10096883", "10103777", "10103801", "10123217", "10129057", "10135145", "10136434", "10142086", "10148016", "10154493", "10170840", "10171158", "20010030789", "20020002040", "20020008672", "20020011960", "20020021716", "20020024424", "20020027481", "20020040439", "20020057223", "20020061217", "20020069417", "20020083194", "20020091807", "20020099949", "20020101852", "20020111997", "20020156917", "20020186694", "20020197979", "20030002125", "20030002476", "20030010528", "20030022694", "20030038753", "20030049003", "20030054793", "20030054811", "20030061346", "20030095208", "20030137464", "20030152331", "20030164794", "20030188308", "20030190110", "20030202756", "20030210197", "20040015725", "20040023640", "20040024913", "20040048596", "20040054425", "20040084582", "20040085153", "20040090312", "20040091032", "20040100343", "20040104410", "20040113756", "20040113757", "20040119564", "20040131310", "20040160382", "20040163135", "20040165669", "20040169572", "20040196784", "20040198228", "20040212481", "20040213147", "20040213189", "20040213294", "20040242185", "20040250069", "20050002408", "20050005854", "20050017825", "20050031267", "20050042989", "20050063422", "20050068223", "20050069321", "20050074208", "20050097396", "20050102185", "20050111533", "20050143868", "20050151659", "20050159187", "20050164666", "20050168326", "20050169056", "20050169401", "20050177463", "20050190101", "20050208949", "20050212626", "20050219126", "20050219135", "20050220180", "20050226353", "20050249245", "20050258920", "20060034724", "20060038660", "20060053486", "20060082516", "20060085813", "20060094439", "20060106741", "20060111047", "20060113425", "20060114925", "20060119528", "20060120399", "20060128322", "20060132380", "20060153878", "20060172781", "20060176124", "20060181394", "20060187023", "20060192672", "20060220833", "20060232493", "20060238347", "20060239501", "20060244672", "20060249622", "20060255930", "20060286927", "20070002771", "20070022475", "20070025265", "20070025386", "20070040628", "20070041464", "20070041554", "20070054622", "20070063914", "20070090185", "20070105508", "20070135044", "20070144779", "20070189182", "20070201540", "20070211689", "20070211786", "20070216596", "20070223381", "20070226779", "20070252998", "20070257858", "20070258484", "20070268124", "20070268846", "20070300280", "20080002652", "20080003872", "20080007416", "20080008116", "20080043655", "20080055149", "20080060832", "20080064331", "20080077336", "20080080389", "20080084937", "20080094298", "20080120667", "20080122723", "20080130639", "20080143491", "20080150790", "20080153416", "20080177678", "20080191851", "20080211727", "20080247716", "20080252522", "20080252541", "20080253723", "20080255782", "20080258993", "20080266060", "20080267076", "20080279199", "20080280574", "20080313691", "20090002137", "20090007189", "20090007190", "20090007194", "20090009408", "20090015239", "20090054056", "20090054737", "20090061940", "20090067441", "20090079660", "20090085726", "20090088907", "20090093267", "20090109981", "20090129301", "20090138931", "20090140852", "20090144417", "20090171780", "20090175195", "20090181664", "20090201133", "20090202020", "20090210901", "20090250449", "20090254971", "20090258652", "20090284435", "20090286482", "20090311960", "20090315668", "20090320058", "20090325479", "20090325628", "20100002618", "20100002731", "20100013696", "20100026607", "20100039339", "20100045447", "20100052799", "20100053019", "20100057894", "20100080203", "20100085036", "20100090887", "20100100918", "20100111521", "20100119234", "20100121945", "20100127848", "20100142435", "20100150215", "20100153990", "20100169937", "20100175080", "20100176894", "20100177894", "20100185614", "20100201313", "20100214183", "20100214185", "20100220024", "20100224732", "20100225426", "20100232539", "20100243633", "20100253450", "20100256955", "20100265877", "20100266063", "20100277003", "20100283693", "20100284446", "20100319068", "20100327880", "20110018704", "20110040861", "20110042120", "20110043051", "20110053498", "20110068893", "20110068988", "20110083399", "20110103274", "20110107364", "20110109936", "20110110404", "20110118888", "20110132658", "20110133865", "20110133867", "20110136432", "20110140911", "20110141555", "20110143673", "20110148578", "20110148687", "20110164514", "20110165847", "20110169336", "20110172000", "20110173447", "20110187578", "20110199265", "20110208450", "20110214176", "20110219402", "20110220394", "20110225046", "20110228814", "20110268085", "20110274396", "20110286506", "20110291878", "20110294509", "20110311231", "20110316645", "20120002973", "20120015382", "20120015654", "20120019420", "20120019427", "20120038520", "20120039366", "20120046891", "20120054571", "20120068903", "20120077485", "20120078452", "20120084807", "20120091820", "20120092161", "20120093078", "20120102568", "20120105246", "20120105637", "20120109545", "20120109566", "20120117584", "20120129566", "20120133373", "20120137332", "20120144420", "20120153087", "20120154239", "20120161543", "20120181258", "20120190386", "20120197558", "20120201145", "20120214538", "20120224807", "20120226394", "20120235864", "20120235881", "20120250534", "20120250752", "20120263152", "20120268340", "20120272741", "20120274528", "20120287922", "20120299671", "20120304294", "20120306587", "20120306708", "20120313895", "20120319903", "20120322380", "20120322492", "20120324018", "20120327908", "20120329523", "20120330756", "20130002409", "20130003876", "20130010679", "20130015922", "20130016022", "20130023302", "20130039624", "20130064178", "20130064311", "20130070621", "20130077664", "20130080290", "20130086639", "20130093638", "20130095875", "20130108206", "20130109317", "20130117852", "20130122828", "20130124365", "20130127678", "20130136410", "20130144750", "20130148194", "20130159153", "20130159856", "20130160122", "20130162490", "20130166690", "20130169499", "20130173807", "20130178998", "20130182804", "20130185552", "20130187636", "20130191052", "20130201006", "20130201904", "20130207859", "20130219308", "20130234904", "20130234961", "20130235845", "20130235871", "20130262656", "20130262857", "20130263263", "20130265732", "20130268414", "20130271349", "20130278464", "20130279523", "20130279561", "20130279868", "20130285864", "20130305369", "20130306351", "20130307645", "20130311661", "20130314182", "20130321225", "20130326063", "20130326494", "20130330050", "20130335165", "20130336418", "20130341094", "20130342287", "20130343213", "20130343351", "20140003394", "20140003775", "20140007076", "20140009270", "20140009822", "20140015705", "20140019576", "20140026170", "20140028184", "20140028190", "20140028532", "20140032005", "20140036694", "20140041925", "20140043189", "20140043977", "20140044139", "20140052810", "20140057576", "20140071818", "20140072299", "20140077995", "20140086080", "20140086152", "20140112184", "20140124236", "20140126914", "20140130111", "20140132728", "20140139375", "20140143055", "20140146902", "20140148107", "20140155054", "20140165145", "20140169186", "20140177692", "20140179302", "20140189677", "20140189732", "20140191913", "20140204000", "20140204754", "20140207844", "20140208272", "20140222997", "20140223527", "20140225129", "20140227905", "20140227966", "20140233900", "20140241718", "20140254516", "20140254896", "20140254979", "20140266946", "20140266953", "20140269260", "20140269972", "20140285277", "20140285293", "20140285373", "20140285389", "20140286189", "20140287702", "20140299349", "20140304498", "20140317229", "20140320364", "20140321273", "20140334773", "20140334789", "20140340271", "20140343883", "20140349696", "20140351571", "20140355525", "20140355989", "20140357269", "20140359275", "20140362374", "20140362694", "20140368301", "20140369430", "20140372068", "20140373053", "20140376655", "20150008996", "20150009089", "20150016260", "20150017473", "20150022399", "20150026460", "20150029065", "20150036610", "20150042526", "20150048238", "20150049998", "20150061859", "20150065166", "20150070231", "20150071594", "20150073594", "20150077740", "20150084660", "20150084703", "20150084814", "20150091650", "20150094104", "20150098387", "20150099555", "20150102972", "20150104005", "20150109178", "20150116154", "20150122886", "20150126107", "20150130675", "20150138022", "20150138144", "20150153248", "20150156266", "20150162988", "20150171522", "20150172036", "20150181449", "20150195349", "20150195719", "20150201228", "20150207527", "20150214615", "20150215268", "20150223078", "20150223113", "20150230109", "20150236778", "20150236779", "20150237519", "20150249965", "20150263424", "20150271830", "20150276577", "20150277569", "20150280328", "20150284079", "20150304045", "20150304869", "20150312774", "20150318610", "20150323948", "20150325913", "20150326274", "20150333804", "20150334769", "20150339912", "20150344136", "20150349415", "20150356482", "20150356848", "20150369660", "20150370251", "20150373557", "20150380814", "20160006129", "20160012460", "20160014749", "20160021545", "20160026301", "20160038074", "20160043478", "20160044705", "20160050028", "20160056543", "20160063642", "20160064794", "20160065252", "20160065335", "20160066191", "20160068265", "20160068277", "20160069934", "20160069935", "20160070265", "20160072191", "20160072287", "20160079769", "20160079771", "20160079809", "20160080035", "20160080839", "20160082460", "20160087344", "20160088498", "20160094420", "20160094879", "20160099749", "20160100324", "20160103199", "20160105218", "20160105233", "20160105239", "20160105255", "20160112092", "20160112093", "20160112094", "20160112115", "20160112132", "20160112133", "20160112135", "20160112263", "20160116914", "20160118717", "20160124071", "20160127931", "20160131347", "20160134006", "20160135132", "20160137311", "20160139731", "20160149312", "20160149614", "20160149636", "20160149665", "20160149731", "20160149753", "20160150427", "20160164571", "20160164573", "20160165472", "20160165478", "20160174040", "20160179134", "20160181701", "20160182096", "20160182161", "20160182981", "20160188291", "20160189101", "20160197392", "20160197409", "20160197630", "20160197642", "20160207627", "20160212065", "20160214717", "20160218407", "20160218437", "20160221039", "20160224235", "20160226681", "20160244165", "20160248149", "20160248165", "20160248509", "20160249233", "20160252970", "20160261309", "20160261310", "20160261311", "20160261312", "20160269156", "20160276725", "20160277939", "20160278094", "20160285508", "20160285512", "20160294444", "20160294517", "20160295431", "20160315659", "20160315660", "20160315661", "20160315662", "20160322691", "20160323015", "20160336092", "20160336636", "20160336996", "20160336997", "20160351987", "20160359523", "20160359524", "20160359529", "20160359530", "20160359541", "20160359542", "20160359543", "20160359544", "20160359546", "20160359547", "20160359649", "20160360511", "20160360533", "20160365175", "20160365966", "20170012667", "20170018332", "20170018851", "20170018856", "20170033465", "20170033466", "20170033953", "20170033954", "20170078064", "20170079024", "20170079037", "20170079038", "20170079039", "20170085003", "20170093693", "20170110795", "20170110804", "20170111805", "20170229782", "20180048497", "20180054232", "20180054233", "20180054234", "20180062886", "20180069594", "20180069731", "20180076982", "20180076988", "20180077709", "20180108997", "20180108998", "20180108999", "20180115040", "20180115044", "20180115058", "20180115060", "20180115075", "20180115081", "20180123207", "20180123208", "20180123643", "20180123836", "20180151957", "20180159195", "20180159196", "20180159197", "20180159228", "20180159229", "20180159230", "20180159232", "20180159235", "20180159238", "20180159240", "20180159243", "20180166761", "20180166784", "20180166785", "20180166787", "20180167130", "20180167927", "20180302162", "20190013577", "20190013837"], "related": []}, {"id": "20180121029", "patent_code": "10373080", "patent_name": "Distributing a user interface for accessing files", "year": "2019", "inventor_and_country_data": " Inventors: \nMrad; Ramzi (Brussels, BE), Frad; Khaled (La Marsa, TN)  ", "description": "<BR><BR>TECHNICAL FIELD\nThis document generally relates to various techniques for distributing a user interface for accessing files.\n<BR><BR>BACKGROUND\nUsers of computing devices may access information in various ways.  In some examples, an individual may use their computing device to view files that are already stored on the computing device due to the individual having previously transferred\nthe files to that computing device.  In some examples, the computing device may be connected to the internet, and the individual may use the computing device to browse web pages and select files to view over the internet.\n<BR><BR>SUMMARY\nThis document describes techniques, methods, systems, and other mechanisms for distributing a user interface for accessing files.\nAs additional description to the embodiment described below, the present disclosure describes the following embodiments.\nDefining and Distributing a Navigation User Interface.\nEmbodiment 1 is directed to a computer-implemented method.  The method comprises receiving, by a computing system, an indication that a first computing device received user input that defined a navigation user interface that is to be presented\nby multiple client devices, with the user input that defined the navigation user interface having specified that the navigation user interface is to include a first selectable interface element and a second selectable interface element.  The method\ncomprises providing, by the computing system and for receipt by the first computing device, first information that is configured to cause the first computing device to present a representation of the navigation user interface that was defined at the\nfirst computing device.  The method comprises receiving, by the computing system, an indication that the first computing device received user input that specified a first collection of files to be accessible to users of the multiple client devices upon\nselection of the first selectable interface element from a presentation of the navigation user interface.  The method comprises receiving, by the computing system, an indication that the first computing device received user input that specified a second\ncollection of files to be accessible to users of the multiple client devices upon selection of the second selectable interface element the presentation of the navigation user interface.  The method comprises providing, by the computing system and for\nreceipt by the first computing device, data that identifies a plurality of individuals that are candidates for display of the navigation user interface.  The method comprises receiving, by the computing system, an indication that the first computing\ndevice received user input that specified multiple different individuals, from among a presentation of the plurality of individuals, to which the navigation user interface is to be displayed, and in response identifying devices that are assigned to\naccounts of the specified multiple individuals, wherein the identified devices include the multiple client devices, wherein the specified multiple different individuals include a first individual that has an account to which a first client device is\nassigned and a second individual that has an account to which a second client device is assigned.  The method comprises providing, by the computing system and for receipt by each of the multiple client devices, second information that is configured to\ncause each of the multiple client devices to each: (i) present the navigation user interface that was defined by the user input at the first computing device, including presentation of the first interface element and the second interface element, (ii)\nprovide user access to the first collection of files in response to user input at the respective client device that selects the first selectable interface element, and (iii) provide user access to the second collection of files in response to user input\nat the respective client device that selects the second selectable interface element.\nEmbodiment 2 is the computer-implemented method of embodiment 1.  The method further comprises receiving, by the computing system, an indication that the first computing device received user input that selected the first selectable interface\nelement.  The method further comprises providing, by the computing system and for receipt by the first computing device, information that is configured to cause the first computing device to present, as a result of the first computing device receiving\nuser input that selected the first selectable interface element, an interface through which the first computing device received the user input that specified the first collection of files.  The method further comprises receiving, by the computing system,\nan indication that the first computing device received user input that selected the second selectable interface element.  The method further comprises providing, by the computing system and for receipt by the first computing device, information that is\nconfigured to cause the first computing device to present, as a result of the first computing device receiving user input that selected the second selectable interface element, an interface through which the first computing device received the user input\nthat specified the second collection of files.\nEmbodiment 3 is the computer-implemented method of embodiment 1, wherein receiving the indication that the first computing device received user input that defined the navigation user interface includes receiving an indication that the first\ncomputing device received user input that specified a first size and shape for the first interface element.  It also includes receiving an indication that the first computing device received user input that specified a second size and shape for the\nsecond interface element, wherein the first size and shape for the first interface element differs from the second size and shape for the second interface element.\nEmbodiment 4 is the computer-implemented method of embodiment 3 wherein the user input that specified the first size and shape for the first selectable interface element includes user input that interacted with a display of the first selectable\ninterface element to resize the first selectable interface element; and the user input that specified the second size and shape for the second selectable interface element includes user input that interacted with a display of the second selectable\ninterface element to resize the second selectable interface element.\nEmbodiment 5 is the computer-implemented method of embodiment 1, wherein receiving the indication that the first computing device received user input that defined the navigation user interface includes: receiving an indication that the first\ncomputing device received user input that specified a first user-specified title and a first user-specified color for the first selectable interface element; and receiving an indication that the first computing device received user input that specified a\nsecond user-specified title and a second user-specified color for the second selectable interface element.  The first user-specified title differs from the second user-specified title.  The first user-specified color differs from the second\nuser-specified color.\nEmbodiment 6 is the computer-implemented method of embodiment 1.  The method further comprises providing, by the computing system and for receipt by the first computing device, information that is configured to cause the first computing device\nto present a collection of graphical elements that respectively represent different navigation user interfaces that are to be presented on various groups selected from plurality of client devices, including the navigation user interface.  The method\nfurther comprises receiving, by the computing system, an indication that user input at the first computing device selected a particular graphical element from among the collection of graphical elements, wherein the particular graphical element is\nassigned to the navigation user interface.  The method further comprises providing, by the computing system and for receipt by the first computing device, information that is configured to cause the first computing device to present, in response to the\nfirst computing device receiving the user input that selected the particular graphical element, accessibility to various interfaces displays, including the navigation user interface, with which user input is able to (i) add and remove selectable\ninterface elements to the navigation user interface, (ii) add files to and remove files from the first collection of files, (iii) add files to and remove files from the second collection of files, and (iv) add individuals to and remove individuals from\nthe specified multiple different individuals to which the navigation user interface is to be displayed.\nEmbodiment 7 is the computer-implemented method of embodiment 6.  The method further comprises receiving, by the computing system, an indication that the first computing device received (i) user input to remove the second selectable interface\nelement from the navigation user interface and (ii) user input to add a third selectable interface element to the navigation user interface.  The method further comprises providing, by the computing system and for receipt by each of the multiple client\ndevices after the second information has already been provided for receipt by each of the multiple client devices, third information that is configured to cause each of the multiple client devices to present the navigation user interface with the first\nselectable interface element and a third selectable interface elements included in the navigation user interface, and the second selectable interface element excluded from the navigation user interface.\nEmbodiment 8 is the computer-implemented method of embodiment 1.  The method further comprises receiving, by the computing system, an indication that the first computing device received (i) user input to remove a second individual as an\nindividual to which the navigation user interface is to be displayed, and (ii) user input to add a third individual as an individual to which the navigation user interface is to be displayed, wherein a second client device of the multiple client devices\nis assigned to an account of the second individual, wherein a third client device is assigned to an account of the third individual.  The method further comprises providing, by the computing system and for receipt by the second client device, information\nthat is configured to cause the second client device to remove access to the navigation user interface.  The method further comprises providing, by the computing system and for receipt by the third client device, information that is configured to cause\nthe third client device present the user interface.\nEmbodiment 9 is the computer-implemented method of embodiment 1.  The method comprises receiving, by the computing system before the second information is provided for receipt by each of the multiple client devices, an indication that user input\nat the first computing device specified that the navigation user interface is to be accessible at the first client device at a first date.  The method comprises receiving, by the computing system before the second information is provided for receipt by\neach of the multiple client devices, an indication that user input at the first computing device specified that the navigation user interface is to be accessible at the second client device at a second date, wherein the first date is different from the\nsecond date, wherein providing the first information for receipt by each of the multiple client devices includes the computing system providing the first information for receipt by the first client device at the first date and providing the first\ninformation for receipt by the second client device at the second date.\nEmbodiment 10 is the computer-implemented method of embodiment 1.  The method further comprises receiving, by the computing system, an indication that the first client device presented a first file from among the first collection of files in\nresponse to user input at the first client device requesting to access the first file.  The method further comprises receiving, by the computing system, an indication that the second client device presented the first file from among the first collection\nof files in response to user input at the second client device requesting to access the first file.  The method further comprises receiving, by the computing system, an indication that the first computing device received user input requesting to view a\nlevel of accesses of the first file by the multiple client devices.  The method further comprises providing, by the computing system and for receipt by the first computing device, information that is configured to cause the first computing device to\npresent an indication that the first client device accessed the first file and that the second client device accessed the first file.\nEmbodiment 11 is the computer-implemented method of embodiment 1.  The method further comprises receiving, by the computing system, an indication that the first client device presented a first screen of a first file in response to user input at\nthe first client device requesting to navigate through screens of the first file.  The method further comprises receiving, by the computing system, an indication that the second client device presented the first screen of the first file and a second\nscreen of the first file in response to user input at the second client device requesting to navigate through screens of the first file.  The method further comprises receiving, by the computing system, an indication that the first computing device\nreceived user input requesting to view a level of accesses of the screens of the first file by the multiple client devices.  The method further comprises providing, by the computing system and for receipt by the first computing device, information that\nis configured to cause the first computing device to present (i) an indication that the first client device and the second client device presented the first screen of the first file, and (ii) an indication that the first client device presented the\nsecond screen of the first file to the exclusion of the second device presenting the second screen of the first file.\nEmbodiment 12 is the computer-implemented method of embodiment 1.  The method further comprises receiving, by the computing system, a first screenshot that the first client device captured in response to user input at the first client device to\ncapture a screenshot.  The method further comprises receiving, by the computing system, a second screenshot that the first client device captured in response to user input at the first client device to capture a screenshot.  The method further comprises\nreceiving, by the computing system, an indication that the first computing device received user input requesting to view screenshots that the first client device captured in response to user input requests at the first client device to capture\nscreenshots.  The method further comprises providing, by the computing system and for receipt by the first computing device, information that is configured to cause the first computing device to present the first screenshot and the second screenshot that\nwere captured at the first client device.\nEmbodiment 13 is directed to one or more computer-readable devices including instructions that, when executed by one or more processors, cause performance of the method of any of embodiments 1-12.\nCreating a Presentation-Creation Product.\nEmbodiment 1 is a computer-implemented method that comprises receiving, by a computing system, an indication that a first computing device received user input to specify a name for a first presentation-creation product.  The method comprises\nreceiving, by the computing system, an indication that the first computing device received user input that adds multiple assets to be available for use in the first presentation-creation product, the multiple assets including a first image and a second\nimage.  The method comprises providing, by the computing system and for receipt by the first computing device, information that is configured to cause the first computing device to visually specify the multiple assets that were added for use in the first\npresentation-creation product.  The method comprises receiving, by the computing system, an indication that the first computing device received user input creating multiple slides for the first presentation-creation product, wherein the user input\nspecified that at least one of the multiple assets to include as part of each of the multiple slides.  The method comprises providing, by the computing system and for receipt by the first computing device, information that is configured to cause the\nfirst computing device to present a slide-selection screen that includes a graphical representation of the multiple slides.  The method comprises receiving, by the computing system, an indication that the first computing device received user input that\ninteracted with the slide-selection screen to select a first group of slides to comprise a first presentation.  The method comprises receiving, by the computing system, an indication that the first computing device received user input that interacted\nwith the slide-selection screen to select a second group of slides to comprise a second presentation.  The method comprises providing, by the computing system and for receipt by a first client device, information that is configured to cause the first\nclient device to: (i) present a selectable interface element that identifies the first presentation-creation product and that has the name that was specified by user input at the first computing device, (ii) in response to selection of the selectable\ninterface element that identifies the first presentation-creation product, concurrently present a first selectable interface element that is associated with the first presentation and a second selectable interface element that is associated with the\nsecond presentation, (iii) in response to selection of the first selectable interface element that is associated with the first presentation, present an initial slide of the first presentation; and (iv) in response to selection of the second selectable\ninterface element that is associated with the second presentation, present an initial slide of the second presentation.\nEmbodiment 2 is the computer-implemented method of embodiment 1.  The method comprises receiving, by the computing system, an indication that the first computing device received user input that assigned a first portion of the multiple slides to\na first section of slides, and user input that assigned a second portion of the multiple slides to a second section of slides, wherein the slide-selection screen that is provided by the first computing device visually presents the first section of slides\nand the second section of slides as being distinct from another.  The information that the computing system provides for receipt by the first client device is configured to cause the first client device to present a menu that includes a selectable option\nthat identifies the first section of slides and a selectable option that identifies the second section of slides, and in response to user selection of the first option, cause the first client device to filter out presentation of slides other than those\nin the first section of slides.\nEmbodiment 3 is the computer-implemented method of embodiment 1.  The method comprises receiving, by the computing system, an indication that the first client device received user input to create a new presentation through use of the multiple\nslides created for the first presentation-creation product.  The method comprises providing, by the computing system and for receipt by the first client device, information that is configured to cause the first client device to present a\npresentation-creation interface that presents the multiple slides that were created for the first presentation-creation product at the first computing device.  The method comprises receiving, by the computing system, an indication that the first client\ndevice received user input that specified: (i) a group of the multiple slides, (ii) an order to slides in the group of multiple slides, and (iii) a presentation name for the group of the multiple slides having the specified order.\nEmbodiment 4 is the computer-implemented method of embodiment 3.  The method comprises receiving, by the computing system, an indication that the first computing device received user input that identified a subset of the multiple slides as being\nmandatory slides.  The presentation-creation interface that is presented at the first client device presents the multiple slides with the subset of the multiple slides as being pre-selected.\nEmbodiment 5 is the computer-implemented method of embodiment 1.  The method comprises receiving, by the computing system, an indication that the first computing device received user input that adds, as one of the multiple assets that are to be\navailable for use in the first presentation-creation product, a reference asset, including user specification of a description for the reference asset and user specification of another asset of the multiple assets to which the reference asset is to link\nwhen selected with user input.  The method comprises receiving, by the computing system, an indication that the first computing device received user input to create a first slide, of the multiple slides, with the user input specifying that the first\nslide is to include the reference asset.  The information that the computing system provides for receipt by the first client device is configured to cause the first client device to present the reference asset as a result of user input selecting an\ninterface element to toggle presentation of the reference asset while the first slide is being presented by the first client device.\nEmbodiment 6 is the computer-implemented method of embodiment 1.  The method comprises receiving, by the computing system, an indication that the first client device received user input that adds, as one of the multiple assets that are to be\navailable for use in the first presentation-creation product, a HyperText Markup Language (HTML) asset that specifies an HTML file.  The method comprises receiving, by the computing system, an indication that the first computing device received user\ninput to create a first slide, of the multiple slides, including user specification that the HTML asset is to be included as part of the first slide.  The information that the computing system provides for receipt by the first client device is configured\nto cause the first client device to (i) present a web page generated from the HTML file as a background of the first slide, and (ii) modify the background of the first slide in response to user interaction with a user interface element that is defined by\nthe HTML file.\nEmbodiment 7 is the computer-implemented method of embodiment 1.  The method comprises receiving, by the computing system, an indication that the first computing device received user input that interacted with the slide-selection screen to\nselect a secondary slide for a particular one of the slides in the first group of slides.  The information that the computing system provides for receipt by the first client device is configured to cause the first client device to present the secondary\nslide overlaid the particular one of the slides in the first group of slides in response to user input at the first client device that selects an interface element for activating presentation of the secondary slide over the particular one of the slides,\nwhile the particular one of the slides is being presented.\nEmbodiment 8 is the computer-implemented method of embodiment 1.  The first presentation includes the first slide, the second slide, and a third slide of the multiple slides.  The information that the computing system provides for receipt by the\nfirst client device is configured to cause the first client device to concurrently present a representation of the first slide, a representation of the second slide, and a representation of the third slide as the first client device presents the first\nslide of the first presentation.\nEmbodiment 9 is the computer-implemented method of embodiment 1.  The first group of slides to comprise the first presentation includes a first slide of the multiple slides and a second slide of the multiple slides, but does not include a third\nslide of the multiple slides.  The second group of slides to comprise the second presentation does not include the third slide, but includes the first slide and the second slide.\nEmbodiment 10 is one or more computer-readable devices including instructions that, when executed by one or more processors, causes performance of the method of any of embodiments 1-9.\nDistributing a Document Subject to an Approval Process.\nEmbodiment 1 is directed to a computer-implemented method.  The method comprises receiving, by a computing system, an indication that a first computing device received user input that selected a first document, from among a group of multiple\ndocuments that are subject to an approval process by an approval computing system, as a document to add to a collection of assets, the assets in the collection of assets being available for presentation on client devices that are remote from the\ncomputing system and the approval computing system.  The method comprises providing, by the computing system, information that is configured to cause the first computing device to identify assets in the collection of assets, including an indication that\nthe first document is included in the collection of assets.  The method comprises providing, by the computing system and for receipt by the first computing device, information that is configured to cause the first computing device to present a status of\nmultiple documents that are subject to the approval process, including: (i) an indication that the first document is currently unapproved, and (ii) an indication that a second document is current approved.  The method comprises receiving, by the\ncomputing system, an indication that the first computing device received user input that specified multiple users to which the computing system is to distribute the first document upon the approval computing system indicating that the first document has\nbeen approved, wherein the multiple users includes a first user and a second user.  The method comprises receiving, by the computing system and from the approval computing system, an indication that the first document has been approved.  The method\ncomprises providing, by the computing system and for receipt by a first client device that is assigned to an account of the first user, in response to the computing system receiving the indication that the first document has been approved, the first\ndocument.  The method comprises providing, by the computing system and for receipt by a second client device that is assigned to an account of the second user, in response to the computing system receiving the indication that the second document has been\napproved, the second document.\nEmbodiment 2 is directed to the computer-implemented method of embodiment 1.  The method comprises providing, by the computing system and for receipt by the first computing device in response to the computing system receiving from the approval\ncomputing system the indication that the first document has been approved, information that is configured to cause the first computing device to update the status of the multiple documents that are subject to the approval process to include an indication\nthe first document is currently approved.\nEmbodiment 3 is directed to the computer-implemented method of claim 1.  The method comprises receiving, by the computing system, an indication that user input at the first computing device specified that the first document is to be accessible\nthrough a page of a user interface.  The method comprises providing, by the computing system and for receipt by the first client device, information that is configured to cause the first client device to present the user interface through which the first\ndocument is to be accessible.\nEmbodiment 4 is directed to the computer-implemented method of embodiment 1.  The information that the computing system provides and that is configured to cause the first computing device to identify the assets in the collection of assets\nincludes: (i) the indication that the first document is included in the collection of assets, including an indication that the first document is subject to the approval process; (ii) the indication that the second document is included in the collection\nof assets, including an indication that the second document is subject to the approval process; and (ii) an indication that a third document is included in the collection of assets, including an indication that the third document is not subject to the\napproval process.\nEmbodiment 5 is directed to the computer-implemented method of embodiment 1.  The computing system provided the first document for receipt by the first client device without receipt of user input that specified that the computing system was to\nprovide the first document for receipt by the first client device, after the computing system received the indication that the first document had been approved.\nEmbodiment 6 is directed to the computer-implemented method of embodiment 1.  The method comprises receiving, by the computing system and from the approval computing system after the first document has been approved and has provided for receipt\nby the first client device, an indication that a new version of the first document is available.  The method comprises providing, by the computing system and for receipt by the first client device in response to the computing system having received the\nindication that the new version of the first document is available, the new version of the first document.\nEmbodiment 7 is directed to the computer-implemented method of embodiment 6.  The method comprises receiving, by the computing system and from the approval computing system, an indication that the new version of the first document is no longer\napproved.  The method comprises providing, by the computing system and for receipt by the first client device in response to the computing system receiving the indication that the new version of the first document is no longer approved, information that\nis configured to cause the first client device to remove access to the new version of the first document.\nEmbodiment 8 is directed to the computer-implemented method of embodiment 7.  The method comprises receiving, by the computing system and from the first computing device, an indication that user input at the first computing device selected\nautomated deletion from client devices of documents that transition from being approved to being no longer approved.  The computing system provides the information that is configured to cause the first client device to remove access to the new version of\nthe first document in response to the computing system having received the indication that the user input at the first computing device selected automated deletion from client devices of documents that are no longer approved.  The computing system is\nconfigured to not cause the first client device to remove access to the new version of the first document in response to the computing system having received an indication that the user input at the first computing device did not select automated\ndeletion from client devices of documents that are no longer approved.\nThe details of one or more implementations are set forth in the accompanying drawings and the description below.  Other features, objects, and advantages will be apparent from the description and drawings, and from the claims.\n<BR><BR>DESCRIPTION OF DRAWINGS\nFIG. 1 shows a diagram of an example system for distributing a user interface for accessing files.\nFIG. 2 shows an introductory user interface of a system for designing and distributing navigation user interfaces.\nFIGS. 3-4B shows user interfaces with which a user is able to manage individuals that are candidates for receiving navigation user interfaces.\nFIG. 5 shows a user interface with which a user is able to create a library of assets that may be used to design navigation user interfaces and that may be accessed through use of navigation user interfaces.\nFIGS. 6-10 show user interfaces with which a user can modify navigation user interfaces or create a new navigation user interfaces.\nFIGS. 11-14 show navigation user interfaces as they are presented on a client device.\nFIGS. 15A-B show a flowchart of a process for defining and providing a navigation user interface.\nFIGS. 16-18 show user interfaces that display statistical information identifying interaction by users of client devices with certain assets or pages of assets.\nFIG. 19 shows a page at which a user can identify documents that have to be validated.\nFIG. 20 shows a user interface with which a user at computing device is able to create a social network page.\nFIG. 21 shows how the social network user interface appears on the client devices.\nFIGS. 22-28 shows user interfaces from which to make presentation-creation products.\nFIGS. 29-32 shows how the presentation-creation products appear on client devices.\nFIGS. 33A-B show flowcharts of a process for creating and using a presentation-creation product.\nFIGS. 34A-C show a swim-lane diagram of a process for distributing a document subject to an approval process.\nFIG. 35 shows a user interface for displaying assets that are subject to approval.\nFIGS. 36-37 show a user interface for creating a survey.\nFIG. 38 shows how the survey appears on client devices.\nFIG. 39 shows how the results of the survey are presented after the survey questions are distributed to the client devices and users take the surveys.\nFIG. 40 is a conceptual diagram of a system that may be used to implement the systems and methods described in this document.\nFIG. 41 is a block diagram of computing devices that may be used to implement the systems and methods described in this document, as either a client or as a server or plurality of servers.\nLike reference symbols in the various drawings indicate like elements.\n<BR><BR>DETAILED DESCRIPTION\nThis document generally describes various techniques for distributing a user interface for accessing files.  In various examples, an individual may use the system described throughout this disclosure to design a user interface that will be\nprovided for display by multiple client devices and with which users of the client devices may access files that were specified by the individual.  This disclosure refers to such a user interface as a \"navigation user interface\" for reference purposes. \nInformation for generating the navigation user interface, along with the files that can be accessed using the navigation user interface, may be transferred from a server system to the client devices while the client devices are online, but may remain\naccessible when the client devices are offline.  As such, users of the client devices may be able to interact with the navigation user interface to access the multiple files even when the client devices are without internet access.\nAn individual may design multiple such navigation user interfaces through use of a web site (sometimes referred to as a web application) that is hosted by a computing system that may be remote from a computing device at which the individual\naccesses the web site.  The individual can specify recipients to which the navigation user interfaces are to be distributed.  When client devices that are assigned to those recipients come online, the computing system may transmit information for\ngenerating those user interfaces to the client devices.  Some of the client devices may be able to present multiple such navigation user interfaces.\nSuch a system may be particularly helpful in situations in which an organization may desire to control the information that is accessible to users of the client devices and monitor use of that information.  An example situation is with\npharmaceutical companies and their sales representatives.  Laws and regulations may prohibit the type of documents that sales representatives are able to show to medical professionals, such as doctors.  Pharmaceutical companies may want to not only\ncontrol which documents are accessible on electronic devices carried by their representatives, but may wish to track usage of these documents.\nIn an example scenario presented in this disclosure, a pharmaceutical company may distribute tablet client devices to sales representatives, and those representatives may use the tablets to review information on various medications and present\nthat information to medical professionals.  Because the information on the tablets is being shown to third parties, the company desires to not only control the files that are accessible through use of the client device, but also the user interface\nthrough which company representatives are able to access those files.  In fact, each tablet may be able to present multiple such user interfaces, for example, each being for a different medication.  The employee at the pharmaceutical company may be able\nto designate which sales representatives are to receive which of multiple user interfaces, and times at which the user interfaces are to be distributed.\nAs discussed in greater detail below, one of the files that may be accessible through a navigation user interface may be a presentation that was designed by an employee of the pharmaceutical company through use of the web site.  In other words,\nthe web site may enable a company employee to quickly create presentations through a multi-step process: developing a list of assets (e.g., images and videos) from which to create slides, creating slides using the assets, assigning the slides to various\nsections, and then creating one or more presentations.  The one or more presentations that are generated from a particular collection of slides may be transmitted to a client device as a group, and that client device may then permit a user to view the\none or more presentations.\nAs also discussed in greater detail below, the navigation user interface may provide access to documents that require approval by a remote computing system.  For example, some documents may be involved in a vetting process by which a group of\nindividuals (e.g., people in a different department at an organization) review and update a document to ensure that it is accurate and does not violate certain laws or regulations.  Once the document is approved, the computing system that distributes the\nnavigation user interface may receive an indication that the document has been approved.  At that time, the computing system may automatically send the approved document to the client devices that are hosting the navigation user interface.  In response,\nthe client devices may update their navigation user interface to permit access to the newly-approved document (a link to the document may not previously have appeared in the navigation user interface, or that may have been non-selectable, may become\nactive).  The computing system may also send out updated versions of documents as those updated versions are approved, and can cause client devices to remove access documents that have been unapproved.\nThese features, and others are described in additional detail with reference to the figures.\nFIG. 1 shows a diagram of an example system for distributing a user interface for accessing files.  The system includes a computing device 110 at which an individual can design various navigation user interfaces and assign assets to those user\ninterfaces.  The navigation user interfaces may be designed through use of a web site that is provided by computing system 120, which communicates with the computing device 110 over a communication network 150, such as the internet.  Information that\nspecifies the design of the user interfaces may be stored in user interface datastore 146.\nThe computing system 120 may provide the navigation user interfaces for receipt by client devices 130a-c. User account datastore 144 may assign these client devices to user accounts.  The assets that computing system 120 may distribute to the\nclient devices 130a-c may be stored in or referenced by the assets datastore 142.  Usage statistics datastore 141 may store statistics that identify user interaction with client devices 130a-c to access certain assets and screens thereof.\nThe functioning of the system that is shown in FIG. 1 is discussed in greater detail with reference to other figures.  The diagram shown in FIG. 1 is intended to aid discussion of the mechanisms that are referred to in this disclosure, but the\ndisclosure is not meant to be limited to this description.\nFIG. 2 shows an introductory user interface of a system for designing and distributing navigation user interfaces.  The introductory user interface that is presented in FIG. 2 may be provided by a web browser at computing device 110.  A user of\nthat computing device may have launched a web browser application program and entered a URL for the web page that is shown in FIG. 2, or selected a bookmark that is directed to the URL.  In response, the computing device 110 may send a request over the\ninternet to a remote server system (e.g., computing system 120) for resources that correspond to the URL.  The computing device 110 may receive responsive resources (e.g., an HTML file, script code, images), from which a web browser application program\nat the computing device 110 may render the web page that is presented in FIG. 2.\nThe web page that is shown in FIG. 2 may be an introductory webpage to a website, and that introductory webpage may be presented to a user that has logged onto the website (e.g., using a login name and credentials for a user account).  The web\npage includes multiple icons 210a-l that a user can select to cause the web browser to navigate to a display of a user interface for the respective icon.  For example, a user may tap a portion of a touchscreen at which the icon is displayed, or click a\nmouse over a display of the icon, to cause the computing device 110 to send a request for resources corresponding to a different web page, to receive the responsive resources, and then to generate a display of a web page from the responsive resources. \nIn some implementations, all or some user interface changes at computing device 110 may involve sending resource requests to computing system 120 and receiving resources responsive thereto.  In this illustration, the user selects the \"Users & Teams\" icon\n210e to cause the web browser to navigate to the user interface that is presented in FIG. 3.\nFIG. 3 shows a user interface with which a user is able to manage individuals that are candidates for receiving navigation user interfaces.  In this example, sidebar 310 includes user interface elements 312a-j that correspond to the icons 210a-l\nthat were presented in FIG. 2.  Here, the \"My Reps\" user interface element 312a is highlighted to indicate that the user is currently at the representatives webpage.  The sidebar 310 may appear in many of the pages of the website, to aid user navigation.\nThe right side of the web page includes a list of representatives portion 320.  This portion indicates all users (e.g., user accounts) that may be modifiable by an account of the user that logged in (here Eric Dupont).  In various\nimplementations, the user that logged in may be a manager or marketer at a pharmaceutical company, and the individuals that are shown at the right portion of the screen may be sales representatives that work for the company.\nA user of the webpage may form groups of representatives.  For example, this webpage shows a \"Genkor Sales Team\" group 330a and a \"Delegue Product A\" group 330b.  Each displayed group may include icons for the representatives that are members of\nthat group.  Representatives may belong to multiple groups, and a representative that is shown in a group may still appear in the list of representatives 320.  A user of the webpage may add a representative to a group of individuals, for example, by\nselecting an icon for a representative in the list of representatives 320 and dragging and dropping that icon in a region presented for an existing group.\nA user may add a new group by selecting the \"Add new group of reps\" selectable interface element 340.  In response, the system may generate a pop-up box that includes a field with which a user can specify a name for a new group.  In response,\nthe computing system may generate a label for the new group in the web page of FIG. 3, and enable dragging and dropping new users to that group.\nA user may add a new representative to the list of representatives 320 by selecting the \"Add new rep\" user interface element 342.  In response, the computing device may present a pop up box that includes fields for entering a first name, a last\nname, an email address, a password, and an avatar/icon to visually represent the user.  The user can then select a \"create\" interface element in the pop up box to add the new representative to the list of representatives 320.  The system can also import\nmultiple users through synchronization with a company directory (e.g., a synchronization with an address book).\nA user can select an interface element to cause navigation to the web page that is shown in FIGS. 4A-B. This web page enables a user to edit a representative's first name, last name, email address, password, and avatar/icon through interaction\nwith user interface elements 410 (e.g., editable text fields).  Under those elements are icons 412a-d with which a user can clear data for the representative (icon 412a), update a license for the representative (icon 412b), delete the representative\n(icon 412c), and deactivate the representative (icon 412d).\nThe top-right portion of the web page includes an \"Assigned Briefcases\" section 420 that lists the briefcases (referred to herein as navigation user interfaces) to which a user is assigned.  These briefcases will be discussed in additional\ndetail in other portions of this disclosure, but in this illustration it is apparent that representative Frank Dubois is assigned to three Briefcases: \"Genkor,\" \"Andrew's briefcase,\" and \"Ocrelizumab Lunch Campaign.\" The web page shows the version of the\nbriefcase and includes a pull-down menu with which the user can see which of Frank's devices are authorized to view the particular briefcase.  For each briefcase, there is also a region that displays a status icon that represents the status of each\nbriefcase for each device (once a device has been selected from the pull-down menu).  A gray element indicates that the device has never connected to the internet to retrieve the briefcase and is not synced to include the briefcase.  A Blue element\nindicates that the device has connected to the internet to retrieve the briefcase and has the latest documents downloaded.\nThe start date for each briefcase indicates the first time that the briefcase was synced to that user (or a particular device for that user).  The \"Remove\" user interface element allows a user to remove a representative's access to a particular\nbriefcase.\nFurther down the web page is a devices section 430.  This section lists all of the devices that are assigned to the representative and with which the representative can view briefcases that are assigned to him.  In some examples, only a single\ndevice may be assigned to the representative, although this example shows multiple devices.  For each device, the web page displays a name of the device, a version of the operating system, and whether that device is currently connected to the server\nsystems that provide updated files and briefcases.  The user of the webpage can select a toggle interface element to add and remove authorization for a particular device to access briefcases and content.\nFIG. 4B shows a bottom portion of the web page that was shown in FIG. 4A.  The bottom portion includes a list of screenshots 440, which shows screenshots that were captured on one or more client devices assigned to the representative.  For\nexample, the first time a client device may receive input to capture a screenshot, the client device may prompt the device user (e.g., the sales representative) whether the device is permitted to capture the screenshot and transmit it to the server\nsystem.  Should the representative authorize transmission, each screenshot captured by the system may be transmitted and stored at the server system (e.g., computing system 120), to be presented on the web page that is shown in FIG. 4B.  Should the\nrepresentative decline capture and transmission of the screenshot, the client device may transmit an indication that transmission was declined, and the web page that is shown in FIG. 4B may state \"User didn't authorize access to the screenshot\" for that\nparticular screenshot.\nFIG. 5 shows a user interface with which a user is able to create a library of assets that may be accessed through use of navigation user interfaces.  In this illustration, and as shown by the sidebar to the left of the web page, the user has\nclicked on a \"My Assets\" interface element.  As a result, the web browser has displayed a list of assets 502.  That list includes various assets (e.g., files such as documents) and folders that include assets therein.\nUsers can add files to the library by selecting a \"Select Files\" interface element 504.  In response, the computing system may display of a dialog box from which a user may be able to select one or more files from the user's computer or from a\nremote system (e.g., a system accessed via the internet).  Upon selection, the files may appear in the list of assets 502.\nA user can also add a file by selecting an \"Import from Zinc\" interface element 506.  In response, the computing system may display a dialog box from which a user may be able to select one or more files that are subject to approval by an\napproval system (as described in greater detail with respect to FIGS. 34A-C and 35).\nAmong those assets that are already included in the list of assets 502, the user interface may include a \"Z\" icon next to any documents that are assets that were imported from the Zinc system (e.g., document 512).  Those assets that are created\nwith the DIALOG system (described later in this document) may be shaded blue.\nA user may also select each asset (e.g., by clicking with a mouse or selecting with touch) to cause the webpage to present a pop-up box 508.  From that pop-up box 508, the user is able to rename a file, share a file, download a file to their\ncomputer, and delete a file.  Should the user select the \"Share\" interface element, the web page may display dialog box 510.  That dialog box may include multiple individuals to which the user is permitted to share the asset (e.g., by clicking a check\nbox next to each representative, where the list of representatives may be those from the list of representatives portion 320 (FIG. 3).\nFIG. 6 shows a user interface with which a user can modify a navigation user interface or create a new navigation user interface.  A navigation user interface may be a user interface that is created for display by another device, and through\nwhich that other device can navigate to subpages to access files.  In this example, a user has selected the \"Briefcases\" interface element in the sidebar of the web page that is shown in FIG. 6.  In response, a request may be sent to computing system 120\nthat in return provides resources to client device 110 for generating the display that is provided in FIG. 6.  (Each of the displays that are provided on client device 110 throughout this disclosure may be generated through a similar client-server\ncommunication model.)\nThe web page includes a collection of interface elements 610a-h that identify previously-created navigation user interfaces.  A user can edit a previously-created navigation user interface by selecting its respective interface element.  The web\npage also includes an interface element 620 that can be selected to create a new navigation user interface.\nIn response to selection of the new navigation user interface element 620, the web browser at the client device 110 may present the web page that is shown in FIG. 7.  The web page that is shown in FIG. 7 may indicate at section 710 that there\nare four steps to creating a navigation user interface, and that the user is currently at a first step.  At this page, the user can specify a name for the navigation user interface through text entry field 720 and a description for the navigation user\ninterface through text entry field 730.  The user can also select a \"Warning\" element 740.  Selecting the warning element 740 can indicate that the navigation user interface that is about to be created may include non-approved materials, and therefore\nwhen an individual opens the newly-created navigation user interface on their client device (e.g., one of devices 130a-c, sometimes referred to herein as just device 130), the client device may present a warning that the navigation user interface may\npresent non-approved materials.\nSelecting the \"Share with\" element 750 allows a user of the computing device 110 to designate other users of devices similar to computing device 110 to which the navigation user interface is to be shared (e.g., there may be multiple marketers at\na pharmaceutical company that are creating navigation user interfaces for field representatives).  Selection of the share with element 750 may cause the webpage to present a dialog box that allows a user to select not only particular users to which the\nnavigation user interface should be shared, but also whether those users have \"Edit\" access and \"Delete\" access.\nIn response to selection of the \"Save and Next\" element 760, the computing device 110 may present the user interface that is shown in FIG. 8.  Using the web page that is shown in FIG. 8, an individual is able to define the appearance of the\nnavigation user interface.  For example, this web page includes a representation of how the user interface may look on any client device 130a-c to which the navigation user interface is to be provided.  Indeed, the web page includes an outline 810 of an\nexample client device tablet computer that includes a display region 812 that represents the screen of the client device (dimensioned the same as the screen on the actual client device), and within which a user is able to specify the features of the\nnavigation user interface that may be presented on the client device.\nIn this example, the navigation user interface (e.g., the interface shown within display region 812) includes seven selectable interface elements 820a-g, which the figures (and this disclosure at times) refers to as \"tiles\" of the user\ninterface.  The size and shape of each of these tiles may have been created by user input at the computing device 110.  For example, the user may have selected the user interface at a particular location (e.g., at the \"Add New Tile\" element 822) to\ncreate a new selectable tile, and then may drag that tile to define a size and shape of the interface element.  In other words, selecting the Add New Tile element 822 may cause the computing system to present an additional tile, after which the user\ninput drag the tile to a new position, and additional user input may drag a side or corner of the tile to change its size and dimensions.  In effect, the user may drag a corner of the newly-created tile to stretch it in various directions.  Once the tile\nhas been created, a user may again select the tile to drag it around to a new location on the display.  In various examples, the tiles may snap to predetermined locations in a grid, and may not be able to overlap other tiles in the display region 812 (at\nleast not permanently, such that the overlap would appear when the navigation user interface is presented for display by a client device).  In various examples, the computing system may require that the entire display region 812 be filled with tiles\nbefore the display region 812 can be presented by a client device.\nUpon mouseover or contact with a tile, the computing device 110 may present icons that allow a user to either remove or edit the tile (e.g., as shown by the edit and delete icons presented on tile 820g).  In response to selection of the edit\ninterface element, the computing system may present a dialog box at which a user is able to provide a name for the tile, a description for the tile, a background color for the tile, and an optional icon to be displayed by the tile.  In FIG. 8, the\nselectable element 820a includes the user-defined title \"Andrew's congress material.\" Any user-specified description may appear in smaller text below this title.  Below that description may be the list of assets (here just \"Congress list.pdf\").  In this\nexample, there is an asset already added (presumably because this screenshot shows a user editing a previously-created navigation user interface).\nFIG. 9 shows a user interface for adding assets to a tile.  The computing device 110 may show this user interface in response to a user selecting an \"Add tile assets\" element from the web page shown in FIG. 8 (or other web pages that include\nthat interface element).  In response, the web browser may present another representation 812 of the navigation user interface, except that this time when a user selects a tile it causes the web browser to present a dialog box 810 from which a user is\nable to assign one or more assets to the selected tile.  In this example, the dialog box includes a list of assets 820 that may represent those assets that were defined through the assets interface that is presented in FIG. 5.  A user may drag one or\nmore of these assets to the \"Tile Content\" region 830 of the dialog box 810 to assign those assets to the selected tile.  The assigned assets may display below the title of the tile, as illustrated for selectable element 820 in FIG. 8, and may be\naccessible to an end user of a client device 130 to which the navigation user interface is ultimately provided.\nFor those assets that are added to the tile (e.g., asset 132), the user can set multiple settings.  For example, selecting an email icon 134a enables a user to specify whether a representative that is accessing the file through a client device\n130 is able to email or otherwise share the file with other devices.  An edit icon 134c enables a user to edit a name of the file.  A remove icon 134d enables a user to remove the document from the selectable element.\nA sync icon 134b enables a user to specify when the file should be synced with a client device and when that file is to expire.  In response to selection of this icon, the computing device 110 may present the dialog box 140.  This dialog box may\ninclude fields that enable a user to specify a validation or start date and time, and an expiration date and time.  With these dates specified, the computing system 120 may not provide the file to any of the client devices 130a-c until that date and time\nhas arrived.  The client devices 130a-c may include code that prevents users from viewing the file after the expiration date has been reached (and the file may disappear from display in its respective tile).\nSelecting the \"Sync all now\" button may cause the computing device 110 to send an indication to the computing system 120 that the assets for the tile are to be synced to any of the client devices 130a-c to which the briefcase has been designated\n(at least for those assets that do not have a validation date that is set in the future).  This tile-by-tile syncing can be particular helpful when a user edited a navigation user interface to add or remove assets from a particular tile.  In a similar\nmanner, a user may add or remove tiles themselves, or the appearances of tiles in real time even after the navigation user interface has been deployed to remote client devices 130a-c.\nFIG. 10 shows a user interface at which an individual can assign a navigation user interface for display by one or more recipients.  This user interface may be navigated to by user selection of the \"Reps\" interface element in various displays. \nIn this example, a list of potential recipients 1010 is presented to the left side of the page.  This list may present those individuals that were specified at the \"My Reps\" web page that is shown in FIG. 3 (although the list may differ slightly here\nbecause screenshots of the user interfaces may have been taken during different sessions).\nIn this example, the groups that are specified in the web page at FIG. 3 (e.g., see groups 330a and 330b) may be presented separately than those representatives that are shown in the list of representatives 320 (FIG. 3) but are not found in any\ngroup.  A user may add one or more of the potential recipients to the list of designated recipients 1020, for example, by selecting a particular individual (with a mouse click or a tap on a touch screen on that person's name) or by selecting an entire\ngroup of individuals (with a mouse click or a tap on that group's name).\nThose individuals that have been added to the designated recipients list are also shown to the right of the screen, where a user can specify the send date, and then click an \"Add\" interface element to add that user as a confirmed recipient for\nthe navigation user interface.  Once the user has been added as a confirmed recipient for the navigation user interface, the next time that the user's client device 130 logs onto the internet and syncs with the computing system 120 (assuming that date is\nafter the \"Send Date\" that is specified in the web page of FIG. 10), the client device may download the navigation user interface.  This process may occur without the user of the client device specifically selecting the navigation user interface for\ndownload.  It may just appear in their list of navigation user interfaces.  Should the navigation user interface change (e.g., through user interaction with the interfaces presented in FIGS. 6-10), the client device 130 may update its navigation user\ninterface when it syncs with computing system 120.\nIn this way, an individual that is designing or updating a navigation user interface can specify different dates for different individuals to receive the navigation user interface (see FIG. 10), and can specify different dates for different\nfiles to be distributed to designated recipients (see FIG. 9).  As such, some users may have access to the navigation user interface on their client devices 130a-c before other users, and even once some or all users have access to the navigation user\ninterface, one or more files may remain unavailable until a later date.\nFIG. 11 shows the navigation user interface as it is presented on a client device 130.  This example shows what a tablet client device may present after a user selects an icon to launch the application program form a home screen of the tablet. \nThe application program that generated the display (and receives updates in the form of navigation user interfaces), may have been downloaded from an application marketplace and may not be part of the operating system.  The first screen that is presented\nupon launching the application program may be that for a navigation user interface, such as that shown in FIG. 11 for the \"Demo Briefcase.\" If there are no navigation user interfaces assigned to the user, then the screen may appear empty (e.g., as a gray\nscreen).\nIn this example, the display presents selectable interface elements (tiles) 1110a-h that a user of the client device can select, which causes device navigation to respective subpages.  The selectable interface elements may not be resizable or\nmodifiable on the client devices 130a-c, as they may be when designed through user input at computing device 110.  The tiles in this example are not fully designed, as only tile 1110a includes a title and a description that was specified by a user of the\ncomputing device 110.\nUpon user selection of tile 1110a, the client device may navigate to a display of a sub-page of the navigation user interface (such as that shown in FIG. 11) that shows certain assets that are assigned to that tile.  For example, the user\ninterface in FIG. 12 shows a \"Congress list.pdf\" file 1210 that user input can select.  Upon selection of the user input, the client device may display the content of the file (e.g., the pages of the PDF) in full screen mode or substantially full-screen\nmode through a viewer application program.  A user can also provide input to view the content of the file in its native application rather than through the viewer application program.  A user can also provide input to view a list of secondary options\n1220 for any given file.  That list of secondary options 1220 may allow a user to re-download a file, add that file to a favorites list (or remove if already added), and share the document, for example, with email if permitted through the corresponding\nsetting that is modifiable at the user interface that shown in FIG. 9.\nThe user interface that is shown in FIG. 12 may also present multiple icons 1230a-f that allow navigation to subpages for respective tiles 1110a-f. In other words, a user does not need to navigate back to the overview page for the navigation\nuser interface (e.g., that shown in FIG. 11) to navigate to a different sub-page and view files that are accessible through interaction with that sub-page.\nFIG. 13 shows an overview page for another navigation user interface, this one being the \"Genkor\" navigation user interface.  A user of a client device may have navigated to this user interface through user interaction with a \"Genkor\" element in\nthe navigation user interface drop-down list 1310 that is presented at the top of the display.  That drop-down list 1310 may present all navigation user interfaces that have been assigned to the recipient that was associated with the client device (e.g.,\nthrough interaction with the page that is shown in FIG. 10 and similar pages for other navigation user interfaces design pages).\nAs may be apparent, the navigation user interface that is shown in FIG. 13 includes tiles that show more content (e.g., titles and descriptions) than those in FIG. 10.  Indeed, each of the tiles includes an icon at the top of the tile (typically\nthe upper left), that was specified through user interaction with the web page shown in FIG. 8 (e.g., a dialog box that appears when a user selects to edit a tile).  These tiles may be those that are shown in the icons 1230a-f that appear in FIG. 12 (and\nFIG. 14, to be described below).\nUser selection of a tile (e.g., one titled \"Detail Aids\" which is hidden behind the drop-down list 1310, may cause user navigation to the interface that is presented in FIG. 14.  This sub-page shows multiple files that are assigned to that tile,\nand that a user can select to view (or potentially share, if settings permit, or add or remove from \"Favorites\").  User input can also prompt the client device change the display from an icon view to a list view through user interaction with element\n1410.  The elements to the right side each include an icon that allows direct navigation to other sub pages.\nAt the top right of the overview page for each navigation user interface and each subpage are five selectable elements.  Element 1422 is an element that toggles display between the currently-selected subpage and the overview page for the current\nnavigation user interface.  If at an overview page, selection of element 1422 may cause navigation to the last-presented subpage.  Element 1424 is an element that causes a display of a list of surveys in a drop-down list upon user selection.\nElement 1426 is an element that, upon user selection, causes the client device to perform a batch download of all files that are queued at the computing system 120 for download to the client device.  For instance, while the client device may\nautomatically download the latest versions of navigation user interfaces (e.g., without user input that specifically requests download of such user interfaces), the client device may not download all or some of the assets that are presented on the sub\npages of each navigation user interface.  To perform this download, user input may have to select element 1426, which may download all such assets that are queue for download for the particular navigation user interface, or for all navigation user\ninterfaces.\nElement 1428, when selected, causes the client device to present a drop-down list of assets that were bookmarked, with the bookmarked assets separated with labels that identify the navigation user interface (briefcase) to which they belong. \nElement 1430 is an element that provides access to various settings, for example, to change a password, re-sync the client device with any updated navigation user interfaces from the network, and provide an ability to log out of a user account.  In some\nexamples, there is an additional element at the top that is hidden and that may be shown in response to a user swiping the elements 1422-1428 to the side.  This additional element may provide user access to the social network screens that are discussed\nthroughout this document.\nFIGS. 15A-B show a flowchart of a process for defining and providing a navigation user interface.\nAt box 1502, a computing device receives user input that defines a navigation user interface that is to be presented by multiple client devices.  For example, the computing device 110 may define the navigation user interface through user\ninteraction with the displays of FIGS. 8 and 9.  The definition may include receiving user input that specifies a size and shape of a first selectable element and a second selectable element (box 1504).  For example, a user may add the selectable\nelements 820a-e to the display and resize those elements, as discussed throughout this document.\nSome portions of this disclosure may refer to a computing system receiving an indication that user input was provided at a computing device to perform certain actions.  This disclosure may refer to the computing system 120 receiving an\nindication (e.g., an internet request or communication) that the user input was provided at computing device 110.  Although not repeated throughout this description exhaustively, it should be understood that most or all user inputs provided at computing\ndevice 110 may cause a communication indicating that the input occurred to be provided for receipt by the computing system 120, and that most or all displays provided by the computing device 110 may be provided as a result of processing information that\nwas provided by the computing system 120, for example, in response to user input at the computing device 110.\nAt box 1506, the computing device receives user input that selects a first element and in response provides an interface to allow a user to specify a first collection of files.  For example, user input may select a tile that is shown in FIG. 9\nin order to present the dialog box 810 that is presented in FIG. 9.\nAt box 1508, the computing device receives user input that specified a first collection of files that are to be accessible by a client device upon selection at that client device of an interface element.  For example, the user may specify one or\nmore files using dialog box 810 (FIG. 9) to permit those files to be presented on a client device in response to a user of the client device selecting the corresponding element (e.g., so that the assets would be presented on a subpage, such as that shown\nin FIG. 14, upon user selection of a tile for the subpage).  At boxes 1510 and 1512, similar operations are performed for a different tile.\nAt box 1514, the computing device provides a representation of the navigation user interface.  For example, the user interfaces that are shown in FIGS. 8 and 9 present mock-ups or representations of the navigation user interface as it may be\nshown on the client device, with the same number of tiles being arranged in the same way and with the same relative dimensions, color, and orientation as they are to be displayed by the client device.  Moreover, the tiles may show at least the titles and\ncorner icons that would be shown by the client device.\nAt box 1516, the computing device receives user input that specifies multiple individuals to which the navigation user interface is to be displayed.  For example, user input may designate multiple recipients through user interaction with the\nuser interface that is shown in FIG. 10.\nAt box 1518, the computing system (e.g., computing system 130, or the combination of the computing system 130 and the computing device 110) causes multiple devices assigned to the multiple individuals to present the navigation user interface. \nFor example, upon the computing system identifying that a client device that is assigned to a user account for one of the multiple individuals is online and connected to the internet (e.g., because the client device sent a request for new content to the\ncomputing system), the computing system may provide data to the client device for generating a display of a navigation user interface.  As discussed throughout this document, the data may be provided for receipt by a particular user only after a date\nthat is specified through interaction with the FIG. 10 user interface.\nAt box 1552, the computing device presents a collection of graphical elements that represent different navigation user interfaces.  For example, a web browser at client device 110 may present the interface shown in FIG. 6, which includes\nmultiple interface elements 610a-g that identify previously-created navigation user interfaces.\nAt box 1554, the computing device receives selection of a particular one of the graphical elements, which allows a user modification of the corresponding navigation user interface.  For example, user input is able to add and remove selectable\ninterface elements to the navigation user interface, add files to and remove files from the collection of files for any of the tiles, and add individuals to and remove individuals from the individuals to which the navigation user interface is to be\ndisplayed (e.g., through user interaction with the interfaces that are presented in FIGS. 7-10).\nAt box 1556, the computing device receives user input to remove a selectable element (e.g., a tile) and add another selectable element (e.g. another tile).  For example, assuming that the navigation user interface that is presented in FIG. 8 was\ncomplete, a user may remove one tile and add one or more tiles in its place.\nAt box 1558, the computing system provides the updated navigation user interface to client devices.  For example, the computing system 120 may receive indications of the changes that a user made at the computing device 110, and may provide\ninformation for receipt by client devices 130a-c that cause the client devices 130a-c to present the updated user interface.\nAt box 1560, the computing system receives user input to remove an individual as a designated recipient and to add another individual as a designated recipient.  For example, user input may interact with the interface that is shown in FIG. 10 to\nremove and add designated individuals.\nAt box 1562, the computing system causes a device assigned to one individual to remove access to the navigation user interface and a device assigned to another individual to add access to the navigation user interface.  For example, for the\nindividual that was removed from having access to the navigation user interface through interaction with the interface of FIG. 10, the computing system 120 may send an indication to the corresponding client device to remove access to the navigation user\ninterface (e.g., to no longer present the name of the navigation user interface in the drop-down list 1310, shown FIG. 13).  For the individual that was added to have access to the navigation user interface, the computing system may send information to\nthe corresponding client device to cause that client device to present the name of the navigation user interface in the drop-down list 1310.\nFIGS. 16-18 show user interfaces that display statistical information identifying interaction by users of client devices 130a-c with certain assets or pages of assets.  These user interfaces are available through user selection of the \"Usage\nTracking\" element from the sidebar that is presented for most pages.  In FIG. 16, a user can select the navigation user interface (e.g., the \"Briefcase\") from pull-down element 1602, which may present a list of multiple navigation user interfaces on\nwhich to view usage statistics.  In this example, \"Andrew's briefcase\" has been selected.  A user may then select the type of asset on which to view usage statistics using the pull-down element 1604.  The user may then select one of multiple\nrepresentatives to which the navigation user interface is distributed using the pull-down element 1606.  The user may also select a date range between an initial date and an end date using the user interface elements 1608a-b.\nThe result of these user selections is an identification, from a set of data that may identify all accesses of all types of assets by all representatives for all navigation user interfaces, those selections that fit the criteria specified\nthrough use of the interface elements 1602, 1604, 1606, and 1608a-b. This data can be displayed in various forms, and is presented in FIG. 16 as a line graph 1620 that plots the number of accesses (on the vertical axis) over time (horizontal axis).  In\nthis example, it shows that sometime just before May 1, the user \"Andrew\" accessed general assets (e.g., all assets in some examples) 11 times, and then accessed (e.g., opened) the general assets three times on each of two days in early May.  The pie\nchart 1630 to the right of the page shows overall usage or accesses of assets for all types.\nFIG. 17 shows a user interface that is similar to that shown in FIG. 16, except that in FIG. 17 the user has selected to view statistics on PDF assets accessed by Frank Dubois using the Genkor navigation user interface.  Since a particular type\nof asset was selected, the user may be able to select an element to identify either all PDF documents or particular PDF documents.  In this example, the user has selected to view just accesses of the document \"PDF1.\" As a result, the line chart 1710\nplots accesses of that document over time.\nFIG. 18 shows a user interface that is similar to that shown in FIGS. 16 and 17, except that the type of asset selected is an HTML asset.  With this type of asset (and others, in various embodiments), the asset may include multiple pages and the\ncomputing device 110 may display a first bar chart 1810 that shows a number of times each of the pages was accessed, and a second bar chart 1820 that shows a total time spent by the respective client device on each of the pages.  In this example, it is\napparent that although the user visited the \"algorithm\" page more often than the \"case_1\" page, the user spent more time on the \"case_1\" page.\nFIG. 19 shows a page at which a user can identify documents that have to be validated.  In this example, a user selected the \"Add document\" element 1910.  In response, the computing system presented a dialog box 1920 with which the user was able\nto identify a document.  The user is then able to designate a \"regulatory,\" or person that is to review the document to identify whether it is valid (e.g., using the dialog box 1920).\nA user that is designated as a regulatory may be able to review the document and provide input to indicate that the document is either validated or refused.  The user interface that is shown in FIG. 19 may indicate each document that the\nparticular user has added (along with potentially other users of the same organization), along with an interface element for each document that indicates whether the document is \"Waiting for validation,\" is \"Validated,\" or is \"Rejected.\" Those documents\nthat are validated may be made available for inclusion in the list of assets (see FIG. 5).\nFIG. 20 shows a user interface with which a user at computing device 110 is able to create a social network page.  This example includes three vertical \"walls.\" Each wall may include a stream of zero or more social network posts.  The user of\nthe computing device may create a new wall by selecting the \"Add new wall+\" element 2010.  In response, the computing system may add a new wall (a fourth wall in this example).  The user may provide input to change a name for the wall, and the wall may\nindicate the user that created the wall (here Eric Dupont).  The user may also add a status post to the wall or a picture.  In this example, the wall \"Andy's conversation with his sales reps\" includes a status update post 2020 that states \"We're doing\ngreat in MM\" and a photo post 2030 that states \"here's a morning picture to motivate you.  Sales from yesterday.\" The web page that is shown in FIG. 20 may indicate how many users have liked and disliked certain posts, along with any comments on the\nposts.  The user may provide input to specify which representatives are to be shown which walls.  In other words, some walls may only go to certain representatives, so that a representative may see a subset of the walls.\nFIG. 21 shows how the social network user interface appears on the client devices (e.g., client devices 130a-c).  The user interface that is shown in FIG. 21 may appear as a result of a user selecting an icon at the top of the user interface for\na client device (e.g., an icon that appears to the right of icons 1422-1430 in FIG. 14).  This user interface only shows a single wall, for example, because the system may not display walls that include no posts, or because the other walls may not have\nbeen designated for presentation to the user of the client device.\nFIG. 22 shows a user interface from which to make a presentation-creation product.  This feature is referred to as \"Dialog\" in the screenshots, and generally provides an ability to create a set of assets and slides from which presentations can\nbe made, and to enable the creation of presentations from those slides.  FIG. 22 shows a user interface that includes multiple graphical elements 2210a-f that represent multiple such presentation-creation products.  The user interface also includes a\ngraphical element 2220 that a user can select to create a new presentation-creation product.\nFIG. 23 shows a user interface from which a user defines general characteristics for a presentation-creation product.  In this example, a user is able to select a checkbox element 2310 to specify whether users of client devices are allowed to\ncreate custom presentations from the collection of slides that are specified for the presentation-creation product (described later in this document).  The user can also select a product thumbnail and a splash screen by selecting respective elements 2320\nand 2330.  The user can select the various template colors for the key message, the key message background, and the icons/buttons/breadcrumbs using the elements 2332, 2334, and 2336, respectively.  Upon selection of any one of those items, a color map\nmay appear that allows the user to select the color for the respective items of the slide templates.  Upon user selection of the preview element 2340, the slide preview screen 2350 is presented, which shows the template for the slide with the\nuser-specified colors for the respective features.\nA user may select the \"Assets\" element from the bar of elements 2311 at the top of the page to navigate to the user interface that is presented in FIG. 24.  At this interface, a user may add one or more images, animated HTML documents, PDF\ndocuments, videos, and references, by selecting the respective element from the bar of elements 2321 and then selecting an add new asset element (e.g., such as element 2331).  In response to selection of element 2331, the user may be able to select an\nelement that was included in the list of assets specified in the user interface of FIG. 5.\nThe middle portion of FIG. 24 includes a cutout 2351 that shows the user interface that is presented upon selection of the references element 2333.  This interface shows that a user has already specified a single reference called \"Dommer A et\nal. Journal of Busted Ear Drums, November 1982.\" A user has also selected the \"Add reference\" element, and in response the computing device has presented the dialog box 2361.  Through user interaction with this dialog box, a user is able to specify a\nname for the reference in a text filed, and can specify an optional additional asset (e.g., a PDF file) that is to be presented when the reference is selected upon presentation on a slide.\nFIG. 25 shows an interface from which a user may view the slides that have been created for a particular presentation-creation product, and may create new slides for such a process.  In this illustration, slides have yet to be created, but user\ninput has selected the new slides element 2510.  In response, the computing device has presented the dialog box 2520, from which a user is able to select any of multiple images or animated HTML files to display as the backdrop to a slide.  The images and\nanimated HTML files that are presented in the dialog box 2520 may be limited to those that were specified in the assets user interface of FIG. 24.\nThe computing device may next present the dialog box 2610 that is shown in FIG. 26.  Through user interaction with this interface, a user is able to add a video (e.g., by selecting the element 2620) or a new reference (e.g., by selecting the\nelement 2630).  The videos and references from which a user is able to select to be presented as part of a slide may be those specified in the assets user interface of FIG. 24.\nFIG. 27 shows a \"Sections\" user interface page for a particular presentation-creation product.  With this interface, a user may create a \"section\" for slides, and add one or more slides to any given section.  For example, a user may select the\n\"+\" element 2710 to create a new section and then may assign a name to the section.  Each section may include a different \"+\" element (e.g., element 2720a-c), and user input may select such an element to cause a dialog box to appear, from which a user\ncan select any of the slides that were created and are presented in the slides user interface (see FIG. 26).\nFIG. 28 shows a presentations user interface with which a user is able to view presentations that have been created for a particular presentation-creation product, and is able to create a new presentation.  In this example, the user has provided\ninput to select the presentation \"Today's presentation for Dr. Evil\" from the list 2810 of presentations.  That presentation includes seven slides, that are to be presented in the order illustrated on the page (e.g., from left to right in the top row,\nand then down to the next row).  The computing device may receive user input that drags and drops the slides into different orders, and may receive user input selecting the element 2820 to add a new slide to the presentation.  The newly-added slide may\nbe selectable from among a presentation of slides, as they are presented in the slides user interface (FIG. 25) or the sections user interface (FIG. 26).\nAlternatively or additionally, a user may select the \"Add new presentation\" element 2830 to create a new presentation.  The user may be able to provide input to specify a name for the new presentation and input to identify a collection of slides\nto comprise the presentation.  A user may also select the \"Custom\" element 2840 to view a list of custom presentation that were created on client devices 130a-c rather than on computing device 110, as described in additional detail below.\nA user may add a presentation-creation product to the list of assets (see FIG. 3), and may be added to a tile as an asset that is to be presented on the subpage for that tile FIG. 29 shows one such sub-page, which shows two presentation-creation\nproducts, including a presentation titled \"Essox.\" User selection of the graphical element for the Essox presentation may cause the client device to navigate to the display that is presented in FIG. 30.\nFIG. 30 shows a user interface that presents multiple presentations created for a particular presentation-creation product.  In this example, the user interface presents at least four presentations 3010a-d, entitled \"Full presentation,\" \"Hi,\"\n\"Jane Custom,\" and \"Efficacy.\" Each presentation is shown with its creation date, its last update, and at least some of the slides from the presentation.  A user may provide input to play the presentation through user selection of a \"Play\" element.  A\nuser can edit a presentation (e.g., by adding or removing slides, or changing an order of slides) by selecting an \"Edit\" element.  A user can delete a presentation from display on their client device by selecting a \"Delete\" element.\nFIG. 31 shows a user interface for creating a presentation using a client device.  This user interface may appear in response to receiving user input that selected the \"Compose your presentation\" element 3020 in FIG. 30.  The user interface may\nfirst appear with certain slides already included at the top region 3110 of the page.  This region may specify which slides are to be included in the custom presentation, and some of the slides may be mandatory, due to user selection of an element that\nindicates that the slide is mandatory when creating the slide (e.g., through user interaction a \"mandatory\" element presented by the user interface of FIG. 25 or 26).  The bottom portion 3120 of the page includes multiple section headers (e.g., \"Home\"\nand \"Mechanism\" of action) with the slides that are assigned to that section shown below that respective section header.  A user can select a slide from the bottom portion 3120 and drag it to the top portion 3110 to add it to the custom presentation. \nUpon adding a slide to the presentation, the slide may appear checked in the bottom portion 3120.  A user may also add a presentation title and description by selecting text boxes at the top of the page and entering appropriate text.  A user can save the\npresentation by selecting the save element 3130.  Upon saving, the custom presentation may appear in the list of presentations that are shown in FIG. 30, and the custom presentation (or data from which the presentation can be generated) is transmitted\nfor receipt by the computing system 120, and the custom presentation is displayed in the list of custom presentations that are accessible through the user interface of FIG. 28.\nFIG. 32 shows a playing of a presentation.  In this example, a user has provided input to play the \"Full presentation\" 3010a through interaction with the user interface that is shown in FIG. 30.  A user may navigate from one page to another in\nthis presentation, in the order defined by user input when the presentation was made, for example, by selecting a back user interface element (not shown in FIG. 31 because the slide is the first slide), or a forward user interface element 3210.  User\ncontact with a bottom of the display (e.g., a triangle icon at the bottom or a sliding input that starts at the bottom and swipes toward the middle) may cause the computing device to present the slide preview dialog box 3220.  This dialog box may include\na graphical preview of each of the slides, and may allow a user to select a corresponding preview to cause navigation to that slide.  As such, a user may view the slides that are in the presentation and quickly jump to a desired slide.\nA user may also filter the slides to show only slides that are assigned to a particular section by selecting a graphical element at the top left of the page to cause presentation of the sections list 3230, which is populated with the title of\neach section of slides.  User selection of an element in the sections list 3230 that identifies a particular section may cause the client device to show only the slides from that selected section.  If a slide from another section is being presented, the\nclient device may switch to the first slide from the selected section that appears in the list.  Regardless, the slide preview dialog box 3220 may still present all slides from the presentation.\nUser selection of reference element 3240 may cause the bottom of the display to present any one or more references that were previously assigned to that slide.  The reference may appear in a reference dialog box 3250 at the bottom of the slide\n(in this example it is shown below the presentation of the slide because dialog box 3220 is already being presented, for illustrative purposes).\nUser selection of the movie element 3242 may cause the client device to overlay the presentation of the slide with a movie that was previously assigned to the slide.  User selection of the backup slide element 3244 may cause the client device to\noverlay the presentation of the slide with a presentation of a backup slide that was assigned to the slide.  User selection of the home element 3246 may cause user navigation to either the overview page of the respective navigation user interface or the\nsubpage to which the presentation-creation product is assigned.\nFIGS. 33A-B show flowcharts of a process for creating and using a presentation-creation product.\nAt box 3302, a computing device (e.g., device 110) receives user input that specifies a name for a first presentation-creation product.  For example, user input may interact with the display of FIG. 23 to provide a name for a\npresentation-creation product.  As described throughout this document, indications of actions that are performed by the computing device 110 may be provided to computing system 120.  As such, portions of this disclosure may refer to the computing system\nreceiving an indication that the first computing device received user input to specify a name for the presentation-creation product.\nAt box 3304, the computing device receives user input that adds multiple assets for use in the presentation-creation product.  For example, user input may add images, HTML files, PDFs, videos, and references through interaction with the user\ninterfaces that are shown in FIG. 24 and described therewith.\nAt box 3306, the computing device receives user input that creates multiple slides for the presentation-creation product.  For example, user input may interact with the user interfaces in FIGS. 25 and 26 to add an image to the slide (e.g., as a\nbackground or main content of the slide), or an HTML file for the slide (box 3308).  If the content is an HTML file, that HTML file may be rendered by a web browser rendering engine at the client device when the slide is displayed by the client device\nand the HTML file may be interactive, such that user interaction with links and buttons on the web page may cause the display to navigate to another page, all while staying on the same slide with the same list of references and other assets that are\nassociated with the slide.\nAt box 3310, user input adds a secondary slide and/or a reference to the slide.  For example, through user interaction with element 2640 (FIG. 26), the user may add a reference to the slide.  Through user interaction with the same user interface\n(or another user interface) a user may add a slide that is considered a backup slide and that is shown overlaid its parent slide upon user selection at a client device of an appropriate interface element.\nAt box 3312, user input assigns a first portion of slides to a first section and a second portion of slides to a second section.  For example, user interaction with the interface that is shown in FIG. 27 can assign various slides to various\nsections, and create new sections.  Slides may be assigned to more than one section in some examples.\nAt box 3314, user input selects slides to comprise a first presentation.  For example, user interaction with the interface that is shown in FIG. 28 assigns slides to existing or new presentations, and can arrange an order of those slides.\nAt box 3316, user input can specify that a slide is mandatory.  The user input may be provided when the slide is created or when that slide is later edited (e.g., through interaction with the interfaces shown in FIGS. 25 and 26).\nAt box 3318, user input selects slides to comprise a second presentation, similar to what was done at box 3314 but for a different presentation.\nAt box 3350, a client device presents an element identifying a presentation-creation product.  For example, the client device 330a may present the graphical element that previews the \"Essox\" presentation-creation product in the user interface of\nFIG. 29.\nAt box 3352, the client device receives user input that selects an element identifying a presentation-creation product.  For example, user input at the client device 330a may select the \"Essox\" presentation-creation product element.\nAt box 3354, the client device, in response, presents elements associated with a first presentation and a second presentation that belong to the presentation-creation product.  For example, the client-device may present the interface of FIG. 30,\nwhich shows elements that are associated with at least four different presentations.\nAt box 3356, user input selects the element that is associated with the first presentation.  For example, user input may select the title, slide representation, or \"Play\" elements for the \"Full presentation.\"\nAt box 3358, the client device presents the selected presentation.  For example, the client device 330 may present the user interface that is illustrated in FIG. 32.  Through interaction with this user interface, a user is able to filter out\npresentation of slides other than those from a selected section (box 3360), for example, by selecting a section from menu 3230 to cause the main display to cause navigation through only slides in that section (e.g., even though only the first, fifth, and\nsixth slides may be from a section, after filtering, those slides may appear first, second, and third in order, with no other slides being navigable through the previous and next elements).\nIn some examples, the slide may present its background as a web page that was generated from an HTML file rather than an image (box 3362).  The background may modify in response to user interaction with the HTML file that, for example, selects a\nlink that causes navigation to another web page.\nIn some examples, the slide may present a reference (box 3364).  For example, the reference 3250 (FIG. 32) may appear in response to user selection of element 3240.\nIn some examples, the slide may present a secondary slide (box 3366).  For example, the secondary slide may appear over a majority or all of the slide that is shown in FIG. 32 in response to user selection of element 3244.\nFIGS. 34A-C show a swim-lane diagram of a process for distributing a document subject to an approval process.\nAt box 3402, the computing device (e.g., computing device 110) displays a group of assets that are subject to approval.  For example, a user may have navigated to the user interface that is displayed in FIG. 5, which shows the user's asset\nlibrary and allows a user to add files to that library.  The user may have selected the \"Import from Zinc\" element 506, and in response the computing system may have presented a dialog box or navigated to another display to allow a user to select one or\nmore of multiple assets that are subject to an approval process.\nAs a general overview, a remote system (here coined with the name \"Zinc\") may serve as a single source of truth for certain documents that may require approval by one or more people, and that may be occasionally updated.  As an example, an image\nthat displays information on a medication may involve the collaboration of multiple people, and one or more people may have the ability to \"approve\" the image for distribution to another service, which in this example is the service for creating\nnavigation user interfaces.  During the collaborative project, the image (or any other asset subject to the approval process, such as PDF documents, videos, and HTML documents) may be presented in the user interface with other assets that are subject to\nthe approval process (and therefore may be awaiting approval or already approved).  One or more of these assets may be selected for inclusion in a library of assets, even though they have yet to be approved.\nAt box 3404, the approval computing system (e.g., the system that manages the collaborative effort to modify the asset and accept user input that approves the asset) may provide a list of assets that are subject to the approval process for\nreceipt by a requesting device.  In this example, the approval system is shown as providing the list of assets that are subject to approval to the computing system at box 3406, which provides it to the computing device, or alternatively directly to the\ncomputing device.\nAt box 3408, the computing device receives user input that selects one of the assets from the list of assets that are subject to approval.  The computing device communicates this selection to either the computing system (illustrated by optional\nbox 3406) or to the approval system.\nAt box 3410, the approval system 3410 provides information regarding the selected asset for receipt by the computing system.  This information may include an approval status of the selected asset (e.g., approved or not approved), along with a\nname of the selected asset, and a type of the selected asset.\nAt box 3412, the computing device displays the selected asset as part of a collection of assets.  For example, the user interface of FIG. 5 that displays the library of assets may display an indication of a document 512 titled\n\"symmetryk-zinc-test.pdf\" along with several other assets.\nAt box 3414, the computing device indicates that the selected asset is subject to approval.  For example, the indication of the document 512 may include a \"Z\" next to the document to indicate that the document is one that is subject to approval\n(regardless whether that document may yet be approved).  In other examples, the indication of the document may be shaded a different color or have a different font to indicate that the document is subject to approval.\nAt box 3416, the computing device indicates that other assets are not subject to approval.  For example, other indications of assets in the list of assets may not include a \"Z\" next their respective names.\nIn this illustration, the computing device is illustrated as communicating with the computing system at box 3406 for various actions.  The presence of box 3406 is presented to illustrate that some or all of the operations provided by the\ncomputing device are processed and provided under a client/server model, as described throughout this document.  Each interaction is not described in detail with reference to this diagram for simplicity, but it should be understood that each user input\nat the computing device could be reported to the computing system, which could provide responsive assets back to the computing device for display.\nAt box 3420, the computing device displays the status of assets that are subject to approval.  For example, the user interface that is presented in FIG. 35 may present a listing of those assets in the library of assets and that are subject to\napproval.  In this illustration, assets 3502 and 3504 represent files that have been added to the library.  The user interface indicates the status of each asset (here both are listed as being \"Approved,\" but an alternative status may be \"Awaiting\nApproval\").  The user interface also includes a \"Briefcases\" icon to the right for each icon, which when selected may cause the computing device to present a list of one or more briefcases to which the asset has been assigned.\nAt box 3422, the approval system provides the status of the assets that are subject to approval and that are in the asset library.  For example, the above-described indication in FIG. 35 regarding whether each asset is Approved or Awaiting\nApproval may be provided by the approval system.  This indication may be provided to the computing system, which then provides the information for receipt by the computing device, but this piecewise description of client/server functioning is omitted for\nsimplicity.\nAt box 3424, the computing device receives user input that selects settings for assets.  Again with reference to FIG. 35, user selection of element 3512 may cause the computing device to replace items displayed at portion 3513 with the display\nillustrated at section 3514 (a user may navigate back to the display of portion 3513 by selecting element 3510).  Portion 3514 may include an element for enabling/disabling auto import 3518, an element for enabling disabling auto resync 3520, and an\nelement for enabling/disabling auto remove 3522.  Enabling auto import may cause client devices to automatically receive a newly-approved file upon the client device connecting to the computing system (although newly-approved files may not be provided\nafter client device connection to the computing, such as 30 minutes after logging in or otherwise connecting).  Enabling auto resync may cause client devices to automatically receive newly-approved files at any time when they are logged in. As such, a\ndevice that has been logged in or otherwise connected to the computing system for 30 minutes may suddenly receive a newly-approved file.  Enabling auto remove may cause client devices to automatically delete assets or remove access to assets that have\nlost their authorization.\nAt box 3426, the computing device selects an asset for a navigation user interface.  For example, as discussed with respect to FIG. 9, a user may add to a tile an asset that is subject to an approval process.  The asset may also be added as a\nbackground image for a tile, or in some examples as a background image, video, or file for a slide of a presentation created through a presentation-creation product.  In some examples, the asset may be added even if that asset has not yet been approved. \nIn some examples, the asset may be added only after it has been approved.\nAt box 3428, the computing device receives user input that selects one or more recipients for a navigation user interface (and by inference selects recipients for the asset that is subject to the approval process).  For example, a user of the\ncomputing device interacts with the user interface of FIG. 10 to designate recipients for the navigation user interface that includes the asset that is subject to approval.\nAt box 3430, the computing system provides the navigation user interface for receipt by one or more client devices that correspond to user accounts for the designated recipients, for example, in response to user input that selects the \"Add\"\nbuttons in FIG. 10.\nAt box 3432, the client device displays the presentation.  For example, the client device may receive from the computing system information from which to generate a display of the navigation user interface.  Should an asset that is subject to\napproval not yet be approved, the client device may not display an indication of that asset, or may display it in a non-selectable state (e.g., as plain text rather than as a selectable link) when the asset is presented on a subpage for a tile.  If the\nasset was to be a background image for a tile or a slide, but the asset is not yet approved, the client device may display a default image or no image as the background image for the tile or slide.\nAt box 3460, the approval system may have received user input approving a particular asset, and may provide an indication that the asset was approved to the computing system.  The computing system may forward this approval to the computing\ndevice which may (at box 3462) update the status for the particular asset to indicate that the asset is now approved (e.g., in the user interface of FIG. 35).\nAt box 3466, the computing system sends the approved asset to the client device.  In other examples, the computing system sends information to the client device that indicates to the client device that the asset is approved and that the client\ndevice is able to access the asset, even though the client device may obtain the asset from another source (e.g., over the internet from a different computer).  This operation may be performed without user input that specifies that the asset is to be\nprovided to the client device, at least after the computing system received the indication that the asset was approved.  In other words, the computing system may automatically perform operations to cause the client device to automatically update its user\ninterfaces to provide access to the asset once that asset is approved by the approval system.\nAt box 3468, the computing system may display an updated presentation, for example, by listing the asset in a list of assets for a subpage of a tile.\nAt box 3470, the approval system may indicate that the asset has been updated.  For example, the computing system may effectively be \"subscribed\" to an asset, and the collaboration on that asset may be ongoing such that the approval system may\noccasionally issue an indication that there is an updated version of the asset available upon user receipt of same by the approval system.\nAt box 3472, the computing device may update its status to indicate that the asset has been updated (e.g., the user interface of FIG. 35 may update to list a new version number for the asset, although the user interface may still indicate that\nthe asset is approved).\nAt box 3476, the computing system sends the updated asset to the client device.  For example, much as with the operations of box 3466, the computing system may provide the client device with an updated version of the asset.\nAt box 3478, the computing system may display an updated presentation.  For example, the previous version of the asset may be replaced with the new version of the asset, such that when a user selects the link for the asset, the presented asset\nmay be different.\nAt box 3480, the approval system may send out an indication that the asset is no longer approved, for example, because a user of the approval system determined that the document was out of date or not in compliance with regulations and that the\ndocument should not be shown on client devices anymore.\nAt box 3482, the computing device may update the status for the respective asset to indicate that the asset is no longer approved, for example, by updating the status for an asset in the user interface of FIG. 35 to indicate that \"Not Approved\"\nor \"Approval Removed.\"\nAt box 3486, the computing system may send the recipient client device an indication that the device is to remove access to the asset.\nAt box 3488, the client device may receive the indication to remove access, and may display an updated presentation.  For example, should the asset have been listed on a subpage for a tile, the client device may remove the indication of that\nasset on that subpage, or may cause the indication of that asset on the subpage to change from a link to non-selectable text.  In some examples, the client device may remove the asset from its memory.\nFIG. 36 shows a user interface for creating a survey.  In this user interface, a user can add a new survey through selection of element 3610.  The interface also shows a list of existing surveys, with each survey in the list indicating a start\ndate, and end date, a time zone, a state of the survey (e.g., either pending, running, or completing), and a participation or number of users in the survey.\nFIG. 37 shows a user interface for creating a survey.  This user interface may be presented in response to user selection of element 3610 (FIG. 36).  With this interface, a user may use elements 3710 to select a new question type (e.g., by\ndragging and dropping it to the \"Drop question here\" element), which causes the question to then be added to the left side of the display and for which a user can then edit various fields for the question.  In this example, Question 1 is a multiple\nchoice question, Question 2 is a unique choice question, and Question 3 is a scale question.  A user may use elements 3720 to specify the representatives to which the survey is to be distributed, and the start date and end date for the survey.  A user\nmay also select the \"Launch survey now\" button to cause the computing system 120 to transmit information to the appropriate client devices for displaying the survey.\nFIG. 38 shows what a single question of a survey looks like on a client device, in this example, Question 3.  As an example, the survey may be defined on the computing device 110, information identifying that definition may be provided to the\ncomputing system 120, and the computing system may provide information for receipt by designated recipients on their respective client devices to cause those device to present a sequence of interfaces like that shown in FIG. 38.\nFIG. 39 shows how the results of the survey are presented after the survey questions are distributed to the client devices and users take the surveys.  For example, this interface shows that a single person answered \"Good\" to the question \"How\nare you?\" and a single person answered \"Sometimes\" to the question \"Are you having fun in your job?\" This interface indicates, at the top right, that the survey was distributed to 10 people and 1 had responded for a participation rate of 10%.  A user is\nable to stop the survey so that it is no longer accessible on the client devices to which it was distributed by selecting the \"Stop survey now\" button.\nReferring now to FIG. 40, a conceptual diagram of a system that may be used to implement the systems and methods described in this document is illustrated.  In the system, mobile computing device 4010 can wirelessly communicate with base station\n4040, which can provide the mobile computing device wireless access to numerous hosted services 4060 through a network 4050.\nIn this illustration, the mobile computing device 4010 is depicted as a handheld mobile telephone (e.g., a smartphone, or an application telephone) that includes a touchscreen display device 4012 for presenting content to a user of the mobile\ncomputing device 4010 and receiving touch-based user inputs.  Other visual, tactile, and auditory output components may also be provided (e.g., LED lights, a vibrating mechanism for tactile output, or a speaker for providing tonal, voice-generated, or\nrecorded output), as may various different input components (e.g., keyboard 4014, physical buttons, trackballs, accelerometers, gyroscopes, and magnetometers).\nExample visual output mechanism in the form of display device 4012 may take the form of a display with resistive or capacitive touch capabilities.  The display device may be for displaying video, graphics, images, and text, and for coordinating\nuser touch input locations with the location of displayed information so that the device 4010 can associate user contact at a location of a displayed item with the item.  The mobile computing device 4010 may also take alternative forms, including as a\nlaptop computer, a tablet or slate computer, a personal digital assistant, an embedded system (e.g., a car navigation system), a desktop personal computer, or a computerized workstation.\nAn example mechanism for receiving user-input includes keyboard 4014, which may be a full qwerty keyboard or a traditional keypad that includes keys for the digits `0-9`, `*`, and `#.` The keyboard 4014 receives input when a user physically\ncontacts or depresses a keyboard key.  User manipulation of a trackball 4016 or interaction with a track pad enables the user to supply directional and rate of movement information to the mobile computing device 4010 (e.g., to manipulate a position of a\ncursor on the display device 4012).\nThe mobile computing device 4010 may be able to determine a position of physical contact with the touchscreen display device 4012 (e.g., a position of contact by a finger or a stylus).  Using the touchscreen 4012, various \"virtual\" input\nmechanisms may be produced, where a user interacts with a graphical user interface element depicted on the touchscreen 4012 by contacting the graphical user interface element.  An example of a \"virtual\" input mechanism is a \"software keyboard,\" where a\nkeyboard is displayed on the touchscreen and a user selects keys by pressing a region of the touchscreen 4012 that corresponds to each key.\nThe mobile computing device 4010 may include mechanical or touch sensitive buttons 4018a-d. Additionally, the mobile computing device may include buttons for adjusting volume output by the one or more speakers 4020, and a button for turning the\nmobile computing device on or off.  A microphone 4022 allows the mobile computing device 4010 to convert audible sounds into an electrical signal that may be digitally encoded and stored in computer-readable memory, or transmitted to another computing\ndevice.  The mobile computing device 4010 may also include a digital compass, an accelerometer, proximity sensors, and ambient light sensors.\nAn operating system may provide an interface between the mobile computing device's hardware (e.g., the input/output mechanisms and a processor executing instructions retrieved from computer-readable medium) and software.  Example operating\nsystems include ANDROID, CHROME, IOS, MAC OS X, WINDOWS 7, WINDOWS PHONE 7, SYMBIAN, BLACKBERRY, WEBOS, a variety of UNIX operating systems; or a proprietary operating system for computerized devices.  The operating system may provide a platform for the\nexecution of application programs that facilitate interaction between the computing device and a user.\nThe mobile computing device 4010 may present a graphical user interface with the touchscreen 4012.  A graphical user interface is a collection of one or more graphical interface elements and may be static (e.g., the display appears to remain the\nsame over a period of time), or may be dynamic (e.g., the graphical user interface includes graphical interface elements that animate without user input).\nA graphical interface element may be text, lines, shapes, images, or combinations thereof.  For example, a graphical interface element may be an icon that is displayed on the desktop and the icon's associated text.  In some examples, a graphical\ninterface element is selectable with user-input.  For example, a user may select a graphical interface element by pressing a region of the touchscreen that corresponds to a display of the graphical interface element.  In some examples, the user may\nmanipulate a trackball to highlight a single graphical interface element as having focus.  User-selection of a graphical interface element may invoke a pre-defined action by the mobile computing device.  In some examples, selectable graphical interface\nelements further or alternatively correspond to a button on the keyboard 4004.  User-selection of the button may invoke the pre-defined action.\nIn some examples, the operating system provides a \"desktop\" graphical user interface that is displayed after turning on the mobile computing device 4010, after activating the mobile computing device 4010 from a sleep state, after \"unlocking\" the\nmobile computing device 4010, or after receiving user-selection of the \"home\" button 4018c.  The desktop graphical user interface may display several graphical interface elements that, when selected, invoke corresponding application programs.  An invoked\napplication program may present a graphical interface that replaces the desktop graphical user interface until the application program terminates or is hidden from view.\nUser-input may influence an executing sequence of mobile computing device 4010 operations.  For example, a single-action user input (e.g., a single tap of the touchscreen, swipe across the touchscreen, contact with a button, or combination of\nthese occurring at a same time) may invoke an operation that changes a display of the user interface.  Without the user-input, the user interface may not have changed at a particular time.  For example, a multi-touch user input with the touchscreen 4012\nmay invoke a mapping application to \"zoom-in\" on a location, even though the mapping application may have by default zoomed-in after several seconds.\nThe desktop graphical interface can also display \"widgets.\" A widget is one or more graphical interface elements that are associated with an application program that is executing, and that display on the desktop content controlled by the\nexecuting application program.  A widget's application program may launch as the mobile device turns on.  Further, a widget may not take focus of the full display.  Instead, a widget may only \"own\" a small portion of the desktop, displaying content and\nreceiving touchscreen user-input within the portion of the desktop.\nThe mobile computing device 4010 may include one or more location-identification mechanisms.  A location-identification mechanism may include a collection of hardware and software that provides the operating system and application programs an\nestimate of the mobile device's geographical position.  A location-identification mechanism may employ satellite-based positioning techniques, base station transmitting antenna identification, multiple base station triangulation, internet access point IP\nlocation determinations, inferential identification of a user's position based on search engine queries, and user-supplied identification of location (e.g., by receiving user a \"check in\" to a location).\nThe mobile computing device 4010 may include other applications, computing sub-systems, and hardware.  A call handling unit may receive an indication of an incoming telephone call and provide a user the capability to answer the incoming\ntelephone call.  A media player may allow a user to listen to music or play movies that are stored in local memory of the mobile computing device 4010.  The mobile device 4010 may include a digital camera sensor, and corresponding image and video capture\nand editing software.  An internet browser may enable the user to view content from a web page by typing in an addresses corresponding to the web page or selecting a link to the web page.\nThe mobile computing device 4010 may include an antenna to wirelessly communicate information with the base station 4040.  The base station 4040 may be one of many base stations in a collection of base stations (e.g., a mobile telephone cellular\nnetwork) that enables the mobile computing device 4010 to maintain communication with a network 4050 as the mobile computing device is geographically moved.  The computing device 4010 may alternatively or additionally communicate with the network 4050\nthrough a Wi-Fi router or a wired connection (e.g., ETHERNET, USB, or FIREWIRE).  The computing device 4010 may also wirelessly communicate with other computing devices using BLUETOOTH protocols, or may employ an ad-hoc wireless network.\nA service provider that operates the network of base stations may connect the mobile computing device 4010 to the network 4050 to enable communication between the mobile computing device 4010 and other computing systems that provide services\n4060.  Although the services 4060 may be provided over different networks (e.g., the service provider's internal network, the Public Switched Telephone Network, and the Internet), network 4050 is illustrated as a single network.  The service provider may\noperate a server system 4052 that routes information packets and voice data between the mobile computing device 4010 and computing systems associated with the services 4060.\nThe network 4050 may connect the mobile computing device 4010 to the Public Switched Telephone Network (PSTN) 4062 in order to establish voice or fax communication between the mobile computing device 4010 and another computing device.  For\nexample, the service provider server system 4052 may receive an indication from the PSTN 4062 of an incoming call for the mobile computing device 4010.  Conversely, the mobile computing device 4010 may send a communication to the service provider server\nsystem 4052 initiating a telephone call using a telephone number that is associated with a device accessible through the PSTN 4062.\nThe network 4050 may connect the mobile computing device 4010 with a Voice over Internet Protocol (VoIP) service 4064 that routes voice communications over an IP network, as opposed to the PSTN.  For example, a user of the mobile computing\ndevice 4010 may invoke a VoIP application and initiate a call using the program.  The service provider server system 4052 may forward voice data from the call to a VoIP service, which may route the call over the internet to a corresponding computing\ndevice, potentially using the PSTN for a final leg of the connection.\nAn application store 4066 may provide a user of the mobile computing device 4010 the ability to browse a list of remotely stored application programs that the user may download over the network 4050 and install on the mobile computing device\n4010.  The application store 4066 may serve as a repository of applications developed by third-party application developers.  An application program that is installed on the mobile computing device 4010 may be able to communicate over the network 4050\nwith server systems that are designated for the application program.  For example, a VoIP application program may be downloaded from the Application Store 4066, enabling the user to communicate with the VoIP service 4064.\nThe mobile computing device 4010 may access content on the internet 4068 through network 4050.  For example, a user of the mobile computing device 4010 may invoke a web browser application that requests data from remote computing devices that\nare accessible at designated universal resource locations.  In various examples, some of the services 4060 are accessible over the internet.\nThe mobile computing device may communicate with a personal computer 4070.  For example, the personal computer 4070 may be the home computer for a user of the mobile computing device 4010.  Thus, the user may be able to stream media from his\npersonal computer 4070.  The user may also view the file structure of his personal computer 4070, and transmit selected documents between the computerized devices.\nA voice recognition service 4072 may receive voice communication data recorded with the mobile computing device's microphone 4022, and translate the voice communication into corresponding textual data.  In some examples, the translated text is\nprovided to a search engine as a web query, and responsive search engine search results are transmitted to the mobile computing device 4010.\nThe mobile computing device 4010 may communicate with a social network 4074.  The social network may include numerous members, some of which have agreed to be related as acquaintances.  Application programs on the mobile computing device 4010\nmay access the social network 4074 to retrieve information based on the acquaintances of the user of the mobile computing device.  For example, an \"address book\" application program may retrieve telephone numbers for the user's acquaintances.  In various\nexamples, content may be delivered to the mobile computing device 4010 based on social network distances from the user to other members in a social network graph of members and connecting relationships.  For example, advertisement and news article\ncontent may be selected for the user based on a level of interaction with such content by members that are \"close\" to the user (e.g., members that are \"friends\" or \"friends of friends\").\nThe mobile computing device 4010 may access a personal set of contacts 4076 through network 4050.  Each contact may identify an individual and include information about that individual (e.g., a phone number, an email address, and a birthday). \nBecause the set of contacts is hosted remotely to the mobile computing device 4010, the user may access and maintain the contacts 4076 across several devices as a common set of contacts.\nThe mobile computing device 4010 may access cloud-based application programs 4078.  Cloud-computing provides application programs (e.g., a word processor or an email program) that are hosted remotely from the mobile computing device 4010, and\nmay be accessed by the device 4010 using a web browser or a dedicated program.  Example cloud-based application programs include GOOGLE DOCS word processor and spreadsheet service, GOOGLE GMAIL webmail service, and PICASA picture manager.\nMapping service 4080 can provide the mobile computing device 4010 with street maps, route planning information, and satellite images.  An example mapping service is GOOGLE MAPS.  The mapping service 4080 may also receive queries and return\nlocation-specific results.  For example, the mobile computing device 4010 may send an estimated location of the mobile computing device and a user-entered query for \"pizza places\" to the mapping service 4080.  The mapping service 4080 may return a street\nmap with \"markers\" superimposed on the map that identify geographical locations of nearby \"pizza places.\"\nTurn-by-turn service 4082 may provide the mobile computing device 4010 with turn-by-turn directions to a user-supplied destination.  For example, the turn-by-turn service 4082 may stream to device 4010 a street-level view of an estimated\nlocation of the device, along with data for providing audio commands and superimposing arrows that direct a user of the device 4010 to the destination.\nVarious forms of streaming media 4084 may be requested by the mobile computing device 4010.  For example, computing device 4010 may request a stream for a pre-recorded video file, a live television program, or a live radio program.  Example\nservices that provide streaming media include YOUTUBE and PANDORA.\nA micro-blogging service 4086 may receive from the mobile computing device 4010 a user-input post that does not identify recipients of the post.  The micro-blogging service 4086 may disseminate the post to other members of the micro-blogging\nservice 4086 that agreed to subscribe to the user.\nA search engine 4088 may receive user-entered textual or verbal queries from the mobile computing device 4010, determine a set of internet-accessible documents that are responsive to the query, and provide to the device 4010 information to\ndisplay a list of search results for the responsive documents.  In examples where a verbal query is received, the voice recognition service 4072 may translate the received audio into a textual query that is sent to the search engine.\nThese and other services may be implemented in a server system 4090.  A server system may be a combination of hardware and software that provides a service or a set of services.  For example, a set of physically separate and networked\ncomputerized devices may operate together as a logical server system unit to handle the operations necessary to offer a service to hundreds of computing devices.  A server system is also referred to herein as a computing system.\nIn various implementations, operations that are performed \"in response to\" or \"as a consequence of\" another operation (e.g., a determination or an identification) are not performed if the prior operation is unsuccessful (e.g., if the\ndetermination was not performed).  Operations that are performed \"automatically\" are operations that are performed without user intervention (e.g., intervening user input).  Features in this document that are described with conditional language may\ndescribe implementations that are optional.  In some examples, \"transmitting\" from a first device to a second device includes the first device placing data into a network for receipt by the second device, but may not include the second device receiving\nthe data.  Conversely, \"receiving\" from a first device may include receiving the data from a network, but may not include the first device transmitting the data.\n\"Determining\" by a computing system can include the computing system requesting that another device perform the determination and supply the results to the computing system.  Moreover, \"displaying\" or \"presenting\" by a computing system can\ninclude the computing system sending data for causing another device to display or present the referenced information.\nFIG. 41 is a block diagram of computing devices 4100, 4150 that may be used to implement the systems and methods described in this document, as either a client or as a server or plurality of servers.  Computing device 4100 is intended to\nrepresent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.  Computing device 4150 is intended to represent various forms of\nmobile devices, such as personal digital assistants, cellular telephones, smartphones, and other similar computing devices.  The components shown here, their connections and relationships, and their functions, are meant to be examples only, and are not\nmeant to limit implementations described and/or claimed in this document.\nComputing device 4100 includes a processor 4102, memory 4104, a storage device 4106, a high-speed interface 4108 connecting to memory 4104 and high-speed expansion ports 4110, and a low speed interface 4112 connecting to low speed bus 4114 and\nstorage device 4106.  Each of the components 4102, 4104, 4106, 4108, 4110, and 4112, are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.  The processor 4102 can process instructions for\nexecution within the computing device 4100, including instructions stored in the memory 4104 or on the storage device 4106 to display graphical information for a GUI on an external input/output device, such as display 4116 coupled to high-speed interface\n4108.  In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.  Also, multiple computing devices 4100 may be connected, with each device providing portions of the\nnecessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).\nThe memory 4104 stores information within the computing device 4100.  In one implementation, the memory 4104 is a volatile memory unit or units.  In another implementation, the memory 4104 is a non-volatile memory unit or units.  The memory 4104\nmay also be another form of computer-readable medium, such as a magnetic or optical disk.\nThe storage device 4106 is capable of providing mass storage for the computing device 4100.  In one implementation, the storage device 4106 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an\noptical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.  A computer program product can be tangibly embodied in an\ninformation carrier.  The computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above.  The information carrier is a computer- or machine-readable medium, such as the memory 4104,\nthe storage device 4106, or memory on processor 4102.\nThe high-speed controller 4108 manages bandwidth-intensive operations for the computing device 4100, while the low speed controller 4112 manages lower bandwidth-intensive operations.  Such allocation of functions is an example only.  In one\nimplementation, the high-speed controller 4108 is coupled to memory 4104, display 4116 (e.g., through a graphics processor or accelerator), and to high-speed expansion ports 4110, which may accept various expansion cards (not shown).  In the\nimplementation, low-speed controller 4112 is coupled to storage device 4106 and low-speed expansion port 4114.  The low-speed expansion port, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled\nto one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.\nThe computing device 4100 may be implemented in a number of different forms, as shown in the figure.  For example, it may be implemented as a standard server 4120, or multiple times in a group of such servers.  It may also be implemented as part\nof a rack server system 4124.  In addition, it may be implemented in a personal computer such as a laptop computer 4122.  Alternatively, components from computing device 4100 may be combined with other components in a mobile device (not shown), such as\ndevice 4150.  Each of such devices may contain one or more of computing device 4100, 4150, and an entire system may be made up of multiple computing devices 4100, 4150 communicating with each other.\nComputing device 4150 includes a processor 4152, memory 4164, an input/output device such as a display 4154, a communication interface 4166, and a transceiver 4168, among other components.  The device 4150 may also be provided with a storage\ndevice, such as a microdrive or other device, to provide additional storage.  Each of the components 4150, 4152, 4164, 4154, 4166, and 4168, are interconnected using various buses, and several of the components may be mounted on a common motherboard or\nin other manners as appropriate.\nThe processor 4152 can execute instructions within the computing device 4150, including instructions stored in the memory 4164.  The processor may be implemented as a chipset of chips that include separate and multiple analog and digital\nprocessors.  Additionally, the processor may be implemented using any of a number of architectures.  For example, the processor may be a CISC (Complex Instruction Set Computers) processor, a RISC (Reduced Instruction Set Computer) processor, or a MISC\n(Minimal Instruction Set Computer) processor.  The processor may provide, for example, for coordination of the other components of the device 4150, such as control of user interfaces, applications run by device 4150, and wireless communication by device\n4150.\nProcessor 4152 may communicate with a user through control interface 4158 and display interface 4156 coupled to a display 4154.  The display 4154 may be, for example, a TFT (Thin-Film-Transistor Liquid Crystal Display) display or an OLED\n(Organic Light Emitting Diode) display, or other appropriate display technology.  The display interface 4156 may comprise appropriate circuitry for driving the display 4154 to present graphical and other information to a user.  The control interface 4158\nmay receive commands from a user and convert them for submission to the processor 4152.  In addition, an external interface 4162 may be provide in communication with processor 4152, so as to enable near area communication of device 4150 with other\ndevices.  External interface 4162 may provided, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used.\nThe memory 4164 stores information within the computing device 4150.  The memory 4164 can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units.  Expansion\nmemory 4174 may also be provided and connected to device 4150 through expansion interface 4172, which may include, for example, a SIMM (Single In Line Memory Module) card interface.  Such expansion memory 4174 may provide extra storage space for device\n4150, or may also store applications or other information for device 4150.  Specifically, expansion memory 4174 may include instructions to carry out or supplement the processes described above, and may include secure information also.  Thus, for\nexample, expansion memory 4174 may be provide as a security module for device 4150, and may be programmed with instructions that permit secure use of device 4150.  In addition, secure applications may be provided via the SIMM cards, along with additional\ninformation, such as placing identifying information on the SIMM card in a non-hackable manner.\nThe memory may include, for example, flash memory and/or NVRAM memory, as discussed below.  In one implementation, a computer program product is tangibly embodied in an information carrier.  The computer program product contains instructions\nthat, when executed, perform one or more methods, such as those described above.  The information carrier is a computer- or machine-readable medium, such as the memory 4164, expansion memory 4174, or memory on processor 4152 that may be received, for\nexample, over transceiver 4168 or external interface 4162.\nDevice 4150 may communicate wirelessly through communication interface 4166, which may include digital signal processing circuitry where necessary.  Communication interface 4166 may provide for communications under various modes or protocols,\nsuch as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others.  Such communication may occur, for example, through radio-frequency transceiver 4168.  In addition, short-range communication may occur, such as\nusing a Bluetooth, WiFi, or other such transceiver (not shown).  In addition, GPS (Global Positioning System) receiver module 4170 may provide additional navigation- and location-related wireless data to device 4150, which may be used as appropriate by\napplications running on device 4150.\nDevice 4150 may also communicate audibly using audio codec 4160, which may receive spoken information from a user and convert it to usable digital information.  Audio codec 4160 may likewise generate audible sound for a user, such as through a\nspeaker, e.g., in a handset of device 4150.  Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device 4150.\nThe computing device 4150 may be implemented in a number of different forms, as shown in the figure.  For example, it may be implemented as a cellular telephone 4180.  It may also be implemented as part of a smartphone 4182, personal digital\nassistant, or other similar mobile device.\nAdditionally computing device 4100 or 4150 can include Universal Serial Bus (USB) flash drives.  The USB flash drives may store operating systems and other applications.  The USB flash drives can include input/output components, such as a\nwireless transmitter or USB connector that may be inserted into a USB port of another computing device.\nVarious implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware,\nsoftware, and/or combinations thereof.  These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may\nbe special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.\nThese computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language,\nand/or in assembly/machine language.  As used herein, the terms \"machine-readable medium\" \"computer-readable medium\" refers to any computer program product, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices\n(PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal.  The term \"machine-readable signal\" refers to any signal used to\nprovide machine instructions and/or data to a programmable processor.\nTo provide for interaction with a user, the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the\nuser and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer.  Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user\ncan be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in any form, including acoustic, speech, or tactile input.\nThe systems and techniques described here can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end\ncomponent (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back end, middleware, or front end\ncomponents.  The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network).  Examples of communication networks include a local area network (\"LAN\"), a wide area network (\"WAN\"),\npeer-to-peer networks (having ad-hoc or static members), grid computing infrastructures, and the Internet.\nThe computing system can include clients and servers.  A client and server are generally remote from each other and typically interact through a communication network.  The relationship of client and server arises by virtue of computer programs\nrunning on the respective computers and having a client-server relationship to each other.\nAlthough a few implementations have been described in detail above, other modifications are possible.  Moreover, other mechanisms for performing the systems and methods described in this document may be used.  In addition, the logic flows\ndepicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results.  Other steps may be provided, or steps may be eliminated, from the described flows, and other components may be added to, or removed\nfrom, the described systems.  Accordingly, other implementations are within the scope of the following claims.", "application_number": "15338039", "abstract": " In general, the subject matter described in this disclosure can be\n     embodied in methods, systems, and program products for defining a\n     navigation user interface that is to be presented by multiple client\n     devices. A representation of the navigation user interface is presented.\n     User input specifies a first collection of files that are to be\n     accessible to users of the multiple client devices upon selection of a\n     first element from the presentation of the navigation user interface.\n     User input specifies a second collection of files that are to be\n     accessible upon selection of a second element from the presentation of\n     the navigation user interface. User input specifies multiple individuals\n     to which the navigation user interface is to be displayed, and the\n     navigation user interface is provided for display on devices assigned to\n     the multiple individuals.\n", "citations": ["6546393", "6684369", "7020663", "7167844", "8065628", "9811673", "20050188332", "20080134344", "20090203415", "20100153866", "20120059906", "20140015827", "20140019868", "20140189592", "20150058235", "20150127628", "20160125376", "20160249106", "20170289234", "20170329780", "20180268319"], "related": []}, {"id": "20180158361", "patent_code": "10373516", "patent_name": "Method for facilitating contextual vocabulary acquisition through\n     association", "year": "2019", "inventor_and_country_data": " Inventors: \nGoll; Paulette S. (Columbia Station, OH), Becker; Richard D. (Munroe Falls, OH)  ", "description": "<BR><BR>BACKGROUND OF INVENTION\n<BR><BR>Field of Invention\nThe present invention relates to methods and systems for providing multi-sensory and contextual vocabulary acquisition through the use of one or more contextual vocabulary features.  More particularly, the present invention relates to methods\nand systems for teaching an individual to learn and retain vocabulary words, particularly vocabulary words found in college and graduate school academic disciplines and postsecondary standardized tests.\n<BR><BR>Brief Description of Related Art\nVocabulary acquisition is an integral component of language development and a staple of standardized testing.  Typically three to five multiple-choice vocabulary word questions are tested on each reading passage in a standardized test, and\ncontext determines meaning and the correct response for each word.\nSeveral well-known techniques continue to be utilized by students to learn vocabulary.  These techniques include looking up \"new\" words in a dictionary, keeping a word log, studying word lists, creating flash cards, using vocabulary workbooks,\nlogging onto word-of-the-day subscriptions, and constructing graphic organizers, etc. For the most part, these strategies offer repetition based learning techniques with limited or no additionally qualities that can enhance students' abilities to more\nefficiently learn and contextualize numerous vocabulary words.  For example, vocabulary words are generally presented with black letters on a white background.  The basic sensory qualities tend to provide a less efficient, less cognitive, lackluster\napproach to vocabulary acquisition.\n<BR><BR>BRIEF SUMMARY OF THE INVENTION\nAccording to one aspect, a method for providing a contiguity-based contextual vocabulary acquisition is disclosed.  Specifically, in accordance with this aspect, the method includes presenting one or more interfaces that can be utilized to\nprovide a plurality of learning tools that are part of a vocabulary learning toolset and a plurality of assessment tools that are part of a vocabulary assessment toolset.  The method also includes determining whether to selectively present one or more\ncontextual vocabulary features on one or more of the learning tools.  Additionally, the method includes determining whether to selectively present one or more contextual vocabulary features on one or more of the assessment tools.  The one or more\ncontextual vocabulary features provide visual, auditory and kinesthetic schemata that are associated with one or more vocabulary words being presented on the one or more learning tools and one or more assessment tools.  The process of contiguity consists\nof the simultaneous visual presentation of a word and a sound, music, video or animation that is immediately followed by the presentation of a contextual sentence.\nAccording to a further aspect, a system for providing contextual vocabulary acquisition is disclosed.  Specifically, in accordance with this aspect, the system includes a contextual vocabulary interface application that is executed on a\nplurality of electronic devices.  The system also includes a vocabulary user interface module that is included as a module of the contextual vocabulary interface application that presents one or more interfaces that can be utilized to provide a plurality\nof learning tools that are part of a vocabulary learning toolset and a plurality of assessment tools that are part of vocabulary assessment toolset.  Additionally, the system includes a vocabulary learning module that is included as a module of the\ncontextual vocabulary interface application that determines whether to selectively present one or more contextual vocabulary features on one or more of the learning tools.  The system further includes a vocabulary assessment module that is included as a\nmodule of the contextual vocabulary interface application that determines whether to selectively present one or more contextual vocabulary features on one or more of the assessment tools.  The one or more contextual vocabulary features provide\nsynchronous or immediately successive visual, auditory and kinesthetic schemata that are associated with one or more vocabulary words being presented on one or more learning tools and one or more assessment tools.\nAccording to still another aspect, a computer readable medium including instructions that when implemented by a processor execute a method for providing contextual vocabulary acquisition is disclosed.  Specifically, in accordance with this\naspect, the method includes presenting one or more interfaces that can be utilized to provide a plurality of learning tools that are part of a vocabulary learning toolset and a plurality of assessment tools that are part of a vocabulary assessment\ntoolset.  The method also includes determining whether to selectively present one or more contextual vocabulary features on one or more of the learning tools.  Additionally, the method includes determining whether to selectively present one or more\ncontextual vocabulary features on one or more of the assessment tools.  The one or more contextual vocabulary features provide visual, auditory and kinesthetic schemata that are associated with one or more vocabulary words being presented on the one or\nmore learning tools and one or more assessment tools. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nIn the descriptions that follow, like parts are marked throughout the specification and drawings with the same numerals, respectively.  The drawing figures are not necessarily drawn to scale and certain figures can be shown in exaggerated or\ngeneralized form in the interest of clarity and conciseness.  The disclosure itself, however, as well as a preferred mode of use, further objects and advances thereof, will be best understood by reference to the following detailed description of\nillustrative embodiments when read in conjunction with the accompanying drawings, wherein:\nFIG. 1 is a schematic view of an exemplary contextual vocabulary acquisition system for presenting multi-sensory and contextual vocabulary acquisition and methods according to an embodiment;\nFIG. 2A is an illustrative example of an exemplary learning tool of the learning toolset that is presented to a user to contextually teach a vocabulary word through one or more contextual vocabulary features according to an embodiment;\nFIG. 2B an illustrative example of an exemplary learning tool of the learning toolset that is presented to the user to contextually teach a vocabulary word through one or more contextual vocabulary features according to an embodiment;\nFIG. 3A is an illustrative example of an exemplary practice assessment tool that is presented to the user to contextually assess a user's knowledge of one or more vocabulary words according to an embodiment;\nFIG. 3B is an illustrative example of an exemplary rehearsal assessment tool that is presented to the user to contextually assess the user's knowledge of one or more vocabulary words according to an embodiment;\nFIG. 3C is an illustrative example of an exemplary of the performance assessment tool that is presented to the user to contextually assess a user's knowledge of one or more vocabulary words according to an embodiment;\nFIG. 3D is an illustrative example of an exemplary master class assessment tool that is presented to the user to contextually access a user's knowledge of one or more vocabulary words according to an embodiment;\nFIG. 4 is a process flow diagram of an exemplary method 400 utilized during the user of an exemplary embodiment of the contextual vocabulary interface application 102 from the operating environment of FIG. 1 according to an embodiment;\nFIG. 5 is a process flow diagram of an exemplary method utilized during the user of an exemplary embodiment of the learning toolset of the contextual vocabulary interface application from the operating environment of FIG. 1 according to an\nembodiment;\nFIG. 6 is a process flow diagram of an exemplary method utilized during the user of an exemplary embodiment of the assessment toolset of the contextual vocabulary interface application from the operating environment of FIG. 1 according to an\nembodiment; and\nFIG. 7 is a process flow diagram of an exemplary method utilized during the use of an exemplary embodiment utilizing assessment advancement thresholds within the assessment toolset of the contextual vocabulary interface application from the\noperating environment of FIG. 1 according to an embodiment.\n<BR><BR>DETAILED DESCRIPTION OF THE INVENTION\nVarious terms are utilized throughout the specification and the appended claims.  Unless otherwise specified expressly or by context, such terms are to be accorded the definitions set forth in the following paragraphs.\nA \"processor,\" as used herein, processes signals and performs general computing and arithmetic functions.  Signals processed by the processor can include digital signals, data signals, computer instructions, processor instructions, messages, a\nbit, a bit stream, or other computing that can be received, transmitted and/or detected.\nA \"bus,\" as used herein, refers to an interconnected architecture that is operably connected to transfer data between computer components within a singular or multiple systems.  The bus can be a memory bus, a memory controller, a peripheral bus,\nan external bus, a crossbar switch, and/or a local bus, among others.\nA \"memory,\" as used herein can include volatile memory and/or nonvolatile memory.  Non-volatile memory can include, for example, ROM (read only memory), PROM (programmable read only memory), EPROM (erasable PROM) and EEPROM (electrically\nerasable PROM).  Volatile memory can include, for example, RAM (random access memory), synchronous RAM (SRAM), dynamic RAM (DRAM), synchronous DRAM (SDRAM), double data rate SDRAM (DDR SDRAM), and direct RAM bus RAM (DRRAM).\nAn \"operable connection,\" as used herein can include a connection by which entities are \"operably connected\", is one in which signals, physical communications, and/or logical communications can be sent and/or received.  An operable connection\ncan include a physical interface, a data interface and/or an electrical interface.\nAn \"input device\" as used herein can include devices for controlling different features which include various components, systems, and subsystems.  The term \"input device\" includes, but it not limited to: push buttons, rotary knobs, and the\nlike.  The term \"input device\" additionally includes graphical input controls that take place within a user interface which can be displayed by various types of mechanisms such as software and hardware based controls, interfaces, or plug and play\ndevices.\nAn \"output device\" as used herein can include devices that can derive from components, systems, subsystems, and electronic devices.  The term \"output devices\" includes, but is not limited to: display devices, and other devices for outputting\ninformation and functions.\nReferring now to the drawings, wherein the showings are for purposes of illustrating one or more exemplary embodiments and not for purposes of limiting the same.  FIG. 1 shows schematic view of an exemplary contextual vocabulary acquisition\nsystem for presenting multi-sensory and contextual vocabulary acquisition and methods according to an embodiment.  The components of the contextual vocabulary acquisition system 100, as well as the components of other systems, hardware architectures and\nsoftware architectures discussed herein, can be combined, omitted or organized into different architecture for various embodiments.  However, the exemplary embodiments discussed herein focus on the environment 100 as illustrated in FIG. 1, with\ncorresponding system components, and related methods.\nAs shown in the illustrated embodiment of FIG. 1, the contextual vocabulary acquisition system 100 provides an overview showing the communications and data transfers between a contextual vocabulary interface application 102 and one or more\ndatabases 104.  In an exemplary embodiment, the contextual vocabulary interface application 102 utilizes data that is pulled from (e.g., by querying) one or more database tables residing on the database(s) 104 to be accessed by the (components of the)\ncontextual vocabulary interface application 102.  In an alternate embodiment, the contextual vocabulary interface application 102 can also utilize data that is pushed (i.e., sent) to be retained/stored within the one or more database tables residing on\nthe database(s) 104 to be accessed by the components of the contextual vocabulary interface application 102.  In yet an another alternate embodiment, the contextual vocabulary interface application 102 can utilize data that is pulled from one or more\ndatabase tables residing on third party databases (e.g., search engine databases) in order to be utilized by the components of the contextual vocabulary interface application 102.\nSpecifically, in an exemplary embodiment, the contextual vocabulary interface application 102 may be utilized by a user (e.g., student) to learn and retain vocabulary words typically found in college and graduate school academic disciplines\nwherein one or more past standardized tests are selected from a group consisting of postsecondary standardized tests (e.g., ACT.RTM., SAT.RTM., TOEFL.RTM., GRP).  The contextual vocabulary interface application 102 provides the user with multi-sensory\ncontextual vocabulary acquisition by providing one or more contextual vocabulary features that provide visual, auditory and kinesthetic schemata that are associated with specific vocabulary words.  The contextual vocabulary interface application 102\nutilizes the association of prior knowledge of ideas and experiences with expanded vocabulary through an associate process by presenting one or more contextual vocabulary features that include but are not limited to images, audio, video, characteristics,\nand associated data that pertain to one or more vocabulary words.  Scaffolding visual, auditory, and kinesthetic pedagogy in acquiring and retaining vocabulary offers optimal, integrated learning experiences that enhance the user's ability to learn\nmultiple vocabulary words in an efficient and effective manner.\nThe contextual vocabulary interface application 102 can be executed on one or more stationary computing devices 106 (e.g., a desktop computer) and/or one or more portable electric devices 110 (e.g., a handheld device, a mobile device, a smart\nphone, a laptop, a tablet, and an e-reader.) In an exemplary embodiment, the contextual vocabulary interface application 102 is a software application that is installed directly onto memory (not shown) of the stationary computing device(s) 106 and/or the\nportable electronic device(s) 110.  In another embodiment, the contextual vocabulary interface application 102 and/or one or more of the components of contextual vocabulary interface application 102 can be installed externally from the memory of the\nportable electronic device(s) 110 and/or the stationary computing device(s) 106.  For example, the contextual vocabulary interface application 102 can include a web based application that is accessed by a communication device (not shown) of the portable\nelectronic device(s) 110 and/or the stationary computing device(s) 106.\nAs a web based application, the contextual vocabulary interface application 102 can be installed on one or more web servers 108.  The one or more web servers 108 can host the contextual vocabulary interface application 102 to be accessed by\nnumerous portable electronic devices 110 and/or stationary computing devices 106 via a wired or wireless (e.g., GSM, CDMA, GPRS, Wi-Fi, WiMax, 3G, and/or LTE) connection through an internet cloud.  In other embodiments, the contextual vocabulary\ninterface application 102 can include a cloud based application that resides on an external host server such as the web server(s) 108 but is accessed through a viewer application that is installed on the memory of the portable electronic device(s) 110\nand/or the stationary computing device(s) 106.\nThe contextual vocabulary interface application 102 includes various modules that are controlled and operated by a processor or control unit of the portable electronic device(s) 110, the stationary computing device(s) 106, and/or the web\nserver(s) 108.  The contextual vocabulary interface application 102 includes a vocabulary user interface module 116, a vocabulary learning module 118, and a vocabulary assessment module 120.\nThe vocabulary user interface module 116 can present a plurality of interfaces and sub-interfaces that are related to the operation of the contextual vocabulary interface application 102.  In an exemplary embodiment, upon initialization (i.e.,\nlaunch) of the contextual vocabulary interface application 102, the vocabulary user interface module 116 can present the user with an opening tool guide interface that allows users to utilize a vocabulary learning toolset (learning toolset) and/or a\nvocabulary assessment toolset (assessment toolset).  In one embodiment, the opening tool guide interface can include a virtual input device that is presented in the form of a virtual musical instrument (e.g., piano or guitar) that the user can input (by\nkinesthetically playing, tapping, or clicking on a virtual key/string) in order to selectively utilize the learning toolset or the assessment toolset of the contextual vocabulary interface application 102.  As will be described in more detail below, the\nlearning toolset can include one or more types of learning tools that can present one or more vocabulary words to the user along with contextual vocabulary features in order for the user to learn the vocabulary word(s).  The assessment toolset can\ninclude one or more types of assessment tools (e.g., same exercises and tests) that can be presented to the user in order for the user to assess and continue to reinforce knowledge of learned vocabulary words (gained by the user while using the learning\ntoolset).\nIn an exemplary embodiment, when the user utilizes the learning toolset of the contextual vocabulary interface application 102, the vocabulary user interface module 116 presents the user with one of a plurality of learning tools that utilize\nvarious contextual vocabulary features to assist users to learn vocabulary words.  The contextual vocabulary features include, but are not limited to, the presentation of images, sounds, colors, music, fonts, etc. that are presented in order to assist\nthe user in grasping one or more vocabulary words.  In some embodiments, the construct of music is utilized to allow the user to interact with virtual musical instruments to virtually play keys and chords that are tied to certain musical tunes, sounds,\nand images to stimulate the user's curiosity and interest in learning certain vocabulary words.\nIn some embodiments, the learning tools can include one or more user interface input icons that the user can input in order to initiate the presentation of various contextual vocabulary features on each of the learning tools.  In some\nembodiments, the virtual input device (e.g., virtual musical instrument) is presented to the user on the upper right hand or left hand corner of the learning tools.  The virtual input device can be inputted (e.g., playing the keys of the virtual musical\ninstrument) in one or more specific manners to initiate various contextual vocabulary features that can be presented on each of the learning tools.  For example, the user can play one octave (input a note) of a virtual musical instrument to view a\ndifferent learning tool showing a new vocabulary word.  Additionally, the user can play a different octave (input a different note) of the virtual musical instrument to view an image associated with the vocabulary word being displayed on the learning\ntool.  Many other non-limiting embodiments of input techniques that can initiate various contextual vocabulary features to be presented on the learning toolset of the multi-sensory vocabulary application 102 will be apparent.\nFIG. 2A is an illustrative example of an exemplary learning tool 200 of the learning toolset that is presented to the user to contextually teach a vocabulary word through one or more contextual vocabulary features.  The learning tool 200 can\npresent one or more color-coded vocabulary words 202 (color has been omitted from the drawing figures due to patent rules, but would appear in an actual embodiment of the invention), the part(s) of speech 204 of the vocabulary word(s) 202, a brief\ndefinition 206 of the vocabulary word(s) 202, and an icon 208 that can be selected by the user to allow the user to hear a pronunciation of the vocabulary word 202.  In some embodiments, the color coded vocabulary word(s) 202 can be presented as\nitalicized, bold-faced, larger, smaller, underlined, or in a distinctive font.  The color and style of the vocabulary word(s) 202 can be presented in a style that incites feelings within the user that represent the context of the vocabulary word 202. \nFor example, the vocabulary word \"ominous\" 202 may be presented in red color in a particular type of font that indicates a threat to reinforce the meaning of the vocabulary word 202.  Other representative examples include the following: FLOURSIH--GROW,\nthrive CAPRICIOUS--erratic, impulisve EQUILIBRIUM--stability, bal a nce\nIn addition, the learning tool can provide the user with kinesthetic contextual features.  For example, the user could obliterate the vocabulary word EFFACE using a digital eraser, kinesthetically finger-wag the vocabulary word REPUDIATE, or\nperform a calisthenics series to simulate the earthquake shake vocabulary word TREMOR.\nIn another embodiment, the user can access scales of synonyms increasing in levels of difficulty and color intensity using the learning tool to expand vocabulary acquisition.  For example, in the key of F Sharp Minor:\nEXPLICIT--comprehensible--obvious--intelligible--overt--lucid--limpid--p- erspicuous\nFor a color-blind individual, gradients of black would appear: EXPLICIT--comprehensible--obvious--intelligible--overt--lucid--limpid--pe- rspicuous\nVocabulary can be upgraded based on children's songs, sayings, and idiomatic phrases.  In addition, the learning tool can provide the user with the opportunity to use collocations to acquire and master vocabulary words.  In an example of an\naudio collocation, the user would be instructed or encouraged to sing the well-known children's song: \"Twinkle, twinkle, little star\", but upgrading the vocabulary using the words \"Scintillate, scintillate minute asteroid\" to the tune of the music of the\nsong Jingle Bells.  In addition, the learning tool can present collocations--word combinations that just sound \"right\" to native English speakers--to link vocabulary words such as, for example: ABORIGINAL tribe; DEPRECATING remark; and MITIGATING\ncircumstances.\nIn one embodiment, the learning tool 200 can include a plurality of user input icons (not shown) that can be inputted by the user to initialize one or more contextual vocabulary features that are associated to the vocabulary word(s) 202 that are\npresented to the user.  However, in the presently preferred embodiment, the learning tool 200 automatically presents applicable contextual vocabulary features that are associated to the vocabulary word(s) 202 presented to the user.\nSome of the contextual vocabulary features that can be presented on the exemplary learning tool 200 will now be discussed.  The learning tool 200 can present a brief audio based musical segment that establishes contiguity and reinforces the\nassociation between the word and the meaning.  For example, the user can be presented with a musical tune/theme from a movie (e.g., alternating pattern of two notes from the movie Jaws) that incites feelings within the user that represent the vocabulary\nword \"ominous\" 202 presented in FIG. 2A.  Optionally, an image can also be presented that can represent and reinforce the vocabulary word (e.g., an image of a shark).  Additionally, video content can also be presented that can reinforce the vocabulary\nword (e.g., a video clip from the movie Jaws that shows the dorsal fin of the shark approaching a swimmer that is representative of the vocabulary word \"ominous\" 202).  In some embodiments, the learning tool 200 can include an icon (not shown) that can\nbe inputted by the user to view a list of words that are associated to the vocabulary word(s) 202.  For example, the user can input the icon to view synonyms of the vocabulary word(s) 202 being presented on the learning tool 200.  It should be noted that\nthe use of antonyms is generally to be avoided, as antonyms can disrupt mental imaging.  The contiguity between a vocabulary word and stimuli (i.e. sounds) needs to be made in a positive mode while an individual is learning the word.  The ability of an\nindividual to delineate a negative patterning in a test question is a different mental process.\nFIG. 2B is an illustrative example of an exemplary learning tool 210 of the learning toolset that is presented to the user to contextually teach a vocabulary word through one or more contextual vocabulary features.  In one embodiment, the\nlearning tool 200 (shown in FIG. 2A) can present a user input icon (not shown), which can be used by the user to request to view one or more sentences that include the color-coded vocabulary word(s) 202.  More preferably, the learning tool 200 simply\ndisplays the one or more sentences automatically, and does not require user input.  As shown in the exemplary sentence 212, the sentence(s) 212 can include the color-coded vocabulary word 202 within the context of the sentence(s) 212 that describes the\nmusical tune that continues to be played as the learning tool 210 displays the vocabulary word 202.  In other words, as the learning tool 210 is presented, the audio based musical segment is played and described within the sentence.  In some embodiments,\nthe learning tool 210 can also include an icon (not shown) (similar to the icon 208 in FIG. 2A) that can be input by the user to hear the audio pronunciation of the vocabulary word 202 within the sentence 212.  Again, more preferably this functionality\nis presented to the user automatically (i.e., does not require user input).\nIn one or more embodiments, the vocabulary user interface module 116 can present various learning tools that include a wide variety of sentence forms, types, subject matter, and contextual vocabulary features in order to reinforce the vocabulary\nword(s) 202 presented to the user.  For example, some learning tools can include paragraphs and/or reading passages (short stories) that include one or more images, sound clips, movie clips, etc. The sentences, paragraphs, and reading passages found on\nthe learning tools can also include idiomatic phrases that can be presented in a highlighted manner in order to aid the user in learning the vocabulary word(s) 202.\nThe vocabulary assessment toolset of the contextual vocabulary interface application 112 will now be discussed.  In one embodiment, the user can initialize the assessment toolset from the opening tool guide interface and/or one or more learning\ntools.  For example, the user can input the virtual input device presented on the opening tool guide in a specific manner (e.g., virtually play two octaves of the virtual musical instrument) in order for the vocabulary user interface module 116 to\npresent the vocabulary assessment toolset.\nIn an exemplary embodiment, when user initiates the vocabulary assessment toolset, the vocabulary user interface module 116 presents an opening assessment interface.  In one embodiment, the opening assessment interface includes a plurality of\nicons that are linked to a plurality of assessment tools that the user can input and utilize.  When input, the plurality of icons initiates assessment tools that apply various assessment techniques to assist users in assessing their knowledge by\nutilizing practice questions/tests in order to further retain learned vocabulary words.\nIn one or more embodiments, the assessment tools can be tailored to one or more subsets of vocabulary words that typically appear on a standardized test.  The vocabulary assessment tools can be individually scored with an assessment score based\non the user's answers, the type of assessment tool presented, and the level of vocabulary word(s) presented.  In one embodiment, the assessment tools can range from being least difficult and including the highest presentation of contextual vocabulary\nfeatures (e.g., color coded vocabulary words, sounds, images, videos, music, etc.) to the most difficult and utilizing no presentation of contextual vocabulary features (e.g., providing a sample standardized test with no color coded vocabulary words,\nsounds, images, videos, music, etc.).  In this embodiment, the user can select which type of assessment tool he/she would like to utilize based on the input of one the icons that are displayed on the opening assessment interface.  However, in other\nembodiments, as are discussed in more detail below, the contextual vocabulary interface application 102 can require the user to take each of the assessment tools in a pre-determined sequential order.  As discussed in more detail below, the contextual\nvocabulary interface application 102 can require the user to achieve an assessment score that meets a minimum assessment advancement threshold value on each of the assessment tools to access the subsequent vocabulary features.\nIn an exemplary embodiment, the plurality of assessment tools of the assessment toolset includes, but are not limited to, a practice assessment, a rehearsal assessment, a performance assessment, and a master class assessment.  FIG. 3A is an\nillustrative example of an exemplary practice assessment tool 302 that is presented to the user to contextually assess a user's knowledge of one or more vocabulary words.  The practice assessment tool 302 can present the user with one or more practice\nexercises 306 that include contextual vocabulary features.  The meaning of one or more vocabulary words 304 can be reinforced in the practice assessment 302 by presenting different types of practice exercises 306 that can include descriptions (e.g.,\nsynonyms) of the one or more vocabulary words 304 to further assist the user to remember/retrieve the vocabulary word from his/her memory.  The practice exercises 306 can be presented in various question formats such as fill in the blank questions, short\nanswer questions, etc. In one or more embodiments, contextual vocabulary features included within the practice assessment 302 can include, but are not limited to, the synchronous playing of one or more audio musical segments that are representative of\nthe one or more vocabulary words 304, the presentation of one or more images that are representative of the one or more vocabulary words 304, the presentation of videos that are representative of the one or more vocabulary words 304, the use of one or\nmore colors and styles of font to help assist the user to remember the context of one or more vocabulary words 304, etc.\nFIG. 3B is an illustrative example of an exemplary rehearsal assessment tool 308 that is presented to the user to contextually assess the user's knowledge of one or more vocabulary words.  The rehearsal assessment tool 308 can present one or\nmore exercises 310 that can also include contextual vocabulary features.  For example, the rehearsal assessment tool 308 can be presented with one or more musical selections representing one or more vocabulary words that were associated with the\nvocabulary word when the user utilized the learning toolset in order to help assist the user to remember and retrieve the vocabulary word from his/her memory.  In an exemplary embodiment, the rehearsal assessment tool 308 will utilize less contextual\nvocabulary features than the practice question assessment so that the user receives less learning support and more practice in learning and remembering the one or more vocabulary words.\nFIG. 3C is an illustrative example of an exemplary of the performance assessment tool 312 that is presented to the user to contextually assess a user's knowledge of one or more vocabulary words.  The performance assessment tool 312 presents one\nor more vocabulary words 314 that are defined and used within a reading passage 316.  In another embodiment, the performance assessment tool 312 can include a user input icon (not shown) that can hide the definition of the one or more vocabulary words\n314, to allow the user to deduce the meaning of the one or more vocabulary words from the reading passage 316 to simulate vocabulary questions on a standardized test.\nThe reading passage 316 can be presented in a format that is similar to reading passages found on post-secondary standardized tests.  The reading passage 316 creates a short story framework that integrates one or more vocabulary words 314 into\nthe text.  In one embodiment, the performance assessment tool 312 can also include some contextual vocabulary features.  However, in an exemplary embodiment, the performance assessment tool 312 will utilize less contextual vocabulary features than the\npractice assessment tool and the rehearsal assessment tool so that the user receives less guidance and more practice in learning and remembering the one or more vocabulary words.  In FIG. 3C, an image 318 is provided as a contextual vocabulary feature. \nThe image 318 is preferably representative of the content of the reading passage 316 and/or the one or more vocabulary words 314 included within the reading passage 316.  The exemplary image 318 shown in FIG. 3C is of the 1909 painting by Oscar Wergeland\nentitled Norsemen Landing in Iceland, and was selected in view of the vocabulary words in the accompanying passage 316 based on context.  The image 318 presented can also be one that was previously associated with one or more vocabulary words 314\npresented when the user utilized the learning toolset in order to help assist the user to remember and retrieve the vocabulary word(s) 314 from his/her memory.  In some embodiments, the user can input a user input icon (not shown) that is presented next\nto the reading passage 316 that initiates the playing of an audio clip (e.g., an audio based musical segment that can be described within the reading passage 316).  However, to the extent such audio clips are provided, it is more preferably for the same\nto be presented to the user automatically rather than in response to user input.  The performance assessment tool 312 can also include one or more multiple-choice vocabulary exercises 320 that can be presented with one or more contextual vocabulary\nfeatures (e.g., musical selections).  The performance assessment tool 312 can include one or more radio button icons or selection icons (not shown) in order for the user to select an answer to each of the multiple-choice vocabulary questions 320\npresented on the performance assessment tool 312.\nFIG. 3D is an illustrative example of an exemplary master class assessment tool 324 that is presented to the user to contextually access a user's knowledge of one or more vocabulary words.  The master class assessment tool 324 is presented to\nthe user in a format that is similar to a standardized test.  Unlike the other types of test assessment tools discussed above, the master class assessment tool 324 does not present the user with contextual vocabulary features.  The master class\nassessment tool 324 is mainly utilized to demonstrate mastery of targeted vocabulary words that the user learned while utilizing the learning toolset and practiced while utilizing one or more of the practice assessment tool, the rehearsal assessment\ntool, and/or the performance assessment tool.  The master class assessment tool 324 can include one or more multiple-choice vocabulary exercises 326 that are construed in standardized test format (e.g., stem with distractors) without any contextual\nfunctional features.  The assessment interface tool 324 can include one or more radio button icons or selection icons (not shown) in order for the user to select an answer to each of the multiple-choice vocabulary exercises 326 presented in the master\nclass assessment tool 324.\nReferring back to FIG. 1, in one or more embodiments, the one or more database(s) 104 can include one or more database tables that can be accessed by one or more applications including the contextual vocabulary interface application 102.  The\none or more database(s) 104 includes a learning table 112 and an assessment table 114.  The learning table 112 can contain a plurality of database records that contain data that is retrieved and applied by the (components of the) contextual vocabulary\ninterface application 102 when the user is utilizing the learning toolset of the application 102.  Specifically, the learning table 112 can contain one or more records that correspond to one or more vocabulary words, vocabulary learning tools, and\ncontextual vocabulary features associated to the one or more vocabulary words and/or learning tools, etc. The assessment table 114 also contains a plurality of database records that contain data that is retrieved and applied by the (components of the)\ncontextual vocabulary interface application 102 when the user is utilizing the assessment toolset of the application 102.  Specifically, the assessment table 114 can contain one or more records that correspond to one or more vocabulary words, vocabulary\nassessment interfaces, and contextual vocabulary features associated to one or more vocabulary words and/or assessment tools, etc.\nIn one embodiment, the database(s) 104 can be hosted on one or more external database servers, web servers, data servers, domain controllers, backup servers, etc. The database(s) 104 can be accessed by the contextual vocabulary interface\napplication 102 through the stationary computing device(s) 106 and/or the portable electronic device(s) 110 through the internet cloud via a wired or wireless (e.g., GSM, CDMA, GPRS, Wi-Fi, WiMax, 3G, and/or LTE) connection.  In one or more embodiments,\nthe database(s) 104 are connected to the web server(s) 108 that can store a plurality of data files that pertain to vocabulary words, contextual vocabulary features and assessment tools that are presented within the learning toolset and the assessment\ntoolset.  The plurality of data files can be organized into a file system that stores the data files based on their classification.\nIn an exemplary embodiment, the components of the contextual vocabulary interface application 102 are capable of executing and presenting various types of data file formats that include a variety of file extensions.  One or more data files\nexecuted by the contextual vocabulary interface application 102 can include files containing one or more vocabulary words, assessment tools, contextual vocabulary features, and related additional data.  For example, the data files can include information\nthat is tied to vocabulary words such as definitions, word associations, etc. The plurality of data files can also include image files of various format types (e.g., JPEG, JIF, JPEG 2000, TIFF, RIF, GIF, BMP, PNG, PPM, PNM, BPG, CPT, PSD, PSP, XCF,\netc.), audio files of various format types (e.g., MP3, M4A, M4P, MMF, MPC, MSV, AIFF, WAV, etc.) and video files of various format types, (e.g., AVI, FLV, WMV, MOV, MP4, etc.).  The plurality of data files can further include data that pertains to font\nand color schemes of the one or more vocabulary words that are presented to the user on the learning toolset and/or the assessment toolset.  Additionally, the plurality of data files can include various types of assessment data that are characterized by\nthe one or more assessment tools.  The assessment data files can include one or more multiple choice exercises, practice exercises, reading passages, etc. in various format types (e.g., .doc, .pages, .wp, etc.) that are presented to the user when the\nuser is utilizing the assessment toolset.\nThe learning table 112 and the assessment table 114 can both include one or more records that include one or more links to the one or more data files discussed above.  As will be described in more detail below, when one or more records\nassociated with one or more specific vocabulary words and/or assessment tools is queried and one or more records are retrieved, data files that are linked to the retrieved records can be executed in order to present contextual vocabulary features, data\nassociated to one or more vocabulary words, and/or data associated to the assessment tools to the user via the learning tools and the assessment tools.\nThe one or more data records of the learning table 112 and the assessment table 114 can also contain records that include a profile that is associated to the user of the contextual vocabulary interface application 102.  In an exemplary\nembodiment, during the user's initial use of the sensory vocabulary interface application 102, the vocabulary user interface module 116 can present the user with an account creation interface.  The account creation interface allows the user to create\naccount credentials (e.g., a username and password) that can be used to login to the contextual vocabulary interface application 102.  The account creation interface can also allow the user to create his/her profile that is stored within a record of the\nlearning table 112 and the assessment table 114.  The account creation interface can list a plurality of questions that allows the user to customize his/her settings when using the contextual vocabulary interface application 102.  For example, the\naccount creation interface can present the user with a question as to which standardized test the user is utilizing the application 102 to prepare for.\nEach user's profile can also be populated with various account information related to the user (e.g., name, address, payment information etc.).  Additionally, each user's profile can include stored values that are populated by one or more\nmodules of the contextual vocabulary interface application 102 that are representative of vocabulary words that have been reviewed by the user within the learning toolset and/or assessment scores of one or more assessment tools completed by the user\nwithin the assessment toolset.  As will be discussed in more detail below, in one embodiment, the one or more records containing each user's profile can be accessed, retrieved, and/or updated by one or more components of the contextual vocabulary\ninterface application 102 upon each user logging into the contextual vocabulary interface application 102.\nReferring again to the application 102, as discussed above, the contextual vocabulary interface application 102 also includes the vocabulary learning module 118 and the vocabulary assessment module 120.  The vocabulary learning module 118 is a\nmodule that is utilized when the user accesses the learning toolset to utilize one or more learning tools.  In an exemplary embodiment, the vocabulary learning module 118 can provide a query on the learning table 112 of the database(s) 104 in order to\npopulate one or more learning tools of the vocabulary learning toolset with vocabulary words, associated definitions, associated reading passages, associated contextual vocabulary features, and other associated vocabulary words.  Specifically, the\nvocabulary learning module 118 can provide a query on the learning table 112 of the database(s) 104 in order to retrieve and/or execute one or more associated contextual vocabulary features such as colors, formats, fonts, audio files, video files, image\nfiles, etc., that are associated to one or more vocabulary words to be presented on one or more learning tools.  In an alternate embodiment, which is presently not preferred, the vocabulary learning module 118 can access third party databases (e.g.,\nsearch engine databases) to query and retrieve data that is associated to the one or more vocabulary words.\nIn an exemplary embodiment, the assessment module 120 can provide a query on the assessment table 114 of the database(s) 104 in order to populate one or more assessment tools of the vocabulary assessment toolset with vocabulary words, associated\ndefinitions, associated multiple choice questions, associated passages, and other associated test data.  The assessment module 120 can provide a query on the assessment table 114 of the database(s) 104 in order to retrieve contextual vocabulary features\nsuch as colors, formats, fonts, audio files, video files, image files, etc., that are associated to one or more vocabulary words.  In addition, the assessment module 120 can access records on the assessment table 114 to create and/or update records that\npertain to scores that the user achieved on the exercises presented on the assessment tools.  For example, when the user takes and completes the practice assessment tool, the assessment module 120 can populate a record on the assessment table 114 with\nthe score that the user achieves on the exercises presented on the practice assessment tool.  In an alternate embodiment, which is presently not preferred, the assessment module 120 can access third party databases (e.g., search engine databases) to\nquery and retrieve data that is associated to the one or more vocabulary words.\nExemplary methods of the contextual vocabulary acquisition system 100 will now be discussed in more detail with reference to FIGS. 4-7.  FIG. 4 is a process flow diagram of an exemplary method 400 utilized during the use of an exemplary\nembodiment of the contextual vocabulary interface application 102 from the operating environment of FIG. 1 according to an embodiment.  FIG. 4 will be described with reference to the components of FIG. 1, though it is to be appreciated that the method of\nFIG. 4 can be used with other systems/components.  The method begins at block 402 wherein the user launches the contextual vocabulary interface application 102.  As discussed, the user can execute the contextual vocabulary interface application 102\nthrough one or more stationary computing devices 106 or one or more portable devices 110.\nAt block 404, the method includes logging into the application 102 with user credentials.  The user can login to the contextual vocabulary interface application 102 using his/her user credentials that the user previously created on the account\ncreation interface during the user's initial use of the contextual vocabulary interface application 102.  At block 406, the user is presented with the opening tool guide interface.  In one embodiment, the opening tool guide can include a plurality of\nuser input icons.  The user can input one of the user input icons to access and utilize the learning toolset and/or the assessment toolset.\nAt block 408, the method includes determining if the user inputs the icon to initiate the learning toolset.  If it is determined that the user does input the user input icon to initiate the learning toolset (at block 408), at block 414, the\nmethod includes presenting the user with the learning tool (e.g. with one or more virtual musical instruments).  In one embodiment, the vocabulary learning module 118 can access the database(s) 104 and perform a query on the learning table 112 to select\none or more records of one or more vocabulary words to present to the user.  Upon selection of the one or more vocabulary words, the vocabulary user interface module 116 can present the learning tool to the user that presents the one or more vocabulary\nwords along with additional data and applicable contextual vocabulary features.  As discussed below (with respect to FIG. 5), the learning tool can include one or more user input icons that can be presented to the user in order for the user to initiate\none or more contextual vocabulary features.\nIf it is determined that the user does not input the icon to initiate the learning toolset (at block 408), at block 410 it is further determined if the user inputs the icon to initiate the assessment toolset.  If it is determined that the user\ninputs the user input icon to initiate the assessment toolset (at block 410), at block 412, the method includes presenting the user with the opening assessment interface with a plurality of icons that are linked to a plurality of assessment tools.  In\none embodiment, when the user initiates the vocabulary assessment toolset, the vocabulary user interface module 116 presents the opening assessment interface that includes a plurality of user input icons that are linked to a plurality of assessment tools\nthat the user can input and utilize.  As discussed below (with respect to FIG. 6), in one embodiment, the user can input one of the user input icons to initiate and utilize one of the assessment tools.\nFIG. 5 is a process flow diagram of an exemplary method 500 utilized during the user of an exemplary embodiment of the learning toolset of the contextual vocabulary interface application 102 from the operating environment of FIG. 1 according to\nan embodiment.  FIG. 5 will be described with reference to the components of FIG. 1, though it is to be appreciated that the method of FIG. 5 can be used with other systems/components.  In the method 500 of FIG. 5, a specific order of determining a\nplurality of contextual vocabulary features to be implemented are shown, however, it is understood that the method and systems described herein can determine the plurality of contextual vocabulary features to be implemented in any order.  It is also\nunderstood that some of the process steps of determining a plurality of contextual vocabulary features can be omitted from the method 500 as one or more vocabulary words may not be associated with all of the contextual vocabulary features that are\ndetermined in the method 500.  Additionally, it is to be understood that the method 500 is a non-limiting exemplary embodiment since additional contextual vocabulary features that are not determined within the process steps of the method 500 can be\nimplemented with respect to one or more vocabulary words.\nThe process starts at block 502, wherein the method includes retrieving the type of standardized test that the user is preparing for.  As discussed above, the learning table 112 can include a record that contains a profile that is associated to\nthe user.  In one embodiment, the vocabulary learning module 118 performs a query on the learning table 112 of the database(s) 104 to search for the record containing the user's profile to determine which standardized test the user is preparing for in\norder to present one or more vocabulary words that are utilized to prepare for the specific standardized test.  For example, in one embodiment the list of vocabulary words that are utilized to prepare for the SAT.RTM.  test may differ than the list of\nvocabulary words that are utilized to prepare for the GRE.RTM.  test.  However, since vocabulary words utilized on such tests are similar in many instances, it may also be the case that the same vocabulary set is utilized for multiple standardized tests. If applicable, the vocabulary learning module 118 can also determine which vocabulary words the user has previously reviewed within the learning toolset from the user's profile.\nAt block 504, the method includes presenting the user with the learning tool that includes one or more virtual input devices.  As discussed above (with respect to block 414), if it is determined that the user inputs the icon to initiate the\nlearning toolset on the opening tool guide interface, the vocabulary user interface module 116 presents the user with the learning tool.  The learning tool can include one or more virtual input devices (e.g., virtual musical instruments) that the user\ncan input (by clicking, swiping, etc.) in order to start learning vocabulary words.\nAt block 506, the method includes determining if the user provides an input to the virtual input device.  In one embodiment, the user can provide an input by selecting one or more user input icons of the virtual input device.  For example, the\nuser can be presented with a virtual piano on which the user can input one or more virtual keys of the virtual musical instrument.  If it is determined that the user provides an input to the virtual input device (at block 506), at block 508, the method\nincludes presenting a one or more vocabulary words on the learning tool.  In one embodiment, the vocabulary learning module 118 presents a query on the learning table 112 of the database(s) 104 to search for the record containing the user's profile to\ndetermine which subset of vocabulary words to present to the user.\nSpecifically, the vocabulary learning module 118 provides a query on the learning table 112 to retrieve one or more records of one or more vocabulary words that pertain to the standardized test the user is preparing for (as determined in block\n502) in order to obtain a list of vocabulary words that apply to the specific test.  In one embodiment, the vocabulary learning module 118 can select one or more vocabulary words to utilize during the session in which the user is utilizing the\napplication 102.  For example, the vocabulary learning module 118 can access a list of five thousand vocabulary words that can be used to prepare for the user's standardized test.  The vocabulary learning module 118 can then select a subset of vocabulary\nwords (e.g., fifty words) from the list of five thousand vocabulary words to present to the user on one or more learning tools presented to the user before selecting a new list of additional vocabulary words as needed.  More preferably, the subsets of\nvocabulary words are pre-determined by the system, and are presented to the user in such manner.\nThe vocabulary learning module 118 can also access the record containing the user's profile on the learning table 112 and on the assessment table 114 in order to update the records with the dataset of vocabulary words that have been presented to\nthe user during the user's utilization of the learning toolset.  In some embodiments, the user can utilize a review session of the learning toolset wherein the vocabulary learning module 118 queries the learning table 112 for the record containing the\nuser's profile to retrieve the dataset of vocabulary words that have been presented to the user so that the user can review words that he/she has already learned during a previous use of the learning toolset.  Additionally, as further discussed below,\nthis functionality ensures that when the user utilizes the assessment toolset, the user is assessed on vocabulary words that have already been presented to the user and that the user has previously learned.\nIn an exemplary embodiment, upon selecting one or more vocabulary words to utilize during the session, the vocabulary learning module 118 can communicate with the vocabulary interface module 116 to present one or more vocabulary words that are\nsynchronously or immediately successively presented along with a brief definition of the word (as shown in FIG. 2A).  In one or more embodiments the vocabulary word(s) can be presented as color-coded and can additionally be presented with the part(s) of\nspeech of the vocabulary word(s) (e.g., noun, verb, adjective, etc.), and a brief definition of the vocabulary word(s).  In addition to presenting the vocabulary word(s) and related information, in one embodiment, the vocabulary user interface module 116\ncan present the user with a plurality of user input icons that can be inputted by the user to initiate specific types of contextual vocabulary features.  In another embodiment, the vocabulary user interface module 116 can present the user with the\nvirtual input device (e.g., virtual musical instrument) in order for the user to input the selection to initiate specific types of contextual vocabulary features.  For example, the virtual Input device may be presented as a virtual musical instrument\nthat presents a set of keys tuned to match those of an actual instrument that can be used to initiate specific types of contextual vocabulary features.\nAt block 510, the method includes determining if the user inputs an audio selection user input icon.  In one embodiment, the vocabulary user interface module 116 can present the user with the user input icon to initiate an associated audio\nselection(s) as a contextual vocabulary feature that is associated with the displayed vocabulary word(s) on the learning tool.  In another embodiment, the vocabulary user interface module 116 can present the user with the virtual input device in order\nfor the user to input the selection to initiate the associated audio selection(s).\nIf it is determined that the user inputs the audio selection user input icon (at block 510), at block 512, the method includes presenting the audio selection(s) associated with the vocabulary word(s).  As described above, each vocabulary word\nincludes a record within the learning table 112 of the database(s) 104.  Each of the records that correspond to each of the vocabulary words contain one or more links to a plurality of contextual vocabulary feature files that are stored on the web\nserver(s) 108 and can be accessed and executed by the vocabulary learning module 118.  In one embodiment, upon the user inputting the audio selection icon on the learning tool, the vocabulary learning module 118 accesses the database(s) 104 and queries\nthe learning table 112 for the record(s) containing the vocabulary word(s) presented on the learning tool.  Upon retrieving the record(s) containing the vocabulary word(s) presented on the learning tool, the vocabulary learning module 118 determines an\naudio file(s) that is linked within the record(s) and executes the audio file(s) from the web server(s) 108.  Upon execution of the audio file(s), one or more audio selections associated to the one or more vocabulary words are presented simultaneously to\nthe user.  The audio selection can include a sound clip or a brief musical segment that reinforces the association between the vocabulary word(s) presented on the learning tool and the definition.\nAt block 514, the method includes determining if the user inputs an image selection user input icon.  In one embodiment, the vocabulary user interface module 116 can present the user with a user interface icon to initiate an associated image\nselection as a contextual vocabulary feature that is associated with the displayed vocabulary word.\nIf it is determined that the user does input the image selection icon (at block 514), at block 516, the method includes presenting an image selection(s) associated with the vocabulary word(s).  In one embodiment, upon the user inputting the\nimage selection user input icon on the learning tool, the vocabulary learning module 118 accesses the database(s) 104 and queries the learning table 112 for a record containing the vocabulary word(s) presented on the learning tool.  Upon retrieving the\nrecord(s) containing the vocabulary word(s) presented on the learning tool, the vocabulary learning module 118 determines an image file that is linked within the record(s) and executes the image file(s) from the web server(s) 108.  Upon execution of the\nimage file(s), the image selection(s) that is associated to the vocabulary word(s) is presented to the user on the learning tool.  The image selection(s) can include a picture or artwork that reinforces the association between the word and the\ndefinition.\nAt block 518, the method includes determining if the user inputs a video selection user input icon.  In one embodiment, the vocabulary user interface module 116 can present the user with the user input icon to initiate an associated video\nselection(s) as a contextual vocabulary feature that is associated with the vocabulary word(s) presented on the learning tool.  In another embodiment, the vocabulary user interface module 116 can present the user with the virtual input device in order\nfor the user to input the selection to initiate the associated video selection(s).\nIf it is determined that the user does input the video selection icon (at block 518), at block 520, the method includes presenting the video selection(s) associated with the vocabulary word(s).  In one embodiment, upon the user inputting the\nvideo selection icon on the learning tool, the vocabulary learning module 118 accesses the database(s) 104 and queries the learning table 112 for a record containing the vocabulary word(s) presented on the learning tool.  Upon retrieving the record\ncontaining the vocabulary word(s) presented on the learning tool, the vocabulary learning module 118 determines a video file(s) that is linked within the record and executes the video file(s) from the web server(s) 108.  Upon execution of the video\nfile(s), the video selection(s) that is associated to the vocabulary word(s) is presented to the user.  The video selection(s) can include a short video clip (e.g., movie clip, media clip) that reinforces the association between the word and its meaning.\nAt block 522, the method includes determining if the user inputs a pronunciation user input icon.  In one embodiment, the vocabulary user interface module 116 can present the user with the user input icon to initiate pronunciation selection(s)\nas a contextual vocabulary feature that is associated with the vocabulary word(s) presented on the learning tool.  In another embodiment, the vocabulary user interface module 116 can present the user with the virtual input device in order for the user to\ninput the selection to initiate the associated pronunciation selection(s).\nIf it is determined that the user does input the pronunciation user interface icon (at block 522), at block 524, the method includes presenting the pronunciation selection(s) of the vocabulary word(s).  Specifically, upon the user inputting the\npronunciation user interface icon on the learning tool, the vocabulary learning module 118 accesses the database(s) 104 and queries the learning table 112 for a record containing the vocabulary word(s) presented on the learning tool.  Upon retrieving the\nrecord(s) containing the vocabulary word(s) presented on the learning tool, the vocabulary learning module 118 determines an audio file(s) that is classified as the pronunciation of the vocabulary word(s) and is linked within the record(s).  For example,\nthe audio file(s) can include a MP3 file(s) that is classified by the word (e.g., ominous.pronounce.mp3).  The vocabulary learning module 118 executes the audio file(s) from the web server(s) 108.  Upon execution of the audio file(s), the pronunciation\nselection(s) of the vocabulary word(s) is presented to the user for the user to hear.\nAt block 526, it is determined if the user inputs a word association user input icon.  In one embodiment, the vocabulary user interface module 116 can present the user with the word association user input icon to initiate the presentation of a\nlist of associated words (including other vocabulary words) that are associated with the vocabulary word(s) presented on the learning tool.  In another embodiment, the vocabulary user interface module 116 can present the user with the virtual input\ndevice in order for the user to input the selection to initiate the presentation of a list of words that are associated to the vocabulary word(s) presented on the learning tool.\nIf it is determined that the user does input the icon to initiate the presentation of associated words (at block 526), at block 528, the method includes presenting one or more words associated to the vocabulary word(s).  In one or more\nembodiments, each record of a particular vocabulary word within the learning table 112 of the database(s) 104 also contains one or more links to other records of vocabulary words and non-vocabulary words that are associated to the particular vocabulary\nword.  Specifically, upon the user inputting the word association user input icon on the learning tool, the vocabulary learning module 118 accesses the database(s) 104 and queries the learning table 112 for the record(s) containing the vocabulary word(s)\npresented on the learning tool.  Upon retrieving the record(s) containing the vocabulary word(s) presented on the learning tool, the vocabulary learning module 118 determines the words that are associated to the displayed vocabulary word(s) that are\nlinked within the record(s).  The vocabulary learning module 118 can present a list of the associated words on the learning tool.  The associated words can include synonyms, homonyms, and homophones that can be presented to the user with additional\ncontextual vocabulary features to help the user learn the vocabulary word(s) presented on the learning tool (in addition to learning other vocabulary words).\nAt block 530, it is determined if the user inputs a sentence presentation icon.  In one embodiment, the vocabulary user interface module 116 can present the user with the sentence presentation user input icon to initiate the presentation of\nanother learning tool that displays one or more sentences that includes the vocabulary word(s) that is presented on the learning tool (as shown in FIG. 2B).  In another embodiment, the vocabulary user interface module 116 can present the user with the\nvirtual input device in order for the user to input the selection to initiate the sentence presentation.\nIf it is determined that the user inputs the sentence presentation user input icon (at block 530), at block 532, the method includes presenting a learning tool with the vocabulary word(s) in one or more sentences.  In one or more embodiments,\neach record of a particular vocabulary word within the learning table 112 of the database(s) 104 also contains one or more links to one or more data files that contains one or more corresponding sentences (i.e., sentences that include the vocabulary word\npresented on the learning tool).  Specifically, upon the user inputting the sentence presentation user input icon on the learning tool, the vocabulary learning module 118 accesses the database(s) 104 and queries the learning table 112 for the record(s)\ncontaining the vocabulary word(s) presented on the learning tool.  Upon retrieving the record(s) containing the vocabulary word(s) presented on the learning tool, the vocabulary learning module 118 executes the data file(s) containing the corresponding\nsentences.  Upon execution of the data file(s), the vocabulary learning module 118 utilizes the vocabulary user interface module 116 to present another learning tool (as shown in FIG. 2B) that includes one or more sentences that include the vocabulary\nword(s) presented to the user on the learning tool.  The vocabulary word(s) can be presented in a color coded format that include one or more contextual vocabulary features that can be utilized by the user on the learning tool.  For example, as one or\nmore sentences are presented on the learning tool, an audio based musical segment can be presented that is described within the sentence.\nIf it is determined that the user does not input the sentence presentation icon (at block 530), the method returns to block 508 wherein the vocabulary word is displayed on the learning tool.  As discussed above, the vocabulary learning module\n118 accesses the record containing the user's profile on the learning table 112 and on the assessment table 114 and can update the record of the vocabulary word(s) that has not been already presented to the user on the learning tool.  The vocabulary\nlearning module 118 can select a vocabulary word(s) from a list of words that has not already been presented to the user to ensure that the user is learning a new vocabulary word(s).  As the new vocabulary word(s) is displayed on the learning tool, the\nuser can once again utilize one or more contextual vocabulary features with respect to the new vocabulary word (per blocks 510-532).\nFIG. 6 is a process flow diagram of an exemplary method 600 utilized during an exemplary embodiment of the assessment toolset of the contextual vocabulary interface application 102 from the operating environment of FIG. 1 according to an\nembodiment.  FIG. 6 will be described with reference to the components of FIG. 1, though it is to be appreciated that the method of FIG. 6 can be used with other systems/components.  In the method 600 of FIG. 6, a specific order of determining assessment\ntools that can be implemented by the user is shown, however, it is understood that the method and systems described herein can determine the assessment tools that can be implemented by the user in any order.\nThe process starts at block 602, wherein the method includes retrieving the type of standardized test that the user is preparing for.  As discussed above, the assessment table 114 can include a record that contains a profile that is associated\nto the user.  In one embodiment, the vocabulary assessment module 120 presents a query on the assessment table 114 of the database(s) 104 to search for the record containing the user's profile to determine which subset of vocabulary words the user has\nbeen learning.  Specifically, the vocabulary assessment module 120 determines which standardized test the user is preparing for in order to present one or more practice exercises within the vocabulary assessment tools that apply to the preparation of the\nstandardized test.  The vocabulary assessment module 120 can also perform a query on the learning table 112 and/or the assessment table 114 to determine the record containing the user's profile in order to determine which vocabulary words the user has\nreviewed within the learning toolset.  Upon determining which standardized test the user is preparing for and which vocabulary words the user has already reviewed, the vocabulary assessment module 120 queries the learning table 112 and/or the assessment\ntable 114 for vocabulary words that are utilized to prepare for the standardized test and that were already reviewed by the user.\nAt block 604, the method includes presenting the user with the opening assessment interface with one or more assessment tool user input icons.  As discussed above (with respect to block 412 of FIG. 4), if it is determined that the user inputs\nthe user input icon to initiate the assessment toolset on the opening tool guide interface, the vocabulary user interface module 116 presents the user with the opening assessment interface.  The opening assessment interface includes a plurality of user\ninput icons that are linked to a plurality of assessment tools.  As discussed below, when one of the plurality of user input icons are input, the vocabulary user interface module 116 can execute and initialize a respective assessment tool to be presented\nto the user.\nAt block 606, it is determined if the user inputs the user interface icon to execute the practice assessment tool.  If it is determined that the user inputs the user interface icon to execute the practice assessment tool (at block 606), at block\n608, the method includes presenting the practice assessment tool.  Specifically, one or more practice assessment tools that apply to the preparation for a specific type of standardized test include one or more corresponding records within the assessment\ntable 114 of the database(s) 104.  Each record can contain one or more links to one or more data files that contain a plurality of practice assessment exercises.  Additionally each record can contain one or more links to one or more contextual vocabulary\nfeature files that are stored on the web server(s) 108.\nIn one embodiment, upon receiving the user's input to initiate the practice assessment tool, the vocabulary assessment module 120 performs a query on the records on the assessment table 114 that are determined to contain one or more links to\ndata files and context vocabulary features files that apply to vocabulary words that were previously reviewed by the user within the learning toolset for a standardized test.  The vocabulary assessment module 120 executes the data file(s) and selects a\nplurality of practice assessment exercises (utilizing the applicable vocabulary words) to present to the user.  The vocabulary assessment module 120 utilizes the vocabulary user interface module 116 to present the user with the practice assessment tool. \nThe practice assessment tool (as shown in FIG. 3A) can be presented with one or more exercises.\nAt block 618, the method includes presenting one or more contextual vocabulary features.  In an exemplary embodiment, upon retrieving one or more records containing one or more links to data files and one or more contextual vocabulary feature\nfiles, the assessment module 120 executes the contextual vocabulary feature file(s) corresponding to the selected exercises presented on the practice assessment tool.  Upon execution of the contextual vocabulary feature file(s) the contextual vocabulary\nfeatures that correspond to the exercises are presented on the practice assessment tool.\nAt block 624, the method includes storing user's answers and scores.  In one embodiment, upon the user completing the practice assessment tool, the vocabulary assessment module 120 queries the assessment table 114 for the record containing the\nuser's profile and updates the record with a dataset that contains the user's answers to the exercises and the score the user attained during the completion of exercises presented on the practice assessment tool.  In some embodiments, when the user\ncompletes the exercises presented on the practice assessment tool, the vocabulary user interface module 116 can present the user with the assessment interface that includes a scoring sheet that can include practice assessment tool scores, trends over\ntime, and other relevant data pertaining to the user's performance of the practice assessment tool.\nReferring again to block 606, if it is determined that the user does not input the user input icon to initiate the practice assessment tool, at block 610, the method determines if the user inputs the icon to initiate the rehearsal assessment\ntool.  If it is determined that the user inputs the icon to execute the rehearsal assessment tool (at block 610), at block 612, the method includes presenting the rehearsal assessment tool.  Specifically, one or more rehearsal assessment tools that apply\nto the preparation for a specific type of standardized test include one or more corresponding records within the assessment table 114 of the database(s) 104.  Each record can contain one or more links to one or more data files that contain a plurality of\nrehearsal assessment exercises.  Additionally each record can contain one or more links to one or more contextual vocabulary feature files that are stored on the web server(s) 108.\nIn one embodiment, upon receiving the user's input to initiate the rehearsal assessment tool, the vocabulary assessment module 120 performs a query on the records on the assessment table 114 that are determined to contain one or more links to\ndata files and context vocabulary features files that apply to vocabulary words that were previously reviewed by the user within the learning toolset and/or that are relevant to the standardized test that the user is preparing for.  The vocabulary\nassessment module 120 executes the data file(s) and selects a plurality of rehearsal assessment exercises (utilizing the applicable vocabulary words) to present to the user.  The vocabulary assessment module 120 utilizes the vocabulary user interface\nmodule 116 to present the user with the rehearsal assessment tool.  The rehearsal assessment tool (as shown in FIG. 3B) can present the user with one or more exercises.\nAt block 618, the method includes presenting one or more contextual vocabulary features.  In an exemplary embodiment, upon retrieving one or more records containing one or more links to data files and one or more contextual vocabulary feature\nfiles, the assessment module 120 executes the contextual vocabulary feature file(s) corresponding to the selected exercises presented on the rehearsal assessment tool.  Upon execution of the contextual vocabulary feature file(s) the contextual vocabulary\nfeature(s) that correspond to the exercises are presented on the rehearsal assessment tool.\nAt block 624, the method includes storing user's answers and scores.  In one embodiment, upon the user completing the rehearsal assessment tool, the vocabulary assessment module 120 queries the assessment table 114 for the record containing the\nuser's profile and updates the record with a dataset that contains the user's answers to the exercises and the score the user attained during the completion of exercises presented on the rehearsal assessment tool.  In some embodiments, when the user\ncompletes the exercises presented on the rehearsal assessment tool, the vocabulary user interface module 116 can present the user with the assessment interface that includes a scoring sheet that can include rehearsal assessment tool scores, trends over\ntime, and other relevant data pertaining to the user's performance of the rehearsal assessment tool.\nReferring again to block 610, if it is determined that the user does not input the user input icon to initiate the rehearsal assessment tool, at block 614, the method determines if the user inputs the user input icon to initiate the performance\nassessment.  If it is determined that the user inputs the user input icon to execute the performance assessment (at block 614), at block 616, the method includes presenting the performance assessment tool.  Specifically, one or more performance\nassessment tools that apply to the preparation for a specific type of standardized test includes one or more corresponding records within the assessment table 114 of the database(s) 104.  The corresponding records can include links to one or more data\nfiles that contain one or more performance assessment reading passages and associated exercises along with one or more contextual vocabulary feature files.\nIn one embodiment, upon receiving the user's input to initiate the performance assessment, the vocabulary assessment module 120 performs a query on the subset of records on the assessment table 114 that are determined to contain links to data\nfiles and context vocabulary features files that apply to vocabulary words that were previously reviewed by the user within the learning toolset and/or that are relevant to the standardized test that the user is preparing for.  The vocabulary assessment\nmodule 120 executes the data file(s) and selects a plurality of reading passages and associated performance assessment exercises (utilizing the applicable vocabulary words) to present to the user.  The vocabulary assessment module 120 utilizes the\nvocabulary user interface module 116 to present the user with the performance assessment tool.  The performance assessment module 120 can present the user with one or more reading passages and one or more associated exercises (as shown in FIG. 3C).\nAt block 618, the method includes presenting one or more contextual vocabulary features.  In an exemplary embodiment, upon retrieving one or more records containing one or more links to data files and one or more contextual vocabulary feature\nfiles, the assessment module 120 executes the contextual vocabulary feature file(s) corresponding to the selected reading passages and associated exercises presented on the performance assessment tool.  Upon execution of the contextual vocabulary feature\nfile(s) the contextual vocabulary features that correspond to the reading passages and associated exercises are presented on the practice assessment tool.\nAt block 624, the method includes storing user's answers and scores.  In one embodiment, upon the user completing the rehearsal assessment tool, the vocabulary assessment module 120 queries the assessment table 114 for the record containing the\nuser's profile and updates the record with a dataset that contains the user's answers to the exercises and the score the user attained during the completion of exercises presented on the rehearsal assessment tool.  In some embodiments, when the user\ncompletes the exercises presented on the rehearsal assessment tool, the vocabulary user interface module 116 can present the user with the assessment interface that includes a scoring sheet that can include rehearsal assessment tool scores, trends over\ntime, and other relevant data pertaining to the user's performance of the performance assessment tool.\nReferring again to block 614, if it is determined that the user does not input the icon to initiate the performance assessment tool, at block 620, the method determines if the user inputs the icon to initiate the master class assessment tool. \nIf it is determined that the user inputs the icon to execute the master class assessment tool (at block 620), at block 622, the method includes presenting the master class assessment tool.  Specifically, one or more master class assessment tools that\napply to the preparation for a specific type of standardized test include one or more corresponding records within the assessment table 114 of the database(s) 104.  Each record can contain one or more links to one or more data files that contain a\nplurality of master class assessment exercises.\nIn one embodiment, upon receiving the user's input to initiate the master class assessment, the vocabulary assessment module 120 performs a query on the subset of records on assessment table 114 that are determined to contain links to data files\nand context vocabulary features files that apply to vocabulary words that were previously reviewed by the user within the learning toolset for a standardized test.  The vocabulary assessment module 120 executes the data file(s) and selects a plurality of\npractice assessment exercises (utilizing the applicable vocabulary words) to present to the user.  The vocabulary assessment module 120 utilizes the vocabulary user interface module 116 to present the user with the master class assessment tool.  The\nmaster class assessment (as shown in FIG. 3D) can present the user with one or more associated exercises in the form of multiple choice questions.  As discussed above, the master class assessment does not utilize any contextual vocabulary features in\norder to provide a simulated version of vocabulary questions on the standardized test.\nAt block 624, the method includes storing user's answers and scores.  In one embodiment, upon the user completing the master class assessment tool, the vocabulary assessment module 120 queries the assessment table 114 for the record containing\nthe user's profile and updates the record with a dataset that contains the user's answers to the exercises and the score the user attained during the completion of exercises presented on the master class assessment tool.  In some embodiments, when the\nuser completes the exercises presented on the master class assessment tool, the vocabulary user interface module 116 can present the user with the assessment interface that includes a scoring sheet that can include master class assessment tool scores,\ntrends over time, and other relevant data pertaining to the user's performance of the master class assessment tool.\nFIG. 7 is a process flow diagram of an exemplary method 700 utilized during the use of an exemplary embodiment utilizing assessment advancement thresholds within the assessment toolset of the contextual vocabulary interface application 102 from\nthe operating environment of FIG. 1 according to an embodiment.  FIG. 7 will be described with reference to the components of FIG. 1, though it is to be appreciated that the method of FIG. 7 can be used with other systems/components.  In one or more\nembodiments of the contextual vocabulary interface application 102 requires the user to complete the assessment toolset in a specific order from assessment tools ranging from the least difficult assessment type (practice assessment) that presents the\nmost cognitive vocabulary features to the most difficult assessment type that presents no cognitive vocabulary features (master class assessment).  Specifically, the vocabulary assessment module 120 can assign assessment advancement thresholds that are\ntied to the user's score achieved on each of the exercises presented on the vocabulary assessment tools.  The advancement thresholds can include a practice assessment threshold associated with the practice assessment tool, a rehearsal assessment\nthreshold associated with the rehearsal assessment tool, and a performance assessment threshold associated with the performance assessment tool.  When a user completes each section of the learning module 118 with an acceptable score, the user is allowed\nto progress to the next section.  The subsequent section may not ordinarily be of a higher difficulty, but may be appropriate for the reinforcement of vocabulary acquisition for the given situation.  It will be appreciated that vocabulary lists for\nstandardized tests such as the SAT.RTM., GRE.RTM.  and TOEFL.RTM.  have significant overlap.  But there is no definitive \"list\" for a particular standardized test.  There are, however, words that occur more frequently across standardized tests.  And\nthese words are preferably utilized in the system.\nThe process starts at block 702, wherein the method includes retrieving the type of standardized test that the user is preparing for.  As discussed above, the assessment table 114 can include a record that contains a profile that is associated\nto the user.  In one embodiment, the vocabulary assessment module 120 presents a query on the assessment table 114 of the database(s) 104 to search for the record containing the user's profile to determine which subset of vocabulary words the user has\nbeen learning.  Specifically, the vocabulary assessment module 120 determines which standardized test the user is preparing for in order to present one or more exercises within the vocabulary assessment tools that are suitable for preparation for\nvocabulary questions on the standardized test.  The vocabulary assessment module 120 can also perform a query on the learning table 112 and/or the assessment table 114 to determine the record containing the user's profile in order to determine which\nvocabulary words the user has reviewed within the learning toolset.  Upon determining which standardized test the user is preparing for and which vocabulary words the user has already reviewed, the vocabulary assessment module 120 queries the learning\ntable 112 for records containing vocabulary words that are utilized to prepare for the standardized test and, optionally, that were already reviewed by the user.\nAt block 704, the method includes presenting the practice assessment tool.  In one embodiment, the vocabulary assessment module 120 performs a query on the subset of records on the assessment table 114 that are determined to contain links to\ndata files and context vocabulary features files that apply to vocabulary words that were previously reviewed by the user within the learning toolset for a standardized test.  The vocabulary assessment module 120 executes the data file(s) and selects a\nplurality of practice assessment exercises (utilizing the applicable vocabulary words) to present to the user.  The vocabulary assessment module 120 utilizes the vocabulary user interface module 116 to present the user with the practice assessment tool.\nAt block 706, the method includes determining if the user achieves a score that meets the practice assessment threshold.  If it is determined that the user's score does not meet the practice assessment threshold (at block 706), the method\nreturns to block 704, where the method once again includes presenting the practice assessment tool.  If it is determined that the user's score meets the practice threshold requirement (at block 706), at block 708, the method includes presenting the\nrehearsal assessment tool.\nIn one embodiment, upon the user completing the practice assessment tool presented on the assessment interface, the vocabulary assessment module 120 queries the assessment table 114 for the record containing the user's profile and updates the\nrecord with a dataset that contains the user's answers to the exercises and the user's score achieved on the practice assessment tool.  Therefore, if the user ends his/her session, and starts a new session of the assessment toolset at a future time, the\nvocabulary assessment module 120 can retrieve the user's profile to determine that the rehearsal assessment tool can be presented to the user (based on meeting the threshold requirement at block 706).  Additionally, the vocabulary assessment module 120\nperforms a query on the subset of records on assessment table 114 that are determined to contain links to data files and context vocabulary features files that apply to vocabulary words that were previously reviewed by the user within the learning\ntoolset for a standardized test.  The vocabulary assessment module 120 executes the data file(s) and selects a plurality of rehearsal assessment exercises (utilizing the applicable vocabulary words) to present to the user.  The vocabulary assessment\nmodule 120 utilizes the vocabulary user interface module 116 to present the user with the rehearsal assessment tool.\nAt block 710, the method includes determining if the user achieves a score that meets the rehearsal assessment threshold.  If it is determined that the user's score does not meet the rehearsal assessment threshold (at block 710), the method\nreturns to block 708, where the method once again includes presenting the rehearsal assessment tool.  If is determined if the user's score meets the rehearsal assessment threshold requirement (at block 710), at block 712, the method includes presenting\nthe performance assessment tool.\nIn one embodiment, upon the user completing the rehearsal assessment tool presented on the assessment interface, the vocabulary assessment module 120 queries the assessment table 114 for the record containing the user's profile and updates the\nrecord with a dataset that contains the user's answers to the exercises and the user's score achieved on the rehearsal assessment tool.  Therefore, if the user ends his/her session, and starts a new session of the assessment toolset at a future time, the\nvocabulary assessment module 120 can retrieve the user's profile to determine that the performance assessment can be presented to the user (based on meeting the threshold requirement at block 710).  Additionally, the vocabulary assessment module 120\nperforms a query on the subset of records on the assessment table 114 that are determined to contain links to data files and context vocabulary features files that apply to vocabulary words that were previously reviewed by the user within the learning\ntoolset for a standardized test.  The vocabulary assessment module 120 executes the data file(s) and selects a plurality of reading passages and associated performance exercises (utilizing the applicable vocabulary words) to present to the user.  The\nvocabulary assessment module 120 utilizes the vocabulary user interface module 116 to present the user with the performance assessment tool.\nAt block 714, the method includes determining if the user's score meets the performance assessment threshold.  If it is determined that the user's score does not meet the performance assessment threshold (at block 714), the method returns to\nblock 712, where the method once again includes presents the performance assessment tool on the assessment interface.  If is determined if the user's score meets the performance assessment threshold requirement (at block 714), at block 716, the method\nincludes presenting the master class assessment tool.\nIn one embodiment, upon the user completing the performance assessment presented on the assessment interface, the vocabulary assessment module 120 queries the assessment table 114 for the record containing the user's profile and updates the\nrecord with a dataset that contains the user's answers to the exercises and the user's score achieved on the performance assessment tool.  Therefore, if the user ends his/her session, and starts a new session of the assessment toolset at a future time,\nthe vocabulary assessment module 120 can retrieve the user's profile to determine that the master class assessment can be presented to the user (based on meeting the threshold requirement at block 714).\nAdditionally, the vocabulary assessment module 120 performs a query on the subset of records on the assessment table 114 that are determined to contain links to data files and context vocabulary features files that apply to vocabulary words that\nwere previously reviewed by the user within the learning toolset and/or that are relevant to the standardized test that the user is preparing for.  The vocabulary assessment module 120 executes the data file(s) and selects a plurality of master class\nassessment exercises (utilizing the applicable vocabulary words) to present to the user.  The vocabulary assessment module 120 utilizes the vocabulary user interface module 116 to present the user with the master class assessment tool.\nAs discussed, various embodiments of the contextual vocabulary interface application 102 can be utilized to provide the user with contextual vocabulary acquisition.  It is to be appreciated that in addition to a stand-alone local or web-based\napplication, the contextual vocabulary interface application 102 can be utilized on different types of platforms, computing infrastructure, and/or devices that are in production and that are not yet in production.  For example, the contextual vocabulary\ninterface application 102 can be executed on a social media platform to be utilized as a social media specific application or plug-in.\nThe embodiments discussed herein can also be described and implemented in the context of computer-readable storage medium storing computer-executable instructions.  Computer-readable storage media includes computer storage media and\ncommunication media.  For example, flash memory drives, digital versatile discs (DVDs), compact discs (CDs), floppy disks, and tape cassettes.  Computer-readable storage media can include volatile and nonvolatile, removable and non-removable media\nimplemented in any method or technology for storage of information such as computer readable instructions, data structures, modules or other data.  Computer readable storage media excludes non-transitory tangible media and propagated data signals.\nIt will be appreciated that various implementations of the above-disclosed and other features and functions, or alternatives or varieties thereof, can be desirably combined into many other different systems or applications.  Also that various\npresently unforeseen or unanticipated alternatives, modifications, variations or improvements therein can be subsequently made by those skilled in the art which are also intended to be encompassed by the following claims.", "application_number": "15888063", "abstract": " Vocabulary acquisition of lists of words drawn from standardized tests\n     are facilitated through contiguity of audio, visual and kinesthetic\n     stimuli. The positive association of each word and a meaning of the word\n     is achieved by separately displaying each word and stimuli on a graphical\n     user interface of an electronic device which simultaneously or\n     immediately successively plays the audio segment and prompts the user to\n     physically interact with the electronic device. Through an ordered\n     combination of steps, the method facilitates rapid verifiable contextual\n     vocabulary acquisition via two levels of assessment testing, one in which\n     the knowledge of the user is potentially aided by the correlation between\n     the audio and visual segments and kinesthetic features, and one which is\n     not.\n", "citations": ["5302132", "5813862", "20070231776", "20120329013"], "related": ["14641142", "61949556"]}, {"id": "20180165883", "patent_code": "10373385", "patent_name": "Subtractive rendering for augmented and virtual reality systems", "year": "2019", "inventor_and_country_data": " Inventors: \nOsotio; Neal (Sammamish, WA), Moulden; Angela (Ravensdale, WA)  ", "description": "<BR><BR>FIELD\nThis application relates generally to virtual and augmented reality systems.  More specifically, embodiments disclosed herein disclose rendering negative space to allow a user to see inside a real world object.\n<BR><BR>BACKGROUND\nVirtual reality and augmented reality hold promise for applications well beyond user entertainment.  Virtual reality (VR) typically refers to computer technologies that use software to generate realistic images, sounds and other sensations that\nreplicate a real environment (or create an imaginary setting), and simulate a user's physical presence in this environment, by enabling the user to interact with this space and any objects depicted therein using specialized display screens or projectors\nand other devices.  Augmented reality (AR) is a live direct or indirect view of a physical, real-world environment whose elements are augmented (or supplemented) by computer-generated sensory input such as sound, video, graphics or GPS data.\nUsers in a VR or AR environment typically are presented objects which can be manipulated and/or otherwise interacted with to modify the environment.\nIt is within this context that the present disclosure arises. <BR><BR>BRIEF DESCRIPTION OF DRAWINGS\nFIG. 1 illustrates an example architecture of a system for rendering negative space in an augmented reality or virtual reality system.\nFIG. 2 illustrates an example interaction diagram between a rendering system and a search service.\nFIG. 3 illustrates an example flow diagram for rendering negative space.\nFIG. 4 illustrates an example of rendering negative space.\nFIG. 5 illustrates another example of rendering negative space.\nFIG. 6 illustrates an example flow diagram for placing positive space objects into negative space model.\nFIG. 7 illustrates a representative machine architecture suitable for implementing the systems and so forth or for executing the methods disclosed herein.\n<BR><BR>DETAILED DESCRIPTION\nThe description that follows includes illustrative systems, methods, user interfaces, techniques, instruction sequences, and computing machine program products that exemplify illustrative embodiments.  In the following description, for purposes\nof explanation, numerous specific details are set forth in order to provide an understanding of various embodiments of the inventive subject matter.  It will be evident, however, to those skilled in the art that embodiments of the inventive subject\nmatter may be practiced without these specific details.  In general, well-known instruction instances, protocols, structures, and techniques have not been shown in detail.\n<BR><BR>OVERVIEW\nVirtual reality allows users to interact with a virtual representation of real world objects while augmented reality overlays additional information on view of the real world.  In all these instances, users interact with positive space\nrepresentations of objects and can manipulate the positive space objects, attach one positive space object to another and so forth.\nPositive space objects are objects defined by their external surfaces.  Thus, a positive space object is the way an object looks.  Negative space objects, on the other hand, are objects defined by internal surfaces of a positive space object. \nSaid another way, negative space objects are the internals of a positive space object of what is underneath the external surfaces of a positive space object.  In real world objects, positive and negative space objects tend to be nested.  Thus, an engine\nis a positive space object.  If the head covers are removed, it reveals negative space objects such as the cylinders and pistons, the latter of which is itself a positive space object.\nThe present disclosure includes systems and methods that allow a user to view and interact with negative space objects in real-world objects.  Different embodiments can be used in an augmented reality or virtual reality system.  In augmented\nreality systems, a real-world three-dimensional (3D) object is viewed with the augmented reality system.  An image or other representation of the object is used to identify negative space models suited to the real-world object.  For example, an image of\nthe real-world object can be submitted to a search service and the search service can identify negative and/or positive space objects associated with the real-world object.  Negative space objects define a representation of the negative space associated\nwith the real-world object.\nSensors, scans or other measurements of the real-world object can provide data about the negative space of the real-world object.  For example, an x-ray image of an engine may reveal the conditions of the cylinders, pistons and/or other\nnegative-space objects.  The data can be used to adjust a negative space object to reflect the actual condition of the internal objects.  For example, if a scan reveals that a cylinder in an engine has a wear pattern, a crack or other damage, one or more\nof the parameters of the negative space model of the cylinder can adjusted to include the wear pattern, crack, and so forth.  Additionally, or alternatively, the scan data can be used to create a negative space model of the actual negative space of a\nreal-world object.\nA negative space model can comprise a plurality of anchor points that define how a negative space model is anchored to a representation of a real-world object.  In some embodiments, these anchor points can be intersection points where the\nreal-world object and the negative space model intersect.  The plurality of anchor points define scaling, rotation, positioning and other adjustments that are made to the negative space model to correctly orient the negative space model within and in\nconjunction with the real-world object.\nDepending on the relationship between the real-world object and the negative space object, it may be sufficient to superimpose the negative space object on top of the real-world object representation in such a way that the negative space object\nobscures the real-world object so that it appears that the real-world object has been opened so the internals can be viewed.  In other situations, the system first removes positive space aspects of the real-world object representation and then overlays\nthe negative space object so that it appears that the real-world object has been opened so the internals can be viewed.\nOnce the negative space model is anchored to the real-world representation, the real-world object is rendered with the negative space model.  The user point of view is then tracked so that as the user moves, the real-world representation and\nnegative space model can be re-rendered based on the changing point of view.\nBecause the real-world object has not actually been opened to reveal the negative space inside, if a user tries to place hands inside the negative space, they will encounter the actual real-world object.  Thus, interactions with the negative\nspace object are virtual interactions using gestures and other input mechanisms to manipulate the real-world representation with the negative space model.  Such gestures can include taking additional positive space objects and placing them inside the\nnegative space and otherwise manipulating the negative space.\n<BR><BR>DESCRIPTION\nFIG. 1 illustrates an example architecture 100 of a system for rendering negative space in an augmented reality or virtual reality system.  A user 102 interacts with augmented and/or virtual reality through an AR or VR device 104.  Hereafter,\nthe embodiments will be presented using an AR example and differences for VR embodiments will be separately explained.  The AR device 104 utilizes a rendering system 106 to present an augmented reality view to the user 102.  In the case of VR, the user\nis presented a VR world within which to manipulate objects.  The rendering system 106 can be part of the AR device 104 or can be a separate system, depending on the embodiment.\nThe rendering system 106 presents a representation of a real-world object 122.  In the AR context, the real-world representation may simply be the object itself, viewed by the user through a lens that allows additional information to be\noverlaid.  In this situation, the real-world representation may be a data structure that describes how the user perceives the real-world object.  This data structure can capture the user's point of view, the lighting conditions, the scale that the user\nsees the object, and other such information that allows the system to identify what the user is seeing so that as information is overlaid on the object, the information lines up appropriately.  Additionally, or alternatively, the AR device 104 and/or the\nrendering system 106 can capture an actual representation of the object such as an image, wireframe, and so forth.  The representation and any associated information allows the rendering system 106 to identify what the user perceives through the AR\ndevice 104.\nThe rendering system 106 is connected to a search service 116, usually over a network 114.  The search service 116 can be utilized to locate models as described herein.  Search services can typically retrieve information by identifier, such as a\nword, key phrase, or other identifier, or can retrieve images by similarity.  For example, both Google.RTM.  and Bing.RTM.  search engines allow users to submit an image and the search engines will return similar images.  This technology can also be used\nto locate images, representations and/or models associated with real-world objects.\nModels are stored in one or more data stores, such as the model store 120.  The model store 120 can be a database or other type of data store that stores 3D and/or 2D models used herein.  Models can be two or three dimensional, depending on the\nexact circumstance and situation.  However, in most instances, the models will be three-dimensional and three-dimensional model examples are primarily used in the description.\nModels can be created in various ways in different embodiments.  For example, models can be hand created by individuals, either with the help of computerized tools or hand drawn/coded.  Many tools exist to allow users to draw three dimensional\nobjects and then convert the three-dimensional drawn object into a 3D model for a virtual reality system.  Models can be created using these tools.\nModels can also be created by model creation process 118, using sensors 112.  Sensors fall into those that image the outside of an object and those that image the inside of an object.  The former can be used to create positive space models and\nthe latter can be used to create negative space models.  For example, Magnetic Resonance Imaging (MRI), Computerized Axial Tomography (CT) scan, ultrasound, and/or other data can be converted into a 3D negative space model.  As a representative example,\nsequentially mounted MRI slices can be used to begin construction of the negative space model.  Segmentation can then be used to select the portions of the MRI slices that will be used to construct the model.  Such MRI data can be augmented by other\nsensor data such as ultrasound data and other data gathered by other sensors.  For materials that do not lend themselves to MRI and/or ultrasound scanning, other sensor/scanning processes can be used such as x-ray, thermal, and other sensors used to\nperceive what is happening below the surface of an object.\nFor positive space models, optical and other such scanning and imaging sensors can be used.  Methods exist that allow a 3D model to be constructed from images from different angles of an object.  These methods can be used to create 3D positive\nspace models from images and other sensor data.  3D scanners also exist that can scan an object and create a 3D positive space model.\nThe model creation process 118 takes information from sensors 112 and creates the positive and/or negative space models as previously described.\nIn operation, a user 102 will view an object 122 with the AR device 104.  In some embodiments, an image or other representation of the object 122 is used to retrieve a model of the object 122 from model store 120.  The model of the object can be\nused in rendering the object 122, in some embodiments.  Additionally, or alternatively, the model can be used to retrieve negative space models as described below.  The rendering system 106 will render the object 122 if necessary or will simply allow the\nobject to be seen through the lens/screen of the AR device 104.\nA user will indicate through gesture or other command that the user wishes to see inside the object.  The gesture can indicate the portion of the object that the user wishes to remove so the inside can be seen.  For example, a doctor may\nindicate that she wishes to see inside the user's chest area or a user may indicate that they wish to see inside the upper portion of a radiator of a car.  Thus, the gesture allows the rendering system 106 to identify the positive space portion that\nshould be removed.  Furthermore, as discussed in greater detail below, the rendering system 106 can identify and reconstruct the negative space that should be rendered as the rendering system 106 removes the positive space portion.\nThe rendering system 106 sends information to the search service 116 and retrieves the negative space portion to be rendered.  Once the negative space portion is received, the rendering system 106 removes the positive space portion, if needed,\nand renders the negative space portion.  The rendered object is presented to the user 102 through the AR device 104.\nAn embodiment using a VR system instead of an AR system would work in much the same way except the VR system would render the object 122 in the VR world instead of allowing the object 122 to be displayed or viewed through the screen, lens, etc.\nof the AR system.  This may entail also retrieving a model of the object 122 itself, such as through the search service 116.\nFIG. 2 illustrates an example interaction diagram 200 between a rendering system 204 and a search service 206.  This diagram describes in greater detail how the architecture in FIG. 1 works.  In the following description, the operations may be\nperformed in different orders in different embodiments.  Additionally, some operations are optional in various embodiments and thus an embodiment may comprise fewer than all the operations illustrated in various combinations.\nThe user 202 views the object through the VR/AR device.  In operation 210 the rendering system receives an object representation from the user/AR device 202.  The representation describes or illustrates the object to the rendering system.  For\nexample, a representation can be a 2D or 3D image of the object, such as might be taken by sensors either in the AR/VR system or from another source (camera, etc.).  A representation can also describe the object in an AR system.  For example, in an AR\nsystem an image of the object is not presented to the user.  Instead, the user directly views the object through the lens/screen of the AR system.  In this case a representation can comprise anything that describes the object, such as an outline of the\nobject, the location, scope, etc. on the lens/screen and so forth.\nThe representation of the object received in operation 210 can come from the VR/AR system 202 or can be captured by other sensors such as cameras, optical scanners, 3D printers, 3D scanners, CAT scans, MRIs, ultrasound, x-ray and so forth.  In\naddition, models can also be used as representations.  Models can be created as previously described using drawing and/or modeling programs.  The VR/AR system 202 can work in conjunction with these sources to produce the representation.\nOperation 212 identifies the object, or identifies information about the object that can be used to retrieve information needed to render the object (operation 216).  In some embodiments, this is the representation, such as an image.  In other\nembodiments, this is a description, an identifier, or other information that identifies the object.  In some embodiments, this operation is optional, as no identification other than the representation is used for operations such as retrieving models from\nmodel store 208.\nIn some embodiments, identifier(s) from operation 212 and/or the representation is used to retrieve a model of the object from the model store 208.  In these embodiments, operation 214 sends information to the search service 206 and the search\nservice 206 retrieves a model of the object from model store 208 (i.e., as illustrated in operation 228).  Search services can take identifiers such as key words, a description, an image and so forth and use the identifier to retrieve a model from the\nmodel store 208.  This functionality is known and implemented in many if not most search services and need not be repeated here.\nThe retrieved model of the object can be used, for example, in rendering the object (operation 216) and for other purposes described below.  In AR systems, a model may not be needed to render an object as the object tends to be directly viewed\nby the user as previously discussed.  In VR systems, the retrieved model can be used to render the object in the virtual world of the VR system.\nThe result of the prior operations is that the user views the object via the VR/AR device 202.  In addition to the object, the system can project additional information about the object to the user.  When the user desires to explore negative\nspace of the object, the user uses a gesture or other command to indicate that the rendering system 204 should render negative space of some part of the object.  This gesture/command is received by the rendering system (operation 218) and the system\nretrieves the appropriate negative space model as indicated by operation 220.\nNegative space models can be located using the search service 206.  The search service can match the real-world positive space object to a corresponding negative space model.  For example, the identifier, image, etc. of operation 212 can be used\nto retrieve a negative space model (operation 230).  In this case the search service 206 would use the image, identifier, etc. to retrieve negative space models instead of similar positive space objects, models, images, etc. Additionally, or\nalternatively, the search service 206 can utilize a positive space model (i.e., retrieved by operation 214/228) to retrieve an appropriate negative space model.  The search service 206 can take any of this input and produce the appropriate negative space\nmodel using the known search methods.\nNegative space models can be created as previously described from modeling software, from sensor data, and/or combinations thereof.  The negative space models are stored in the model store 208 in a way that allows them to be retrieved using\nimages, identifiers, models, and so forth of the positive space object.\nWhen the negative space model is received in operation 220, the negative space model can be modified and/or combined with sensor data to match the model to the actual negative space of the object.  This can include adjusting one or more\nparameters of the negative space model.  The parameters can comprise location of various objects/features of the negative space model, size of the various objects/features of the negative space model, condition of the various objects/features of the\nnegative space model, the actual objects/features that are part of the negative space model and so forth.  These parameters can be identified by sensor data and/or scanning data that can sense the internal negative space of the object.  As previously\ndiscussed, ultrasound, MRI, CAT scan, infrared, x-ray and so forth can be used to sense the negative space of an object, and identify one or more of the listed parameters.\nAs examples of parameter sensing, sensor data can identify such things as the location, size, condition of various internal organs, spaces and so forth of a human body.  The same and/or similar technology can be used to sense the location,\ncondition and so forth of the internals of non-human objects.  As discussed above, the pistons and piston cylinder of an engine can be modified to account for wear, damage and other conditions that are sensed by sensors, scanners, and so forth.  Negative\nspace models can comprise positive space objects as well as voids/spaces.  The negative space model can be adjusted by modifying the location, size, existence, condition, and so forth of the positive space objects and voids that comprise the negative\nspace objects.\nOnce the rendering system 204 retrieves the negative space model and (optionally) modifies the negative space model to account for the actual condition, location, size, existence and so forth of the various aspects of the negative space model to\nreflect the parameters of the negative space of the object, the negative space model is combined with the positive space object (operation 222) and rendered (operation 224).  The combination of a negative space model and positive space object is\ndiscussed in greater detail below.  As a summary, the negative space model is rotated, scaled, and so forth as needed and anchored to the positive space model in the appropriate location, orientation and scale.\nThe combination of positive space object and negative space model of operation 222 depends on the VR/AR system being used.  For example, if the system is a VR system that utilizes a 3D model of the object rendered in a VR world, then operation\n222 will combine the negative space model and the positive space object model.  This can comprise removing the portions of the positive space object model that would otherwise hide the negative space model and the negative space model is anchored to the\nappropriate location at the correct orientation and scale and the positive space and negative space models are rendered as a single object.  Known graphic rendering algorithms can be used to render the combined positive and negative space models.\nIf the system is an AR system that allows direct view of the object through the lens/screen of the AR system, then one of two approaches can be taken.  The first approach is to replace the real-world object with a positive space model and\nproceed as if the system were a VR system (described above).  In this option, the model would be placed so that it covers the real-world object and the real-world object would be hidden behind the rendered combined negative and positive space models. \nAlternatively, the negative space object can be combined with the real-world object so that the negative space model is rendered overlaid on the actual positive space object.  This is discussed in greater detail in conjunction with FIGS. 3-4 below.\nOnce the object is rendered with the negative space model (operation 224), the system tracks the user point of view (POV) (operation 226), the room lighting, and other information to keep the object and negative space model rendered from the\nproper point of view and in the proper lighting so that the negative space model stays in an appropriate relationship with the object.\nIn the case of a VR system, the entire model (positive space object and negative space model) is virtual so the VR system can allow the user to interact directly with the positive and negative space models.  In the case of an AR system, the\nactual positive space object exists and so if the user attempts to manipulate the negative space directly, the real-positive space may interfere.  For example, if an engine is rendered without the heads and a negative space model of the pistons,\ncylinders and so forth rendered in its place, if the user attempts to touch one of the pistons of the negative space, the heads, which still exist even if they have been removed in the rendering, will block the user's hands.  Thus, in an AR system,\ninteractions with the negative space model are generally performed via gestures and so forth that do not actually touch the virtual model in a real sense.  Virtual touches and/or other interactions are accommodated through gestures, virtual hands,\ncursors, instruments (i.e., tools, surgical instruments, etc.) and so forth.\nAs the user interacts with the positive space and negative space models, the state of the models is updated to reflect the manipulation/interaction with the user.  The updated models can also be saved and/or stored in a store (model store 208 or\nother data store).\nFIG. 3 illustrates an example flow diagram 300 for rendering negative space.  The flow diagram starts at operation 302 and execution proceeds to operation 304 where the object is identified.  As previously discussed, this can comprise accessing\nan image or other description/identifier of the object that the search service can use to retrieve an associated appropriate negative space model.  Additionally, or alternatively, operation 304 can identify which portion of the object the user desires to\nsee inside.\nA single object may have one or more negative space models associated therewith.  For example, if the object is a human body, there can be negative space models for the various parts and areas of the body.  If the object is a vehicle, there are\nmany parts/subassemblies inside of the vehicle and each part/subassembly may, in turn, have negative space models associated therewith.  Thus, operation 304 can identify which portion of the positive space object the user desires to see inside, i.e., as\nindicated by the gesture/command of the user.  Furthermore, the orientation of the object may dictate what portion of the object the user wishes to see inside.\nOnce the rendering system has identified the object and/or which portion of the object the user desires to see inside, the appropriate negative space model is identified and/or retrieved as illustrated in operation 306.  Thus, the system can\nsend to the search service an indicator of the object (image, identifier, etc.) as well as an indication of the orientation, portion selected, and so forth to facilitate retrieval of the proper negative space model.  As noted above, the object image,\ndescription, identifier, orientation, etc. can be used to retrieve associated negative space model(s), such as from the search service previously discussed.\nOperation 308 removes the portion of the positive space object that will interfere with the rendering of the negative space model during rendering.  As discussed above, in some instances, there is no need to remove any aspect of the positive\nspace object and the negative space model can be overlaid on the positive space object.  This approach can be used whenever rendering the negative space model will appropriately obscure the portions of the positive space object that would otherwise hide\nthe negative space model.  Stated another way, the portions of the positive space object will not obscure the rendered negative space model.  Whether the portions of the positive space object will obscure the rendered negative space model can be\ndetermined by placing the negative space model at the anchor points (see below) and using vector geometry to determine whether positive space object features would obscure the negative space model for an observer at the POV of the user.\nIf portions of the positive space object will obscure the rendered negative space model, the portions of the object that would obscure the negative space model can be removed.  Removing portions of the positive space object is accomplished\ndifferently in a VR system and an AR system.  In the VR system, the positive space model of the object is modified to remove the desired portions of the object.  Since the positive space model resides entirely in a virtual world, there is no physical\naspect of the object that must be obscured or otherwise removed.  The model can be adjusted by using vector geometry from the POV of the user to identify which portions of the positive space object model will obscure the negative space model.  Using a\ncommon example in the disclosure, the heads, valve covers and other parts can be removed from a positive space model of an engine so the negative space model showing the pistons and cylinders can be rendered without interference.\nIn an AR system, the object is directly visible through the screen/lens of the AR system.  Thus, removing a portion of the object is performed by rendering something over the portion of the object to be removed in such a way that the portion is\nobscured.  One way to do this is to identify what is on the \"other\" side of the object and to render the scene on the other side on top of the portion of the object.  For example, if an engine is sitting on engine stand and the valve covers and heads are\nto be removed, the rendering system can receive a view of what would otherwise be visible to the user in the environment if those parts were removed.  The view is then rendered on top of the portion of the object, which will cause the object to\n\"disappear\" from the object since the AR system will present what the user would see if the portion of the object was not there.\nThe view of what is on the other side of the object can be obtained in several ways.  If there are sensors in the environment, the sensors can identify what the environment looks like and the data can then be used to construct a model of the\nenvironment.  Additionally, or alternatively, the AR system can be used to capture a view of the environment and the model be constructed based on what the AR system sees.  In yet another example, what the AR system sees from the current POV of the user\ncan be used to extrapolate what is behind the portion to be removed.  Graphic extrapolation method typically clone what is visible and extend it to what would be \"behind\" the portion.  In other words, the area surrounding the portion to be removed is\nreplicated and rendered on top of the portion to be removed.  During the replication process a blur and/or other filter can be applied to randomize color and texture of the replicated portion so the user's eye will not discern the replication.\nOperation 310 adjusts one or more parameters in the negative space model to make the negative space more representative of the actual negative space of the object.  As discussed above, this may entail adjusting one or more of location, size,\norientation, existence condition, and so forth of the negative space model aspects.  Thus, if the negative space model contains something that does not exist in the negative space of the object, the non-existent thing can be removed.  Similar adjustments\ncan be made for location, size, orientation, condition and so forth.  The parameters to be adjusted are identified through scans and sensor data that yield information about the negative space of the object as previously described.  In some embodiments,\noperation 310 is not performed.\nOperation 312 identifies anchor points on the negative space model and on the object in order to know how to orient and scale the negative space model for rendering.  the anchor points how to align the negative space model with the object.  In\n3D space, a plurality of anchor points is used to determine how to anchor the model to the object.  Three points define a plane and thus often three or more anchor points are used.  However, fewer anchor points can be used when there is no ambiguity on\nthe orientation and scaling of the negative space model relative to the object.\nThe negative space models that are associated with an object can comprise one or more anchor points that define the relationship between the object and the negative space model.  These anchor points can be encoded into the negative space model\nor can be stored as separate metadata associated with the negative space model.  Anchor points can be created when the model is created.  If the negative space model is crated via scan and/or sensor data, the sensor data illustrates how the negative\nspace model is related to the object.  In this situation, the model creation process, such as that described above, can identify features in the object and negative space model where anchor points can be placed.  Features that make good candidates for\nanchor points include features that are easily identifiable (i.e., in the model, object, sensor data, etc.), features that define natural points (i.e., corners, intersection of features, etc.), features that are unambiguous/unique, how well the feature\nties into the positive space object (i.e., is the corresponding anchor point on the object easily identifiable, a natural point, unambiguous, etc.), and so forth.  When the negative space model is created, edge detection and/or feature detection methods\ncan be used to identify anchor points.  When multiple anchor points are available, the anchor points can be ranked based on criteria such as how easy they are to identify, distance that separates a potential anchor point from other anchor points, and so\nforth as described.  The top N points can then be selected.  If multiple criteria are used, the criteria can be combined into a combined score, such as by using a weighted sum.  The anchor points can then be sorted based on the score and the top N anchor\npoints selected.\nOnce the anchor points on the negative space model and corresponding anchor points on the object are identified, the system can scale, rotate, or perform other transformation on the negative space model so that the anchor points on the negative\nspace model and corresponding anchor points on the object align.  This process is illustrated in operation 314.  Rotation, scaling and so forth are standard operations that can utilize known methods to align the anchor points.\nOperation 316 renders the negative space model in the appropriate location and at the appropriate orientation and scale.  For the VR device, this rendering takes place in the virtual world using the methods used by the VR device to render\nobjects.  For the AR device, the negative space model is rendered on the object as viewed through the screen/lens of the device so that as the user views the object, the negative space model is superimposed on the object.\nOperation 318 tracks the user POV, user gestures, user commands, user location and so forth and re-renders the negative space model as the user moves around, changes their POV, manipulates the object/negative space model and/or the object, and\nso forth.  These algorithms are part of the AR/VR device and rendering system and no special accommodation need be made except to render the negative space model and/or object to maintain alignment of the anchor points.  In the case of the AR device, the\nrendering must also ensure that any parts that have been removed (i.e., through rendering a background on them to `erase` them from user view) stay removed by re-rendering the background that the user would see if the portion of the object were removed\nfrom the object.\nOperation 320 can update the negative space model and/or the object as the user moves POV, manipulates the object/negative space model and so forth.\nFIG. 4 illustrates an example of rendering negative space.  The sequence 400 shows what an object might look like if the rendering system rendered the object and/or negative space model in a step by step process.\nThe object of FIG. 4 comprises a top 402 and a lower portion 404.  In this example, the top 402 will be removed so the user can see \"inside\" the box.  As previously described, the system renders/allows direct view of the object.  The user\nindicates the top 402 should be removed to see the negative space inside.  The rendering system retrieves the negative space model 414.\nThe top 402 is removed from the object as described above and as illustrated in the figure.  Anchor points 406, 408, 410 and 412 are identified on the lower portion 404.  Anchor points 416, 418, 420 and 422 are also identified on the negative\nspace model 414 as described above.\nThe respective anchor points are aligned and the negative space model 414 is rendered on the lower portion 404 as illustrated.\nFIG. 5 illustrates another example of rendering negative space.  This figure represents a healthcare scenario where a cardiologist is working with a patient 502 for a heart transplant.  Using an AR device, the patient 502 would be visible\ndirectly through the screen/lens of the AR device by the cardiologist.\nThe cardiologist can indicate through a gesture, command, etc. that the cardiologist wishes to view inside the patient's 502 chest cavity (i.e. the negative space).  The rendering system knows the patient 502 that the cardiologist is working\nwith either through facial recognition, through the cardiologist entering the patient's name, or through another mechanism.  Scan and other sensor data can be used to create a model of the inside of the patient's chest cavity as previously described,\neither by creating a model using the scan/sensor data or by using the scan/sensor data to modify a more general negative space model to create the modal unique to the patient 502.  Thus, there is a negative space model that is tailored to the patient\n502.\nThe rendering system retrieves the appropriate negative space model, aligns the anchor points and renders the negative space model 504 at the appropriate location, giving the cardiologist the ability to see inside the patient's chest cavity. \nBecause the model has been tailored to the patient 502, the cardiologist can see the volume which the transplanted heart will need to fit into.  She can also see the surrounding organs, their placement, condition, size and so forth.  She can rotate, zoom\nand otherwise explore the negative space model to assess the situation she will be facing in the transplant.  Furthermore, the cardiologist can `remove` the patient's heart from the negative space model to identify the volume, surrounding tissues and so\nforth where the transplanted heart will be located.\nThrough the AR device, the cardiologist can ask the system to retrieve a positive space model of a heart 506.  Through gestures, commands (voice or other), and so forth the cardiologist can place the heart model 506 into the negative space model\n504.  As the heart model 506 is placed into the negative space model, the cardiologist can rotate, move and otherwise manipulate the heart in the negative space model.  Through a gesture, command, etc. the cardiologist can also connect the veins,\narteries, and so forth to see how the hear looks in place and whether there will be any unforeseen challenges with the transplant.\nThe negative space model 504 and positive space model 506 can be retrieved through the search service as previously described.\nFIG. 6 illustrates an example flow diagram 600 for placing positive space objects into negative space model.  This would work, for example, in the scenario described in FIG. 5 where a positive space model (heart) is placed in the negative space\nmodel (chest cavity).  Operations 602 through 616 work the same as the corresponding operations in FIG. 3 work and their functions and methods need not be repeated here.\nWhen operation 618 is reached, the negative space model that corresponds to the object has been identified, retrieved, scaled, rotated, and anchored at the proper location in the object.  Operation 618 determines whether the user would like to\ninsert to insert a positive space model into the negative space model.  If so, the \"YES\" branch is taken out of operation 618 to operation 622.\nOperation 622 identifies and retrieves the appropriate positive space model, such as through the search service previously described.  The positive space model may be identified by the user, such as with a gesture, command, etc. that indicates a\npositive space model should be retrieved.  The user can similarly indicate which positive space model should be retrieved.  Additionally, or alternatively, the rendering system can identify a corresponding model by evaluating the object and/or negative\nspace model.  Thus, if the object is an engine of a particular type and the negative space is of the crankshaft area, the rendering system could request a positive space model of a crankshaft of the type that fits into the engine.\nOperation 624 adjusts one or more parameters of the positive space model to account for existence, condition, size, shape, etc. as discussed above in conjunction with modifying negative space models.  The parameters can be adjusted based on\nsensor and/or scan data as previously described.\nOperation 626 identifies anchor points on the positive space model and corresponding anchor points on the negative space model in a manner similar to that previously described.  Operation 628 scales, rotates and otherwise changes the size,\norientation and so forth of the positive space model so that the anchor points on the positive space model align with the anchor points in the negative space model.\nOperation 630 updates the negative space model for inclusion of the positive space model and execution proceeds to operation 620 where the negative space model, object and positive space model are re-rendered to include the positive space model.\nExample Machine Architecture and Machine-Readable Medium\nFIG. 7 illustrates a representative machine architecture suitable for implementing the systems and so forth or for executing the methods disclosed herein.  The machine of FIG. 7 is shown as a standalone device, which is suitable for\nimplementation of the concepts above.  For the server aspects described above a plurality of such machines operating in a data center, part of a cloud architecture, and so forth can be used.  In server aspects, not all of the illustrated functions and\ndevices are utilized.  For example, while a system, device, etc. that a user uses to interact with a server and/or the cloud architectures may have a screen, a touch screen input, etc., servers often do not have screens, touch screens, cameras and so\nforth and typically interact with users through connected systems that have appropriate input and output aspects.  Therefore, the architecture below should be taken as encompassing multiple types of devices and machines and various aspects may or may not\nexist in any particular device or machine depending on its form factor and purpose (for example, servers rarely have cameras, while wearables rarely comprise magnetic disks).  However, the example explanation of FIG. 7 is suitable to allow those of skill\nin the art to determine how to implement the embodiments previously described with an appropriate combination of hardware and software, with appropriate modification to the illustrated embodiment to the particular device, machine, etc. used.\nWhile only a single machine is illustrated, the term \"machine\" shall also be taken to include any collection of machines that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the\nmethodologies discussed herein.\nThe example of the machine 700 includes at least one processor 702 (e.g., a central processing unit (CPU), a graphics processing unit (GPU), advanced processing unit (APU), or combinations thereof), one or more memories such as a main memory\n704, a static memory 706, or other types of memory, which communicate with each other via link 708.  Link 708 may be a bus or other type of connection channel.  The machine 700 may include further optional aspects such as a graphics display unit 710\ncomprising any type of display.  The machine 700 may also include other optional aspects such as an alphanumeric input device 712 (e.g., a keyboard, touch screen, and so forth), a user interface (UI) navigation device 714 (e.g., a mouse, trackball, touch\ndevice, and so forth), a storage unit 716 (e.g., disk drive or other storage device(s)), a signal generation device 718 (e.g., a speaker), sensor(s) 721 (e.g., global positioning sensor, accelerometer(s), microphone(s), camera(s), and so forth), output\ncontroller 728 (e.g., wired or wireless connection to connect and/or communicate with one or more other devices such as a universal serial bus (USB), near field communication (NFC), infrared (IR), serial/parallel bus, etc.), and a network interface\ndevice 720 (e.g., wired and/or wireless) to connect to and/or communicate over one or more networks 726.\nExecutable Instructions and Machine-Readable Medium\nThe various memories (i.e., 704, 706, and/or memory of the processor(s) 702) and/or storage unit 716 may store one or more sets of instructions and data structures (e.g., software) 724 embodying or utilized by any one or more of the\nmethodologies or functions described herein.  These instructions, when executed by processor(s) 702 cause various operations to implement the disclosed embodiments.\nAs used herein, the terms \"machine-readable medium,\" \"computer-readable medium\" and \"device-readable medium\" mean the same thing and may be used interchangeably in this disclosure.  The terms include a single medium or multiple media (e.g., a\ncentralized or distributed database, and/or associated caches and servers) that store the one or more instructions or data structures.  The terms shall also be taken to include any tangible medium that is capable of storing, encoding or carrying\ninstructions for execution by the machine and that cause the machine to perform any one or more of the methodologies of the present invention, or that is capable of storing, encoding or carrying data structures utilized by or associated with such\ninstructions.  The terms shall accordingly be taken to include, but not be limited to, solid-state memories, and optical and magnetic media.  Specific examples of machine-readable media, computer-readable media and/or device-readable media include\nnon-volatile memory, including by way of example semiconductor memory devices, e.g., erasable programmable read-only memory (EPROM), electrically erasable programmable read-only memory (EEPROM), and flash memory devices; magnetic disks such as internal\nhard disks and removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.  The terms machine-readable media, computer-readable media, and device-readable media specifically exclude non-statutory signals per se, which are covered under the term\n\"signal medium\" discussed below.\nSignal Medium\nThe term \"signal medium\" shall be taken to include any form of modulated data signal and signals per se.  The term \"modulated data signal\" means a signal that has one or more of its characteristics set or changed in such a matter as to encode\ninformation in the signal.\n<BR><BR>EXAMPLE EMBODIMENTS\n<BR><BR>Example 1\nA method comprising:\nreceiving a representation of a real-world object;\nretrieving, based on the representation, a negative space model for the real-world object;\nidentifying a plurality of anchor points on the negative space model and on the representation of the real-world object;\nblanking out a portion of the representation of the real-world object;\naligning the plurality of anchor points on the negative space model and on the representation and overlaying the negative space model onto the representation; and\nrendering the negative space model and the representation.\n<BR><BR>Example 2\nThe method of example 1, further comprising:\nsubmitting the representation to a search service; and\nreceiving the negative space model from the search service.\n<BR><BR>Example 3\nThe method of example 1, further comprising modifying a parameter of the negative space model to match the parameter to real-world negative space of the real-world object.\n<BR><BR>Example 4\nThe method of example 1, further comprising:\nretrieving a positive space model;\nidentifying second anchor points in the positive space model and in the negative space model; and\nrendering the positive space model within the negative space model such that the anchor points are aligned.\n<BR><BR>Example 5\nThe method of example 1, further comprising scanning the real-world object to make a three-dimensional model of the real-world object.\n<BR><BR>Example 6\nThe method of example 1, further comprising:\nscaling and rotating the negative space model to align the anchor points.\n<BR><BR>Example 7\nThe method of example 1, further comprising:\ntracking the point of view of a user via a virtual reality or augmented reality system; and\nre-rendering the negative space model and the representation based on the point of view to keep the negative space model aligned with the representation.\n<BR><BR>Example 8\nThe method of example 1, 2, 3, 4, 5, 6, or 7 wherein the negative space model is constructed by a scan of the real-world object.\n<BR><BR>Example 9\nA computing system comprising:\na processor and executable instructions accessible on a machine-readable medium that, when executed, cause the system to perform operations comprising:\nreceiving an image of a real-world object;\nretrieving, based on the image, a negative space model for the real-world object;\nidentifying a plurality of anchor points on the negative space model and on a representation of the real-world object;\nblanking out a portion of the representation of the real-world object;\naligning the plurality of anchor points on the negative space model and on the representation and overlaying the negative space model onto the representation and\nrendering the negative space model and the representation.\n<BR><BR>Example 10\nThe system of example 9, further comprising:\nsubmitting the image to a search service; and\nreceiving the negative space model from the search service.\n<BR><BR>Example 11\nThe system of example 9, or 10, further comprising modifying a parameter of the negative space model to match the parameter to real-world negative space of the real-world object.\n<BR><BR>Example 12\nThe system of example 9, or 10, further comprising:\nretrieving a positive space model;\nidentifying second anchor points in the positive space model and in the negative space model; and\nrendering the positive space model within the negative space model such that the anchor points are aligned.\n<BR><BR>Example 13\nThe system of example 9, or 10, further comprising scanning the real-world object to make a three-dimensional model of the real-world object.\n<BR><BR>Example 14\nThe system of example 9, or 10, further comprising:\nscaling and rotating the negative space model to align the anchor points.\n<BR><BR>Example 15\nThe system of example 9, or 10, further comprising:\ntracking the point of view of a user via a virtual reality or augmented reality system; and\nre-rendering the negative space model and the representation based on the point of view to keep the negative space model aligned with the representation.\n<BR><BR>Example 16\nThe system of example 9, or 10, wherein the negative space model is constructed by a scan of the real-world object.\n<BR><BR>Example 17\nA machine-readable medium having executable instructions encoded thereon, which, when executed by at least one processor of a machine, cause the machine to perform operations comprising:\nreceiving a representation of a real-world object;\nretrieving, based on the representation, a negative space model for the real-world object;\nidentifying a plurality of anchor points on the negative space model and on a representation of the real-world object;\nadjusting the negative space model to include the representation of a real-world aspect associated with negative space in the real-world object;\nblanking out a portion of the representation of the real-world object;\naligning the plurality of anchor points on the negative space model and on the representation and overlaying the negative space model onto the representation and\nrendering the negative space model and the representation.\n<BR><BR>Example 18\nThe machine-readable medium of example 17, further comprising:\nretrieving a positive space model;\nidentifying second anchor points in the positive space model and in the negative space model; and\nrendering the positive space model within the negative space model such that the anchor points are aligned.\n<BR><BR>Example 19\nThe machine-readable medium of example 17, further comprising:\nsubmitting the image to a search service; and\nreceiving the negative space model from the search service.\n<BR><BR>Example 20\nThe machine-readable medium of example 17, 18, or 19, further comprising:\nreceiving sensor data related to the real-world object from a plurality of sensors;\ntracking the point of view of a user via a virtual reality or augmented reality system; and\nre-rendering the negative space model and the representation based on the point of view to keep the negative space model aligned with the representation.\n<BR><BR>Example 21\nA method comprising:\nreceiving a representation of a real-world object;\nretrieving, based on the representation, a negative space model for the real-world object;\nidentifying a plurality of anchor points on the negative space model and on the representation of the real-world object;\nblanking out a portion of the representation of the real-world object;\naligning the plurality of anchor points on the negative space model and on the representation and overlaying the negative space model onto the representation; and\nrendering the negative space model and the representation.\n<BR><BR>Example 22\nThe method of example 21, further comprising:\nsubmitting the image to a search service; and\nreceiving the negative space model from the search service.\n<BR><BR>Example 23\nThe method of example 21 or 22, further comprising modifying a parameter of the negative space model to match the parameter to real-world negative space of the real-world object.\n<BR><BR>Example 24\nThe method of example 21, 22 or 23, further comprising:\nretrieving a positive space model;\nidentifying second anchor points in the positive space model and in the negative space model; and\nrendering the positive space model within the negative space model such that the anchor points are aligned.\n<BR><BR>Example 25\nThe method of example 24, further comprising:\nsubmitting an indicator to a search engine to retrieve the positive space model.\n<BR><BR>Example 26\nThe method of example 12, 22, 23, 24, or 25, further comprising scanning the real-world object to make a three-dimensional model of the real-world object.\n<BR><BR>Example 27\nThe method of example 21, 22, 23, 24, 25 or 26, further comprising:\nscaling and rotating the negative space model to align the anchor points.\n<BR><BR>Example 28\nThe method of example 21, 22, 23, 24, 25, 26 or 27 further comprising:\ntracking the point of view of a user via a virtual reality or augmented reality system; and\nre-rendering the negative space model and the representation based on the point of view to keep the negative space model aligned with the representation.\n<BR><BR>Example 29\nThe method of example 21, 22, 23, 24, 25, 26, 27 or 28, wherein the negative space model is constructed by a scan of the real-world object.\n<BR><BR>Example 30\nThe method of example 21, 22, 23, 24, 25, 26, 27, 28, or 29, further comprising:\nscaling and rotating the negative space model to align the anchor points.\n<BR><BR>Example 31\nThe method of example 21, 22, 23, 24, 25, 26, 27, 28, 29 or 30, wherein the representation is a 3D model of the object and wherein the method further comprises:\nreceiving an image of the object; and\nsubmitting the image of the object to a search service to retrieve the 3D model of the object.\n<BR><BR>Example 32\nThe method of example 21, 22, 23, 24, 25, 26, 27, 28, 29 or 30, wherein the representation comprises an image of the object.\n<BR><BR>Example 33\nThe method of example 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, or 32, wherein the representation comprises an identifier or keyword associated with the object.\n<BR><BR>Example 34\nAn apparatus comprising means to perform a method as in any preceding example.\n<BR><BR>Example 35\nMachine-readable storage including machine-readable instructions, when executed, to implement a method or realize an apparatus as in any preceding example.\n<BR><BR>CONCLUSION\nIn view of the many possible embodiments to which the principles of the present invention and the forgoing examples may be applied, it should be recognized that the examples described herein are meant to be illustrative only and should not be\ntaken as limiting the scope of the present invention.  Therefore, the invention as described herein contemplates all such embodiments as may come within the scope of the following claims and any equivalents thereto.", "application_number": "15378373", "abstract": " Representative embodiments allow rendering of negative space in real\n     world objects in a virtual reality or augmented reality system. Negative\n     space is what exists within a real-world object. Positive space is what\n     exists outside of a real-world object. Scans of a real-world object are\n     submitted to a search engine to retrieve a negative space model for the\n     positive space object. The negative space model is optionally adjusted to\n     account for real world parameters of the negative space. Anchor points\n     are identified for the negative space model. A positive space portion of\n     the real-world object is removed and the negative space model is scaled,\n     rotated and rendered on top of the real-world object at the appropriate\n     location. A user can interact with both the real-world object and\n     negative space object through the virtual reality or augmented reality\n     system.\n", "citations": ["6031519", "8643569", "20010056230", "20090102845", "20130335301", "20140043433", "20140049559", "20140210856", "20140354689", "20150084989", "20150277378", "20150363978", "20160030131", "20160166336", "20160324580", "20160364914", "20170018204"], "related": []}, {"id": "20180166072", "patent_code": "10373614", "patent_name": "Web portal declarations for smart assistants", "year": "2019", "inventor_and_country_data": " Inventors: \nSeksenov; Kiril (Seattle, WA), Mazumder; Avishek (Hyderabad, IN), Hill; Kevin (Redmond, WA), Pruthi; Aditya (Waterloo, CA)  ", "description": "<BR><BR>BACKGROUND\nA user device may execute a smart assistant module to facilitate user access to various applications on the user device.  The smart assistant module may coordinate interactions between applications for the user.  The smart assistant module may\nreduce overhead on the user device by offloading the functions of the smart assistant module to an assistant support server accessible via a data network.  The smart assistant module may forward a user request to the assistant support server for\nprocessing.\n<BR><BR>SUMMARY\nThis Summary is provided to introduce a selection of concepts in a simplified form that is further described below in the Detailed Description.  This Summary is not intended to identify key features or essential features of the claimed subject\nmatter, nor is it intended to be used to limit the scope of the claimed subject matter.\nExamples discussed below relate to maintaining a web portal to crowdsource responses to a user input.  The assistant support server may maintain a web portal accessible by a developer device.  The assistant support server may store an assistant\nrule based on a developer input associating an input word set describing a hypothetical user input with a deep link for a website.  The assistant support server may receive in the web portal the developer input.  The assistant support server may direct a\nsmart assistant module executed by a user device to connect to the deep link in response to receiving a user input from the smart assistant module matching the input word set. <BR><BR>DRAWINGS\nIn order to describe the manner in which the above-recited and other advantages and features can be obtained, a more particular description is set forth and will be rendered by reference to specific examples thereof which are illustrated in the\nappended drawings.  Understanding that these drawings depict only typical examples and are not therefore to be considered to be limiting of its scope, implementations will be described and explained with additional specificity and detail through the use\nof the accompanying drawings.\nFIG. 1 illustrates, in a block diagram, one example of a data network.\nFIG. 2 illustrates, in a block diagram, one example of a computing device.\nFIG. 3 illustrates, in a block diagram, one example of a developer interface architecture.\nFIG. 4 illustrates, in a block diagram, one example of a user device interface architecture.\nFIG. 5 illustrates, in a block diagram, one example of an assistant rule.\nFIG. 6 illustrates, in a flowchart, one example of a method of providing a deep link of a website to a web portal with a developer device.\nFIG. 7 illustrates, in a flowchart, one example of a method of receiving an assistant rule for a deep link of a website in a web portal.\nFIG. 8 illustrates, in a flowchart, one example of a method of responding to a user input to a smart assistant module at the assistant support server with a deep link to a web site.\nFIG. 9 illustrates, in a flowchart, one example of a method of responding to a user input to a smart assistant module at the user device with a deep link to a website.\nFIG. 10 illustrates, in a flowchart, one example of a method of responding to an input to a smart assistant module at the website with a network-enhanced action.\nFIG. 11 illustrates, in a flowchart, one example of a method of providing a network-enhanced action to a web portal with a developer device.\nFIG. 12 illustrates, in a flowchart, one example of a method of receiving an assistant rule for a network-enhanced action in a web portal.\nFIG. 13 illustrates, in a flowchart, one example of a method of responding to a user input to a smart assistant module at the assistant support server with a network-enhanced action.\nFIG. 14 illustrates, in a flowchart, one example of a method of responding to a user input to a smart assistant module at the user device with a network-enhanced action.\n<BR><BR>DETAILED DESCRIPTION\nExamples are discussed in detail below.  While specific implementations are discussed, it should be understood that this is done for illustration purposes only.  A person skilled in the relevant art will recognize that other components and\nconfigurations may be used without parting from the spirit and scope of the subject matter of this disclosure.  The implementations may be an assistant support server, a computing device, or a machine-implemented method.\nIn one example, an assistant support server may maintain a web portal to crowdsource responses to a user input.  The assistant support server may maintain a web portal accessible by a developer device.  The assistant support server may store an\nassistant rule based on a developer input associating an input word set describing a hypothetical user input with a deep link for a website.  The assistant support server may receive in the web portal the developer input.  The assistant support server\nmay direct a smart assistant module executed by a user device to connect to the deep link in response to receiving a user input from the smart assistant module matching the input word set.\nA user device may implement a smart assistant module, such as Sin.RTM.  or Cortana.RTM., to allow a user to interact with the user device to perform a network-enhanced action.  A network-enhanced action is an action that incorporates processing\nor data tasks executed on a different device from the user device, such as finding directions on a mapping application or access an online account.  A smart assistant module may connect to an assistant support server via a data network.  The smart\nassistant module may offload various processing tasks of the smart assistant module to the assistant support server, allowing the user device to perform tasks beyond the resource capability of the user device alone.  For example, the smart assistant\nmodule may process a natural language query from a user, either verbal or textual, by sending the natural language query to the assistant support server.\nThe assistant support server may have a set of assistant rules describing network-enhanced actions in response to a variety of user inputs.  A programmer for the smart assistant module may have difficulty formulating a network-enhanced action\nfor every possible user input.  To better develop a comprehensive set of assistant rules, the assistant support server may establish a web portal to crowdsource the development of the assistant rules.\nA developer of a website may access the web portal to submit an input word set describing a hypothetical user input.  The developer may then propose a network-enhanced action for the browser to execute in response to a user input matching the\ninput word set.  Further, rather than proscribing a specific action to the smart assistant module, a developer may specify a deep link on the website to direct the user device to access.  The deep link may then perform the network-enhanced action for the\nuser device, further offloading resource draining activities from the user device.  The assistant support server may limit application of an assistant rule to when the user has specifically invoked the domain for the website associated with the deep\nlink.\nEven when the developer submits just the deep link to access in response to a user input matching the input word set, the assistant support server may extrapolate a network-enhanced action for broader applicability.  The smart assistant module\nmay track the actions taken at the deep link in response to a user input.  The smart assistant module may then report those actions back to the assistant support server.  The assistant support server may then update the assistant rule to reflect the\nnetwork-enhanced action.\nFIG. 1 illustrates, in a block diagram, one example of a data network 100.  A user device 110 may implement a browser 112 to access a domain 120 of a website via a data network connection 130.  The user device 110 may be a desktop computer, a\nworkstation, a smart phone, a tablet, a smart watch, or other computing device capable of executing a browser 112.  The domain 120, such as \"jcpenney.com\", may be hosted on a server or server farm.  The domain 120 may have one or more deep links 122\nindicating virtual locations within the domain 120, such as \"jcpenney.com/shoes\", accessible with the browser 112.  The data network connection 130 may be an enterprise network connection, an Internet connection, a wide area network connection, a local\narea network connection, a mobile telephone network, or other type of data network connection.\nThe user device 110 may execute a smart assistant module 114, such as Siri.RTM.  or Cortana.RTM., to coordinate user access to the applications of the user device 110, such as the browser 112.  The smart assistant module 114 may connect to an\nassistant support server 140 via the data network connection 130.  The smart assistant module 114 may offload specific functions to the assistant support server 140 to increase the processing, data, and memory resources available to the user device 110. \nThe smart assistant module 114 may receive a user input from the user.  The user input may be a verbal input received via a microphone or a text input received via a hardware keypad or a virtual keypad.  The smart assistant module 114 may forward the\nuser input to the assistant support server 140 for processing.  The assistant support server 140 may convert a verbal input into a text input.  The assistant support server 140 may determine the intent of the user input to generate an appropriate\nresponse.  The assistant support server 140 may consult an assistant rule proscribing an action for the smart assistant module 114 to take in response to a specified user input.  The assistant support server 140 may generate a user directive to send to\nthe smart assistant module 114 proscribing the action for the smart assistant module 114.  The smart assistant module 114 may receive the user directive and perform the action based on the user directive.\nFIG. 2 illustrates a block diagram of an exemplary computing device 200 which may act as an assistant support server or a user device.  The computing device 200 may combine one or more of hardware, software, firmware, and system-on-a-chip\ntechnology to implement an assistant support server or a user device.  The computing device 200 may include a bus 210, a processing core 220, a memory 230, a data storage 240, an input device 250, an output device 260, and a communication interface 270. \nThe bus 210, or other component interconnection, may permit communication among the components of the computing device 200.\nThe processing core 220 may include at least one conventional processor or microprocessor that interprets and executes a set of instructions.  The processing core 220 may be configured to maintain a web portal accessible by a developer device. \nThe processing core 220 may be further configured to authenticate a developer identifier associated with a website.  The processing core 220 may be also configured to generate a rule weight based on a developer weight.  The processing core 220 may be\nadditionally configured to adjust a rule weight based on at least one of a user response and a secondary developer proposal.  The processing core 220 may be further configured to convert a verbal user input from a user device to a text user input.  The\nprocessing core may be also configured to parse a domain identified in the user input.  The processing core 220 may be additionally configured to calculate a confidence score based on a comparison of the user input and the input word set.\nThe memory 230 may be a random access memory (RAM) or another type of dynamic data storage that stores information and instructions for execution by the processing core 220.  The memory 230 may also store temporary variables or other\nintermediate information used during execution of instructions by the processing core 220.  The memory 230 may be configured to store an assistant rule based on a developer input associating an input word set describing a hypothetical user input with a\ndeep link for a website.  The memory 230 may be further configured to associate a domain for the website with a developer identity.  The memory may be also configured to associate a developer identifier with a developer weight indicating relative\nprominence of the developer.  The memory 230 may be additionally configured to associate the assistant rule with a rule weight describing a rule applicability likelihood.  The memory 230 may be further configured to associate a network-enhanced action\nperformed by the deep link with the input word set in the assistant rule.\nThe data storage 240 may include a conventional ROM device or another type of static data storage that stores static information and instructions for the processing core 220.  The data storage 240 may include any type of tangible\nmachine-readable medium, such as, for example, magnetic or optical recording media, such as a digital video disk, and its corresponding drive.  A tangible machine-readable medium is a physical medium storing machine-readable code or instructions, as\nopposed to a signal.  Having instructions stored on computer-readable media as described herein is distinguishable from having instructions propagated or transmitted, as the propagation transfers the instructions, versus stores the instructions such as\ncan occur with a computer-readable medium having instructions stored thereon.  Therefore, unless otherwise noted, references to computer-readable media/medium having instructions stored thereon, in this or an analogous form, references tangible media on\nwhich data may be stored or retained.  The data storage 240 may store a set of instructions detailing a method that when executed by one or more processors cause the one or more processors to perform the method.  The data storage 240 may also be a\ndatabase or a database interface for storing an assistant rule.\nThe input device 250 may include one or more conventional mechanisms that permit a user to input information to the computing device 200, such as a keyboard, a mouse, a voice recognition device, a microphone, a headset, a touch screen 252, a\ntouch pad 254, a gesture recognition device 256, etc. The output device 260 may include one or more conventional mechanisms that output information to the user, including a display screen 262, a printer, one or more speakers 264, a headset, a vibrator,\nor a medium, such as a memory, or a magnetic or optical disk and a corresponding disk drive.\nThe communication interface 270 may include any transceiver-like mechanism that enables computing device 200 to communicate with other devices or networks.  The communication interface 270 may include a network interface or a transceiver\ninterface.  The communication interface 270 may be a wireless, wired, or optical interface.  The communication interface 270 may be configured to receive a login credential associated with a website from a developer device.  The communication interface\n270 may be further configured to receive in a web portal a developer input.  The communication interface 270 may be also configured to receive a user input from the user device.  The communication interface 270 may be additionally configured to send a\nuser directive to instruct a smart assistant module executed by a user device to connect to a deep link of the website in response to receiving a user input from the smart assistant module matching an input word set.  The communication interface 270 may\nbe further configured to receive a user report to track at the deep link a network-enhanced action performed in response to the user input.  The communication interface 270 may be also configured to send a user directive to instruct the smart assistant\nmodule to perform a network-enhanced action in response to the user input.\nThe computing device 200 may perform such functions in response to processing core 220 executing sequences of instructions contained in a computer-readable medium, such as, for example, the memory 230, a magnetic disk, or an optical disk.  Such\ninstructions may be read into the memory 230 from another computer-readable medium, such as the data storage 240, or from a separate device via the communication interface 260.\nFIG. 3 illustrates, in a block diagram, one example of a developer interface architecture 300.  A developer associated with a domain of a website may operate a developer device 310.  The developer device 310 may execute a webmaster toolkit 312\nto access a web portal 320 connected to the assistant support server.  The developer device 310 may provide to the web portal 320 a login credential identifying the developer and the associated domain.  The web portal 320 may use the login credential to\nauthenticate the developer is associated with the identified domain.  The developer device 310 may use the webmaster toolkit 312 to submit an assistant rule to the web portal 320.  The assistant rule may associate an input word set matching a user input\nreceived from a user device with a deep link at the domain.\nThe web portal 320 may provide a backend 322 for interfacing with the developer and a frontend 324 for interfacing with the user device.  The backend 322 may associate a domain uniform resource locator (URL) 326 with one or more deep link\nuniform resource locators 328.  The backend 322 may store an assistant rule for a domain received from a developer device 310 associated with that domain.\nFIG. 4 illustrates, in a block diagram, one example of a user device interface architecture 400.  A user device 410 may execute a smart assistant module 412 to provide access to various application to the user.  The user device 410 may receive a\nuser input, such as a verbal user input or a text user input, from the user.  The user device 410 may connect to an assistant support server 420 executing a frontend interface 422 via a data network.  The assistant support server 420 may interface with a\nbackend 424 storing one or more assistant rules submitted by domain developers.  The smart assistant module 412 may send the user input to the assistant support server 420 for processing.  The smart assistant module 412 may indicate to the frontend\ninterface 422 if the user device 410 is executing a browser that is currently accessing a domain 430.  Alternately, the frontend interface 422 may parse the user input to identify a domain contained therein.  The frontend interface 422 may compare the\nuser input to one or more input word sets 426 in the assistant rules provided by the domain developers.  The frontend interface 422 may send a user directive to the smart assistant module 412 proscribing a course of action based on the user input.\nBased on the user directive, the smart assistant module 412 may invoke a browser module 414 on the user device 410 to access a virtual network location.  Alternately, the smart assistant module 412 may perform an action described by the user\ndirective by invoking a different application, such as a domain specific application.  The browser module 414 may access a deep link 432 on the domain 430 indicated in the user directive.  Alternately, if the frontend interface 422 did not find a match\nfor the user input, the browser module 414 may provide the user input to a search engine 440 to find a virtual network location that responds to the user input.\nFIG. 5 illustrates, in a block diagram, one example of an assistant rule 500.  The assistant rule 500 may have a developer identifier (ID) 510 identifying the developer providing the assistant rule 500.  The developer identifier 510 may have an\nassociated developer weight 512.  The developer weight 512 is a value indicating a relative prominence of the developer.  The assistant rule 500 may associate a domain field 520 with the developer identifier 510 indicating the website produced by the\ndeveloper.\nThe assistant rule 500 may have an input word set field 530 listing a set of one or more input words 532 that may be hypothetically provided by the user to the smart assistant module.  The assistant rule 500 may associate a word weight 534 with\neach individual word 532.  The word weight 534 is a value indicating the relative value of a word 532 in the input word set field 530 in comparison to the other words 532 in the input word set.  The word weight 534 may be dynamic based on the proximity\nof the word 532 to other words 532 in the input word set, so that certain words 532 may have a greater word weight 534 when closer to specific other words 532.  The assistant support server may use the word weight 532 to assist in comparing the input\nword set to a user input from the smart assistant module.  The assistant support server may factor the word weight 532 when calculating a confidence score that the user input matches the input word set.\nThe assistant rule 500 may associate the input word set field 530 with a deep link 540.  The assistant support server may direct a smart assistant module to access the deep link 540 upon receiving a user input that matches the input word set. \nThe assistant rule 500 may associate the input word set field 530 with an action field 550 describing an action to be taken by the smart assistant module, such as accessing a user account, finding a geographic location, or calling a telephone number. \nThe developer may describe the action when submitting the assistant rule 500.  Alternately, the assistant support server may track an action performed by the domain at the deep link 540 and add a description of the action to the assistant rule 500.\nThe assistant rule 500 may have a rule weight 560.  The rule weight 560 is value indicating the relative probability of the assistant rule 500 being used.  The assistant support server may generate the rule weight 560 based on the developer\nweight 512.  The assistant support server may use the rule weight 560 to identify an assistant rule 500 if a user input matches more than one assistant rule 500.  The assistant support server may adjust the rule weight 560 based on a successful\ninteraction with the smart assistant module.  For example, if the user resubmits a request, the assistant support server may lower the rule weight 560.\nFIG. 6 illustrates, in a flowchart, one example of a method 600 of providing a deep link to a web portal with a developer device.  The developer device may submit a login credential for the developer to the assistant support server (Block 602). \nThe developer device may receive authentication from the assistant support server (Block 604).  The developer device may submit an associated domain to the assistant support server (Block 606).  The developer device may receive an input word set from the\ndeveloper (Block 608).  The developer device may receive a word weight distribution for the input word set from the developer (Block 610).  The developer device may receive from the developer a deep link for a website associated with the input word set\n(Block 612).  The developer device may generate a developer input describing the relationship between the input word set and the deep link (Block 614).  The developer device may submit the developer input to the assistant support server (Block 616).\nFIG. 7 illustrates, in a flowchart, one example of a method 700 of receiving an assistant rule for a deep link in a web portal.  An assistant support server may maintain a web portal accessible by a developer device (Block 702).  The web portal\nmay receive a login credential associated with a website from a developer device (Block 704).  The web portal may authenticate a developer identifier in the login credential associated with the website (Block 706).  The web portal may identify a domain\nfor the website associated with the developer identifier (Block 708).  The web portal may associate a developer identifier with the developer weight indicating relative prominence (Block 710).\nThe web portal may receive in the web portal a developer input associating an input word set describing a hypothetical user input with a deep link for the website (Block 712).  The assistant support server may identify the input word set (Block\n714).  The assistant support server may identify a word weight for each word in the input word set (Block 716).  The assistant support server may identify a deep link for the website (Block 718).\nThe assistant support server may generate an assistant rule associating the input word set with the deep link (Block 720).  The assistant support server may generate a rule weight describing a rule applicability likelihood based on a developer\nweight indicating relative prominence (Block 722).  The assistant support server may associate the assistant rule with a rule weight describing a rule applicability likelihood (Block 724).  The assistant support server may adjust the rule weight based on\na secondary developer proposal (Block 726).  For example, if a second developer proposes the same deep link in response to the same input word set, the assistant support server may increase the rule weight.\nFIG. 8 illustrates, in a flowchart, one example of a method 800 of responding to an input to a smart assistant module at the assistant support server with a deep link.  The assistant support server may receive a user input from a user device\n(Block 802).  If the user input is a verbal user input (Block 804), the assistant support server may convert the verbal user input from the user device to a text user input (Block 806).  The assistant support server may parse a domain identified in the\nuser input (Block 808).\nThe assistant support server may compare the user input from the user device to each input word set from each assistant rule (Block 810).  The assistant support server may factor the domain from the user input into the comparison (Block 812). \nThe assistant support server may calculate a confidence score based on a comparison of the user input and the input word set for each assistant rule (Block 814).  If no assistant rule has a confidence score above a matching threshold (Block 816), the\nassistant support server may generate a null user directive indicating no match found to be sent to the user device (Block 818).  If a conflict occurs because more than one assistant rule has a similar confidence score (Block 820), the assistant support\nserver may compare the rule weights between the conflicting assistant rules (Block 822).  The assistant support server may select the assistant rule with the optimal confidence score and rule weight (Block 824).  The assistant support server may apply\nthe assistant rule to generate a user directive (Block 826).  The assistant support server may generate a user directive instructing a smart assistant module executed by the user device to connect to the associated deep link in response to receiving the\ninput word set (Block 818).  The assistant support server may send the user directive to the smart assistant module to direct the smart assistant module to access a deep link for the website in response to the user input (Block 828).\nThe assistant support server may track any network-enhanced action performed by the deep link in response to the user input (Block 830).  The assistant support server may update the assistant rule to associate a network-enhanced action performed\nby the deep link with the input word set (Block 832).  The assistant support server may adjust the rule weight and the developer weight based on a user response (Block 834).\nFIG. 9 illustrates, in a flowchart, one example of a method 900 of responding to an input to a smart assistant module at the user device with a deep link.  The user device may execute a smart assistant module (Block 902).  The smart assistant\nmodule may receive a user input (Block 904).  The smart assistant module may send the user input to an assistant support server (Block 906).\nThe smart assistant module may receive a user directive identifying a deep link of a website from the assistant support server based on a comparison of the user input to an input word set of an assistant rule provided by a developer input (Block\n908).  The smart assistant module may invoke a browser module or other web-connected application on the user device (Block 910).  If the user directive is a null user directive (Block 912), the browser may access a search engine to search for a\nrepresentative deep link for the user input upon receiving a null user directive (Block 914).  Otherwise, the smart assistant module may identify a deep link of a website in the user directive (Block 916).  The browser may connect to the deep link of the\nwebsite based on the user directive (Block 918).  The smart assistant module may track a network-enhanced action performed by the deep link in response to the user input (Block 920).  The smart assistant module may update the assistant rule with a\nnetwork-enhanced action performed by the deep link in response to the user input (Block 922).  The smart assistant module may adjust a rule weight based on a user response (Block 924).\nFIG. 10 illustrates, in a flowchart, one example of a method 1000 of responding to an input to a smart assistant module at a website server with a network-enhanced action.  The website server may receive a domain access from a user device\nexecuting a browser (Block 1002).  The website server may receive a deep link access from the browser (Block 1004).  The website server may execute a network-enhanced action based on a user input for a smart assistant module (Block 1006).\nAlternately, the developer may identify a network-enhanced action for the input word set.  FIG. 11 illustrates, in a flowchart, one example of a method 1100 of providing a network-enhanced action to a web portal with a developer device.  The\ndeveloper device may submit a login credential for the developer to the assistant support server (Block 1102).  The developer device may receive authentication from the assistant support server (Block 1104).  The developer device may submit an associated\ndomain to the assistant support server (Block 1106).  The developer device may receive an input word set from the developer (Block 1108).  The developer device may receive a word weight distribution for the input word set from the developer (Block 1110). The developer device may receive a network-enhanced action implemented by a smart assistant module executed by a user device from the developer (Block 1112).  The developer device may generate a developer input describing the relationship between the\ninput word set and the network-enhanced action (Block 1114).  The developer device may submit the developer input to the assistant support server (Block 1116).\nFIG. 12 illustrates, in a flowchart, one example of a method 1200 of receiving an assistant rule for a network-enhanced action in a web portal.  An assistant support server may maintain a web portal accessible by a developer device (Block 1202). The web portal may receive a login credential associated with a website for performing the network-enhanced action from a developer device (Block 1204).  The web portal may authenticate a developer identifier associated with a website for performing the\nnetwork-enhanced action in the login credential associated with the website (Block 1206).  The web portal may identify a domain for the website associated with the developer identifier (Block 1208).  The web portal may associate the developer identifier\nwith a developer weight indicating relative prominence (Block 1210).\nThe web portal may receive a developer input associating an input word set describing a hypothetical user input with a network-enhanced action implemented by a smart assistant module executed by a user device in response to the input word set\n(Block 1212).  The assistant support server may identify the input word set (Block 1214).  The assistant support server may identify a word weight for each word in the input word set (Block 1216).  The assistant support server may identify a\nnetwork-enhanced action implemented by a smart assistant module executed by a user device in response to the input word set (Block 1218).\nThe assistant support server may generate an assistant rule associating the input word set with the network-enhanced action (Block 1220).  The assistant support server may generate a rule weight describing a rule applicability likelihood based\non the developer weight indicating relative prominence (Block 1222).  The assistant support server may associate the assistant rule with a rule weight describing a rule applicability likelihood (Block 1224).  The assistant support server may adjust the\nrule weight based on a secondary developer proposal (Block 1226).  For example, if a second developer proposes the same network-enhanced action in response to the same input word set, the assistant support server may increase the rule weight.\nFIG. 13 illustrates, in a flowchart, one example of a method 1300 of responding to an input to a smart assistant module at the assistant support server with a network-enhanced action.  The assistant support server may receive a user input from a\nuser device (Block 1302).  If the user input is a verbal user input (Block 1304), the assistant support server may convert the verbal user input from the user device to a text user input (Block 1306).  The assistant support server may parse a domain\nidentified in the user input (Block 1308).\nThe assistant support server may compare the user input to each input word set from each assistant rule (Block 1310).  The assistant support server may factor the domain from the user input into the comparison (Block 1312).  The assistant\nsupport server may calculate a confidence score based on a comparison of the user input and the input word set for each assistant rule (Block 1314).  If no assistant rule has a confidence score above a matching threshold (Block 1316), the assistant\nsupport server may generate a null user directive to be sent to the user device (Block 1318).  If a conflict occurs because more than one assistant rule has a similar confidence score (Block 1320), the assistant support server may compare the rule\nweights between the conflicting assistant rules (Block 1322).  The assistant support server may select the assistant rule with the optimal confidence score and rule weight (Block 1324).  The assistant support server may apply the assistant rule to\ngenerate a user directive (Block 1326).  The assistant support server may generate a user directive instructing a smart assistant module executed by the user device to perform the network-enhanced action in response to receiving a user input from the\nsmart assistant module matching the input word set (Block 1318).  The assistant support server may send the user directive to the smart assistant module (Block 1328).\nThe assistant support server may track any deep link of the website accessed as part of performing the network-enhanced action in response to the user input (Block 1330).  The assistant support server may update the assistant rule to associate a\ndeep link of the website with the network-enhanced action (Block 1332).  The assistant support server may adjust the rule weight and the developer weight based on a user response (Block 1334).\nFIG. 14 illustrates, in a flowchart, one example of a method 1400 of responding to an input to a smart assistant module at the user device with a network-enhanced action.  The user device may execute a smart assistant module (Block 1402).  The\nsmart assistant module may receive a user input (Block 1404).  The smart assistant module may send the user input to an assistant support server (Block 1406).\nThe smart assistant module may receive a user directive from the assistant support server based on a comparison of the user input to an input word set of an assistant rule provided by a developer input (Block 1408).  The smart assistant module\nmay identify a network-enhanced action based on the user directive (Block 1410).  The smart assistant module may execute a network-enhanced action in response to the user directive (Block 1412).  The smart assistant module may access a deep link as part\nof a network-enhanced action (Block 1414).  The smart assistant module may update the assistant rule with a deep link for a website performing a network-enhanced action in response to the user input (Block 1416).  The smart assistant module may adjust a\nrule weight based on a user response (Block 1418).\nAlthough the subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that the subject matter in the appended claims is not necessarily limited to the specific features or\nacts described above.  Rather, the specific features and acts described above are disclosed as example forms for implementing the claims.\nExamples within the scope of the present invention may also include computer-readable storage media for carrying or having computer-executable instructions or data structures stored thereon.  Such computer-readable storage media may be any\navailable media that can be accessed by a general purpose or special purpose computer.  By way of example, and not limitation, such computer-readable storage media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage\nor other magnetic data storages, or any other medium which can be used to store desired program code means in the form of computer-executable instructions or data structures, as opposed to propagating media such as a signal or carrier wave. \nComputer-readable storage media explicitly does not refer to such propagating media.  Combinations of the above should also be included within the scope of the computer-readable storage media.\nExamples may also be practiced in distributed computing environments where tasks are performed by local and remote processing devices that are linked (either by hardwired links, wireless links, or by a combination thereof) through a\ncommunications network.\nComputer-executable instructions include, for example, instructions and data which cause a general purpose computer, special purpose computer, or special purpose processing device to perform a certain function or group of functions. \nComputer-executable instructions also include program modules that are executed by computers in stand-alone or network environments.  Generally, program modules include routines, programs, objects, components, and data structures, etc. that perform\nparticular tasks or implement particular abstract data types.  Computer-executable instructions, associated data structures, and program modules represent examples of the program code means for executing steps of the methods disclosed herein.  The\nparticular sequence of such executable instructions or associated data structures represents examples of corresponding acts for implementing the functions described in such steps.\nAlthough the above description may contain specific details, they should not be construed as limiting the claims in any way.  Other configurations of the described examples are part of the scope of the disclosure.  For example, the principles of\nthe disclosure may be applied to each individual user where each user may individually deploy such a system.  This enables each user to utilize the benefits of the disclosure even if any one of a large number of possible applications do not use the\nfunctionality described herein.  Multiple instances of electronic devices each may process the content in various possible ways.  Implementations are not necessarily in one system used by all end users.  Accordingly, the appended claims and their legal\nequivalents should only define the invention, rather than any specific examples given.", "application_number": "15373458", "abstract": " In one example, an assistant support server may maintain a web portal to\n     crowdsource responses to a user input. The assistant support server may\n     maintain a web portal accessible by a developer device. The assistant\n     support server may store an assistant rule based on a developer input\n     associating an input word set describing a hypothetical user input with a\n     deep link for a website. The assistant support server may receive in the\n     web portal the developer input. The assistant support server may direct a\n     smart assistant module executed by a user device to connect to the deep\n     link in response to receiving a user input from the smart assistant\n     module matching the input word set.\n", "citations": ["6157705", "6473734", "8843376", "9195477", "9292252", "9652209", "9740751", "20020010584", "20020072916", "20030078779", "20030125953", "20040128136", "20040141597", "20120124061", "20150143241", "20160078141", "20160155442", "20160234330"], "related": []}, {"id": "20180167276", "patent_code": "10374883", "patent_name": "Application-based configuration of network data capture by remote capture\n     agents", "year": "2019", "inventor_and_country_data": " Inventors: \nDickey; Michael (Palo Alto, CA)  ", "description": "<BR><BR>FIELD OF THE INVENTION\nAt least certain embodiments of the invention relate generally to computer networks, and more particularly to a system configured for capturing and processing network data across a distributed network environment.\n<BR><BR>BACKGROUND OF THE INVENTION\nOver the past decade, the age of virtualization has triggered a sea change in the world of network data capture.  Almost every network capture product available today is a physical hardware appliance that customers have to purchase and\nconfigure.  In addition, most network data capture technologies are built from scratch to serve a specific purpose and address the needs of a particular vertical market.  For example, network capture systems may be customized to extract data for security\nand intrusion-detection purposes, collect network performance data, perform Quality of Service (QoS), redirect data, block network traffic, and/or perform other analysis or management of network traffic.  Such targeted and/or fixed implementation and use\nof network capture technologies may preclude modification of the network capture technologies to address different and changing business needs.\nMoreover, customers using conventional hardware-based network capture devices typically connect the devices to other hardware devices in a network.  The connections may allow the network capture devices to access the network and monitor network\ntraffic between two or more points in the network.  Examples of such devices include a network Test Access Point (TAP) or Switched Port Analyzer (SPAN) port.  After the network traffic is captured, cumbersome Extraction, Transform, and Load (\"ETL\")\nprocesses may be performed to filter, transform, and/or aggregate data from the network traffic and enable the extraction of business value from the data.\nHowever, customers are moving away from managing physical servers and data centers and toward public and private cloud computing environments that provide software, hardware, infrastructure, and/or platform resources as hosted services using\ncomputing, storage, and/or network devices at remote locations.  For these customers, it is either impossible, or at best extremely challenging, to deploy physical network capture devices and infrastructure in the cloud computing environments.\nConsequently, network data capture may be facilitated by mechanisms for deploying and configuring network capture technology at distributed and/or remote locations. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nFor a better understanding of at least certain embodiments, reference will be made to the following detailed description, which is to be read in conjunction with the accompanying drawings, wherein:\nFIG. 1 depicts an example block diagram embodiment of a data processing system for capturing and processing network data in a distributed network environment;\nFIG. 2 depicts an example block diagram embodiment of a remote capture agent;\nFIG. 3 depicts an example block diagram embodiment of a configuration server;\nFIG. 4 shows a flowchart illustrating the processing of network data;\nFIG. 5 shows a flowchart illustrating the process of facilitating the processing of network data;\nFIG. 6 shows a flowchart illustrating the process of facilitating network data capture;\nFIG. 7 shows a flowchart illustrating the process of facilitating the processing of network data;\nFIG. 8 depicts an example screen shot of an embodiment of a graphical interface that is adapted to display configurable components within a distributed data capture and processing system;\nFIG. 9 depicts an example screen shot of an embodiment of a configuration dialog for obtaining configuration information for configuring the generation of event data from network data at one or more remote capture agents;\nFIG. 10 depicts an example block diagram of an embodiment of a time-based data storage architecture that includes a late-binding schema;\nFIG. 11 illustrates a flowchart of an example embodiment of a process for storing collected data in a data storage architecture that includes a late-binding schema;\nFIG. 12 illustrates a flowchart of an example embodiment of a process for generating a query result in a data storage architecture that includes a late-binding schema; and\nFIG. 13 depicts an example data processing system upon which the embodiments described herein may be implemented.\n<BR><BR>DETAILED DESCRIPTION\nThroughout the description, for the purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the present techniques described herein.  It will be apparent to one skilled in the art,\nhowever, that the present invention may be practiced without some of these specific details.  In other instances, well-known structures and devices are shown in block diagram form to avoid obscuring the underlying principles of embodiments of the\ninvention.\n1.0.  General Overview\n1.1.  Event-Based Data Storage Systems\nGenerally, a data-processing system may perform data operations on data stored in one or more data repositories.  Depending on the type of data-processing system, the data operations may range from simple operations such as storing and\nretrieving the data to more complex operations such as calculating statistics from the data, or arranging or formatting the data.  One example of a data-processing system is a relational database system, in which data is stored in highly structured\ntables and accessed through rigid data storage rules (e.g., data storage and retrieval \"schemas\").  Another example of a data-processing system is a file system, such as a Network File System (NFS) server.  Yet another example of a data-processing system\nis a web application server.\nA data-processing system may also include an event-based system, such as the SPLUNK.RTM.  ENTERPRISE system produced and sold for on-premise and cloud use by Splunk Inc.  of San Francisco, Calif.  In some event-based systems, data is derived\nfrom lines or rows of unstructured time-series data, such as data from web logs and/or machine logs.  Each row and/or group of rows is generally associated with a timestamp and one or more associated data points or parameter-value pairs.  A timestamp may\nbe any sequence of characters or encoded information that identifies the time at which a certain event is recorded.  For example, a timestamp may provide the date, hour, minute, and/or second at which an application is initialized on a computer system. \nBased on the timestamps, data structures representing events may be derived from the associated data and include some or all of the associated data.  A variety of event types may be derived from such data.  For example, in the context of web logs, events\nmay be derived from errors, specific user inputs, navigation events, and so forth.\nAs used herein, the term \"events\" may refer to anything that occurs and carries information in an event-based system.  Some event-based systems feature flexible data storage and retrieval schemas that may be redefined as needed and applied after\nthe associated data is stored in a database or other memory structure of the data storage system.  For example, the schemas may be applied upon receiving a request to perform an operation on such data.  Such schemas may indicate how to extract one or\nmore pieces of data from data associated with an event.  In addition, in connection-oriented network communications systems, a \"data stream\" generally refers to a sequence of encoded signals (e.g., in network packets) used to transmit or receive\ninformation over a network.\n1.2.  Remote Capture Agent Architecture\nOne or more embodiments include a network architecture for capturing network data in one or more networks using a configuration server working in combination with a set of remote capture agents distributed throughout the network(s).  The remote\ncapture agents may capture network packets from multiple sources (e.g., hosts, servers, etc.) and analyze the network packets to determine the packets' contents.  The remote capture agents may then generate one or more events from the network packets and\ncommunicate the events to the configuration server over one or more additional networks.\nIn one or more embodiments, the configuration server includes configuration information used to determine how remote capture agents capture network data and build events therefrom.  The remote capture agents may obtain the configuration\ninformation from the configuration server (e.g., using a push or pull mechanism) and use the configuration information to generate event data containing a series of timestamped events from the network data.  The event data may be included in an event\nstream that is transmitted to additional network elements within the distributed network for additional processing and/or storage.\nIn this manner, both network traffic between the remote capture agents and other network elements and subsequent processing of the network traffic by the other network elements may be drastically reduced because capturing and pre-processing of\nthe network data may be performed at the remote capture agents.  For example, the remote capture agents may transmit events in lieu of network packets from which the events were generated to one or more centralized servers for further processing,\nindexing, and/or storage.\n1.3.  Dynamically Configurable Remote Capture Agents\nRemote capture agents may be dynamically configured based on configuration information stored at the configuration server.  For example, the remote capture agents may be configured in real-time as events are processed by the remote capture\nagents.  The remote capture agents may be dynamically configured during runtime with: (1) events (or types of events) to be included in event streams for use by other components of the remote capture agent architecture, (2) fields to be included in each\nof the events streams, and (3) additional parameters associated with generation of the events and/or event streams.\nThe configuration information may be modified on-demand by users (e.g., administrators) at the configuration server and/or at a network component in communication with the configuration server.  The configuration information may also be\ndynamically updated during processing of event streams by one or more applications running on separate servers in communication with the configuration server, such as one or more data storage servers in communication with the configuration server. \nEvents may then be generated from the captured network packets based on the configuration information and/or any updates to the configuration information.\nWhen changes are made to the configuration information at the configuration server, logic in the remote capture agents may be automatically updated in response.  In one embodiment, the remote capture agents poll the configuration server at\nperiodic intervals to determine if there have been any changes to the configuration information stored therein.  If changes to the configuration information have been made, the remote capture agents may pull this configuration information from the\nconfiguration server.  Alternatively, changes to the configuration information may be pushed from the configuration server to the remote capture agents at periodic intervals.  Such propagation of updates to the configuration information to the remote\ncapture agents may allow the remote capture agents to be dynamically configured to store different types of network data in events, generate different types of events, aggregate event data, and/or send event data to other network components at different\ntimes and/or intervals.\n1.4.  Transforming Event Data at the Remote Capture Agents\nThe configuration information may also be used by the remote capture agents to perform higher-level processing of the events before communicating the events to the configuration server.  More specifically, the remote capture agents may use some\nor all of the configuration information to transform (e.g., aggregate, process, clean, filter, etc.) events into one or more sets of transformed event data.  The remote capture agents may provide the transformed event data to the configuration server\nand/or other network components, in lieu of or in addition to the events.  The network components may further process the transformed event data and/or store the transformed event data (e.g., in a data storage server).\nIn one or more embodiments, some or all of the configuration information related to transforming events is specified by applications running on other servers or systems and communicated to the configuration server.  For example, the applications\nmay run on a data-processing system such as the SPLUNK.RTM.  ENTERPRISE system.  Users may use the applications to perform queries and/or visualizations related to event data from the remote capture agents.  The applications may provide the configuration\nserver with information regarding the events (or types of events) the application is adapted to receive, along with information related to subsequent processing and/or transformation of those events.  The configuration server may obtain the information\nfrom the applications for propagation to the remote capture agents, and the remote capture agents may use the information to configure or reconfigure the creation and processing of event data accordingly.  In one embodiment, the applications include data\nstorage applications running on a data storage server to facilitate optimizing data storage and retrieval operations.\n1.5.  Graphical Interface for Configuring Event Streams\nA graphical user interface (GUI) may facilitate the configuration of the remote capture agents and/or other network components in generating and/or processing event streams containing event data.  The GUI may provide a visual way to create,\nmanage, and/or process event streams based on configuration information associated with each event stream.  The GUI may be provided by the configuration server and/or by a network element in communication with the configuration server.  The GUI may\ndisplay representations of one or more components associated with creating and/or processing event streams generated from network traffic.  The components may be configured or reconfigured using various icons and/or other user-interface elements in the\nGUI.\n2.0.  Structural Overview\n2.1.  Operating Environment\nThe data processing techniques described herein are suitable for use by systems deployed in a variety of operating environments.  FIG. 1 depicts an example block diagram embodiment of a data-processing system 100 for capturing and processing\nnetwork data in a distributed network environment.  In the illustrated embodiment, system 100 includes a set of configuration servers 120 in communication with a set of remote capture agents 151-153 over one or more networks 190.\nAlthough system 100 only depicts three configuration servers 120 and three remote capture agents 151-153, any number of configuration servers 120 and/or remote capture agents 151-153 may be configured to operate and/or communicate with one\nanother within the data-processing system.  For example, a single physical and/or virtual server may perform the functions of configuration servers 120.  Alternatively, multiple physical and/or virtual servers or network elements may be logically\nconnected to provide the functionality of configuration servers 120.  The configuration server(s) may direct the activity of multiple distributed remote capture agents 151-153 installed on various client computing devices across one or more networks.  In\nturn, remote capture agents 151-153 may be used to capture network data from multiple remote network data sources.\nFurther, embodiments described herein can be configured to capture network data in a cloud-based environment, such as cloud 140 depicted in the illustrated embodiment, and to generate events such as clickstream events and/or business\ntransactions out of the network data.  Remote capture agents 151-153 may capture network data originating from numerous distributed network servers, whether they are physical hardware servers or virtual machines running in cloud 140.  In cloud-based\nimplementations, remote capture agents 151-153 will generally only have access to information that is communicated to and received from machines running in the cloud-based environment.  This is because, in a cloud environment, there is generally no\naccess to any of the physical network infrastructure, as cloud computing may utilize a \"hosted services\" delivery model where the physical network infrastructure is typically managed by a third party.\nEmbodiments further include the capability to separate the data capture technology into a standalone component that can be installed directly on client servers, which may be physical servers or virtual machines residing on a cloud-based network\n(e.g., cloud 140), and used to capture and generate events for all network traffic that is transmitted in and out of the client servers.  This eliminates the need to deploy and connect physical hardware to network TAPS or SPAN ports, thus allowing users\nto configure and change their data capture configuration on-the-fly rather than in fixed formats.\nIn the illustrated embodiment, remote capture agents 152-153 are in communication with network servers 130 residing in cloud 140, and remote capture agent 151 is located in cloud 140.  Cloud 140 may represent any number of public and private\nclouds, and is not limited to any particular cloud configuration.  Network servers 130 residing in cloud 140 may be physical servers and/or virtual machines in cloud 140, and network traffic to and from network servers 130 may be monitored by remote\ncapture agent 151 and/or other remote capture agents connected to network servers 130.  Further, remote capture agents 152-153 may also run in cloud 140 on physical servers and/or virtual machines.  Those skilled in the art will appreciate that any\nnumber of remote capture agents may be included inside or outside of cloud 140.\nRemote capture agents 151-153 may analyze network packets received from the networks(s) to which remote capture agents 151-153 are connected to obtain network data from the network packets and generate a number of events from the network data. \nFor example, each remote capture agent 151-153 may listen for network traffic on network interfaces available to the remote capture agent.  Network packets transmitted to and/or from the network interfaces may be intercepted by the remote capture agent\nand analyzed, and relevant network data from the network packets may be used by the remote capture agent to create events related to the network data.  Such events may be generated by aggregating network data from multiple network packets, or each event\nmay be generated using the contents of only one network packet.  A sequence of events from a remote capture agent may then be included in one or more event streams that are provided to other components of system 100.\nConfiguration servers 120, data storage servers 135, and/or other network components may receive event data (e.g., event streams) from remote capture agents 151-153 and further process the event data before the event data is stored by data\nstorage servers 135.  In the illustrated embodiment, configuration servers 120 may transmit event data to data storage servers 135 over a network 101 such as a local area network (LAN), wide area network (WAN), personal area network (PAN), virtual\nprivate network, intranet, mobile phone network (e.g., a cellular network), WiFi network, Ethernet network, and/or other type of network that enables communication among computing devices.  The event data may be received over a network (e.g., network\n101, network 190) at one or more event indexers (see FIG. 10) associated with data storage servers 135.\nIn addition, system 100 may include functionality to determine the types of network data collected and/or processed by each remote capture agent 151-153 to avoid data duplication at the indexers, data storage servers 135, and/or other components\nof system 100.  For example, remote capture agents 152-153 may process network traffic from the same network.  However, remote capture agent 152 may generate page view events from the network traffic, and remote capture agent 153 may generate request\nevents (e.g., of HyperText Transfer Protocol (HTTP) requests and responses) from the network traffic.\nIn one or more embodiments, configuration servers 120 include configuration information that is used to configure the creation of events from network data on remote capture agents 151-153.  In addition, such configuration may occur dynamically\nduring event processing (e.g., at runtime).  Conversely, because most conventional network capture technologies target specific end uses, they have been designed to operate in a fixed way and generally cannot be dynamically or easily modified to address\ndifferent and changing business needs.\nAt least certain embodiments described herein are adapted to provide a distributed remote capture platform in which the times at which events are communicated to the configuration servers 120 and the fields to be included in the events are\ncontrolled by way of user-modifiable configuration rather than by \"hard coding\" fixed events with pre-determined fields for a given network capture mechanism.  The remote configuration capability described herein also enables additional in-memory\nprocessing (e.g., filtering, transformation, normalization, aggregation, etc.) on events at the point of capture (e.g., remote capture agents 151-153) before the events are transmitted to other components of system 100.\nConfiguration information stored at each configuration server 120 may be created and/or updated manually at the configuration server and/or at a network element in communication with the configuration server.  For example, a user may upload a\nconfiguration file containing configuration information for a remote capture agent to one or more configuration servers 120 for subsequent propagation to the remote capture agent.  Alternatively, the user may use a GUI to provide the configuration\ninformation, as described in further detail below with respect to FIGS. 8-9.  The configuration information may further be provided by one or more applications running on a separate server or network element, such as data storage servers 135.\nRemote capture agents 151-153 may then use the configuration information to generate events from captured network packets.  When changes in the configuration information at the configuration server are detected at the remote capture agents,\nlogic in the remote capture agents may be automatically reconfigured in response.  This means the remote capture agents may be configured dynamically to produce different events, transform the events, and/or communicate event streams to different\ncomponents of system 100.\nTo detect changes in configuration information at configuration servers 120, remote capture agents 151-153 may poll configuration servers 120 at periodic intervals for updates to the configuration information.  The updates may then be pulled\nfrom configuration servers 120 by remote capture agents 151-153.  Conversely, updates to the configuration information may be pushed from configuration servers 120 to remote capture agents 151-153 at periodic intervals and/or when changes to the\nconfiguration information have been made.\nIn one embodiment, configuration servers 120 include a list of event streams generated by remote capture agents 151-153, as well as the configuration information used to generate the event streams at remote capture agents 151-153.  The\nconfiguration information may include a unique identifier for each event stream, the types of events to be included in the event stream, one or more fields to be included in each event, and/or one or more filtering rules for filtering events to be\nincluded in the event stream.  Configuration information for dynamically modifying network data capture by remote capture agents (e.g., remote capture agents 151-153) is described in further detail below with respect to FIG. 2.\nThe configuration information may also specify transformations of network data and/or events into transformed events.  Such transformations may include, for example, aggregations of network data and/or events, generation of statistics and/or\nmetrics from the network data or events, and/or cleaning and/or filtering of the network data and/or events.  As with other event streams, event streams containing transformed event data may be transmitted from remote capture agents 151-153 to\nconfiguration servers 120, data storage servers 135, and/or other components of system 100 for further processing, storage, and/or use.\nConfiguration information associated with transformed events may be obtained from end users and/or applications running on various network elements that receive the events.  For example, an application executing on a data storage server (e.g.,\ndata storage servers 135) may provide statistics associated with network usage in cloud 140.  To reduce overhead associated with real-time processing of event data by the application into the statistics, the application may provide configuration\ninformation for generating some or all of the statistics at one or more remote capture agents (e.g., remote capture agents 151-153) connected to cloud 140.  The configuration information may be transmitted to configuration servers 120 and subsequently\npropagated to the relevant remote capture agents.  In turn, the remote capture agents may use the configuration information to generate transformed events containing statistics associated with events captured by the remote capture agents, and the\ntransformed events may be provided to the application to enable access to the statistics by users of the application without requiring the application to calculate the statistics at query time.\nSuch use of distributed remote capture agents 151-153 may offload processing tasks from configuration servers 120 and/or other components of system 100 to remote capture agents 120 (e.g., similar to parallelizing a network), while avoiding\noverloading of client network servers at remote networks by burdening the client network servers with the full functionality of configuration servers 120.  System 100 may further reduce network traffic between remote capture agents 151-153 and the other\ncomponents of system 100 because remote capture agents 120 convert a potentially large volume of raw network traffic into a smaller volume of events and further filter the event data as directed by the configuration information before transmitting the\nevent data to other components of system 100.\nAnother advantage is that the work performed by system 100 may be distributed among multiple remote capture agents 151-153 on one or more networks.  Remote capture agents 151-153 may occupy small footprints on remote client servers, thus\nmitigating resource usage by remote capture agents 151-153 on the client servers.  For example, remote capture agents 151-153 may execute as background processes on physical and/or virtualized servers.  On the other hand, configuration servers 120 may\nexecute from one or more centralized locations and/or on one or more sets of dedicated resources because the operation of configuration servers 120 may require significantly more computing resources than the operation of remote capture agents 151-153.\nAs depicted in FIG. 1, system 100 further includes one or more data storage servers 135.  Data storage servers 135 may be general or special-purpose computers configured to process and manipulate data within one or more data repositories.  As\ndepicted, data storage servers 135 may be coupled to data storage devices 155 using any suitable mechanism, such as a Fiber Channel network, a Serial ATA (SATA) link, a Universal Serial Bus (USB) connection, an Infiniband link, an Ethernet connection,\nand/or other type of interface.  Data storage servers 135 can be configured to communicate input/output (I/O) requests to storage devices 155.  These I/O requests may be communicated via messages in protocols such as Server Message Block protocol,\nNetwork File System (NFS) protocol, Small Computer System Interface (SCSI) protocol, and/or Fibre Channel.  In response to the requests, data storage servers 135 may read and write data structures such as data blocks, files, tables, and/or result sets\nfrom storage devices 155.  In an embodiment, data storage servers 135 may include some or all of storage devices 155.\nInstructions for processing and manipulating data (e.g., event data) may be executed by data storage servers 135.  For example, data storage servers 135 may perform data operations with respect to one or more data repositories.  Data operations\nsupported by these processes may include relatively simple operations such as adding or retrieving lines or rows of data from the data storage devices.  The supported data operations may further include operations such as filtering the contents of\nretrieved data and./or performing transformations (e.g., aggregations, calculations, processing, cleaning, filtering, etc.) of the retrieved data.\nIn one or more embodiments, data storage servers 135 and/or configuration servers 120 provide one or more transformation servers that perform additional processing of event data from remote capture agents 151-153.  Conversely, one or more\nconfiguration servers 120 and/or data storage servers 135 may be installed within a transformation server and/or execute independently from transformation servers in the data-processing system 100.  The transformation servers may be used to aggregate,\nfilter, format, query, transform, store, and/or otherwise manipulate event data, as described in further detail below with respect to FIG. 8.\nIn another embodiment, data storage servers 135 may constitute one or more conventional database servers, such as a relational database server.  These processes need not necessarily support the entire functionality of a database server or\noperate on conventional database structures.\nData repositories accessed by data storage servers 135 may be stored on data storage devices 155.  Data storage devices 155 may be, for instance, nonvolatile computer-readable media such as hard disk drives, flash/SSD drives, nonvolatile memory,\noptical storage devices, disk arrays, storage area network devices, networked-attached storage devices, and/or file server devices.  Storage devices 155 may store the data repositories in any suitable underlying form(s), such as disk blocks, file\nstructures, or database tables.  If multiple storage devices 155 are used in system 100, different portions of a data repository may be stored on different storage devices 155.  Optionally, certain storage devices 155 may be configured to store some or\nall portions of a data repository redundantly, using any suitable backup or synchronization mechanism(s).\nIn an embodiment, each storage device 155 is equally accessible to each data storage server 135, and thus any data storage server 135 may perform operations on any data stored within the data repositories.  In other embodiments, each data\nstorage server 135 is assigned to only some or even one of the data storage devices 155, and is only configured to perform operations on data storage device(s) 155 to which it is assigned.\nSystem 100 is only one example of the many types of operating environments in which the techniques described herein may be practiced.  Other suitable operating environments may include additional or fewer elements, in varying arrangements.  For\ninstance, some or all data storage servers 135 may be replaced by virtual computing environments (e.g., virtual machines), some or all of which may execute on a single computing device.\nSystem 100 further utilizes data repositories provided by storage devices 155.  The data repositories may include one or more data collections, and each data collection may be a collection of data structures having a variety of forms.  For\nexample, a data collection may include a collection of time-based event data structures (e.g., one or more event streams), a group of data rows, a relational database, a relational database table, set of Extended Markup Language (XML) elements, and/or\none or more files.  Different data collections within the same repository may support different data structure types.  In an embodiment, a data collection containing of any of the foregoing data structures is augmented with system-defined or user-defined\nvariables that can be updated to describe certain characteristics of the data stored in the data collection.  Examples of such variables may include counters or metrics.  In an embodiment, each data collection is stored redundantly on multiple data\nstorage devices 155, and synchronized therebetween.  In an embodiment, each data collection is found on only some or even one of the data storage devices 155.\nFIG. 2 depicts an example block diagram embodiment of a remote capture agent 250.  In the illustrated embodiment, remote capture agent 250 is adapted to receive configuration information from one or more configuration servers 120 over network\n101.  Remote capture agent 250 may be installed at a customer's premises on one or more of the customer's computing resources.  For example, remote capture agent 250 may be installed on a physical server and/or in a virtual computing environment (e.g.,\nvirtual machine) that is distributed across one or more physical machines.\nRemote capture agent 250 includes a network communications component 203 configured to communicate with network elements on one or more networks (e.g., network 101) and send and receive network data (e.g., network packets) over the network(s). \nAs depicted, network communications component 203 may communicate with configuration servers 120 over network 101.  Network communications component 203 may also communicate with one or more sources of network data, such as network servers 130 of FIG. 1.\nNetwork data received at network communications component 203 may be captured by a capture component 205 coupled with network communications component 203.  Capture component 205 may capture some or all network data from network communications\ncomponent 203.  For example, capture component 205 may capture network data based on the sources and/or destinations of the network data, the types of network data, the protocol associated with the network data, and/or other characteristics of the\nnetwork data.\nIn addition, the network data may be captured based on configuration information stored in a configuration component 204 of remote capture agent 250.  As mentioned above, the configuration information may be received from configuration servers\n120 over network 101.  The configuration information may then be used to dynamically configure or reconfigure remote capture agent 250 in real-time.  For example, newly received configuration information in configuration component 204 may be used to\nconfigure the operation of remote capture agent 250 during processing of events from network data by remote capture agent 250.\nTo dynamically configure remote capture agent 250, configuration information received by configuration component 204 from configuration servers 120 may be provided to other components of remote capture agent 250.  More specifically, remote\ncapture agent 250 includes an events generator 207 that receives network data from network data capture component 205 and generates events from the network data based on configuration information from configuration component 204.\nUsing configuration information provided by configuration servers 120, remote capture agent 250 can be instructed to perform any number of event-based processing operations.  For example, the configuration information may specify the generation\nof event streams associated with network (e.g., HTTP, Simple Mail Transfer Protocol (SMTP), Domain Name System (DNS)) transactions, business transactions, errors, alerts, clickstream events, and/or other types of events.  The configuration information\nmay also describe custom fields to be included in the events, such as values associated with specific clickstream terms.  The configuration information may include additional parameters related to the generation of event data, such as an interval between\nconsecutive events and/or the inclusion of transactions and/or errors matching a given event in event data for the event.\nAn events transformer 209 may further use the configuration information to transform some or all of the network data from capture component 205 and/or events from events generator 207 into one or more sets of transformed events.  In one or more\nembodiments, transformations performed by events transformer 209 include aggregating, filtering, cleaning, and/or otherwise processing events from events generator 207.  Configuration information for the transformations may thus include a number of\nparameters that specify the types of transformations to be performed, the types of data on which the transformations are to be performed, and/or the formatting of the transformed data.\nFor example, configuration information for generating an event stream from network data (e.g., at events generator 207) may include the following Javascript Object Notation (JSON) data:\nTABLE-US-00001 { ''id'' : ''trans_class'', ''name'': ''auto-classified transactions'', ''streamType'' : ''trans_class'' }\nThe JSON data may include a unique identifier (e.g., \"id\") of \"trans_class\" for the event stream, a descriptive name (e.g., \"name\") of \"auto-classified transactions\" for the event stream, and an event stream type (e.g., \"streamType\") of\n\"trans_class.\" Event data in the event stream may be identified by the identifier and/or descriptive name.  The \"trans_class\" event stream type may indicate that events in the event stream represent automatically classified transactions such as user\nlogins and logouts, shopping cart checkouts, new user signups, and/or file transfers, with a new event generated per automatically classified transaction.  In addition, the event may include a unique identifier for the classified transaction type, as\nwell as a Uniform Resource Identifier (URI) stem, a query string, a host name, and/or a page title for the transaction.\nIn another example, configuration information for performing transformations on events from events generator 207 (e.g., at events transformer 209) may include the following JSON data:\nTABLE-US-00002 { ''id'' : ''trans_metrics'', ''name'': ''transaction metrics aggregated by id'', ''streamType'' : ''agg_trans'', ''fields'' : [ { ''name'' : ''sessions'', ''desc'' : ''total number of visitor sessions'', ''term'' :\n''clickstream.new-session'', ''aggType'' : ''sum'' }, { ''name'' : ''hits'', ''desc'' : ''total number of HTTP transactions'', ''term'' : ''clickstream.page-hits'', ''aggType'' : ''sum'' }, { ''name'' : ''cs_bytes'', ''desc'' : ''total octets from client\nto server (ingress)'', ''term'' : ''clickstream.cs-bytes'', ''aggType'' : ''sum'' }, { ''name'' : ''sc_bytes'', ''desc'' : ''total octets from server to client (egress)'', ''term'' : ''clickstream.sc-bytes'', ''aggType'' : ''sum'' }, { ''name'' :\n''total_time'', ''desc'' : ''total clock time from start to end of the transaction (microsec)'', ''term'' : ''clickstream.page-load'', ''aggType'' : ''sum'' }, { ''name'' : ''redirect_time'', ''desc'' : ''total clock time spent processing HTTP redirects\n(microsec)'', ''term'' : ''clickstream.page-load-redirect'', ''aggType'' : ''sum'' }, { ''name'' : ''base_time'', ''desc'' : ''total clock time spent loading the base HTML file (microsec)'', ''term'' : ''clickstream.page-load-base'', ''aggType'' :\n''sum'' }, { ''name'' : ''content_time'', ''desc'' : ''total clock time spent loading everything else (microsec)'', ''term'' : ''clickstream.page-load-content'', ''aggType'' : ''sum'' }, { ''name'' : ''time_taken'', ''desc'' : ''sum of measurements from\nstart to end of each HTTP transaction (microsec)'', ''term'' : ''clickstream.time-taken'', ''aggType'' : ''sum'' }, { ''name'' : ''client_rtt_sum'', ''desc'' : ''sum of round trip time measurements between client & agent (microsec)'', ''term'' :\n''clickstream.cp-rtt-sum'', ''aggType'' : ''sum'' }, { ''name'' : ''client_rtt_count'', ''desc'' : ''count of round trip time measurements between client & agent'', ''term'' : ''clickstream.cp-rtt-packets'', ''aggType'' : ''sum'' }, { ''name'' :\n''server_rtt_sum'', ''desc'' : ''sum of round trip time measurements between server & agent (microsec)'', ''term'' : ''clickstream.ps-rtt-sum'', ''aggType'' : ''sum'' }, { ''name'' : ''server_rtt_count'', ''desc'' : ''count of round trip time\nmeasurements between server & agent'', ''term'' : ''clickstream.ps-rtt-packets'', ''aggType'' : ''sum'' }, { ''name'' : ''refused'', ''desc'' : ''total number of HTTP transactions that were refused by the server'', ''term'' : ''clickstream.refused'',\n''aggType'' : ''sum'' }, { ''name'' : ''canceled'', ''desc'' : ''total number of HTTP transactions that were canceled by the client'', ''term'' : ''clickstream.canceled'', ''aggType'' : ''sum'' }, { ''name'' : ''cached'', ''desc'' : ''total number of\nHTTP transactions that had cached responses'', ''term'' : ''clickstream.cached'', ''aggType'' : ''sum'' } ] }\nThe JSON data may include a unique identifier (e.g., \"id\") of \"trans_metrics\" for the set of transformed events and a descriptive name (e.g., \"name\") of \"transaction metrics aggregated by id\" for the transformed events.  The JSON data may also\nprovide an event stream type (e.g., \"streamType\") of \"agg_trans,\" indicating that the configuration relates to transformations that aggregate transactions from other event data, such as event data generated using the \"trans_class\" configuration above. \nThe JSON data may additionally include a list of custom fields (e.g., \"fields\") that specify the types of data to be aggregated, such as numbers of visitor sessions or HTTP transactions, octets between clients and servers, clock times associated with\npage loads, and/or round-trip time (RTT) measurements between various network components.  Each field may include a name (e.g., \"name\") for the corresponding aggregation, a description (e.g., \"desc\") of the aggregation, a clickstream term (e.g., \"term\")\nrepresenting the data to be aggregated, and an aggregation type (e.g., \"aggType\").  While the exemplary configuration information above shows an aggregation type of \"sum\" (e.g., summing of values represented by \"term\" across all events within an\naggregation interval) for all aggregations, other aggregation types may be supported by remote capture agent 250.  Such aggregation types may include, for example, a key (e.g., hash) for each set of aggregated values, statistics (e.g., mean, median,\nvariance, standard deviation, minimum value, maximum value, etc.) associated with the aggregated values, a uniqueness count for each unique value within an aggregation interval, and/or calculations used to aggregate values from two or more fields.\nA rules comparison engine 208 in remote capture agent 250 may receive events from event generator 207 and compare one or more fields from the events to a set of filtering rules in the configuration information to determine whether to include the\nevents in an event stream.  For example, the configuration information may specify packet-level, protocol-level, and/or application-level filtering of event data from event streams generated by remote capture agent 250.\nFinally, a data enrichment component 211 may further transform event data to a different form or format based on the configuration information from configuration component 204.  For example, data enrichment component 211 may use the\nconfiguration information to normalize the data so that multiple representations of the same value (e.g., timestamps, measurements, etc.) are converted into the same value in transformed event data.\nData can be transformed by data enrichment component 211 in any number of ways.  For example, remote capture agent 250 may reside on a client server in Cupertino, Calif., where all the laptops associated with the client server have been\nregistered with the hostname of the client server.  Remote capture agent 250 may use the registration data to look up an Internet Protocol (IP) address in a look-up table (LUT) that is associated with one or more network elements of the client server's\nlocal network.  Remote capture agent 250 may then resolve a user's IP address into the name of the user's laptop, thereby enabling inclusion of the user's laptop name in transformed event data associated with the IP address.  The transformed event data\nmay then be communicated to configuration servers 120 and/or a central transformation server residing in San Francisco for further processing, indexing, and/or storage.\nA further advantage of the techniques described herein includes relates to the transformation of network data at least at two distinct levels, including at the remote capture agents during generation of the events and at the configuration server\nand/or other components during subsequent processing of event data.  FIG. 3 depicts an example block diagram embodiment of a configuration server 320.  As shown in the illustrated embodiment, configuration server 320 is in communication with multiple\nremote capture agents 350 over network 190, and remote capture agents 350 are distributed throughout network 190 and cloud 140.  Configuration server 320 includes a network communications component 303 that receives events from remote capture agents 350\nover networks 190 and/or 140.  Communications component 303 may also communicate with one or more data storage servers, such as data storage servers 135 of FIG. 1.\nConfiguration server 320 also includes a configuration component 304 that stores configuration information for remote capture agents 350.  As described above, the configuration information may specify the types of events to produce, data to be\nincluded in the events, and/or transformations to be applied to the data and/or events to produce transformed events.  Some or all of the transformations may be specified in a set of filtering rules 321 that may be applied to event data at remote capture\nagents 350 to determine a subset of the event data to be included in one or more event streams that are sent to configuration server 320 and/or other components.\nConfiguration server 320 also includes a data processing component 311 that performs additional processing of the event streams based on configuration information from configuration component 304.  As discussed in the above example with respect\nto FIG. 2, event data may be transformed at a remote capture agent (e.g., remote capture agent 250) during resolution of the user's IP address was into the name of the user's laptop.  The transformed event data may be sent to configuration server 320\nand/or a transformation server for additional processing and/or transformation, such as taking the host name from the transformed event data, using an additional LUT to obtain a user identifier (user ID) of the person to which the laptop is registered,\nand further transforming the event data by including the user ID in the event data before forwarding the event data to a third server (e.g., a transformation server) for another round of processing.\nConfiguration server 320 may also provide a GUI 325 that can be used to configure or reconfigure the information contained in configuration component 304.  The operation of GUI 325 is discussed in further detail below with respect to FIGS. 7-9.\n3.0.  Functional Overview\n3.1.  Remote Capture Agent Architecture\nThe techniques described in this section can be performed by the data processing system for capturing and processing network data in a distributed network environment as shown in FIG. 1.  FIG. 4 shows a flowchart illustrating the processing of\nnetwork data.  More specifically, FIG. 4 shows a flowchart of network data capture and processing in accordance with the disclosed embodiments.  In one or more embodiments, one or more of the steps may be omitted, repeated, and/or performed in a\ndifferent order.  Accordingly, the specific arrangement of steps shown in FIG. 4 should not be construed as limiting the scope of the embodiments.\nInitially, one or more event streams are obtained from one or more remote capture agents on one or more networks (operation 402).  The event streams may include event data that is generated from network data (e.g., network packets) captured by\nthe remote capture agent(s) on the network(s).  For example, the event streams may include a series of sequentially timestamped events, with each event generated from data in one or more network packets related to the event.  As a result, event data for\nthe event may include information such as an identifier, a transaction type (e.g., for an HTTP transaction and/or business transaction), a timestamp, and/or any errors associated with the event.  In addition, the event data may be associated with (e.g.,\nrepresent) clickstream data, transactions, business transactions, errors, and/or alerts.\nThe event streams may additionally include transformed event data generated from the network data and/or event data by the remote capture agent(s).  For example, the event streams may include transformed event data that is obtained by performing\naggregations, calculations, filtering, normalization, and/or formatting of the network data and/or event data at the remote capture agent(s).\nNext, one or more transformations are applied to the event stream(s) to obtain transformed event data from the event data (operation 404).  As with any transformations already applied at the remote capture agent(s), the transformation(s) may\ninclude aggregations, calculations, filtering, normalization, and/or formatting of the network data and/or event data at the remote capture agent(s).  Moreover, the transformation(s) may be applied on top of previous transformations performed by the\nremote capture agent(s), so that one round of transformations may initially be applied at the remote capture agent(s) during generation of the event streams and another round after the event streams are received from the remote capture agent(s).  Such\ntransformation(s) may be performed by one or more reactors on one or more transformation servers, as described in further detail below with respect to FIG. 7.\nThe transformation(s) may also be used to store the event data and/or transformed event data (operation 406).  For example, the transformation(s) may be used to store the event data and/or transformed event data in a database and/or log file. \nFinally, querying of the transformed event data is enabled (operation 408).  For example, the transformed event data may be indexed, and queries may be executed on the indexed, transformed event data.  The queries may further be performed in parallel on\ndifferent subsets of the transformed event data.  For example, a set of indexers may be used to index mutually exclusive time spans of the transformed event data and query the transformed event data using a map-reduce technique that operates on the time\nspans in parallel, as described in further detail below with respect to FIGS. 10-12.\nSimilarly, capturing of the network data may be divided among the remote capture agents to avoid data duplication.  In addition, the remote capture agents may execute in and/or capture the network data from one or more virtual machines running\nin a cloud-based environment.  This avoids the necessity of using a network TAP or SPAN port connection for access to and/or capturing of network data from physical network infrastructure.\n3.2.  Dynamically Configurable Remote Capture Agents for Capturing Network Data\nFIG. 5 shows a flowchart illustrating the process of facilitating the processing of network data.  More specifically, FIG. 5 shows a flowchart of configuring a remote capture agent in accordance with the disclosed embodiments.  In one or more\nembodiments, one or more of the steps may be omitted, repeated, and/or performed in a different order.  Accordingly, the specific arrangement of steps shown in FIG. 5 should not be construed as limiting the scope of the embodiments.\nFirst, configuration information for a remote capture agent is obtained at the remote capture agent from a configuration server (operation 502).  The remote capture agent may be located on a separate network from that of the configuration\nserver.  For example, the remote capture agent may be installed on a physical and/or virtual machine on a remote network and/or cloud.  As discussed above, the remote capture agent and other remote capture agents may be used to capture network data from\na set of remote networks in a distributed manner.  The captured network data may then be converted into event data that is included in a number of event streams by the remote capture agent(s).  For example, a remote capture agent may generate an event to\nbe included in an event stream by identifying one or more network packets associated with a packet and using the network data from the network packet(s) to generate event data corresponding to the event.\nThe configuration information may include a unique numeric or string identifier for each event stream to be generated by the remote capture agent.  The configuration information may also include a description and/or a descriptive name of the\nevent stream.  The configuration information may further specify an event stream type that identifies the type of event data (e.g., clickstream events, HTTP transactions, business transactions, errors, alerts, classified transactions, etc.) to be\nincluded in the event stream.  Finally, the configuration information may include a list of custom fields (e.g., for including specific pieces of network data in the events) and/or one or more additional parameters associated with generating the event\ndata (e.g., time interval between events, maximum number of cached and/or aggregated events, inclusion of matching transactions or errors in the event data, types of events used by the event stream, etc.).\nNext, the configuration information is used to configure the generation of event data from network data (e.g., from network packets) at the remote capture agent (operation 504).  For example, the configuration information may be used to\nconfigure the remote capture agent to identify certain types of network packets, extract network data from the network packets, and/or include the network data in the event data.  The configuration information may also be used to configure the\ntransformation of event data or network data into transformed event data at the remote capture agent (operation 506).\nFor example, the configuration information may specify that the event data and/or network data be aggregated into a sum, statistic (e.g., mean, median, minimum, maximum, etc.), and/or uniqueness count (e.g., number of times a unique value is\nfound in an aggregation interval).  To aggregate the event data and/or network data, a time interval associated with aggregation of the event data and/or network data may be obtained, and the event data and/or network data within the time interval may be\naggregated into an event count, statistic, and/or uniqueness count.  The configuration information may also specify a calculation (e.g., mathematical function, mathematical formula, etc.) to be performed on the network data and/or event data to produce\nthe transformed event data.  The configuration information may further provide a filter (e.g., regular expression, range of values, exact value, etc.) for removing a subset of the event data and/or network data to produce the transformed event data.  The\nconfiguration information may additionally specify a normalization that is used to transform different representations of the same value (e.g., timestamp, host name, resource name, location, etc.) into the same normalized value.  Finally, the\nconfiguration information may provide a formatting that may be applied to the event data and/or network data to generate transformed event data that adheres to a specific format.\nAfter the remote capture agent is configured, one or more event streams containing the event data and/or transformed event data from the remote capture agent are provided to one or more transformation servers for further transformation of the\nevent data and/or transformed event data by the transformation server(s) (operation 508).  For example, the event stream(s) may be transmitted over one or more networks to the transformation server(s), and the transformation server(s) may perform\nadditional aggregations, calculations, filtering, normalization, and/or formatting associated with the event data and/or transformed event data.\nAn update to the configuration information may be received (operation 512) by the remote capture agent.  For example, the update may be detected by the remote capture agent after polling the configuration server and determining that the version\nof configuration information at the configuration server is newer than the version at the remote capture agent.  The remote capture agent may then pull the update from the configuration server.  Alternatively, the update may be pushed from the\nconfiguration server to the remote capture agent.  If no update is received, the remote capture agent may continue to be used (operation 516) to capture network data as-is.\nIf an update to the configuration information is received, the update is used to reconfigure the generation and/or transformation of event data and/or network data at the remote capture agent during runtime of the remote capture agent (operation\n514).  For example, the remote capture agent may be reconfigured to generate and/or transform the event data and/or network data while the remote capture agent continues to generate event streams containing event data and/or network data according to the\nold configuration.\nThe remote capture agent may continue to be used (operation 516) to capture network data with or without reconfiguring the remote capture agent using updates to the configuration information.  If the remote capture agent is to be used, one or\nmore event streams from the remote capture agent are continually provided to one or more transformation servers for further transformation by the transformation server(s) (operation 508), and any updates to the configuration information are used to\nreconfigure the operation of the remote capture agent (operations 512-514) during generation of the event stream(s).  Capture of network data by the remote capture agent may continue until the remote capture agent is no longer used to generate event data\nand/or transformed event data from network data at the network to which the remote capture agent is connected.\nIn one or more embodiments, some or all of the configuration information is provided to the configuration server by an application used to access the transformed event data.  The application may be designed around one or more specific use cases\nassociated with network data captured by the remote capture agent, such as managing virtual machines, assessing network security, performing web analytics, and/or managing web application performance.  The application may also execute on the SPLUNK.RTM. \nENTERPRISE platform and have access to both the configuration server and event data generated by the remote capture agent.\nTo offload processing of the event data at the application (e.g., during real-time querying and/or visualization of the event data), the application may provide configuration information for performing the processing at the remote capture agent\nto the configuration server, and the configuration server may propagate the configuration information to the remote capture agent.  In turn, the remote capture agent may use the configuration to perform the processing as the event data is generated\nand/or transformed instead of requiring the application to perform significant processing the event data in real-time.  In other words, subsequent real-time processing of event data by the application and the associated overhead associated with such\nprocessing may be reduced by providing configuration information that causes the remote capture agent to transform event data into a form that can be used by the application.\nThis may integrate better with a late-binding schema, such as the late-binding schema implemented by Splunk Inc.  of San Francisco, Calif., because significant resources may be required to aggregate, format, and/or otherwise transform event data\nand extract fields at run-time.  The term \"late-binding schema\" refers to a system, such as SPLUNK.RTM.  ENTERPRISE, where the schema need not be defined at index time, as with database technology.  Rather, in a system involving late-binding schema, the\nschema can be developed on an ongoing basis up until a query, during execution, applies (binds) the schema to data to evaluate the data.  As a user learns more about the data in stored events, in a late-binding schema, he/she can continue to develop the\nschema up until the next time it is needed for a query.  Because SPLUNK.RTM.  ENTERPRISE maintains the underlying raw data and enables application of a late-binding schema, SPLUNK.RTM.  ENTERPRISE may have greater capability to enable deep exploration of\nthe data to solve problems reflected in the data and answer questions about the data than conventional databases or data-processing systems that merely store summaries or portions of data.\nFor example, a security application monitoring login attempts on a web application may use incorrect password entries by users during the login attempts to assess the security of the web application.  The security application may provide\nconfiguration information for generating event data corresponding to login failures, with the event data containing usernames, IP addresses, timestamps, and/or passwords entered for the login failures.  Because the security application may receive events\nonly when failed login attempts occur, the security application may not be required to filter the event data for failed login attempts.\nContinuing with the above example, the configuration information may specify the aggregation of failed login attempts into failed login attempts per minute.  Thus, instead of receiving an event every time a failed login attempt occurs, the\nsecurity application may receive event data every minute that indicates the number of failed login attempts for the last minute.\n3.3.  Operation of Configuration Server\nFIG. 6 shows a flowchart illustrating the process of facilitating data capture.  In particular, FIG. 6 shows a flowchart illustrating the process of operating a configuration server in accordance with the disclosed embodiments.  In one or more\nembodiments, one or more of the steps may be omitted, repeated, and/or performed in a different order.  Accordingly, the specific arrangement of steps shown in FIG. 6 should not be construed as limiting the scope of the embodiments.\nFirst, configuration information for a set of remote capture agents on a set of networks is obtained at the configuration server (operation 602).  The configuration information may be obtained from a user e, g, an administrator) and/or an\napplication used to access event data generated by the remote capture agents.  Next, the configuration server is used to provide the configuration information to the remote capture agents (operation 604).  For example, the configuration server may use a\npush and/or pull mechanism to transmit the or more event streams to generate visualizations based onhe configuration information may then be used by the remote capture agents to configure the generation and/or transformation of event data, as described\nabove.\nAn update to the configuration information may be obtained (operation 606).  For example, an update to the configuration information may be obtained to enable the generation of new event streams at one or more of the remote capture agents for\nuse with one or more new use cases associated with network data capture by the remote capture agent(s).  If an update to the configuration information is obtained, the configuration server is used to provide the update to the remote capture agents\n(operation 608), and the update is used to reconfigure the generation and/or transformation of the event data at the remote capture agents during runtime of the remote capture agents.  If no update is received, no additional configuration information may\nbe transmitted between the configuration server and remote capture agents.\nThe remote capture agents may continue to be configured (operation 610) using configuration information from the configuration server.  If the remote capture agents are to be configured using the configuration server, any updates to the\nconfiguration information are transmitted from the configuration server to the remote capture agents (operation 606-608) to enable reconfiguration of the remote capture agents.  Such transmission of updates to the configuration information to the remote\ncapture agents may continue until the configuration server is no longer used to dynamically configure the remote capture agents.\n3.4.  GUI for Configuring Event Streams\nFIG. 7 shows a flowchart illustrating the process of facilitating the processing of data.  More specifically, FIG. 7 shows a flowchart of using a GUI to obtain configuration information for managing event streams in accordance with the disclosed\nembodiments.  In one or more embodiments, one or more of the steps may be omitted, repeated, and/or performed in a different order.  Accordingly, the specific arrangement of steps shown in FIG. 5 should not be construed as limiting the scope of the\nembodiments.\nInitially, the GUI is provided for obtainining configuration information for configuring the generation of event data from network data obtained from network packets at one or more remote capture agents (operation 702).  The configuration\ninformation may be obtained using a configuration dialog of the GUI, as discussed in further detail below with respect to FIG. 9.\nNext, use of the GUI in configuring the connection of one or more event streams containing the event data to one or more reactors for subsequent processing of the event data by the reactor(s) is enabled (operation 704).  For example, graphical\nrepresentations of the event stream(s) and reactor(s) may be displayed in the GUI, and directed edges for connecting the graphical representations may be provided by the GUI.  A directed edge from one component (e.g., event stream or reactor) to another\nmay thus represent the passing of output from the component as input to the second component.  Using GUIs to connect event streams and reactors is described in further detail below with respect to FIG. 8.\nUse of the GUI in configuring the subsequent processing of the event data by the reactor(s) is also enabled (operation 706).  For example, the GUI may provide a separate configuration dialog for configuring each type of reactor used to process\nevent streams.  Finally, the configuration information is provided to the remote capture agent(s), where the configuration information is used to configure the generation of the event data at the remote capture agent(s) during runtime of the remote\ncapture agent(s).\nIn one or more embodiments, reactors are provided by one or more transformation servers that transform the event data after the event data is created and/or initially transformed at the remote capture agent(s).  As noted above, configuration\nservers may be transformation servers.  Alternatively, a configuration server may be included within a transformation server and/or execute independently from the transformation server.  The reactors may include collection reactors that collect event\nand/or network data, processing reactors that process event and/or network data, and/or storage reactors that store event and/or network data.  Within the GUI, the reactors may be represented by icons and/or other user-interface elements that may be\nselected to configure the operation of the reactors.\nFIG. 8 depicts an example screen shot of an embodiment of a GUI 800 that is adapted to display configurable components within a distributed data capture and processing system.  GUI 800 may be provided by a configuration server, such as\nconfiguration servers 120 of FIG. 1.\nIn the illustrated embodiment, GUI 800 includes two stream icons 801 and 802 that correspond to graphical representations of two event streams.  Icon 801 is connected to a filter reactor icon 803 using a directed edge, which is further connected\nto a python reactor icon 806 using another directed edge.  Filter reactor icon 803 may be a graphical representation of a filter reactor that filters event streams provided as input to the filter reactor according to one or more filtering rules (e.g.,\nregular expressions, network data types, event types, time spans, etc.) and outputs the filtered event streams.  Python reactor icon 806 may be a graphical representation of a python reactor that creates, processes, or stores events using the Python\nprogramming language.  As a result, event data from the event stream represented by stream icon 801 may be filtered by the filter reactor before being processed by the python reactor.\nAnother series of directed edges in GUI 800 may connect stream icon 802 to a cleansing transformation reactor icon 804, which in turn is connected to both a filter reactor icon 805 and an aggregator reactor icon 807.  Cleansing transformation\nreactor icon 804 may be a graphical representation of a cleansing transformation reactor that normalizes different representations of the same value into the same normalized value.  For example, the cleansing transformation reactor may convert different\ntimestamp formats into the same normalized timestamp format.  Aggregator reactor icon 807 may be a graphical representation of an aggregator reactor that aggregates event data for multiple events received during a time interval and produces new events\nrepresenting the aggregated information.  The new events may include event counts, statistics, and/or uniqueness counts related to the aggregated information.  For example, the aggregated event data may include total page views, average numbers of\nrequests, minimum RTT, and/or counts of requests for uniquely named resources.\nOther examples of reactors usable with the techniques described herein include:\n<BR><BR>Collection Reactors\nLogInputReactor: Uses a Codec to store events into log files.  SnifferReactor: Passively sniffs network packets, reassembles TCP and decrypts SSL/TLS.  Protocol plugins allow you to generate events from any type of network traffic.\n<BR><BR>Processing Reactors\nAggregateReactor: Aggregates information across multiple events received during an interval of time.  Produces new events representing the aggregated information.  Can also store historical information into external database tables and produce\nreal-time reports.  ClickstreamReactor: Sessionizes a stream of HTTP request events (or clickstream hits) by grouping them into page views and sessionizes.  Appends additional session attributes to the request events and produces two new types of events,\none each for page views and sessions.  ContentHashReactor: Performs a hashing algorithm on a content field and uses the result to populate field.  This Reactor controls which content is stored in the Stream Replay database.  FilterReactor: Uses\nconfigurable rules to detect new events, sequences or patterns.  Delivers events to the reactors it is connected to only when these occur.  FissionReactor: Used to generate multiple events derived from a single source event.  Primarily used to extract\nRSS and Atom content from individual HTTP requests.  PythonReactor: The PythonReactor allows you to build fully-featured Reactors that can create, process, or store events using the Python programming language.  ScriptReactor: Executes a shell script to\nprocess each event it receives.  SessionFilterReactor: Uses rules to detect patterns within visitor sessions.  Events for a session are queued in memory until a match is found.  If a match is found, all the session's events are passed through as output\nto other Reactors.  If no match is found, the events are discarded.  SQLReactor: Uses Database plugins to perform real-time SQL queries derived from the events that it receives.  The results of the queries can be used to add additional information to the\noriginal event.  TransformReactor: Creates new events which are derived from the events that it receives.  This can be used to create entirely new types of complex events (for example, to signify that a pattern has been detected), or to derive new\nattributes which are based on attributes in existing events (i.e. assign a new attribute to \"Internet Explorer\" if an existing attribute contains \"MSIE\").\n<BR><BR>Storage Reactors\nDatabaseOutputReactor: Stores events directly into database tables using Database plugins GoogleAnalyticsReactor: Replicates website page tags by delivering real-time clickstream events to Google Analytics using their HTTP interface. \nHTTPOutputReactor: Converts incoming events into HTTP requests.  LogOutputReactor: Uses a Codec to store events into log files.  MultiDatabaseReactor: Stores events into a collection of partitioned database tables.  Used by Stream Replay to store traffic\ninto an embedded database.  OmnitureAnalyticsReactor: Replicates website page tags by delivering real-time clickstream events to Omniture using their XML/HTTP data insertion API.  UnicaAnalyticsReactor: Replicates website page tags by delivering\nreal-time clickstream events to Webtrends Analytics using their On Demand HTTP API.  WebtrendsReactors: Replicates website page tags by delivering real-time clickstream events to Webtrends Analytics using their On Demand HTTP API.\nGUI 800 may thus provide a visual mechanism for configuring event streams that are generated from network traffic.  Users may connect graphical representations of event streams and reactors to allow filtering, cleaning, aggregating,\ntransforming, and/or other processing of events in the event streams.  Output from the reactors may then be provided to other reactors using connections (e.g., directed edges) specified in GUI 800 for further processing.\nIn addition, selecting (e.g., double-clicking) on stream icons 801-802 may invoke the configuration dialog for the corresponding event stream, which allows users to configure the generation of event data in the event stream.  FIG. 9 depicts an\nexample screen shot of an embodiment of a configuration dialog 901 for obtaining configuration information for configuring the generation of event data from network data at one or more remote capture agents.\nIn the illustrated embodiment, configuration dialog 901 includes a section 902 for specifying a descriptive stream name (e.g., \"Home Page Requests\") and an event type (e.g., \"clickstream.http-event\") associated with the event stream.  Another\nsection 903 may be used to provide terms (e.g., for clickstream data) to be included in event data the event stream.  For example, section 903 may display a list of terms (e.g., \"clickestream.c-ip,\" \"clickstream.host,\" \"clickstream.uri-stem\") to be\nincluded in the event data, as well as a mechanism 904 for adding a new term to the list.\nConfiguration dialog 901 further includes a section 905 that enables the definition of one or more filtering rules.  For example, section 905 may include a filtering rule that requires an exact match between a URI stem of an event and the value\n\"/index.html.\" Section 905 may also include a mechanism 906 for adding new filtering rules for the event stream.\n4.0.  Implementation Mechanisms\n4.1.  Exemplary Systems for Storing and Retrieving Events\nAs noted above, the visualization techniques described herein can be applied to a variety of types of events, including those generated and used in SPLUNK.RTM.  ENTERPRISE.  Further details of underlying architecture of SPLUNK.RTM.  ENTERPRISE\nare now provided.  FIG. 10 depicts an example block diagram of an embodiment of a time-based data storage architecture that includes a late-binding schema.\nGenerally, the system includes one or more forwarders 1010 that collect data from a variety of different data sources 1005 and forwards the data using forwarders 1010 to one or more data indexers 1015.  In one embodiment, forwarders 1010 and\nindexers 1015 can be implemented in one or more hardware servers.  Moreover, the functionality of one or more forwarders 1010 may be implemented by one or more remote capture agents (e.g., remote capture agents 151-153 of FIG. 1) and/or transformation\nservers.  For example, event data from a set of remote capture agents may be sent over a network to a set of transformation servers and/or reactors (e.g., collection reactors, processing reactors, storage reactors) that implement the indexing, storage\nand querying functionality of SPLUNK.RTM.  ENTERPRISE.  The data typically includes streams of time-series data.  Time-series data refers to any data that can be associated with a time stamp.  The data can be structured, unstructured, or semi-structured\nand come from files or directories.  Unstructured data may be data that is not organized to facilitate extraction of values for fields from the data, as is often the case with machine data and web logs.  The data indexers 1015 may provide the\ntime-stamped data for storage in one or more data stores 1020.\nFIG. 11 illustrates a flowchart of an example embodiment of a process for storing collected data in a data storage architecture that includes a late-binding schema.  FIG. 11 depicts a process that indexers 1015 may use to process, index, and\nstore data received from the forwarders 1010.  At operation 1105, an indexer 1015 receives data from a forwarder 1010.  At operation 1110, the data is segmented into events.  The events can be broken at event boundaries, which can include character\ncombinations and/or line breaks.  In some instances, the software discovers event boundaries automatically, and in other instances the event boundaries may be configured by the user.  A time stamp is determined for each event at operation 1115.  The time\nstamp can be determined by extracting the time from data in an event or by interpolating the time based on time stamps from other events.  In alternative embodiments, a time stamp may be determined from the time the data was received or generated.  The\ntime stamp is associated with each event at operation 1120.  For example, the time stamp may be stored as metadata for the event.\nAt operation 1125, the data included in a given event may be transformed.  Such a transformation can include such things as removing part of an event (e.g., a portion used to define event boundaries) or removing redundant portions of an event. \nA client data processing system may specify a portion to remove using a regular expression or any similar method.\nOptionally, a keyword index can be built to facilitate fast keyword searching of events.  To build such an index, in operation 1130, a set of keywords contained in the events is identified.  At operation 1135, each identified keyword is included\nin an index, which associates with each stored keyword pointers to each event containing that keyword (or locations within events where that keyword is found).  When a keyword-based query is received by an indexer, the indexer may then consult this index\nto quickly find those events containing the keyword without having to examine again each individual event, thereby greatly accelerating keyword searches.\nThe events are stored in a data store at operation 1140.  The data can be stored in working, short-term and/or long-term memory in a manner retrievable by query.  The time stamp may be stored along with each event to help optimize searching the\nevents by time range.\nIn some instances, the data store includes a plurality of individual storage buckets, each corresponding to a time range.  An event can then be stored in a bucket associated with a time range inclusive of the event's time stamp.  This not only\noptimizes time based searches, but it can allow events with recent time stamps that may have a higher likelihood of being accessed to be stored at preferable memory locations that lend to quicker subsequent retrieval (such as flash memory instead of\nhard-drive memory).\nAs shown in FIG. 10, data stores 1020 may be distributed across multiple indexers, each responsible for storing and searching a subset of the events generated by the system.  By distributing the time-based buckets among them, the indexers may\nfind events responsive to a query from a search engine 1025 in parallel using map-reduce techniques, each returning their partial responses to the query to a search head that combines the results together to answer the query.  This query handling is\nillustrated in FIG. 12.\nFIG. 12 illustrates a flowchart of an example embodiment of a process for generating a query result in a data storage architecture that includes a late-binding schema.  At operation 1205, a search heard receives a query from a search engine.  At\noperation 1210, the search head distributes the query to one or more distributed indexers.  These indexers can include those with access to data stores having events responsive to the query.  For example, the indexers can include those with access to\nevents with time stamps within part or all of a time period identified in the query.  At operation 1215, each of one or more indexers to which the query was distributed searches its data store for events responsive to the query.  To determine events\nresponsive to the query, a searching indexer finds events specified by the criteria in the query.  This criteria can include that the events have particular keywords or contain a specified value or values for a specified field or fields (because this\nemploys a late-binding schema, extraction of values from events to determine those that meet the specified criteria occurs at the time this query is processed).\nIt should be appreciated that, to achieve high availability and to provide for disaster recovery, events may be replicated in multiple data stores, in which case indexers with access to the redundant events would not respond to the query by\nprocessing the redundant events.  The indexers 1015 may either stream the relevant events back to the search head or use the events to calculate a partial result responsive to the query and send the partial result back to the search head.  At operation\n1220, the search head combines all the partial results or events received from the parallel processing together to determine a final result responsive to the query.\nData intake and query system 145 and the processes described with respect to FIGS. 10-12 are further discussed and elaborated upon in Carasso, David.  Exploring Splunk Search Processing Language (SPL) Primer and Cookbook.  New York: CITO\nResearch, 2012 and in Ledion Bitincka, Archana Ganapathi, Stephen Sorkin, and Steve Zhang.  Optimizing data analysis with a semi-structured time series database.  In SLAML, 2010.  Each of these references is hereby incorporated by reference in its\nentirety for all purposes.\n4.2.  Hardware Overview\nFIG. 13 depicts an example data processing system upon which the embodiments described herein may be implemented.  As shown in FIG. 13, the data processing system 1301 includes a system bus 1302, which is coupled to a processor 1303, a Read-Only\nMemory (\"ROM\") 1307, a Random Access Memory (\"RAM\") 1305, as well as other nonvolatile memory 1306, e.g., a hard drive.  In the illustrated embodiment, processor 1303 is coupled to a cache memory 1304.  System bus 1302 can be adapted to interconnect\nthese various components together and also interconnect components 1303, 1307, 1305, and 1306 to a display controller and display device 1308, and to peripheral devices such as input/output (\"I/O\") devices 1310.  Types of I/O devices can include\nkeyboards, modems, network interfaces, printers, scanners, video cameras, or other devices well known in the art.  Typically, I/O devices 1310 are coupled to the system bus 1302 through I/O controllers 1309.  In one embodiment the I/O controller 1309\nincludes a Universal Serial Bus (\"USB\") adapter for controlling USB peripherals or other type of bus adapter.\nRAM 1305 can be implemented as dynamic RAM (\"DRAM\"), which requires power continually in order to refresh or maintain the data in the memory.  The other nonvolatile memory 1306 can be a magnetic hard drive, magnetic optical drive, optical drive,\nDVD RAM, or other type of memory system that maintains data after power is removed from the system.  While FIG. 13 shows that nonvolatile memory 1306 as a local device coupled with the rest of the components in the data processing system, it will be\nappreciated by skilled artisans that the described techniques may use a nonvolatile memory remote from the system, such as a network storage device coupled with the data processing system through a network interface such as a modem or Ethernet interface\n(not shown).\n5.0.  Extensions and Alternatives\nWith these embodiments in mind, it will be apparent from this description that aspects of the described techniques may be embodied, at least in part, in software, hardware, firmware, or any combination thereof.  It should also be understood that\nembodiments can employ various computer-implemented functions involving data stored in a computer system.  The techniques may be carried out in a computer system or other data processing system in response executing sequences of instructions stored in\nmemory.  In various embodiments, hardwired circuitry may be used independently or in combination with software instructions to implement these techniques.  For instance, the described functionality may be performed by specific hardware components\ncontaining hardwired logic for performing operations, or by any combination of custom hardware components and programmed computer components.  The techniques described herein are not limited to any specific combination of hardware circuitry and software.\nEmbodiments herein may also be implemented in computer-readable instructions stored on an article of manufacture referred to as a computer-readable medium, which is adapted to store data that can thereafter be read and processed by a computer. \nComputer-readable media is adapted to store these computer instructions, which when executed by a computer or other data processing system such as data processing system 1300, are adapted to cause the system to perform operations according to the\ntechniques described herein.  Computer-readable media can include any mechanism that stores information in a form accessible by a data processing device such as a computer, network device, tablet, smartphone, or any device having similar functionality. \nExamples of computer-readable media include any type of tangible article of manufacture capable of storing information thereon including floppy disks, hard drive disks (\"HDDs\"), solid-state devices (\"SSDs\") or other flash memory, optical disks, digital\nvideo disks (\"DVDs\"), CD-ROMs, magnetic-optical disks, ROMs, RAMs, erasable programmable read only memory (\"EPROMs\"), electrically erasable programmable read only memory (\"EEPROMs\"), magnetic or optical cards, or any other type of media suitable for\nstoring instructions in an electronic format.  Computer-readable media can also be distributed over a network-coupled computer system stored and executed in a distributed fashion.\nThroughout the foregoing description, for the purposes of explanation, numerous specific details were set forth in order to provide a thorough understanding of the invention.  It will be apparent, however, to persons skilled in the art that\nthese embodiments may be practiced without some of these specific details.  Although various embodiments incorporating the teachings of the present invention have been shown and described in detail herein, those skilled in the art can readily devise many\nother varied embodiments that still incorporate these techniques.  Embodiments of the invention may include various operations as set forth above or fewer operations or more operations; or operations in an order, which is different from the order\ndescribed herein.  Accordingly, the scope and spirit of the invention should be judged in terms of the claims that follow as well as the legal equivalents thereof.", "application_number": "15885712", "abstract": " The disclosed embodiments provide a method and system for facilitating\n     the processing of network data. During operation, the system obtains, at\n     a remote capture agent, configuration information for the remote capture\n     agent from a configuration server over a network. Next, the system uses\n     the configuration information to configure the generation of event data\n     from network packets at the remote capture agent. Upon receiving an\n     update to the configuration information from the configuration server,\n     the system uses the update to reconfigure the generation of the event\n     data by the remote capture agent during runtime of the remote capture\n     agent.\n", "citations": ["5436618", "5787253", "5796942", "5892903", "5920711", "5983270", "6044401", "6418471", "6542861", "6584501", "6587969", "6594634", "6748343", "6792460", "6810494", "6892167", "6898556", "6915308", "6973489", "6985944", "6988141", "7016813", "7080399", "7100091", "7171689", "7177441", "7197559", "7219138", "7228348", "7228564", "7231445", "7246101", "7246162", "7257719", "7260846", "7277957", "7299277", "7376896", "7380272", "7444263", "7490066", "7519964", "7543051", "7552238", "7577623", "7594011", "7594269", "7607170", "7610344", "7623463", "7644365", "7644438", "7650396", "7650634", "7660892", "7702806", "7729240", "7735063", "7738396", "7751393", "7761918", "7774369", "7797443", "41903", "7814218", "7818370", "7836498", "7869352", "7886054", "7899444", "7921459", "7948889", "7953425", "7954109", "7962616", "7979555", "8019064", "8020211", "8032564", "8037175", "8042055", "8046443", "8056130", "8064350", "8068986", "8095640", "8103765", "8122006", "8125908", "8127000", "8140665", "8144609", "8185953", "8191136", "8195661", "8248958", "8255511", "8266271", "8301745", "8335848", "8385532", "8386371", "8386598", "8387076", "8392553", "8423894", "8473606", "8473620", "8498956", "8522219", "8533532", "8543534", "8549650", "8583772", "8589375", "8589436", "8589876", "8601122", "8676841", "8705639", "8779921", "8782787", "8806361", "8842548", "8850064", "8918430", "8958318", "8978034", "8984101", "9043439", "9098587", "9110101", "9112915", "9189449", "9244978", "9270545", "9305238", "9330395", "9405854", "9509553", "9542708", "9558225", "9680846", "9762443", "9923767", "20020015387", "20030061506", "20030101449", "20030120619", "20030135612", "20030191599", "20030221000", "20040015579", "20040030796", "20040042470", "20040044912", "20040088405", "20040109453", "20040152444", "20040215747", "20050021715", "20050060402", "20050120160", "20050131876", "20050138013", "20050138426", "20050273593", "20050278731", "20060047721", "20060077895", "20060101101", "20060198318", "20060242694", "20060256735", "20060279628", "20070011309", "20070033408", "20070043861", "20070050846", "20070067450", "20070076312", "20070083644", "20070106692", "20070121872", "20070150584", "20070156916", "20070208852", "20070260932", "20080056139", "20080082679", "20080159146", "20080209505", "20080259910", "20080281963", "20090070786", "20090129316", "20090228474", "20090238088", "20090271504", "20090319247", "20100031274", "20100064307", "20100070929", "20100095370", "20100136943", "20100153316", "20100318665", "20100318836", "20110026521", "20110029665", "20110106935", "20110166982", "20110178775", "20110231935", "20110238723", "20110246134", "20110292818", "20110296015", "20110302305", "20120017270", "20120054246", "20120084437", "20120106354", "20120158987", "20120173966", "20120197934", "20120198047", "20120239681", "20120250610", "20120278455", "20120314616", "20130024431", "20130067034", "20130070622", "20130080620", "20130091278", "20130111011", "20130114456", "20130128742", "20130132833", "20130136253", "20130173782", "20130179855", "20130182700", "20130198391", "20130212689", "20130227689", "20130232137", "20130246925", "20130258995", "20130276000", "20130318236", "20130318514", "20130318536", "20130318603", "20130318604", "20130326620", "20140012864", "20140013309", "20140046645", "20140068102", "20140173512", "20140201375", "20140230062", "20140237292", "20140280737", "20140317228", "20140317684", "20140325058", "20140325363", "20140328189", "20140351415", "20150013006", "20150062113", "20150095359", "20150125807", "20150156170", "20150178342", "20150180891", "20150295766", "20150319058", "20160028758", "20160127517", "20160155314", "20160182283", "20160323172", "20160330086", "20170237634"], "related": ["14253744"]}, {"id": "20180255090", "patent_code": "10375105", "patent_name": "Blockchain web browser interface", "year": "2019", "inventor_and_country_data": " Inventors: \nKozloski; James R. (New Fairfield, CT), Pickover; Clifford A. (Yorktown Heights, NY), Weldemariam; Komminist (Nairobi, KE)  ", "description": "<BR><BR>BACKGROUND\nThe present invention relates to the field of telecommunication devices, and particularly to telecommunication devices that are capable of browsing the World Wide Web.  Still more particularly, the present invention relates to logging browser\nevents on a computer in a distributed manner.\n<BR><BR>SUMMARY\nIn one or more embodiments of the present invention, a computer-implemented method records and maintains a record of browser events in a blockchain using a peer-to-peer network.  One or more processors detect one or more user's browser events\nfor a browser on a computer.  One or more processors then transmit transactions associated with the one or more user's browser events from the computer to a peer-to-peer network of devices that create a blockchain, which includes one or more blocks that\ndescribe the one or more user's browser events, such that the blockchain ledgers and maintains a record of historical user's browser events that occur at the computer.\nIn an embodiment of the present invention, a computer-implemented method tracks and maintains a record of disparate browser events.  One or more processors detect a browser event for a browser on a computer.  One or more processors transmit\ntransactions associated with the disparate browser events from the computer to a trans-vendor service, which operates across disparate browsers, devices, and operating systems, such that the trans-vendor service generates a blockchain ledger that\nincludes one or more blocks that comprise the transactions associated with one or more user browser events from the computer and blocks that describe the one or more user browser events from the disparate browsers, devices, and operating systems.\nThe described inventions may also be implemented in a computer system and/or as a computer program product. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nFIG. 1 depicts an exemplary system and network in which the present disclosure may be implemented;\nFIG. 2 illustrates a browser used in one or more embodiments of the present invention;\nFIG. 3 depicts an exemplary user interface with a blockchain enabling button as used in one or more embodiments of the present invention;\nFIG. 4 illustrates an exemplary blockchain architecture as used in one or more embodiments of the present invention;\nFIG. 5 depicts an exemplary blockchain topology as used in one or more embodiments of the present invention;\nFIG. 6 depicts a deployment of a blockchain using multiple validating peers in accordance with one or more embodiments of the present invention;\nFIG. 7 illustrates a high-level overview of one or more embodiments of the present invention;\nFIG. 8 is a high-level flow chart illustrating a process in accordance with one or more embodiments of the present invention;\nFIG. 9 depicts a cloud computing environment according to an embodiment of the present invention;\nFIG. 10 depicts abstraction model layers of a cloud computing environment according to an embodiment of the present invention; and\nFIG. 11 illustrates a description of an exemplary blockchain as used in one or more embodiments of the present invention.\n<BR><BR>DETAILED DESCRIPTION\nThe present invention may be a system, a method, and/or a computer program product at any possible technical detail level of integration.  The computer program product may include a computer readable storage medium (or media) having computer\nreadable program instructions thereon for causing a processor to carry out aspects of the present invention.\nThe computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device.  The computer readable storage medium may be, for example, but is not limited to, an electronic\nstorage device, a magnetic storage device, an optical storage device, an electromagnetic storage device, a semiconductor storage device, or any suitable combination of the foregoing.  A non-exhaustive list of more specific examples of the computer\nreadable storage medium includes the following: a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a static random access memory (SRAM), a\nportable compact disc read-only memory (CD-ROM), a digital versatile disk (DVD), a memory stick, a floppy disk, a mechanically encoded device such as punch-cards or raised structures in a groove having instructions recorded thereon, and any suitable\ncombination of the foregoing.  A computer readable storage medium, as used herein, is not to be construed as being transitory signals per se, such as radio waves or other freely propagating electromagnetic waves, electromagnetic waves propagating through\na waveguide or other transmission media (e.g., light pulses passing through a fiber-optic cable), or electrical signals transmitted through a wire.\nComputer readable program instructions described herein can be downloaded to respective computing/processing devices from a computer readable storage medium or to an external computer or external storage device via a network, for example, the\nInternet, a local area network, a wide area network and/or a wireless network.  The network may comprise copper transmission cables, optical transmission fibers, wireless transmission, routers, firewalls, switches, gateway computers and/or edge servers. \nA network adapter card or network interface in each computing/processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium\nwithin the respective computing/processing device.\nComputer readable program instructions for carrying out operations of the present invention may be assembler instructions, instruction-set-architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, firmware\ninstructions, state-setting data, or either source code or object code written in any combination of one or more programming languages, including an object oriented programming language such as Java, Smalltalk, C++ or the like, and conventional\nprocedural programming languages, such as the \"C\" programming language or similar programming languages.  The computer readable program instructions may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software\npackage, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server.  In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area\nnetwork (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).  In some embodiments, electronic circuitry including, for example, programmable\nlogic circuitry, field-programmable gate arrays (FPGA), or programmable logic arrays (PLA) may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to personalize the electronic\ncircuitry, in order to perform aspects of the present invention.\nAspects of the present invention are described herein with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems), and computer program products according to embodiments of the invention.  It will be\nunderstood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer readable program instructions.\nThese computer readable program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute\nvia the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.  These computer readable program instructions may also be\nstored in a computer readable storage medium that can direct a computer, a programmable data processing apparatus, and/or other devices to function in a particular manner, such that the computer readable storage medium having instructions stored therein\ncomprises an article of manufacture including instructions which implement aspects of the function/act specified in the flowchart and/or block diagram block or blocks.\nThe computer readable program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other device to cause a series of operational steps to be performed on the computer, other programmable apparatus or\nother device to produce a computer implemented process, such that the instructions which execute on the computer, other programmable apparatus, or other device implement the functions/acts specified in the flowchart and/or block diagram block or blocks.\nThe flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present invention. \nIn this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of instructions, which comprises one or more executable instructions for implementing the specified logical function(s).  In some alternative\nimplementations, the functions noted in the block may occur out of the order noted in the figures.  For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse\norder, depending upon the functionality involved.  It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special\npurpose hardware-based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.\nWith reference now to the figures, and in particular to FIG. 1, there is depicted a block diagram of an exemplary system and network that may be utilized by and/or in the implementation of the present invention.  Some or all of the exemplary\narchitecture, including both depicted hardware and software, shown for and within computer 101 may be utilized by software deploying server 149 and/or peer-to-peer devices 151 and/or trans-vendor service server 155 shown in FIG. 1, and/or peers 501a-501d\nshown in FIG. 5, and/or clients 601a-601n and/or non-validating peers 604a-604n and/or validating peers 608a-608n shown in FIG. 6.\nExemplary computer 101 includes one or more processor(s) 103 that are coupled to a system bus 105.  Processor(s) 103 may each utilize one or more core(s) 123, which contain execution units and other hardware beyond that found in the rest of the\nprocessor(s) 103 (e.g., on-board random access memory, etc.).  A video adapter 107, which drives/supports a display 109 (which may be a touch-screen display capable of detecting touch inputs onto the display 109), is also coupled to system bus 105. \nSystem bus 105 is coupled via a bus bridge 111 to an input/output (I/O) bus 113.  An I/O interface 115 is coupled to I/O bus 113.  I/O interface 115 affords communication with various I/O devices, including a camera 117, a microphone 119, a media tray\n121 (which may include storage devices such as CD-ROM drives, multi-media interfaces, etc.), and external USB port(s) 125.  While the format of the ports connected to I/O interface 115 may be any known to those skilled in the art of computer\narchitecture, in one embodiment some or all of these ports are universal serial bus (USB) ports.\nAs depicted, computer 101 is able to communicate with a software deploying server 149 and/or other devices/systems, such as peer-to-peer devices 151 and trans-vendor service server 155, using a network interface 129.  Network interface 129 is a\nhardware network interface, such as a network interface card (NIC), etc. Network 127 may be an external network such as the Internet, or an internal network such as an Ethernet or a virtual private network (VPN).  In one or more embodiments, network 127\nis a wireless network, such as a Wi-Fi network, a cellular network, etc.\nA hard drive interface 131 is also coupled to system bus 105.  Hard drive interface 131 interfaces with a hard drive 133.  In one embodiment, hard drive 133 populates a system memory 135, which is also coupled to system bus 105.  System memory\nis defined as a lowest level of volatile memory in computer 101.  This volatile memory includes additional higher levels of volatile memory (not shown), including, but not limited to, cache memory, registers and buffers.  Data that populates system\nmemory 135 includes computer 101's operating system (OS) 137 and application programs 143.\nOS 137 includes a shell 139, for providing transparent user access to resources such as application programs 143.  Generally, shell 139 is a program that provides an interpreter and an interface between the user and the operating system.  More\nspecifically, shell 139 executes commands that are entered into a command line user interface or from a file.  Thus, shell 139, also called a command processor, is generally the highest level of the operating system software hierarchy and serves as a\ncommand interpreter.  The shell provides a system prompt, interprets commands entered by keyboard, mouse, or other user input media, and sends the interpreted command(s) to the appropriate lower levels of the operating system (e.g., a kernel 141) for\nprocessing.  While shell 139 is a text-based, line-oriented user interface, the present invention will equally well support other user interface modes, such as graphical, voice, gestural, etc.\nAs depicted, OS 137 also includes kernel 141, which includes lower levels of functionality for OS 137, including providing essential services required by other parts of OS 137 and application programs 143, including memory management, process\nand task management, disk management, and mouse and keyboard management.\nApplication programs 143 include a renderer, shown in exemplary manner as a browser 145.  Browser 145 includes program modules and instructions enabling a world wide web (WWW) client (i.e., computer 101) to send and receive network messages to\nthe Internet using hypertext transfer protocol (HTTP) messaging, thus enabling communication with software deploying server 149 and other systems.\nApplication programs 143 in computer 101's system memory (as well as software deploying server 149's system memory) also include a Program for Securely Maintaining a Record of Browser Events using a Blockchain (PSMRBEB) 147.  PSMRBEB 147\nincludes code for implementing the processes described below, including those described in FIGS. 2-8.  In one embodiment, computer 101 is able to download PSMRBEB 147 from software deploying server 149, including in an on-demand basis, wherein the code\nin PSMRBEB 147 is not downloaded until needed for execution.  In one embodiment of the present invention, software deploying server 149 performs all of the functions associated with the present invention (including execution of PSMRBEB 147), thus freeing\ncomputer 101 from having to use its own internal computing resources to execute PSMRBEB 147.\nAlso within computer 101 in one or more embodiments of the present invention is a positioning device 153, which provides sensor readings describing a real-time position of computer 101.  Exemplary embodiments of positioning device 153\nincorporate the use of accelerometers, global positioning system (GPS) sensors, etc.\nThe hardware elements depicted in computer 101 are not intended to be exhaustive, but rather are representative to highlight essential components required by the present invention.  For instance, computer 101 may include alternate memory storage\ndevices such as magnetic cassettes, digital versatile disks (DVDs), Bernoulli cartridges, and the like.  These and other variations are intended to be within the spirit and scope of the present invention.\nAs described herein, one or more embodiments of the present invention provides Web browsing security while also giving the user more control.  For a user (or more specifically the events/conditions related to a Web browser), bookmarks, searches,\nwebsites (i.e., associated with specific uniform resource locator--URLs) visited, etc. are stored in a block.  The block is anchored to a user and stores the user's browser events through time in a secure manner.  The present invention then uses\nblockchain technology to securely track and maintain a record of Web browser events.  Browser transactions associated with a user/stakeholder are compiled into a chain of browser transaction blocks (referred to herein as a \"ledger\").  Thus, the chain\nand/or blockchain can be considered a chronicle of the user's browser path through time.  When a transaction occurs, one or more corresponding browser parameters (e.g., including the user's added bookmarks, searches, URLs visited, etc.) are sent to one\nor more validation modules.  These validation modules establish a validity of the transaction and generate a new block.  Once the new block has been calculated it is appended to the user/stakeholder's browser historic blockchain.\nA transaction (that is added to the block) may be any Web browser event associated with a user.  Such items include: user bookmarks, search words (e.g. in a word-based search engine), history of URLs visited, time, geolocation, device used,\ninternet protocol (IP) address of the user's computer, textual content typed into entry forms, default/user settings (e.g., browser security settings), etc. One or more of items are securely stored in a growing block.  If storing too much of this\ninformation is deemed too sensitive or unsecure, the user or system may optionally restrict content to less sensitive items like bookmarks.\nWith reference now to FIG. 2, an exemplary browser 200 as used in one or more embodiments of the present invention is presented.\nA controller 202 receives an input from a mouse 216 and/or keyboard 218 indicating what the user is searching for (e.g., a particular subject).  The controller 202 communicates with a hypertext transfer protocol (HTTP) client 204 (and/or an\noptional client 206 that is not HTTP-based), which interacts with a network interface 208, which retrieves webpages from a remote webpage server 220.\nThe controller 202 also communicates with a HyperText Markup Language (HTML) interpreter 210 (and/or an optional interpreter 212 that is not HTML-based), which directs a driver 214 to display content (including webpages) on the user's computer\ndisplay 209.\nIn one or more embodiments of the present invention, a graphical user interface (GUI) is used to control what and when something is stored in the block.  Alternatively, the use of the block(s) is automatic by enforcing appropriate access control\nprinciples.\nThus and with reference now to FIG. 3, a user interface 303 includes a toolbar 305.  The present invention adds a unique blockchain enabling button 307 that, when activated (e.g., clicked with a mouse) causes the system to store browser events\non a blockchain as described herein.  This way the GUI may present as an optional blockchain button, giving a user control over whether to enter blockchain mode or to change the frequency with which events to the block are recorded.\nAs described herein, the blockchain ledger is made of blocks that describe various historical user's browser content (e.g., transactions, parameters, etc. for a particular user).  Thus, content that may be added to a block may relate to a\nbrowser cache, browser tabs, cookies, version of the browser, a browser mode (private vs.  public), browser plugin/extensions and application programming interfaces (APIs) information, a privilege level of a browser API or a plugin/extension API, browser\nmode of access (mobile or web), changing/resetting passwords on web browser, etc.\nFurthermore and in one or more embodiments of the present invention, content may be added to the block when a user: browses the web, visits a web site, adds a bookmark, speaks a command, performs a gesture, performs a task, browses the web in a\ncertain geolocation, selects a browser GUI element to add to the block, updates the browser version, installs/updates a plugin/extension, adds a security patch, etc. In one or more embodiments, this block addition is performed only in certain\nscenarios/settings (work/home/office/government location/on behalf of another person).\nThe rate of addition of information to a block may optionally automatically change by a risk assessment or forecast.  For example, the rate may change in a setting where some kind of risk (e.g., vulnerability analysis of plugin/extension code)\nmay be forecast to increase.  For instance, this may optionally depend on the risk level or importance of the site being browsed (e.g., logging onto and browsing on banking site, accessing personal healthcare, etc.).\nThe content that is added to a block/blockchain may be a scalar or vector quantity (e.g. multidimensional).  For example, one or more risk characteristics or risk based on needs may have more than one dimension.\nThe kind or nature of the added content may be changed by some context, such as work browsing vs.  home, \"In Private\" browsing vs.  normal browsing, types and purpose of browsing (e.g. browsing for banking, healthcare, etc.).\nThe content may be added to the block in real-time, as a user browses the Web.\nThus, the present invention affords a system for storing browsing information such that privacy is preserved and places privacy in the \"hands of a user\" rather than a third party.\nIn one embodiment, the system and method of validating and determining the sensitivity of information may use information security such as that obtained from a vulnerability analysis, a taint analysis, an information flow security analysis, etc.\nThat is, the chaincodes for validating transactions implement business logic(s) for tracking and validating unsafe or malicious browser/code extensions that may allow attackers to run their own code in the victim's browser with elevated privileges.\nIn accordance with one or more embodiments of the present invention, validating the browser transactions uses a set of browser tokens representative of user or browser activities taken with respect to a browser (including critical APIs, browser\nextension APIs, etc.).  The browser tokens may include inputs such as user generated queries (including user interactions with a web browser interface), program/service execution results (e.g. vulnerability scanner, information flow analysis, malware\ndetection tools, response to a user query, etc.), user cohort and context; output such as risk assessment, forecast, etc.\nThus in one or more embodiments of the present invention, the validation device is configured for: obtaining a historical block identifier from a browser historic blockchain representative of historical activities the user conducted with respect\nto the browser; receiving one or more validity requirements with respect to the user activities on the browser; obtaining a validation token indicative of a validity of the user actions and based on the set of browser tokens comprising one or more inputs\n(such as user generated queries (including user interactions with web browser interface), program/service execution results (e.g. vulnerability scanner, information flow analysis, malware detection tools, response to a user query, etc.), user cohort and\ncontext, and outputs (risk assessment such as data sensitivity, forecast of leaking sensitive information based on the browser plugin/extension); and/or computing the chaincode block for a transaction against said validation requirement as a function of\nthe following browser parameters: the validation token, historical browser's block identifier, the set of browser tokens.\nIn one embodiment of the present invention, the rate of transaction validation and rate of adding blocks are determined based on context analysis of the user/browser data, criticality of browser APIs (e.g., API to store and retrieve user\ncredentials such as passwords on filesystem), privileged extension APIs, etc.\nIn one or more embodiments, the present invention employs a method and system for detecting or predicting privacy and security breaches based on the user browsing history, cohort (user roles--e.g. government officials, user expertize level, user\npast browsing history, etc.) and context (e.g. banking, healthcare) by obtaining historical browser's block identifiers.  The detection or prediction of a privacy or security breach may be based on real-time advanced vulnerability scanning and malicious\nintent detection algorithms (including learning based methods).\nIn one or more embodiments, the present invention tracks and stores a device history of the browser running on a mobile device or on a desktop environment.  The browsing history of the user on mobile or web or any other device will be stored on\nthe user browser history blockchain.  This will allow the user to utilize data collected and stored on the ledger from any device.\nIn one or more embodiments of the present invention, a trans-vendor service is used to provide browsing data across different Web browsers.\nIn one or more embodiments of the present invention, a blockchain fabric, such as blockchain fabric 400 depicted in FIG. 4, is used to provide the infrastructure (e.g. execution of the chaincodes) and services (e.g., Membership services such as\nIdentify management) for securely and transparently storing, tracking and managing transactions on a \"single point of truth\", that is, historical records related to browser events.  The blockchain fabric 400 maintains a verifiable record (of the single\npoint of truth) of every single transaction ever made within the system.  Once data are entered onto the blockchain, they can never be erased (immutability) or changed, a change to a record is regarded as issuing/introducing a new transaction, thus\nensuring auditability and verifiability of data.\nThe blockchain fabric 400 (also known as the \"blockchain system\", \"open blockchain\" or \"hyperledger fabric\") is based on a distributed database of records of all transactions or digital events that have been executed and shared among\nparticipating parties.  An individual transaction in the blockchain is validated or verified through a consensus mechanism (e.g., consensus services 402) incorporating a majority of the participants in the system.  This allows the participating entities\nto know for certain that a digital event (transaction) happened by creating an irrefutable record in a permissioned public ledger.\nAs mentioned above, transactions associated with an entity (e.g., a user's browser events on a Web browser interface) are compiled into a chain of \"transaction blocks\" that constitutes the lifelong record of what has happened to that entity. \nThe chain can be considered a chronicle of a browser's path through time.\nWhen a transaction is executed, its corresponding chaincode is executed by several validating peers of the system, as shown in the depiction of blockchain fabric 400 in FIG. 5.\nThus, as shown in FIG. 5, peers 501a-501d (i.e., other computers, smartphones, servers, etc.) establish the validity of the transaction parameters and, once they reach consensus, a new block is generated and appended onto the blockchain network. That is, an application process 503 running on a smart phone or other device utilizing a browser executes an application such as the depicted WebApp 505, causing a software development kit (SDK) 507 to communicate using general remote procedure calls\n(grpc) to membership services 509 that support the peer-to-peer network 511 that supports the blockchain 513 using the peers 501a-501d.\nThus, the open blockchain fabric 400 shown in FIG. 4 and FIG. 5 is a blockchain deployment topology that provides a distributed ledger, which persists and manages digital events, called transactions, shared among several participants, each\nhaving a stake in these events.  The ledger can only be updated by consensus among the participants.  Furthermore, once transactions are recorded, they can never be altered (they are immutable).  Every such recorded transaction is cryptographically\nverifiable with proof of agreement from the participants, thus providing a robust provenance mechanism tracking their origination.\nSolutions built on the open blockchain fabric 400 can be broken down into several components shown in FIG. 6: member services 606 (analogous to membership services 509 shown in FIG. 5), validating peers 608a-608n (one or more of the peers\n501a-501d shown in FIG. 5), non-validating peers 604a-604n (one or more of the peers 501a-501d shown in FIG. 5), and an application 602 (analogous to application process 503 shown in FIG. 5) used by clients 601a-601n.\nIn the context of the proposed system for smart phone event management, there can be multiple blockchains (e.g., phone history blockchain, contact record blockchain, text messages blockchain, etc.), each one having its own operating parameters\nand security requirements.\nMember services 606 manages data access.\nValidating peers 608a-608n are designated nodes that participate in consensus algorithms.  They are responsible for validating the data that gets persisted on the blockchain and also for the execution of logic called chaincode against the data\ncontained in the ledger.\nNon-validating peers 604a-604n maintain request services from member services 606 and validating peers 608a-608n on behalf of external client applications (application 602).  In one or more embodiments of the invention, non-validating peers\n604a-604n may be optionally used, with application 602 capable of direct communication with validating peers 608a-608n.\nA blockchain is a distributed database that maintains a continuously-growing list of data records hardened against tampering and revision.  It consists of data structure blocks--which may exclusively hold data in initial blockchain\nimplementations, and in another embodiment, both data and programs--with each block holding batches of individual transactions and the results of any blockchain executables.  Each block contains a timestamp and information linking it to a previous block.\nWith reference now to FIG. 11, an illustration of exemplary blockchains as used in one or more embodiments of the present invention is presented.  As shown in FIG. 11, computers 1101, 1102, 1103, 1104, 1105, and 1106 represent an exemplary\npeer-to-peer network of devices used to support a peer blockchain (in which more or fewer computers/machines may form the peer-to-peer network of devices).  Each of the computers 1101, 1102, 1103, 1104, 1105, and 1106 in the peer-to-peer network has a\nsame copy of data (e.g., data that represents browser events), as held in ledgers stored within the depicted blockchains 1108, 1109, 1110 that are associated with respective computers 1104, 1105, 1106.\nAs shown in FIG. 11, a client 1107 (e.g., the browser) sends a transaction Tx (e.g., an event that occurred with the browser) to the client's peer (depicted as computer 1101).  Computer 1101 then sends the transaction Tx to ledgers known as the\ndepicted blockchains 1108, 1109, 1110 that are associated with other peers, including the depicted computers 1102, 1104, 1105.\nBlocks within exemplary blockchain 1108 are depicted as block 1111, block 1112, and block 1113.  Block 1113 is depicted as a newest entry into ledger 1108, and includes not only the newest transactions but also a hash of the data from the older\nblock 1112, which includes a hash of the even older block 1111.  Thus, oldest blocks are made even more secure each time a new block is created, due to the hashing operations.\nAs shown in FIG. 11, computer 1105 has been designated as a leader peer according to a consensus model of the peer-to-peer network.  As such, the leader peer (computer 1105) organizes all transactions from the nodes/peers/computers 1101-1106,\nand then shares new blocks/transactions (Tx) with other nodes (e.g., computers 1103, 1106) as depicted.  The nodes/computers that receive the new block/transaction (Tx) then validate the new block/transaction.  If enough (i.e., some predefined\nquantity/percentage) of the nodes/computers validate the new block/transaction, then the new block/transaction is deemed valid for the entire peer-to-peer network of computers 1101-1106 and is added to the blockchains (including the depicted blockchains\n1108, 1109, 1110) associated with all of the nodes/peers/computers 1101-1106.\nThus, a blockchain serves as a public ledger of all transactions in a series using a peer-to-peer network/registry.  This allows users to connect to the network/registry in order to send new transactions to the blockchain, in order to verify\ntransactions, create new blocks, etc.\nAs such, a blockchain fabric uses a distributed network to maintain a digital ledger of events, thus providing excellent security for the digital ledger, since the blockchain stored in each peer is dependent upon earlier blocks, which provide\nencryption data for subsequent blocks in the blockchain.\nTransactions are the content that is stored in the blocks in the blockchain.  The validating peers 608 in the blockchain confirm that the transactions are valid (i.e., legitimate).  That is, the system implementing the blockchain defines a valid\ntransaction.  In one or more embodiments of the present invention, a valid transaction is digitally signed.\nBlocks contain a description of the transactions.  Blocks are created by users known as \"miners\" who use specialized software or equipment designed specifically to create blocks.  In a web browser security system, miners may be non-validating\npeers 604 or validating peers 608.\nThus, the open blockchain fabric 400 provides a decentralized system in which every node in a decentralized system has a copy of the blockchain.  This avoids the need to have a centralized database managed by a trusted third party.  Transactions\nare broadcast to the network using software applications.  Network nodes can validate transactions, add them to their copy and then broadcast these additions to other nodes.  However, as noted above, the blockchain is nonetheless highly secure, since\neach new block is protected (e.g., encrypted) based on one or more previous blocks.\nOther parameters stored in the block may include a domain name, a unique identification code for a web browser, etc. Fragments of cached web pages from a user device may be added.\nFurther and in one or more embodiments, the present invention supports a browser privacy mode in a plugin of the browser, which may use blockchain.  Even if using this mode triggers locally cached data to exist entirely in volatile memory\nwithout saving to persistent storage, the block can be updated with some portions of this information.  In this way, standard opt-in privacy methods which are broad and catch-all (e.g., cookies enablement) can be specified for only certain marketing,\nvendor, or web service providers.  Cookies can then be granted access to the block chain based on a key or other de-encryption mechanism, allowing individuals to control their own private web browsing data using block-chain.\nWith reference now to FIG. 7, a high-level overview of one or more embodiments of the present invention is presented.\nAs shown in block 701, a browser performs a transaction (e.g., retrieves a webpage, searches for a term, stores a cookie, etc.).\nAs shown in block 703, the user then signs/encrypts the transaction using a private key, and broadcasts the signed/encrypted transaction to a peer-to-peer network (e.g., peer-to-peer network 511 shown in FIG. 6), as described in block 705.\nThus, the actions performed in blocks 701, 703, and 705 are performed on the computer that is running the browser.\nAs shown in block 707, a new user (e.g., one of the peers 501a-501d shown in FIG. 5) then aggregates transactions into a \"block\" (i.e., a block that contains a description of the transaction created in block 701).  As shown in block 709, a new\nuser (preferably a different peer from peers 501a-501d that created the \"block\") generates a proof-of-work to validate the \"block\".  That is, as described in block 711, each block is verified by the validator as containing a link to a previous block,\nthus making it able to be incorporated into the blockchain.  More specifically, each new block is encrypted using information in the previous block in the blockchain, thus ensuring the security of the blockchain.\nAs described in block 713, the block (describing the newly created transaction) is then broadcast into the peer-to-peer network.  As described in block 715, if two blocks attempt to validate the same transaction, then the largest blockchain is\nused.  That is, two blockchains may be appropriate for incorporating the new block.  If so, then the blockchain that is the largest will be given preference for receiving and incorporating the new block.\nThus, the actions performed in blocks 707, 709, and 713 are validator/miner actions (rather than user actions).\nAs shown in block 717, the user (i.e., the computer on which the browser is running) listens for new blocks from the peer-to-peer network, and merges them into local ledgers (i.e., records of transactions/actions/settings/etc.) on the computer.\nAs described herein, the presently-described blockchain-implemented system may be used to facilitate any of:\nLock In Attribution: The present system can create a permanent and unbreakable link between the user and his browsing information.  That link--the record of browsing information--can be forever verified and tracked.\nSecurely Share: The present system can securely share a user's digital browsing information with others.  Transferring browsing information is made as easy as transferring or copying a browsing information record.\nWith reference now to FIG. 8, a high-level flow chart illustrates a computer-implemented method for recording and maintaining a record of browser events in a blockchain in accordance with one or more embodiments of the present invention.\nAfter initiator block 802, one or more processors (e.g., processor(s) 103 shown in FIG. 1) detect one or more browser events for a browser (e.g., browser 145 shown in FIG. 1) on a computer (e.g., computer 101 shown in FIG. 1), as described in\nblock 804.  These events may be retrieving a webpage, accepting a cookie, updating the browser, searching for a particular term on the World Wide Web, etc.).\nAs described in block 806, one or more processors then transmit transactions associated with one or more of the user's browser events from the computer to a peer-to-peer network of devices (e.g., peer-to-peer network 511 shown in FIG. 5) that\ncreate a blockchain (e.g., blockchain 513).  As described herein, the blockchain includes one or more blocks that describe the one or more browser events for the browser on the computer, such that the blockchain records and maintains a record of browser\nevents that occur at the computer.  In one embodiment, the record is for all browser events that occur at the computer.  In another embodiment, the record is for a portion of the browser events that occur at the computer.\nThe flow-chart ends at terminator block 808.\nIn an embodiment of the present invention, the computer-implemented method further comprises receiving, by the computer, the blockchain from the peer-to-peer network; and merging, by the computer, blocks from the blockchain into a ledger of\nbrowser events in the computer.  That is, information found in the blocks of the blockchain can then be extracted and stored on local ledgers in the computer on which the browser events occurred.\nIn an embodiment of the present invention, the record of browser events maintained in the blockchain includes records of uniform resource locators (URLs) browsed by the browser, bookmarks added to the browser, and search terms searched by the\nbrowser.\nIn an embodiment of the present invention, the record of browser events maintained in the blockchain includes a record of cookies sent to the computer, wherein the cookie are files that hold data specific to the computer and websites visited by\nthe computer.\nIn an embodiment of the present invention, the computer-implemented method further comprises adjusting a frequency of transmitting the descriptions of the one or more browser events from the computer to the description of the one or more events\nfrom the computer to the peer-to-peer network of devices according to a current location of the computer.\nIn an embodiment of the present invention, the computer-implemented method further includes adjusting, by one or more processors, a frequency of transmitting the transactions associated with one or more of the user's browser events from the\ncomputer to peer-to-peer network of devices according to types of webpages being browsed by the computer.  For example, if the types of webpages being browsed (e.g., searched for and retrieved) contain highly transitory information (i.e., is scrubbed\nfrom this type of website every five minutes), and the browser events are retrieving this highly transitory information, then the blockchain may be updated (i.e., by \"transmitting the transactions associated with one or more of the user's browser events\nfrom the computer to peer-to-peer network of devices\") every minute, in order to ensure that the blockchain contains the highly transitory information.\nIn an embodiment of the present invention, the computer-implemented method further includes adjusting, by one or more processors, a frequency of transmitting the transactions associated with the one or more user's browser events from the\ncomputer to the transactions of the one or more events from the telecommunication device to the peer-to-peer network of devices according to types of webpages being browsed by the computer.\nIn an embodiment of the present invention, the computer-implemented method further comprises additional steps related to validating transactions.  One or more processors obtain a historical block identifier from a browser historic blockchain\nthat is representative of historical activities the user conducted with respect to the browser.  That is, the browser historic block identifier identifies blocks in the blockchain that already exist (e.g., for a particular user).  The processor(s)\nreceive one or more validity requirements with respect to the user activities on the browser.  That it, the activities of a particular user of the blockchain need to be valid (e.g., authorized, accurate, etc.) according to these validity requirements. \nAs such, the processors obtain a validation token that is indicative of a validity of the user actions and is based on a set of browser tokens (which are representative of user or browser activities taken with respect to a browser (including critical\nAPIs, browser extension APIs, etc.)) That is, the browser tokens may be based on inputs such as user generated queries (including user interactions with web browser interface), program/service execution results (e.g. vulnerability scanner, information\nflow analysis, malware detection tools, response to a user query, etc.), user cohort (e.g., which group the user belongs to) and context (e.g., what the user is doing at a particular time), and outputs (e.g., risk assessment such as data sensitivity,\nforecast of leaking sensitive information based on the browser plugin/extension).  The processor(s) then compute the chaincode block for a transaction against the validation requirement.  This computation is a function of the following browser\nparameters: the validation token, the historical browser's block identifier, and the set of browser tokens.\nIn an embodiment of the present invention, the computer-implemented method further includes performing, by one or more processors, a vulnerability analysis for the computer, where the vulnerability analysis determines how vulnerable the computer\nis to a malicious attack; and adjusting, by one or more processors, a frequency of transmitting the transactions associated with the one or more browser events (of the user) from the computer to the peer-to-peer network of devices according to how\nvulnerable the computer is to the malicious attack.  That is, a vulnerability analysis for the browser/computer (i.e., the \"browser vulnerability\") may determine, based on the lack of a firewall for the computer, a history of the computer being\nvulnerable to attacks, defective browser plugins, risky security settings (e.g., setting security settings on the browser so low that the browser is susceptible to attacks), unauthorized browser helper objects, etc., that computer 101 (and thus the\nbrowser) is highly vulnerable to attacks.  As such, the blockchain is updated more frequently (\"frequency of transmitting the transactions associated with the one or more user's browser events from the computer to the peer-to-peer network of devices\"),\nsuch that if the computer 101 is attacked, the blockchain will contain the most current browser history.\nIn an embodiment of the present invention, the peer-to-peer network selectively generates blocks to be incorporated into the blockchain for only predefined types of browser events that are received from the computer.  For example, assume that\npeer-to-peer network 511 shown in FIG. 5 receives browser event information about 1) a history of web sites retrieved by the browser and 2) what cookies were stored by the browser.  Peer-to-peer network 511 may be authorized to create blockchains that\ncontain the search history (history of websites retrieved by the browser), but not to create a blockchain that contains cookie information.  As such, peer-to-peer network 511 would create a blockchain for the search history, but would ignore the cookie\ninformation sent from the application process 503.\nIn an embodiment of the present invention, the computer-implemented method further includes transmitting, by one or more processors, the transaction associated with the one or more user's events to a validation module in the peer-to-peer\nnetwork, where the one or more blocks are not added to the blockchain until the validation module validates the description of the one or more events and the one or more blocks, as described in FIG. 6.\nWhile the present invention has been described primarily in the context of a peer-to-peer network creating the blockchain, a trans-vendor service may create the blockchain in one or more embodiments.  This trans-vendor service is a service that\n1) is able to receive information from different types of browsers, devices, operating systems, etc., and nonetheless still be able to 2) create a distributed ledger using blocks from these disparate systems/carriers.  This allows the invention to create\na blockchain that is not specific for a particular type of browser, but rather describes events that occur in various browsers on disparate computers using disparate operating systems etc. Thus, in an embodiment of the present invention, a\ncomputer-implemented method of tracking and maintaining a record of disparate browser events includes: detecting, by one or more processors, one or more browser events for a browser on a computer; and transmitting, by one or more processors, transactions\nassociated with the one or more browser events of the user from the computer to a trans-vendor service, where the trans-vendor service operates across disparate browsers, devices, and operating systems, and wherein the trans-vendor service generates a\ndistributed ledger that includes one or more blocks that include the transactions associated with the one or more browser events of the user from the computer and blocks that describe browser events from the disparate browsers, devices, and operating\nsystems.\nThe present invention may be implemented in one or more embodiments using cloud computing.  Nonetheless, it is understood in advance that although this disclosure includes a detailed description on cloud computing, implementation of the\nteachings recited herein is not limited to a cloud computing environment.  Rather, embodiments of the present invention are capable of being implemented in conjunction with any other type of computing environment now known or later developed.\nCloud computing is a model of service delivery for enabling convenient, on-demand network access to a shared pool of configurable computing resources (e.g. networks, network bandwidth, servers, processing, memory, storage, applications, virtual\nmachines, and services) that can be rapidly provisioned and released with minimal management effort or interaction with a provider of the service.  This cloud model may include at least five characteristics, at least three service models, and at least\nfour deployment models.\nCharacteristics are as follows:\nOn-demand self-service: a cloud consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with the service's provider.\nBroad network access: capabilities are available over a network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, laptops, and PDAs).\nResource pooling: the provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to demand.  There is a sense of\nlocation independence in that the consumer generally has no control or knowledge over the exact location of the provided resources but may be able to specify location at a higher level of abstraction (e.g., country, state, or datacenter).\nRapid elasticity: capabilities can be rapidly and elastically provisioned, in some cases automatically, to quickly scale out and rapidly released to quickly scale in. To the consumer, the capabilities available for provisioning often appear to\nbe unlimited and can be purchased in any quantity at any time.\nMeasured service: cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported providing transparency for both the provider and consumer of the utilized service.\nSoftware as a Service (SaaS): the capability provided to the consumer is to use the provider's applications running on a cloud infrastructure.  The applications are accessible from various client devices through a thin client interface such as a\nweb browser (e.g., web-based e-mail).  The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage, or even individual application capabilities, with the possible exception of limited\nuser-specific application configuration settings.\nPlatform as a Service (PaaS): the capability provided to the consumer is to deploy onto the cloud infrastructure consumer-created or acquired applications created using programming languages and tools supported by the provider.  The consumer\ndoes not manage or control the underlying cloud infrastructure including networks, servers, operating systems, or storage, but has control over the deployed applications and possibly application hosting environment configurations.\nInfrastructure as a Service (IaaS): the capability provided to the consumer is to provision processing, storage, networks, and other fundamental computing resources where the consumer is able to deploy and run arbitrary software, which can\ninclude operating systems and applications.  The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, deployed applications, and possibly limited control of select networking components\n(e.g., host firewalls).\nDeployment Models are as follows:\nPrivate cloud: the cloud infrastructure is operated solely for an organization.  It may be managed by the organization or a third party and may exist on-premises or off-premises.\nCommunity cloud: the cloud infrastructure is shared by several organizations and supports a specific community that has shared concerns (e.g., mission, security requirements, policy, and compliance considerations).  It may be managed by the\norganizations or a third party and may exist on-premises or off-premises.\nPublic cloud: the cloud infrastructure is made available to the general public or a large industry group and is owned by an organization selling cloud services.\nHybrid cloud: the cloud infrastructure is a composition of two or more clouds (private, community, or public) that remain unique entities but are bound together by standardized or proprietary technology that enables data and application\nportability (e.g., cloud bursting for load-balancing between clouds).\nA cloud computing environment is service oriented with a focus on statelessness, low coupling, modularity, and semantic interoperability.  At the heart of cloud computing is an infrastructure comprising a network of interconnected nodes.\nReferring now to FIG. 9, illustrative cloud computing environment 50 is depicted.  As shown, cloud computing environment 50 comprises one or more cloud computing nodes 10 with which local computing devices used by cloud consumers, such as, for\nexample, personal digital assistant (PDA) or cellular telephone 54A, desktop computer 54B, laptop computer 54C, and/or automobile computer system 54N may communicate.  Nodes 10 may communicate with one another.  They may be grouped (not shown) physically\nor virtually, in one or more networks, such as Private, Community, Public, or Hybrid clouds as described hereinabove, or a combination thereof.  This allows cloud computing environment 50 to offer infrastructure, platforms and/or software as services for\nwhich a cloud consumer does not need to maintain resources on a local computing device.  It is understood that the types of computing devices 54A-54N shown in FIG. 9 are intended to be illustrative only and that computing nodes 10 and cloud computing\nenvironment 50 can communicate with any type of computerized device over any type of network and/or network addressable connection (e.g., using a web browser).\nReferring now to FIG. 10, a set of functional abstraction layers provided by cloud computing environment 50 (FIG. 9) is shown.  It should be understood in advance that the components, layers, and functions shown in FIG. 10 are intended to be\nillustrative only and embodiments of the invention are not limited thereto.  As depicted, the following layers and corresponding functions are provided:\nHardware and software layer 60 includes hardware and software components.  Examples of hardware components include: mainframes 61; RISC (Reduced Instruction Set Computer) architecture based servers 62; servers 63; blade servers 64; storage\ndevices 65; and networks and networking components 66.  In some embodiments, software components include network application server software 67 and database software 68.\nVirtualization layer 70 provides an abstraction layer from which the following examples of virtual entities may be provided: virtual servers 71; virtual storage 72; virtual networks 73, including virtual private networks; virtual applications\nand operating systems 74; and virtual clients 75.\nIn one example, management layer 80 may provide the functions described below.  Resource provisioning 81 provides dynamic procurement of computing resources and other resources that are utilized to perform tasks within the cloud computing\nenvironment.  Metering and Pricing 82 provide cost tracking as resources are utilized within the cloud computing environment, and billing or invoicing for consumption of these resources.  In one example, these resources may comprise application software\nlicenses.  Security provides identity verification for cloud consumers and tasks, as well as protection for data and other resources.  User portal 83 provides access to the cloud computing environment for consumers and system administrators.  Service\nlevel management 84 provides cloud computing resource allocation and management such that required service levels are met.  Service Level Agreement (SLA) planning and fulfillment 85 provide pre-arrangement for, and procurement of, cloud computing\nresources for which a future requirement is anticipated in accordance with an SLA.\nWorkloads layer 90 provides examples of functionality for which the cloud computing environment may be utilized.  Examples of workloads and functions which may be provided from this layer include: mapping and navigation 91; software development\nand lifecycle management 92; virtual classroom education delivery 93; data analytics processing 94; transaction processing 95; and browser event storage and maintenance processing 96, which performs one or more of the features of the present invention\ndescribed herein.\nThe terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the present invention.  As used herein, the singular forms \"a\", \"an\" and \"the\" are intended to include the plural\nforms as well, unless the context clearly indicates otherwise.  It will be further understood that the terms \"comprises\" and/or \"comprising,\" when used in this specification, specify the presence of stated features, integers, steps, operations, elements,\nand/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof.\nThe corresponding structures, materials, acts, and equivalents of all means or step plus function elements in the claims below are intended to include any structure, material, or act for performing the function in combination with other claimed\nelements as specifically claimed.  The description of various embodiments of the present invention has been presented for purposes of illustration and description, but is not intended to be exhaustive or limited to the present invention in the form\ndisclosed.  Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the present invention.  The embodiment was chosen and described in order to best explain the principles of\nthe present invention and the practical application, and to enable others of ordinary skill in the art to understand the present invention for various embodiments with various modifications as are suited to the particular use contemplated.\nAny methods described in the present disclosure may be implemented through the use of a VHDL (VHSIC Hardware Description Language) program and a VHDL chip.  VHDL is an exemplary design-entry language for Field Programmable Gate Arrays (FPGAs),\nApplication Specific Integrated Circuits (ASICs), and other similar electronic devices.  Thus, any software-implemented method described herein may be emulated by a hardware-based VHDL program, which is then applied to a VHDL chip, such as a FPGA.\nHaving thus described embodiments of the present invention of the present application in detail and by reference to illustrative embodiments thereof, it will be apparent that modifications and variations are possible without departing from the\nscope of the present invention defined in the appended claims.", "application_number": "15448995", "abstract": " A computer-implemented method records and maintains a record of browser\n     events in a blockchain using a peer-to-peer network. One or more\n     processors detect one or more browser events for a browser on a computer.\n     One or more processors then transmit transactions that are associated\n     with the one or more browser events from the computer to a peer-to-peer\n     network of devices that create a blockchain, which includes one or more\n     blocks that describe the one or more browser events, such that the\n     blockchain records and maintains a record of browser events that occur at\n     the computer.\n", "citations": ["6073241", "7552081", "8763136", "9137319", "9990418", "20080162292", "20100138437", "20140279623", "20150206106", "20150332283", "20170221029", "20170295180", "20170324738", "20180150835", "20180197186"], "related": []}, {"id": "20180288119", "patent_code": "10375135", "patent_name": "Method and system for event pattern guided mobile content services", "year": "2019", "inventor_and_country_data": " Inventors: \nLee; Kuo-Chu (Princeton Junction, NJ), Loeb; Shoshana (Philadelphia, PA)  ", "description": "<BR><BR>BACKGROUND\nRecent developments in wearable devices such as, for example, the GOOGLE GLASS, IWATCH models, FITBIT wrist bands and the like, have enabled new types of pervasive and personalized services in healthcare, fitness, and training.  Combining these\nwearable devices with three-dimensional (3D) graphics engines, social networking and crowd sourcing platforms, mobile apps and games may contribute towards the emerging trend of real-world activities in 3D virtual worlds and create rich 3D\naugmented/virtual reality content that may be shared among users of the mobile apps and games.  In addition to the content required for the application itself, additional on-line help support content such as tutorials, therapies, recipes, coaching\ninstructions, and warning messages may also be created and delivered together with applications or offered as services to the users.\nSome rich 3D content may be used as live coaching tutorials for education, healthcare, fitness, and entertainment purposes.  For example, a mobile app or game may support a wearable or bike mounted camera with location and orientation sensors, a\nmicrophone for voice activated commands, a wrist band for temperature and heart rate monitoring.  In addition, the mobile app or game may detect, recognize and project real-world sensor data to create an avatar in a 3D graphics engine with gesture\nrecognition sensors (e.g., MICROSOFT KINECT, SOFTKINECT Camera, LEAP MOTION Controller and the like).  While these new technologies may create much richer content, the technologies may also introduce more complexity for supporting live users in real-time\nwith minimal or no manual intervention.\nThis rich content may be created using web content and 3D animation recording tools, which may involve multiple manual steps in content design, development, editing, and publishing.  The published content may be consumed by users through\nadvertising, search, delivery, and display services.\n<BR><BR>SUMMARY\nA method and system for event pattern (EP) guided content services are disclosed.  A service entity, which may include one or more processors, may receive sensor events from a plurality of users, where the sensor events correspond to behavior\ndata detected from the plurality of users, identify content creator candidates from among the plurality of users based on a pattern of events which correspond to sensor events received from an active user, and select a content creator from among the\ncontent creator candidates based on a best match to the pattern of events.  The service entity may transmit a request for content of the selected content creator.  The service entity may deliver the content created by the content creator to the active\nuser and may monitor progress of the active user based on a playback of the content by the active user.\nIn addition, the service entity may receive automatically sensor events of the active user on a continuous real-time basis, and the service entity may determine automatically from the sensor events whether a user behavior is abnormal.  The\nservice entity may generate a behavior model for detecting abnormal behavior based on the received sensor events.  Further, if the user behavior is abnormal, the service entity may generate the pattern of events based on the sensor events.\nThe progress of the requestor entity may be monitored by the service entity.  A positive point or rating may be given to the content based on the progress.  The system may maintain a set of high ranking content creators and associated recorded\ncontent.  Older or lower ranked content may be archived and removed.\nAn EP guided content services system may perform the content selection and matching methods.  An application programming interface (API) and a network element may also perform the content selection and matching methods. <BR><BR>BRIEF DESCRIPTION\nOF THE DRAWINGS\nA more detailed understanding may be had from the following description, given by way of example in conjunction with the accompanying drawings wherein:\nFIG. 1A is a system diagram of an example communications system in which one or more disclosed embodiments may be implemented;\nFIG. 1B is a system diagram of an example wireless transmit/receive unit (WTRU) that may be used within the communications system illustrated in FIG. 1A;\nFIG. 1C is a system diagram of an example radio access network and an example core network that may be used within the communications system illustrated in FIG. 1A;\nFIG. 1D is a system diagram of an example communications system in which one or more disclosed embodiments may be implemented;\nFIG. 2 is a diagram of an example \"Pattern of Events\" guided content creation and consumption service;\nFIG. 3 is a diagram of an example of dynamic event pattern detection and matching;\nFIG. 4 is a flow diagram of an example of user data collection and delivery service.\nFIG. 5 is a diagram of an example of healthcare hip replacement rehab routine training and motivation content generation;\nFIG. 6 is a diagram of an example education game with real-time guidance and warnings generated from qualified players; and\nFIG. 7 is a diagram of an example fitness game with real-time guidance and warnings generated from qualified players.\n<BR><BR>DETAILED DESCRIPTION\nFIG. 1A is a diagram of an example communications system 100 in which one or more disclosed embodiments may be implemented.  The communications system 100 may be a multiple access system that provides content, such as voice, data, video,\nmessaging, broadcast, etc., to multiple wireless users.  The communications system 100 may enable multiple wireless users to access such content through the sharing of system resources, including wireless bandwidth.  For example, the communications\nsystems 100 may employ one or more channel access methods, such as code division multiple access (CDMA), time division multiple access (TDMA), frequency division multiple access (FDMA), orthogonal FDMA (OFDMA), single-carrier FDMA (SC-FDMA), and the\nlike.\nAs shown in FIG. 1A, the communications system 100 may include wireless transmit/receive units (WTRUs) 102a, 102b, 102c, 102d, a radio access network (RAN) 104, a core network 106, a public switched telephone network (PSTN) 108, the Internet\n110, and other networks 112, though it will be appreciated that the disclosed embodiments contemplate any number of WTRUs, base stations, networks, and/or network elements.  Each of the WTRUs 102a, 102b, 102c, 102d may be any type of device configured to\noperate and/or communicate in a wireless environment.  By way of example, the WTRUs 102a, 102b, 102c, 102d may be configured to transmit and/or receive wireless signals and may include user equipment (UE), a mobile station, a fixed or mobile subscriber\nunit, a pager, a cellular telephone, a personal digital assistant (PDA), a smartphone, a laptop, a netbook, a personal computer, a wireless sensor, consumer electronics, and the like.\nThe communications systems 100 may also include a base station 114a and a base station 114b.  Each of the base stations 114a, 114b may be any type of device configured to wirelessly interface with at least one of the WTRUs 102a, 102b, 102c, 102d\nto facilitate access to one or more communication networks, such as the core network 106, the Internet 110, and/or the other networks 112.  By way of example, the base stations 114a, 114b may be a base transceiver station (BTS), a Node-B, an eNode B, a\nHome Node B, a Home eNode B, a site controller, an access point (AP), a wireless router, and the like.  While the base stations 114a, 114b are each depicted as a single element, it will be appreciated that the base stations 114a, 114b may include any\nnumber of interconnected base stations and/or network elements.\nThe base station 114a may be part of the RAN 104, which may also include other base stations and/or network elements (not shown), such as a base station controller (BSC), a radio network controller (RNC), relay nodes, etc. The base station 114a\nand/or the base station 114b may be configured to transmit and/or receive wireless signals within a particular geographic region, which may be referred to as a cell (not shown).  The cell may further be divided into cell sectors.  For example, the cell\nassociated with the base station 114a may be divided into three sectors.  Thus, in one embodiment, the base station 114a may include three transceivers, i.e., one for each sector of the cell.  In another embodiment, the base station 114a may employ\nmultiple-input multiple-output (MIMO) technology and, therefore, may utilize multiple transceivers for each sector of the cell.\nThe base stations 114a, 114b may communicate with one or more of the WTRUs 102a, 102b, 102c, 102d over an air interface 116, which may be any suitable wireless communication link (e.g., radio frequency (RF), microwave, infrared (IR), ultraviolet\n(UV), visible light, etc.).  The air interface 116 may be established using any suitable radio access technology (RAT).\nMore specifically, as noted above, the communications system 100 may be a multiple access system and may employ one or more channel access schemes, such as CDMA, TDMA, FDMA, OFDMA, SC-FDMA, and the like.  For example, the base station 114a in\nthe RAN 104 and the WTRUs 102a, 102b, 102c may implement a radio technology such as Universal Mobile Telecommunications System (UMTS) Terrestrial Radio Access (UTRA), which may establish the air interface 116 using wideband CDMA (WCDMA).  WCDMA may\ninclude communication protocols such as High-Speed Packet Access (HSPA) and/or Evolved HSPA (HSPA+).  HSPA may include High-Speed Downlink Packet Access (HSDPA) and/or High-Speed Uplink Packet Access (HSUPA).\nIn another embodiment, the base station 114a and the WTRUs 102a, 102b, 102c may implement a radio technology such as Evolved UMTS Terrestrial Radio Access (E-UTRA), which may establish the air interface 116 using Long Term Evolution (LTE) and/or\nLTE-Advanced (LTE-A).\nIn other embodiments, the base station 114a and the WTRUs 102a, 102b, 102c may implement radio technologies such as IEEE 802.16 (i.e., Worldwide Interoperability for Microwave Access (WiMAX)), CDMA2000, CDMA2000 1.times., CDMA2000 EV-DO, Interim\nStandard 2000 (IS-2000), Interim Standard 95 (IS-95), Interim Standard 856 (IS-856), Global System for Mobile communications (GSM), Enhanced Data rates for GSM Evolution (EDGE), GSM EDGE (GERAN), and the like.\nThe base station 114b in FIG. 1A may be a wireless router, Home Node B, Home eNode B, or access point, for example, and may utilize any suitable RAT for facilitating wireless connectivity in a localized area, such as a place of business, a home,\na vehicle, a campus, and the like.  In one embodiment, the base station 114b and the WTRUs 102c, 102d may implement a radio technology such as IEEE 802.11 to establish a wireless local area network (WLAN).  In another embodiment, the base station 114b\nand the WTRUs 102c, 102d may implement a radio technology such as IEEE 802.15 to establish a wireless personal area network (WPAN).  In yet another embodiment, the base station 114b and the WTRUs 102c, 102d may utilize a cellular-based RAT (e.g., WCDMA,\nCDMA2000, GSM, LTE, LTE-A, etc.) to establish a picocell or femtocell.  As shown in FIG. 1A, the base station 114b may have a direct connection to the Internet 110.  Thus, the base station 114b may not be required to access the Internet 110 via the core\nnetwork 106.\nThe RAN 104 may be in communication with the core network 106, which may be any type of network configured to provide voice, data, applications, and/or voice over internet protocol (VoIP) services to one or more of the WTRUs 102a, 102b, 102c,\n102d.  For example, the core network 106 may provide call control, billing services, mobile location-based services, pre-paid calling, Internet connectivity, video distribution, etc., and/or perform high-level security functions, such as user\nauthentication.  Although not shown in FIG. 1A, it will be appreciated that the RAN 104 and/or the core network 106 may be in direct or indirect communication with other RANs that employ the same RAT as the RAN 104 or a different RAT.  For example, in\naddition to being connected to the RAN 104, which may be utilizing an E-UTRA radio technology, the core network 106 may also be in communication with another RAN (not shown) employing a GSM radio technology.\nThe core network 106 may also serve as a gateway for the WTRUs 102a, 102b, 102c, 102d to access the PSTN 108, the Internet 110, and/or other networks 112.  The PSTN 108 may include circuit-switched telephone networks that provide plain old\ntelephone service (POTS).  The Internet 110 may include a global system of interconnected computer networks and devices that use common communication protocols, such as the transmission control protocol (TCP), user datagram protocol (UDP) and the\ninternet protocol (IP) in the TCP/IP internet protocol suite.  The networks 112 may include wired or wireless communications networks owned and/or operated by other service providers.  For example, the networks 112 may include another core network\nconnected to one or more RANs, which may employ the same RAT as the RAN 104 or a different RAT.\nSome or all of the WTRUs 102a, 102b, 102c, 102d in the communications system 100 may include multi-mode capabilities, i.e., the WTRUs 102a, 102b, 102c, 102d may include multiple transceivers for communicating with different wireless networks\nover different wireless links.  For example, the WTRU 102c shown in FIG. 1A may be configured to communicate with the base station 114a, which may employ a cellular-based radio technology, and with the base station 114b, which may employ an IEEE 802\nradio technology.\nFIG. 1B is a system diagram of an example WTRU 102.  As shown in FIG. 1B, the WTRU 102 may include a processor 118, a transceiver 120, a transmit/receive element 122, a speaker/microphone 124, a keypad 126, a display/touchpad 128, non-removable\nmemory 130, removable memory 132, a power source 134, a global positioning system (GPS) chipset 136, and other peripherals 138.  It will be appreciated that the WTRU 102 may include any sub-combination of the foregoing elements while remaining consistent\nwith an embodiment.\nThe processor 118 may be a general purpose processor, a special purpose processor, a conventional processor, a digital signal processor (DSP), a plurality of microprocessors, one or more microprocessors in association with a DSP core, a\ncontroller, a microcontroller, Application Specific Integrated Circuits (ASICs), Field Programmable Gate Array (FPGAs) circuits, any other type of integrated circuit (IC), a state machine, and the like.  The processor 118 may perform signal coding, data\nprocessing, power control, input/output processing, and/or any other functionality that enables the WTRU 102 to operate in a wireless environment.  The processor 118 may be coupled to the transceiver 120, which may be coupled to the transmit/receive\nelement 122.  While FIG. 1B depicts the processor 118 and the transceiver 120 as separate components, it will be appreciated that the processor 118 and the transceiver 120 may be integrated together in an electronic package or chip.\nThe transmit/receive element 122 may be configured to transmit signals to, or receive signals from, a base station (e.g., the base station 114a) over the air interface 116.  For example, in one embodiment, the transmit/receive element 122 may be\nan antenna configured to transmit and/or receive RF signals.  In another embodiment, the transmit/receive element 122 may be an emitter/detector configured to transmit and/or receive IR, UV, or visible light signals, for example.  In yet another\nembodiment, the transmit/receive element 122 may be configured to transmit and receive both RF and light signals.  It will be appreciated that the transmit/receive element 122 may be configured to transmit and/or receive any combination of wireless\nsignals.\nIn addition, although the transmit/receive element 122 is depicted in FIG. 1B as a single element, the WTRU 102 may include any number of transmit/receive elements 122.  More specifically, the WTRU 102 may employ MIMO technology.  Thus, in one\nembodiment, the WTRU 102 may include two or more transmit/receive elements 122 (e.g., multiple antennas) for transmitting and receiving wireless signals over the air interface 116.\nThe transceiver 120 may be configured to modulate the signals that are to be transmitted by the transmit/receive element 122 and to demodulate the signals that are received by the transmit/receive element 122.  As noted above, the WTRU 102 may\nhave multi-mode capabilities.  Thus, the transceiver 120 may include multiple transceivers for enabling the WTRU 102 to communicate via multiple RATs, such as UTRA and IEEE 802.11, for example.\nThe processor 118 of the WTRU 102 may be coupled to, and may receive user input data from, the speaker/microphone 124, the keypad 126, and/or the display/touchpad 128 (e.g., a liquid crystal display (LCD) display unit or organic light-emitting\ndiode (OLED) display unit).  The processor 118 may also output user data to the speaker/microphone 124, the keypad 126, and/or the display/touchpad 128.  In addition, the processor 118 may access information from, and store data in, any type of suitable\nmemory, such as the non-removable memory 130 and/or the removable memory 132.  The non-removable memory 130 may include random-access memory (RAM), read-only memory (ROM), a hard disk, or any other type of memory storage device.  The removable memory 132\nmay include a subscriber identity module (SIM) card, a memory stick, a secure digital (SD) memory card, and the like.  In other embodiments, the processor 118 may access information from, and store data in, memory that is not physically located on the\nWTRU 102, such as on a server or a home computer (not shown).\nThe processor 118 may receive power from the power source 134, and may be configured to distribute and/or control the power to the other components in the WTRU 102.  The power source 134 may be any suitable device for powering the WTRU 102.  For\nexample, the power source 134 may include one or more dry cell batteries (e.g., nickel-cadmium (NiCd), nickel-zinc (NiZn), nickel metal hydride (NiMH), lithium-ion (Li-ion), etc.), solar cells, fuel cells, and the like.\nThe processor 118 may also be coupled to the GPS chipset 136, which may be configured to provide location information (e.g., longitude and latitude) regarding the current location of the WTRU 102.  In addition to, or in lieu of, the information\nfrom the GPS chipset 136, the WTRU 102 may receive location information over the air interface 116 from a base station (e.g., base stations 114a, 114b) and/or determine its location based on the timing of the signals being received from two or more\nnearby base stations.  It will be appreciated that the WTRU 102 may acquire location information by way of any suitable location-determination method while remaining consistent with an embodiment.\nThe processor 118 may further be coupled to other peripherals 138, which may include one or more software and/or hardware modules that provide additional features, functionality and/or wired or wireless connectivity.  For example, the\nperipherals 138 may include an accelerometer, an e-compass, a satellite transceiver, a digital camera (for photographs or video), a universal serial bus (USB) port, a vibration device, a television transceiver, a hands free headset, a Bluetooth.RTM. \nmodule, a frequency modulated (FM) radio unit, a digital music player, a media player, a video game player module, an Internet browser, and the like.\nFIG. 1C is a system diagram of the RAN 104 and the core network 106 according to an embodiment.  As noted above, the RAN 104 may employ an E-UTRA radio technology to communicate with the WTRUs 102a, 102b, 102c over the air interface 116.  The\nRAN 104 may also be in communication with the core network 106.\nThe RAN 104 may include eNode-Bs 140a, 140b, 140c, though it will be appreciated that the RAN 104 may include any number of eNode-Bs while remaining consistent with an embodiment.  The eNode-Bs 140a, 140b, 140c may each include one or more\ntransceivers for communicating with the WTRUs 102a, 102b, 102c over the air interface 116.  In one embodiment, the eNode-Bs 140a, 140b, 140c may implement MIMO technology.  Thus, the eNode-B 140a, for example, may use multiple antennas to transmit\nwireless signals to, and receive wireless signals from, the WTRU 102a.\nEach of the eNode-Bs 140a, 140b, 140c may be associated with a particular cell (not shown) and may be configured to handle radio resource management decisions, handover decisions, scheduling of users in the uplink and/or downlink, and the like. \nAs shown in FIG. 1C, the eNode-Bs 140a, 140b, 140c may communicate with one another over an X2 interface.\nThe core network 106 shown in FIG. 1C may include a mobility management entity gateway (MME) 142, a serving gateway 144, and a packet data network (PDN) gateway 146.  While each of the foregoing elements are depicted as part of the core network\n106, it will be appreciated that any one of these elements may be owned and/or operated by an entity other than the core network operator.\nThe MME 142 may be connected to each of the eNode-Bs 140a, 140b, 140c in the RAN 104 via an S1 interface and may serve as a control node.  For example, the MME 142 may be responsible for authenticating users of the WTRUs 102a, 102b, 102c, bearer\nactivation/deactivation, selecting a particular serving gateway during an initial attach of the WTRUs 102a, 102b, 102c, and the like.  The MME 142 may also provide a control plane function for switching between the RAN 104 and other RANs (not shown) that\nemploy other radio technologies, such as GSM or WCDMA.\nThe serving gateway 144 may be connected to each of the eNode Bs 140a, 140b, 140c in the RAN 104 via the S1 interface.  The serving gateway 144 may generally route and forward user data packets to/from the WTRUs 102a, 102b, 102c.  The serving\ngateway 144 may also perform other functions, such as anchoring user planes during inter-eNode B handovers, triggering paging when downlink data is available for the WTRUs 102a, 102b, 102c, managing and storing contexts of the WTRUs 102a, 102b, 102c, and\nthe like.\nThe serving gateway 144 may also be connected to the PDN gateway 146, which may provide the WTRUs 102a, 102b, 102c with access to packet-switched networks, such as the Internet 110, to facilitate communications between the WTRUs 102a, 102b, 102c\nand IP-enabled devices.\nThe core network 106 may facilitate communications with other networks.  For example, the core network 106 may provide the WTRUs 102a, 102b, 102c with access to circuit-switched networks, such as the PSTN 108, to facilitate communications\nbetween the WTRUs 102a, 102b, 102c and traditional land-line communications devices.  For example, the core network 106 may include, or may communicate with, an IP gateway (e.g., an IP multimedia subsystem (IMS) server) that serves as an interface\nbetween the core network 106 and the PSTN 108.  In addition, the core network 106 may provide the WTRUs 102a, 102b, 102c with access to the networks 112, which may include other wired or wireless networks that are owned and/or operated by other service\nproviders.\nOther network 112 may further be connected to an IEEE 802.11 based wireless local area network (WLAN) 160.  The WLAN 160 may include an access router 165.  The access router may contain gateway functionality.  The access router 165 may be in\ncommunication with a plurality of access points (APs) 170a, 170b.  The communication between access router 165 and APs 170a, 170b may be via wired Ethernet (IEEE 802.3 standards), or any type of wireless communication protocol.  AP 170a is in wireless\ncommunication over an air interface with WTRU 102d.\nFIG. 1D is a system diagram of an example communications system 175 in which one or more disclosed embodiments may be implemented.  In some embodiments, communications system 175 may be implemented using all or a portion of system 100 as shown\nand described with respect to FIG. 1A.\nUser device 180a, server 185, and/or service server 190 may communicate over communications network 195.  These communications may be wireless, wired, or any combination of wireless and wired.  Communications network 195 may include the internet\n110, core network 106, other networks 112, or any other suitable communications network or combination of communications networks.\nUser device 180a may include a WTRU (such as WTRU 102a), or any suitable user computing and/or communications device such as a desktop computer, web appliance, interactive television (ITV) device, gaming console (such as Microsoft XBOX.TM.  or\nSony Playstation.TM.) or the like.  User device 180a and/or applications executing on user device 180a may generate events such as mouse clicks, keyboard strokes, and the like.  These events may be processed by user device 180a and/or may be transmitted\nto another device such as server 185 or service server 190.\nServer 185 may include a web server, application server, data server, or any combination of these or other types of servers.  Server 185 may include any suitable server device such as a server computer, personal computer, or the like.  Server\n185 may host applications accessible to user device 185a.  For example, server 185 may include a gaming server hosting a massively multiplayer online game (MMOG), an email server, a web server hosting a website such as a social media website or blog, or\nother types of servers typically accessible by a user device over a computer communications network.\nUser device 180a may access server 185 over computer communications network 175 to interact with services that it provides.  For example, user device 180a may access a game server hosted on server 185 to participate in a multiplayer online game. Access of server 185 by user device 180a may be via a client application executing on user device 180a or any other suitable mechanism.  In some cases, the server 185 may receive events from user device 180a, or may send events to user device 180a.  For\nexample, the server 185 may send an event to user device 180a indicating that additional in-game resources are required for continued play.\nService server 190 may include a web server, application server, data server, or any combination of these or other types of servers hosted on a server device.  Service server 190 may include any suitable server device such as a server computer,\npersonal computer, or the like.  Service server 190 may be configured to communicate with server 185, for example, over network 195 or any other suitable communications medium.  Service server may be co-located with, combined with, or in direct\ncommunication with server 185.\nService server 190 may communicate with server 185 to provide services, such as third party services, to users of server 185.  For example, a subscriber to a game hosted on server 185 may access server 185 from user device 180A and may subscribe\nto third party services for the game which are hosted on service server 190.\nService server 190 may be configured to receive and/or intercept events transmitted between user device 180a and server 185.  For example, in some embodiments server 185 and service server 190 may be configured such that server 185 may send an\nevent destined for user device 180a instead or additionally to service server 190, and service server 190 may send the event or another event, signal, or message to device 180a.  For instance, in a case where server 185 includes a game server, server 185\nmay send an event to service server 190 indicating a requirement of a user of user device 180a, and server 190 may send the event or another signal or message to device 180a indicating that a resource is available to acquire the requirement.  In some\nembodiments, service server 190 may only forward the event to device 180a under certain conditions, such as based on a user preference and/or context information relating to the user of device 180a.\nIn some embodiments, the functions of service server 190 and server 185 may be implemented using the same device, or across a number of additional devices.\nIn some embodiments, user devices 180b and 180c may communicate with server 185 and/or service server 190 via user device 180a.  For example, user device 180a may forward a notification message from service server 190 to user device 180b via a\npeer to peer connection and may forward a notification message from service server 190 to user device 180c via network 195.  In some embodiments, user devices 180a, 180b, and 180c may form a network, such as a peer-to-peer network, and such network may\nhave a mesh topology, a star topology using user device 180a as a coordinating node, or any other suitable topology.  In such embodiments, the peer-to-peer network may operate independently of server 185 and/or service server 190, and may incorporate\nfunctionality that otherwise would be hosted by server 185 and/or service server 190, such as functionality described herein.\nEverything that follows may, but is not required to be, employed and/or implemented using one or more, or part of one or more of the example systems discussed above.\nAs used herein, a content requestor entity may refer to a content requesting entity and the terms may be used interchangeably.  As used herein, a content creation entity may refer to a content creator entity or content creating entity and the\nterms may be used interchangeably.  An event pattern (EP) guided mobile content services system may perform the content selection and matching methods disclosed herein.  An application programming interface (API) and a network element may also perform\nthe content selection and matching methods disclosed herein.\nWeb content creation and consumption processes may be decoupled and may be mediated by content delivery networks and service portals (for example, YOUTUBE and TWITCH).  Such decoupled processes may not be suitable for pervasive service because\nthe service may not be fast and efficient enough to address the live events and context sensitive issues that the users may encounter in interacting in augmented or virtual environments.  It may also be costly for the content developers or producers to\ndevelop help menus or tutorials anticipating all possible problem scenarios in advance.  In an example, text key word base search engines may be used for the pre-filtering of archived content stored in multiple Websites.\nExamples are disclosed herein of efficient real-time customer retention services with event pattern learning, detection and remedial action rules to support users that have difficulties in using the application or playing the game.  These\nservices may improve user experiences and customer retention.  Further, examples are disclosed herein to use tutorial and third party created credit earning tasks as one type of remedial action.  Further, examples are disclosed herein for modified\ncontent creation and consumption mechanisms to support automated help content (for example, tutorials and the like) generation for customer retention systems.  Example mechanisms may also be generalized and applied to automate content creation needed for\npervasive personal services with minimal or no manual operations.  Furthermore, example mechanisms may enable new types of automated content brokerage model which may support dynamic selection of qualified content creators to generate sequences of\npersonalized content directly consumable by the users.  The dynamically selected content creators may be linked to the content requestor in real-time with knowledge of the context and specific problem the user is facing.  This dynamic content creation\nprocess may be extended and integrated with the existing content aggregators (for example, YOUTUBE, TWITCH and the like) and search engines (for example, BING, GOOGLE and the like).\nSome main challenges in creating tutorial or coaching material or other personalized pieces of content for multiple mobile apps and games using wearable devices are summarized as follows.  Pre-made content may not predict all possible user\nconditions.  Mobile apps and games may be shipped with pre-made tutorials and hints.  The pre-made content may not cover all the possible use cases and may not be tailored to each user's background, changing environment, and unexpected situations that\nmay happen in real-time in the real world.  For example, some mobile games (e.g., PUDDING MONSTERS and the like) may provide hints for different puzzles when players fail the puzzle many times.  There may be many different types of problems that\ndifferent users may face at different situations.  Pre-made hints or tutorials covering all cases may require substantial development efforts.  Allowing players to request help content at a specific situation from other players directly may reduce the\nneed to create pre-made hints and at the same time may provide personalized service and a sense of social engagement.\nUsers may also create content dynamically.  While dynamically created content may enrich personalized experiences in both the real and virtual worlds for mobile app and game users, such content may also cause information overflow if the content\ncreation process is not controlled and organized with a set of purposes.  For example, wearable cameras may record a large amount of video in many different places whenever the users felt like.  However, most of the recordings may not be reviewed or\nshared because most users would not have the time to review most of the recording without a purpose.\nA user may consume hands-free immersive content.  When a user may encounter problems related to such content consumption, the user may go online and manually search for solutions from other users who may have encountered similar problems. \nOnline content may be consumed from websites (such as, for example, Bing.com, Youtube.com, Twitch.com, Machinima.com and the like) using browsers manually.  These tools may not be designed for hands-free operation and may not be convenient to use when\nusing wearable devices (for example, cameras, displays, gesture devices, and the like).\nExamples are disclosed herein of system architecture, processes, and methods to automate the dynamic content creation and consumption processes based on requests automatically detected from behavior patterns of mobile app and game users.  The\nautomated behavior pattern detections may be supported by the real-time analysis of data collected from a simulated 3D virtual reality world or from a sensor directly.  The content creation processes may be performed using the 3D virtual reality graphic\ntools used in the game engine or other 3D virtual reality graphic engines.\nIn examples disclosed herein, event patterns (EPs) may be generated by collecting real-time user behavior tracking information, or sensed events, from a user's device, which may or may not be a wearable device, from a monitoring system, such as\na camera or global positioning system (GPS) device, or from any other device which is capable of receiving and collecting information corresponding to real-time behavior content of a user.  Real-time behavior content may be created (i.e., created\ncontent) by tracking the behavior of a first party or multiple first parties to be used to aid, motivate or improve an experience of a second party or multiple second parties.  A real-time content request (i.e., requested content) may be generated\nautomatically by a user device of a second party if is it determined from real-time tracking of the behavior of the second party that content created by a first party is desired.\nAccordingly, the system disclosed herein may offer content services to two types of entities: the content requestor entities and the content creator entities.  The system may automatically allocate content creators from any number of qualified\ncontent creator candidates to generate dynamic content to improve user experience, alleviate stress or solve a specific user problem at hand.  The content creators may be selected based on \"qualifications\" that matches the \"need\" patterns of content\nrequestors.  The need patterns may be modeled as a set of attributes used to assess the problems that the user or potential content requestor may encounter.  It is noted that \"content requestor entity,\" \"content requestor,\" \"potential content requestor,\"\n\"qualified content requestor candidate,\" \"content requestor participants\" or the like may be used interchangeably.  Similarly, \"content creator entity,\" \"content creator,\" \"potential content creator,\" \"qualified content creator candidate,\" \"content\ncreation participants\" or the like may be used interchangeably.\nIn examples disclosed herein, multiple types of smart sensor data analysis tools may be used by the mobile applications and games to detect user behaviors.  Further, in examples disclosed herein, the system may integrate the analysis tools and\ngenerate summary event patterns including objective, environment and problems to match participating qualified content creators.\nIn an example, the system may automate and extend the traditional decoupled content creation and consumption service model.  First, from the content creation perspective, the traditional content creation model may be categorized into three\ndifferent types depending upon the size of an audience.  For mass market content, such as video and games, the content may be targeted for a large number of users and consumed based on a user's preference.  For community social media, the content may be\ntargeted for social groups and may be consumed by users in the group who are interested in the content.  For custom specific content, such as training materials or targeted personal advertisements, the content may be created based on personal need.  In\nall three types of the content creation categories, the content creator or producer may create the content based on their understanding of the user need and the context information, such as market trends, to make decisions on what types of content to\ncreate.  The larger the audience base, the less specific the content may be to a specific user's problem or preference.\nIn a further example, from the content consumption perspective, content created for the mass market, a community and specific application domain or interest group, or a specific person may be organized and kept in content service providers. \nUsers may use search engines, feeds, and email to retrieve the content.  Information aggregators such as FACEBOOK, YOUTUBE and TWITCH may provide local search filters to help users filter out a massive amount of information crated and organized in the\nuser support sites or areas.  However, each user may need to study the content and decide if the content is from a reputable source before using the content.  Each user also may need to check if the published content is up to date and matches the context\nof user's environment and application.\nIn examples disclosed herein, the system may enable a pervasive content creation and consumption service model that may automate and extend the traditional content creation and delivery model with a dynamic real-time content creation process. \nThe process may consist of the following functions and features.  The process may include dynamic content request generation.  Content request may be generated based on observations of user behavior in a specific application context instead of manual\nsearch or filtering operation.  The observation may be made by analyzing data collected from a set of sensors monitoring the user's motion, gesture, facial expression, vital signs, biometric data, and other types of data collected from other, different\ntypes of sensors.  The process may also include the dynamic selection of qualified content creators.  The content creators may be dynamically selected based on the qualification or skill level required to provide personalized content.  In addition, the\ncontent creators may be selected based on matching the EPs containing the context information concerning application, user and environment.  Further, the process may include personalized and context aware content creation.  The content may be created in\na similar virtual and physical environment and application context that matches the requested objective, environment and problem descriptions of the content request.\nThe process may further include seamless hands-free consumption.  Content may be created, assembled, annotated and delivered to the user automatically and consumed seamlessly with minimal or no manual operations.  The content creation and\nconsumption may be tightly coupled without needing a content aggregator and distributor to store and support search and filtering operations.  The process may further include integrated content creation and consumption.  Instead of having a content\naggregator and search engine to mediate the decoupled content creation and consumption processes, the proposed mechanism may enable integrated content creation and consumption sessions that may persist for the life span of a set of applications and games\nin a pervasive service environment.  The process may further include automated effectiveness rating.  The effectiveness of the content created for one user may be rated based on an assessment of progress of the user who consumed the content.  This\nprogress assessment is more objective than manual preferences (for example, a \"like\" and \"star\" rating used in social media, movie services and the like).\nContent creators may provide live or pre-recorded content to the content requestors based on criteria defined by the content creator, content requestor or the content matching service.  The content matching service may provide service agents to\nassist content pre-recording and context extraction to allow customization of criteria for weighted proximity matching and creator content creation and reuse criteria.  The weighted proximity matching service may be customizable for each application. \nThe service may define weights, W.sub.i (range from [0-1]) for each of the attributes of a requested content and/or of a content creator with index i used by each application or bundled applications for multi-dimensional proximity distance calculation. \nExample attributes that may be weighted by W.sub.i include the following: geo-distance, distance in the game world, time difference in synchronization event (e.g., start time of an application task, or completion time of application task), experience\nlevel for a specific game, review-score (e.g., a score that may be provided based on the effectiveness of the content to the requestor described in a progress based effectiveness assessment in the competitive content quality improvement services\nsection).  Accordingly, selection of content from among different content candidates and content creator candidates may be optimized according to the weighted attributes.\nThe service may apply weights on the results of the distance calculation.  For example, the service may apply the weights W=[0.5, 0, 0, 0.5, 0] for applications which only consider geo-distance and experience level for selecting a content\ncreator for a content requestor.  The service may also apply the weights W=[0, 0.25, 0.25, 0.5, 0] for selecting a content creator based on close proximity to the content requestor in time and space of the game world, as well as the experience level of\nthe content creator for the specific application (or game).  Further, the service may apply the weights W=[0, 0, 0, 0, 0, 1] for applications independent of time and space, and only the quality of the tutorial for the application may be important.  In\naddition, the service may select the closest content ID and creator IDs (e.g., closest three content creators).\nThe content matching service may include creator content creation and reuse criteria.  The criteria may be used to decide whether to create new content from scratch or use automated pre-recorded content by a content creator.  Creating new\ncontent may improve the previous content and stay current with the content requestors.  Using previous content that works well may build competitive content with a high review-score.  The criteria may also be used to decide whether to provide live\ncoaching where the content may be sent to a content delivery queue to be delivered to a user or to deliver archived contents previously stored in the content delivery queue from a previous inquiry to the user.  The stored content may be edited based on\nthe EPs.  Based on the above criteria, the content service may provide live content or reuse previously created content for the best interests of content creators, service providers, and content requestors.\nThe system may include a content request model and EP detection.  The request patterns of a user may be detected based on sequences of event patterns using multiple types of sensors.  Examples of request patterns may be listed as follows.  A\nrequest pattern may include a normal sequence of events that are expected to happen or that did not happen in a timely manner.  A request pattern may also include the detection of abnormal sequences or abnormal values in sensor data.  Further, a request\npattern may include the detection of problems in the environment or system errors.  An example of a data model and event pattern detection is disclosed herein.\nFIG. 2 is a diagram of an example system 200 that includes content requesting entities 202a, 202b and 202c, a patterns of events guided content creation and consumption service 204, and content creation entities 206a, 206b and 206c.  The content\nrequesting entities 202a, 202b and 202c and content creation entities 206a, 206b and 206c may be any user device, monitoring device, and/or monitoring system, each of which may include any number of processors, memory devices, input/output devices (e.g.,\nreceivers, transmitters and transceivers), sensors (e.g., image, sound, temperature, vital sign, brainwave, location, orientation, and motion), and any combination thereof.  For example, the content requestor may also act as content creator.  Some of\napplication specific behavior model and event pattern processing functions may be distributed to local servers, devices or sensors such that the service server may focus on processing high level composite models or events from multiple servers, devices,\nand/or sensors.  The patterns of events guided content creation and consumption service 204 may be configured as an automated content collection, matching and distribution module and it may be integrated as a single service providing entity, or may be\nprovided as separate service providing entities corresponding to different use applications.  The patterns of events guided content creation and consumption service 204 may include one or more processors coupled to one or more memory devices to perform\nits functions, and it may include one or more inputs and one or more outputs to receive and/or to output data and information, respectively (e.g., via a receiver, transmitter or transceiver).  The patterns of events guided content creation and\nconsumption service 204 may be a server and may include or be implemented with computer-readable storage media that includes program instructions to be implemented by a computer to cause a processor to execute or perform the program instructions.  The\ncontent requesting entities 202a, 202b and 202c, the patterns of events guided content creation and consumption service 204, and the content creation entities 206a, 206b and 206c may each be configured to be communicatively coupled to one another and\ncommunicate with each other, directly or indirectly, via any number of communication channels and communication mediums (e.g., wirelessly and/or by wired connection).\nAs shown in FIG. 2, a 3D augmented reality enabled fitness application (e.g., a biking or hiking application) may search for other content creation participants among the content creation entities 206a, 206b and 206c for content requestor 202a\nin the same trail to provide real-time warnings or suggestions to the content requestor 202a.  Likewise, a multi-user game application session may look for skilled players among the content creation entities 206a, 206b and 206c who are playing the same\ngame session corresponding to the game play of a content requestor 202b to give real-time suggestions to the content requestor 202b.  Likewise, a 3D augmented reality enabled healthcare application (e.g., rehabilitation application) may search for other\ncontent creation participants among the content creation entities 206a, 206b and 206c performing the same healthcare activities corresponding to the activities of a content requestor 202c to provide real-time health suggestions to the content requestor\n202c.  In an example, the skill level or experience level of the content creator and content requestor may be provided by the application or game service.  Such skill ranking methods may be used by game systems, such as, for example, XBOX and the like,\nto match players.  The skill level or experience level may also be used as part of the attributes for selecting the qualified content creators.  Furthermore, application context may also include avatar and weapon types as additional game specific context\nattributes.\nAn example dynamic content creation and consumption services work flow may be described as follows.  A set of content requestor entities 202a, 202b and 202c may generate requests to content creation entities 206a, 206b and 206c via the patterns\nof events guided content creation and consumption service 204.  These requests may include event patters (EPs).  Upon receiving a set of requests, the patterns of events guided content creation and consumption service 204 may search for qualified content\ncreator candidates among the content creation entities 206a, 206b and 206c based on application tasks and proximity in time and space to the requestor.  The patterns of events guided content creation and consumption service 204 may use multiple types of\ncommon and application specific attributes in the event patterns to rank the qualified content creator candidates to create and annotate multiple pieces of content and select the most suitable content to deliver to the content requestors 202a, 202b and\n202c.  Upon selecting the most suitable content from the content creation entities 206a, 206b and 206c, the requested content is delivered to the appropriate content requestor entity 202a, 202b and 202c via the patterns of events guided content creation\nand consumption service 204 though one or more communication interfaces.  For example, the selected content may be annotated with the context information (e.g., activity, time and space) of one or more qualified content creator candidates (i.e., content\ncreation entities 206a, 206b and 206c) and may be received by the patterns of events guided content creation and consumption service 204 from the content creation entity via a communication channel.  The selected content and any corresponding context\ninformation, which may be stored in the patterns of events guided content creation and consumption service 204, may then by delivered by the patterns of events guided content creation and consumption service 204 to one or more of the content requestor\nentities 202a, 202b and 202c for consumption (e.g., for audio and/or video playback of the content, or any other utilization configured to convey the content to a user) via another communication channel.\nOn receiving the content, the requestor entities 202a, 202b and 202c may play back the pieces of content in whole or in steps.  The patterns of events guided content creation and consumption service 204 may monitor the progress of the content\nrequestor while the content is being consumed (e.g., via play back) by the content requestor entity.  During the play back, the content requestor entity may transmit additional sensor events, progress information and other behavior data to the patterns\nof events guided content creation and consumption service 204, such that the patterns of events guided content creation and consumption service 204 may analyze the received information and monitor the behavior of the content requestor entity through the\nanalysis.  This information received by the patterns of events guided content creation and consumption service 204 may also be used by the patterns of events guided content creation and consumption service 204 to continue dynamically build and adapt\nbehavioral models corresponding to a user environment.\nIf the content requestor makes progress in the applications (e.g., kill a monster, solve a puzzle, avoid a hazard, or pass a level), a positive point or rating may be given to the received content automatically on the location and context\nspecified in the EPs.  If the patterns of events guided content creation and consumption service 204 determines that the delivered content was not effective based on the requestor's progress, a negative point may be given to the received content and a\nmodified request for content may be automatically sent to the content creation entities 206a, 206b and 206c.  In response to the modified request, another content creator and/or additional content may be selected and the additional content may be sent to\nthe applicable content requestor entity 202a, 202b and 202c.  This automatic rating and monitoring scheme may continue until satisfactory results are achieved by the applicable content requestor entity 202a, 202b and 202c.  The location may be a physical\nlocation in the real world or a mapped location to the virtual world.  For each location and context, the patterns of events guided content creation and consumption service 204 may maintain in a queue stored in a memory device a set of high ranking\ncontent creators and associated recorded content created in the past as a history log for future reference.  Older or low ranked pieces of content may be archived and removed from the history by the patterns of events guided content creation and\nconsumption service 204.\nFIG. 3 is a diagram of an example of dynamic event pattern detection and matching system 300.  The system 300 includes user devices 302, a Dynamic Content Services System 304, and content creator devices 306.\nThe user devices 302 may include both content requestors and content creators that provide sensor events (SE) to the system 304 based on user behavior and/or performance.  For example, a user device 302 automatically tracks a behavior of a user\nand monitors the behavior for an occurrence of a sensor event (SE).  Upon detection of a sensor event (SE) occurrence, the user device 302 transmits the sensor event (SE) and any corresponding context information to the system 304 via any appropriate\ncommunication interface (e.g., transmitter, transceiver, input/output interface, and the like) through a communication medium.  As shown in FIG. 3, a user device 302, which may or may not be a wearable device, tracks a performance, status or behavior of\na user to generate sensor events (SE) and may send the raw sensor events from the user device 302 to a Dynamic Content Services System 304.  The sensor events may be tracked and generated by an application or game engine which may be included in the user\ndevice 302 or may be configured to communicate with the user device 302 to receive sensed information from the user device 302.  Sensor events (SE) provided by both a content requestor and a content creator may be continuously and automatically collected\nby the system 304 in order to create, improve and dynamically adapt behavior models built by the system 304 on a real-time basis.\nThe Dynamic Services System 304 may, similar to patterns of events guided content creation and consumption service 204, be a server made up of a number of modules which are configured to receive, store, process and output content, information\nand/or data.  In particular, each module may include one or more processors and/or memory devices to process and store information.  In addition, each module may be combined with other modules without departing from its intended processes and\nfunctionalities.  The system 304 may process the sensor events (SE) to generate event patterns (EP) based on behavior models (VStats).  The system 304 may transmit a request to a content creator device 306 to create content for a user device 302 (e.g., a\nrequestor device).  In the request, the system 304 may provide the event pattern (EP) in the request to the content creator device 306.\nUpon receiving a content request from system 304, the content creator device 306 may generate content based on the event pattern (EP) and any context information associated thereto.  The generated content may then be provided by the content\ncreator device 306 to the system 304.  The system 304 may then store the generated content and provide the generated content to a user device 302 that has requested the content.  The system 304 may then continue to receive behavior information and sensor\nevents from the user device 302 and monitor the behavior information and sensor events of the requestor to assess the effectiveness of the provided content.  If the system 304 determines that the provided content has been ineffective based on continued\nmonitoring of behavior information and sensor events, the system 304 may update behavior models and event patterns, and request a content creator device 306 to update the generated content based on the updated behavior models and event patterns, or the\nsystem 304 may search for another qualified content creator device 306 and request content from the other content creator device 306 that may provide better results.\nIn an example, sensor events (SE) and context information may be transmitted by a user device 302 to the Dynamic Content Service System 304.  The raw sensor events (SE) and context information may be filtered and formatted by a sensor event\nreceiver and filter 308 to produce a sensor event vector (V) or a selected set of sensor event vectors (V), which may include a selected set of variables derived from the sensor events (SE) and context information.  An example of a data model for a\nsensor event vector V may be defined as follows: V={[AppID, TasklD, UserToken], [Avatar, weapon type], [ObjectiveList], [BioMetric, VitalSign], [Timestamp, Duration], [Position, Orientation], [MotionVectors], [GestureVectors], [ActionVectors], [other app\nspecific data]}\nA sensor event vector V may be further processed by an abnormal and predictive behavior assessment module 310 and may be fed into a VStats behavior model builder module.  The abnormal and predictive behavior assessment module 310 determines\nwhether a sensor event vector V corresponds to predictive behavior of a user device 302 to be used to build a behavior model, to abnormal behavior of a user device 302 which indicates a request for content is desired by the user device 302, or to a\ncombination thereof.\nA model builder module 312 may include a VStats behavior module builder 312a, which may generate statistical models VStats, and a VStats model data cube 312b, which may store and index statistical models VStats provided by VStats behavior module\nbuilder 312a for retrieval by the abnormal and predictive behavior assessment module 310.  The VStats behavior module builder 312a may use different types of methods to generate statistical models VStats.  For example, different types of statistical data\nmay be defined and calculated to assess the average and abnormal value of sensor event vector V. The average values and abnormal values, according to a model, may be defined as one or more ranges, one or more thresholds, one or more data points, or a\ncombination thereof.  AppID and taskID may provide the high level context to support application specific matching rules down to the task level.  Avatar and weapon type may be used to support a game with multiple character types and weapons used in the\ngame that have different capabilities.  A target exercise Heart Rate may be Heart Rate=[85-140] as 50% to 85%.  A walking speed may be walking speed=1.2-1.6 m/s for 50% to 85% of human.  An average motion trajectory may be the average motion trajectory\nat coordinate (Longitude, Latitude)={avgV.sub.x:1.3, avgV.sub.y=0.9}.  An average score earning rate may be included for GameX.SessionY, ScoreRate={Avg: 2000, Std: 600}.  An average score for one specific game may be X={Avg:34000, Std: 4600}.  In an\nexample, the statistical models may be context aware and change over time, space, and applications.  The results of the model builder 312a may be saved in a data cube 312b to track the statistical models VStats efficiently.\nBased on the statistical model VStats, the input sensor event variables may be analyzed and tested to obtain a score (e.g., a percentile of the input sensor event variables) against the distribution model VStats, in real-time to generate a set\nof attributes defining the scores of a \"problem\" assessment vector \"B\".\nFor example, the following types of behavior problems may be defined based on a percentile of the population from the VStats distribution.  B.Bio={TargetHeartrate: 0.7} //Detect a higher than normal exercise heart rate. \nB.Motion={Disorientation: 0.8} //Detect a player or a biker changing direction more frequent than normal and did not move to targeted destination B.Action={Inaction: 0.8, Slowness: 0.9} //Detect player did not act and/or is slow.  B.Perf={GamelD: GameX,\nSession: Y, ScoreRate: 0.2} //Detect player did not make progress as expected.\nThe abnormal and predictive behavior assessment module 310 may generate event patterns (EPs) based on one or more received statistical models VStats and sensor event vectors V. Multiple different types of behavior problem summaries may be added\nto the event patterns (EPs) to establish context and to guide the content creation process.  The resulting score for each variable represents the behavior problem assessment vector, B, which may be included in generated EPs.  The EP may be sent to a\nproximity EP pattern matching and creator selection module 314 to select content creators 306 with similar EPs to the EP generated by the abnormal and predictive behavior assessment module 310.  The proximity EP pattern matching and creator selection\nmodule 314 may further use proximity information received from user devices 302 to select content creators 306 with similar EPs to the EP generated by the abnormal and predictive behavior assessment module 310 and that are in a proximity to the user\ndevice 302 that is requesting the content.  A proximity may refer to a proximity in a real-world or in a virtual world environment depending on the application.  EPs derived from both creators and requestors may be stored and indexed in a user EP data\ncube 316 for referencing, searching and retrieval by the proximity EP pattern matching and creator selection module 314.  An EP may consist of a sequence of detected behavior problems [B(t-w), .  . . , B(t)] associated with an application context, a user\nobjective, and an environment description.  An example of an EP may be given as follows: EP={[AppID, TasklD, UserToken], [ObjectiveList], [Environment Description] [B(t-tw), .  . . , B(t)], V}, where, tw=1, .  . . , w B(t-1)={B.Bio: {TargetHeartrate:\n0.8}, B.Motion: {Disorientation: 0.8}, B.Perf: {ScoreRate: 0.2, Timestamp: TS1} B(t)={B.Motion: Inaction: 0.9, Timestamp: TS2}\nIn an example, according to FIG. 3, users (both requesters and creators) 302 send sensor events and application context information (\"Sensor Events\") to system 304 for the system 304 to build VStats and Event Patterns (EPs).  The system may then\nsend a request to a creator 306 (\"C\") to create content for a requestor 302 (\"R\") based on a matched EPs between R and C (\"Create(C, R, EP)\").  Content generated by the content creator 306 may then be delivered to the system 304 (\"D(C, R, Content, EP)\")\nand stored in a content delivery queue 318.  The content stored in the content delivery queue 318 may then be delivered to the requestor 302 for consumption by a user (\"Get(R, EP, C, Content)\").\nFIG. 4 is a flow chart 400 illustrating an example method.  The method of flow chart 400 may be implemented using any suitable mechanism or method, including, for example, those shown and described with respect to FIG. 2, 3 or 5-7.\nAccording to FIG. 4, user behavior data, which may include user performance data or user status information, associated with a specific task performed by a user within a specific user application may be collected in step 410 (e.g., sensor events\nand context information).  The user behavior data may be analyzed in step 420.  If an abnormal trend in the user behavior data is detected in step 430, a content request may be initiated and the content delivered to the user in step 440.  The content is\nintended to be effective to maintain user performance and satisfaction levels for the specific task.  In addition, the abnormal user behavior data may be used to construct statistical distribution and predictive analytic models (i.e., behavior models) in\nstep 440.  If an abnormal trend in the user behavior data is not detected in step 430, statistical distribution and predictive analytic models (i.e., behavior models) may be constructed using data collected from users in step 450.  That is, in view of\nsteps 440 and 450, it will be appreciated that all user behavior data may be predictive and that any user behavior data, whether abnormal or normal, may be used as a source or input for constructing statistical distribution and predictive analytic models\n(i.e., behavior models).  This is because all user behavioral data may be used to define a threshold for what is and/or what is not normal, and behavior models may be constructed and updated based on all user behavior data available.  In addition, some\nusers from which data is collected from may be qualified specialists or users with situational experience that matches a specific situation, environment or other user context.  Qualified specialists may be identified by user input or the system may\ndetermine autonomously which users are qualified specialists by analyzing historical trends from their provided user behavior data and/or provided content.  For example, user behavior data provided by a user (e.g., user behavior data which corresponds to\na high score in a gaming environment, or fastest task completion time) determined to be over a specified effectiveness or rating threshold, may qualify the user as a qualified specialist.  Similarly, content provided by a user to be delivered to a\ncontent requestor that is determined to be over a specified effectiveness or rating threshold, may qualify the user as a qualified specialist.  Peer ratings may also be supplied by other users to qualify a user as a qualified specialist Once the content\nis delivered to the content requestor in step 440, the delivered content is analyzed for effective results in step 460.  If the content does not achieve effective results, an additional content request is initiated and the additional content is delivered\nto the user in step 470.\nVarious types of event and condition parameters and choices of deliverable content may be programmable for different use cases.  For example, data may be collected and models built for overall level of play of the user, motivation for playing,\ntemperament of play, amount of money the user usually spends and under what circumstances, user context, or user performance.\nOverall level of play may include a level of mastery of the user, i.e., whether they are a master or a novice level player.  Motivations for playing may include winning, social interaction, or experience.  Temperament of play may include speed\nor other conditions.  For example, temperament may indicate aggressive or slow play.  The amount of money the user spends may be real money or some substitute for money, such as credits, in-game gold pieces, and so forth which are usable in a specific\ngame.\nUser context may include a location from which the user usually plays, such as home or school, a device used or usually used by the user when playing from a particular location.  For example, a user may usually use a tablet on the bus, or a PC\nat home.  User context may also include an amount of time the user usually plays or usually plays in the various locations using the various devices, or other correlations regarding user location, device, and/or in-game behaviors.  User performance may\nbe in the context of the user's past performance or in relation to other players or similar players.\nUser performance may be measured, for example, as a number of times the user failed to acquire a game resource or score that the user or another user would normally succeed in acquiring/attaining, an amount of time taken to complete a quest,\nlevel, or other task within a game beyond that normally taken by another user with similar experience, or the user's efficiency in using game resources.  User efficiency may be calculated for example by comparing the efficiency of a path taken by the\nuser within a game world to accomplish a task with the efficiency of paths taken by other or comparable users.\nBased on a predicted abnormality trend, one or more event and/or condition detection rules may be invoked, which in turn may execute one or more content delivery actions in order to provide the user with virtual resources that may enable a user\nto enhance performance or improve a user experience.\nThe following is an example pseudo code to illustrate the proximity event pattern matching and creator selection function module 314 shown in FIG. 3.  The example shows one possible implementation using an event processing rule to filter a\ndisoriented user with low score and has been inaction for a while.  On detecting such user, the system may match the App Id and Task ID with other active users who have good performance within a time window of 60 second.  Within the small time window,\nthe good performance user may have either completing the session of the task or about to do the task.  In either case, the 3D engine of the creator may keep the 3D action, context, and environment content in cache and render the content (e.g., animation\nsequence data) for the requestor that have been detected to have problem when performing a task.\nAn example of the code used in the proximity event pattern matching and creator selection function module 314 may be:\nTABLE-US-00001 /* Assume that a set of custom function package including calculating L2 Distance, L2Distance( ), creating content, create( ), and notify( ) are ready for use.  For example, Import Custom.package.L2DistanceCalulation L2Distance... A sequence of problem assessment vector Btn, Bt2, Bt1 are also included in the events*/ Rule: \"Create coaching content for disorientation users in NewExampleQuest Level 3.\" When //Detect players likely to be disoriented, have low scores, and being\ninaction $Requestor: EP((AppID == \"NewExampleQuest\", TaskID == \"Level 3\"), $RequestorPosition: Position( ), // (Bt2.B.Motion.Disorientation &gt; 0.70), (Bt1.B.Motion.Disorientation &gt; 0.75), //Increasingly disoriented.  (Bt2.B.Perf.ScoreRate &lt;\n0.35), (Bt1.B.Perf.ScoreRate &lt; 0.30), //Decreasing performance.  (Bt1.B.Motion.  Inaction&gt; 0.85) //Inaction longer than normal.  ) from entry-point \"RequestorEPs\" // from data requestor data stream.  //Find creators with good performance who is\nusing the same App/Game task within 60 second window before or after the requestors.  */ $QualifiedCreator: EP((AppID == \"WordQuest\", TaskID == \"Level 3\", $CreatorPosition: Position( ), (Bt1.B.Perf.ScoreRate &gt; 0.7), (Bt1.B.Perf.scoreRate -\n$Requestor.Perf.ScoreRate &gt; 0.2), /* ...  Other criteria for selecting a skillful and qualified content creator */ ...*/ /*Match temporal events interval between creator and requestor */ this include [60s] $Requestor) //Creator events time window //\ninclude the requestor event ) from entry-point \"CreatorEPs\" Then /*Select a list of qualified creator in the radius of \"MaxL2Distance\" of the requestor's position.  Use custom function to calculate the L2 distance.  */ $SelectedCreator: List( ) from\naccumulate( $QualifiedCreator( $CreatorPosition : Position), Radius = L2Distance($CreatorPosition, $RequestorPoistion); Radius &lt; MaxL2Distance, collectList($QualifiedCreator) //Collect a list of qualified creators ) /*Call the creator service\ninterface to create content for requestor Requestor.token and with the EP, Requester.EP (e.g., event pattern with AppID, TaskID, Env, Obj, and problem assessment vectors, other custom attributes).  */ For all ($SelectedCreator in collectList) {\nCreate($SelectedCreator.Token, $requestor.Token, $Requestor.EP) Notify (EP.UserToken) //Notify the user to get the content from the PlayerX } End\nThe above example illustrates the function steps and sample pseudo codes to show one example of the basic event pattern matching function used in the dynamic creator and requestor matching service.  In further examples, a large set of rules may\nbe executed in parallel in multiple rule engines.\nExample automated support service scenarios are disclosed herein.  Based on the data models and event pattern processing rules, the proposed dynamic content creator and requestor matching services may apply to multiple different pervasive\npersonalized service application areas such as healthcare, education game, and fitness applications.  The following examples describe new pervasive application scenarios that may be supported by the matching service efficiently in more details.\nThe matching service may efficiently support healthcare patient rehabilitation scenarios.  An example is a pervasive rehab application designed for multiple patients and specialists with monitoring cameras, wireless hearing aid headsets and 3D\ngraphic animation engines.\nFIG. 5 is a diagram of an example of healthcare hip replacement rehab routine training and motivation content generation.  As shown in FIG. 5, the rehab routing scheduled for each patient may be monitored by a group of specialists with a camera,\nwhich may be a wearable camera depending on the monitored activity.  At the same time, the specialists may be conducting scheduled therapy for one or more patients at home or in multiple rehab centers.  The example shows a patient 502a which had right\nhip replacement surgery and is scheduled to conduct the physical therapy at home.  A remote camera 502b may be set up to monitor the patient's motion, gesture and facial expressions.  A 3D virtual graphic engine 502c may map motion and gestures into an\navatar in a 3D virtual world with geometry and space similar to the patient's exercise area using available 3D mapping techniques (e.g., MICROSOFT KINECT).  A set of sensor variables V, may be extracted from the 3D virtual graphic engine 508 and analyzed\nby a behavior assessment module 510 to detect abnormal gesture and action patterns by comparing the behavior patterns of the user with an expected normal behavior pattern.  Abnormal behavior patterns may include inactivity, improper movements, improper\nroutine or the like.  3D tracking in the game engine or 3D virtual reality engine may provide friendly user interface and may not require additional 3D rendering software in a separated vision system.\nA behavior model builder, such as that shown in FIG. 3, may construct the statistical distribution and predictive analytic models using data collected from qualified specialists.  Based on the normal behavior patterns, the module detecting\nabnormal motions and gestures may produce a sequence of problem assessment vectors, Btn, .  . . , Bt2, Bt1.  An event pattern (EP) may consist of the following information sent to the event pattern matching engine.\nTABLE-US-00002 EP1: UserX @Time.Place Needed by @ 10 am 9/23/1014 Env: Climbing stair at home Obj: Right side hip replacement rehab routine Bt2: {Inactive:0.8} Bt1: { Abandonment:0.8, Incorrect-motion:0.7}\nAt the same time, on-going rehab sessions 506a and 506b with specialists may also be monitored by a system and which results in the generating of EPs.  These EPs may contain qualification information of the specialist, the types of rehab, and\nthe context of the tasks that the specialist is working on.  Based on the EP detection rules designed for matching the event patterns from qualified specialists and the patients, administered by the patterns of events guided content creation and\nconsumption service 514, a specialist who is currently conducting the same rehab routine for a right hip replacement patient may be selected to generate the video automatically.  Note that in this case, the objective of the event may contain the\nrequirement for right hip replacement patient rehab routine.  No left hip replacement patient or other types of rehab patient may be selected.\nWhen a specialist is selected, the content creation may start automatically upon the permission or by default enabled for the wearable device used by the application.  The created content 516a, 516b and 516c may be a voice recording, a 3D\nanimation sequence generated from a camera in the rehab center, or a live video shot from the wearable camera of a rehab session that specialist instructs a patient with similar condition.  When patient has a special condition specified in additional\nattributes in the event (e.g., right side hip replacement), the specialist may add the recorded content with annotated voice to remind the patient to use the correct foot to climb up or down the stair.  If the patient has been in \"inaction\" (e.g.,\nmissing multiple sessions), the specialist may send a group therapy video to encourage the patient according to the patient's scheduled rehab routings.\nThe matching service may also efficiently support educational game tutorial generation scenarios.  Another example service scenario is for in-app and in-game tutorials generation from real-time users and players in similar application and games\nrunning concurrently.  In this scenario, play logs from students with high scores may be used to provide examples for students having difficulties navigating through the game space or solving the puzzles.\nFIG. 6 is a diagram of an example education game with real-time guidance and warnings generated from qualified players.  As shown in FIG. 6, trajectories or facial expressions of a user 602 detected by sensors (not shown) may be animated by an\nin-game avatar controlled by a student.  The user may encounter difficulties such as: wandering around or staying in places longer than normal compared to other players with high scores; inaction upon the sequence of puzzles longer than normal; changing\navatar direction aimlessly and is away from the target defined in the game; and getting wrong answers multiple times more than an average of other users.\nWhen the user encounters these difficulties, the system may detect these difficulties via an in-game avatar behavior monitor 608 and may generated and send an EP with a summary problem description to the creator and requestor matching service\nvia behavior assessment module 610.  The event pattern (EP) may contain the following context information:\nTABLE-US-00003 EP2: UserY@Time.GameEngineK //The game engine that the user is currently using.  Env.: Game W, Level 2 //The game ID and level that the user is having difficulties.  Obj: Success examples for level 2.  //Find a user who has\nsuccessfully pass level 2 Bt1: {Wandering: 0.8, Confusion: 0.9}.  //Assessments on problems of the user.\nBased on the context of the environment, objective, and problem assessment vector information, a qualified player who has high score and moved quickly passed the level may be selected.  A sequence of play logs recorded by the good player's 3D\ngame engine may be extracted as created content 606a and 606b, transferred, and rendered in the place where the student is having difficulties via the patterns of events guided content creation and consumption service 514.\nThis example illustrates some of the following example features of an event guided content creation system disclosed.  The players may not need to go on-line to look for content posted by other users.  If the student may not be actively\nparticipated in the education game, proactive tutorial generated from classmate may motivate the student to catch up.  The game developer may not need to develop detailed animated scenarios for all possible situations that the student may have problems\nwith.  The tutorial content of different variations may be generated from the play logs of the better and more enthusiastic users.  Recording of animated play sequences for important player event patterns, such as animation path to a treasure, kill a\nmonster, and answer to solve a puzzle, may not take as much storage as recoding rendered video clips.  Play back of the \"key events\" in play log may also be much easier to use to drive a coaching avatar in the requestor's 3D world.\nA further example feature includes that when a user starts the game, multiple different types of avatars may be created from other players that may provide multiple guided tours for the new player.  The player may be engaged in the education\ngame when other players' play logs may be synchronized with the player as if they are trying to solve the puzzle at the same time or taking a treasure hunting tour together.  The event patterns may constantly guide the generation of new content from\nother players and blended into the player's 3D world.  This may help the user stay engaged in the game.  Further, selecting qualified users to create content may avoid the potential distraction from abusive players in multi-play games.  The selected\ncontent creator may provide quality support to the content requestor to guide the player toward a positive direction rather than the distractions that may frequently occur in a multiplayer game.\nThe matching service may also efficiently support biking trail mobile fitness app scenarios.  In an example, the dynamic content creation model may also apply to pervasive services combining augmented reality, surveillance and virtual world\napplication with motion, location, orientation and time sensitive guidance and warning content.\nFIG. 7 is a diagram of an example fitness game with real-time guidance and warnings generated from qualified players.  As shown in FIG. 7, a biker may have participated in an augmented reality application with 3D mapped treasure hunt and avatars\nanimated personal coach.  The biker's trajectory and orientation may be tracked (for example, through a GPS and compass 702) and mapped into an avatar in a 3D world via behavior and conditions monitor 708 and behavior and conditions assessment module\n710, as similarly described above.  Selected content creators may display content 706a and 706b in the virtual world based on the behavior event patterns of the content requesting bikers.  For example, when the biker encounters problems such as, for\nexample: being lost in the wrong path; being stopped at a detour sign for more than ten seconds; approaching a range (for example, 300 meters) close to a wild animal sighted by other bikers; and displaying an irregular vital sign (for example, a\nheartbeat rate much higher than normal bikers in similar place).\nThese problems may be detected in the 3D virtual world (fused with the sensor data) and sent to the event pattern matching service 714 to match with other bikers' event patterns.  Using the event detection mechanism described in FIG. 2, the\nbikers in the same trails and making good progress may provide guidance to the biker having problems.  For example, the bikers may send the trajectory log to the augmented reality display engine to render trajectory in the augmented reality display of a\nrequestor.  Warning or alert message 706c may also be delivered to the requestor when the requestor approaching a dangerous area that other bikers may have spotted earlier or is encountering at the moment.\nIn an example, in addition to the event pattern matching based content creator selection process and proximity matching, the system disclosed herein may also integrate and utilize external web content aggregation sites, such as, for example,\nYOUTUBE, for archived content.  Content creators may create content for one content requester and archive the content in anticipation of usage (for example, download) by web users or new requests from other content requestors subscribed to the dynamic\ncontent creation service.  The archived content may be stored in the external web sites.\nThe content delivery queue shown in FIG. 2, may contain references to content archived (for example, posted) in multiple external content web sites such as, for example, YOUTUBE and TWITCH.  The EPs associated with the dynamically created\ncontent as described in the previous sections may be stored in the EP data cube shown in FIG. 2, with references to the archived content in the external sites or in the content delivery queue.  After matching with archived content, the system may use the\nreferences stored in the data cube to retrieve the archive content from external Website and store them into the content delivery queue to be consumed by requestors.\nThis external content archival integration may extend the dynamic content creation service to support networked content services for the types of content that are not sensitive to time or for those times when there may be no qualified content\ncreator available on-line.  For example, promotional tutorials with application ID, task ID and general EPs may be stored as descriptors for the content.  In this case, the general EPs may be review score, and other qualification attributes such as\navatar type.  Since, potentially, a large number of content may be returned by searching based on AppID and TasklD, (as key words) from the Websites, the additional EP may be used as filtering criteria to select the most suitable content and filter out\nless relevant content to minimize the manual operation required from the requestors.\nThe dynamic event pattern guided content creation and consumption service disclosed herein may create an eco-system among the app/game developers, pervasive service providers, content creators, and content consumers.  The system (e.g., system\n304) disclosed herein may support the following features for different entities in the eco-system.  The system may support customizable content creator and content requestor matching as a service.  As an example, app/game developers may provide value\nadded content matching rule group to implement new attributes for problem assessment vectors and qualification criteria to refine and expand the automated \"support service menu\" for a specialized content creator (for example, a coach) to offer more\npertinent help with higher quality content.  Also, the content matching service providers may charge a fee on the usage of matching rules.  The more matching rules, the higher the subscription fees may be.  The fee may be based on virtual currency or\nreal currency.  Users may also send virtual currency or real currency to each other.  In this case, the content service provider may support the recording of content usages and monitor the currency transactions.  In addition, the service provider may\nimplement value based pricing to charge fee based on assessment on the progress of the content requestor.  For example, if the content requestor used the tip provided in the content of a content creator to gain X virtual points, a percentage of the\npoints may be retained by the service provider.  The value based profit sharing may also include app/game developers and the content creators.\nAs a further example, the value of the content may be ranked.  A service provider may track the player's EP and rank the efficiency of the content (for example, tutorial) based on progress of the requesting user who consumed the content.  The\nsystem may give points to the content creator for content (for example, tutorials or training sessions) that may be helpful and deduct points otherwise.\nAs yet a further example, the system (e.g., system 304) may provide value based monetization.  Content that may be useful to the content requestors may be given points based on a percentage of score that the user obtained from using the content. App and game developers may subscribe to the EP matching service to access the point system.  The content service may use the point sharing system as incentive for participants to become qualified content creators by practicing and using the application\nmore frequently and possibly in a specialty area.  The specialty area may be defined by a behavior problem assessment vector, B, and a skill assessment vector, SV.  Players with the best skills to solve the problem defined in assessment vector may be the\nmost qualified to generate the tutorial content for the specified problem.  Different than the specialists, content creators with best in class skill or points earned may be qualified as elite content creators.  The elite content creators may be selected\nto create special promotional content requested by an organizer of special events for large number of users.\nThe system (e.g., system 304) disclosed herein may support various extended services described in the following examples.  In an example, the system may provide a domain specific problem and qualification skill assessment service.  The system\nmay provide a problem and skill assessment tracking and storage service.  It may support dynamic creation of new domain specific problem assessment attributes that may best assess the different types of problems of a user for different types of\napplications and games.  For example, the following attributes may be added to define more types of problems the users may encounter: deficiency (deficiency in avatar control or knowledge of the game for first time player), aimless (aimless navigation in\ntreasure hunting), hardship (hardship in solving puzzles), and confusion (confusion in usage of resources).  These attributes may be used by different types of behavior assessment methods to calculate the severity levels of \"problem\" behavior for each\napplication/game.  For example, user X and Y may have different \"problem assessment\" vectors.  The user with a smaller \"problem\" rating may offer help to the user with greater \"problem\" rating.  For example, User_X may have a need-index where\nneed-index={Deficiency: 0.8, Aimless: 0.2, Hardship: 0.2, Confusion: 0.7}.  Also, User_Y may have a need-index where need-index={Deficiency: 0.2, Aimless: 0.3, Hardship: 0.8, Confusion: 0.1}.  User_X may support User_Y on detecting deficiency in avatar\ncontrol or confusion on the use of a new weapon.\nIn addition, the system (e.g., system 304) may also support a skill level assessment vector to qualify content creators across multiple applications and games.  The skill vector may include generic player characteristics such as reaction time,\naccuracy, strategy, persistence, avatar control, navigation, and puzzle solving.  Depending upon the types of the application and game profile, the skill vector may be used as additional criteria to qualify creators.  High skill creators may form\ncoaching teams or serve as personal trainer for individual content requestors.\nAs a further example, the system (e.g., system 304) may include a content stitching service.  The system may monitor all of the EPs and progress made by the content requestor after consuming the content.  As a result, the system may add content\nstitching rules to combine multiple content from multiple content creators based on the effectiveness of the content services.  For example, the content stitching service may support the following content stitching rules.  The service may group the\ncontent created for events such as passing a subtask, task, or a level (e.g., getting a treasure, solved one puzzle, kill a monster and avoiding a hazardous condition).  In parallel, the service may rank the content used by the content requestor. \nFurther, the service may select content based on ranks to provide alternative content for a subtask, a task or multiple tasks to one or more content requestors.  For example:\nTABLE-US-00004 If A user passed a subtask X after consumed content C within T. second, Then, Save the (T, X, C) with the content creator ID, the EP used to create the content and the content itself.  Track the average of T, T.sub.avg for each\npair of (X, C) in a data cube.  If a qualified creator received new EP indicates that a user having problems with a task, Then, select content for each subtasks that have smallest T.sub.avg, for each (X, C) from multiple creators.\nThe above example may stitch content from one content creator who is most effective for one subtask (e.g., fighting a monster or hip rehab) with content created by another content creator who is the best for a different subtask (e.g., building a\nbridge or driving a race car).  Note that C may contain metadata related to ID, title, date, and descriptors for the content and the content itself.\nFurther, the system (e.g., system 304) may learn the associations between the most frequent and time consuming subtasks and stitch together the highest ranked content for the highest ranked problems in the beginning of the game as a tutorial to\nbeginners to reduce the chance that a user encounter a popular problem and abandon the game.\nAs a further example, the system (e.g., system 304) may include pre-recording and ranking services.  Pre-recording may mean that the system may continue to record and keep a fixed amount of content over a pre-defined amount of recording time\nwindow.  The proposed system may support pre-recording services so that a content creator will not miss the portion of the content which happened before the EP requested by the content requestor.  In addition, the pre-recorded content may be ranked by\ncomparing the user performance on the subtask with that of other users.  For example, a game engine or the 3D graphic engine may log all the animation sequences continuously when a user has been identified as a qualified content creator.  As a further\nexample, to minimize the storage, the recording may only be kept for the section of play within a time window before an \"important subtask events\" such as solving a puzzle, entering a new space, or killing a monster.  As another example, when the user\nhas excellent performance for the subtask relatively to the average performance collected from multiple users (e.g., in the data cube), pre-recorded content may be kept for a longer period of time (e.g., kept the recording for the whole duration of the\nplay when achieving excellent performance on the subtask).  The performance ranking, PerfRank, may also be recorded as metadata along with the T.sub.avg as metadata.  For example, a tuple (PerfRank, T.sub.avg, X, Content meta data descriptor, C) may be\nstored in data cube with reference to the recorded content.  The skills assessment, behavior assessment, problem assessment or effectiveness assessment, or a combination of these, may also be stored as metadata.  The metadata may be stored with the\ncontent associated with the important subtask within the application or game session.\nAs a further example, the system (e.g., system 304) may include competitive content quality improvement services.  The system may rank and keep a set of highly ranked content for each qualified content creator.  The ranking may be based on\nmultiple performance indicators of each creator, for example: a performance score on subtasks (e.g., PerfRank), length of the content (e.g., faster time to completion of subtask), a progress assessment of the content requestor who used the content (e.g.,\nT.sub.avg), and other user defined parameters.  Further, the ranking may be based on multiple ranking rules, such as, for example, Rule 1: highest value on a specific indicator; and Rule 2: highest weighted sum of two or more indicators.\nIf the content creator cannot or did not create new content with a better performance indicator (for example, having a skill level, better score or better navigation), the previously recorded content may still be used to qualify the content\ncreator.  The content creators may make efforts to continue to improve their performance indicators to stay competitive with other content creators or to create more varieties of content.  The content requestor may benefit from the continuous improvement\nor enjoy more variety of content.\nAs a further example, the system (e.g., system 304) may optimize content requestor and content creator matching services.  The default matching method may include a sequence of pattern evaluation between the problem summary and skill assessment\nof the qualified content creators (e.g., coach) in the proximity of the content requestor based on temporal spatial trajectory in the application tasks and objectives.  To support fast matching, the system may remember the high ranking content creators\nin the proximity of the content requestors.  For example, a previously qualified content creator in the same app or game server may be selected before searching for matched events from other content creators.\nFurthermore, to select suitable skill and performance levels of requestors (e.g., instructors), thresholds may be defined for each attribute of the problem assessment vectors such that the system may select content creators with \"adequate\" skill\nand performance levels to the content requestors.  This adequate level threshold matching may increase the probability of finding a content creator in proximity of the content requestor to offer the content and prevent poor performers and beginners from\nreceiving tutorials that are too advanced for their levels.\nAlthough features and elements are described above in particular combinations, one of ordinary skill in the art will appreciate that each feature or element can be used alone or in any combination with the other features and elements.  \"Modules\"\nand \"cubes\" described herein may be provided as a computer-readable medium for execution by a computer or processor.  In addition, the methods and functions described herein may be implemented in a computer program, software, or firmware incorporated in\na computer-readable medium for execution by a computer or processor.  Examples of computer-readable media include electronic signals (transmitted over wired or wireless connections) and computer-readable storage media.  Examples of computer-readable\nstorage media include, but are not limited to, a read only memory (ROM), a random access memory (RAM), a register, cache memory, semiconductor memory devices, magnetic media such as internal hard disks and removable disks, magneto-optical media, and\noptical media such as CD-ROM disks, and digital versatile disks (DVDs).  A processor in association with software may be used to implement a radio frequency transceiver for use in a WTRU, UE, terminal, base station, RNC, or any host computer.", "application_number": "15524434", "abstract": " A method and system for event pattern (EP) guided content services are\n     disclosed. A service entity may receive sensor events from a plurality of\n     users, the sensor events corresponding to behavior data detected from the\n     plurality of users, identify content creator candidates from among the\n     plurality of users based on a pattern of events which correspond to\n     sensor events received from an active user, select a content creator from\n     among the content creator candidates based on a best match to the pattern\n     of events, transmit a request for content of the selected content\n     creator, deliver content created by the content creator to the active\n     user, and monitor progress of the active user based on a playback of the\n     content by the active user.\n", "citations": ["6057856", "7786874", "7967731", "8639764", "8702516", "8832599", "8976007", "8977585", "9046917", "9075911", "9110958", "9171201", "9280640", "9413947", "9474970", "9723381", "9858584", "9886871", "9911352", "9999825", "20090299945", "20100008639", "20100332304", "20110093100", "20140100835", "20140195949", "20140278308", "20140372430", "20150120023", "20150258415", "20170065892"], "related": ["62076243"]}, {"id": "20180288211", "patent_code": "10375224", "patent_name": "System, a computer readable medium, and a method for providing an\n     integrated management of message information", "year": "2019", "inventor_and_country_data": " Inventors: \nKim; Dong Wook (Seongnam-si, KR)  ", "description": "<BR><BR>CROSS REFERENCE TO RELATED\nAPPLICATION\nThis application claims priority from and the benefit of Korean Patent Application No. 10-2017-0040405, filed on Mar.  30, 2017, which is hereby incorporated by reference for all purposes as if fully set forth herein.\n<BR><BR>BACKGROUND\n<BR><BR>Field\nExemplary embodiments of the invention relate generally to an apparatus and method for managing information on voice calls and text messages exchanged through a mobile telephone switching network, and more specifically, to an apparatus and a\nmethod for providing an integrated message management service that manages information on a voice call and a text message.\n<BR><BR>Discussion of the Background\nA mobile telephone system supports voice calls and the exchange of text messages between mobile terminals.  Recently, smartphones have sent and received voice calls and text messages through the mobile phone switching network.  A smartphone\ngenerally has a voice call management application for controlling the voice call function and managing voice call information, such as the time of receiving and sending a voice call, the counterpart (i.e., caller) information (phone number or contact\nname) of the voice call, total call duration, etc. The user can execute the voice call management application to inquire the voice call history and the detailed information of the voice call, and delete the inquired voice call information.  On the other\nhand, a smartphone generally also has a text message management application that controls a text message function and stores information, for example, the time for receiving and sending a text message, the counterpart (i.e., person texting) information\n(phone number or contact name) of the text message, contents of the text message, etc. The user can execute the text message management application to inquire the text message history and the detailed information of the text message, and delete the\ninquired text message information.\nHowever, since the voice call application and the text message application in a conventional smartphone are executed independently, the voice call information and the text message information cannot be managed at once.  In addition, the\nconventional voice call application and the text message application do not provide any additional functions other than its functions of receiving/transmitting a voice call/text message and managing voice call/text message information.  Therefore, even\nwhen a user desires other additional functions in association with voice calls and text messages sent and received by the user, it may be difficult to immediately execute the additional functions.\nThe above information disclosed in this Background section is only for understanding of the background of the inventive concepts, and, therefore, it may contain information that does not constitute prior art.\n<BR><BR>SUMMARY\nExemplary embodiments of the present invention provide a method and a mobile device for providing a user interface for managing voice call information and text message information.\nAdditional features of the inventive concepts will be set forth in the description which follows, and in part will be apparent from the description, or may be learned by practice of the inventive concepts.\nAn exemplary embodiment provides a mobile communication device comprising a voice call management part configured to manage a voice call, a text message management part configured to manage text messages exchanged through a mobile telephone\nswitching network, a display part, an integrated message management unit, and an additional function process part configured to process at least one additional function in association with the integrated message management unit.  The integrated message\nmanagement unit may comprise a message monitoring unit configured to monitor voice call information and text message information in association with the voice call management part and the text message management part, a message information managing unit\nconfigured to generate integrated message information, which is to be provided to a user, based on the voice call information and the text message information, an interface managing unit configured to generate an integrated message management user\ninterface displaying the integrated message information, and an artificial intelligence agent analyzing the voice call information and the text message information and providing a service associated with at least one additional function in association\nwith the additional function processing unit based on the analyzed result.\nAnother exemplary embodiment provides a method, performed by an artificial agent of a mobile device, for creating a schedule based on analysis of incoming and outgoing message, the method comprises performing morpheme analysis and speech act\nanalysis on a first message transmitted or received by the mobile device, determining whether the first message is a message for arranging a schedule, extracting entities constituting a schedule from the first message to generate schedule information and\ntemporarily storing the schedule information, determining whether the first message is a notification of a fixed schedule using the result of the morpheme analysis and speech act analysis, and registering the schedule information when the first message\nis determined as a notification of a fixed schedule.\nThe foregoing general description and the following detailed description are exemplary and explanatory and are intended to provide further explanation of the claimed subject matter.\nIt is to be understood that both the foregoing general description and the following detailed description are exemplary and explanatory and are intended to provide further explanation of the invention as claimed. <BR><BR>BRIEF DESCRIPTION OF THE\nDRAWINGS\nThe accompanying drawings, which are included to provide a further understanding of the invention and are incorporated in and constitute a part of this specification, illustrate exemplary embodiments of the invention, and together with the\ndescription serve to explain the inventive concepts.\nFIG. 1 is a block diagram illustrating a configuration of an integrated message management service system according to an exemplary embodiment of the present invention.\nFIG. 2 is a block diagram illustrating a configuration of a mobile terminal according to an exemplary embodiment of the present invention.\nFIG. 3 is a detailed block diagram illustrating a mobile terminal according to an exemplary embodiment of the present invention.\nFIG. 4 is a screenshot illustrating an example of a first message management user interface displayed on a mobile terminal.\nFIG. 5 is a flowchart illustrating a process for generating a first message management user interface having the configuration shown in FIG. 4.\nFIG. 6 is a flowchart illustrating a process of generating a second message management user interface.\nFIG. 7 is a screenshot showing an example of a second message management user interface provided by the integrated message management part.\nFIG. 8 is a block diagram illustrating an artificial intelligence (AI) agent according to an exemplary embodiment of the present invention.\nFIG. 9 is a flow chart illustrating a general process of a service performed by an AI agent according to an exemplary embodiment of the present invention.\nFIG. 10 is a flowchart illustrating a process for generating schedule information performed by an AI agent according to an exemplary embodiment of the present invention.\nFIG. 11 is a screen shot showing a second message management user interface displaying a dialog between the user of the mobile terminal and a friend of the user.\nFIG. 12 is a screen shot showing an example of a schedule information registration graphical user interface capable of receiving a user input for approval and rejection of registering schedule information.\nFIG. 13 is an exemplary screenshot of a first message management user interface 100 where a first event panel is displayed.\nFIG. 14 is a flowchart illustrating a process for generating a first event panel according to an exemplary embodiment of the present invention.\nFIG. 15 is a flowchart illustrating a process for generating a first event panel according to an exemplary embodiment of the present invention.\nFIG. 16 is a flowchart showing processes for generating an authentication code copy interface.\nFIG. 17 is a flowchart illustrating a process for generating an authentication code copy interface.\nFIG. 18 is a screenshot illustrating an example of the authentication code copy interface provided by the present invention.\nFIG. 19 is a block diagram illustrating an integrated message management service server configured according to a first system configuration according to an exemplary embodiment of the present invention.\nFIG. 20 is a block diagram illustrating an integrated message management service server configured according to the second system configuration according to an exemplary embodiment of the present invention.\nFIG. 21 is a block diagram illustrating a mobile terminal configured according to a second system configuration according to an exemplary embodiment of the present invention.\n<BR><BR>DETAILED DESCRIPTION\nIn the following description, for the purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of various exemplary embodiments or implementations of the invention.  As used herein\n\"embodiments\" and \"implementations\" are interchangeable words that are non-limiting examples of devices or methods employing one or more of the inventive concepts disclosed herein.  It is apparent, however, that various exemplary embodiments may be\npracticed without these specific details or with one or more equivalent arrangements.  In other instances, well-known structures and devices are shown in block diagram form in order to avoid unnecessarily obscuring various exemplary embodiments. \nFurther, various exemplary embodiments may be different, but do not have to be exclusive.  For example, specific shapes, configurations, and characteristics of an exemplary embodiment may be used or implemented in another exemplary embodiment without\ndeparting from the inventive concepts.\nUnless otherwise specified, the illustrated exemplary embodiments are to be understood as providing exemplary features of varying detail of some ways in which the inventive concepts may be implemented in practice.  Therefore, unless otherwise\nspecified, the features, components, modules, layers, films, panels, regions, and/or aspects, etc. (hereinafter individually or collectively referred to as \"elements\"), of the various embodiments may be otherwise combined, separated, interchanged, and/or\nrearranged without departing from the inventive concepts.\nThe use of cross-hatching and/or shading in the accompanying drawings is generally provided to clarify boundaries between adjacent elements.  As such, neither the presence nor the absence of cross-hatching or shading conveys or indicates any\npreference or requirement for particular materials, material properties, dimensions, proportions, commonalities between illustrated elements, and/or any other characteristic, attribute, property, etc., of the elements, unless specified.  Further, in the\naccompanying drawings, the size and relative sizes of elements may be exaggerated for clarity and/or descriptive purposes.  When an exemplary embodiment may be implemented differently, a specific process order may be performed differently from the\ndescribed order.  For example, two consecutively described processes may be performed substantially at the same time or performed in an order opposite to the described order.  Also, like reference numerals denote like elements.\nWhen an element, such as a layer, is referred to as being \"on,\" \"connected to,\" or \"coupled to\" another element or layer, it may be directly on, connected to, or coupled to the other element or layer or intervening elements or layers may be\npresent.  When, however, an element or layer is referred to as being \"directly on,\" \"directly connected to,\" or \"directly coupled to\" another element or layer, there are no intervening elements or layers present.  To this end, the term \"connected\" may\nrefer to physical, electrical, and/or fluid connection, with or without intervening elements.  Further, the D1-axis, the D2-axis, and the D3-axis are not limited to three axes of a rectangular coordinate system, such as the x, y, and z--axes, and may be\ninterpreted in a broader sense.  For example, the D1-axis, the D2-axis, and the D3-axis may be perpendicular to one another, or may represent different directions that are not perpendicular to one another.  For the purposes of this disclosure, \"at least\none of X, Y, and Z\" and \"at least one selected from the group consisting of X, Y, and Z\" may be construed as X only, Y only, Z only, or any combination of two or more of X, Y, and Z, such as, for instance, XYZ, XYY, YZ, and ZZ.  As used herein, the term\n\"and/or\" includes any and all combinations of one or more of the associated listed items.\nAlthough the terms \"first,\" \"second,\" etc. may be used herein to describe various types of elements, these elements should not be limited by these terms.  These terms are used to distinguish one element from another element.  Thus, a first\nelement discussed below could be termed a second element without departing from the teachings of the disclosure.\nSpatially relative terms, such as \"beneath,\" \"below,\" \"under,\" \"lower,\" \"above,\" \"upper,\" \"over,\" \"higher,\" \"side\" (e.g., as in \"sidewall\"), and the like, may be used herein for descriptive purposes, and, thereby, to describe one elements\nrelationship to another element(s) as illustrated in the drawings.  Spatially relative terms are intended to encompass different orientations of an apparatus in use, operation, and/or manufacture in addition to the orientation depicted in the drawings. \nFor example, if the apparatus in the drawings is turned over, elements described as \"below\" or \"beneath\" other elements or features would then be oriented \"above\" the other elements or features.  Thus, the exemplary term \"below\" can encompass both an\norientation of above and below.  Furthermore, the apparatus may be otherwise oriented (e.g., rotated 90 degrees or at other orientations), and, as such, the spatially relative descriptors used herein interpreted accordingly.\nThe terminology used herein is for the purpose of describing particular embodiments and is not intended to be limiting.  As used herein, the singular forms, \"a,\" \"an,\" and \"the\" are intended to include the plural forms as well, unless the\ncontext clearly indicates otherwise.  Moreover, the terms \"comprises,\" \"comprising,\" \"includes,\" and/or \"including,\" when used in this specification, specify the presence of stated features, integers, steps, operations, elements, components, and/or\ngroups thereof, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof.  It is also noted that, as used herein, the terms \"substantially,\" \"about,\" and other\nsimilar terms, are used as terms of approximation and not as terms of degree, and, as such, are utilized to account for inherent deviations in measured, calculated, and/or provided values that would be recognized by one of ordinary skill in the art.\nIn exemplary embodiments, one or more components thereof, including components described as a \"part\", may be implemented via one or more general purpose and/or special purpose components, such as one or more discrete circuits, digital signal\nprocessing chips, integrated circuits, application specific integrated circuits, microprocessors, processors, programmable arrays, field programmable arrays, instruction set processors, and/or the like.\nAccording to one or more exemplary embodiments, the parts, features, functions, processes, etc., described herein may be implemented via software, hardware (e.g., general processor, digital signal processing (DSP) chip, an application specific\nintegrated circuit (ASIC), field programmable gate arrays (FPGAs), etc.), firmware, or a combination thereof.  In this manner, one or more components thereof may include or otherwise be associated with one or more memories (not shown) including code\n(e.g., instructions) configured to cause information management, and/or one or more components thereof to perform one or more of the parts, features, functions, processes, etc., described herein.\nThe memories may be any medium that participates in providing code to the one or more software, hardware, and/or firmware components for execution.  Such memories may be implemented in any suitable form, including, but not limited to,\nnon-volatile media, volatile media, and transmission media.  Non-volatile media include, for example, optical or magnetic disks.  Volatile media include dynamic memory.  Transmission media include coaxial cables, copper wire and fiber optics. \nTransmission media can also take the form of acoustic, optical, or electromagnetic waves.  Common forms of computer-readable media include, for example, a floppy disk, a flexible disk, hard disk, magnetic tape, any other magnetic medium, a compact\ndisk-read only memory (CD-ROM), a rewriteable compact disk (CD-RW), a digital video disk (DVD), a rewriteable DVD (DVD-RW), any other optical medium, punch cards, paper tape, optical mark sheets, any other physical medium with patterns of holes or other\noptically recognizable indicia, a random-access memory (RAM), a programmable read only memory (PROM), and erasable programmable read only memory (EPROM), a FLASH-EPROM, any other memory chip or cartridge, a carrier wave, or any other medium from which\ninformation may be read by, for example, a controller/processor.\nUnless otherwise defined, all terms (including technical and scientific terms) used herein have the same meaning as commonly understood by one of ordinary skill in the art to which this disclosure is a part.  Terms, such as those defined in\ncommonly used dictionaries, should be interpreted as having a meaning that is consistent with their meaning in the context of the relevant art and should not be interpreted in an idealized or overly formal sense, unless expressly so defined herein.\nFIG. 1 is a block diagram illustrating a configuration of an integrated message management service system according to an exemplary embodiment of the present invention.\nThe integrated management service system according to an exemplary embodiment may include a plurality of mobile terminals 10, an integrated message management service server 20, and data communication network 30.  The mobile terminals 10 and the\nintegrated message management service server 20 may be in communication with each other to transmit signals or data for providing an integrated message information management service via the data communication network 30.  The mobile terminal 10 may be a\ntelecommunication device that may transmit and receive a voice call and a text message via a communication network, and exchange data via wireless communication network.  The mobile terminal 10 may include a display device configured to display various\ngraphical user interfaces.\nAn integrated message management service system according to an exemplary embodiment of the present invention may have various system configurations.  Hereinafter, an integrated message information management system and an integrated message\ninformation management service provided by the system will be described with reference to exemplary embodiments of the present invention.\nFIG. 2 is a block diagram illustrating a configuration of a mobile terminal according to an exemplary embodiment of the present invention.\nThe mobile terminal 10 according to an exemplary embodiment of the present invention includes a voice call management part 1100, a text message management part 1200, an additional function process part 1300, an integrated message management part\n1400, a memory 1500, and a transceiver 1600.  Additional input/output elements may also be included in terminal 10, including but not limited to a speaker, microphone, switch, touch sensor, keyboard, trackball, pen, camera, LED light, etc.\nThe voice call management part 1100 may be software, hardware, or a combination of hardware and software, each of which is configured to process a voice call exchanged through the mobile-phone switching network, and to manage information on the\nvoice call (hereinafter, referred to as \"voice call information\").  The voice call information may include, for example, a receiving and transmitting time of a voice call, a counterpart (phone number or contact name) information of a voice call, a total\ncall time, success or failure of the voice call connection, and the like.  The voice call management part 1100 collects the voice call information on all of the voice calls received or transmitted by the mobile terminal 10 and stores the voice call\ninformation.  The management of the voice call information may be performed in conjunction with an address book management application installed in the mobile terminal 10.  For example, if caller identification (CID) is included in the incoming voice\ncall, the received CID is compared with the phone numbers stored in the address book.  If the same number is stored in the address book, the voice call information management part 1100 retrieves the contact name of the CID and records it in the voice\ncall information.  Otherwise, the phone number (CID) is recorded in the voice call information.\nThe text message management part 1200 may be software, hardware, or a combination of hardware and software, which is configured to transmit/receive a text message exchanged through a mobile telephone switching network, such as a Short Message\nService (SMS) and a Multimedia Messaging Service (MMS), and to manage information about a text message (hereinafter, \"text message information\").  The text message information managed by the text message management part 1200 may include, for example, a\nreceiving and transmitting time, a receiver/sender (phone number), content of the text message, and the like.  In addition, management of the text message information may be performed in cooperation with an address book application installed in the\nmobile terminal 10.  For example, if there is caller identification (CID) included in the received text message, the received CID is compared with the phone numbers stored in the address book.  If the same number is stored in the address book, the text\nmessage management part 1200 retrieves a contact name having the CID and records the contact name in the text message information.  Otherwise, the phone number (CID) is recorded in the text message information.\nThe transceiver 1600 performs a function of receiving and transmitting a voice call and a text message through the mobile telephone switching network.  The transceiver 1600 typically may also function to receive and transmit signals related to\npacket data communication, other than voice communication and text messages, through a wireless data communication network.\nThe additional function process part 1300 may be hardware, software, or a combination of hardware and software configured to provide various additional functions.  For example, the additional function process part 1300 may be a combination of\nsoftware including instructions necessary for performing the additional functions, and a processor performing processes according to the instructions.  At this time, the software, as a part of the additional function process part 1300, may be installed\nin the mobile terminal 10 in a form of an application.  The additional function process part 1300 may be configured to perform one or more functions.  Hereinafter, the operation of the additional function process part 1300 will be described for each\nfunction.\nThe mobile terminal 10 according to an exemplary embodiment of the present invention may include a schedule information management application, as an exemplary part of the additional function process part 1300.  The schedule information\nmanagement application is configured to manage schedule information of the user.  The schedule information management application may provide a schedule management user interface through which a user creates, modifies, and deletes his or her schedule. \nThe schedule information input through the schedule management user interface may include, for example, a scheduled date and time, a place, a scheduled entry, and the like.  The schedule information is stored in a memory 1500.\nThe mobile terminal 10 according to an exemplary embodiment of the present invention may include a memo application as an example of the additional function process part 1300.  The memo application provides a memo user interface for creating,\nmodifying, deleting, and viewing a memo, and stores the created memo in the memory 1500 of the mobile terminal 10.\nThe mobile terminal 10 according to an exemplary embodiment of the present invention may include a social network service (SNS) application as an example of the additional function process part 1300.  An SNS application is used to share a\nvariety of content over a data network for social relationships among multiple users.\nThe above-mentioned additional functions may be provided by an additional function process part 1300 embedded in the integrated message management part 1400.\nThe integrated message management part 1400 manages voice call information and text message information in association with the voice call management part 1100 and the text message management part 1200, and provides an additional function in\nassociation with the additional function process part 1300 (e.g., a schedule information management application, a memo managing unit, and an SNS application).\nFIG. 3 is a detailed block diagram illustrating a mobile terminal according to an exemplary embodiment of the present invention.\nReferring to FIG. 3, the integrated message management part 1400 includes a message monitoring unit 1410, a message information managing unit 1420, a user interface managing unit 1430, and an artificial intelligence agent (AI agent) 1440, each\nof which may be software, hardware, or a combination of hardware and software configured to perform its respective function.\nThe message monitoring unit 1410 monitors, in association with the voice call management part 1100 and the text message management part 1200, updates of information on voice calls and/or text messages received or transmitted by the mobile\nterminal 10.\nWhen the mobile terminal 10 receives a voice call, the voice call management part 1100 stores information on the incoming voice call, such as counterpart (phone number or contact name) information and a receiving time of the incoming voice call,\nin the memory 1500 of the mobile terminal 10.  In addition, when the user of the mobile terminal 10 has actually answered the incoming voice call, the total conversation time and/or the call termination time may be recorded in the memory 1500.  On the\nother hand, if the user of the mobile terminal 10 does not respond to the incoming voice call, that is, if a voice call connection fails, the voice call connection failure may be recorded in the memory 1500.  As such, the voice call management part 1100\nstores the information on the incoming voice call in the memory 1500 every time a voice call is received.\nThe message monitoring unit 1410 accesses the memory 1500 to look up voice call information stored therein.  The message monitoring unit 1410 monitors whether the incoming voice call information is updated in various manners.  For example, the\nmessage monitoring unit 1410 may be configured to periodically access the memory 1500 to monitor whether new incoming voice call information is updated.  Alternatively, the message monitoring unit 1410 may be configured to monitor updates of incoming\nvoice call information in real-time.  Still alternatively, the message monitoring unit 1410 may be configured to access to the memory 1500 to monitor an update of incoming voice call information only upon activation of the integrated message management\npart 1400.\nThe above-described monitoring update of incoming voice call information performed by the message monitoring unit 1410 may also be performed for outgoing voice call.  In particular, the message monitoring unit 1410 monitors updates of\ninformation on outgoing voice calls originating from the user through the mobile terminal 10 in substantially the same manner as described above for the incoming voice call information.\nMonitoring of updates of information on text messages, performed by the message monitoring unit 1410, is described in detail below.\nWhen a text message is received by the user's mobile terminal 10, the text message management part 1200 stores information on incoming text message in the memory 1500 of the mobile terminal 10.  The incoming text message information may include,\nfor example, a sender, a receiving time, content of the message, and the like.  The message monitoring unit 1410 may access the memory 1500 to monitor an update of incoming text message information.  The message monitoring unit 1410 can monitor whether\nthe received text message information is updated in various manners.  For example, the message monitoring unit 1410 may be configured to periodically access the memory 1500 to monitor whether new incoming text message information is updated. \nAlternatively, the message monitoring unit 1410 may be configured to detect an update of incoming text message information in real-time.  Still alternatively, the message monitoring unit 1410 may be configured to access the memory 1500 to monitor an\nupdate of incoming text message information only upon activation of the integrated message management part 1400.\nThe above-described monitoring update of incoming text message information performed by the message monitoring unit 1410 may also be performed for outgoing text message.  In particular, the message monitoring unit 1410 monitors updates of\ninformation on outgoing text messages transmitted from the user through the mobile terminal 10 in substantially the same manner as described above for the incoming text message information.\nThe message information managing unit 1420 is configured to retrieve the updated voice call information and updated text message information detected by the message monitoring unit 1410, and to generate integrated message information which will\nbe provided for the user through an integrated message management user interface 100.  For example, when the message monitoring unit 1410 detects a new incoming voice call, the message information managing unit 1420 retrieves the incoming voice call\ninformation, and generates integrated message information based on the retrieved incoming voice call information.  The generated integrated message information may be stored in the memory 1500.\nThe above described operation of the integrated message information managing unit 1420 may be performed in synchronization with the message monitoring operation of the message monitoring unit 1410.\nOn the other hand, the message information managing unit 1420 may be configured to operate asynchronously with the operation of the message monitoring unit 1410.  More particularly, regardless of configuration of message monitoring operation,\nthe message information managing unit 1420 may be configured to retrieve the update of the voice call information and the text message information at a different time period than the monitoring period, or may be configured to operate only when integrated\nmessage information management application is activated.\nOn the other hand, the message information managing unit 1420 is configured to access the memory 1500 to read information about update of the message inquired by the message monitoring unit 1410, and to generate the integrated message\ninformation based on the information.\nThe user interface managing unit 1430 generates and displays an integrated message management user interface, receives various user inputs through the integrated message management user interface, and performs processes corresponding to the user\ninput.  More specifically, the user interface managing unit 1430, in response to a user input, generates a first message management user interface and displays it on a screen of the mobile terminal 10.  In addition, the user interface managing unit 1430,\nin response to a user input, may generate a second message management user interface and displays it on a screen of the mobile terminal 10.  In addition, the user interface managing unit 1430 may access the additional function process part 1300 to\nprocess an additional function in association with the first and second message management user interfaces, and display the processed results on the first and second message management user interface.\nThe integrated message management part 1400 may be hardware, software, or a combination of hardware and software configured to perform the above-described processes.  For example, the integrated message management part 1400 may be a combination\nof software including instructions necessary to perform the above-described message monitoring/message managing user interface managing functions, and a processor performing processes corresponding to the instructions.  In this case, the software which\nis a part of the integrated message management part 1400 may be installed in the mobile terminal 10 in the form of an application.  The message monitoring unit 1410, the message information managing unit 1420, and the user interface managing unit 1430\nmay be independent software and hardware, respectively.  On the other hand, each function of the integrated message management part 1400 may be performed in a form of distributed process.\nThe operation of the integrated message management part 1400 according an exemplary embodiment of the present invention will be described in more detail.\nFirst, the process of generating the first message management user interface by the user interface managing unit 1430 will be described in detail with reference to FIG. 4 and FIG. 5.\nFIG. 4 is a screenshot illustrating an example of a first message management user interface displayed on a mobile terminal.\nReferring to FIG. 4, a first message management user interface 100 according to an exemplary embodiment includes a message information display area 110 and a menu bar 120.  The message information display area 110 includes a plurality of message\nblocks 111 and 112.  Each of the message blocks 111 and 112 includes integrated message information corresponding to a voice call or a text message.  The integrated message information is information generated from voice call information or text message\ninformation stored in the memory 1500, and may include all or part of the entries included in the voice call information and the text message information.\nFor example, the message block 111 and 112 display recipient or sender information.  The recipient or sender information may be provided with reference to the address book.  In addition, the message blocks 111 and 112 may include an\nincoming/outgoing indicator indicating whether a message was received or sent.  For example, whether a voice call and a text message are received or transmitted may be indicated by direction of an arrow.  The integrated message information may also\ninclude a message type indicator indicating whether the message is a voice call or a text message.  Referring to FIG. 4, a message block 111 for a text message includes a counterpart 111a of the text message, a content 111b of a text message, a\nreceiving/sending time 111c.  On the other hand, in the message block 112 for a voice call includes a counterpart 112a, the receiving/sending time 112b, the total call time 112c, and receiving/sending indicator 112d.\nThe user interface managing unit 1430 may sort the message blocks 111 and 112 in the order of time.  For example, the message blocks 111 and 112 may be arranged in descending order according to the receiving/transmitting time.  Accordingly,\nmessage blocks 111 and 112 of the most recently received or transmitted message are displayed at the top of the message information display area 110.\nFIG. 5 is a flowchart illustrating a process of generating a first message management user interface having the configuration shown of FIG. 4.\nReferring to FIGS. 4 and 5, in step S1100, the message monitoring unit 1410 monitors whether voice call information/text message information is updated in association with the voice call management part 1100 and text message management part\n1200.\nWhen the message monitoring unit 1410 detects that voice call information or text message information is updated (e.g., new voice call and/or text message is received or transmitted), in step S1200, the message information managing unit 1420\nretrieves the updated voice call information and/or text message information.  In step S1300, the message information managing unit 1420 edits the call information and/or text message information according to a message block format to generate the\nintegrated message information.\nThe message block format may define the entries of the message information to be included in the message blocks 111 and 112.  The message block format may be set by the user or the integrated message management service server 20.  Referring back\nto FIG. 4, the text message block 111 includes a recipient/sender (counterpart) contact 111a, receiving/transmitting time 111c, and message body 111b.  In addition, the voice call block 112 includes the recipient/sender (counterpart) contact 112a,\nreceiving/transmitting time 112b, total talk time (or call connection failure indication) 112c, and receiving/sending indicator 112d.  The message information managing unit 1420 may extract only the entries defined by the message block format from the\nvoice call information and the text message information, to generate the integrated message information.\nIn step S1500, the user interface managing unit 1430 then generates message blocks 111 and 112, in which the respective entries of the integrated message information are arranged, according to an arrangement rule defined by the message block\nformat.  The arrangement rule defines the position of each entry of the retrieved integrated message information to be displayed in the message blocks 111 and 112.  In the example of FIG. 4, the counterpart identification information 111a, the message\nbody 111b, and the receiving/transmitting time 111c may be defined to be displayed at a specific location of the message blocks 111 and 112, respectively.  According to the arrangement rule, the user interface managing unit 1430 generates message blocks\n111 and 112, in which the each entry of the retrieved integrated message information is arranged, at a position as shown in FIG. 4.\nThen, in step S1600, the user interface managing unit 1430 generates a first message management user interface 100, in which voice call information and text message information are arranged, in the order of receiving/transmitting time.\nThe integrated message information generated by the message information managing unit 1420, and/or the message blocks 111 and 112 generated by the user interface managing unit 1430 may be stored in the memory 1500.  Once the integrated message\ninformation and/or message blocks 111 and 112 are stored in the memory 1500, the user interface managing unit 1430 may retrieve the stored integrated message information and/or message blocks 111 and 112 to generate the first message management user\ninterface 100 without repeating the above processes.  In this case, for a new voice call and/or text message, the above-described processes may be performed to display the newly generate the message blocks 111 and 112 in the first message management user\ninterface 100.\nIn the above examples, the message information managing unit 1420 edits of voice call information and text message information to generate and edit the integrated message information.  However, the above processes may be performed by the user\ninterface managing unit 1430 partially or entirely.  For example, when a message block format defines that an entry related to the receiving/sending indicator is to be displayed using an arrow image, in step S1600, the user interface managing unit 1430\nmay replace the receiving/sending indicator with the arrow image when generating the first message management user interface 100.\nThe first message management user interface 100 of FIG. 4 includes a menu bar 120.  The first message management user interface mode selection icons 121a, 121b and 121c, a search tool 122, and an address book icon 123 are displayed on the menu\nbar 120.\nWhen the input window of the search tool 122 is touched, the user interface managing unit 1430 provides a character input user interface.  When the user inputs a search word through the character input user interface, the user interface managing\nunit 1430 looks for the integrated message information including the input search word through a search engine built in the integrated message management part 1400, and displays the search result on the first message management user interface 100.\nWhen the user touches the address book icon 123 of the menu bar 120, the user interface managing unit 1430 accesses the address book application and retrieves the contact information.  At the same time, the user interface managing unit 1430\nconverts the first message management user interface 100 into an address book user interface, and displays it on a screen of the mobile terminal 10.\nUpon receiving a user inputs selecting a message block of FIGS. 4 and 5, e.g., touching a message block 111, 112, details of the selected message block may be displayed on the terminal 10.  The user interface managing unit 1430 may be configured\nto provide a second message management user interface, which displays details of a message in response to the user input of selecting the message blocks 111 and 112 of the first message management user interface 100.\nHereinafter, the process of generating the second message management user interface will be described in detail with reference to FIG. 6 and FIG. 7.\nFIG. 6 is a flowchart illustrating a process of creating a second message management user interface, and FIG. 7 is a screenshot illustrating an example of a second message management user interface provided by the integrated message management\npart 1400.\nIn step S2100, the user interface managing unit 1430 receives a user input activating a second message management user interface.  The user input activating a second message management user interface may be, for example, the user touching the\nmessage blocks 111 and 112 of the first message management user interface 100, as described above.  In addition, the user input activating the second message management user interface may be the user touching the message notification displayed on the\nstatus bar of the mobile terminal 10 when the integrated message information management application is not activated.\nIn step S2200, when a user input activating the second message management user interface is received, the user interface managing unit 1430 identifies the counterpart (recipient/sender) of the selected message block.  In step S2300, the message\ninformation managing unit 1420 retrieves all of the voice call information and the text message information of which recipient/sender is the identified counterpart.  In step S2400, the message information managing unit 1420 edits the voice call\ninformation and the text message information to generate a dialogue information according to a predetermined dialogue information format.  The dialogue information format defines entries to be displayed and a notation of each entry.  For example, the\ndialogue information format for a voice call may be defined as displaying receiving/sending time in a form of \"HH:MM AM (or PM)\" and total conversation time in a form of \"HH:MM:SS\".  On the other hand, the dialogue format for the text message may be\ndefined as displaying receiving/sending time in a form of \"HH:MM AM (or PM)\" and message body in a form of full-text of the message.  The message information managing unit 1420 extracts only the entries defined by the dialogue information format from the\nvoice call information and the text message information, and edits the extracted entries in the form defined by the dialogue information format to generate the dialogue information.\nIn the above described example, the step S2400 of generating dialogue information is described as being performed by the message information managing unit 1420.  However, according to an exemplary embodiment, all or a part of the process in step\nS2400 may be performed by the user interface managing unit 1430.\nNext, in step S2500, the user interface managing unit 1430 visualizes the dialogue information according to the dialogue configuration rule, and in step S2600, generates a second message management user interface including the dialogue\ninformation.  The dialogue configuration rule defines the arrangement and display form of each entry of the dialogue information.  For example, the dialogue information rule may define: 1) dialogue information of the incoming voice call and the text\nmessage is placed on the left side of the interface and dialogue information of the outgoing voice call and the text message is placed on the right side of the second message management user interface 500; 2) total conversation time of a voice call and\nfull text of a message is provided in a form of a speech bubble; 3) the receiving/sending time is displayed outside the speech bubble; and 4) all the dialogue information is listed in descending order according to the receiving/sending time.  The user\ninterface managing unit 1430 processes and visualizes each entry of dialogue information according to the dialogue configuration rule, and generates a second message management user interface displaying the visualized results.\nThe second message management user interface generated according to the above exemplary dialog information format and dialog configuration rule is as shown in FIG. 7.\nReferring to FIG. 7, the second message management user interface 500 includes a dialogue title region 510, a dialogue display region 520, and a character input region 530.  In the title region 510, counterpart information and a telephone\ndialing icon is provided.  In the dialogue display region 520, the dialogue information is displayed.  For example, the unit dialogue 521 corresponds to an outgoing voice call.  In accordance with the dialogue information format and the dialogue\nconfiguration rule, the total call time entry is inserted into a speech bubble in the form of \"voice call 03:07\" and sending time entry is displayed at left side of the speech bubble in a form of text string \"11:03 AM\".  The unit dialogue 522 relates to\na received text message.  In accordance with the dialogue information format and the dialogue configuration rule, the received text is inserted into a speech bubble in the form of full text \"OK!\", and receiving time is displayed at right side of the\nspeech bubble in a form of text string \"08:02 PM\".  In addition, the unit dialogue 522 related to the received text message is arranged on the left side of the interface, the unit dialogue 521 related to the outgoing voice dialogue is arranged on the\nright side of the interface, and the dialogues are arranged in order of receiving/transmitting time.\nThe second message management user interface 500 is configured to provide detailed information on messages exchanged with a specific person (counterpart).  For example, the second message management user interface 500 may be configured to\nprovide information on voice calls and text messages exchanged with a specific user in an interactive manner.  While the first message management user interface 100 lists information on all message in a time series, the second message management user\ninterface 500 displays more detailed contents of messages exchanged with a specific entity.\nThe second message management user interface 500 may be provided in response to various user inputs.  First, as described above, when the user interface managing unit 1430 detects a user input, such as touching a specific message block displayed\non the first message management user interface 100 (\"message block selection input\"), the user interface managing unit 1430 identifies the counterpart (recipient or sender) of the selected message block.  The user interface managing unit 1430 accesses\nthe memory 1500 to retrieve integrated message information on voice calls/text messages received from and transmitted to the corresponding counterpart, generates the second message management user interface 500 by listing the retrieved integrated message\ninformation in a time series, and displays it on the mobile terminal 10.  As such, in response to the user's message block selection input, the user interface managing unit 1430 switches the first message management user interface 100 to the second\nmessage management user interface 500.\nThe second message management user interface 500 may display and provide each message information in an interactive format.  For example, referring to FIG. 7, the information on a received message is displayed on the left side, and the\ninformation on a transmitted message is displayed on the right side.  The incoming speech bubble 522 and the outgoing speech bubble 521 may have different colors or fonts for easy identification.\nThe second message management user interface 500 may be configured to include a character input region 530.  The character input region 530 may be disposed at the lower end of the second message management user interface 500, as shown in second\nmessage management user interface 200 of FIG. 17.  The character input region 530 includes a character input window 531 and a \"send\" graphical user interface 532.\nThe artificial intelligence agent 1440 analyzes the contents of the incoming and outgoing text messages, and provides one or more services corresponding to the analyzed message contents to the user.\nHereinafter, the configuration and operation of the AI agent 1440 will be described in detail with reference to FIG. 8.\nFIG. 8 is a block diagram illustrating the configuration of an artificial intelligence agent 1440 according to an embodiment of the present invention.\nThe artificial intelligence agent 1440 is configured to include a speech act analysis module 1441, a dialog tracking module 1442, and a service response generation module 1443, each of which may be software, hardware, or a combination of\nhardware and software configured to perform its respective function.\nThe speech act analysis module 1441 performs morpheme analysis and speech act analysis on the messages of the user and the conversation partner.  The morpheme analysis is a process of separating the text of the message into morpheme units, that\nis, dictionary headwords, restoring the original form of the deformed morpheme, and obtaining consecutive morphemes matching a word formation rules from the separated unit morphemes.  Speech act analysis is performed for each unit of speech, and it is a\nprocess of deducing the purpose of a message from the format of the unit of speech.  A speech unit may be a corpus which may end with a sentence final ending and ending signs such as a period, a question mark, an exclamation mark, and the like.  The unit\nof speech is analyzed to a primary speech act which is the speaker's purpose of the communication, an intended subject which is a subject intended to convey to an audience through the message, and the related element which is a specific entity related to\nthe intended subject.  Each speech unit may be analyzed by the speech act analysis module 1441 and tagged in the form of a `[primary speech act]_[intended subject]_[related element]`.  For example, the speech act analysis process for a sentence \"When\nwill be the next meeting?\" will be described as follows.  First, this sentence is divided into plural morphemes by morpheme analysis, and the meaning of each morpheme is grasped.  Next, the speech act analysis is performed for analyzing the format and\nintent of this message.  Since the message begins with an interrogative adverbial \"when\" and ends with the question mark \"?\", the purpose of this sentence (primary speech act) is `question on time` (i.e. request for information on time).  In addition,\nthrough this message, the subject intended by the speaker, that is, the intended subject is a `decision of meeting date`, and the related element which is a concrete object related to the meaning action is `next meeting`.  As another example, the process\nof the speech act analysis of the message \"6 o'clock is not available because the reservation is full\" is as follows.  Through morpheme analysis and analysis of sentences, the purpose of the sentence may be analyzed as `delivery of information` since\nthis sentence is terminated by a period and contains specific information.  The subject of the speaker intended to deliver through the sentence, that is, the intended subject is `rejection`, and the related element (object of rejection) related to the\nrejection is `6 o'clock reservation`.  The speech act analysis module 1441 of the present invention grasps the purpose, the intention of the speaker, and the related elements of a message as a speech act unit through the speech act analysis.\nThe dialog tracking module 1442 performs a function of continuously tracking messages included in a conversation between two or more speakers.  The dialog tracking function trace of morpheme/speech act analysis on an each message based on the\ncontext of conversation between two or more speakers.  For example, the dialog tracking module 1442 may be configured to trace the results of speech act analysis of each message included in the conversation list displayed on the second message management\nuser interface according to one embodiment of the present invention.\nThe service response generation module 1443 generates responses corresponding to the intent and purpose analyzed by the speech act analysis module 1441 and/or the dialog tracking module 1442 and provides the responses for the user.  A response\ngenerated by the service response generation module 1443 of the AI agent 1440 may be provided in association with the additional function process part 1300.\nFIG. 9 is a flowchart illustrating a service processing procedure of the AI agent 1440 according to an embodiment of the present invention.\nFirst, the message monitoring unit 1410 monitors reception and transmission of a text message from the text message management part 1200 (S3100).  The artificial intelligence agent 1440 performs morpheme analysis and speech act analysis on the\nreceived or transmitted text message (S3200), and analyzes the intent of the message using the result of the morpheme analysis and the speech act analysis (S3300).  Morpheme/speech act analysis and message intention analysis is performed by the speech\nact analysis module 1441 and dialog tracking module 1442 of the artificial intelligence agent 1440.  Then, the service response generation module 1443 of the AI agent 1440 searches for a service corresponding to the analyzed message intention (S3400),\ngenerates a service response corresponding to the analyzed message intention, and provides the service response for the user terminal S3500).\nHereinafter, the operation of the AI agent 1440 will be described in detail with an example of a service.\n&lt;Generation of Schedule Information&gt;\nAccording to an embodiment of the present invention, the AI 1440 may be configured to provide a service for analyzing text included in the conversation list to generate schedule information.\nFirst, the speech act analysis module 1441 of the artificial intelligence agent 1440 performs morpheme analysis and speech act analysis on a new incoming/outgoing text message, and determines whether there is a morpheme related to a schedule in\nthe result of the analysis.  A schedule is a list of something to be done at a specific time or period, so it is related to time.  Therefore, it is possible to determine the relevance to a schedule by determining whether there is a time-related morpheme\nin the receiving or originating message.  Identification of the morpheme associated with a schedule can be performed in various ways.  For example, the speech act analysis module 1441 may be configured to identify morpheme having a specific format\nindicating a date and time.  For more specific example, the artificial intelligence agent 1440 may be configured to identify a format of YYYY/MM/DD, YYYY.MM.DD, MM/DD or, MM.DD, YYYY, Monday, today, tomorrow, hh:mm am(pm) and the like, which indicates a\nspecific time.  In addition, since an interrogative sentence may be used for arranging time before the schedule has not be confirmed yet, the AI 1440 maybe configured to identify morphemes such as \"when\", \"which day\" and the question mark \"?\" as\nmorphemes related to a schedule.\nIf it is determined by the morpheme analysis that the message includes the morphemes related to a schedule, the speech act analysis module 1441 of the artificial intelligence agent 1440 performs the analysis of the speech to grasp the intent and\nthe contents of the message.  In other words, the speech act analysis module 1441 performs an analysis of a single incoming/outgoing message to identify specific information related to a schedule included in the message, that is, information such as\nplanned work, place, related persons, and so on.\nIn general, a schedule includes a specific time and a task (action) scheduled at the specific time, as necessary items.  In order to perform the process of generating schedule information according to the present invention, either time or task\nneeds to be specified.  For a message of which a task has been specified but time has not been yet specified, or for a message of which time has been specified but a task has not been specified, subsequent processing for generating schedule information\nis performed.  However, for messages that do not have two items, No subsequent processing for generating schedule information is performed.  A typical example of a schedule-related message that does not specify time is a suggestion such as \"Let's go to a\nmovie.\" A typical example of a schedule-related message that does not specify a task is \"What are you doing today?\".  Additional information that constitutes the schedule may include the space (place) where the task is performed, the other party (or\ncollaborator) related to the task, the end time (period) of the work, and other information.\nThe AI 1440 according to an exemplary embodiment of the present invention analyzes the message and generates schedule information having the following data entries.\nSchedule information: [time]_[task]_[place]_[related person]_[end time of work]_[additional information]\nFor such morpheme analysis and speech act analysis of the artificial intelligence agent 1440, rules for each analysis are required.  Analysis rules are generated based on the dialogue model.  A dialog model is data modeling conversations, i.e.,\nmessages exchanged between two or more speakers.  The server may be configured to collect user's conversation data for establishing a dialog model.  The server may use the message exchange through the message integration management platform according to\nthe present invention as well as use the dialog data outside the platform to construct the dialog model.\nThe dialogue model is largely classified into a rule-based model and a statistical-based model.  Representative rule-based models include a plan-based dialogue model and an active ontology-based dialogue model.  A plan-based dialogue model is a\nway of designing a dialogue model by dividing a work into a domain, problem solving, and detailed plan of discourse so that the work can be easily and flexibly adjusted according to the agenda.  An active ontology-based dialogue model divides work into\nconcepts in ontology and generates a system response structure necessary for service through inference.  Representative statistical based models include an example-based dialogue model and a Partially Observable Markov Decision Process (POMDP).  The\nexample-based dialog model is configured to generate a dialogue example index using the language analysis engines for the dialogue example big data object, and to search the dialogue example index to find the most similar example to the user's speech\nact.  POMDP is a reinforcement learning model that probabilistically models user behavior by learning from past conversation histories.\nThe server generates analysis rules and response rules based on the dialog model.\nThe analysis rules and response rules generated by the server are downloaded and installed in the user terminal.  The server may update the dialog model and analysis rules 1444 and response rules based on the collected data.  In this case, the\nserver may transmit the updated analysis rule 1444 and the response rule to the user terminal, so that the artificial intelligence agent 1440 installed in the user terminal performs the operation with reference to the updated analysis rule 1444 and the\nresponse rule.\nThe artificial intelligence agent 1440 according to the present invention is configured to analyze incoming and outgoing messages according to analysis rules 1444 generated based on one of the above dialog model and to provide a service, as a\nresponse, corresponding to the analysis results.  That is, the artificial intelligence agent 1440 may provide various services through the service response generation module 1443.\nIt is possible to classify the schedule related messages by type and follow the process of determining the schedule in the conversation between two or more speakers according to the message type.  According to an embodiment of the present\ninvention, the artificial intelligence agent 1440 is configured to determine the type of dialogue through speech act analysis and to perform the message analysis routine according to the identified message type of the dialogue to generate the schedule\ninformation.\nAccording to one embodiment of the present invention, the type of conversation for arranging a schedule may be determined according to the first message that triggers the conversation related to the schedule, which may be classified as follows.\nType 1 (notification type): A message that unilaterally specifies time and task, which is the minimum item necessary for schedule establishment.  A schedule specified in the message is established regardless of the recipient's intention.\nType 2 (complete proposal type): A message that proposes to the other party a time and task, which are the minimum items necessary for scheduling A consent of the recipient of the message is required.  The first proposed schedule may be replaced\nby another schedule (alternative) in process of conversation between the two speakers.  In order to fix an alternative as a schedule, the consent of the recipient of the message containing the alternative is required.  Therefore, for the initial proposal\nof Type 2, the follow-up messages should be analyzed to determine whether the other party's consent, whether or not an alternative is presented, and whether there is consent to the alternative.  \"Are you ok with Next meeting on February 18?\" would be an\nexample of this type.\nType 3 (incomplete proposal type): A message containing a proposal that does not specify either time or task.  In case of belonging to this type, it is necessary to specify the non-specified item that in order to generate schedule information. \nTherefore, for the initial proposal of type 3, the follow-up message should be analyzed to determine whether the unspecified item of the initial proposal has been specified.  If the initial proposal is specified through the follow-up messages, the\nspecified proposal has the same properties as the type 2.  \"Let's play golf\" and \"When will the next general meeting be?\" is an example of the Type 3 message.\nThe above message classification is an exemplary classification, but the messages may be classified in a different way based on the collected conversation data and the dialog model.\nFirst, a process of determine a schedule for a first type message, i.e., a notification type message will be described as follows.\nWhen one party notifies that a specific action is scheduled at a specific time, the notified time and action are fixed to a schedule.  The message to notify the schedule is usually in the form of a declarative or imperative sentence.  However,\nmessages delivering schedule information in the form of statements and statements are not necessarily notice of a fixed schedule.  The speech act analysis module 1441 of the artificial intelligence agent 1440 according to an embodiment of the present\ninvention may be configured to refer to the sender information of the message to determine whether the received message is a message for notice of a fixed schedule.  Generally, a fixed schedule notification is often received from a specific opponent. \nFor example, a notice of various due dates delivered by a government office, a notice of maturity and payment due delivered by a financial institution, a reservation confirmation delivered by a travel agency or an airline, and a notice of a scheduled\nshipping time delivered by a shipping company are notifications related to a fixed schedule.  Thus, the speech act analysis module 1441 of the AI 1440 may be configured to analyze the sender information included in the message to determine whether the\nmessage is related to a fixed schedule.  In general, a schedule includes a task and at a specific time as essential items.  Other additional information constituting the schedule may include a space (place) where the task is performed, a partner (or\ncollaborator) related to the task, a termination time (period) of the task, and other information.\nType 2 and Type 3 have different schedule generation paths than Type 1.  That is, when the messages of type 2 and type 3 are transmitted to the other party in the form of a proposal or a question of one of a plurality of speakers, a conversation\nfor establishing a schedule is started by this message.  Hereinafter, the first proposal or question relating to a schedule will be referred to as an `initial proposal`, and the speaker presenting the initial proposal will be referred to as a `first\nspeaker` and the other party referred to as a `second speaker`.  Also, a set of messages exchanged between two speakers is called `conversation`, and a message generated after one message is called `subsequent message`.  Further, a pair of consecutive\nmessages between two speakers is called a `neighbor message pair`.  For example, a message of a first speaker and a message of subsequent response of the second speaker form a neighbor message pair.  In general, the neighbor message pair may be two\nadjacent messages displayed on the second message management user interface, but in some cases, a plurality of messages may form a neighboring message pair.  For example, when a second speaker responds to a plurality of consecutive messages of the first\nspeaker with one message, the entire (logical AND) of the plurality of messages of the first speaker and the messages of the second speaker form a neighbor message pair.\nMessages belonging to type 2 or type 3 have the form of a suggestion or question.  The suggesting sentence is typically terminated by a sentence starting with a syntactic unit \"Let's .about.\", \"Why don't .about.\" \"Would .about.\", and the like. \nA question typically start with an interrogative and terminates with a question mark \"?\".  Questions may be classified into polar questions (yes-no questions) to which answer is yes or no, and non-polar questions (wh-questions) which requires a specific\nanswer accompanied by an interrogatory.\nAlthough time and task are specified in the type 2 message, the schedule contained in the message is confirmed only when the consent of the second speaker is received.  Thus, the AI 1440 tracks subsequent messages to determine whether the second\nspeaker agrees with the schedule proposed by the first speaker.  If there is consent of the second speaker, the schedule proposed by the first speaker is confirmed.  If there is no consent of the second speaker, the artificial intelligence agent 1440\ndetermines whether there is a counter offer for the initial offer of the initial message.  The alternative (counter offer) may also be presented by the first speaker or the second speaker.  Since this alternative proposal also requires the consent of the\nother party who proposed the alternative, the AI 1440 tracks and analyzes the follow-up messages to confirm whether the other party agrees to the alternative presentation.  The presentation of the alternative may be repeated several times between the\nfirst speaker and the second speaker.\nAccording to ordinary message exchange cases, consent or rejection of a proposal may not follow it immediately.  For example, a person who has been offered a schedule may ask after the proposer before approval or rejection.  In addition, after a\nnegative reply, the conversation for rearrangement of a schedule may be continued by the presentation of the first proposer's or the other party's alternative.  Therefore, when a response to the proposal of a schedule does not include an agreement, the\nschedule information generation function does not work correctly if the determination of agreement is performed for only a single message consecutive to the proposal.  Therefore, it is desirable to keep track of the messages exchanged after the initial\nproposal for a considerable period of time to determine whether the consent or alternatives are presented.  However, infinitely continuing message tracking for detection of alternatives and consent for the initial offer may increase the processing load\nof the mobile terminal and exceed the capabilities of the AI 1440, the number of performing the message tracking may be suitably limited by the analysis rule 1444.  For example, the analysis rule 1444 may regulate that the second speaker's consent and\nalternative presentation are to be tracked only for N subsequent messages (where N is a natural number) from the initial proposal.  That is, the AI 1440 stops message tracking and the schedule information generation process if no consent of the second\nspeaker or presentation of an alternative within the n subsequent messages from the initial proposal.  If a consent message of the second speaker is detected within the subsequent message from the initial proposal, the artificial intelligence agent 1440\nconfirms the schedule information.  Also, if there is an alternative presentation by the first speaker or the second speaker, the message tracking is continued from the presentation of the alternative, even if there is no agreement of the second speaker\nwithin the subsequent message.  In this case, the same restriction as the initial proposal is applied to the presentation of alternatives, that is, the AI agent monitors whether a consent or other alternatives are presented within N subsequent messages\nafter the presentation of the alternative.  N is determined on the basis of a dialogue model.  In a typical conversation, a consent on a schedule or presentation of an alternative is made within 1-5 consecutive messages, so N may be determined about 5.\nA message of type 3 cannot confirm the schedule even with the consent of the other party.  This is because the initial proposal of this type is a simple proposal that does not specify a date and time.  In order for specific schedule information\nto be generated by the initial proposal of type 3, the time must be specified by the first speaker and the second speaker.  Thus, if a suggestion of type 3 is presented, the AI 1440 tracks the subsequent messages to determine if the schedule is\nmaterialized.  The materialization of the schedule is made by proposal of a specific time and agreement to the proposal of the time between the first speaker and the second speaker.  In other words, the AI 1440 tracks and analyzes the messages following\nthe initial proposal and determines whether the first speaker or the second speaker suggests a specific date and time.  The process of tracking and analyzing the proposal and the consent of the other party is carried out in the same manner as tracking\nand analyzing the first proposal (or alternative) and agreement for the type 2 message.\nThe artificial intelligence agent 1440 may generate and use schedule information even when the dialog including the type 2 and type 3 messages is tracked and analyzed.  For example, according to an embodiment of the present invention, the AI\n1440 may be configured to generate and temporarily store schedule information for an initial proposal, and to update the temporarily stored schedule information whenever an alternative is presented.  The updated schedule information is temporarily stored\nin the memory 1500 of the user terminal until the other party agrees.  At this time, temporary storage of the schedule information is maintained only within the message tracking limit described above.  For example, if no consent or alternative is\npresented within the N subsequent messages from the initial proposal, the temporarily stored schedule information is deleted in memory 1500 and the tracking is stopped.\nFIG. 10 is a flowchart illustrating a process of generating schedule information of the AI agent 1440 according to an embodiment of the present invention.\nThe artificial intelligence agent 1440 receives the text message received from the mobile terminal or transmitted through the mobile terminal (S5110).  The incoming and outgoing text messages may be transmitted to the artificial intelligence\nagent through the text message management part 1200 or the message monitoring unit 1410 of the integrated message management part 1400.\nThe speech act analysis module 1441 of the artificial intelligence agent 1440 performs morpheme analysis and speech act analysis on the received or transmitted text message (S5120), and determines whether the message includes necessary items for\ngenerating schedule information (S5130).  Whether or not the necessary items for generating the schedule information are included is judged based on presence of a morpheme having a temporal meaning (time related morpheme), of the relationship between the\nmorpheme representing time (time related morpheme) and presence or absence of a morpheme representing an action or a state (task related morpheme), and a relationship between the two morphemes.  Specifically, the speech act analysis module 1441 of the\nartificial intelligence agent 1440 divides the texts of the received or outgoing text messages by morphemes (morpheme analysis), analyzes the meaning of each morphemes, the relationship between the morphemes, and the structure of the sentences to\ninterpret the intent of the message (speech act analysis).  That is, the artificial intelligence agent 1440 judges whether a time-related morpheme and a work-related morpheme present in the message through the morpheme analysis and the speech act\nanalysis as mentioned above, and determines whether the intent of the message is to establish a schedule.\nIf it is determined that the message contains a necessary item for generating schedule information, the service response generation module 1443 generates schedule information (S5140).  The generated schedule information is temporarily stored in\nthe memory 1500.\nIf it is determined that the message includes none of the necessary items, the service response generation module 1443 ends (END) the process for generating schedule information.\nFollowing the morpheme/speech act analysis of the message by the speech act analysis module 1441, the artificial intelligence agent 1440 determines whether the message belongs to a type 1, i.e., a notification type message for notifying a\nspecific task at a particular time (S5150).\nIf the message is determined to be a type 1 message, the artificial intelligence agent 1440 executes the schedule management application and registers the generated schedule information (S5160).\nThe schedule information registration process may be performed in various ways.  According to an embodiment of the present invention, the service response generation module 1443 may be configured to automatically execute the schedule management\napplication without the user's intervention, write the generated schedule information into the schedule management application, and register the schedule information.  At this time, the service response generation module 1443 may be configured to store a\nmessage related to schedule information or entire conversation in association with the generated schedule information.  For example, the service response generation module 1443 may be configured to insert, into the schedule information, a link to access\na final agreement message or the conversation containing the message.  The user can view the schedule information through the schedule management application.  At this time, the user may execute the link to check the message or conversation related to\nthe schedule.\nAccording to another embodiment of the present invention, the service response generation module 1443 can be configured to check whether the user approves the registration of the generated schedule information.  For example, the service response\ngeneration module 1443 transmits the generated schedule information to the user interface managing unit 1430, and requests the generation of the schedule information registration graphical user interface thorough which a user can confirm the\napproval/rejection of the user for the schedule information.  At this time, the service response generation module 1443 may be configured to have a speech generation function of constructing the generated schedule information as a sentence and providing\nit to the user.  When the approval input of the user is received through the schedule information registration graphical user interface, the service response generation module 1443 executes schedule management application and registers the generated\nschedule information.\nIf it is determined that the incoming/outgoing message is not a type 1 of notification message, the AI agent determines that the message is a Type 2 of complete proposal message (S5210).  If it is determined which the incoming/outgoing message\ncorresponds to a type 2 message, the AI agent 1440 tracks and analyzes the subsequent message.  At this time, as described above, the message counter is initialized to limit the execution length of the trace analysis routine for the subsequent messages\nto N messages (N is a natural number).  Therefore, the value i of the message counter is set to the initial value 1 (S5220).\nThe artificial intelligence agent 1440 performs the morpheme analysis and the speech act analysis on the adjacent subsequent message (i=1) (S5230).  That is, the speech act analysis module 1441 of the artificial intelligence agent 1440 performs\nmorpheme analysis and speech act analysis on the message immediately following the initial proposal.\nThe artificial intelligence agent 1440 determines whether the neighboring subsequent message (i=1) agrees with the initial proposal using the morpheme/speech act analysis result (S5240).  If it is determined that the neighboring response message\n(i=1) is consent to the schedule proposed by the first speaker, the service response generation module 1443 performs a process for registering the schedule information (S5160).  The processing for the schedule information registration is performed in the\nsame manner as the case of the type 1 message described above.\nIf it is determined that the response message is not a consent to the proposed schedule, the AI 1440 determines whether an alternative is presented in this message (i=1) (S5250), through the morpheme/phonetic analysis for the subsequent message\n(i=1).  Since the presentation of the alternative is another proposal, it may be determined whether the message (i=1) is an alternative presentation by the same method as the morpheme/speech act analysis method for the initial proposal.\nIf it is determined that the message (i=1) is an alternative presentation, the artificial intelligence agent 1440 updates the schedule information using the items included in the alternative and temporarily stores the updated schedule\ninformation in the memory 1500 (S5260).\nSince the alternative is a new proposal, the confirmation of the schedule requires the other's agreement on the alternative.  Therefore, the AI 1440 should track and analyze the message following the alternative message (i=1).  At this time, it\nis preferable to apply the same restriction of the tracking routine for the initial proposal to the alternative consent tracking routine, so the message counter is initialized (S5230).  That is, by the alternative presentation message, the message\ncounter value is initialized to i=1, and the number of performance of tracking messages subsequent to the alternative message is limited to N.\nIf the message (i=1) does not correspond to the alternative presentation, the AI 1440 increments the message counter by 1 (S5270) to analyze the next message, and if the increased message counter value exceeds the maximum allowed routine repeat\nIt is determined whether the number N is exceeded (S5280).  If the increased message counter value does not exceed N, the AI agent 1440 repeats the above-described routine of S5230 to S5280 for the subsequent message (i=2).  The execution of the routine\nof S5230 to S5280 is stopped when the message counter value exceeds the maximum number of routine repetitions N, and the AI agent 1440 ends the constant information generation processing.\nIf the message does not correspond to type 2, the artificial intelligence agent 1440 determines whether the message corresponds to type 3 using the morpheme/speech act analysis result of the message (S5310).\nIf it is determined that the message does not correspond to type 3, the AI 1440 ends the process of generating the predetermined information.\nThe type 3 message needs to specify the essential items by the subsequent messages for schedule confirmation.  Accordingly, if it is determined that the incoming/outgoing message corresponds to the type 3 message, the AI agent 1440 tracks and\nanalyzes the subsequent message.  In this case, as described above, the AI agent 1440 operates the message counter to limit the execution length of the trace analysis routine of the subsequent message to M messages (M is a natural number).  At this time,\nthe value j of the message counter is set to the initial value 1 (S5320).\nThe artificial intelligence agent 1440 performs morpheme analysis and speech act analysis on the adjacent message (j=1).  That is, the artificial intelligence agent 1440 performs morpheme analysis and speech act analysis on the message\nimmediately following the initial proposal (S5330).\nThen, the artificial intelligence agent 1440 determines whether the adjacent subsequent message (j=1) is a message specifying the schedule proposed by the first speaker, using the morpheme analysis and speech act analysis results (S5340).\nIf it is determined that the neighbor subsequent message (j=1) is a message specifying the first proposal, the AI 1440 updates the schedule information using the items included in the neighbor subsequent message (S5350).  A message materializing\nthe incomplete proposal belongs to type 1 or type 2.  Therefore, the artificial intelligent agent determines whether the neighbor subsequent message (j=1) belongs to type 1 using the morpheme/speech act analysis result of the adjacent subsequent message\n(j=1) (S5150), and registering the schedule information if the neighbor subsequent message is type 1 message (S5160).  If the neighbor subsequent message (j=1) does not belong to type 1, it belongs to type 2, and thus the AI agent 1440 performs the\nprocesses of S5220 to S5280 described above.\nIf it is determined that the neighbor subsequent message (j=1) is not a message specifying the first proposal, the AI 1440 increments the message counter value by 1 (S5360), and if the increased message counter value is the maximum number of\nroutine repetitions M is exceeded (S5370).  If the increased message counter value exceeds M, the AI 1440 stops the message tracking and the schedule generation process.  If the increased message counter value does not exceed M, the process of S5330 to\nS5370 is repeated for the next message.\nHereinafter, a type 1 message analysis and schedule information generation operation according to an exemplary embodiment of the present invention will be described in detail with reference to the following received text message.\n&lt;Exemplary Received Text Message&gt;\n\"From Seoul City,\nThe deadline for the local tax payment of Hong Gil-dong is Mar.  30, 2017.  For more information, please visit www.tax.seolul.go\".\nFirst, the speech act analysis module 1441 divides the text included in the received text message into morphemes, which are the smallest units having meaning, and analyzes the usage of each morpheme.  This received text message may be\nmorpheme-analyzed as follows.\n&lt;Morpheme Analysis Result&gt;\nThe (definite article)-deadline (noun)-for (preposition)-local tax (noun+noun)-payment (noun)/of (preposition)-Hong Gil-dong (proper name)-is (verb)-Mar.  30, 2017 (proper noun+numeral+numeral, date).\nThe artificial intelligence agent 1440 recognizes that the morpheme continuum \"Mar.  30, 2017\" indicating the specific time is included in the received text message through the speech act analysis module 1441.  Then, the speech act analysis\nmodule 1441 analyzes the relationship of each morpheme, that is, the structure of the sentence, to determine what the date is related to.  The speech act analysis module 1441 determines that the message is a declarative sentence based on the relationship\nbetween the date and the verb \"is\", and that the purpose of the message (primary speech act) is the delivery (i.e., notification) of the information on date.  Subsequently, the speech act analysis module 1441 recognizes that the subject of this sentence\nis deadline of local tax payment based on connection between \"deadline of local tax payment\" and \"is\".  Therefore, the speech act analysis module 1441 interprets \"Mar.  30, 2017\" and \"deadline of local tax payment\" is time and task, respectively for a\nschedule, and then generates schedule information as follows.  In addition, `Seoul City`, a sender of the message, is added to the related item, and the hyperlink providing the related information is added to the additional information item,\nrespectively.\n&lt;Schedule Information Data&gt;\n[2017.3.30]_[deadline of local tax payment]_[N/A]_[Seoul City]_[www.tax.seolul.go]\nAccording to the analysis, since this message corresponds to the type 1 for a certain notification purpose, the service response generation module 1443 performs a process of registering the generated schedule information.\nThe schedule information registration process is performed through the above-described process.\nAs described above, when the schedule cannot be determined by the morpheme analysis and the speech act analysis, that is, for the type 2 or type 3 message, the context of the conversation should be analyzed by the dialog analysis module in order\nto generate the schedule information.\nHereinafter, an operation of message analysis and schedule information generation for the type 3 messages according to an embodiment of the present invention will be described in detail with reference to FIG. 11.\nFIG. 11 is a screen shot showing a second message management user interface 200 displaying a dialog between the user of the mobile terminal and a friend of the user.\nFirst, the artificial intelligence agent 1440 performs morpheme analysis on the received message A1 through the speech act analysis module 1441, and detects a morpheme when the message A1 refers to an unspecified time.  Next, the speech act\nanalysis module performs speech act analysis for the message A1.  The speech act analysis module recognizes that the message A1 suggests playing \"golf\" without specifying the time, based on the entire sentence structure and the connection or relationship\nbetween each morpheme.  More specifically, since the morphemes \"would you\" contained in the message A1 is a typical sentence start indicating an ask or suggestion, the speech act analysis module determines the purpose of the message, that is, the primary\nspeech act is a suggestion, and that object of the suggestion is playing golf together.  The `playing golf together` which is the object of the suggestion, but the message A1 does not specify time.  Accordingly, the AI agent 1440 recognizes that the\nmessage A1 is a message corresponding to type 3, and that the user intends to propose golf without specifying the time.  Therefore, the AI agent tracks and analyzes the message following the message A1.  At this time, the artificial intelligence agent\n1440 initializes the message counter and counts the message.  That is, the artificial intelligence agent 1440 performs morpheme analysis and transcriptional analysis on the message B1 (j=1) following the message A1.  Meanwhile, the artificial\nintelligence agent 1440 generates schedule information and temporarily stores the generated schedule information in the memory 1500 of the user terminal.  The data structure of the generated schedule information is as follows.\n[N/A]_[Golf]_[N/A]_[Friends 1]_[N/A]_[N/A]\nMessage B1 contains two phrase, \"sounds good\" and \"how about next week\".  The speech act analysis module 1441 performs morpheme analysis on two sentences, and performs speech act analysis based on sentence structure and relationship between the\nanalyzed morphemes.  The speech act analysis module regards the message B1 as an agreement to Friend 1's proposal from the first sentence, \"sounds good\" In addition, the speech act analysis determines that the message B1 has a purpose of materializing\nthe initial proposal of Friend 1, which does not specify `when`, based on the contextual connection between the morphemes \"how about\" and \"next week\".  However, the morphemes \"next week\" contained in message B1 is not perfect to specify the starting time\nof the planned task `golf`.  Thus, the AI agent 1440 increments the message counter value by one and then compares this value with 5, which is the maximum allowable number of repeating the routine.  Since the increased message counter value is 2, it is\nsmaller than the maximum allowable number 5, so that dialog tracking and the message analysis are continued for the next message (j=2).  Meanwhile, since the starting point of the planned task `golf` may not be specified, the schedule information\ngenerated and temporarily stored by the initial proposal is not updated.\nArtificial intelligence agent 1440 performs morpheme analysis and speech act analysis on the following message A2.  The message A2 contains a morpheme \"OK\" expressing positive intent, a preposition \"except\" meaning exclusion, nouns \"Wed\" and\n\"Tue\", which specifies a day of the `next week`.  The speech act analysis module of the artificial intelligence agent 1440 recognizes, from the meaning of these morphemes and their connection, that message A2 is intended to convey partial agreement on\nmessage B1.  However, since no concrete time is provided by this agreement, the AI 1440 increments the message counter value by 1 and then compares this value with the maximum allowable number of routine iterations 5.  Since the increased message counter\nvalue is 3 smaller than the maximum allowable number 5, the trace and analysis for the next subsequent message B2 (j=3) is continued.\nArtificial intelligence agent 1440 performs morpheme analysis and speech act analysis on the following message B2 (j=3).  Message B2 is a morpheme continuum containing \"how about\", which express a suggestion, \"7:30 am\", which represent time,\n\"Friday\", which represent a day of a week, \"Korea Country Club\", which is a name of a place.  The speech act analysis module determines, from the meaning of each morpheme in the message B2 and its connection relationship, that the message B2 proposes a\nspecific date and time and place, and asks whether the other party agrees.  Since the message B2 includes the specific time and place, it is necessary to update the temporarily stored schedule information.  Accordingly, the artificial intelligence agent\n1440 updates the temporarily stored schedule information as follows.\n[2017.4.7, 7:30]_[Golf]_[Korea Country Club]_[Friends 1]_[ ]\nSince the message B2 corresponds to a proposal of a schedule belonging to type 2, it is necessary to determine whether or not the other party agrees with the message.  Therefore, the AI 1440 traces following messages and performs morpheme\nanalysis and transcription analysis.\nThe AI 1440 initializes the message counter prior to the tracking analysis of the subsequent message following the message B2.  Then, the AI 1440 performs morpheme analysis and speech act analysis on the subsequent message A3 (i=1) following the\nmessage B2 through the speech act analysis module 1441.  Message A3 is a morpheme continuum containing \"too early\", which implies impermissibility, so the speech act analysis determines that the purpose of the message A3 is notification of refusal of the\nproposed time 7:30 am.  In other words, it is determined that the message A3 is neither an agreement on the proposal of the message B2 nor an alternative presentation.  Thus, the AI 1440 increments the message counter value by one and compares it with\nthe maximum iteration count of five.  Since the increased message counter value 2 is less than the maximum number of iterations 5, the AI agent 1440 performs the morpheme analysis and the speech act analysis on the message B3 (i=2) following the message\nA3.  This is because, as described above, an alternative may be proposed by the user of the mobile terminal or the friend 1 even after the rejection of the message A3.  At this time, since the message A3 has no contents to change the items included in\nthe temporarily stored schedule information, the schedule information is not updated.\nMessage B3 is a morpheme continuum containing \"then\", which is adverb meaning addition, \"12:15 pm\", which indicates a specific time, and question mark \"?\".  The speech act analysis module 1441 determines, based on the arrangement of the\nmorphemes, that the message B3 corresponds to the presentation of a new alternative, and that the new alternative is `12:15 pm (next Friday)`.  That is, the artificial intelligence agent 1440 determines that the message B3 is to propose a new alternative\nas a response to the message A3.\nMeanwhile, since message B3 includes `12:15 pm (next Friday)` which is an alternative time proposed in message B2, the temporarily stored schedule information should be updated.  The artificial intelligence agent 1440 updates the temporarily\nstored schedule information as follows.\n[2017.4.7, 12:15]_[Golf]_[Korea Country Club]_[Friends 1]_[ ]\nThe artificial intelligence agent 1440 holds confirmation of the schedule and continues tracing messages and morpheme/speech act analysis on the following message until the response of the friend 1 to the new proposal is detected.\nAt this time, since the routine for tracking consent regarding the alternative is newly started, the artificial intelligence agent 1440 initializes the message counter and performs morpheme analysis on the message A4 (i=1) following the message\nB3.  The message A4 contains the positive morpheme \"perfect\" and exclamation mart \"!!!\".  The speech act analysis module recognizes that the message A4 is for conveying consent to the preceding message B3 based on the meaning of the morphemes included in\nthe message A4 and the connection relationship.  Therefore, the artificial intelligence agent 1440 interprets that the date and time proposed by the message B3, i.e., `12:15 PM next Friday` is confirmed, and performs processing for registering the\ntemporarily stored schedule information.\nThe schedule information registration process is performed through the above-described process.\nFIG. 12 is a screen shot showing an example of a schedule information registration graphical user interface capable of receiving a user input for approval and rejection of registering schedule information.\nReferring to FIG. 12, a schedule information registration graphical user interface 240 is provided on the second message management user interface 200.  The schedule information registration graphical user interface includes a schedule\ninformation creation notification window 241, a registration button 242, and a cancel button 243.\nThe service response generation module 1443 of the artificial intelligence agent 1440 generates a sentence \"An appointment of playing golf with friend 1 at Korea country club at 12:15 pm April 7 is made.  Do you register the schedule?\" using the\ngenerated schedule information, transmits the sentence to the user interface managing unit 1430, and requests for generation of the schedule information registration graphical user interface.\nThe user interface managing unit 1430 inserts the sentence received from the AI 1440 into the schedule information creation notification window 241 and displays the schedule information registration graphical user interface 240 including the\nregistration button 242 and the cancel button 243 on the second message management user interface 200.\nAs described above, the artificial intelligence agent 1440 according to the present invention is configured to track a series of conversations and perform morpheme analysis and speech act analysis on each message constituting the conversation,\nthereby grasping specific contents.\nMeanwhile, according to an embodiment of the present invention, the AI 1440 may be configured to determine whether a place presents in the confirmed schedule information, and to provide information about a place if no place is presented.  For\nexample, in case that a schedule is confirmed through the processes of FIG. 10, the service response generation module 1443 of the artificial intelligence agent 1440 inquires the confirmed schedule information to determine whether the confirmed schedule\nincludes place item, and, if the confirmed schedule information has no place, searches for adequate places related to the task and provide the search result.  At this time, the service response generation module of the artificial intelligence agent 1440\nmay be configured to search for a place using the location information of the user.  For example, the service response generation module may be configured to receive the location information of the user and retrieve the relevant places at a predetermined\ndistance from the location of the user.  Also, the service response generation module may search the places by analyzing the contents of the messages tracked and analyzed in the schedule information determination process.\n&lt;Event Panel&gt;\nAccording to an exemplary embodiment of the present invention, the integrated message management part 1400 may be further configured to visualize the schedule information, which is created by the AI agent 1440, and to provide the visualized\nschedule information through the first and second message management user interface 100 and 200.\nAccording to an exemplary embodiment of the present invention, first message management user interface 100 may further include a first event panel 150 for displaying user schedule information created by the AI agent 1440.\nFIG. 13 is an exemplary screenshot of a first message management user interface 100 where a first event panel is displayed, and FIG. 14 is a flowchart illustrating processes for providing a first event panel according to an exemplary embodiment\nof the present invention.\nReferring to FIG. 13, the first message management user interface 100 may include a first event panel 150 that displays information related to a user's schedule as a graphic object, such as a form of a card.  The first event panel 150 is\nprovided in the upper area of the first message management user interface 100, and the first event panel 150 includes a plurality of schedule cards 151, each of which displays information on a schedule.\nIn order to provide the first event panel 150, the integrated message management part 1400 may be configured to receive the user schedule information from the AI agent 1440, and to reconstruct the received schedule information into a\npredetermined format.  For example, the schedule information of a user may include entries, such as a start date and time, an end date and time of a scheduled task, a place, a related person, and description of the schedule.  The integrated message\nmanagement part 1400 may be configured to generate summary information only for some of these entries.\nReferring to FIG. 14, in step S6100, the user interface managing unit 1430 receives schedule information created by the AI agent 1440.  The schedule information receiving process of step S6100 of the user interface managing unit 1430 may be\nperformed at the time when the first message management user interface 100 is activated or in real-time (e.g., immediately after schedule information is generated by the AI agent 1440).\nIf an update of the schedule information is received in the schedule information update inquiry step S6100, in step S6200, the message information managing unit 1420 retrieves the received schedule information, and in step S6300, edits the\nreceived schedule information according to the schedule card information format.  The schedule card information format defines entries to be displayed on a schedule card and notation of entries.  For example, the schedule card information format\nspecifies the date in the form of \"MM.DD.\", time in the form of \"hh:mm AM (or PM)\", and headline in the form of full text of the title of the schedule, as display entries.  In this case, the message information managing unit 1420 extracts only the\nentries defined by the schedule card information format from the schedule information, and edits the entries according to the display format defined in the schedule card information format to generate the schedule card information.\nIn the above description, the generation of the schedule card information is performed by the message information managing unit 1420.  However, the inventive concepts are not limited thereto, and all or a part of such processes may be performed\nby the user interface managing unit 1430.\nThen, in step S6400, the user interface managing unit 1430 visualizes the schedule card information according to the schedule card configuration rule to generate a schedule card.  In step S6500, the user interface managing unit 1430 arranges and\ndisplays the created schedule cards on the first event panel 150 according to a schedule card configuration rule.  The schedule card configuration rule regulates display form of the schedule card information.  For example, the schedule card configuration\nrules may be defined as follows: 1) the schedule card information is displayed on a card-shaped image divided into a small block at the top and a large block at the bottom; 2) the date is displayed in a small block, and time and title are placed in the\nlarge block; 3) the scheduled time is displayed using a large font; and 4) the title is displayed using a small font under the time.  The user interface managing unit 1430 processes and visualizes each entry of schedule card information according to the\nconfiguration rule as described above, and displays the result on the first event panel 150 of the first message management user interface 100.\nThe schedule card may include a link for access to details of the schedule information.  In response to a user input, such as touching a certain schedule card, the user interface managing unit 1430 accesses a schedule management application to\nretrieve detailed schedule information corresponding to the selected schedule card, and displays the detailed schedule information on the mobile terminal 10.\nReferring to FIG. 13, in the first event panel 150, a schedule card 151 generated according to a schedule card information format and a schedule card configuration rule is displayed.  In the example of FIG. 13, according to the above-exemplified\nschedule card information format and the schedule card configuration rule, the date \"April 4\" is inserted into the upper small block 151a, the time \"12:15 PM\" is displayed in the lower large block 151b using large fonts, and the title \"playing golf\" is\ndisplayed under the time \"12:15 PM\" in the lower large block using small fonts.\nIn step S6600, the schedule card information generated by the message information managing unit 1420 may be stored in the memory 1500.  The process of storing the schedule card information at step S6600 may be performed simultaneously with the\nschedule card generating process S6400 or the displaying process S5500 of the schedule card, or independently.\nThe schedule card information format and the schedule card configuration rule may be changed according to the user setting.  That is, the user can change at least one of the entries and the display format of the schedule information card.  The\nprocesses for reconstructing the schedule card according to the change of schedule card information format and the schedule card configuration rule may be performed in a substantially similar manner to the above described reconstruction process for the\nmessage block 111 and 112 according to changing the integrated message block format.\nThe plurality of schedule cards 151 displayed on the first event panel 150 may be arranged based on time included in the schedule information.  That is, the user interface managing unit 1430 may arrange the schedule card 151 so that the most\nup-to-date schedule is placed at the front-most of the first event panel 150.  The first event panel 150 may be configured to explore schedule cards 151 in response to a user input, such as sweeping or scrolling the currently displayed schedule cards\n151.\nThe schedule card 151 may be generated so as to be identifiable from each other based on the attributes of the schedule.  For example, the schedule information may include information indicating the importance of the schedule.  In this case, the\nuser interface managing unit 1430 may be configured to set the color of the card or the color of the characters differently based on the degree of importance.  Such a configuration may be defined in a schedule card configuration rule.\nMeanwhile, the second message management user interface 200 may include a second event panel 250 displaying schedule information created by the AI agent 1440.\nFIG. 15 is an exemplary screenshot of a second message management user interface 200 where a second event panel is displayed, and FIG. 16 is a flowchart illustrating processes for providing a second event panel according to an exemplary\nembodiment of the present invention.\nIn the embodiment of FIG. 15, the second event panel 250 may be configured to display only the earliest arriving schedule among the schedule associated with the counterpart of the second message management user interface 200.\nReferring to FIG. 16, in step S7100, the user interface managing unit 1430 receives schedule information that is created by the AI agent 1440 in associated with the counterpart of the second message management user interface 200.  Here, the\ncounterpart refers to the other party of the conversations (voice calls and text messages) displayed on the second message management user interface 200.  The schedule information associated with the counterpart refers to schedule information created by\nthe AI agent 1440 based on conversations with the counterpart.  The schedule information receiving step S7100 may be performed at the activation of the second message management user interface 200 or may be performed in real-time (e.g., immediately after\nschedule information is generated by AI agent 1440).\nIn step S7200, the message information managing unit 1420 retrieves the earliest coming schedule from the received schedules, and in step S7300, generates second event panel information according to the second event panel information format. \nThe second event panel information format defines entries to be displayed on a second event panel 250 and notation of entries.  For example, in the second event panel information format, a scheduled date in the form of \"MM.  DD\", scheduled time in the\nform of \"hh:mm AM (or PM)\", a headline in the form of full text or title of the schedule, and remaining time until scheduled time in the form of \"n days later\", \"n hours later\", or \"n minutes later\" are specified as display entries.  In this case, the\nmessage information managing unit 1420 extracts only the entry defined by the second event panel information format from the schedule information, and edits it according to the second event panel information format to generate the second event panel\ninformation.\nIn the above description, the generation of the second event panel information is described as being performed by the message information managing unit 1420.  However, the inventive concepts are not limited thereto, and all or a part of such\nprocesses may be performed by the user interface managing unit 1430.\nThen, in step S7400, the user interface managing unit 1430 visualizes the second event panel information according to a predetermined rule to generate a second event panel 250 including the visualized second event panel information, and in step\nS5520, displays the second event panel 250 on the message management user interface 200.  The second event panel configuration rule may define display form of the second event panel information generated by the message information managing unit 1420. \nFor example, the second event panel configuration rule may define whether: 1) the second event panel information is displayed on a card-shaped image (schedule card) divided into a small block and a large block in the horizontal direction; 2) the\nscheduled date is displayed in the left small block, and the scheduled time, and the headline and the remaining time are displayed on the right large block; 3) the scheduled time is displayed using large fonts; 4) headline is displayed under the\nscheduled time using small fonts; and 5) the remaining time is inserted into an elliptical image located at the upper right of the large block.  The user interface managing unit 1430 processes and visualizes each entry of the second event panel\ninformation according to the above-described configuration rule, and displays the generated second event panel on the second message management user interface 200.\nReferring back to FIG. 15, in the second message management user interface 200, a second event panel 250 generated according to the above-described second event panel information format and the second event panel configuration rule is displayed.\nThe second event panel 250 displays schedule card created according to the second event panel information format and the second event panel configuration rule.  In the example of FIG. 15, according to the above-described second event panel\ninformation format and second event panel configuration rule, the scheduled date \"April 07\" is inserted into the left small block 251, the scheduled time \"12:15 PM\" 252a is displayed on the left side of the right large block 252 using large fonts, the\nheadline entry \"Playing Golf\" 252b is placed below the scheduled time 252a in the large block 252, and the remaining time \"9 days later\" 252c is displayed into an elliptical image at the upper right of the large block 252.\nIn step S7600, the second event panel information generated by the message information managing unit 1420 or the second event panel generated by the user interface managing unit 1430 may be stored in the memory 1500 in association with the\ncounterpart of the second message management user interface 200.  The process of storing the second event panel information at step S7600 may be performed at the same time as the generation process of the second event panel S7400 or the display process\nof the second event panel S7500, or independently.\nThe user may select a schedule card displayed on the event panel through a user input (for example, touch input) to see detailed information of the corresponding schedule, or to perform additional operations, such as modification or deletion. \nIn other words, the interface managing unit may be configured to receive the user selection of a schedule card, and activate a schedule information management application for providing detailed information on the selected schedule.\nThe user can transmit the schedule card displayed in the above-described event panel to other service subscribers.  That is, the user can select one of the schedule card provided in the event panel, and transmit the selected card to a mobile\nterminal 10 of another user subscribed to the integrated message management service.\n&lt;Authentication Code Copying&gt;\nIn online financial transactions and electronic commerce, a one-time authentication code is often used as a means of identity verification, which is generated by a certification authority and transmitted to a user in a form of a text message. \nConventionally, when a text message including an authentication code is received from a certification authority, the user opens a text message, memorizes the authentication code, and input the code manually.\nMeanwhile, the integrated message management part 1400 may be configured to provide a user interface that automatically copies the authentication code from the received message containing the authentication code sent from the certification\nauthority.\nHereinafter, the configuration and operation of the AI agent 1440 and user interface managing unit 1430 associated with the authentication code copy interface will be described in detail.\nFIG. 16 is a flowchart showing processes for generating an authentication code copy interface, and FIG. 17 is a screenshot showing an example of the authentication code copy interface.\nReferring to FIG. 16, first, the AI agent 1440 receives a text message S8100.  The speech act analysis module 1441 of the AI agent 1440 performs morpheme analysis and speech act analysis on the received message S8200.  Then, the AI agent 1440\ndetermines whether the received message has a purpose of delivery of an authentication code S8300.  For example, the user interface managing unit 1430 may be configured to determine whether a received message includes words, such as \"authentication\nnumber (code)\", \"personal identification number (code)\", and \"password\" and provide a user interface for copying the authentication code.\nWhen the received message is determined as a message for delivery of an authentication code, the AI agent 1440 extracts the authentication code from the message S8400.\nSince the authentication code is usually composed of random numeric strings or character strings, the user interface managing unit 1430 may be configured to find and extract random numeric strings or strings from among the text messages.\nThen, the AI agent 1440 transmits to the user interface managing unit 1430 the extracted authentication code with a request for generating an authentication code copy user interface S8500.\nIn step S8600, the user interface managing unit 1430 create an authentication code copy interface, inserts the received character string into the authentication code copy interface, and displays the authentication code copy interface on the\nmobile terminal 10.\nFIG. 18 is a screenshot showing an example of the authentication code copy interface.\nReferring to FIG. 18, the received authentication code message is displayed on a card-shaped authentication code copy interface 700.  The authentication code copy interface 700 may be displayed on the mobile terminal 10 without activating the\ntext message application.  At this time, the above-described steps S8100 to S8600 are performed upon receipt of the text message.  The authentication number copy interface 700 includes a text message display window 710, an authentication code display\nwindow 720, and a copy GUI 730.  The text message display window 710 displays the sender and the contents of the text message.  The authentication code display window 720 displays the authentication code extracted from the text message.  The copy GUI 730\nmay be used to receive a user input of copying the authentication code displayed in the authentication code display window 720.  The user can simply copy the authentication code through a user input (e.g., touching the copy GUI 730).  The \"close\" button\nand \"read\" button are provided at the bottom of the copy GUI 730.  When the user selects \"close\", the card then disappears.  When the user selects \"read\", the user interface managing unit 1430 accesses the text message information managing unit 1420, and\ndisplays the original received message on the mobile terminal 10.  In addition, the user interface managing unit 1430 may be configured to automatically close the card when the user selects the copy GUI 730 to complete copying the authentication code. \nIn FIG. 17, \"307346\" included in the text message is extracted as the authentication code.\nIn this manner, the user can copy the authentication code using the authentication code copy interface 700 and paste it into a desired place.  Therefore, the user convenience is much improved compared to the conventional method, which requires\ntroublesome user actions such as reading a text message, searching for a necessary authentication code, and memorizing the code to manually input the code into a desired place.\n&lt;Generating an Absence Response Message&gt;\nIf the user fails to answer an incoming voice call, a message indicating that the user is absent is sent to the calling party.  Such conventional absence response message may be sent by a mobile communication carrier or directly from a user's\nmobile terminal.  However, since such an absence response message repeatedly uses a predetermined phrase, the intimacy with the calling party is not considered at all.\nAccording to one embodiment of the present invention, upon failure of answering an incoming voice call, the AI agent 1440 may be configured to selects tone of messages based on the conversation history analysis with the calling party.\nWhen failure to answer an incoming voice call is detected, the AI agent 1440 receives the voice call information of the missed voice call from the voice call management part 1100.\nThe artificial intelligence agent 1440 identifies the calling party from the received voice call information, and analyzes the conversation history between the identified calling party and the user to determine the level of tone used for the\nresponse.  The determination of the tone level may be performed based on the morpheme analysis result of the messages sent by the user to the calling party.  Especially in case of Korean, the sentence ending of the messages are an important criterion for\ndetermining the level of respect.\nFor example, if a voice call is received from `buddy 1` who is the conversation partner in FIG. 11 but does not respond, the AI 1440 receives information about the missed incoming voice call from the voice call management part 1100.  The AI\nagent 1440 recognizes that the sender of the incoming voice call is `friend 1` based on the received voice call information.  The artificial intelligence agent 1440 searches and analyzes the conversation history with `friend 1` to create a response\nmessage for the missed voice call.  As shown in FIG. 11, since any honorific titles is not used at all in the messages sent by the user to `Friend 1` and tone is usually informal, the artificial intelligence agent 1440 generates and transmits a response\nmessage without honorific titles, for example, \"call later\".\nThe response message is generated through the speech act generation function of the service response generation module 1443.  The response message is created based on the conversation data collected by the server.  That is, the service response\ngeneration module 1443 may combine tagged corpus, which is the result of analyzing the collected conversation data, to generate a response to the missed call.\nOne embodiment of the present invention may provide a computer-readable storage medium including instructions for a processor mounted on a mobile terminal 10 to perform the processes described above.  As described above, the integrated message\nmanagement part 1400 according to the present invention may be combination of a processor embedded in the mobile terminal 10 and an application executed in the processor.  The application may be downloaded from the integrated message management service\nserver 20 and installed in the memory 1500.  The application includes a plurality of instructions for causing the processor to perform all processing for providing additional functions by the integrated information management and message intelligence\nagent according to the present invention\n&lt;The Integrated Message Management Server&gt;\nFIG. 19 is a block diagram showing a configuration of the integrated message management service server 20 having a first system configuration according to an exemplary embodiment of the present invention.\nThe integrated message management service server 20 having the first system configuration according to an exemplary embodiment may include a data transceiver module 2100, a member management module 2200, a database 2300, a friend management\nmodule 2400, and a AI agent management module 2500.\nThe data transceiver module 2100 receives information generated in association with the integrated message information management application (e.g., integrated message information, schedule information, memo information, and the like) from the\nuser's mobile terminal 10, and transmits the information managed by the integrated message management service server 20 to the mobile terminal 10.\nThe member management module 2200 may include a member information managing part 2210, a membership authentication part 2220, and a friend registration managing part 2230.  The member information managing part 2210 stores various member related\ninformation, which is obtained when each user subscribes to a service or a database, and updates of changes to the membership-related information.\nThe membership authentication part 2220 performs authentication in cooperation with an authentication server (not shown), when a user accesses the integrated message management service server 20.  In addition, when the user who does not\nsubscribe to the integrated message management service accesses the integrated message management service server 20, the membership authentication part 2220 may provide the above-described integrated message information management application to the\nmobile terminal of the user (e.g., providing a link to download and install the integrated message information management application, or automatically providing the application).\nThe friend registration managing part 2230 manages integrated message management service to a friend, who is registered as a friend in various methods for each member.\nThe friend management module 2400 may include an address book friend managing part 2410 and an SNS friend managing part 2420 to perform functions described above.  The address book friend managing part 2410 searches the address book stored in\nthe client terminal 10, and automatically registers the searched friend in the address book as an the integrated message management service friend.  The SNS friend managing part 2420 searches the SNS friend registered by himself/herself in the SNS\nservice (e.g., `Facebook` or `Kakao Talk` service), and automatically registers the searched SNS friend as an the integrated message management service friend.  As described above, the friend management module 2400 provides a function of registering an\nacquaintance as an integrated message management service friend of in various ways.\nFor example, when an integrated message management service application installed in the client terminal 10 is executed, the client application directly reads the address book data stored in the client terminal 10, and transmits the address book\ndata to the service server 20.  Then, the integrated message management service server 20 inquires the member information stored in the member information database to determine whether each contact in the address book data has been registered as a member\nof the integrated message management service, and then transmits the determination results to the client terminal 10 such that the client terminal 10 may perform an integrated message management service friend registration.\nThe database 2300 for storing various data may include a membership information database 2310, an integrated message information database 2320, a friend information database 2330, conversation model database 2340, and other database necessary\nfor providing the integrated message management service.\nThe membership information database 2310 stores various information of members who subscribe to the integrated message information management service.  For example, the member information may include personal information, a profile photograph,\nthe nickname, the latest access time, the SNS subscription information, the personal information exposure setting information, and the login setting information.\nThe integrated message information database 2320 stores the integrated message information generated through the integrated message information management application of the mobile terminal 10.\nThe friend information database 2330 stores various kinds of information on integrated message management service friends registered automatically or by a member.  For example, the information may include ID and nickname of registered friends,\nregistration path information, friend blocking information, and the like.\nThe conversation model database 2340 stores data related to the conversation model described above.\nThe artificial intelligence agent management module 2500 may include a conversation model management unit 2510, a conversation analysis rule management unit 2520, and a service response managing unit 2530.\nThe conversation model management unit 2510 manages the conversation model used for generating the service response generation rule and analysis rule of the AI agent 1440.  The conversation model management unit 2510 collects conversation data\nthrough the integrated message management platform according to the present invention, analyzes the collected data, and reflects the collected data to the generated conversation model.\nThe conversation analysis rule managing unit 2520 performs a function of updating or creating the dialog analysis rule based on the dialog model generated or updated by the conversation model managing unit 2510.  The conversation analysis rule\nmanagement unit 2520 also performs a function of transmitting the updated or added analysis rule to the user's mobile terminal 10.  That is, the analysis rule of the AI agent 1440 installed in the user's mobile terminal 10 is updated by the conversation\nanalysis rule management unit 2520 of the server 20.\nThe service response managing unit 2530 manages a service response performed by the AI agent 1440 of the user's mobile terminal 10.  Specifically, the service response managing unit 2530 performs a function of creating and changing a service\nresponse rule, and transmitting the changed/generated service response rule to the user's mobile terminal 10.\nAccording to an exemplary embodiment, the integrated message management service server 20 may have a second system configuration.  In the first system configuration, most functions related to the integrated message management service, such as\ngeneration and management of integrated message information, processing of additional functions, and management of user interface are performed in the user mobile terminal 10, and the server 20 stores the processed result data and retrieves the stored\ndata in response to a request from the mobile terminal 10.\nOn the other hand, the integrated message management service server 20 having the second system configuration according to an exemplary embodiment may be configured to provide a part of above-described various functions performed in the\nintegrated message management part 1400 of the user mobile terminal 10 in the first system configuration.  That is, some of the components of the integrated message management part 1400 of the user mobile terminal 10 of the first system configuration may\nbe included in the integrated message management service server 20 having the second system configuration, and the corresponding functions may be performed by the integrated message management service server 20 without involvement of the user mobile\nterminal 10.\nFIGS. 20 and 21 are block diagrams showing an integrated message management service server and a user mobile terminal having the second system configuration according to exemplary embodiments.\nReferring to FIG. 21, a user mobile terminal 10 includes a voice call management part 1100, a text message management part 1200, an integrated message management part 1400, and a transceiver 1600.  The integrated message management part 1400\nincludes a user interface managing unit 1430, and an artificial intelligence agent 1440.  Meanwhile, referring to FIG. 20, the integrated message management service server 20 further includes a server integrated message management module 2600 including a\nmessage monitoring unit 2610, a message information managing unit 2620, and an additional function process part 2630 in addition to the components of the integrated message management service server 20 having the first system configuration shown in FIG.\n19.\nAmong the components of the user mobile terminal 10 of FIG. 21, the same components as those of the first system configuration perform the same functions as the first system configuration.  Therefore, duplicated description of the same functions\nand operations will be omitted.  However, since some operations is necessary to be performed in association with the integrated message management service server 20 having the second system configuration, those will be described again.  Similarly, with\nrespect to the integrated message management service server 20, only the operations performed in connection with the user mobile terminal 10 will be described, and the same components as those in the first system configuration example will be omitted to\navoid redundancy.\nThe message monitoring unit 1410 of the user mobile terminal 10 inquires whether the voice call information and the text message information are updated.  The access to the voice call management part 1100 and the text message management part\n1200, and the inquiry about the update of the voice call information and the text message information are performed in substantially the same manner as those described in the first system configuration.  As a result of the inquiry, if there are newly\nreceived/transmitted voice call information and text message information, the message monitoring unit 1410 transmits the corresponding voice call information and the text message information to the integrated message management service server 20 via the\ntransceiver 1600.\nUpon receiving the voice call information and the text message information from the user mobile terminal 10, the message information managing unit 2620 of the integrated message management service server 20 generates integrated message\ninformation, which is to be provided for the user through the integrated message management application, to the user mobile terminal 10.  The detailed operation of the message information managing unit 2620 is substantially the same as that of the\nmessage information managing unit 2620 of the mobile terminal 10 in the first system configuration.  In other words, the message information managing unit 260 of the integrated message management service server 20 is configured to perform substantially\nthe same process performed by the message information managing unit 1420 of the mobile terminal 10 in the first system configuration.  For example, the message information managing unit 2620 of the integrated message management service server 20 performs\na process for generating the integrated message information according to a message block format, which is the same as the step S1300 of FIG. 5.\nThe integrated message information generated by the message information managing unit 2620 is stored in the integrated message information database 2320.  The data transceiver module 2100 of the integrated message management service server 20\ntransmits the integrated message information to the user mobile terminal 10.\nThe user interface managing unit 1430 of the user mobile terminal 10 generates the first message management user interface 100 and the second message management user interface 200 using the integrated message information received from the\nintegrated message management service server 20.  The details of the operation of the user interface managing unit 1430, the first message management user interface 100, and the second message management user interface 200 are substantially the same as\nthose in the first system configuration.  Information on the text message and the voice call newly exchanged through the first and second message management user interfaces 100 and 200 are transmitted to the server 20, and the message information\nmanaging unit 2620 of the server 20 processes information on the received text message and voice call, and stores it in the integrated message information database 2320.\nThe additional function process part 2630 of the integrated message management service server 20 performs substantially the same operation as the additional function process part 1300 of the user mobile terminal 10 in the first system\nconfiguration, in response to the user input performed on the first and second message management user interfaces 100 and 200.  That is, upon receiving a user request for activating a specific additional function, in which the request is input through\nthe first and second message management user interfaces 100 and 200 provided to the user's mobile terminal 10, the additional function process part 2630 runs a server application performing the requested additional function.  The server application\nprovides the user mobile terminal with an additional function user interface capable of receiving user input related to the additional function.\nWhen the user input related to the additional function is inputted through the additional function user interface, the user mobile terminal 10 transmits the inputted user input to the integrated message management service server 20.  The\nadditional function process part 2630 of the integrated message management service server 20 performs the process corresponding to the received user input, stores the result in the database, and transmits the result to the user mobile terminal 10.  The\nuser interface managing unit 1430 of the user mobile terminal 10 performs substantially the same process as those described in the first system configuration to display the received additional function processing result on the first and second message\nmanagement user interfaces 100 and 200.\nFor example, when the user touches the schedule management icon displayed on the first message management user interface 100, the user mobile terminal 10 transmits a signal requesting the schedule management function to the integrated message\nmanagement service server 20.  The integrated message management service server 20 executes the schedule management server application in response to the user request, and provides the user terminal 10 with a schedule management user input interface as\nshown in FIG. 13 for the mobile terminal 10.  The schedule management user input interface may be stored in the user mobile terminal 10 to be provided through the first and second message management user interfaces 100 and 200 when the schedule\nmanagement server application is executed on the integrated message management service server 20.\nWhen the user inputs necessary information through the schedule management user interface, the user mobile terminal 10 transmits the inputted information to the integrated message management service server 20.  The schedule management server\napplication of the integrated message management service server 20 generates schedule information using the received user input information, and transmits the result to the user mobile terminal 10.  The user interface managing unit 1430 of the user\nmobile terminal 10 displays the received schedule information on the first and second message management user interfaces 100 and 200.\nThe above-described methods and the process flow diagrams are provided as illustrative examples and are not intended to require or imply that the steps of the various exemplary embodiments must be performed in the order presented.  Instead, the\norder of steps in the foregoing exemplary embodiments may be performed in any order.  Words such as \"after\", \"then,\" \"next,\" etc. are merely intended to aid the reader through description of the methods.\nThe various illustrative logical blocks, units, modules, circuits, and algorithm steps described in connection with the exemplary embodiments may be implemented as electronic hardware, computer software, or combinations of both.  In order to\ndescribe the interchangeability of hardware and software, various illustrative features, blocks, units, modules, circuits, and steps have been described above in terms of their general functionality.  Whether such functionality is implemented as hardware\nor software depends upon the particular application and design constraints for the overall system.  A person of ordinary skill in the art may implement the functionality in various ways for each particular application without departing from the scope of\nthe present invention.\nThe hardware such as the server 20 and the terminal 10 used to implement the various illustrative logics, logical blocks, units, modules, and circuits described in connection with the exemplary embodiments disclosed herein may be implemented or\nperformed with a general purpose processor, a digital signal processor (DSP) an application specific integrated circuit (ASIC), a field programmable gate array (FPGA) or other programmable logic device, discrete gate or transistor logic, discrete\nhardware components, or any combination thereof designed to perform the functions described herein.  A general-purpose processor may be a microprocessor, but, in the alternative, the processor may be any conventional processor, controller,\nmicrocontroller, or state machine.  A processor may also be implemented as a combination of computing devices, e.g., a combination of a DSP and a microprocessor, a plurality of microprocessors, one or more microprocessors in conjunction with a DSP core,\nor any other such configuration.  Alternatively, some steps or methods may be performed by circuitry that is specific to a given function.\nIn one or more exemplary embodiments, the functions described may be implemented in hardware, software, firmware, or any combination thereof.  If implemented in software, the functions may be stored as one or more instructions or code on a\nnon-transitory computer-readable medium or non-transitory processor-readable medium.  The steps of a method or algorithm disclosed herein may be embodied in a processor-executable software module which may reside on a non-transitory processor-readable\nstorage medium or a non-transitory computer-readable storage medium.  Non-transitory computer-readable or processor-readable storage media may be any storage media that may be accessed by a computer or a processor.  By way of example but not limitation,\nsuch non-transitory computer-readable or processor-readable media may include RAM, ROM, EEPROM, FLASH memory, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium that may be used to store\ndesired program code in the form of instructions or data structures and that may be accessed by a computer.  Disc includes optically reproducible data such as a compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), and Blu-ray disc. \nDisk includes magnetically reproducible data such as a floppy disk.  Combinations of the above are also included within the scope of non-transitory computer-readable and processor-readable media.  Additionally, the operations of a method or algorithm may\nreside as one or any combination or set of codes and/or instructions on a non-transitory processor-readable medium and/or computer-readable medium, which may be incorporated into a computer program product.\nAlthough certain exemplary embodiments and implementations have been described herein, other embodiments and modifications will be apparent from this description.  Accordingly, the inventive concepts are not limited to such embodiments, but\nrather to the broader scope of the appended claims and various obvious modifications and equivalent arrangements as would be apparent to a person of ordinary skill in the art.", "application_number": "15938696", "abstract": " A mobile device providing integrated management of message information\n     and service provision through artificial intelligence is disclosed. The\n     mobile device includes an integrated message management unit comprising a\n     message monitoring unit configured to monitor voice call information and\n     text message information in association with the voice call management\n     part and the text message management part, a message information managing\n     unit configured to generate integrated message information, which is to\n     be provided to a user, based on the voice call information and the text\n     message information, an interface managing unit configured to generate an\n     integrated message management user interface displaying the integrated\n     message information, and an artificial intelligence agent analyzing the\n     voice call information and the text message information and providing a\n     service associated with at least one additional function in association\n     with the additional function process part based on the analyzed result.\n", "citations": ["20040167779", "20120253788", "20140229560", "20150134646", "20160174074", "20160203819", "20160205049", "20170132019", "20170222961", "20180159805", "20180227424"], "related": []}, {"id": "20180288355", "patent_code": "10375374", "patent_name": "Dimension extractable object comprising spatial metadata for a captured\n     image or video", "year": "2019", "inventor_and_country_data": " Inventors: \nMinami; Eric (Palo Alto, CA), Chu; Charles (Cupertino, CA)  ", "description": "<BR><BR>TECHNICAL FIELD\nThis invention relates to capturing a still or moving image as well as spatial metadata for the image, storing the image and the spatial metadata as a dimension extractable object, and utilizing the dimension extractable object.\n<BR><BR>BACKGROUND OF THE INVENTION\nCameras are well-known in the prior art.  FIG. 1 depicts a simplified diagram of prior art camera 110 comprising photodiode 111.  Photodiode 111 receives light through an aperture in camera 110 and converts light into electric current or\nvoltage.  As is well-known, a still image or moving image (video) can be captured of an object of interest, such as physical object 120 in this example.\nAlso in the prior art, certain digital image capture formats allow for the insertion of metadata, that is, data that is related to the image but not necessary for reconstruction of the image.  Example of such image formats are the JPEG format\nfor still images and the MPEG-4 (H.261) format for moving images (video).  These formats allow for the embedding of arbitrary metadata that describes information related to the capture of the image.  Examples of metadata that is frequently embedded in\ndigital images include date of capture, GPS coordinates of capture location, camera settings (shutter speed, aperture), copyright data, image size, and software used to process the image.\nThere are numerous metadata standards that define the data model for the metadata.  One example is Extensible Metadata Platform (XMP).  XMP is a standard format that does not restrict the attribute field of the metadata and has a flexible\ndefinition of the type of data value.  Other standard formats, such as Dublin Core (DCI), Information Interchange Model (IIM), and Exchangeable Image File Format (EXIF), have specific elements (attributes) and value types.  For example, XMP standard\ndefines special namespaces for DCI elements.  In general, image formats do not require any metadata.  Furthermore, image formats do not require existing metadata to follow a standard.\nSome encoding formats for digital still image are JPEG, GIF, PNG, JPEG2000, BMP, FITS, TIFF, RAW, and FITS.  All these formats allow insertion of metadata into the file.  Common video encoding formats typically are all container formats because\nthe formats need to support multiple types of data streams--e.g. video, audio, and subtitles.  The formats typically support metadata for the container itself; a few support a metadata stream that can be synchronized with the video image.  Examples of\nvideo container formats include MP4, AVI, MPEG, MKV, Ogg, MXF, DPX, and Quicktime, of which only MKV (Matroska) and Ogg are open source.  The most common video stream encoding format is MPEG-2 (H.261), which is streamed video supported in most container\nformats.\nThe prior art also includes numerous three-dimensional (3D) modeling formats, some proprietary and tied to a specific software tool such as the products sold with trademarks AutoCad and Lightwave, while others are more general.  Simple 3D\nmodeling formats like STL and OBJ do not have definitions for metadata, but most proprietary and newer formats supports embedded metadata.  Examples of 3D modeling formats include AMF, STL, OBJ, Blender, DWG (used by the product with trademark Autocad),\nX3D, SKP (used by the product with trademark Google Sketchup), and LWO (used by the product with trademark Lightwave).\nAlso known in the prior art are laser distance measuring devices for measuring the distance between the device and an object.  FIG. 2 depicts prior art distance measuring device 210, which comprises laser diode 211, photodiode 212, lens 213, and\nlens 214.  In one prior art technique, laser diode 211 emits modulated laser light.  The light is focused through lens 213, hits physical object 120, and the light reflects off of physical object 120.  A portion of the light will return to distance\nmeasuring device 210 through lens 214 and hit photodiode 212.  Distance measuring device 210 can capture the distance between photodiode 212 and each portion of physical object 120 using numerous different techniques.  In one technique, distance\nmeasuring device 210 measures the time that elapses between the emission of the laser light from laser diode 211 and the moment when reflected light is received by photodiode 212, and it then calculates distance from that time measurement.  An example of\na novel laser distance measuring device and calibration technique is described in U.S.  patent application Ser.  No. 15/458,969, filed on Mar.  14, 2017, and titled \"Using Integrated Silicon LED to Calibrate Phase Offset in Optical Receiver in Laser\nRange Finder,\" which is incorporated by reference herein.\nTo date, the prior art has not integrated a laser distance measuring device with a camera to capture spatial information for an object with sufficient accuracy to enable the types of applications described herein.  The prior art also lacks a\ndata structure for sending and receiving spatial metadata related to an image.  The prior art also lacks the ability to capture, transmit, and modify spatial metadata and transactional metadata for a product that is captured in an image, which limits the\ndetail that can be exchanged as part of an e-commerce transaction.\nWhat is needed is the ability to capture spatial metadata with the captured image, to store spatial metadata with the image, and to later utilize the spatial metadata.  What is further needed are applications that utilize such spatial metadata\nand transactional metadata that can be associated with the image.\n<BR><BR>SUMMARY OF THE INVENTION\nThe invention enables capturing an image as well as spatial metadata for the image, storing the image and the spatial metadata as a dimension extractable object, and utilizing the dimension extractable object.  As used herein, \"dimension\nextractable object\" refers to an object that comprises 2D or 3D still or video image data and spatial metadata, such as some or all of the metadata described in Table 1, below.  The dimensional extractable object optionally comprises transactional\nmetadata, such as some or all of the metadata described in Table 2, below. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nFIG. 1 depicts a prior art camera.\nFIG. 2 depicts a prior art distance measuring device.\nFIG. 3 depicts the creation of a dimension extractable object by an image and distance capture device.\nFIG. 4 depicts an alternative configuration for the image and distance capture device.\nFIG. 5 depicts characteristics of the dimension extractable object.\nFIG. 6 depicts an example of distance data within a dimension extractable object.\nFIG. 7 depicts an embodiment of the dimension extractable object within a JPEG file.\nFIG. 8 depicts an embodiment of the dimension extractable object within a video file.\nFIG. 9 depicts an embodiment of the dimension extractable object within a tar or zip archive file.\nFIG. 10 depicts an embodiment of the dimension extractable object within a container.\nFIG. 11 depicts an embodiment of the dimension extractable object used in conjunction with a lens model transform.\nFIG. 12 depicts an embodiment of a method of generating a stitched-image dimension extractable object from a plurality of image dimension extractable objects.\nFIG. 13 depicts an embodiment of a method of generating a stitched-image dimension extractable object from a video dimension extractable object.\nFIG. 14 depicts an embodiment of a method of generating a 3D model dimension extractable object from a video dimension extractable object.\nFIG. 15 depicts another embodiment of a method of generating a 3D model dimension extractable object from a video dimension extractable object.\nFIG. 16 depicts an embodiment of a method of calculating the velocity of a moving physical object using a video dimension extractable object captured using a stationary image and distance capture device.\nFIG. 17 depicts an embodiment of a method of calculating the velocity of a moving physical object using a video dimension extractable object captured using a moving or rotating image and distance capture device.\nFIG. 18 depicts an embodiment of a system and method for performing e-commerce using dimension extractable objects.\nFIG. 19 depicts an example of a dimension extractable object and a modified dimension extractable object.\nFIG. 20 depicts an example of a transaction initiated by a buyer application.\nFIG. 21 depicts an example of a transaction initiated by a seller application.\nFIG. 22 depicts an example of a transaction involving a manager application.\nFIG. 23 depicts another example of a transaction involving a manager application.\nFIG. 24 depicts an example of a transaction involving a broker application.\nFIG. 25 depicts a server providing results to a computing device based on criteria from the computing device.\nFIG. 26 depicts a server providing results to a computing device based on user data.\n<BR><BR>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS\nI. Image Format\nReferring to FIG. 3, system 300 comprises image and distance capture device 310 and computing device 330.  In this embodiment, image and distance capture device 310 comprises camera 110, distance measuring device 210, and processing unit 350. \nImage and distance capture device 310 captures an image of physical object 140 and spatial metadata for physical object 140, and processing unit 350 generates dimension extractable object 320.\nImage and distance capture device 310 can transmit dimension extractable object 320 over a link or network to computing device 330, which can store, process, modify, transmit, or otherwise utilize dimension extractable object 320.  For example,\ncomputing device 330 can provide e-commerce services that utilize the information stored in dimension extractable object.\nComputing device 330 can comprise a server, laptop, desktop, mobile device, cloud system, or other known device.  Computing device 330 comprises one or more processing units and one or more storage units and is capable of executing software\ncode.\nFIG. 4 depicts an alternative configuration.  Here, processing unit 350 is external to image and distance capture device 310.  In one embodiment, image and distance capture device 310 is a peripheral that plugs into processing unit 350, which\nmight be a smartphone or other computing device.  In another embodiment, image and distance capture device 310 and processing unit 350 communicate over a network or link, and processing unit 350 might be a server or cloud system.\nIt is to be understood that camera 110 and distance measuring device 210 can be part of a single physical structure, or they can be part of separate physical structures.\nFIG. 5 depicts additional aspects of dimension extractable object 320.  Dimension extractable object 320 comprises image data 510, metadata 520, spatial metadata 530, and optionally, transactional metadata 540.  Image data 510 comprises image\ndata known in the prior art, such as the image data stored in JPEG, MP4, and 3D model files.  Metadata 520 comprises metadata known in the prior art, such as date and time of capture, GPS location, etc. Spatial metadata 530 and transactional metadata 540\ncomprise metadata as described with reference to embodiments contained below.\nIn the preferred embodiments, spatial metadata 530 comprises data encoding the absolute distance of physical object 140 from a camera lens of image and distance capture device 310, as well as other metadata that enables a wide variety of\nphotogrammetric and e-commerce applications.  Photogrammetry is a science of making measurements from photographs and can apply to both still and moving (video) digital images.\nTable 1 describes an exemplary embodiment of spatial metadata 530 within dimension extractable object 320 that can enable the applications described herein.  The term \"spatial metadata\" encompasses any or all of the data described in Table 1, as\nwell as any other data that represents or can be used to determine distance, dimensions, shapes, or contours of the captured physical object or that can used to increase the accuracy or precision of such determinations.\nTABLE-US-00001 TABLE 1 EXEMPLARY SPATIAL METADATA 530 OF DIMENSION EXTRACTABLE OBJECT 320 Element Description Identifier/ A unique alphanumeric identification string Classification Part of the identification is used for classification Extensible\n- (a) possible to have multiple hierarchical categories for classification; (b) possible to have reference (link) to related dimension extractable object Distance May be in any standard or arbitrary measurement unit Millimeter + phase shift + calibration\ndata Frequency + number of periods There is no requirement to interpret the data as a standard distance.  The data can be specified in terms of phase shift, frequency, and calibration if so desired.  One or more points in the image, must be paired with\nthe image position data.  Image position In any coordinate system or at pre-defined points in the image.  Coordinate system: cartesian, polar, complex plane.  Pre-defined: (center, mid-left, mid-right) where each has a specific definition relative to the\ncenter/edges of the image One or more points in the image, must be paired with the distance data.  May include image position data as before and/or after lens model correction.  Lens model Lens model includes distortion and zoom.  The lens model can be\nequation based, e.g. empirical polynomial fitting, or physics based model.  The lens model can be look-up table based.  Image sequence Reference images for stitched dimension extractable object model- May be a pointer to multiple images that interact\nwith the current image.  May include information of the entire sequence of images necessary to reproduce the whole view.  May include any data relevant to reproducing the stitching algorithm used in the global image model Global map For stitched\ndimension extractable object model- May include thumbnail of the whole view (global map).  The global map may include metadata on the all points that exist in the 2D image/3D object model The (point, distance) data for the global image model may be\nincluded in each image.  Camera angular Raw data from gyroscope velocity May include pointer to reference image in a sequence of images used to create the global image model (see Global image sequence).  Camera linear Raw data from accelerometer velocity\nMay include pointer to reference image in a sequence of images used to create the global image model (see Global image sequence).  Camera tilt 3-axis tilt angle calculated from accelerometer/gyro data.  position May include pointer to reference image in\na sequence of images used to create the global image model (see Global image sequence).  Camera velocity Velocity of camera calculated from accelerometer/GPS data May include pointer to reference image in a sequence of images used to create the global\nimage model (see Global image sequence).  Embedded EXIF Already included in many digital images.  data EXIF format typically includes all relevant camera setting data Auxiliary sensor Include any other sensor data that may be important in downstream data\napplication.  Temperature, humidity, anemometer readings GPS lock information Cell phone tower location Accuracy/ Include data related to the accuracy of distance measurement.  Precision Number of samples and statistics (sigma, variance) Sampling time\nA simple example of spatial metadata 530 is shown in FIG. 6.  Processing unit 350 generates dimension extractable object 320, which comprises image 510 of physical object 140 captured by image and distance capture device 310.  Image and distance\ncapture device 310 and processing unit 350 also generate metadata 520, such as date and file size information, and spatial metadata 530, which here comprises image position data 531, and distance data 532.  Image position data 531 is the location of\ndistance measurement within image 510, here expressed as row and column numbers.  Distance data 532 is the distance, at the time image 510 was captured, between a specific portion of distance capture device 310, such as lens 213, and the portion of\nphysical object 140 corresponding to that particular pixel in image 510.\nDimension extractable object 320 optionally can utilize known file and data structure formats as long as such file and data structure formats can accommodate spatial metadata 530.\nFor example, FIG. 7 depicts the structure of an exemplary image file 700.  Image file 700 in this example is based on the prior art JPEG file format.  Image file 700 comprises numerous fields, one of which is field 710, which in this example is\nthe APPn field within the JPEG file format.  Field 710 comprises unspecified space that can be used by the implementer.  In this embodiment, field 710 is used to store spatial metadata 530, such as some or all of the metadata set forth in Table 1.  In\nthis manner, image file 700 is an embodiment of dimension extractable object 320.\nFIG. 8 depicts an embodiment for video data.  Here, image and distance capture device 310 captures video data 810.  Image and distance capture device 310 generates video dimension extractable object 320.  Dimension extractable object 320\ncomprises a video stream (image data 510) and an audio stream and subtitles (metadata 520).  In this embodiment, spatial metadata 530 is split into two portions, with each portion stored separately.  The portion of dimension metadata 530 that applies to\nthe entire video stream is stored in a metadata section within dimension extractable object 320.\nThe portion of dimension metadata 530 that is specific to a frame within the video stream is stored in a section of dimension extractable object that allows such metadata to be associated with the particular frame.  For video container formats\nthat support metadata streams or extensible streams, this portion of spatial metadata 530 is stored as encoded per-frame metadata in one or more metadata streams.  For video container formats that do not support a metadata stream but support other types\nof overlay data streams, this portion of spatial metadata 530 is stored as encoded per-frame metadata in a supported overlay data stream.\nFIG. 9 depicts another embodiment of dimension extractable object 320.  Here, dimension extractable object 320 comprises tar or zip file 910.  An image is captured by image and distance capture device 310 of physical object 140 and is stored as\nimage data 510 in tar or zip file 910.  Spatial metadata 530 also is captured and stored in sidecar files 920.  Sidecar files 920 also comprise pointers to image data 510 and vice versa.\nFIG. 10 depicts another embodiment.  Here, container 1000 is generated.  Container 1000 comprises a plurality of dimension extractable objects 320.  Each dimension extractable object 320 optionally is associated with other metadata 1010 that is\nstored in container 1000.  Each dimension extractable object 320 comprises image data 510 (still, video, or 3D) and may or may not contain spatial metadata 530.  In this embodiment, spatial metadata 530 can be stored outside of the dimension extractable\nobject 320 itself and instead can be stored in one or more metadata sections 1010 within container 1000.\nContainer 1000 is an optimal format for storage of multiple images that are related to one another.  Examples include: Multiple images from a stitched-together scene; A 3D model with the source images that were used to create the 3D model; A\nvideo image with some key frames extracted as still images; A 3D image of a main object and individual 3D images of components required to assemble the main object; and A main image and post-processed or marked-up derivative images, for example, with key\nfeatures extracted.\nIn this embodiment, container 1000 may include other containers 1000 of dimension extractable objects 320.  That is, the structure of this embodiment can be nested to include multiple hierarchical levels.\nEach container 1000 comprises a unique identification sequence 1020.  A dimension extractable object 320 can reference any number of additional dimension extractable objects 320 using the unique identification sequences 1020.  Unique\nidentification sequence 1020 comprises a segment that encodes a classification for container 1000.  The classification code may have multiple hierarchical levels.\nFIG. 11 depicts an application that integrates a lens model transform function with a dimension extractable object 320.  Here, image and distance capture device 310 captures image 1110 and spatial metadata 530 and generates dimension extractable\nobject 320.  Lens model transform 1120 (which optionally is an algorithm performed by a processing unit in image and distance capture device 310 or in computing device 330) performs a transformation on image 1110 to generate corrected image 1130, which\nis combined with spatial metadata 530 to generate a new dimension extractable object 320'.\nUsing this embodiment, a downstream application in computing device 330 can use lens model transform 1120 and spatial metadata 530, including distance information to an image point, to calculate the size of the physical object that was captured\nin the image.  Features of interest in the image can be extracted by using image recognition and object detection algorithms.\nBy storing the lens model, the downstream application in computing device 330 can correct for a large physical object that is captured using a wide-angle lens (as was the case with image 1110).  Using a wide-angle lens on image and distance\ncapture device 310 allows distance measurements to be made at closer ranges, which will increase the accuracy and sampling rate of image and distance capture device 310.\nFIG. 12 depicts a method that provides additional functionality based on the embodiment of FIG. 11.  Method 1200 comprises multiple steps that together allows a sequence of still images to be stitched together.\nIn step 1210, image and distance capture device 310 captures a sequence of images at close range and generates a sequence of dimension extractable objects 320.\nIn step 1220, the lens model transform 1120 of FIG. 11 is performed on each image in the sequence of images.\nIn step 1230, image and distance capture device 310 or computing device 330 stitches together the sequence of transformed images.  Even without any additional sensor data, the additional distance measurements contained in spatial metadata 530\nwill allow accurate stitching of the transformed images.  Additional data collected from an accelerometer and gyroscope in image and distance capture device 310 can help make corrections to the position of the camera and can further improve the accuracy\nof the stitching process.  Additional sensor data to calculate the camera tilt angle and position (accelerometer and gyro) assist in the creation of a 3D dimension extractable object 320 by (a) correcting for potential errors from stitching algorithm,\nand (b) reducing the number of images required to create the full 3D images.  The multiplicity of distance measurements in overlapping images can be used generate accurate surface profile of the target object.\nIn step 1240, a new dimension extractable object 320 is generated to embody the stitched-together image and spatial metadata 530.\nFIG. 13 depicts a method that utilizes an improved stitching capability for images extracted from video using spatial metadata 530.\nIn step 1310, video image dimension extractable object 320 is generated.  For example, image and distance capture device 310 can record video as the user walks around the physical extent of physical object 1350.\nIn step 1320, a sequence of still images is extracted from the captured video stream.\nIn step 1330, the still images are stitched together using the same technique discussed previously with reference to FIG. 12.\nIn step 1340, a new dimension extractable object 320 is generated to embody the stitched-together image and spatial metadata 530.  Thus, using the spatial metadata 530, a dimension extractable object comprising a single, stitched-together still\nimage can be generated for a large physical object 1350 using a captured video stream of the object that enables extraction of object surface profile.  The accuracy of this process can be improved through slower movement of image and distance capture\ndevice 310.\nFIG. 14 depicts a method for generating a 3D model of physical object 1440.  In this method, a user hold image and distance capture device 310 and captures video of physical object 1440.  For example, the user might walk around physical object\n1440.  If physical object 1440 is a room, then the user might stand in the center of the room and record video while facing the perimeter of the room and rotating.\nIn step 1410, image and distance capture device 310 captures video image dimension extractable object 1410.\nIn step 1420, a sequence of image frames is extracted from the video.\nIn step 1430, a 3D model dimension extractable object is generated from the sequence of image frames.  Thus, a single video recording, stored as a video dimension extractable object 320, contains all data necessary to construct a 3D model.\nFIG. 15 depicts a method 1500 for generating a 3D model of physical object 1540.  The first three steps are the same as in method 1400 described previously with reference to FIG. 14.  In step 1510, image and distance capture device 310 captures\nvideo image dimension extractable object 320.  In step 1520, a sequence of image frames is extracted from the video.  In step 1530, a 3D model dimension extractable object is generated from the sequence of image frames.  Thus, a single video recording,\nstored as a video dimension extractable object 320, contains all data necessary to construct a 3D model.\nIn step 1540, for objects with complex surface details, additional dimension extractable objects can be used to progressively include more surface details in the 3D model.\nWith reference to FIGS. 16 and 17, a video dimension extractable object 320 can be used to calculate the velocity of a moving physical object 1610.\nIn FIG. 16, image capturing device 110 is stationary, and moving physical object 1610 crosses the field of view and rangefinder range.  Multiple distance measurement points and time metadata, which is stored as spatial metadata 530 in dimension\nextractable object 320, can be used to calculate the velocity of moving physical object 1610.\nIn FIG. 17, image capturing device 110 is moving or rotating.  Multiple distance measurement points, time metadata, and accelerometer and gyroscope metadata, which is stored as spatial metadata 530 in dimension extractable object 320, can be\nused to calculate the velocity of moving physical object 1610.\nIn each of the embodiments described above, spatial metadata 530 in dimension extractable object 320 will allow a user operating computing device 130 to be provided with accurate dimension information for the physical object that is the subject\nof the captured images.  For example, if the physical object is a room, spatial metadata 530 will allow the user to be provided with the exact dimensions of the room, including all size measurements and angles.  If the physical object is a car, spatial\nmetadata 530 will allow the user to be provided with the exact dimensions of each visible surface and aspect of the car.  If the physical object is a landscape, spatial metadata 530 will allow the user to be provided with the exact distances between\nobjects in the landscape (e.g., distance between two trees in a landscape plan).\nII.  E-Commerce Applications of Dimension Extractable Objects\nThere are numerous benefits in using dimension extractable objects for e-commerce.  Optionally, metadata that is particularly useful for e-commerce can be added to a dimension extractable object.  With reference again to FIG. 5, dimension\nextractable object 320 optionally comprises transactional metadata 540.  Table 2 describes an exemplary embodiment of transactional metadata 540 within dimension extractable object 320 that can enable the applications described herein.  The term\n\"transactional metadata\" encompasses any or all of the data described in Table 2, as well as any other data that is useful to a commercial transaction involving the product that is the subject of dimension extractable object 320.\nTABLE-US-00002 TABLE 2 EXEMPLARY TRANSACTIONAL METADATA 540 OF DIMENSION EXTRACTABLE OBJECT 320 Element Description Project related Min, max total cost Min, max cost of material Min number required.  Type of material Min, max cost for shipping\nMaterial - color, type (e.g. bamboo, maple, Brazilian teak, 300 thread cotton) Accuracy specifications Validity date Bid related Limit geographical location for bids Job due/required by date Payment methods Contractual Return/restocking requirements\nBonus/penalty provisions for early/late completion Intellectual property registrations - trademark, copyright, patent information Supplier Ratings Relevant rating system, reviews Recommendations, references Buyer Ratings Relevant rating system, reviews\nShipping related Fragile/non-Fragile Ship method, conditions Others freight constraints Transaction related Maximum number of bids accepted.  Optimization criteria - examples include cost, quality, precision, yield, reputation.  Informational links Links\nto public/private webs page Links to shared file on a serve\nEach metadata field within transactional metadata 540 (such as the metadata listed in Table 2, above) may encode additional data that categorizes the level confidentiality of the data.  For example, in a completely open transaction, all fields\nare public to the parties involved in the transaction.  However, adding more specific confidentiality levels can improve the flow of the transaction process being handled by software applications.  For example, a buyer may make the minimum cost public,\nbut may wish to hide the maximum cost.  In general, completely open delineation of the requirements is often not desired by one or both of the parties.\nFIG. 18 depicts an embodiment of e-commerce system and method 1800.  An end user, which can be the buyer or the supplier, operates an end user application 1810 (buyer) or 1830 (supplier) in conjunction with image and distance capture device 310\nto capture image data and dimension data from physical object 140.  The end user application 1810/1830 generates dimension extractable object 320, which includes the image data and spatial metadata 530 as well as transactional metadata 540 such as some\nor all of the metadata described in Table 2, above.  The end user applications 1810/1830 transmits and receives dimension extractable object 320 to and from transaction application 1820.  The end user application 1810/1830 and transaction application\n1820 may modify dimension extractable object 320 by modifying spatial metadata 530 and/or transactional metadata 540 field or adding/removing dimension extractable object(s) 320 within the original dimension extractable object container.  The modified\ndimension extractable object 320' is treated as any other dimension extractable object.\nAn example of dimension extractable object 320 is depicted in FIG. 19.  Dimension extractable object 320 comprises image data 1901, spatial metadata 1902, and transactional metadata 1903.  In this example, the buyer wishes to purchase the\nproduct shown in image data 1901.  The product is further defined by spatial metadata 1902 (input by the buyer), which in this example includes an identifier of \"7H6% R3\"; distance data paired with image position (expressed in matrix of binary numbers),\nsuch as a distance measurement for each pixel in the image, where the distance is the measurement from image and distance capture device 310 to that portion of physical object 140; and a lens model, and transactional metadata 1903 (input by the buyer),\nwhich in this example includes a maximum cost per unit of $3.50; the minimum number of the product required of 500; a material of aluminum, and shipping of 2-day by courier.\nAn example of dimension extractable object 320' is shown in FIG. 19.  Dimension extractable object 320' comprises image data 1901 (which has not changed compared to dimension extractable object 320), spatial metadata 1902', and transactional\nmetadata 1903'.  In this example, spatial metadata 1902' is the same as spatial metadata 1902.  In other instances, a buyer or seller might change spatial metadata 1902 to create spatial metadata 1902', for example, if a seller wanted to offer a buyer a\nproduct that was similar to but not identical the product represented by spatial metadata 1902.  Transactional metadata 1903' is identical to transactional metadata 1903 except that the supplier has changed the maximum cost per unit to $3.75 and has\nchanged the material to iron.\nTransaction application 1820 can establish communication between buyer and supplier, or buyer application 1810 and supplier application 1830, through the internet and can complete the contractual agreement for the job if both parties agree to\nthe terms indicated in dimension extractable object 320.  One of ordinary skill in the art will appreciate that numerous rounds of dimension extractable objects can be exchanged between the buyer and the seller, similar to the manner in which drafts of\ncontracts can be exchanged in a negotiation.\nTransaction application 1820 optionally operates an event-driven engine that responds to transactional events using machine-learning or other computational algorithms.  For example, a dimension extractable object registration or retrieval by a\nbuyer can trigger an advertising widget for a related product.\nOne of ordinary skill in the art will understand that any number of individuals might make modifications to the same dimension extractable object, or that multiple modified versions of the same dimension extractable object may be created.  The\nexamples of the Figures included herein are merely illustrative.\nFIG. 20 depicts an embodiment of e-commerce system and method 2000 implementing an e-commerce transaction initiated by the buyer.  The buyer application 1810 registers a dimension extractable object 320 through transaction application 1820 and\nrequests a search for possible supplier matches through search engine 2030, which may be a part of transaction application 1820.  In other cases, buyer application 1810 may request direct access to the pool of dimension extractable objects 1840 through\nan application interface 2090 to implement a customized search algorithm.  The search responses 2020 are sent back to the buyer, who selects suppliers and contacts the potential supplier through the transaction application 1820.  Once the supplier\napplication 1830 receives the request for attention from the buyer application 1310, a communication path 2080 is established through transaction application 1820 for further negotiation on the terms, as necessary.  Applications 1810, 1820, and 1830 may\nalso conduct the initial phases of the negotiation algorithmically through an artificial intelligence (AI)-directed negotiation engine 2070 based on private/public e-commerce metadata.\nFIG. 21 depicts an embodiment of e-commerce system and method 1800 implementing an e-commerce transaction initiated by the supplier.  The supplier application 1830 registers a dimension extractable object 320 through the transaction application\n1820 and requests a search for possible buyer matches through the search engine 2030, which may be a part of transaction application 1820.  In other cases, supplier application 1830 may request direct access to the pool of dimension extractable objects\n1840 through an application interface 2190 to implement a customized search algorithm.  The search responses 2120 are sent back to the supplier, who selects and markets the product through the transaction application 1820.  Once the buyer application\n1810 receives the request for attention from the seller application 1830, a communication path 2180 is established through the transaction application for further negotiation on the terms, as necessary.  Applications 1810, 1820, and 1830 may also conduct\nthe initial phases of the negotiation algorithmically through a AI-directed negotiation engine 2070 based on private/public e-commerce metadata.\nFIG. 22 shows an embodiment of e-commerce system and method 2200 where buyer application 1810 and supplier application 1830 communicate directly through application interfaces 2190 and 2090, respectively, to manager application 2210, which\nmanages pool of dimension extractable objects 1840.  Manager application 2210 provides services commonly performed by storage servers, such as implementing access controls and monitoring access.  The application interface functions performed over\napplication interfaces 2190 and 2090 may include some functions ascribed to transaction application 1820 in previous examples, such as adding, removing, and returning qualified applications as directed by supplier and buyer applications.  The\ncommunications between the end-users (buyer and seller) are conducted independently of manager application 2210.\nFIG. 23 shows an embodiment of e-commerce system and method 2300, which is a variation of the embodiment 2200 of FIG. 22.  As in FIG. 22, buyer application 1810 and the supplier application 1830 communicate directly through an application\ninterface to manager application 2210, which manages pool of dimension extractable objects 1840.  The communications between the end-users (buyer and seller) are conducted independently of manager application 2210.  Manager application 2210 has the\nability to provide access for buyer application 1810 and supplier application 1830 to a plurality of manager applications (such as exemplary manager applications 2310, 2311, and 2312), each of which manages a pool of dimension extractable objects (such\nas exemplary pool of dimension extractable objects 1841, 1842, and 1843, respectively) Thus, pools of dimension extractable objects may be distributed across many servers.  Any number of additional manager applications and pools of dimension extractable\nobjects may exist.  Any given pool of dimension extractable objects may have private or public access.  The manager application controlling each pool will have a standard interface for communicating with the other manager applications and pools.  Thus,\ninstead of buyer application 1810 and supplier application 1830 accessing pool of dimension extractable objects 1841 or manager application 2310 directly, access is provided through manager application 2210.  Manager application 2210 will then provide\naccess to one or more pools of dimension extractable objects depending on its access rights.\nThus, in one implementation of e-commerce system and method 2300, the operator of manager application 2210 may negotiate or pay for access to various other manager applications and/or pools and market and sell such access to supplier application\n1830 and/or buyer application 1810.  For instance, a large retail provider might have access to a greater number of manager applications and pools than a smaller retail provider.\nIn another implementation of e-commerce system and method 2300, the cost of maintaining, collecting, and marketing a pool of dimension extractable objects can be funded by membership fees levied on buyers, suppliers, brokers, or others, or on a\n\"per transaction\" fee, or using any other pricing model.  Under this model, the fact that manager application 2210 acts as a gateway to manager applications 2310, 2311, and 2312 will make is easier for such a fee system to be imposed on buyers,\nsuppliers, or other users who wish to access pools 1841, 1842, and 1843.\nFIG. 24 shows an embodiment of e-commerce system and method 2400.  A broker operates broker application 2410 that communicates with buyer application 1810 through communication path 2420 and with supplier application 1830 through communication\npath 2430.  To access pool of dimension extractable objects 1840, broker application 2410 communicates with transaction application 1820 through communication path 2430 or with manager application 1810 through application interface 2440.\nOne benefit of system and method 2400 is that broker application 2410 can provide an additional layer of security for transaction application 1820 and pool of dimension extractable objects 1840.  This architecture also might be useful if pool of\ndimension extractable objects 1840 are proprietary in nature and if its owner does not wish them to be publicly accessible\nSystem and method 2400 also may be particularly useful in a situation where the operator of broker application has specialized expertise that itself provides value to potential transactions, as might be the case if the operator of broker\napplication 2410 is an interior designer, architect, systems designer, assembly line designer, or other professional with specialized knowledge and experience in design or manufacturing.  For example, broker application 2410 might have access rights to\nexemplary manager application 2210 and pool 1840 through application interface 2450, where manager application 2210 specializes in managing objects within a particular niche area (e.g., customized home furniture).  Broker application 2410 can have access\nrights to any number of other manager applications and pools.\nThis architecture also would allow a broker to modify dimension extractable objects to suit the needs of the buyer, seller, or other user.  For example, if a buyer uses buyer application 1810 to create dimension extractable object 1840, broker\ncan review the contents of dimension extractable object 1840, and modify it into dimension extractable objet 1841 using the broker's expertise in a given field.  Broker application 2410 can then find relevant objects managed by manager application 1810\nand can then provide them or provide a modified version of them to buyer application 1810.\nAn example of such a scenario would be if the broker is an interior designer.  The buyer can create a dimension extractable object 320 using buyer application, and might specify a space within his home that he wishes to decorate.  The broker can\nthen review an image of the space and the associated dimensions and can then choose dimension extractable objects from manager application 2210 (and perhaps other manager applications), which in this example might be suppliers of furniture, artwork, etc.\nThe broker might then create a new container that includes the original dimension extractable object 320 and the dimension extractable objects obtained from manager application 1810.  The broker might create multiple containers, each container including\ndifferent options and dimension extractable objects for the buyer to consider.  In this example, the broker will have access to the pools that are useful for the job either directly (e.g., through access to manager applications and pools) or indirectly\n(e.g. choosing light fixtures through a specialty lighting supplier).  Indirect access might be available only through another broker.  In another variation of this embodiment, the broker may just give the buyer application 1810 access to a subset of\ndimension extractable object pools that he has access to or that he has filtered based upon the buyer's taste, and the broker then can ask the buyer to choose dimension extractable objects for the design job or to place objects into the original\ndimension extractable object.\nIf broker does not find sufficient dimension extractable objects from manager application 1810 or other manager applications, the broker might communicate with the supplier pools managed by supplier application 1830 and other supplier\napplications and request customization by exchanging dimension extractable objects with the suppliers, as discussed previously.\nIn these situations, the broker optionally may ask for a service fee from the buyer, or the broker might ask for a commission from the supplier pool.\nIn a variation of this embodiment, a human broker need not be used at all.  Broker application 2410 can perform the tasks described above on its own without human intervention.\nAdditional detail is shown in FIG. 25 regarding performing searches with a pool of dimension extractable objects.  Server 2510 maintains pool of dimension extractable objects 1840.  Here, server 2510 is a high-level representation of transaction\napplication 1820, supplier application 1830, manager applications 2210, 2310, 2311, or 2312, and/or broker application 2410 shown in FIGS. 21-24.  Computing device 2520 is a high-level representation of buyer application 1810, supplier application 1830,\ntransaction application 1820, manager applications 2210, 2310, 2311, or 2312, and/or broker application 2410 shown in FIGS. 21-24 Computing device 2520 communicates with server 2510 through application interface 2550 to implement a customized search\nalgorithm.  Computing device 2520 provides server 2510 with a set of criteria for products that are of interest to the user of computing device 2520, such as type of device, size of device, cost of device, etc. Server 2510 identifies a set of dimension\nextractable objects within pool 1840 that satisfy the set of criteria.  Server 2510 then provides the set of dimension extractable objects to computing device 2520.\nAnother embodiment is shown in FIG. 26.  Server 2610 maintains pool of dimension extractable objects 1840.  Computing device 2620 is operated by User A and communicates with server 2610.  Server 2610 operates recommendation engine 2640 and\nmaintains user data 2630.  Recommendation engine 2640 uses prior art method of recommending products that might be of interest to a particular user based on data collected for that user and other users.  Recommendation engine 2640 optionally uses an AI\nengine.  User Data 2630 optionally comprises purchasing history, credit history, web activity information, demographic information, and other information for User A and other users.  Recommendation engine 2640 identified dimension extractable objects\nwithin pool 1840 that might be of interest to User A and sends those objects to computing device 2620 for User A's consideration.\nIn the embodiments described above, buyer application 1810, transaction application 1820, supplier application 1830, manager applications 2210 and 2310, and broker application 2410 each comprise lines of software code that operate on local\ndevices (such as image and distance capture device 310, computing device 130, or other computing devices) or may reside in cloud-based servers.\nExamples of transactions that can be performed using e-commerce system and method 2000 include the following: A buyer generates a dimension extractable object for a target model (e.g., replacement mechanical parts, custom fittings).  A supplier\nmatches the target model from stock or generates bids to reproduce the target model.  A buyer generates a dimension extractable object for a target model.  A supplier matches the derivative part from stock or generates bids to produce the derivative\npart.  For example, a furniture cover (derivative part) for a sofa model (target model) or a helmet (derivative part) for a human model (target model) A buyer generates a dimension extractable object for a target space, e.g. unusually-shaped alcove\nwithin a home and requests custom furniture to fill the space.  A supplier generates a bid to produce the object for the target space.  Or, a buyer uses the supplier's dimensional extractable object to visualize the product within the target space.  A\nsupplier creates dimension extractable objects for his products and registers them with the transaction application.  The transaction application may be enabled for automatic searches of potential buyers.  The buyer application may search for and bid on\nthe [suppliers'] registered dimension extractable objects.  A buyer generates a dimension extractable object for his body with fashion preferences.  A supplier or broker searches for clothing items to suggest to the buyer.\nThere may be more than two parties involved in a transaction.  The parties may be multiple buyers or multiple suppliers.  For example, a city may create a dimension extractable object container with multiple dimension extractable objects with\ndetails of sections of a community center and take bids for completing different sections of the project--e.g. landscaping, furniture, art, lighting.  This will be the case of one buyer, multiple suppliers.  Or, a city may subcontract each section of the\nproject, each with responsibility for completing a section of the community center.  Because each subcontractor may use different set of suppliers, the transaction may involve multiple buyer (subcontractors) and multiple suppliers.\nThe invention will have the following benefits for B2B (business-to-business), C2B (consumer-to-business), C2C (consumer-to-consumer), and other e-commerce transactions: Low-overhead search for matching supplier.  Because a dimension extractable\nobject encapsulates all dimensions necessary to spec-out a part or a job, there is no need to take additional measurements.  Transaction application 1320 sends supplier application 1330 a list of dimension extractable objects based on the supplier's\nprofile.  Machine-learning algorithms may be used to select dimension extractable objects based on the history of the supplier's transactions.  Supplier application 1330 can perform second-level filtering of the dimension extractable objects by matching\nagainst supplier stock or capability.  Expansion of supplier/buyer market.  When a buyer creates a request for a job in an internet-based open market, all necessary information is encapsulated in the dimension extractable object.  Supplier application\n1330 automatically generates the bid, for example, based on supplier capacity and stock, required date, and material cost.  There is no need for on-site estimate or additional discussion with the buyer to take measurements.  The buyer can choose from a\nlarge pool of global suppliers and choose a supplier that best fits his or her needs.  The supplier has a much bigger pool of customers with very low marketing cost.  A remote supplier (e.g., a custom machine-shop in Wyoming) can bid on projects\nnationwide or even worldwide.  An artisan in Africa can market his jewelry worldwide.  Rapid qualification and execution of transaction.  E-commerce metadata quickly qualifies the transactional parameters.  For example, buyer application 1310 or supplier\napplication 1330 can insert metadata related to maximum cost, required date, payment terms/method into a dimension extractable object.  A qualified buyer/supplier can be identified by a private or open rating system (e.g., consumer credit scores,\nrecommendations, qualified reviews, bank collaterals).  Full contractual text can be appended to the dimension extractable object and the transaction completed with digital signatures.  Flexible, open system for project management.  A dimension\nextractable object can be a request for an identical part, or a dimension extractable object job request can be more flexible.  The buyer can specify whether he or she is willing to accept a modified proposal for a job.  If there is a reasonable\nmechanical/aesthetic substitute for the original dimension extractable object request, the supplier can propose the modification through another dimension extractable object or more spec-based information (e.g. bamboo floor instead of hardwood).  The\nflexible model will work especially well when the buyer requires expert feedback for a project.  For example, the initial dimension extractable object can contain 3D images of an apartment for a re-model.  A supplier or a broker (possibly an interior\ndecorator or architect) can add elements into the dimension extractable object and bid for the job.  The supplier dimension extractable object can spawn off new dimension extractable objects to furnish the interior space (custom furniture, light\nfixtures).  Object-based visualization/negotiation Additional image formats/dimension extractable objects can be inserted into dimension extractable object containers.  This allows suppliers to add information on the project.  If the images are dimension\nextractable objects, then buyer can use another application (e.g. 3D viewing software or virtual reality software) to visualize the project.  Visualization can be used before contract and during the project to assist in the interaction between buyer and\nsupplier.  This process can reduce miscommunication/errors during the project.  A single dimension extractable object can contain information that describes many different parts of a complex architecture, landscaping, or manufacturing project with links\nto related dimension extractable objects.  Anonymity/Privacy Personal information is not required for dimension extractable objects but each dimension extractable object must have identifiers so a transaction can later establish communication between\nbuyers and suppliers.  By keeping all e-commerce related metadata with the object model, buyer application 1310, transaction application 1320, and supplier application 1330 can automate and optimize the work required to create bids.  Using digital\nidentifiers and signatures, the initial negotiations can be anonymous.\nReferences to the present invention herein are not intended to limit the scope of any claim or claim term, but instead merely make reference to one or more features that may be covered by one or more of the claims.  Structures, processes and\nnumerical examples described above are exemplary only, and should not be deemed to limit the claims.  It should be noted that, as used herein, the terms \"over\" and \"on\" both inclusively include \"directly on\" (no intermediate materials, elements or space\ndisposed there between) and \"indirectly on\" (intermediate materials, elements or space disposed there between).", "application_number": "15473098", "abstract": " The invention relates to capturing a still or moving image as well as\n     object position and displacement data for the image, storing the image\n     and the data as a dimension extractable object, and utilizing the\n     dimension extractable object.\n", "citations": ["9369727", "20050086582", "20130142431", "20150325038", "20170046594", "20180276841"], "related": []}, {"id": "20180366014", "patent_code": "10373510", "patent_name": "Apparatus, method, and system of insight-based cognitive assistant for\n     enhancing user's expertise in learning, review, rehearsal, and\n     memorization", "year": "2019", "inventor_and_country_data": " Inventors: \nNguyen; Phu-Vinh (Sherborn, MA)  ", "description": "<BR><BR>BACKGROUND\n<BR><BR>1.  Field\nApparatuses, methods, systems, and computer readable mediums consistent with exemplary embodiments broadly relate to cognitive technology, and more particularly, to learning and memory related technology.\n<BR><BR>2.  Description of Related Art\nNowadays, the world's knowledge grows very quickly.  Students are challenged with vast amounts of information from schools and other places.  The vast amount of information is often presented to the student in a lecture format throughout the\nyears of learning.\nResearch in the field of memory appears to show that a person can retain very limited amount of sensory information in his or her short-term memory.  Unless a segment of sensory information receives a cognitive processing task in a working\nmemory of a person, it is forgotten in just a few seconds.  This is particularly true when a person does not like a topic of the sensory information, a pain center in his or her brain is activated and causes a procrastination.  A person in such\nsituations tends to funnel his or her attention to more pleasant thoughts as opposed to studying the disliked topic.  As a consequence, the learning results will be poor for this person.\nResearch also shows that a person can attain a more comprehensive understanding on a complex topic by navigating, scanning, correlating, and establishing chunks of information.  A chunk includes a correlative net for several correlated features\nwithin the topic.  And if each new established chunk is again correlated to other chunks in that topic and that person's insights, a fully comprehensive understanding on the topic is attained and a new long-term memory is well established in that person\ninsights through an insight chunking network.  See e.g., about procrastination, focused mode, diffused mode and chunking method in \"Learning How to Learn,\" Barbara Oakley.\nTo apply the above method for today's learning technology, students can use cameras to take videos of their lectures during various classroom sessions during the day at school.  Students can also take notes during the lecture into their\nnotebooks or laptops.  At home, they may play back the videos on screens or TV and then correlate the notes with corresponding episodes to review and rehearse the lecture from one point to another or from topic to topic as guided by the notes.\nHowever, reviewing a long video, from beginning to an end may take a long time and a large amount of effort.  Furthermore, students are only able to take notes at time points or for topics that they understand and notes may be lost or confusing\nat the time points or on topics they are confused about or when they are disinterested.\nWhen reviewing, it is not easy to track and put the notes, comments into the correlated specific episodes in the video.  When saving the works, the links between notes, comments, and the correlated specific episodes are usually lost.  Therefore,\nthe effectiveness of learning process and the use of such saved information in future is limited.\nThe lecturer also has very limited feedback signals from their students to help him or her recognize the points or the time that his or her students are bored, sleepy, or confused and the time or points that are interesting and exciting for the\nstudents.\nThere is a need in the art to monitor, collect, and mark the cognitive states of a student or students onto and along their lecture so that they can navigate the necessary points to review.  There is a need to add notes, sketches, and/or\ncomments directly into the specific episodes of a video lecture.  Additionally, there is a need to document a part or parts, components, or full data of learning process including recorded video lecture at a classroom, synchronized cognitive performance\nof a user along or during the lecture, synchronized cognitive performance of student along or during the review, rehearsal, contents of notes, comments, questions, searches, and the navigating system for the content of the identified document.\nThere is also a need in the art to exchange, discuss, and improve the learning process via a network to help the users learn the material in a more efficient way and help the lecturer to improve the teaching process by providing better feedback\nregarding the lecture so that the lecturer can present the material in an easy, effective, and interesting fashion.  There is a need in the art to improve the information intake process and the information presentation process.\n<BR><BR>SUMMARY\nAccording to exemplary, non-limiting embodiments, cognitive assistant system is provided based on captured synchronized visual and audio information, captured synchronized user's cognitive states information, a display, an intuition-based\nnavigating map and a note or a comment input by a user.\nAccording to exemplary, non-limiting embodiments, intuition-based navigating map is provided based on a script window embodying subtitle modules displayed in sub-windows and marked with user's synchronized cognitive states.\nAccording to exemplary, non-limiting embodiments, captured synchronized visual and audio information is divided into sub-videos or episodes which are synchronized with subtitle modules displayed in sub-windows and marked with user's synchronized\ncognitive states.\nAccording to exemplary, non-limiting embodiments, the sub-videos which are synchronized with subtitle modules displayed in sub-windows and marked with user's synchronized cognitive states may be added with correlated notes, comments, sketch and\nso on as input by a user.\nIllustrative, non-limiting embodiments may overcome the above-noted disadvantages and problems in the prior art, and also may have been developed to provide solutions to other disadvantages and problems that were not described above.  However, a\nmethod, an apparatus, a system, and a computer readable medium that operates according to the teachings of the present disclosure is not necessarily required to overcome any of the particular problems or disadvantages described above.  It is understood\nthat one or more exemplary embodiment is not required to overcome the disadvantages described above, and may not overcome any of the problems described above.\nAccording to an aspect of exemplary embodiments, a personal emotion-based cognitive assistant system is provided, which includes at least one apparatus configured to capture, from an environment, data including at least one of visual and audio\ninformation, at least one sensor configured to capture emotional state of a user corresponding to the data captured by the apparatus, a review apparatus including a memory and a processor.  The processor is configured to: divide the data captured by the\napparatus into a plurality of segments according to a predetermined criteria, for each of the plurality of segments, determine an intuitive state of the user, comprising a type of correlations and a level of correlations, generated from a comparison\nbetween predetermined components of the emotional state of the user captured by the sensor which corresponds to the respective segment, from among the plurality of segments, and distinctive reference signals stored in the memory, wherein the distinctive\nreference signals represent distinctive intuitive reference samples, generate at least one timeline, and control to display the generated timeline comprising an emotional indicator for each of the plurality of segments.  The emotional indicator indicates\nthe determined intuitive state of the user.\nAccording to yet another exemplary embodiment, a personal emotion-based cognitive assistant method is provided.  The method includes receiving, by a computer, data including at least one of visual information and audio information captured from\nenvironment, receiving, by the computer, an emotional state of a user corresponding to the data captured by at least one sensor, dividing, by the computer, said data into a plurality of segments according to a predetermined criteria, for each of the\nplurality of segments, determining, by the computer, an intuitive state of the user comprising a type of correlations and a level of correlations, generated from a comparison between predetermined components of the received emotional state of the user\nwhich corresponds to the respective segment, from among the plurality of segments, and a plurality of distinctive reference signals stored in the memory.  The distinctive reference signals represent distinctive intuitive reference samples.  The method\nfurther includes generating, by the computer, at least one timeline for the data.  The timeline includes an emotional indicator for each of the plurality of segments, which indicates the determined intuitive state of the user, and outputting, by the\ncomputer, the generated timeline and at least a portion of the received data.\nAccording to yet another exemplary embodiment, a non-transitory computer readable recording medium storing therein a personal emotion-based cognitive assistant method is provided.  When the method is executed by a computer, it causes the\ncomputer to: receive data including at least one of visual information and audio information captured from environment, receive an emotional state of a user corresponding to the data captured by at least one sensor, divide said data into a plurality of\nsegments according to a predetermined criteria, for each of the plurality of segments, determine an intuitive state of the user including a type of correlations and a level of correlations, generated from a comparison between predetermined components of\nthe received emotional state of the user which corresponds to the respective segment, from among the plurality of segments, and a plurality of distinctive reference signals stored in the memory.  The distinctive reference signals represent distinctive\nintuitive reference samples.  The computer further generates at least one timeline for the data, the at least one timeline includes an emotional indicator for each of the plurality of segments, which indicates the determined intuitive state of the user,\nand outputs the generated timeline and at least a portion of the received data. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe accompanying drawings, which are incorporated in and constitute a part of this specification exemplify exemplary embodiments and, together with the description, serve to explain and illustrate exemplary embodiments.  Specifically:\nFIG. 1A is a view illustrating a device which captures environment of a user such as video lectures and synchronized cognitive states sensory data, according to an exemplary embodiment.\nFIG. 1B is a view illustrating synchronized cognitive states sensory data captured by the sensors and interpreted to correspond to a classified cognitive state of a user, according to an exemplary embodiment.\nFIG. 1C is a view illustrating the devices in a practical use for capturing video lecture and synchronized cognitive states sensory data in a classroom, according to an exemplary embodiment.\nFIG. 1D is a view illustrating reviewing, rehearsing, and consolidating of a recorded content such as a lecture based on synchronized classified cognitive states of a user, according to an exemplary embodiment.\nFIG. 2 is a block diagram illustrating a system capturing lecture material and sensory data, according to yet another exemplary embodiment.\nFIG. 3 is a block diagram illustrating a review apparatus, according to an exemplary embodiment.\nFIG. 4 is a flowchart illustrating a method of generating a timeline, according to an exemplary embodiment.\nFIG. 5 is a view illustrating displaying contents with cognitive state of a user, according to an exemplary embodiment.\nFIG. 6 is a block diagram of a review apparatus, according to an exemplary embodiment.\nFIG. 7 is a view illustrating synchronization of video and audio contents with sensory data by a review apparatus, according to an exemplary embodiment.\nFIGS. 8A and 8B are views illustrating methods of building correlations, according to exemplary embodiments.\nFIGS. 9A-9C are views illustrating methods of building correlations after studying contents, according to exemplary embodiments.\nFIG. 10 is a flow chart illustrating a method of building correlations or an understanding of contents, according to an exemplary embodiment.\nFIG. 11 is a view illustrating a method of building correlations according to yet another exemplary embodiment.\nFIGS. 12 and 13 are views illustrating a method of building correlations via a group setting according to yet another exemplary embodiment.\nFIG. 14 is a view illustrating a method of determining a skill level of a user with respect to a particular task according to yet another exemplary embodiment.\nFIG. 15 is a flow char illustrating a method of determining a skill level of a user with respect to a particular task according to yet another exemplary embodiment.\n<BR><BR>DETAILED DESCRIPTION OF EXEMPLARY EMBODIMENTS\nExemplary embodiments will now be described in detail with reference to the accompanying drawings.  Exemplary embodiments may be embodied in many different forms and should not be construed as being limited to the illustrative exemplary\nembodiments set forth herein.  Rather, the exemplary embodiments are provided so that this disclosure will be thorough and complete, and will fully convey the illustrative concept to those skilled in the art.  Also, well-known functions or constructions\nmay be omitted to provide a clear and concise description of exemplary embodiments.  The claims and their equivalents should be consulted to ascertain the true scope of an inventive concept.\nAccording to exemplary, non-limiting embodiments, cognitive assistant system is provided based on captured synchronized visual and audio information, captured synchronized user's cognitive states information, a display, an intuition-based\nnavigating map and a note, comment input by a user for example.\nAccording to exemplary, non-limiting embodiments, intuition-based navigating map is provided based on a script window embodying subtitle modules displayed in sub-windows and marked with user's synchronized cognitive states, as explained in\ngreater detailed below.\nAccording to exemplary, non-limiting embodiments, captured synchronized visual and audio information is divided into sub-videos or episodes which are synchronized with subtitle modules displayed in sub-windows and marked with user's synchronized\ncognitive states.\nAccording to exemplary, non-limiting embodiments, the sub-videos which are synchronized with subtitle modules displayed in sub-windows and marked with user's synchronized cognitive states may be added correlated notes, comments, and/or sketch.\nIn related art, for example, neuroscience shows that all human mental functions are based on our memory or insights including declarative and non-declarative memory or implicit and explicit memory.  See e.g., Eric R. Kandel, \"We are what we\nremember: memory and the biological basis of individuality\", Public Lecture Royal Society, which is incorporated by reference for its helpful background.\nThe process to build up insights (or process of learning) may include: sensing the meaningful information into working memory, establishing correlations or chunks and making a repetition or rehearsing then distributing and storing this\ninformation in different areas of the brain through biological processes.  The process of comprehensive learning is the process to build new long-term memory from new short-term memory and the process requires at least one of the three conditions: first,\ncorrelation establishment, second, a repetition and/or emotional stimulation see e.g., Frank Longo, \"Learning and Memory: How it Works and When it Fails\", Stanford University, and Eleanor Maguire, \"The neuroscience of Memory\", The Royal Institution,\n\"Learning How to Learn,\" Barbara Oakley, and \"How We Learn Versus How We Think We Learn\" Robert Bjork, Distinguished Research Professor in the UCLA Department of Psychology, which are incorporated by reference for their helpful background.\nFor example, U.S.  Pat.  No. 9,711,056 to Nguyen (same inventive entity) describes capturing, detecting and identifying different types of emotional stimulation generated by human's organs while the human is exploring and observing the\nenvironment, incorporated by reference for its helpful background.\nHowever, there is a need to build an apparatus, a method, and a system to capture the observation and the emotions, display the captured material, build correlations between the observation and insights, perform repetition and monitoring the\nemotional stimulations of the learning process to enhance the ability of human learning.\nAnother example of learning enhancement by capturing the information of the process with synchronized emotional information then reviewing for improvement of the process is provided below.\nA golf teacher can set up a camera to capture the swing of golf learners.  Then, the teacher can use a software to analyze the motion of golfer's forehead to determine the quality of a swing, a chipping, or a putting.\nHowever, the result of this method has some limits because the motion of the head is small and vary depending on the distance from camera position to the head which the player usually move from shot to shot.\nThere is a need in the art to improve this process.  According to an exemplary embodiment, a camera is placed on the head of a golfer and the camera captures what the golfer observes in which the golf ball is the most important element. \nAccordingly, the golfer should keep observing and keep the distance from his forehead to the ball stable until the club head impacts the ball.\nAlso, there is a need to capture synchronized golfer's emotional signals during his or her swing, and then, analysis and the learning process for the golfer can be improved during the review process after the round of the game has been played. \nIn an exemplary embodiment, information gathered by the camera reflects the concentration of a golfer during his swing or putting which very much influences the quality of the swing or the putting.\nAs described in the U.S.  Pat.  No. 9,711,056, mentioned above and incorporated herein by reference, human cognitive state may be captured based on emotional and/or sensory data obtained from a user and synchronized with the environmental data. \nFor example, FIG. 1A is a view illustrating a device which captures sensory and emotional data according to an exemplary embodiment.\nAs shown in FIG. 1A, one or more cameras 11 may be provided on a headset 1 according to an exemplary embodiment.  That is, a left camera, a central camera, and a right camera (not shown) may be provided to capture visual data and/or audio data\naccording to an exemplary embodiment.  In an exemplary embodiment, one video camera 11, which includes a microphone to capture audio data, may be provided on a front of the headset 1.  These are provided by way of examples and not by way of a limitation. One of ordinary skill in the art would readily appreciate that visual data and/or audio data may be captured with a personal device such as a user's personal data assistant or a cellular telephone.  Additionally, one of ordinary skill in the art would\nreadily appreciate that any number of cameras and/or microphones may be used and that the visual data and/or audio data may be provided by a single camera or by a plurality of cameras, by a separate microphone or a plurality of microphones.  The captured\nvisual and audio data (VI) may then be transferred to an electronic board 10, which includes at least a memory coupled with a processor (not shown).\nIn an exemplary embodiment, the electronic board 10 may process sensory information and emotional information to generate cognitive state of a user.  In yet another exemplary embodiment, the generated cognitive sensory information may be\ntransmitted to another remote device for storage, monitoring or further processing via a communication interface (not shown) provided on the headset 1.  For example, the headset 1 may include a communication interface (e.g., a network card, an antenna,\nand other interfaces known to one of ordinary skill in the art or later developed) to transmit the data wirelessly e.g., a Bluetooth, Infrared, WiFi, and/or a cellular network to a remote server or a cloud for further storage, processing or monitoring\nand co-supervising.  The communication interface may be built into the electronic board 10 or may be provided as a separate device on the headset 1.\nIn an exemplary embodiment, one or more emotional sensors or cognitive state sensors 13 are further provided on a headset 1.  While FIG. 1A depicts four cognitive state sensors, this is provided by way of an example and not by way of a\nlimitation.  One of ordinary skill in the art would readily appreciate that a single sensory or cognitive state sensor may be used but preferably multiple cognitive state sensors are provided to capture cognitive state of a user.  The cognitive state\nsensors 13 may be provided on both sides of the headset 1.  In an exemplary embodiment depicted in FIG. 1A, only one side of the user's head is shown but the other side may also include four cognitive state sensors 13 that detect the cognitive state of\nthe user.  That is, in an exemplary embodiment, cognitive state is obtained from multiple sensors 13 by detecting activities in various parts of the brain.\nAccording to an exemplary embodiment, raw EEG signals obtained from the cognitive state sensors are combined and synchronized with the video and audio signals.  In an exemplary embodiment, the raw EEG signals embody cognitive distinctive\ninstinct components.  In an exemplary embodiment, EEG signals are filtered at distinctive reference signals to detect ET 10 component for evident states towards explorative/learning observations, ET 9 for confident states towards evident observations to\nobtain attentive cognitive state of the user, as explained in greater detail below.  The output of the filter is at different levels based on different attentive state of the user.  That is, the EEG frequency and amplitude change based on user's\nemotional level or cognitive state.  In an exemplary embodiment, inactivity means that the amplitude of all of the EEG components at a identified frequency are below a predetermined threshold value.  Accordingly, in an exemplary embodiment, a\ndetermination can be made that a user is sleeping or not paying attention.  According to an exemplary embodiment, a plurality of cognitive state sensors 13 generate respective channel signals (ET9-ET10 for example), as explained in greater detail below\nwith reference to FIG. 1B.  The respective channel signals are used to determine the cognitive state of a user wearing the headset 1.\nEEG is provided by way of an example and not by way of a limitation.  According to an exemplary embodiment, when the discrimination between certain EEG components and referenced signals are above a predetermined threshold value or above\npredetermined respective threshold values that means that the user is at an identified state such as evident (correlated, known), confident (well-correlated, experienced) and so on.  For some examples, see U.S.  Pat.  No. 9,711,056, which is incorporated\nby reference.\nAccording to an exemplary embodiment, the cognitive state sensors 13 may be positioned around the frontal lobe of the user to detect executive emotions of the user such as the user being desired, concerned, evident or confident.  For example, as\nshown in FIG. 1 B, five cognitive state sensors 13 (S1-S5) are placed around the frontal lobe of the user and output respective sensory signals ss1-ss5.  In an exemplary embodiment, if the ET 10 signals detected from sensors S1 and S3 output respective\nsignals above predetermined respective threshold values, a determination can be made that the user feel evident, e.g., the observation is known.  If the detected ET 11 signals from sensors S2, S4, and S5 output respective signals above predetermined\nrespective threshold values, a determination can be made that the user feel comfortable and/or expected a good result.\nIn an exemplary embodiment, because the user reads text from top to bottom (i.e., from the upper line to the lower line), therefore signals of voice which are interpreted synchronously with the text lines.  As such, in an exemplary embodiment\ndepicted in FIG. 1B, audio signals i.e., voice output, is displayed in a vertical line(s) and the timeline is also displayed in a vertical direction, for consistency with the text.\nFIG. 1B, according to an exemplary embodiment, depicts voice signals captured for a duration of 30 seconds.  For example, a timeline 14 is depicted on the left side of FIG. 1B and is split into intervals depicted by 15 seconds point, 10 seconds\npoint, 0 seconds point and -15 seconds point.  The 15 seconds mark depicts voice data already played and the -15 seconds mark depicts voice data to be played in 15 seconds.  0 seconds mark indicate the current position of the voice data being output. \nFIG. 1B further shows voice signals 16 that has 2 durations at low level, from -2s to +1s and from +9s to +11s, which would indicate a period of silence or a pause between the spoken words.  In an exemplary embodiment, these durations separate the\ncontinuous voice signals into modules which embody independent meanings.  According to an exemplary embodiment shown in FIG. 1B, voice data 16 is separated into three separate voice modules, V1, V2, and V3.  In an exemplary embodiment, the voice data 16\nis split into three voice modules V1, V2, and V3 based on natural breaking points in a recorded voice such as when a pause is made or when a sentence is finished, which can be detected based on recorded voice intonation, which will correspond with an\namplitude.\nAccording to an exemplary embodiment, based on the split voice modules or segments V1, V2, V3, voice to text conversion is performed.  That is, voice data is converted into text data and may be displayed to the user.  In an exemplary embodiment,\none text box is provided for each voice segment or module.  For example, as depicted in FIG. 1B, the text, obtained from converting the voice signal, is displayed in respective text boxes TB1-TB3.  That is, the first text corresponding to the converted\nvoice segment V1, is displayed in the text box TB1; the second text corresponding to the converted voice segment V2, is displayed in the text box TB2; and the third text corresponding to the converted voice segment V3, is displayed in the text box TB3. \nIn an exemplary embodiment, text output corresponding to the converted voice is placed into three corresponding windows, which are TB1, TB2, and TB3 depicted in FIG. 1B.  In an exemplary embodiment, the number of sub-windows being displayed will\ncorrespond to the number of voice segments generated during the division of continuous voice output.  As shown in FIG. 1B, cognitive state of the user is also displayed as five cognitive state signals (ss1-ss5) 17 obtained from the sensors S1, S2 .  . .\nS5.  These cognitive state signals are also displayed synchronized with voice signal in the vertical direction.  According to an exemplary embodiment, the cognitive state signals ss1-ss5 are also split into segments corresponding to the voice segments\nV1-V3.  That is, in an exemplary embodiment, the cognitive state signals are analyzed in segments which correspond to the voice segments to determine an emotional state of the user.\nAs shown in FIG. 1B, E1 is a cognitive state obtained from analyzing signals ss1, ss2, ss3, ss4, and ss5 produced at time 15 sec to 10 sec and corresponding to the voice segment V1, the text of which is displayed in the TB1 box.  In an exemplary\nembodiment, E1 is interpreted to correspond to an emotional state 1 and can be marked with a first color (e.g., blue) to indicate that the user is somewhat sleepy or unsure of at least some of the material corresponding to the voice segment V1.  That is\nE1 indicates a state in which the user appears to be somewhat confused (not entirely confident) with the material in the voice segment V1.  E2 is the cognitive state obtained from signals ss1-ss5 recorded during the time the user hears voice V2 with\ncontent displayed in TB2.  E2 is interpreted to correspond to an emotional state 2 and can be marked with a second color (e.g., green) to indicate that the user knows and/or understands the material (voice segment V2).  E3 is the emotional state obtained\nfrom signals ss1-ss5 recorded during the time the user hears voice V3 with content displayed in TB3.  E3 is interpreted to correspond to an emotional state 3 and can be marked with a third color (e.g., red) to indicate that the user try to focus but does\nnot yet understand the material (voice segment V3).  The marking depicted in FIG. 1B are provided by way of an example only and not by way of a limitation.  According to another exemplary embodiment, the text in the respective text boxes TB1-TB3 can be\ncolor coded based on the determined emotional state (E1-E3) of the user.  As an alternative, various marking and indicators could be used to depict the user's state including % signs, color shading within various colors used and so on.  According to an\nexemplary embodiment, the voice signal 16 may be a voice of a presenter giving a lecture in a classroom for example.  Contents depicted on a blackboard or projected onto screen maybe recorded as visual data and stored in synchronization with the audio\nsignals output by the presenter, for example.\nAs shown in FIG. 1C, the presenter may be giving a lecture presenting certain content or information to a user.  The presenter may show an example 18 of the content being presented, on a blackboard.  A plurality of students may be wearing\nheadsets 1 and observing the lecture.  Each one of the headset 1 worn by each or some of the plurality of students may output respective cognitive state of the respective user for each portion of voice signal of the presenter.  The plurality of cognitive\nstates of the plurality of users/students may be combined to provide output to the presenter such as this portion of the lecture is confusing, this portion is boring, or this portion is well understood.  Using this feedback, the lecturer may improve his\nor her presentation material by simplifying concepts that confuse the students and/or shortening presentation of the contents that is understood well.  These are provided by way of an example only and not by way of a limitation.\nFIG. 1D is a view illustrating reviewing, rehearsing, and consolidating of a recorded content such as a lecture based on synchronized classified cognitive states of a user, according to an exemplary embodiment.  According to an exemplary\nembodiment depicted in FIG. 1D, a user or a student is reviewing and/or studying the lecture using a large display 19a such as a television and a separate remote controller such as a touch screen or a sketch pad 19b.  The touch screen 19b may include an\niPad, a mobile terminal, a universal remote controller, and so on.  These are provided by way of an example and not by way of a limitation.  According to an exemplary embodiment, the touch screen 19b is mainly for input of notes and/or sketches, for\ncontrolling and/or navigating the display device 19a, and is not primarily intended for displaying images and/or video.  According to an exemplary embodiment, it is especially convenient when working in groups and/or working at home where a big screen is\navailable.\nAs shown in FIG. 1D, the display 19a outputs a playback of video data captured during the lecture in a left portion of the screen, the converted corresponding voice data may be displayed as text segments in the middle of the screen, as well as\noutput via a speaker in its audio form, and user notes captured during the initial lecture and/or made during review may be depicted on a right side of the screen on the right portion of the screen, which will be explained in greater detail below with\nreference to FIGS. 8A-9C.\nFIG. 2 is a block diagram illustrating a system capturing lecture material and sensory data according to yet another exemplary embodiment.  In FIG. 2, the user records a lecture e.g., topics and/or notes provided by the lecturer on a blackboard\n21, for example.  Additionally, the user may also record audio data such as explanations provided by the lecturer along with the illustrations depicted on the blackboard 21.  The user may record via one or more of a plurality of personal devices 22a .  .\n. 22n such as a smart phone 22a, a personal computer or a notebook 22b, a video recorder 22c, and a camera 22n.  These devices are provided by way of an example only and not by way of a limitation.\nIn an exemplary embodiment, the audio and video data may be output directly to a server 24.  The audio and video data is output to a server 24 with a corresponding time stamp(s) e.g., every 10 seconds of recording is transmitted to a server 24\nvia a network such as Internet, WiFi, Bluetooth, and so on.  Although the server 24 is depicted in FIG. 2 as a separate device, the server 24 may be located in a personal computer 22b or some other personal device 22a .  . . 22n depending on the\nprocessing power and memory of the personal device.\nThe user may be wearing a headset 23, which monitors the user's cognitive state.  The headset 23 is provided by way of an example only and not by way of a limitation.  In an exemplary embodiment, the user may be wearing another device that would\ninclude a number of sensors to measure the cognitive state of the user via sensors described with reference to FIG. 1A.  In an exemplary embodiment, signals detected by the sensors are used to determine the cognitive state or the emotional state of the\nuser.\nIn an exemplary embodiment, as explained above with reference to FIG. 1B, a human brain outputs low explorative signals while the user is relaxing and not concentrating (ET9-ET10-ET11-ET12).  Low explorative signals indicate that the user is not\nactively learning.  On the other hand, in an exemplary embodiment, amplitude at identified frequencies of explorative signals change as the user is interested and paying attention (ET1-ET12).  According to an exemplary embodiment, different amplitude\nbands (at different identified frequency bands) provide for detecting different cognitive or emotional state of the user i.e., determine variations in the cognitive or emotional state of the user.\nIn an exemplary embodiment, 8 channel EEG signals may be captured from the user and based on these captured signals, it may be determined whether the user is paying attention and the level of understanding the material being presented For\nexample, whether the user is sleepy, wandering, not paying attention to the topic (may have a strong sense of being bored or tired).  8 channel EEG signals are provided by way of an example only and not by way of a limitation.\nIn an exemplary, the sensory data may be saved in a memory card within the headset 23.  In another exemplary embodiment, the sensory data may be periodically output via a network to the server 24.  By way of an example, the network may include\nInternet, WiFi, Bluetooth or even a wired network e.g., the headset is connected via a wire to the server 24, which may be located in the personal device 22a .  . . 22n.  In an exemplary embodiment, the sensory data e.g., accumulated 10 seconds interval,\nby way of an example, is output to the server 24 with a time stamp for further processing.\nThe server 24 includes a processor and a memory, as detailed below with reference to FIG. 3.  In an exemplary embodiment, the server 24 may compare time stamps of the audio/video data (hereinafter referred to as contents or environmental data)\nand the sensory data to generate cognitive or emotional status of the user, which may then be stored in a memory 25.  The memory 25 may include one or more databases, internal or remote to the server 24.\nIn an exemplary embodiment, as shown in FIG. 3, a server 30, is a review and/or processing apparatus, which includes a processor 31, which may be a central processing unit (CPU), which controls the apparatus and its hardware components and\nexecutes software instructions stored in one or more memories such as a memory 34.  By way of an example, the processor 31 may also include a random access memory (RAM), a read only memory (ROM), one or more graphical processes, interfaces, and so on. \nComponents of the processor 31 may be connected to each other via a bus.  The processor 31 is further connected to and controls a display 32, which outputs recorded or original video signals in various forms and formats.  The display 32 includes a\nspeaker which outputs an audio sound.  This is provided by way of an example and not by way of a limitation.  Multiple speakers may be provided and maybe external to the display 32.  The processor 31 may be connected to a network interface or a network\ncard 33, which may include a WiFi chip, a Bluetooth chip, wireless network chip, and so on.  The network card 33 may further include one or more ports for wired connections.  Additionally, the apparatus 30 may include a memory 34, which may store one or\nmore of executable instructions which when executed by the processor 31 cause the processor to control the apparatus 30 and its components.  The memory 34 may further store audio and video data (contents) generated by one of the devices 22a to 22n (see\ne.g. FIG. 2).  The apparatus 30 may further include a user interface 35, which may include buttons, keyboard, a mouse, a USB port, a microphone, a gesture sensor, and so on.  The user interface 35 receives user input in various formats such as gestures,\naudio via a microphone, keyboard, mouse, touch screen, and so on, provided by way of an example and not by way of a limitation.\nIn an exemplary embodiment, the processor 31 compares time stamps of the audio/video data with time stamps of the sensory data and generates a timeline for the contents.  FIG. 4 is a flowchart illustrating a method of generating a timeline\naccording to an exemplary embodiment.  In an exemplary embodiment, visual, audio, and cognitive data may be synchronized by a clock of the system (controlled by CPU).  The system captures synchronized visual, audio, and cognitive information in real-time\nthen stores the synchronized raw material in a memory card, by way of an example.  When reviewing, a software application filters from the raw cognitive information at different reference frequencies to detect cognitive states at different durations in\nreal-time.  Based on the synchronization, the system can infer different cognitive states of user along the lecture in real-time using the software application embodied on a non-transitory computer-readable medium and executed by a processor.\nAlso, in an exemplary embodiment, during the reviewing, the video and analyzed user's cognitive statuses maybe prepared in advance and not in real time.\nAs shown in FIG. 4, in operation 401a, the apparatus receives environmental data such as audio and/or video data and in operation 401b, the apparatus receives the sensory data from the sensors, for example, worn by the user.  These are provided\nby way of an example only and not by way of a limitation.  According to yet another exemplary embodiment, the video and/or audio contents may be provided together with the sensory data in a predetermined time intervals such as 10 second intervals.  For\nexample, a set of (V, A, E) may be synchronized by sampling frequency where V is visual data, A is for audio data, and E is for emotional state of the user.\nIn operation 402, the received contents (video/audio data) is split into segments based on continuity of the voice signal.  For example, the processor determines where a pause is made or an end of a sentence based on voice intonation.  According\nto an exemplary embodiment, maximum length for a segment may also be set e.g., 5 seconds.  In an event, a pause or an end of sentence is not found in voice data of one minute duration, the voice is segmented at five seconds point (five seconds interval\nblocks).  This is provided by way of an example only and not by way of a limitation.  In an exemplary embodiment, \"evident\" and \"confident\" frequency is used to detect explorative cognitive states of a user.  These two main signals reflects user's\ncorrelations of recent observation and insights.  For example, known or unknown or evident or strange states are being used.  In an exemplary embodiment, emotional signals which relate to survival and reproductive area (e.g., ET1 to ET 8) like, love,\nscared, closed, open are not being used.\nIn operation 403, the cognitive state of the user is determined (ET9-ET10) based on the processed sensory data for each segmented portion of the data i.e., for each piece of contents.  As explained above, when the sensors on a certain part(s) of\nthe brain output signals of a first amplitude (small amplitude) in one or more frequency bands, the apparatus may determine that the user is confused and/or scared and/or concerned.  On the other hand, when the sensors on another part of the brain output\nsignals with a second amplitude (large amplitude), the apparatus may determine that the user is confident.  According to an exemplary embodiment, signals with high amplitude in a first frequency band and low amplitude in a second frequency band may\nindicate a confused cognitive state of the user, whereas signal with low amplitude in the first frequency band and high amplitude in the second frequency band may indicate confident state of the user.  If all of the sensors produce signals of the first\namplitude, this may indicate that the user is asleep or not paying attention.  This is provided by way of an example only and not by way of a limitation.\nFor example, according to another exemplary embodiment, the sensory data is used to determine that the user is distracted (ET-1).  For example, the headset 1 may detect that the user is looking at the blackboard but his mind is wandering away\nfrom the presentation topic or is elsewhere (sleeping).  Camera still is recording the lecture.  In addition, the apparatus can mark this duration (this time segment) to help the user easily recognize the portion of the lecture to review i.e., what the\nuser has missed in classroom.  According to yet another exemplary embodiment, if the sensory data trigger other emotions such as people, food, and/or things outside of the audio/video data, the cognitive state of the user may be determined to be\ndistracted with outside thoughts and not paying attention to the lecture.\nIn operation 404, the audio/video data (contents) is synchronized or linked with a corresponding determined cognitive state and (optionally) the segmented audio data may be converted into text for a display.  In operation 405, a timeline to be\ndisplayed is generated, which depicts the cognitive or emotional state of the user, optionally along with a corresponding portion of the audio/video data or contents.  In an exemplary embodiment, the timeline may be generated for playing back the\naudio/video contents obtained during the lecture.  The timeline may be color coded based on the determined cognitive or emotional state of the user.  That is, a portion of the timeline may be displayed in green for the user knowing the material well,\nwhereas another portion of the timeline may be displayed in red for the contents of the lecture the user is confused about or has not paid attention to.  This is explained in greater detail below by way of an example.\nFIG. 5 is a view illustrating contents displayed with corresponding cognitive states of a user according to an exemplary embodiment.\nAs shown in FIG. 5, the contents 51 is displayed to the user via a display together with a timeline 52, which is known in the art to show the time point of the contents currently being displayed.  The contents 51 may include video and audio\ncontents.  Additionally, a timeline 53 may be provided and displayed to a user via a display.  The timeline 53 depicts the cognitive or emotional state of the user in correspondence with the audio and/or video content.  By way of an example, the timeline\n53 depicts the cognitive state of the user synchronized with the displayed contents.  The cognitive state of the user is selected from among a disinterested state 54 in which the user is not interested or asleep, a confident state 55 in which the user is\ncomfortable with the output contents, and a confused state 56 in which the user does not understand the material provided.  These are provided by way of an example only and not by way of a limitation.  According to another exemplary embodiment, color\ncoding for various cognitive states may be used and the degree of confidence or confusion may correspond to a particular shade of the color.  For example, a dark red color on a timeline 53 may indicate that the user is very confused, a pink color may\nindicate that the user is just a little confused.\nAccording to another exemplary embodiment, timelines 52 and 53 may be combined into a single integrated timeline which illustrates a point in time currently being displayed with respect to the contents and the cognitive state of the user.\nAccording to various exemplary embodiments, the user may determine portions of the lecture (contents) that are confusing or were missed and practice his or her review session on these portions of the contents.  According to various exemplary\nembodiments, instead of reviewing the entire lecture (contents), the user may focus on the confusing portions or the portions that were missed.  Further, cognitive states of multiple users that attend the lecture can provide feedback for the lecturer. \nFor example, if 70% of the users are confused at certain portions of the lecture, the lecturer can determine to revise or simplify materials.  On the other hand, if the users (students) are not interested in yet another part of the lecture, the lecturer\ncan revise the lecture to capture user's attention.\nAccording to another exemplary embodiment, a review apparatus is configured to facilitate the study of the material after the lecture was presented.\nFIG. 6 is a block diagram of a review apparatus according to an exemplary embodiment.\nAs shown in FIG. 6, a review apparatus 60 includes a processor 61, a memory 62, and an outputter 63.  The memory 62 stores captured video data 62a, captured audio data 62b, captured sensory data 62c, which is captured 8-channels EEG signals. \nThe processor 61 executes a synchronizer 61a, which is configured to synchronize the captured video data 62a, with the captured audio data 62b and the captured sensory data 62c, as described below with reference to FIG. 7, by way of an example.  The\nprocessor 61 further executes a cognitive state determiner 61b, which determines the cognitive state of the user ET0-ET10, a timeline(s) generator 61c, which generates one or more timelines for audio and video contents and the sensory data using the\noutput from the synchronizer 61a and the cognitive state determiner 61b, and a speech to text converter 61d, described in greater detail below.  The environmental audio/video contents, along with cognitive states may be output to a user via an outputter\n63 which includes at least one display and speakers.\nFIG. 7 is a view illustrating synchronization of video and audio contents with sensory data by a review apparatus, according to an exemplary embodiment.  FIG. 7 depicts synchronization of raw input data.  As shown in FIG. 7, the video data 62a\nmay be displayed on a display along timelines 71 and 72.  The timeline 71 may be a timeline for a predetermined interval of the lecture.  According to an exemplary embodiment, the timeline 71 is presented for the current time point t.sub.n and five\nminutes before and after the current time point.  The timeline 72 is a timeline for an entire duration of contents.  According to an exemplary embodiment, timelines 71 and 72 show progression of the lecture currently being viewed or played, as is known\nin the art.  According to a depicted exemplary embodiment, the video data 62a is displayed at a time point tn and the timelines 71 and 72 show an indicator at a current time point tn.  Timeline 71 has a bar for a ten minute interval that is scrollable by\na user with points +5 minutes and -5 minutes from the currently displayed time point tn.  The timeline 72 has the same time point tn with respect to entire contents of the lecture.  In correlation with the video data 62a, audio data 62b is provided.  The\naudio data 62b currently output at the same time point tn and is provided for a 30 seconds interval corresponding to the 30 seconds interval of the video data 62a.  Additionally, a time line 73 shows an indicator at a current time point tn.  Further,\nsensory data 62c is synchronized with the audio data 62b and video data 62a.  According to an exemplary embodiment, originally captured 8-channel EEG signals, CH1-CH8 are synchronized by the synchronizer 61a.  The synchronized cognitive state of the user\n62c and synchronized audio 62b may be depicted on the same timeline 73 (-15 second-+15 second).  The cognitive state of the user is output in correlation with the video data 62a, audio data 62b.  The cognitive state determined from the sensory data 62c,\nmay be indicated on one or more timelines in a form of indicators, shading, coloring, and so on.  According to another exemplary embodiment, a dedicated timeline exclusively for the cognitive state of the user may be generated.\nAccording to an exemplary embodiment, the timeline 73 is for the nearest time around t.sub.n: 30 seconds.  The audio information in this 30 seconds period is displayed in text to enhance the ability of detailed recognition around the t.sub.n. \nThe timeline 71 is for the medium-portion of time around t.sub.n: 10 minutes.  The audio information in this 10 minutes period is displayed along with the cognitive state of the user to enhance the ability of wider recognition and navigation.  The\ntimeline 72 is for the full story video: The history of cognitive states along the original video is displayed to enhance the ability of performance evaluation and navigation.\nAccording to an exemplary embodiment, cognitive state signals 62c of the user embodies cognitive distinctive instinct components that are described and claimed in patent U.S.  Pat.  No. 9,711,056 for example, ET 10 component for evident states\ntowards explorative/learning observations, ET 9 for confident states towards evident observations, ET 4 for cognitive states towards observations to things, ET 3 for cognitive state towards observations to food.  Based on reference cognitive signals, the\nsystem can filter and detect the states of the user towards the cognition on the observation of the user e.g., video 62a and/or audio 62b.  By a way of example, by filtering the data 62c by ET 10 reference sample, the system can identify the states of\nuser throughout the lecture and grade them at a three levels system, by way of an example.  For example, a level 1 indicates that the user is not focused (bored, sleepy, his mind is wandering elsewhere), a level 2 indicates that the user is confident and\nhis knowledge is evident (correlated, known, understood), a level 3 indicates that the user is focused but is confused about the material (no correlations to insights yet).  This is provided by way of an example only and not by way of a limitation.\nFIG. 8A is a view illustrating a method of building correlations, according to an exemplary embodiment.\nAs shown in FIG. 8A, the user is in a process of reviewing or studying the contents of the original lecture.  According to an exemplary embodiment, the speech to text converter 61d splits audio data 81 such as the lecturer's voice into portions,\nas explained in greater detail above.  According to yet another exemplary embodiment, the audio data 81 is split into 5, 10, or 15 seconds intervals (portions before and after the current point (n): n-3, n-2, n-1, n, .  . . , n+3, where n is the current\nportion of the audio data being output such that 30 seconds of content nearest to the current time or current point (n) in a lecture is split) by a speech to text converter 61d.  In an exemplary embodiment, the audio contents are separated at intervals\nbetween two sentences (or between two meanings, two semantics and so on).  Based on this audio signal, the video is also split into discrete videos in meanings and the speech to text converter 61d interprets the voice to convert the audio data to text in\nportions n-3, .  . . , n+3.  The displayed video image 62a corresponds to the displayed text portion n and is the currently output video and audio data.  Additionally, the text for the audio contents before and after the currently displayed content is\nalso provided as n-3, n-2, n-1 and n+1, n+2, and n+3 segments or modules.  According to an exemplary embodiment, each portion is five seconds and the text 81 being displayed corresponds to the timeline 73.  In FIG. 8A, visible correlations 15 seconds\nbefore and after the recent meanings as shown by the timeline 73.  For example, the content of a 30-second video is provided to view in ONE SIGHT.  A 30-second video content is visible and is scrollable to be synchronized with video content.  The\nscrolling may appear automatically according to an exemplary embodiment.  According to an exemplary embodiment, a user may view full content of 30 second long video at one moment so as to build the correlations between previous 15 seconds content, recent\ncontent, and next (future) 15 second content.  In related art, a viewer only views a single image in a video at a time.  In an exemplary embodiment, however, a viewer may view, at the same time, notes on the right 87, previous 15 seconds of content\ndepicted in an upper portion of audio data 81, and next 15 seconds of content, depicted in a lower portion of the audio data 81, and may also view the video 62a.\nIn FIG. 8A, timelines 71 and 88 are provided for a 10 minutes interval (medium portion) of the cognitive state history around the currently played content.  According to an exemplary embodiment in FIG. 8A, the timelines 71 and 88 are ten minute\ntimelines around the recent content (original) and the recent content (being reviewed), respectively.  Accordingly, a view is provided with wider perspectives and correlations from recent content.  Similarly, according to an exemplary embodiment,\ntimelines 72 and 85 allows a viewer to view full story of user's states.  According to an exemplary embodiment, a more general view provide indications about specific areas of concern.  The display of an exemplary embodiment depicted in FIG. 8A trains a\nuser to have a full view of the story, from specific time points to a wider scene and from a wider scene to a full story (full lecture in an exemplary embodiment or even all the material being studied for a test or an exam).  According to an exemplary\nembodiment, entire contents or lecture(s) can be viewed at the same time.  As a result, the user may compare and evaluate progress by seeing different perspectives of the lecture.\nSuch that the timelines 71 and 88 are provided with t.sub.n being shown in the middle and a time bar with five minutes prior to the currently displayed time and five minutes after the currently displayed time are provided.  Further, the\ntimelines 72 and 85, generated by the timeline generator 61c, are displayed to the user and are the timelines for the entire cognitive state history along the entire lecture (entire content).  The currently output position or video portion is shown as\ntime t.sub.n and it is displayed with respect to the entire contents.  According to an exemplary embodiment, timelines 72 and 85 are similar to video controls known in the art but with cognitive states of a user marked thereon.\nFIG. 8A shows that the currently displayed image is approximately at the middle of the entire contents illustrated by the timelines 72 and 85.  As shown in FIG. 8A, the user may be studying and/or reviewing the contents (lecture).  Accordingly,\nthe headset depicted in FIG. 1 may also be worn by the user during the review time.  In an exemplary embodiment, additional sensory data generated during the review time is synchronized by the synchronizer 61a with the contents (video and audio) being\ndisplayed and the timeline(s) generator 61c may generate review timelines 85 and 88 which would depict the cognitive state of the user during the real-time of review.  According to an exemplary embodiment, the cognitive state of the user in recent or\ncurrent time (during the review time) is depicted in the timelines 85 and 88.  The review timelines 85 and 88 maybe updated in real time and displayed to the user so that the understanding of the material during the review or studying process is easily\ncomprehended by the user.  According to an exemplary embodiment, the cognitive state 86 of the user in real time may be output on a display on the fly.  Additionally, the synchronizer 61a may synchronize notes 87 taken by the user during the lecture\n(original presentation of the contents) and/or notes made during the review or study of the content, via a tool bar 89.  According to an exemplary embodiment, correlations from insights may be retrieved, displayed, and edited by a user.\nAccording to an exemplary embodiment depicted in FIG. 8A, the timelines 71, 72, 85, and 88 depict the cognitive state of the user.  However, this is provided by way of an example and not by way of a limitation.  According to various exemplary\nembodiments, only one timeline or a portion of the timelines may show the cognitive state of the user.  For example, only timelines 72 and 85 may show the cognitive state of the user during the original lecture presentation and during review,\nrespectively.  Further, FIG. 8A depicts the cognitive state of the user using coloring or shading but this is provided by way of an example only and not by way of a limitation.  One of ordinary skill in the art would readily appreciate that other\nmarkings, indicators, and coding techniques may be used to depict the cognitive state of the user.  According to an exemplary embodiment, a user may scroll via any one of the timelines to navigate to needed portions in the contents.  In an exemplary\nembodiment, by way on an example, a user may click or select a particular point on the timeline 72 or the timeline 85 and the UI will switch to a new screen using the selected point as point n.\nFIG. 8B is a view illustrating audio signals being displayed together with the text to enhance the ability of visible speech recognition, for example for a foreign language student, according to another exemplary embodiment.  In FIG. 8B, in\naddition to the text 81 being displayed, audio data 801 is provided to allow the user to listen in addition to viewing the contents in a text format.\nFIG. 9A is a view illustrating a method of building correlations after studying contents according to an exemplary embodiment.\nAs shown in FIG. 9A, after the user has reviewed the content, the timeline generator 61c updates the timelines 95 and 98 indicating that the cognitive state of the user is confident with a larger portion of the entire contents than prior to the\nreview.  For comparison, the timeline 94 shows that the user is confused with respect to a larger portion of the content 94a as opposed to after reviewing, the user is confused with respect to a smaller portion 95a depicted in the timeline 95.  In FIG.\n9A, the original content 92 is displayed with a timeline 93 being in a shade of color indicating that the user is confused as well as the timeline 94 showing a portion of the total contents that the user is confused to be about (28%) and indicated by a\ndifferent shading portion 94a.  After reviewing the contents, the user now understands most of the material or a significant portion of the material.  After the review, the timeline 95 depicts the cognitive state of the user as confident and the\nconverted text portions 91 are now depicted in a shade indicating confidence or user's understanding of the material/contents.  The current state 96 of the user is also shown as confident.  Combined notes 97 are shown to the user.  The notes 97a made\nduring the original presentation of contents and additional notes 97b made during the review or studying process are shown together, synchronized by the synchronizer 61a and may be further edited using the toolbar 99.\nFIG. 9B is a view illustrating building correlations with support tools according to an exemplary embodiment.  As shown in FIG. 9B, in addition to viewing the original contents 92 with timelines 93 and 94, the user is also provided with\ntimelines 95 and 98, as explained above.  Also, the user may be able to listen as well as view the words being spoken, as shown in the display of audio data 91.  The user may also scroll the content using arrow keys 9004.  By selecting or emphasizing a\ncertain word in the audio data 91, the user may be provided with a definition in a view 9003.  That is, in an exemplary embodiment, a view 9003 may include a search engine such as a dictionary 9001 or a tutorial (if a math concept is being studied for\nexample).  The type of a tutorial is depicted in a view 9001.  In an example provided, an English dictionary or an image for the word may be provided.  The definitions may be depicted in a notes section 9002.  Additionally, a timely 9003 may be provided\nto explain where in the text the term is found.  According to an exemplary embodiment, concepts and meanings of the lecture may be improved with the additional support tools such as tutorials and dictionaries.\nFIG. 9C is a view illustrating building correlations with support tools according to an exemplary embodiment.  In FIG. 9C, in a tutorial depicted in a view 9001, image is selected and as such, images illustrating the concept is depicted in the\nnotes section 9006.  According to an exemplary embodiment, a method to enhance the ability of visible correlation building to support foreign students, for example, in understanding English, is provided.  According to an exemplary embodiment, support\ntools are depicted as an image database and a definitions database.  This is provided by way of an example and not by way of a limitation.  According to an exemplary embodiment, other support tools may be provided including but not limited to textbooks,\ninternet searches, and even related exercises and homework.\nFIG. 10 is a flow chart illustrating a method of building correlations or an understanding of contents according to an exemplary embodiment.\nAs shown in FIG. 10, in operation 1001, the review apparatus retrieves from a memory captured contents and corresponding captured cognitive state.  The captured contents may include video and/or audio data.  The captured cognitive state of the\nuser may include ET0-ET10, as explained in greater detail above.  The determined cognitive state may be shown on one or more timelines corresponding to the viewed and/or listened contents.  In operation 1002, the contents is split into content submodules\n(parts) based on time, semantic meaning, or as specified by the user e.g., into five second time intervals.  The contents may be displayed to the user in parts as submodules.  Audio contents is converted, part by part, consecutively, into text, in\noperation 1003.  The cognitive state of the user during the review is determined with respect to the parts/submodules being output on a display and/or via a speaker, in operation 1004.  In operation 1005, the cognitive state of the user with respect to\nthe review material is generated and presented to the user.  According to an exemplary embodiment, cognitive state of the user during the review is determined and output so that the user can determine his further understanding of the material.\nFIG. 11 is a view illustrating a method of building correlations according to yet another exemplary embodiment.  As shown in FIG. 11, original data may be displayed in a first area 1101 of a screen 1100.  The original data 1101 may include any\none of video data and/or audio data recorded during a meeting, a conference, an experiment, a sport exercise, and so on.  In an exemplary embodiment depicted in FIG. 11, the original data 1101 is video data (without audio data) that is recorded during a\nlaboratory class such as a chemistry experiment.  The timelines 1104 and 1105 are similar to the timelines described above and are not further described here to avoid redundancy.  As shown in FIG. 11, a key map 1106 is provided indicating cognitive state\nof the user with respect to the shading technique used in the timeline 1105.  Although only timeline 1105 is shown shaded with emotional state of the user, one of ordinary skill in the art would readily appreciate that the timeline 1104 may also be\nshaded with the cognitive state of the user.\nThe segmented data is provided in a second area 1102 of the screen 1100.  According to an exemplary embodiment, the original data 1101 may be segmented according to a predetermined criteria.  By way of an example, the video data may be segmented\ninto a predetermined chunks of five seconds intervals, by way of an example.  One of the frames such as a first frame, a last frame, one of the frames in the middle may be used as an icon or image for a respective segment.  In FIG. 11, data segments 1107\nare provided which respectively correspond to five seconds of video data.  By clicking on each of the plurality of segments, the user may view the respective video segment.  The segments are shaded according to the cognitive or emotional state of the\nuser.  In FIG. 11, a third area 1103 of the screen 1100 may include user notes, professor notes, textbook links, internet links, guides, dictionaries, and tutorials regarding the original data.  Accordingly, various data types are segmented or split into\nparts based on various criteria.  For example, the data may be split into parts based on time intervals (each five seconds).  According to another exemplary embodiment, the meaning of the data may be analyzed and the data may be split into portions using\nspeech pauses or image recognition techniques that would recognize a scene change.  These are provided by way of an example and not by way of a limitation.\nAccording to yet another exemplary embodiment, the learning process is further enhanced via group learning.  For example, a social networking application feature is provided to enhance the learning process.  FIG. 12 is a view illustrating a user\nhome page of a social networking application according to an exemplary embodiment.  In an exemplary embodiment, a social networking application similar to Facebook or Twitter is utilized to enhance the learning process.\nAs shown in FIG. 12, a user home screen 1200 is displayed to a user via a mobile device such as a smart telephone for example.  In the user's home page screen 1200, a list of lectures stored in user's database 1200a are displayed.  The list is\naccompanied by a timeline 1200a-1 indicating when the lecture topic was obtained.  For example, as shown in FIG. 12, a list of lecture topics 1200a includes lecture topics N, N-1, N-2, .  . . N-8.  These lectures have a corresponding time in which they\nwere acquired.  As shown in FIG. 12, lecture topic N was obtained at time/date N, as shown in the timeline 1200a-1.  The lecture topic N-1 was obtained at a time/date N-1, as also shown in the timeline 1200a-1 and so on.  The lecture topic N is at a\ncurrently position of the timeline (top position) which indicates the currently studied lecture topic.  The topic N is summarized on the left portion 1201 of the home screen page 1200.  According to an exemplary embodiment depicted in FIG. 12, the topic\nN is a math lecture where Mary is the lecturer of the topic with the length of the lecture being 1 h 46 minutes and recorded on time/date N. That is, in an exemplary embodiment, the left portion 1201 of the home screen page 1200 provides metadata about\nthe current lecture topic i.e., topic N. Additionally, according to an exemplary embodiment, a timeline 1201a is provided with respect to the lecture topic N. The timeline 1201a shows the confidence levels at various portions of the lecture topic N. The\nuser may review the script of the lecture topic N, as shown in a lower portion 1202 of the user's home page screen 1200.  That is, the lower portion 1202 shows the lecture topic N converted to text.  The top text module 1202a corresponds to the current\nposition of the lecture topic N that the user is currently reviewing, as indicated by the current position indicator 1201b on the timeline 1201a.  According to an exemplary embodiment, the user can use the timeline 1201a to scroll to a desired portion in\nthe script 1202.  The user can review the lecture topic N without load the video of the lecture topic N by scrolling the timeline 1201a or by scrolling through the text modules 1202.  The user home page screen 1200 further includes a notes section 1203\nfor displaying user notes corresponding to the lecture topic N. In an exemplary embodiment, the notes section 1203 will display user's notes input during the review and/or during the original lecture based on a current portion of the lecture topic N\nbeing reviewed.  In other words, the notes section 1203 is synchronized with the current position indicator 1201b on the timeline 1201a and with the current text module being review 1202a.  The notes section 1203 may present notes made during the\noriginal lecture, during the review, and/or both depending on user defined settings.  Default settings would provide for displaying all available notes corresponding to the current position indicator 120 lb.\nWhen the user wants to view the features or metadata about the next topic, the user scrolls the list 1200a up/down, and the areas 1201, 1202, and 1203 will provide the contents of topic corresponding to the lecture topic on top of the list.\nAdditionally, as shown in FIG. 12, the home screen 1200 includes a display component or a display element for the friends of the user.  In an example, depicted in FIG. 12, friends 1-7 are shown in the display area 1205 and friends -1, -2, -3,\nand -4 are shown in the display area 1206.  As shown in FIG. 12, friends with no new posts are shown in the display area 1205 and friends with new posts i.e., the ones that the user has not yet seen are shown in the display area 1206.  The user has\ncommented on the posts of the friends 3, 5, 7, which is reflected with a display indicator such as a checkmark and the user has not commented on the posts of the friends 2, 4, and 6, which may be visible via another indicator such as an unchecked box.\nIn an exemplary embodiment, the posts are topic specific such that by viewing a particular topic, the screen will display posts corresponding to the topic and/or indicate friends that have comments on the particular topic.  In an exemplary\nembodiment, the user may swipe an icon of a friend to view one or more posts of that friend.\nAs shown in FIG. 13, the user selects a lecture topic N and the lecture topic N is depicted in the area 1301 in a form of a video, according to an exemplary embodiment.  Additionally, in the area 1301, a timeline 1301a may be displayed.  That\nis, according to an exemplary embodiment, when a user selects any point on the 1200a portion of the user home page screen depicted in FIG. 12, the video of the lecture topic N will play at a screen portion 1301, as shown in FIG. 13.  The user can\npause/play video at any time by touching an area 1301, 1302, or 1303.  The user can fast forward, rewind the video of the lecture topic N by scrolling 1302 or by manipulating the timeline of 1301a.  During playing of the video of the lecture topic N, the\nuser can view the notes made by her and additionally notes made by friends and comments of the friends by manipulating screen section 1305.\nAs depicted in FIG. 13, the audio of the video of the lecture topic N played in the area 1301 is converted into text and displayed in the area 1302.  In the area 1303, user notes (including friends' notes depending on user settings and/or\ndefault settings) are displayed.  In the area 1304, comments made by a user (including friends' comments depending on user settings and/or default settings) are displayed.  For example, friends n, n-3, n+1, n+3, n+5, n+7 may have comments that are\ndirected to the current portion 1302a of the lecture topic N, as indicated in a section 1305 via a display indicator of a checkmark, provided by way of an example only and not by way of a limitation.  Additionally, an area with replies 1306 may be\nprovided.  In an exemplary embodiment depicted in FIG. 13, friend n-1, n-2, n+2, n+4, and n+6 may have comments on comments of user's friend (replies).  In an exemplary embodiment, user comments displayed in the area 1304 may be a mathematical problem\ngiven by the lecturer during class to be solved at home.  As such, in an exemplary embodiment, replies from friends displayed in an area 1306 may include a solution to the problem and the user may check various solutions of his friends by reviewing the\nreplies Rn provided in the area 1306 of the lecture topic N screen depicted in FIG. 13.\nAccording to an exemplary embodiment, a user may browse through comments/replies of various friends by selecting a friend in the friend display area 1305.  By selecting one or more friends in the friend display area 1305, the user will see the\nreplies/comments made by the selected friends in the area 1306.  This is provided by way of an example and not by way of a limitation.\nFIG. 14 is a view illustrating a method of determining a skill level of a user with respect to a particular task according to yet another exemplary embodiment.\nAs shown in FIG. 14, a reference frame 1410 of images captured by a camera such as a camera on a headset 1, described above with reference to FIG. 1, may be used to determine motion of an object 141.  In the reference frame 1401, a stationary\nobject 141 may be displayed in the center.  That is, according to an exemplary embodiment, the stationary object 141 may be an object that the user is looking at such as a golf ball (when the user is playing golf), a point on the blackboard (when the\nuser is in a classroom), or a soccer ball (when the user is playing soccer).  These stationary objects are provided by way of an example and not by way of a limitation.  Base on analyzing a plurality of captured images, the motion of the object 141\ninside the frame 1410 is detected.  By analyzing the motion of the object 141 inside the frame 1410, an emotional state of the user may be detected.  For example, the detected motion of the object 141 reflects the motion of the forehead of user, which\nmay reflect the user's cognitive state.  According to an exemplary embodiment, the detection method may detect additional emotional/cognitive information of the user to set aside the noise influence in the EEG signals.\nIn an exemplary embodiment, a horizontal motion of the user's head is detected if the stationary object 141 is moved from the center in the directions 141a and 141b.  Additionally, a vertical motion of the user's head is detected if the\nstationary object 141 is moved from the center in the directions 141c and 141d, as shown in FIG. 14.  The motion is detected by analyzing a subset of image frames i.e., a portion of the video.  As explained in greater detail above, a portion of the video\nmay correspond to the chunks or split up blocks, described above with reference to the text to speech conversion.\nAccording to yet another exemplary embodiment, the portion of the video used to determine motion may be context specific.\nFor example, if the user is looking at a particular point on a black board, the motion of the user shaking his head or nodding his head may be detected.  That is, detected horizontal motion may indicate that the user is shaking his head and is\nthus, appear to be confused.  On the other hand, detected vertical motion of the head may indicate that the user understands the material being presented.  As such, the portion of the video to be used as a reference set will depend on the detected motion\nof the user's head i.e., until a pause for example.\nAccording to yet another exemplary embodiment, the portion of the video to be used to detect the motion may depend on a task being performed.  If the user is playing golf, the motion may help detect the quality of a task.  For example, when the\ngolfer is putting, the object 141 should remain stationary inside the reference frame 1410 throughout the putting process.  If the putting was a fail, the golfer can review and view that the stationary object 141 moved down, and the user can thus\ndetermine that he was heading up during his putting.  By analyzing his motion during the putting process, his putting may be improved.\nFIG. 15 is a flowchart illustrating a method of determining a skill level of a user with respect to a particular task according to an exemplary embodiment described above with reference to FIG. 14.\nAs shown in FIG. 15, in operation 1501, a stationary object is detected.  In an exemplary embodiment, a stationary object may be detected by using a variety of image recognition techniques known in the art such as a center point of where the\nuser is looking such as a golf ball or a blackboard.  In operation 1502, divide a video into chunks or segments based on a motion detected with respect to the stationary object.  According to an exemplary embodiment, the video is analyzed to determine\nmotion of the stationary object from image frame to image frame.  Each motion may be determined to be a chunk of a segment of the video.  For each determined chunk or segment, the type of motion is determined in operation 1503.  The type of motion being\ndetermined is based on a context.\nFor example, with respect to the blackboard example, the type of motion being detected maybe horizontal or vertical motions that would indicate whether the user is confused (shaking his head) or is confident (nodding his head).  With respect to\nthe golf example, the type of motion being detected may be the movement of the head with respect to the golf ball and the timing from the beginning of the movement until the golf ball is hit.\nIn operation 1504, a skill level or confidence level of the user is determined based at least in part on the determined type of motion.  For example, if the user is nodding his head and the sensory data indicates that the user's cognitive state\nis confident, these factors can be combined to determine that the user knows and understands the materials being presented.  On the other hand, if the user's swing is determined to be slow (taking a long time) and the golf ball is moving from frame to\nframe, these factors can be combined with sensory data (which may indicate that the user's memory is working hard) to determine low skill level.  Expert golfer would mostly utilize his motor skills as opposed to memory skill, swing fast, and keep his\neyes on the ball, for example.\nIn an exemplary embodiment, the determined skill level may be output to a user or operations may be modified based on the determined skill level in operation 1505.  According to an exemplary embodiment, additional tutorials or materials may be\npresented to the user based on the determined skill level.  As an alternative, the lecture may be presented at a slower speed with additional pauses.  According to yet another exemplary embodiment, a golf game may be color coded to indicate that the user\nneeds further practice with respect to a particular hole, a particular motion, a task, and so on.  The user may be directed to a certain area within the course to practice a particular task.\nAccording to yet another exemplary embodiment, when a complex task is being performed (e.g., operating a complex machinery or equipment), the determined skill level may be used to output alarms or even shut down the equipment if the skill level\nis inadequate or if it appears that the user is falling asleep, for example.\nAccording to an aspect of exemplary embodiments, a personal intuition-based cognitive assistant system is provided, which includes: one or more apparatuses configured to capture data from an environment comprising synchronized visual and audio\ninformation, at least one sensor configured to capture intuitive state or cognitive state of a user corresponding to the synchronized visual and audio information captured from environment and observed, listened by the user; at least one display\napparatus configured to display captured cognitive information and processed cognitive information comprising captured synchronized visual, audio information, captured user's intuitive state information and processed synchronized visual, audio\ninformation and processed user's intuitive state information.  The apparatus further includes a processor configured to: identify the distinctive intuitive states of the user based on the captured intuitive states or sensory data and the distinctive\nreference signals stored in a database, interpret the identified distinctive intuitive states of the user into identified distinctive visible intuitive marks, interpret the captured synchronized audio information into synchronized text and symbols, chunk\nthe serial of interpreted synchronized text and symbols into separated consecutive synchronized subtitle modules, divide continuous captured synchronized visual and audio information into discrete consecutive synchronized videos in corresponding to\nconsecutive synchronized subtitle modules, and a display, which displays separated consecutive synchronized subtitle modules in separate consecutive subtitle sub-windows within a script window.  The processor is further configured to mark synchronized\nsubtitle windows with the synchronized identified distinctive visible intuitive marks in corresponding to the identified distinctive intuitive states of the user.  The apparatus further includes a memory which stores the synchronized cognitive\ninformation including captured synchronized visual, audio, intuitive information, processed synchronized visual, audio, intuitive information.\nAccording to various exemplary embodiment, a user may readily appreciate topics that require further attention when studying, face-to-face meeting, video telephonic conversation and so on.  According to various exemplary embodiment, personalized\nnotes and comments made, thoughts formed based on environment observed and listened by the user can be synchronized with the environment.  These personalized conversational documents, thoughts may mimic the learning and the thoughts of the user's working\nmemory.  They are documented then they are stored, output, shared to assist the user in making various kinds of \"post-conversation\" information store and exchange.  The output may take various forms including email, social media, and so on.\nThe descriptions of the various exemplary embodiments have been presented for purposes of illustration, but are not intended to be exhaustive or limited to the embodiments disclosed.\nMany changes may be apparent to those of ordinary skill in the art without departing from the scope and spirit of the described embodiments.  The terminology used herein was chosen to best explain the principles of the embodiments, the practical\napplication or technical improvement over technologies found in the market place or to enable ordinary skill in the art to understand the embodiments disclosed herein.\nIn an exemplary embodiment, the cognitive module processor may be implemented on a tangible computer-readable medium.  The term \"computer-readable medium\" as used herein refers to any medium that participates in providing instructions to a\nprocessor for execution.  A computer readable medium may be, for example, but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any suitable combination of the foregoing.  More\nspecific examples (a non-exhaustive list) of the computer readable medium would include the following: an electrical connection having two or more wires, a portable computer diskette such as a floppy disk or a flexible disk, magnetic tape or any other\nmagnetic medium, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a memory card, any other memory chip or cartridge, an optical fiber, a portable compact disc\nread-only memory (CD-ROM), any other optical medium, punchcards, papertape, any other physical medium with patterns of holes, or any other medium from which a computer can read or suitable combination of the foregoing.\nIn the context of this document, a computer readable medium may be any tangible, non-transitory medium that can contain, or store a program for use by or in connection with an instruction execution system, apparatus, or device.\nAnother form is signal medium and may include a propagated data signal with computer readable program code embodied therein, for example, in a base band or as part of a carrier wave.  Such a propagated signal may take any of a variety of forms,\nincluding, but not limited to, the electro-magnetic, optical, or any suitable combination thereof.  The signal medium may include coaxial cables, copper wire and fiber optics, including the wires that comprise data bus.  The signal medium may be any\nmedium that is not a computer readable storage medium and that can communicate, propagate, or transport a program for use by or in connection with an instruction execution system, apparatus, or device.\nProgram code embodied on a computer readable medium may be transmitted using any appropriate medium, including but not limited to wireless, wire line, optical fiber cable, RF, etc. or any suitable combination of the foregoing.\nComputer program code for carrying out operations for aspects of the exemplary embodiments may be written in any combination of one or more programming languages, including an object oriented programming language such as Java, Smalltalk, C++,\n.Net or the like and conventional procedural programming languages.  The program code may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote\ncomputer or entirely on the remote computer or server.  The remote computer may be connected to the user's computer through any type of network, including a local area network (LAN) or a wide area network (WAN), or the connection may be made to an\nexternal computer (for example, through the Internet using an Internet Service Provider).\nThe computer-readable medium is just one example of a machine-readable medium, which may carry instructions for implementing any of the methods and/or techniques described herein.  Such a medium may take many forms, including but not limited to,\nnon-volatile media and volatile media.  Non-volatile media includes, for example, optical or magnetic disks.  Volatile media includes dynamic memory.\nVarious forms of computer readable media may be involved in carrying one or more sequences of one or more instructions to a processor such as a CPU for execution.  For example, the instructions may initially be carried on a magnetic disk from a\nremote computer.  Alternatively, a remote computer can load the instructions into its dynamic memory and send the instructions over a telephone line using a modem.  A modem local to a computer system can receive the data on the telephone line and use an\ninfra-red transmitter to convert the data to an infra-red signal.  An infra-red detector can receive the data carried in the infra-red signal and appropriate circuitry can place the data on the data bus.  The bus carries the data to the volatile storage,\nfrom which processor retrieves and executes the instructions.  The instructions received by the volatile memory may optionally be stored on persistent storage device either before or after execution by a processor.  The instructions may also be\ndownloaded into the computer platform via Internet using a variety of network data communication protocols well known in the art.\nThe flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods and computer program products according to various exemplary embodiments.  In this regard,\neach block in the flowchart or block diagrams may represent a module, segment, or portion of code, which comprises one or more executable instructions for implementing the specified logical functions.  It should also be noted that, in some alternative\nimplementations, the functions noted in the block may occur out of the order noted in the figures.  For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or two blocks may sometimes be executed in the reverse\norder, depending upon the functionality involved.  It will also be noted that each block of the block diagram and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special\npurpose hardware-based systems that perform the specified functions or acts, or combinations of special purpose hardware and computer instructions.\nThe terminology as used herein is for the purpose of describing particular embodiments only and is not intended to be limiting.  As used herein, the singular forms \"a\", \"an\" and \"the\" are intended to include the plural forms as well, unless the\ncontext clearly indicates otherwise.  It will be further understood that the terms \"comprises\" and/or \"comprising\" when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and/or components, but do\nnot preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof.\nThe corresponding structures, materials, acts, and equivalents of all means or step plus function elements in the claims below are intended to include any structure, material, or acts for performing the function in combination with other claimed\nelements as specifically claimed.\nThe description of the exemplary embodiments has been presented for purposes of illustration and description, but is not intended to be exhaustive or limiting in any form.  Many modifications and variations will be apparent to those of ordinary\nskill in the art without departing from the scope and spirit of the invention.  Exemplary embodiments were chosen and described in order to explain operations and the practical applications thereof, and to enable others of ordinary skill in the art to\nunderstand various embodiments with various modifications as are suited to the particular use contemplated.  That is, various modifications to these embodiments will be readily apparent to those skilled in the art, and the generic principles and specific\nexamples defined herein may be applied to other embodiments without the use of inventive faculty.  For example, some or all of the features of the different embodiments discussed above may be combined into a single embodiment.  Conversely, some of the\nfeatures of a single embodiment discussed above may be deleted from the embodiment.  Therefore, the present disclosure is not intended to be limited to exemplary embodiments described herein but is to be accorded the widest scope as defined by the\nfeatures of the claims and equivalents thereof.", "application_number": "16111675", "abstract": " A personal intuition-based cognitive assistant system includes one or\n     more components which may be worn by a user as a camera-headset, one or\n     more sensors that capture an intuitive state of the user, a camera that\n     capture videos, a processor that provides a cognitive navigating map for\n     the captured videos based on the captured intuitive states of the user,\n     and an inputter that input notes, comments to the videos linked by\n     cognitive navigating map, and a memory to store all components of the\n     information with links and identified cognitive map.\n", "citations": ["6190314", "9164590", "9355110", "9711056", "9721614", "9761277", "10013892", "20020032689", "20020197593", "20030027121", "20080275358", "20100046911", "20100100546", "20110085780", "20120047437", "20130063550", "20130211238", "20130260361", "20130308922", "20140033025", "20140067730", "20140223462", "20140310746", "20140323817", "20140347265", "20140350349", "20140351337", "20140366049", "20150016801", "20150051451", "20150052092", "20150079560", "20150213002", "20150297108", "20150297109", "20160042648", "20160077547", "20160217807", "20160379106", "20170007165", "20170061034", "20170236441", "20170315825", "20170330482", "20170337476", "20170351330", "20180032126", "20180032682", "20180303397"], "related": ["15870210", "62518824"]}, {"id": "20180374538", "patent_code": "10373684", "patent_name": "Semiconductor device", "year": "2019", "inventor_and_country_data": " Inventors: \nHayashi; Daigo (Tokyo, JP)  ", "description": "<BR><BR>CROSS-REFERENCE TO RELATED APPLICATIONS\nThe disclosure of Japanese Patent Application No. 2017-122246 filed on Jun.  22, 2017 including the specification, drawings and abstract is incorporated herein by reference in its entirety.\n<BR><BR>BACKGROUND\nThe present disclosure relates to a semiconductor device, and relates, for example, to an associative memory.\nThe storage device called an associative memory or a CAM (Content Addressable Memory) searches for a matching search word from stored data words, and, when a matching data word has been found, outputs its address.\nThe CAM includes BCAMs (Binary CAM) and TCAMs (Ternary CAM).  Each memory cell of the BCAM stores information \"0\" or \"1\".  In the TACM, each memory cell can store information not only \"0\" and \"1\", but also \"Don't Care\" (in this example, a symbol\n\"*\" is used).  The symbol \"*\" represents that any of \"0\" and \"1\" is possible.\nThe TCAM device is widely used for address search and access control of a router for network, such as the Internet.  To deal with the large memory capacity, the TCAM device generally has a configuration with a plurality of arrays and for\nexecuting search operations simultaneously for the arrays.\nThe TCAM device can compare input search data (input packet) and TCAM cell data at once.  Thus, it can perform the entire searches at a higher speed than the RAM (Random Access Memory).  However, a problem is that the consumption power is\nincreased due to generation of a search current at the time of the search.\nFor this point, when a non-matching result (MISS) is generated based on searching (pre-searching) of the initial-stage segment by a time division search, it is possible to attain low power consumption for the search by not executing searching\n(post-searching) of the post-stage segment (Japanese Unexamined Patent Application Publication No. Sho 62-293596).\n<BR><BR>SUMMARY\nHowever, from a viewpoint of the power consumption, it is preferred that the pre-search results in the non-matching in the time division search.  For example, when the time division search is executed for fields included in a table, such as an\nACL (Access Control List), power saving effects by the time division search are changed in accordance with the way of writing to the TCAM device even in the same table, because it is easy to obtain the \"MISS\" in some fields, while it is difficult to\nobtain the \"MISS\" in other fields.\nThe present invention seeks to solve the above problem.  It is accordingly an object thereof to provide a semiconductor device which can realize low power consumption.\nAny other objects and new features will be apparent from the descriptions of this specification and the accompanying drawings.\nAccording to an embodiment, there is provided a semiconductor device comprising: an N number of sub-blocks each of which includes a memory cell array; a setting register which specifies number of entry data for pre-searching, of first to N-th\nentry data which are divided and correspond respectively to the N number of sub-blocks; and a search data changing unit which changes a data arrangement order for search data input based on a value of the setting register.  A sub-block for pre-searching,\nof the N number of sub-blocks, searches for entry data which matches with data for pre-searching in accordance with the data arrangement order which has been changed by the search data changing unit, of entry data stored in each row of the memory cell\narray, in response to a search instruction, and outputs a search result representing matching or non-matching in association with each row.  A sub-block for post-searching, of the N number of sub-blocks, searches for entry data which matches with data\nfor post-searching other than the data for pre-searching of search data, of entry data stored in association with each row of the memory cell array, based on a search result of the sub-block for pre-searching, and outputs a search result representing\nmatching or non-matching in association with each row.\nAccording to an embodiment, the semiconductor device of the present disclosure can realize low consumption power. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nFIG. 1 is a diagram for explaining a configuration of a communication unit 1 based on a first embodiment.\nFIG. 2 is a diagram for explaining a configuration of a search memory 8 based on the first embodiment.\nFIG. 3 is a diagram for explaining a configuration of a TCAM device 12 based on the first embodiment.\nFIG. 4 is a circuit diagram illustrating an example of a configuration of a TCAM cell.\nFIG. 5 is a diagram illustrating a correspondence relation of storage contents of the X cell and the Y cell of FIG. 4 and TCAM data.\nFIG. 6 is a diagram for explaining a configuration of a segment (sub block) 12A based on the embodiment.\nFIG. 7 is a circuit diagram illustrating an example of a configuration of a search line driver 22.\nFIG. 8 is a circuit diagram illustrating an example of a configuration of a match amplifier.\nFIG. 9 is a diagram for explaining a search method of a search block 10 based on the first embodiment.\nFIG. 10 is a diagram for explaining a search timing of the search block 10.\nFIG. 11 is a diagram for explaining a maintenance operation of the search memory 8.\nFIG. 12 is a diagram for explaining a search method of a search block 10# based on a second embodiment.\nFIG. 13 is a circuit diagram illustrating an example of a configuration of a BCAM cell MC# of a BCAM device 12P.\nFIG. 14 is a diagram for explaining a search method of a search block 10A based on a modification of the second embodiment.\nFIG. 15 is a diagram for explaining a search timing of the search block 10A.\n<BR><BR>DETAILED DESCRIPTION\nDescriptions will hereinafter be made to preferred embodiments in reference to the accompanying drawings.  The same or corresponding parts in the illustrations are identified by the same reference numerals, and thus will not be described over\nand over.\n<BR><BR>First Embodiment\n&lt;Entire Configuration of Communication Unit 1&gt;\nFIG. 1 is a diagram for explaining a configuration of a communication unit 1 based on a first embodiment.\nAs illustrated in FIG. 1, the communication unit 1 is a communication device, such as a switch or a router.\nThe communication unit 1 includes a CPU (Central Processing Unit) 2, a transfer control circuit 4, a general memory 6, and a search memory 8.\nThe CPU 2 controls the entire unit.\nThe CPU 2 realizes various functions in cooperation with a program stored in the general memory 6.  For example, the general memory 6 can be configured with a DRAM (Dynamic Random Access Memory), to build an Operating System (OS) in cooperation\nwith the CPU 2.  The CPU 2 exchanges information with an adjacent communication unit, and maintains and manages information necessary for a transfer process.\nThe transfer control circuit 4 executes a transfer process for communication packets.  The transfer control circuit 4 includes a dedicated hardware, such as an ASIC (Application Specific Integrated Circuit) specialized for a transfer process or\nan NPU (Network Processing Unit).  The transfer control circuit 4 accesses the search memory 8, and acquires information necessary for the transfer process.\nDescriptions will be made to a case in which the TCAM device is used as the search memory 8, in this embodiment.\n&lt;Configuration of Search Memory 8&gt;\nFIG. 2 is a diagram for explaining a configuration of the search memory 8, based on the first embodiment.\nAs illustrated in FIG. 2, the search memory 8 includes a search key generation unit 29 which generates a search key from input data, a profile register 26, a plurality of search blocks 10, and a search result generation unit 28.\nThe search memory 8 is entirely managed in the unit of search blocks 10 having a certain grain size.  In this embodiment, descriptions will be made to a configuration in which a plurality of search blocks 10-1 to 10-n are provided.\nAs will specifically be described later, the configuration of each search block 10 includes a TCAM device 12, a setting register 16 for specifying number of entry data for pre-search, and a data changing unit 27 changing the data arrangement\norder based on the value of the setting register 16.\nThe profile register 26 has various settings necessary for the search.\nBased on information of the profile register 26, the search key generation unit 29 generates a search key from input data.\nThe search blocks 10-1 to 10-n are parallelly provided, and have a parallel search function.\nThe search result generation unit 28 gives priority to the search results from the search blocks 10, and generates the final search results.\nWhen a search command is issued, the search key generation unit 29 generates a search key based on received data and various information items, such as the profile register 26.\nThe search key generation unit 29 distributes the keys to the search blocks 10.  In each of the search blocks 10, the data changing unit 27 changes and outputs the data arrangement order of the search keys, based on a register value set in\nadvance in the setting register 16.  The search keys which have been changed in this manner have unique values for each search block 10.\n&lt;Configuration of TCAM Device 12&gt;\nFIG. 3 is a diagram for explaining a configuration of the TCAM device 12 based on the first embodiment.\nAs illustrated in FIG. 3, the TCAM device 12 has a time division search function.\nThe TCAM device 12 is divided into a plurality of segments (sub-blocks).  In this embodiment, it is divided into four segments (sub-blocks).\nAs an example, descriptions are made to a case in which it is divided into segments 12A to 12D.\nThe segments 12A to 12D are coupled from the initial stage to the post stage.\nEach of the segments 12A to 12D substantially has the same configuration, and has a memory cell array and a match amplifier.  The segment 12D of the final stage has a memory cell array, a match amplifier, and a priority encoder.\nThe match amplifier exists at least in each segment, and amplifies and outputs a judgment result regarding matching or non-matching indicated by match lines.\nThe priority encoder converts an input from the match amplifier into an address, and generates the final output.\nThe memory cell array has a plurality of \"se\" (Search Enable) terminals.  The pre-charging for the match line is performed for only in an asserted region based on the \"se\" terminals, thereby enabling to limit the search range.\nIn this embodiment, a flip flop FF1 for keeping a signal for activating the match amplifier is provided, as an example.  Specifically, when a search command is input to the flip flop FF1, the signal for activating the match amplifier is kept,\nand output to the match amplifier of each segment.\nThe segments 12B to 12D have a configuration, in which flip flops FF2 to FF4 are provided, and the search key can be kept until completion of a search process in the initial stage segment.  In this embodiment, a search key SB is input to the\nsegment 12A, a search key SA is input to the segment 12B, and search keys SC and SD are input to the segments 12C and 12D.\nA match amplifier output is coupled to the \"se\" terminal of the post stage segment.  As a result, only the entries matching in the initial stage segment can be assumed as a target to be searched.\nSpecifically, only the segment 12A of the initial stage is activated, to execute a first search.  A second search is executed only for the matching regions as a result of the first search in the segment 12B.  Similarly, a third search and a\nfourth search are executed.  It is possible to suppress consumption power of the segment of the post stage by an amount corresponding to the non-matching region as a result of the search of the initial stage segment.\n&lt;Configuration of Segment (Sub-Block)&gt;\n[Configuration of TCAM Cell]\nFIG. 4 is a circuit diagram illustrating an example of a configuration of the TCAM cell.\nBy reference to FIG. 4, the TCAM cell (referred to also a memory cell MC) includes two SRAM cells (Static Random Access Memory Cell) 11 and 12 and a data comparison unit 13.  The SRAM cell 11 is referred to also as an X cell, and an SRAM cell 14\nis referred to also as a Y cell.  The X cell 11 stores complementary 1-bit data (that is, when one is \"1\", the other one is \"0\") in a pair of internal storage nodes ND1 and ND1_n. The Y cell 14 stores complementary 1-bit data in a pair of internal\nstorage nodes ND2 and ND2_n. The TCAM cell is referred to also as an associative memory.\nThe TCAM cell is coupled to a pair of bit lines BL and BL_n, a pair of search lines SL and SL_n, a match line ML, and word lines WLX and WLY.  The pair of bit lines BL and BL_n extend in a column direction (Y direction) of the TCAM cell array 20\nof FIG. 6, and are commonly shared by a plurality of TCAM cells arranged in the column direction.  The pair of search lines SL and SL_n extend in the column direction (Y direction) of the TCAM cell array 20, and are commonly shared by the plurality of\nTCAM cells arranged in the column direction.\nThe match line ML extends in a row direction (X direction) of the TCAM cell array 20, and is commonly shared by a plurality of TCAM cells arranged in a row direction.  The word lines WLX and WLY extend in the row direction (X direction) of the\nTCAM cell array 20, and are commonly shared by the plurality of TCAM cells arranged in the row direction.\nThe X cell 11 includes inverters INV1 and INV2 and N-channel MOS (Metal Oxide Semiconductor) transistors Q1 and Q2.  The inverter INV1 is coupled between the storage node ND1 and the storage node ND1_n, to attain a forward direction from the\nstorage node ND1_n to the storage node ND1.  The inverter INV2 is coupled parallelly to and in a backward direction to the inverter INV1.  The MOS transistor Q1 is coupled between the storage node ND1 and the bit line BL.  The MOS transistor Q2 is\ncoupled between the storage node ND1_n and the bit line BL_n. The gates of the MOS transistors Q1 and Q2 are coupled to the word line WLX.\nThe Y cell 14 includes inverter INV3 and INV4 and MOS (Metal Oxide Semiconductor) transistors Q3 and Q4.  The inverter INV3 is coupled between the storage node ND2 and the storage node ND2_n, to attain a forward direction from the storage node\nND2_n to the storage node ND2.  The inverter INV4 is coupled parallelly to and in a backward direction to the inerter INV3.  The MOS transistor Q3 is coupled between the storage node ND2 and the bit line BL.  The MOS transistor Q4 is coupled between the\nstorage node ND2_n and the bit line BL_n. The MOS transistor Q4 is coupled between the storage node ND2_n and the bit line BL_n. The gates of the MOS transistors Q3 and Q4 are coupled to the word line WLY.\nThe data comparison unit 13 includes N-channel MOS transistors Q6 to Q9.  The MOS transistors Q6 and Q7 are coupled in series between a node ND3 as a coupling point to the match line ML and a ground node GND.  The MOS transistors Q8 and Q9 are\ncoupled in series between the node ND3 and the ground node GND, and are coupled parallelly entirely to the MOS transistors Q6 and Q7 which are coupled in series.  The gates of the MOS transistors Q6 and Q8 are coupled respectively to the storage nodes\nND1 and ND2.  The gates of the MOS transistors Q7 and Q9 are coupled respectively to the search lines SL and SL_n.\nFIG. 5 is a diagram illustrating a correspondence relation of storage contents of the X cell and the Y cell of FIG. 4 and the TCAM data.\nBy reference to FIG. 4 and FIG. 5, the TCAM cell can store any of three values, \"0\", \"1\", \"*\" (don't care), using the SRAM cells for 2 bits.  Specifically, when the storage node ND1 of the X cell 11 stores \"1\", and when the storage node ND2 of\nthe Y cell 14 stores \"0\", the TCAM cell stores \"0\".  When the storage node ND1 of the X cell 11 stores \"0\", and when the storage node ND2 of the Y cell 14 stores \"1\", the TCAM cell stores \"1\".  When the storage node ND1 of the X cell 11 stores \"0\", and\nwhen the storage node ND2 of the Y cell 14 stores \"0\", the TCAM cell stores \"*\" (Don't care).  When the storage node ND1 of the X cell 11 stores \"1\", and when the storage node ND2 of the Y cell 14 stores \"1\", it is not used.\nIn the configuration of the above-described TCAM cell, when the search data is \"1\" (that is, the search line SL is \"1\", and the search line SL_n is \"0\"), and when the TCAM data is \"0\" (the storage node ND1 is \"1\", and the storage node ND2 is\n\"0\"), the MOS transistors Q6 and Q7 are ON.  Then, the potential of the pre-charged match line ML is pulled down to the ground potential.  When the search data is \"0\" (that is, the search line SL is \"0\", and the search line SL_n is \"1\"), and when the\nTCAM data is \"1\" (the storage node ND1 is \"0\", and the storage node ND2 is \"1\"), the MOS transistors Q8 and Q9 are ON.  Then, the potential of the pre-charged match line ML is pulled down to the ground potential.  That is, when the search data does not\nmatch with the TCAM data, the potential of the match line ML is pulled down to the ground potential.\nOn the contrary, when the input search data is \"1\", and when the TCAM data is \"1\" or \"*\", alternatively, when the search data is \"0\", and when the TCAM data is \"0\" or \"*\" (that is, when both data match with each other), the potential (power\nsupply potential VDD level) of the pre-charged match line ML is maintained.\nAs described above, in the TCAM, unless the data of the entire TCAM cells coupled to the match line corresponding to one entry (row) matches with input search data, the charges accumulated in the match line ML are extracted.  Thus, though the\nhigh-speed search is performed in the TCAM, a problem is that the consumption current is large.\nFIG. 6 is a diagram for explaining a configuration of the segment (sub-block) 12A based on the embodiment.\nAs illustrated in FIG. 6, the segment 12A includes a TCAM cell array 20 (referred to also as a cell array), a writing driver 21, a search line driver 22, a match amplifier unit 23, a control logical circuit 24, and a reading circuit 25.\nThe segment 12A includes a word line driver (not illustrated) for driving the word lines WLX and WLY and an input/output circuit (not illustrated) receiving an input of a control signal or an address signal.\nThe TCAM cell array 20 includes TCAM cells arranged in matrix (m-rows, k-columns).  In this embodiment, in the cell array 20, the \"m\" number of rows (number of entries) is N, and the \"k\" number of columns (number of bits) is 40.\nIn a manner corresponding to each column of the cell array 20, there are provided a \"k\" (k=40) number of pairs of bit lines (from BL[0] and BL_n[0] up to BL[k-1] and BL_n[k-1]) and a \"k\" number of pairs of search lines (from SL[0] and SL_n[0] up\nto SL[k-1] and SL_n[k-1]).\nIn a manner corresponding to each row of the cell array 20, there are provided an \"m\" number (m=N) of match lines (from ML[0] up to ML[N]), a non-illustrative \"m\" number of word lines for X cell (from WLX[0] up to WLX[m-1]), and a\nnon-illustrative \"m\" number of word lines for Y cell (from XLY[0] up to WLY[m-1]).\nAt the time of writing, the writing driver 21 supplies writing data to each of TCAM cells through the pair of bit lines BL and BL_n. At the time of searching, the search line driver 22 supplies search data to each of the TCAM cells through the\npair of search lines SL and SL_n.\nThe control logical circuit 24 controls the entire operation of the segment 12A.  For example, at the time of searching, the control logical circuit 24 receives a search command, and outputs a control signal to the search line driver 22 and the\nmatch amplifier unit 23, thereby controlling the operation of the search line driver 22, the match amplifier unit 23, and the pre-charge circuit.  The control logical circuit 24 receives a read command at the time of reading, and outputs a control signal\nfor controlling the reading circuit 25.  As a result, it is possible to read and output entry data stored in the cell array 20.\nThe match amplifier unit 23 includes a plurality of match amplifiers MA corresponding respectively to the rows of the cell array.  At the time of searching, the match amplifiers MA detect whether corresponding TCAM cell data and a corresponding\npart of input search data match with each other, based on the potential of a corresponding match line ML.  In this embodiment, the match amplifiers MA include a pre-charge circuit for precharging the corresponding match line ML, at the time of searching.\nFIG. 7 is a circuit diagram illustrating an example of a configuration of the search line driver 22.\nAs illustrated in FIG. 7, the search line driver 22 outputs input search data \"skey [i]\" (i=0, 1, .  . . [k]) to the search line SL [i], when a search line enable signal \"sena\" is activated to an [H] level.  It outputs a signal obtained by\ninverting the logical level of the input search data \"skev[i]\" to the complementary search line SL_n[i].\nIn this embodiment, as an example, descriptions will be made to a case in which the search key SB is configured as the input search data \"skey[i]\" (i=0, 1, .  . . , k).\nSpecifically, the search line driver 22 includes AND gates 60[0] to 60[k] respectively corresponding to the search lines SL[0] to SL[k], AND gates 61[0] to 61[k] respectively corresponding to the search lines SL_n[0] to SL_n[k], and inverters\n62[0] to 62[k].\nThe search line enable signal \"sena\" is commonly input to the AND gates 60[0] to 60[k] and the AND gates 61[0] to 61[k].  Further, to the AND gate 60[i] (i=0, 1, .  . . , k), corresponding input search data \"skey\"[i] is input.\nAn output signal of the AND gate 60[i] (i=0, 1, .  . . , k) is transmitted to the search line SL[i].  To the AND gate 61[i] (i=0, 1, .  . . , k), a signal which is obtained by inverting corresponding input search data \"skey[i]\" is input.\nAccording to the above-described configuration, when, for example, the search line enable signal \"sena\" is activated to an [H] level, and when the input search data \"skey[i]\" is at an [H] level (\"1\"), the voltage of the search line SL[i] is at\nthe [H] level, and the voltage of the search line SL_n[i] is at an [L] level.  When the search line enable signal \"sena\" is activated to the [H] level, and when the input search data \"skey[i]\" is at the [L] level (\"0\"), the voltage of the search line\nSL[i] is at the [L] level, and the voltage of the search line SL_n[i] is at the [H] level.\nFIG. 8 is a circuit diagram illustrating an example of a configuration of a match amplifier.\nBy reference to FIG. 8, the match amplifier MA includes a P-channel MOS transistor 70 as a pre-charge circuit and inverters 71 to 74.\nAs illustrated, the MOS transistor 70 as a pre-charge circuit is included in the match amplifier MA.  However, the MOS transistor 70 may be provided outside the match amplifier MA.  The control logical circuit 24 outputs a match amplifier enable\nsignal \"mae\".\nAs described above, in this embodiment, the \"se\" terminal is provided for a pre-charge signal for precharging the match line.  In this case, an input from the \"se\" terminal is inverted by the inverter, and the inverted signal is input to the MOS\ntransistor 70 as a pre-charge signal.\nTherefore, as explained in FIG. 3, in the segment of the initial stage, when the [H] level (\"1\") is input to the \"se\" terminal as a search command, the entire match lines are pre-charged.\nIn the segment of the second stage or after, when the output of the match amplifier MA of the initial stage is at the [L] level, because the output from the match amplifier MA in the segment of the initial stage is coupled to the \"se\" terminal,\nthe match line on the same row in the segment of the next stage is not pre-charged.  When the output from the match amplifier MA of the initial stage is at the [H] level, the match line on the same row in the segment of the next stage is pre-charged. \nThus, the match amplifier output of the segment of the initial stage is coupled to the \"se\" terminal of the post-stage segment.  As a result, only the entries matching in the initial stage can be assumed as targets to be searched.  It is possible to\nsuppress consumption power of the post-stage segment by an amount corresponding to the non-matching region as a result of the search of the initial-stage segment.\nDescriptions will hereinafter be made to coupling of the above-described constituent elements.  The MOS transistor 70 is coupled between a corresponding match line ML and a power supply node for supplying a power supply potential VDD.  To the\ngate of the MOS transistor 70, a match line pre-charge signal as a inverted signal of the \"se\" terminal is input.  The match line ML is further coupled to the input node of the inverter 71.  The output node of the inverter 71 is coupled to the input node\nof the inverter 74.  The output node of the inverter 74 is coupled to the input node of the inverter 72 through the inverter 72.\nThe match amplifier enable signal \"mae\" and a signal which is obtained by inverting the logical level thereof by the inverter 73 are coupled to the driving power supply node of the inverters 71 and 72.  When the match amplifier enable signal\n\"mae\" is in a non-activated state ([L]) level, the inverter 71 is in a non-operation state, and the inverter 72 is in an operation state.  When the match amplifier enable signal \"mae\" is activated ([H] level), the inverter 71 is in an operation state,\nand the inverter 72 is in a non-operation state.\nDescriptions will be made to a circuit operation of the match amplifier MA.  If the match line pre-charge signal is activated ([L] level), the MOS transistor 70 is conductive.  As a result, the match line ML is charged (pre-charged) up to the\npower supply potential VDD.\nAfter the match line pre-charged signal is non-activated, the search line enable signal \"sena\" is activated ([H] level), thus inputting search data to the pair of search lines SL and SL_n. As a result, the potential of the match line ML depends\non the search result (a comparison result of a corresponding part of the input search data and the TCAM cell data).  That is, in the case of \"matching\" (hit), the potential of the match line ML is maintained as the power supply potential VDD ([H] level). On the other hand, in the case of \"non-matching\" (MISS), the charges of the match line ML are discharged to the ground node.  As a result, the potential of the match line is changed to the ground potential ([L] level).\nThe match amplifier enable signal is activated now ([H] level).  Then, the potential of the match line ML based on the search result is output through the inverter 71 and the inverter 74 as a match amplifier output signal \"mo\".  If the match\namplifier enable signal \"mae\" is non-activated ([L] level), the potential of the match line ML based on the search result is maintained by a latch circuit which is configured with the inverter 74 and the inverter 72.\n&lt;Search Method of Search Block 10&gt;\nFIG. 9 is a diagram for explaining a search method of the search block 10 based on the first embodiment.\nAs illustrated in FIG. 9, the search block 10 includes the setting register 16, a rearrangement control signal generation unit 15, rearrangement circuits 17 and 18, and the TCAM device 12.\nThe rearrangement control signal generation unit 15 and the rearrangement circuits 17 and 18 are provided to form a data changing unit 27.\nThe TCAM device 12 has a time division search function.\nThe rearrangement circuits 17 and 18 are crossbar switches.\nThe rearrangement control signal generation unit 15 outputs a control signal for instructing rearrangement to the rearrangement circuits 17 and 18 based on a register value of the setting register 16.\nIn this embodiment, the crossbar switches are used as the rearrangement circuits 17 and 18.  However, it is possible to use any circuit, as long as it can rearrange a region effective for the time division search into a particular position.  For\nexample, it is possible to use a configuration for switching upper bits and lower bits of data, or it is possible to use a rotation circuit.\nIn this embodiment, descriptions will be made to the configuration of the TCAM device as the search block 10, by way of example.  However, it is not limited to this.  Any other device is possible, as long as it has a data structure for which the\ntime division search can be performed.\nThe search block 10 has a data bus for use at the time of writing and searching and control system pins, as input pins.  The search block 10 has, as output pins, search result output pins and a read result output data bus.\nThe rearrangement circuits 17 and 18 perform rearrangement for the signals of the input and output data buses.  Specifically, they change the data arrangement order.\nThe rearrangement circuits 17 and 18 change the data arrangement order in accordance with a control signal generated by the rearrangement control signal generation unit 15.\nIn this embodiment, descriptions will be made to a case in which the TCAM device 12 is divided into four segments (sub-blocks).  Of the four segments (sub-blocks), the first segment (sub-block) is a target for the pre-search, and the second to\nfourth segments (sub-blocks) are targets for the post-search.\nIn this embodiment, the setting register 16 specifies the number of entry data for the pre-search, of the first to fourth entry data after divided in a manner corresponding respectively to the four segments (sub-blocks).  The setting register 16\nstores 2-bit data, as an example.\nThe rearrangement control signal generation unit 15 generates control signals S1 to S4 based on the data of the setting register 16.\nFor example, when the setting register 16 stores \"00\", the rearrangement control signal generation unit 15 outputs the control signal S1 to the rearrangement circuits 17 and 18.\nWhen the control signal S1 is input, the rearrangement circuits 17 and 18 output it as is, without changing its data arrangement order.\nWhen the setting register 16 stores \"01\", the rearrangement control signal generation unit 15 outputs the control signal S2 to the rearrangement circuits 17 and 18.\nWhen the control signal S2 is input, the rearrangement circuits 17 and 18 change the data arrangement order, and output it.  Because the TCAM device 12 is divided into the four segments (sub-blocks), write data or a search key is also divided\ninto four fields to respectively correspond to the segments.  The rearrangement circuit 17 changes the data arrangement order, for switching the second field and the first field of the write data or the search key, in accordance with the input of the\ncontrol signal S2.\nThe rearrangement circuit 18 changes the data arrangement order for switching the second field and the first field of the read data, in accordance with the input of the control signal S2.\nWhen the setting register 16 stores \"10\", the rearrangement control signal generation unit 15 outputs the control signal S3 to the rearrangement circuits 17 and 18.\nWhen the control signal S3 is input, the rearrangement circuits 17 and 18 change the data arrangement order, and output it.  The write data or the search key is divided into four fields.  The rearrangement circuit 17 changes the data arrangement\norder for switching the third field and the first field of the write data or the search key, in accordance with the input of the control signal S3.\nThe rearrangement circuit 18 changes the data arrangement order for switching the third field and the first field of the read data, in accordance with the input of the control signal S3.\nWhen the setting register 16 stores \"11\", the rearrangement control signal generation unit 15 outputs the control signal S4 to the rearrangement circuits 17 and 18.\nWhen the control signal S4 is input, the rearrangement circuits 17 and 18 change the data arrangement order, and output it.  The write data or the search key is divided into four fields.  The rearrangement circuit 17 changes the data arrangement\norder for switching the fourth field and the first field of the write data or the search key, in accordance with the input of the control signal S4.\nThe rearrangement circuit 18 changes the data arrangement order for switching the fourth field and the first field of the read data, in accordance with the input of the control signal S4.\n(Initialization (Setting) of Search Memory 8)\nThe communication unit 1 explained in FIG. 1 has rule sets necessary for performing a transfer process, for example, ACL or FIB (Forwarding Information Base).\nIn any of the cases, it may be set manually by the user, or may automatically be generated using a particular communication protocol.\nEach of the rule sets is configured with a group of fields including information, such as an IP address or a port number, necessary for a packet transfer process.\nOf the rule sets, there may exists a rule in which information is set entirely in the all fields, and there may also exists a rule in which information is partially set only in some field.\nAs an example, an IPv4 address is 32-bit data.  There is a concept of a network part and a host part therein.  An operation is performed in a manner that only the upper N bits of the network part are usually targets to be searched, while the\nlower (32-N) bits are not included in the targets.\nTo correspond to the part excluded from the targets to be searched, data of \"Don't care\" is written in the search memory 8.\nWhen a field including a large number of bits of \"Don't care\" is assigned to the initial-stage segment (sub-block) of the time division search, the segment does not have a function for limiting the entries.  Thus, it is difficult to reduce the\nconsumption power in the time division search.\nThe port numbers may be collectively handled in a particular range, for example, a range between 1024 and 65335.  For the rule set including this range information, there is applied a method for expressing one rule for a plurality of entries\nusing a range development algorithm.\nAt this time, the same data is copied in the entire fields, excluding the port numbers.\nThus, there locally exist field regions in which limiting of entries is not effectively performed, when the rule set is stored in the search memory 8.\nIn this embodiment, in the time division search, a field region in which limiting of the entries is effectively performed is assigned to the initial-stage segment (sub-block).\nAt the time of performing initialization (setting) of the search memory 8, the CPU 2 executes an analysis process for the rule sets stored in the search memory 8.\nNormally, the rule sets are kept in the form of a table.  However, rule tables are expressed in a form that a human can easily manage it.  Thus, it is necessary to convert the rule tables into a form suitable for the hardware for searching.\nThe converted table is assigned to each block of the search memory 8, and is used for setting the data width or mask of the setting register 16 and setting the profile register 26.\nThe CPU 2 performs data analysis for the sub-tables which are obtained by dividing the rule table in the unit of search blocks 10.\nThe CPU 2 specifies a field effective for the time division search in each search block 10 by the data analysis for the sub-tables.\nIn this embodiment, descriptions will be made to a case in which the TCAM device 12 included in the search block 10 has four sub-blocks.\nAs an example, when the entry data of the sub-table has a length of 160 bits, it is divided into four fields of 40 bits.\nThe CPU 2 specifies any field which is effective for the time division search, of the four fields respectively corresponding to the four sub-blocks stored in each search block 10.\nAt this point, it specifies, as the effective field, the field without the bit of \"Don't Care\" or with only very few thereof.\nAs an example, it specifies, as the effective field, the second field, of the first to fourth fields.\nIn this case, the CPU 2 sets \"01\" in the setting register 16 of the search block 10, based on the analysis result.\nAs a result, when \"01\" is stored in the setting register 16, the rearrangement control signal generation unit 15 outputs the control signal S2 to the rearrangement circuits 17 and 18.\nThe rearrangement circuits 17 and 18 change the data arrangement order, when the control signal S2 is input.\nIn this case, the rearrangement circuit 17 changes the data arrangement order for switching the second field and the first field of the entry data of the sub-table.\nThen, the sub-tables corresponding to the changed data arrangement order are stored respectively in the four sub-blocks.  That is, if the data arrangement order is not changed, data of the first field in the sub-table is stored in the first\nsub-block, and the second field in the sub-table is stored in the second sub-block.  However, if the data arrangement order is changed, the second field in the sub-table is stored in the first sub-block, and the first field in the sub-table is stored in\nthe second sub-block.\nThe above settings are performed for each search block 10.\nThus, in the search blocks 10-1 to 10-_n illustrated in FIG. 2, different values may be stored in the setting register 16 based on the analysis result of the stored sub-table.\nIn this manner, the initialization (setting) of the search memory 8 is completed, and an actual transfer process, that is, a data writing process starts.\n(Search of Search Memory 8)\nAt the time of searching, the search key generation unit 29 generates a search key.\nThis search key is output to each search block 10.\nEach search block 10 generates a control signal based on the value of the setting register 16.\nAs an example, when the value of the setting register 16 is \"01\", the rearrangement control signal generation unit 15 outputs the control signal S2 to the rearrangement circuits 17 and 18.\nWhen the control signal S2 is input, the rearrangement circuits 17 and 18 change the data arrangement order, and output the search key.\nIn this case, rearrangement circuit 17 divides the input search key into four search keys SA to SD, and changes the data arrangement order of the second search key SB and the first search key SA.\nThe rearrangement circuit 17 outputs the search keys of the changed data arrangement order respectively to the segments 12A to 12D.  That is, the second search key SB is output to the segment 12A, and the first search key SA is output to the\nsegment 12B.\nThen, a comparison process for the second field of the sub-table and the search key SB is executed as a first search, though it is originally executed as a second search.  The second field of the sub-table is specified as an effective field\nwithout the bit of \"Don't Care\" or with only very few thereof, and is easily non-matched with it.\nThus, when the field is non-matched with it, the non-matching entry of the post-stage segment is not a target to be searched.  This enables to suppress the consumption power of the segment of the post stage.\nIn this embodiment, the descriptions have been made to the case in which the second field of the sub-table is specified as the effective field.  However, it is not limited to this case.  The same applies to a case in which the third field and\nthe fourth field are specified as effective fields.\nFIG. 10 is a diagram for explaining a search timing of the search block 10.\nAs illustrated in FIG. 10, the search block 10 operates in synchronization with a clock CLK.\nThe rearrangement circuit 18 acquires a search key (keyA) in a time T1, in synchronization with the clock CLK.  Then, in a time T2, it outputs a search key (keyB) in accordance with the data arrangement order which has been changed based on a\ncontrol signal output from the setting register 16, to the TCAM device 12.  In a time T3, the TCAM device 12 receives an input of the search key (keyB) output from the rearrangement circuit 18, and then executes a search operation.\nIn a time T4 in a synchronization timing of the next clock CLK, in an illustrated case, a result from the segment 12A of the initial stage is output.\nIn a time T5 in a synchronization timing of the next clock CLK, in an illustrated case, a result from the segment 12B of the next stage is output.\nBy repeating the above process, the search result from the segment 12D of the final stage is output.\n(About Reading of Search Memory 8)\nDescriptions will be made to the reading of the entry data stored in the search memory 8 now.\nWhen the value of the setting register 16 is \"01\", the rearrangement control signal generation unit 15 outputs the control signal S2 to the rearrangement circuit 18.\nWhen the control signal S2 is input, the rearrangement circuit 18 changes the data arrangement order, and outputs it.\nIn this case, the rearrangement circuit 18 changes the data arrangement order of the entry data read respectively from the segments 12A to 12D.\nSpecifically, it changes the data arrangement order of the data read from the segment 12A and the data read from the segment 12B.  The data read from the segment 12B is output as the first data, the data read from the segment 12B is output as\nthe second data, the data read from the segments 12C and 12D are output as the third and fourth data.\nUsing this method, it is possible to easily restore the data to data before changing the data arrangement order, before it is stored in the search block 10.\nIn this embodiment, by setting the setting register 16 in a manner that the field which is assumed to be easily non-matched with the compared data is written into the initial-stage segment, it is possible to maximize the effect of this time\ndivision search.\n(Maintenance of Search Memory 8)\nDescriptions will be made to a maintenance operation of the search memory 8 now.\nThe rule table is updated in an actual operation due to various reasons.  For example, it is automatically updated using the network control protocol, or its policy is changed by the network manager.  Specifically, the entry is overwritten, or\nnewly added.\nBecause the setting register 16 cannot be rewritten during its operation, a difference is made between the analysis content of the first rule table and the analysis content of the information of the updated rule table, due to the updating of the\nrule table.  This may decrease the effect of the low power consumption by the time division search.\nIn this embodiment, descriptions will be made to a case in which the CPU 2 executes a maintenance process of the search memory 8 now.  It is not limited to the CPU 2, and possible to have a configuration in which the transfer control circuit 4\n(for example, NPU) may execute the process, or in which the search memory 8 has a chip for executing the maintenance process and mounted thereon.\nFIG. 11 is a diagram for explaining the maintenance operation of the search memory 8.\nAs illustrated in FIG. 11, the maintenance process is executed at the elapse of a predetermined time or the number of times of issue of a write command for the search memory 8 as a trigger.  In this case, the trigger is generated as an\ninterruption at the elapse of a predetermined time, using the timer.\nThe CPU 2 reads the updated rule table (read all entries) stored in the target search block 10 (for example, the search block 10-1) of the search memory 8 (1).  As described, at the time of reading from the search block 10-1, the data\narrangement order is changed by the rearrangement circuit 18 based on the register value of the setting register 16.  As a result, it is restored to the initial data arrangement, and it can be used as is as write data for an empty search block 10, for\nexample, the search block 10-n, as will be described later.\nNext, the CPU 2 executes a process for analyzing the updated rule table read from the target search block 10-1, and generates information regarding the register value stored in the setting register 16 based on the analysis result (2).\nThe CPU 2 checks whether the register value stored in the current setting register 16, that is, the setting register 16 of the search block 10-1 is the same as the information regarding the generated register value.\nThat is, it judges whether the register value of the setting register 16 of the search block 10-1 specifies a field which is effective for limiting the entries.  That is, it judges whether the specified field is effective for the time division\nsearch.\nWhen it is judged that the information regarding the generated register value is different from the register value stored in the setting register 16 of the search block 10-1, the CPU 2 stores the information regarding the generated register\nvalue in an empty search block 10, that is, the setting register 16 of the search block 10_n (3).\nThen, it copies information regarding the updated rule table acquired from the original search block 10-1, to the TCAM device 12 of this empty search block 10_n (4).\nAt this time, the rearrangement circuits 17 and 18 of the search block 10_n change the data arrangement order, based on the stored register value of the setting register 16.  Then, the data of the field which is effective for searching is stored\nin the segment of the initial stage of the empty search block 10-n.\nWhen the copying is completed, the CPU 2 sets again the profile register 26, and replaces from the original target search block 10-1 to be searched to the copy-destination search block 10_n (5).\nBecause a new search block 10 has the maximum effect of the low consumption power in the time division search, it contributes to the low consumption power of the entire system.\nUsing this maintenance method, it is possible to reduce the consumption power without stopping the operation of the entire system.\nInto the search memory, various rule tables having different characteristics, such as NetFlow/FIG/QoS/ACL, are written, and the process using range development algorithm is performed, as described above.  Thus, in fact, a deviation is easily\ngenerated in the data to be written into the search block.  If the time division search is executed in the same manner for this data structure, the time division search is quite effective for a particular block, while the time division search is not at\nall effective for another block.\nParticularly, a large group of \"Don't care\" are easily generated in the table using an IPv6 address, and this part is not suitable for limiting the target entries to be searched.  Thus, when it is assigned to the segment of the initial stage for\nthe time division search, the time division search is not effective at all.\nBy adopting the method based on the embodiment, it is possible to elimination the deviation of the blocks, and to maximize the effect of saving the power due to the time division search in the end.\nBy the method according to the first embodiment, it is possible to generate the search key suitable for the time division search for each search block using one search key.  Thus, it is possible to get a plurality of search instructions together\nat one time and to improve the throughput.\nIn a device of large consumption power, such as the TCAM device, the search operation may be divided to be executed, with the limitation of the target search block to be searched, from a viewpoint of the thermal design of the system and the\npower supply design.  In this case, reduction of the consumption power results in eliminating or reducing the division operation, thereby contributing to the improvement of the throughput of the search operation.\n<BR><BR>Second Embodiment\nIn a second embodiment as well, the rule set created by the end user is converted in a form of a table suitable for the search memory 8, and is divided further into smaller region to be analyzed, thereby setting the setting register 16.\nIn the above-described first embodiment, the descriptions have been made to the case in which the information of the field effective for the search is stored in the initial-stage segment of the TCAM device 12.\nIn the second embodiment, as the information of the field effective for the search, any of the regions without \"Don't care\" bit is stored in a BCAM (Binary Content Addressable Memory) device.  The rest of the regions are stored in the TCAM\ndevice.  In this embodiment, descriptions will hereinafter be made to a case in which the BCAM device is used.  However, it is possible to use an ASE (Algorithmic Search Engine) for realizing searching in combination of a memory device (SRAM or FF) and a\nlogical circuit realizing a search algorithm (tree hash).\nFIG. 12 is a diagram for explaining a search method of a search block 10# based on the second embodiment.\nAs illustrated in FIG. 12, unlike the search block 10, the search block 10# includes a BCAM device 12P and a TCAM device 12Q, instead of the TCAM device 12.  Any other parts of the configurations are the same as those explained in FIG. 9, and\nthus will not be described over and over.\nFIG. 13 is a circuit diagram illustrating an example of a configuration of a BCAM cell MC# of a BCAM device 12P.\nBy reference to FIG. 13, the BCAM cell MC# includes one SRAM cell 11 and a data comparison unit 13#.  The SRAM cell 11 stores complementary 1-bit data (when one is \"1\", the other one is \"0\") in the pair of internal storage nodes ND1 and ND1_n.\nThe BCAM cell is coupled to the pair of bit lines BL and BL_n, the pair of search lines SL and SL_n, the match line ML, and the word line WLX.  The pair of bit lines BL and BL_n extend in a column direction (Y direction), and are commonly shared\nby a plurality of BCAM cells which are arranged in the column direction.  The pair of search lines SL and SL_n extend in the column direction (Y direction), and are commonly shared by the BCAM cells arranged in the column direction.\nThe match line ML extends in a row direction (X direction), and is commonly shared by the BCAM cells arranged in the row direction.  The word line WLX extends in the row direction (X direction), and is commonly shared by the BCAM cells arranged\nin the row direction.\nUnlike the data comparison unit 13, the data comparison unit 13# does not include the MOS transistors Q8 and Q9.  Any other parts of the configuration are the same as those of the data comparison unit 13, and thus will not be described over and\nover.\nIn the search block 10# of the second embodiment, the first search is performed by the BCAM device, and the second and following searches are performed by the TCAM device.  Then, the time division operation for generating the search result is\nexecuted.  Note that the BCAM device stores any field without \"Don't care\" in the rule set.\nThe value of the setting register 16 is set in a manner that the Binary segment is assigned to the initial stage segment, in the BCAM device for the time division search.\nThe rearrangement control signal generation unit 15 generates a control signal based on the value of the setting register 16.\nThe rearrangement circuits 17 and 18 change the data arrangement order of the search key, based on the control signal generated by the rearrangement control signal generation unit 15.\nThe value of the setting register 16 may be any value, as long as it leads to a judgment to which BCAM device or TCAM device, each of the segments is assigned.  Note that, when the ASE for the tree search is used, an LPM (Longest Prefix Match)\ncan be used as effective information, or it is possible to use anything for describing the specific search order.\nIn this case, from the perspective of the ASIC circuit or NPU, there is no need to pay attention to the internal structure of the search memory 8, and it is possible to automatically execute division and rearrangement of the search table by\nappropriately setting the register value of the setting register of each search block.\n(Search of Search Memory 8)\nAt the time of searching, the search key generation unit 29 generates a search key.\nThis search key is output to each search block 10#.\nThe data arrangement order of the search keys distributed to the search blocks 10# is changed, by the rearrangement circuit 17 based on the value of the setting register 16.\nAfter the rearrangement is performed by the rearrangement circuit 17, it is divided and output to the BCAM device 12P and the TCAM device 12Q.\nThe BCAM device 12P executes a comparison operation of the received search key and each entry data stored in the BCAM device 12P.  Then, the BCAM device 12P outputs the entire comparison results as a control signal MLOUT.\nThe TCAM device 12Q controls the comparison operation of the received search key and each entry data, using the control signal MLOUT.  Specifically, like the first embodiment, the control signal MLOUT is coupled to the \"se\" terminal.  In this\nconfiguration, it is possible to suppress the consumption power of the TCAM device 12Q, by executing a comparison operation for the search key with only the entry data corresponding to the entry data which is hit by the BCAM device 12P.\nIn the TCAM device 12Q, the search result is given to the priority encoder, thereby generating a final search result address.\nBy this configuration, a part of the rule table is stored in the BCAM device 12P, thereby enabling to reduce the area, as compared to a case in which all rule tables are stored in the TACM device.\nIn the first embodiment, it is necessary to specify the field which is assumed as effective for limiting the entries in the time division search, by analyzing the rule table at the time of setting the setting register.  It is necessary to\nanalyze the number of bits of \"Don't care\" and the deviation of data included in a certain range.  In the second embodiment, it is possible to eliminate the process of the CPU 2 in the maintenance, because the analysis can be performed based only on a\nbinary determination or a ternary determination.\n<BR><BR>Modification\nIn the above-described second embodiment, the descriptions have been made to the case in which a plurality of different search modules are coupled to each other to perform the time division search.\nIt is necessary to delay a signal input for the second search in accordance with a delay until the first search is output.\nFIG. 14 is a diagram for explaining a search method of the search block 10A based on the modification of the second embodiment.\nAs illustrated in FIG. 14, unlike the search block 10#, the search block 10A has a timing adjustment circuit 19 provided therein.\nThe timing adjustment circuit 19 includes the rearrangement circuit 18, data flip flop circuits DFF1 to DFF4 for use in keeping data, and selectors SEL1 to SEL4.\nThe data flip flop circuits DFF1 to DFF4 keep a signal output from the rearrangement circuit 18.\nThe selectors SEL1 to SEL4 select and switch to output the signal output from the rearrangement circuit 18 and any of the data flip flop circuits DFF1 to DFF4.\nA switching signal for switching selection of any of the selectors SEL1 to SEL4 is output from a rearrangement control signal generation unit 15#.\nFor example, in the case of this embodiment, the selectors SEL1 to SEL3 have a configuration for outputting a search key to the TCAM device 12Q, thus outputting data from the data flip flop circuits DFF1 to DFF3.\nThe selector SEL4 has a configuration for outputting a search key to the BCAM device 12P, thus outputting data as is from the rearrangement circuit 18, instead of data from the data flip flop circuit DFF4.\nFIG. 15 is a diagram for explaining a search timing of the search block 10A.\nAs illustrated in FIG. 15, the search block 10A operates in synchronization with a clock CLK.\nIn synchronization with the clock CLK, the rearrangement circuit 18 acquires a search key (keyA) in a time T10.  In a time T11, a search key (keyB) in accordance with the data arrangement order changed based on a control signal output from the\nsetting register 16 is output to the BCAM device 12P.  As an example, descriptions will be made to a case, in which a search key having width of 160 bits is input, a 40-bit search key is output to the BCAM device 12P, and the remaining 120-bit search key\nis divided to be output to the TCAM device 12Q.\nIn a time T12, the BCAM device 12P receives an input of a search key (keyB) output from the rearrangement circuit 18, and executes a search operation.\nAs illustrated, in a time T13 in the synchronization timing of the next clock CLK, a result is output from the BCAM device 12P.\nIn the time T13 in the same timing, a search key (keyC) kept in the data flip flop circuits DFF1 to DFF3 is output to the TCAM device 12Q.\nIn this embodiment, because the remaining 120-bit search key for use in the TCAM device 12Q as the post-stage segment is once taken into the data flip flop circuit DFF, it is input to the TCAM device 12Q one cycle behind.  Thus, the TCAM device\n12Q can receive an input of the search key and an output of the search result of the BCAM device 12P substantially at the same timing.\nIn a time T14, a search operation of the TCAM device 12Q is executed with the search key (keyC), based on the result from the BCAM device 12P.\nAs illustrated, in a time T15 in the synchronization timing of the next clock CLK, a result is output from the TCAM device 12Q.\nThe present disclosure has specifically been described based on the preferred embodiments.  The present disclosure is not limited to the preferred embodiments, and various changes may be made without departing from the scope thereof.", "application_number": "15968331", "abstract": " A semiconductor device includes an N number of sub-blocks each of\n     including a memory cell array, a setting register specifying number of\n     entry data for pre-searching, of first to N-th entry data divided and\n     correspond respectively to the sub-blocks, and a search data changing\n     unit changing a data arrangement order for search data input based on a\n     value of the register. A sub-block for pre-searching searches for entry\n     data matching with data for pre-searching in accordance with the\n     arrangement order changed by the changing unit, in response to an\n     instruction, and outputs a search result representing matching or\n     non-matching. A sub-block for post-searching searches for entry data\n     matching with data for post-searching other than the data for\n     pre-searching, of entry data stored in association with each row of the\n     array, based on a search result of the sub-block for pre-searching, and\n     outputs a search result representing matching or non-matching.\n", "citations": ["6731526", "8984217", "9502112", "10121541", "10191839", "20070247885", "20130283135", "20130297649", "20180025755"], "related": []}, {"id": "20190082311", "patent_code": "10375541", "patent_name": "Methods and systems for social networking with autonomous mobile agents", "year": "2019", "inventor_and_country_data": " Inventors: \nRonnau; Andrew (Los Alamitos, CA)  ", "description": "<BR><BR>BACKGROUND OF THE DISCLOSURE\n1.  Field of the Disclosure\nThe disclosure relates generally to social networking methods and systems and specifically in certain embodiments to methods and systems for proximity-driven social networking applications implemented with autonomous mobile agents incorporating\ndata mining and machine learning classification.\n2.  General Background\nMobile device users (agents) provide a platform for a social networking interaction that is motivated by groupings of proximate users, independent of other typical social network connections.  Geographical proximity, and additional features\nproximity, provides categorizations which establish a set of proximate agents which are thus associated for interaction.\nThe information exchange between any two agents may be performed with open identity, or with partial or complete anonymity\nTypes of data exchanged may be user-driven communication, user-defined auto-categorizations, and automatic machine classification operating on feature data inputs from associated agents.\nData mining and machine learning technologies may be used to develop automatic computer constructs for agents whom can operate on feature data sets from associated neighbor agents as input.  Computer machine constructs within an agent perform\nclassification operations on feature data sets from associated agents and the classification results may be communicated back to associated agents.  This information exchange may be autonomously derived.\nAs an example, facial recognition technology (FRT) provides a means to derive feature data to describe the facial appearance of a user agent.  Machine learning provides a technology by which the facial attraction preferences of an associated\n(proximate group) agent may be modeled.  The computer machine of the associated agent may operate on the feature data set from the parent agent and thus classify the associated agent as attracted, or not attracted to the parent agent.  This process\nallows the associated agents to be thusly classified.  Classification using facial recognition technology (FRT) and machine learning is one example.  Other classifications may be implemented into this structure.\nIt is desirable to address the limitations in the art, e.g., to apply methods and systems for proximity-driven social networking applications implemented with autonomous mobile agents incorporating data mining and machine learning\nclassification. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nBy way of example, reference will now be made to the accompanying drawings, which are not to scale.\nFIG. 1A illustrates an exemplary networked environment and its relevant components according to certain embodiments.\nFIG. 1B is an exemplary block diagram of a computing device that may be used to implement certain embodiments.\nFIG. 2 is an exemplary block diagram of a network that may be used to implement certain embodiments.\nFIG. 3 illustrates an exemplary environment of agents in which the proximate group (neighbor group) is shown within the discrimination domain.\nFIG. 4 illustrates an implementation in which the proximate zone is established with locally provided log-on key.\nFIG. 5 illustrates an implementation in which the proximate zone is established with a log-on key provided at more than one location.\nFIG. 6 illustrates and exemplary environment of agents created by joining two remote proximate zones.\nFIG. 7 illustrates the inter-agent communication in a proximate subgroup, independent of any classification operation.\nFIG. 8 illustrates the inter-agent communication for the classification operation in an exemplary environment proximate subgroup of agents in which the proximate group (neighbor group) has been classified.\nFIG. 9 is a schematic diagram of a prototypical agent structure as implemented herein.\nFIG. 10 is a diagram of an agent layer structure for an example binary classification.\nFIG. 11 is a diagram of an agent layer structure for an example binary multiplex classification.\nFIG. 12 is a diagram of an agent layer structure for an example multivariate classification.\nFIG. 13 depicts the agent initialization process.\nFIG. 14 depicts the network discrimination process to determine a proximate group.\nFIG. 15 shows an exemplary process in which parent agent feature data is classified by an associated neighbor agent.\nFIG. 16 shows the use of additional computer resources by the computer machine of an agent.\nFIG. 17 shows the use of a proximity zone to request data from a target location.\nFIG. 18 is an exemplary block diagram of a process to detect and identify a proximate group of agents and establish communication, and classification amongst the agents.\nFIG. 19A is a detailed exemplary block diagram of the first portion of the processes depicted in FIG. 18.\nFIG. 19B is a detailed exemplary block diagram of the second portion of the processes depicted in FIG. 18.\nFIG. 19C is a detailed exemplary block diagram of the third portion of the processes depicted in FIG. 18.\n<BR><BR>DETAILED DESCRIPTION\nThose of ordinary skill in the art will realize that the following description of the present invention is illustrative only and not in any way limiting.  Other embodiments of the invention will readily suggest themselves to such skilled\npersons, having the benefit of this disclosure.  Reference will now be made in detail to specific implementations of the present invention as illustrated in the accompanying drawings.  The same reference numbers will be used throughout the drawings and\nthe following description to refer to the same or like parts.\nFurther, certain figures in this specification are flow charts illustrating methods and systems.  It will be understood that each block of these flow charts, and combinations of blocks in these flow charts, may be implemented by computer program\ninstructions.  These computer program instructions may be loaded onto a computer or other programmable apparatus to produce a machine, such that the instructions which execute on the computer or other programmable apparatus create structures for\nimplementing the functions specified in the flow chart block or blocks.  These computer program instructions may also be stored in a computer-readable memory that can direct a computer or other programmable apparatus to function in a particular manner,\nsuch that the instructions stored in the computer-readable memory produce an article of manufacture including instruction structures which implement the function specified in the flow chart block or blocks.  The computer program instructions may also be\nloaded onto a computer or other programmable apparatus to cause a series of operational steps to be performed on the computer or other programmable apparatus to produce a computer implemented process such that the instructions which execute on the\ncomputer or other programmable apparatus provide steps for implementing the functions specified in the flow chart block or blocks.\nAccordingly, blocks of the flow charts support combinations of structures for performing the specified functions and combinations of steps for performing the specified functions.  It will also be understood that each block of the flow charts,\nand combinations of blocks in the flow charts, can be implemented by special purpose hardware-based computer systems which perform the specified functions or steps, or combinations of special purpose hardware and computer instructions.\nFor example, any number of computer programming languages, such as C, C++, C# (CSharp), Perl, Ada, Python, Pascal, SmallTalk, FORTRAN, assembly language, and the like, may be used to implement aspects of the present invention.  Further, various\nprogramming approaches such as procedural, object-oriented or artificial intelligence techniques may be employed, depending on the requirements of each particular implementation.  Compiler programs and/or virtual machine programs executed by computer\nsystems generally translate higher level programming languages to generate sets of machine instructions that may be executed by one or more processors to perform a programmed function or set of functions.\nThe term \"machine-readable medium\" should be understood to include any structure that participates in providing data which may be read by an element of a computer system.  Such a medium may take many forms, including but not limited to,\nnon-volatile media, volatile media, and transmission media.  Non-volatile media include, for example, optical or magnetic disks and other persistent memory.  Volatile media include dynamic random access memory (DRAM) and/or static random access memory\n(SRAM).  Transmission media include cables, wires, and fibers, including the wires that comprise a system bus coupled to processor.  Common forms of machine-readable media include, for example, a floppy disk, a flexible disk, a hard disk, a magnetic\ntape, any other magnetic medium, a CD-ROM, a DVD, any other optical medium.\nFIG. 1A depicts an exemplary networked environment 100 in which systems and methods, consistent with exemplary embodiments, may be implemented.  As illustrated, networked environment 100 may include a content server 105, a receiver 115, and a\nnetwork 110.  The exemplary simplified number of content servers 105, receivers 115, and networks 110 illustrated in FIG. 1A can be modified as appropriate in a particular implementation.  In practice, there may be additional content servers 105,\nreceivers 115, and/or networks 110.\nIn certain embodiments, a receiver 115 may include any suitable form of multimedia playback device, including, without limitation, a cable or satellite television set-top box, a DVD player, a digital video recorder (DVR), or a digital\naudio/video stream receiver, decoder, and player.  A receiver 115 may connect to network 110 via wired and/or wireless connections, and thereby communicate or become coupled with content server 105, either directly or indirectly.  Alternatively, receiver\n115 may be associated with content server 105 through any suitable tangible computer-readable media or data storage device (such as a disk drive, CD-ROM, DVD, or the like), data stream, file, or communication channel.\nNetwork 110 may include one or more networks of any type, including a Public Land Mobile Network (PLMN), a telephone network (e.g., a Public Switched Telephone Network (PSTN) and/or a wireless network), a local area network (LAN), a metropolitan\narea network (MAN), a wide area network (WAN), an Internet Protocol Multimedia Subsystem (IMS) network, a private network, the Internet, an intranet, and/or another type of suitable network, depending on the requirements of each particular\nimplementation.\nOne or more components of networked environment 100 may perform one or more of the tasks described as being performed by one or more other components of networked environment 100.\nFIG. 1B is an exemplary diagram of a computing device 150 that may be used to implement aspects of certain embodiments of the present invention, such as aspects of content server 105 or of receiver 115.  Computing device 150 may include a bus\n190, one or more processors 175, a main memory 170, a read-only memory (ROM) 180, a storage device 185, one or more input devices 155, one or more output devices 160, and a communication interface 165.  Bus 190 may include one or more conductors that\npermit communication among the components of computing device 150.\nProcessor 175 may include any type of conventional processor, microprocessor, or processing logic that interprets and executes instructions.  Main memory 170 may include a random-access memory (RAM) or another type of dynamic storage device that\nstores information and instructions for execution by processor 175.  ROM 180 may include a conventional ROM device or another type of static storage device that stores static information and instructions for use by processor 175.  Storage device 185 may\ninclude a magnetic and/or optical recording medium and its corresponding drive.\nInput device(s) 155 may include one or more conventional mechanisms that permit a user to input information to computing device 150, such as a keyboard, a mouse, a pen, a stylus, handwriting recognition, voice recognition, biometric mechanisms,\nand the like.  Output device(s) 160 may include one or more conventional mechanisms that output information to the user, including a display, a projector, an A/V receiver, a printer, a speaker, and the like.  Communication interface 165 may include any\ntransceiver-like mechanism that enables computing device/server 150 to communicate with other devices and/or systems.  For example, communication interface 165 may include mechanisms for communicating with another device or system via a network, such as\nnetwork 110 as shown in FIG. 1A.\nAs will be described in detail below, computing device 150 may perform operations based on software instructions that may be read into memory 170 from another computer-readable medium, such as data storage device 185, or from another device via\ncommunication interface 165.  The software instructions contained in memory 170 cause processor, 175, to perform processes that will be described later.  Alternatively, hardwired circuitry may be used in place of or in combination with software\ninstructions to implement processes consistent with the present invention.  Thus, various implementations are not limited to any specific combination of hardware circuitry and software.\nA web browser comprising a web browser user interface may be used to display information (such as textual and graphical information) on the computing device 150.  The web browser may comprise any type of visual display capable of displaying\ninformation received via the network 110 shown in FIG. 1A, such as Microsoft's Internet Explorer browser, Netscape's Navigator browser, Mozilla's Firefox browser, PalmSource's Web Browser, Google's Chrome browser or any other commercially available or\ncustomized browsing or other application software capable of communicating with network 110.  The computing device 150 may also include a browser assistant.  The browser assistant may include a plug-in, an applet, a dynamic link library (DLL), or a\nsimilar executable object or process.  Further, the browser assistant may be a toolbar, software button, or menu that provides an extension to the web browser.  Alternatively, the browser assistant may be a part of the web browser, in which case the\nbrowser would implement the functionality of the browser assistant.\nThe browser and/or the browser assistant may act as an intermediary between the user and the computing device 150 and/or the network 110.  For example, source data or other information received from devices connected to the network 110 may be\noutput via the browser.  Also, both the browser and the browser assistant are capable of performing operations on the received source information prior to outputting the source information.  Further, the browser and/or the browser assistant may receive\nuser input and transmit the inputted data to devices connected to network 110.\nSimilarly, certain embodiments of the present invention described herein are discussed in the context of the global data communication network commonly referred to as the Internet.  Those skilled in the art will realize that embodiments of the\npresent invention may use any other suitable data communication network, including without limitation direct point-to-point data communication systems, dial-up networks, personal or corporate Intranets, proprietary networks, or combinations of any of\nthese with or without connections to the Internet.\nThe building blocks of a system according to certain embodiments consist of a universe of agents (typically embodied as mobile users), a proximity discrimination module, a classification module, and notification interface.  The structure and\nimplementation of the system in certain embodiments, and of each of its building blocks, are described below and depicted in the accompanying figures.\nFIG. 2 depicts an exemplary networked environment 200 in which systems and methods, consistent with exemplary embodiments, may be implemented.  As illustrated, networked environment, 200, will include a proximity discrimination module\n(discrimination module), 220, and a classification module, 230.  The parent agent, 205, initiates a query in which the discrimination module, 220, discriminates the group of all agents, 215, to a proximate group, 225.  The discrimination parameters may\nbe refined in an iterative process, 240.  The classification module, 230, further operates on the proximate group to yield the classified proximate group, 235.  An abbreviated structure may be utilized in which classification is not performed, 245.\nThe group of all agents, 215, consists of all members participating in the system.  Members of the system may consist of, but are not limited to, users with cell phones, tablets, desktops, and shared environment computational devices.\nThe parent agent, 205, is a subgroup of all agents which typically consists of a single member.  An agent may be designated as parent agent by specifying a set of parameters for proximity discrimination.  In implementations where a single agent\nfunctions as parent agent, one or more agents, or all agents, may independently serve as the parent agent.\nThe functional concept according to certain embodiments is that a neighborhood of users can be established about some location.  Communication between the users in the proximate subgroup may be established.  Additional operations\n(classifications) may be performed and communicated openly or anonymously so that the parent (or other) agent knows some characteristic of the associated neighbor agents.  For example, the parent agent may know that there are 20 Republican in the\nproximate group and 10 others.  Relative geographic data may be included in the inter-agent exchange to be communicated to an agent.  For example, the parent agent may see that of the 30 agents in the room, 20 of them are Republicans, and they are all at\nthe front of the room.\nReferring back to the exemplary embodiment depicted in FIG. 2, the discrimination module, 220, consists of a system by which the proximate subgroup, 225, is determined.\nThe proximate group, 225 consists of the subgroup of all users that are within specified proximity to geographical position data and/or proximity input parameters specified by the parent agent, 205.\nThe proximate group, 225, is the group of agents amongst which inter-agent communication may occur.\nFIG. 3 depicts an exemplary proximate group of associated agents consisting of the parent agent, 205, and associated neighbor agents, 210, that are all within the discrimination zone, 250, and excluded agents, 255, that are not part of the\nassociated neighbor group.\nNote that for each of the agents that functions as a parent agent, the resultant proximate group may be different.\nIt may be possible to specify parameters such that the discrimination zone does not contain the parent agent.\nProximity parameters may be of any qualitative nature.  Examples of such proximity parameters include, but are not limited to, hair color, age range, height range, political affiliation, and the like.\nProximity parameters may be geographical, e.g., at (or within 0.25 mile of) latitude/longitude coordinates 34.8940-117.0472.  Other parameters may be used, depending on the requirements of each particular implementation.  For example, the\nproximity parameters may specify teenage males within the boundaries of Disneyland, Calif.  As another example, the proximity parameters may specify a domain that is within 50 feet of the centerline of Colorado Blvd, in Pasadena, Ca, during the period\nfrom 8:00 AM to 11:00 AM on Jan.  1, 2016.\nThe parameters may be specified which create a collection of domains which are not contiguous.  For example, the proximity parameters may specify the collection of domains defined as within 100 feet of any of the In-N-Out.RTM.  Burger\nrestaurants in Long Beach, Calif.\nThe discrimination operation may be parametric only, utilizing agent inputs.\nIn one exemplary implementation, the discrimination operation may perform in whole or in part using agent position data.  For example, individual agents, or groups of agents may be added to or removed from the proximate group using a map display\ndepicting the relative position of agents.\nIn certain embodiments, the discrimination module, 220, may consist of a remote server which receives geographic position data and/or additional proximity input parameters from all agents, 215, to discriminate the subgroup of all agents that are\nwithin specified proximity to geographical position data and/or proximity input parameters of the parent agent, 205.\nIn certain embodiments, the discrimination module, 220, may consist of a set of operations that are performed on one or more agents without communication with a remote device to perform the discrimination operation.\nIn certain embodiments, the discrimination module, 220, may consist, in part, of a proximity key, e.g., a log-on displayed at a bar, to facilitate an associative collection of participating agents at that location.  This is depicted in FIG. 4,\nin which a proximate group of neighbor agents, 210, are established at some locality, 300, using the locally provided log-on key, 305.  Agents that do not apply the log-on key comprise the group of excluded agents, 255.\nThe element that provides the log-on key, 305, may serve as a parent agent, or may simply be a mechanism to provide a log-on process initiated by an agent within the local group, or by an agent external to the local group.\nA log-on key may be provided at different locations to establish a proximate group consisting of geographically remote subgroups, as depicted in FIG. 5, which shows two remote localities, 301 and 302, in which a log-on key, 305, is provided at\nmore than one location via a communication link, 350, to establish a proximate group of neighbor agents, 210.  Agents that do not apply the log-on key comprise the group of excluded agents, 255.\nReferring back to exemplary diagram in FIG. 2, upon development of a proximate group, 225, the proximity parameters may be modified, 240, whereupon the proximate group is re-evaluated.  This process may be repeated and adjusted any number of\ntimes.\nRemote proximate groups may be joined into one proximate group comprised of remote groups of associated agents, as shown in FIG. 6, which shows the proximate group within the discrimination zone, 250, composed of a parent agent, 205 and\nassociated neighbor agents, 210, and the proximate group within the discrimination zone, 251, composed of a parent agent, 206 and associated neighbor agents, 211, joined, 400, to form one effective associated group\nThe joined group thus formed may have either parent agent, 205, or 206, assume the role as the joined group parent agent.\nA proximate group may have more than one parent agent.\nThe role of parent agent may be switched amongst agents, or shared amongst agents.\nA proximate group of associated agents may be broken into more than one smaller disjoint proximate groups.\nThe grouping thus established may be temporary, with associations expiring in a fixed time, or expiring after there has been a lapse of inter-agent communication for a fixed time.\nThe groupings thus established may be saved, e.g., the group that was at the bar on Friday night.\nThe proximity group may be geographically transient, in which the group consists of agents that are within a certain geographical proximity to a location, e.g., the parent agent's location, which may be moving.  As an example, consider agents in\ncars on the highway, wherein the proximity group consists of agents that are sufficiently near to the car transporting the parent agent.  In essence, the discrimination zone, 250, as shown in FIG. 3 moves with a moving parent agent.\nIn another exemplary implementation, the proximity parameters may be specified such that the proximity region does not follow a parent agent, but does move.  As an example, a zone may move with a float in a parade, even though no parent agent is\nmoving with the float.\nIn one embodiment, communication and notification may commence, and continue between agents in the proximate group, 225, with our without classification.  This is shown in FIG. 7, in which the parent agent, 205, and associated neighbor agents,\n210, within the discrimination zone, 250, communicate, 450.  Agents, 255, outside of the discrimination zone are excluded from the communication.\nIn one implementation, each neighbor agent communicates only with the parent agent (restricted communication).\nIn one implementation, all agents may communicate to each other agent in the proximate group (open communication).\nIn one implementation, communication permissions may be different for different agents (structured communication).\nIn one implementation the communication structure may be initialized, but is not locked such that the communication structure may emerge based on agent level decisions (free communication)\nThe inter-agent communication may include notification to the parent agent of the existence of each neighbor agent (anonymous notification).\nThe communication to the parent agent may also include relative positional data, and possible additional neighbor agent data (non-anonymous notification).\nThe inter-agent communication may include notification to the neighbor agent that the neighbor agent has been discriminated into a proximate group (anonymous notification).\nThe communication to the neighbor agent may also include relative positional data, and possible additional neighbor agent data (non-anonymous notification).\nThe inter-agent communication structure may be configured such that combinations of one-way, two-way, anonymous, and non-anonymous communication are used within an agent group.\nThe communication may also include any combination of directed communications, not limited to, text messages, emails, file transfers, and stored photo exchanges, real time photo exchange, stored video exchange and real time video.\nThe classification module, 150, comprises hardware and or software that transmits and receives feature data, and that operates on feature data which classifies the agents relative to the feature data.\nThe classification information which may be exchanged between agents is in addition to the inter-agent communication described herein.\nIn one embodiment, the classification operates on feature data from an agent of the proximate group, generally the parent agent, which classifies the associated neighbor agent relative to the feature data.\nIn one embodiment, the classification operates on directed inputs from one or more participants in the associated neighbor group (auto-classification).\nIn one embodiment, the classification operates on directed inputs or data mined from about one or more sources that are not in the associated neighbor group (external auto-classification).  For example, the agents may be classified as members of\nan organization, e.g., The Sierra Club.\nThe classification module may consist of classification operations executed on a distributed platform.  Such computations may be performed in residence on the agent, or in part or in whole utilizing computational resources external to the agent.\nDepending on the requirements of each particular implementation, the classification module may consist of combinations of the classification module examples described above.\nThe classification operation may be one that provides results which may are qualitative.\nThe classification operation may be one that provides results which may are binary.\nThe classification operation may be one that provides results which may are variable.\nThe classification operation may be one that provides results which are a derived types with any combination of types described herein.\nThe classification operation may be dynamically driven via inter-agent communication.  For example, a member of an active proximate subgroup at a lecture may send out a classification query, e.g., \"likes speaker.\" The complementary members of\nthe proximate subgroup may actively auto-classify into the binary groupings, e.g., \"like the current speaker,\" or \"dislike the current speaker.\"\nThe classification may be parametrically stored in the agent structure.  The information may be transient and stored only during the period of participation in the particular associated proximate group.\nIn one implementation, the classification query, for selected layers, is initiated by the parent agent.  The feature data is transmitted to the associated proximate subgroup neighbors, and the classification results are obtained for each member\nof the proximate group.  The classification results are transmitted to the parent agent.\nThis classification is shown in FIG. 8, which shows the subgroup of elements in a binary classification operation.  A classification query, 500, from the parent agent, 205, to elements within the discrimination zone, 250, which activates the\nclassification process on each associated neighbor agent.  In FIG. 8, the binary classification results are depicted schematically as classified neighbor agents with either a solid border, 210, or with a dashed border, 510.  The classification results\nare transmitted, 500, back to the parent agent, 205.\nAgents, 255, outside of the discrimination zone are excluded from the classification operation.\nThe data exchange and classification may not provide notification to the associated neighbor agent that the classification occurs (one-way anonymous classification).\nThe data exchange and classification in may also include relative positional data, and possible additional neighbor agent data (one-way non-anonymous notification).\nThe data exchange and classification may include notification to the neighbor agent that the classification occurs (anonymous classification).\nThe communication to the neighbor agent may also include relative positional data, and possible additional neighbor agent data (non-anonymous classification).\nThe data exchange and classification structure may be configured such that combinations of one-way, two-way, anonymous, and non-anonymous communication and exchange are used within an agent group.\nIn certain embodiments, neighbor member agents may initiate a classification query within the active existing proximate group, and receive classification data, as acting parent agent.\nFIG. 9 depicts an exemplary structure of a prototypical agent, 550, including the communication/notification interface, 555, and three data layers, 561, 571, and 581.  Each of the three layers has a feature data component, 562, 572, and 582,\nrespectively, and corresponding computer machine components, 563, 573, and 583, respectively.\nThe executive information principal to each data layer contains the layer name, and status, i.e.; active/inactive.\nNote that although the data layers are shown in the figure, the number of data layers may vary by agent, and by implementation.\nThe communication/notification interface, 555, may send and receive data from associated agents, from external entities, and from direct user input/output.\nCommunication to and from other agents may be in the form of wireless network transmissions, or hardwired network transmissions.\nCommunication to and from other agents may be in the form of wireless, or hardwired signals independent of any internet, or intranet.\nCommunication to and from other agents may by way of audible or inaudible sound or vibration or other form of transducer interaction.\nCommunication to and from other agents may by way of infrared, or any other band of EMF.\nThe communication/notification interface to the user may consist of any suitable combination of audible, sensory, text, touch, and graphical input and display used to facilitate the implementations described herein.\nIn one exemplary implementation, the interface consists of a graphical map display which shows the relative positional data, as available, of agents in the proximate group.\nClassification and notification data may be displayed as coloration, text tagging, or other visual indications associated with proximate agents.\nCommunications and data may be associated and accessible with user interaction, For example, classification results may be displayed in a drop down layer type display structure.\nDifferent communication and notification interface implementations may be used for different aspects of inter-agent classification and communication.\nCombinations of different notification and input modes may be used.\nCommunication and notification may be implemented on any suitable electronic device.  Note that the communication and notification may be exchanged using a device other than the agent to or from which said communication is directed.  For\nexample, a laptop computer may be used to view a map like environment representing the proximate group of agents as displayed for one of the member agents, although said member agent may be embodied by a user with a cell phone.\nThe layer structure, consisting of feature data and computer machine may be used to develop classification data beyond communications established between agents in the proximate group.\nThe feature element of each layer has the data structure for the classification, and contains the relevant data that may be necessary for the classification.\nThe layer structure may be dynamic.\nThe computer machine has the construct necessary to classify the feature data input from an agent.\nThe feature data component and computer machine component of each classification layer may be completely different, or may have the same structure as for other layers.\nThe classification may be dynamic (performed in real time upon the request query), or it may be stored as part of the agent structure.\nIn one implementation, the classification layer may consist of feature data and computer machine to provide a binary classification, i.e.; and is or is not classification relative to a category per the feature data.\nThe binary classification may be a self-categorization (auto classification), e.g.; male.  The feature data set consists of the data descriptor (which may even be the same as the layer name), and the computer machine consists of the\ncategorization which may be set dynamically or may be pre-set.\nIn one implementation, the feature data will contain the data name, and the categorization will simply have the result of the auto-categorization.  An exemplary layer structure for a binary feature/classification layer is shown in FIG. 10, in\nwhich the data layer, 600, shows executive structure, i.e.; layer name and status.  The feature data component, 605, contains a binary classification element, \"Republican\".  The computer machine component, 610, holds the binary classification switch for\neach corresponding element of the feature data.\nThe computer machine may be simply an external categorization, by another agent or organization.  For example, the layer may be Sierra Club membership, and the feature data consists of personal identity information, and the classification (is a\nmember, or is not a member) is provided to the layer by the Sierra Club.\nIn an additional implementation, the classification layer may consist of feature data and computer machine to provide a multiplex classification, of several possible variables.  Such a layer is similar to the construct of multiple binary layers. An exemplary structure is shown in FIG. 11, showing a multiple binary classification layer, 615.  The feature data component, 620, contains three binary classification elements, \"Republican\", \"Democrat\", and \"Independent\".  The computer machine\ncomponent, 625, holds the binary classification switches for each corresponding element of the feature data.\nIn another implementation, the classification layer may consist of feature data and computer machine to provide a multivariate classification.  Such a layer may classify in a manner that is not binary, but rather has multiple possible answers. \nAn exemplary structure is shown in FIG. 12, showing a multivariate classification data layer, 630.  The feature data component, 635, and computer machine component, 640, are structured appropriately for a classification that admits some range of possible\nanswers.\nThe multivariate example, \"Political Affiliation\", shown In FIG. 12 admits a discrete multivariate classification, i.e.; \"Republican\", or \"Democrat\", or \"Independent\", etc. It is also possible to have a continuous multivariate classification,\nfor example, \"Height\", in which case the possible classification is number from a continuum.\nThe classification structures may consist of auto-categorizations, or externally derived classifications.\nIn one embodiment, the classification is performed by the computer machine using data mining techniques.\nIn one embodiment, the computer machine may consist of statistically derived group preferences using agent data,\nFeature data may consist of sufficient agent (personal) information from which the computer machine may make data mining inquiries over external data bases.  The computer machine may extract data from internet sources, social networks, and other\nsocial media, e.g., Twitter, from which the computer machine may classify the agent.\nFeature data in the form of other classifications or categorizations may be used by the computer machine to classify using data mining techniques.  For example, a computer machine may suggest that there is a strong correlation between National\nRifle Association membership and Republican political registration.  Thus, if the feature data from an agents contains the classification \"National Rifle Association\", then the computer machine may thus classify the agent as \"Republican\".\nThe feature data may serve as input to a computer machine that is statistically constructed.  For example, the computer machine for an associated neighbor agent may contain the statistical construct that the agent has a preference for blond\nwomen between 20-25 years old.  That associated agent would thus be classified as attracted to a parent agent that is a 22-year old blond.\nCombinations of said feature data and classification operations may be used.\nFeature data may consist of descriptive elements that are specifically tailored to special classification computer machines.  For example, an attraction classification may use feature data that is abstract, such as eigenvectors from image\nprinciple component analysis, or other image processing feature extractions.\nThe computer machine to operate on such abstract feature extractions may also rely on training and parameterization using abstract feature data sets.\nThe agent structure and classification mechanisms described permit additional implementations somewhat outside the notion of communication and classification within the proximate agent group.\nEntities mentioned herein may be people, corporations, groups, products, institutions, etc.\nSuch an exemplary implementation is one in which entities rather than participating agents are classified for the parent agent.  For example natural language processing may be utilized to perform sentiment analysis on a collection of\npublications to determine the political disposition of a newspaper author.\nIn such an exemplary implementation, news articles may be tagged following the inclinations (as derived by the analysis) of the author.\nOther data mining and analysis methodologies may be implemented.  For example, correlative models may associate membership in some specific group with a certain political affiliation.\nSuch data mining and analysis models may be implemented on the system level, or may be implemented at the agent level.  Such models may be individually derived, or may be made available by the system institution.\nOther similar versions of such implementations are possible using data mining and natural language processing analysis directed at entities to be classified.\nAgent input may also be used to verify or refute such classifications.  Data obtained from such processes provide relative classification placements.\nSuch models described herein, embodied in the computer machine element within an agent, may be stored, shared, exchanged, created, and modified as utility objects.\nSuch classification may consist of classification operations executed on a distributed platform, on the agent level, or in part or in whole utilizing computational resources external to the agent.\nThe body of agents for the implementations described herein is created through an initialization process shown in FIG. 13.\nA potential agent, 650, downloads the application software, 660, from a content server, 655.  The potential agent with the software, 665, then registers, 675, with the software registration server, 670, whereupon the agent becomes a member\nagent, 680, and is a member of the collective universe of member agents, 215.\nThe collective universe of agents, 215, is the group from which all parent agents and proximate groups may be drawn.\nIt is possible for an agent to reside in a refined sub-universe of agents.  Said refinement is a semi-permanent step of group discrimination, as described above.\nNote that the content server, 655, and the registration server, 670, are process entities, which may be embodied on the same device, or on multiple devices.\nThe download, 660, and registration, 675, processes may be accomplished in one step or multiple steps.\nThe potential agents consist of, but are not limited to, users with cell phones, tablets, desktops, and shared environment computational devices.\nIt may be possible for an agent to have more than one member presence on a single device, i.e., different user profiles.\nFIG. 14 schematically depicts the process of forming a proximate agent subgroup.  The parent agent, 205, initiates a discrimination request, 705, which is transmitted to the network agent, 700.\nThe discrimination request may contain any combination of geographic location data, and discrimination parameters.  The network agent, 700, may forward the discrimination request to other network agents, 701, 702.\nOne or more of the network agents may then initiate communication, 720, to and or from the group of agents covered by the network agents.  The location parameters, and or any additional discrimination parameters are then used to distinguish the\nappropriate proximate neighbor subgroup agents, 210.\nIt certain implementations, it may be possible for agents in the universe group to utilize preference filters to prevent their incorporation into certain parent agent proximate groupings.\nIn certain embodiments, preference filters on the proximate group restrict inclusion to certain features.  Said preference filters may be provided by the parent agent.  Said preference filters may be implemented at the system level.\nAgents, 255, that do not meet the proximity classification requirements and/or preference filters are excluded from the associated neighbor group.\nIn one exemplary embodiment, an existing proximate group will have an associated log-on key so that agents may be joined in to the proximate group following the structure in FIG. 4.\nSuch log-on key usage may be open (all agents), or limited (some agents), or restricted (parent agent) only.  Such log-on usage may be determined at the system level.  Combinations of the usage structure may be used.\nIn certain implementations, it may be possible for agents that have been discriminated into a proximate group to sever ties with (to leave) the proximate group.\nThe neighbor agent subgroup(s) and communication links thus established are then used, in part, for subsequent communication and classifications as shown in FIG. 7 and FIG. 8.\nThe proximate group associations and communications channels may be stored for semi-permanent use.  In such instances, the associates are maintained even if a subsequent discrimination request, using the exact same parameters, would not result\nin the same proximate group.\nThe proximate group associations may be programmed to terminate in a specified time, or may be terminated instantaneously by the parent agent.\nIn one implementation, and agent may be removed from the proximate group following a period of inactivity.\nThe temporal nature of the agent membership in the proximate neighbor group is established by combinations of preference settings of the parent agent, the neighbor agent, and the system parameters.\nThe settings established during the initialization process determine the communications that transpire during the discrimination operation.\nIn certain implementations, the communication settings may be adjusted by some or all of the user agents.\nThe communication settings may be different for different agents in the proximate group.\nUpon discrimination, the presence of an agent in the proximate group may be transmitted back to the parent agent.\nUpon discrimination, the presence and any combination of location and other identifying features may be transmitted from one or more of the neighbor agent to the parent agent.\nOpen discrimination, notification of the discrimination operation may be transmitted to one or more potential neighbor agents.\nOpen discrimination, notification of the discrimination operation any combination of location and other identifying features may be transmitted from the parent agent to one or more potential neighbor agents.\nThe transmitted information to/from agents may be representative in that the information may be stored on one or more network entities and transmitted to or from said network entity.\nIn one implementation, the discrimination operation is reinitiated periodically, as the parent agent location moves.\nIn another implementation, the discrimination operation is periodically reinitiated as the proximate zone following the moving path of the parameterized discrimination target.\nIn one implementation, the structure of the communication (restricted communication, open communication, structured communication, and free communication) is established at the outset by the parent agent, as part of the permissions in the query.\nIn one implementation, the structure of the communication (restricted communication, open communication, structured communication, and free communication) is established at the system level.\nCombinations of the communication structuring process may be used, depending on the requirements of each particular implementation.\nThe layer structure may be initialized at a system level, or may be set up by the user during the initialization stage, or may be initially tailored by implementation at inception.\nIn one implementation, the layer structure may be altered using a list of layers available at the system level.  In such an implementation, layers may be initialized (populated with relevant data by the user).\nIn one implementation, layers may be created and added by user agents.\nIt may be possible to send or receive layer structures, as in the form of an invite.\nLayer structures may be added, as in layer allowances, using a trade metric.\nLayers may be active or inactive and have permission preferences.  For example, an agent may have initialized the auto-classification layers.  \"Republican\", and \"Sierra Club\".  Said agent may have the layer \"Republican\" active and the layer\n\"Sierra Club\" inactive.  In such an implementation, for example, a parent agent query would reveal the agent's \"Republican\" classification, but would not reveal the agent's \"Sierra Club\" classification.\nDifferent levels of information transmitted, or restricted based on agent layer user preferences.  For example, that the agent has a \"Sierra Club\" layer at all may be transmitted, but not the related classification.  As another example, the\nagent may appear invisible to other agents viewing by layer, and choosing to view the \"Sierra Club\".\nIn one implementation, permissions and preferences on layers may be determined at an agent level and controlled by the agent.\nIn another implementation, permissions and preferences are controlled at a system level.\nLayer permissions and preferences may be established on a layer by layer basis.\nCombinations of the approaches to layer permissions and usage are possible.\nSeveral exemplary embodiments are now described to illustrate the potential utilities available within the present structure.  The examples are in no way restrictive.  They are constructions that demonstrate a variety of implementations.\nIn one exemplary implementation, the structure described herein is utilized to create a local communication group.  A query is initiated to develop a proximate group of agents, amongst which communication may occur.  Following the process shown\nin FIG. 14, for example, a proximate group of agents is established.  The group, exemplified as shown in FIG. 7, then has an established communication link motivated by certain commonality, i.e., all at the same bar.\nThe discrimination operation is periodically performed to establish and refine the group of agents that are presently within the proximate region.  In this manner, new agents within the locality are added to the proximate group and agents that\nhave left the locality are removed from the proximate group.\nIn one exemplary implementation, potential member agents have their agent presence set up so that any proximity query results in notification of the member agent's classification status upon discrimination.  The structure results in a form of a\ndigital bumper sticker in which agents prominently display certain traits upon discrimination.  For example, the agents may have the binary classification layer \"Republican\" with the computer machine set as \"yes\", reference FIG. 10.\nDuring a query by any agent, the discrimination communication from the neighbor agent back to the parent agent, shown as 3046 in FIG. 14, contains trigger information to automatically notify the parent agent regarding the classification status\nof the neighbor agent.\nThe notification is independent of any direct communication, or subsequent classification queries.  In essence, said classification information is transmitted at first contact, thus forcing the classification to prominence.\nSuch classification notification is the first part of any communication to the parent agent or to other member agents.\nAn extension of the implementation may incorporate notification for more than one classification layer.\nAn extension of the implementation may incorporate notification for more than one classification layer\nIn one exemplary implementation, a parent agent initiates a selective search in which a refined proximate group is determined containing only those neighbor agents that have a particular layer and classification status.  For example, the parent\nagent query for \"Republican\" agents will indicate the existence of proximate agents that have the layer \"Republican\" and an auto classification \"yes\".\nSuch an implementation may proceed as shown in FIG. 14, in which the selective layer status is an additional parameter utilized within the operations performed by network agent, 3040, to further discriminate the proximate group to satisfy the\nselective classification request, i.e., \"Republican\".\nSuch an implementation may also proceed by first performing a proximity query without the aforementioned layer selectivity.  The base proximate group is then divided by a network agent into a selective discriminant subgroup, per the initial\nrequest, and a complementary proximate subgroup of agents that do not meet the selective search criterion of the initial search.\nIn another exemplary implantation a parent agent may initiate a selective search in which a refined proximate group is determined containing only those neighbor agents that do not have a particular layer and classification status.\nThe implementation may be extended to selective searches that are for combinations of attributes within the proximate subgroup.\nIn one exemplary implementation using the structure described herein, a parent agent initiates a query to develop a proximate subgroup from which classification information is gleaned.  As one particular example, an attraction layer is utilized\nso that the parent agent may determine if there are agents in the proximate group which are likely to find the parent agent attractive.  The situation is as depicted in FIG. 8\nThe process is proceeds as depicted in FIG. 14, in which a parent agent query discriminates neighbor agents.  The neighbor agents in the proximate group might be, for example, all participating agents with 100 feet of a certain locale.\nUpon discrimination, the neighbor agents are classified as shown in FIG. 15, which depicts the classification interaction between the parent agent, 205, and an associated neighbor agent, 210.  The parent agent is shown with one active layer,\n750, and two inactive layers 765.  The associated neighbor agent is shown with one active layer, 775, and two inactive layers, 765.\nThe parent agent feature data, 755, from the active layer, 750, is transmitted, 770, to the computer machine, 785, of the associated active layer, 775, of the associated neighbor agent, 210.\nThe computer machine, 785, on the associated neighbor agent active layer, 775 operates on the feature data to create a classification result which is transmitted, 790, back to the communication/notification interface, 555, of the parent agent. \nThe feature data, 780, of the associated neighbor agent, may or may not be utilized in the classification process.\nThe classification may involve notification and communication through the associated agent communication/notification module, 555.\nThe process depicted in FIG. 15 is repeated for each of the associated neighbor agents in the proximate group.\nUsing the aforementioned example of attraction, the feature data of the parent agent may consist of demographic data, descriptive data, and statistical data that have been previously generated as part of the training process for the attraction\nmetric.\nThe computer machine in the corresponding layer of each associated neighbor agent may consist of a computer construct generated to statistically capture the neighbor agent's attraction preferences.  The construct has been previously generated as\npart of the training process for the attraction metric.\nAdditional attraction metric structures outside of the present example are available.\nThe classification process, using the computer machine, 785, is symbolically contained in the associated neighbor agent, 210.  The classification may be performed in part or in whole on an external device.\nFIG. 16 depicts an agent, 800, with one active layer, 775, and two inactive layers, 765, utilizing external computing resource to process a classification operation for the active layer.  Relevant data, i.e., feature data from the parent agent,\nis transmitted, 810, from the agent computer machine, 785, to a network agent, 700, that provides computational resource to perform the classification operation.\nSome classification operations utilize externally provided data.  Such externally available data may obtain through a query to an external source.  The request is transmitted, 810, from the computer machine, 785, to the network agent, 700, and\nthen transmitted, 815, to the network resource server, 805, The data from the external source is downloaded, 820, to the network agent, 700, for processing.  The data and is then transmitted, 825, to the agent computer machine, 785.  Additional cycles of\nthe data search and retrieval may be performed as part of the classification operation.\nExternal data and external computer processing may be performed in any combination by the network agent, 700, and network resource server, 805.  Processing may also be performed in whole or in part in combination by the computer machine, 785,\nand the external resources.\nFeature data, 780, from the agent active layer, may or may not be utilized in the classification process.\nNotification to the parent agent may use any combination of the agent interaction types, anonymous, non-anonymous, one-way, and two-way.\nIn one exemplary implementation, the agent structure and classification mechanisms described herein are used to create a mechanism by which the political predisposition of manuscript authors are determined and used to tag (classify) the\nassociated work.  The process is different from other example implementations described herein in that the classification operation is not performed on an agent\nThe classification uses the same process as shown in FIG. 16, where the name of the author of the article is supplied to the computer machine, 785, by the agent communication/notification interface, 555.  The computer machine, 785, transmits,\n810, the author name to a network agent, 700.  A data query, 815 is made to network resource server, 805, where data mining techniques and natural language processing are used to assess the sentiment bias of the article author.  The bias assessment\nresults are transmitted back, 820, to the network agent, 800, and then back, 825, the computer machine, 785.  The final result is transmitted to the notification interface, 555, where the article is tagged according to the author bias.\nClassification results may be stored as multivariate classification structures, reference FIG. 12 to save on future computational overhead.  The classification bias may be refreshed at any time, or may be performed every time.\nNumerous data mining and classification operations are available using existing art, for example, natural language processing and sentiment analysis, or a statistical preference model following a market basket analysis.\nThe present structures and processes permit the implementation of peer to peer data exchange with dynamic demand.\nIn one exemplary embodiment, a peer to peer exchange for real time video with proximity driven dynamic demand.\nThe process is depicted in FIG. 17, which shows a parent agent, 205, which queries for video at the target location, 850.  Information concerning the request is transmitted, 870, to an adjacent network agent, 700, and then to another network\nagent, 701, adjacent to the target location.  A zone of proximity, 860, is established and scanned, 880, for candidate source agents therein.\nShould no appropriate agent exist in the zone of proximity, the zone is monitored until a member agent which is a potential neighbor agent, 685, enters the zone and becomes neighbor agent, 210, which is a candidate source agent.\nThe parent agent request is enunciated to candidate source agents.\nWhen a candidate source agent accepts the parent agent request, the video is uploaded, 885, to the network agent, 701, and transmitted via the agent network back to the parent agent.\nThe video thus sourced and transmitted may be archived as part of a library structure for future availability, or may be abandoned.\nData other than video may be exchanged in the same fashion.  The intent is to provide dynamic demand for data of geographic and temporal importance.  For example, data may be requested, in real time on location at breaking news story.  As\nanother example, real time video may be requested following a Rose Parade float.\nLocation categorized information about agents creates an environment from which a game may be created.  The present exemplary implementation presumes that proximate group has been established at some location, i.e., a bar, and geographic\nposition data of each proximate group member agent has been openly broadcast.  Additionally, it is presumed that one or more agents have been classified, and the classification has been has been enunciated openly to the group anonymously.\nThe example configuration thus described is one in which the communication/notification module of each element displays the approximate location of each agent in the proximate group.  The classification data, not being linked to any geographic\ndata, is not attached to any particular one of the proximate agents.\nThe exemplary game challenge is for an agent to guess, based upon appearance, for instance, which of the proximate agents fit the available classification data.  The guess can be directed to a particular agent using the positional aspect of the\ncommunication/notification interface.\nThe guess can be verified as correct or incorrect using the available system agent data and the result can be communicated to the agent making the guess.\nVariations of the game are possible.  As another example, a proximate agent's classification with regard to some layer, i.e., \"Republican\" is made.  The guess is tested against system agent information, and the result is transmitted back to the\nagent making the guess.\nCertain embodiments described herein may generally be described through the process diagram shown in FIG. 18.\nThe proximity query, 1800, is the operation by which the parent agent initiates the formation of a proximate group.  The parent agent uses the communication/notification interface, 555, to select and enter proximity parameters, and communication\nand notification preferences.  The classification option is specified and active layers for classification are selected.  The proximity query parameters, communication/notification preferences, and classification options, are entered into the parent\nagent communication/notification interface to form a data input set that used as input in subsequent operations.  The communication/notification interface is used to commence the discrimination operation, 1805.\nWhen the discrimination option is commenced, 1805, the proximity query parameters, communication/notification preferences, and classification options that had been entered into the parent agent communication/notification interface are uploaded\nto a network agent.  The network agent uses the proximity parameters as input for the discrimination operation.  In certain implementations, other parts of the data entered during the proximity query, 1800, are also used as input to the discrimination\noperation, 1805.  The network agent uses network resources to determine which member agents, 685, satisfy the proximity parameters selected in 800.  The network identity and additional agent information relevant to the data input set entered in the\nproximity query, 1800, are obtained and utilized in the subsequent processes shown in FIG. 18.  This completes the discrimination operation, 805.\nThe notification process, 1810, provides announcements and indications to the agents concerning various aspects of the discrimination process, 1805.  The existences of each of the neighbor agents, 210, that are detected and discriminated during\nthe discrimination process, 1805, are transmitted to the parent agent, 205, from the network agent, 700.  Depending upon the communication/notification preferences, geographic information and/or additional neighbor agent, 210, data are also transmitted\nfrom the network agent, 700, to the parent agent.  Depending upon the communication/notification preferences, the member agents, 685, that are queried as part of the discrimination process are notified of the discrimination process.  Depending upon the\ncommunication/notification preferences, the neighbor agents, 210, that are established as part of the discrimination process are notified of their inclusion in the proximate group, and possibly geographic information and/or additional information about\nthe parent agent.  The notification process communicates the proximate group structure to the parent agent, 205, and neighbor agents, 210, and indicates the classification, 1820, and/or communication and data exchange, 1830, operations to follow.\nThe communication/classification decision point, 1815, shows the process branch, from which, the classification and communication operations begin.  If neighbor agent classification has been requested as part of the query, 1800, then the parent\nagent, 205, communication/notification interface, 555, provides a prompt for the classification operation, 1820, to commence, and also for communication and data exchange amongst agents, 1830, to commence.  Each of the operations, classification, 1820,\nand communication, 1830, commences upon execution by the parent agent, 205, at the decision point, 1815.  The classification operation, 1820, and communication operation, 1830, may be performed in either order.  Additional cycles of each may be\nperformed, as shown on FIG. 18.  If neither operation can be performed, the process stops, 1835.  Neither operation can be performed when the proximate group is vacated, or expires, or is terminated by the parent agent or network agent.\nUpon commencement of the classification operation, 1820, from the decision point, 1815, the classification operation, 1820, evaluates neighbor agents, 210, relative to feature data from parent agent, 755, or relative to other data independent of\nparent agent feature data, 755.  Using the communication links established as part of the discrimination operation, 1805, the selected classification layer data are transmitted from the parent agent, 205, to the network agent, 700, and then to the\nneighbor agents, 210.  The neighbor agents, 210 operate on the feature data, 755, and are classified accordingly.  The classification results are transmitted to the network agent, 700, and then to the parent agent, 205, where the results are displayed on\nthe communication/notification interface, 555.  Depending upon the communication/notification preferences, the neighbor agents, 210, may or may not be notified of the classification process.  Subsequent classifications may be initiated within the same\nprocess block, 1820, when the parent agent, 205, selects a different data layer for classification, and the classification process just described is repeated.  Each classification operation is completed when the neighbor agents, 210, are classified, and\nthe classification process results have been transmitted to the agents.  From the classification process block, 1820, control returns to the decision point, 1815.\nThe communication process, 1830, provides notifications and data exchange between agents in the proximate group, 225.  Using the communication links established as part of the discrimination operation, 1805, agents within the proximate group\ncommunicate and transmit data within the confines of the permissions and preferences established during the discrimination process, 1805.  The communication and data exchange is performed (or initiated) using the agent communication/notification\ninterface, 555.  The communication process, 1830, is completed when the message, email, text, video, or data file to or from the agent has been transmitted and received, or the process has been interrupted by the parent agent, 205, neighbor agent, 210,\nor network agent, 700.  From the communication block, 1830, control returns to the decision point, 1815.\nAdditional process structures are possible, describing implementation variations permitted by processes and structures described herein.\nThe process depicted on FIG. 18 is shown in greater detail in FIGS. 19A, 19B, and 19C.\nThe parent agent proximity parameters, 1900, are the operation by which the parent agent initiates the formation of a proximate group.  The parent agent uses the communication/notification interface to select and enter proximity parameters, and\ncommunication and notification preferences.  The classification option is specified and active layers for classification are selected.  Such parameters and such preferences may be stored as profile data and selected for subsequent queries.\nProximity parameters include geometric proximity parameters, and also additional parametric proximity parameters.  Geometric proximity parameters include location data, i.e.; centroid and radius of the proximity zone.  Additional proximity\nparameters include parametric data as permissible age, or sex.  Such parameters are part of each agent's information profile and/or part of (or not) an agent's layer data structure\nCommunication preferences indicate the level of notification and communication to be provided to and from agents during the discrimination and classification options, such as one-way non-anonymous communication in which the parent agent is to be\nnotified of the existence of agents in the proximate region, and also of the location of each agent in the proximate region.\nThe classification option indicates whether or not any classification operation will be performed on the agents in the proximate region.  The option to classify and to also establish a communication link is also provided.\nAdditional temporal and geographical specifications, such as the duration of the proximate grouping or multiple proximity region locations are also indicated.\nThe communication/notification interface is used to commence the query, upon which the proximity query data is uploaded to a network agent, 1905.  The data entered to establish the proximate zone discrimination and other communication\npreferences and permissions, are transmitted to a network agent, 700, in preparation for the subsequent operations.\nThe network agent, 700, transmits the proximity parameters, as appropriate to other network agents, 700, to facilitate agent detection, 1910.  Member agents, 685, that are within the geometric limits of the discrimination zone, 250, are detected\nand identified, 1910, by network agents, 700.  The network identity and agent data for agents within the geometric limits of the proximity zone are used in the subsequent discrimination operation, 1915.\nThe discrimination operation is shown in 1915.  Using the agent data obtained in 1910, the additional proximity parameters, beyond geometric proximity, are applied to each of the agents detected in 1910, to refine the group of agents.  Agents\nthat satisfy the geometric proximity parameters and that also satisfy the additional proximity parameters are categorized as neighbor agents, 210.  Agents that do not satisfy all of the proximity parameters are categorized as excluded agents, 255.  The\ndiscrimination operation, 1915, stops when all of the agents detected in 1910 have been adjudicated relative to all additional proximity parameters.  The network identities, agent data, and categorization (neighbor agent, 210, or excluded agent, 255) are\nutilized as part of the input for the notification process, 1920.\nThe parent agent, 205, is notified regarding the process and results of the detection, 1910, and discrimination, 1915, operations.  The existence of each of the neighbor agents, 210, that are detected, 1910, and discriminated, 1920, are\ntransmitted to the parent agent, 205, from the network agent, 700.  Depending upon the communication/notification preferences, geographic information and/or additional neighbor agent, 210, data are also transmitted from the network agent, 700, to the\nparent agent.  The notifications are utilized as input for the subsequent parent agent, 205, decision point, 1925.\nThe notifications provided in 1920 are displayed on the communication/notification interface, 555, and reviewed by the parent agent, 205, from which a decision, 1925, to revise the proximity parameters is made.  If the resultant proximate group\nof agents, 225, is not satisfactory, the proximity parameters may be revised, to restart the process, 1900.  A prompt on the communication/notification interface is provided to either revise the proximate group, 225, or to accept the proximate group,\n225.  If the resultant proximate group of agents, 225, is satisfactory, the neighbor agent data and proximity query preferences are utilized as input for the neighbor agent notification decision point, 1930.\nThe decision to notify neighbor agents, or not, is shown in 1930.  The neighbor agents, 210, are notified of their inclusion in the proximate group, 225, if the proximity query preferences input in 1900 indicate notification to neighbor agents. \nIf the notification is indicated, a message is sent from the network agent, 700, to each of the neighbor agents notifying them of their inclusion into a proximate neighbor group.  Depending upon the preferences set forth as part when establishing the\ndiscrimination parameters, geographical and/or other data about the parent agent, 205, is transmitted from the network agent, 700, to the neighbor agents, 210.  Following notification, the process control flows to the decision point in which the parent\nchooses, 1940, to classify the neighbor agents, or communicate with the group of neighbor agents.\nThe communication/classification decision point, 1940, shows the process branch, from which, the classification and communication operations begin.  If neighbor agent classification has been requested as part of the query parameters, 1900, then\nthe parent agent, 205, communication/notification interface, 555, provides a prompt for the classification operations to commence, and also for communication and data exchange amongst agents, 1945, to commence.  Each of the operations, classification,\nand communication commences upon execution by the parent agent, 205, at the decision point, 1940.  The classification operations, and communication operations may be performed in either order.  Additional cycles of each may be performed, as shown on FIG.\n19.  If neither operation can be performed, the process stops, 1980.  Neither operation can be performed when the proximate group is vacated, or expires, or is terminated by the parent agent or network agent.\nIf classification is chosen at the decision point, 1940, the process control flows to process block 1950, where the parent agent, 205, information is transmitted, 1950, to each neighbor agent, 210.  The feature data, 755, from the parent agent,\n205 is transmitted to a network agent, 700, and then to the computer machine, 785, of each neighbor agent, 210.  The feature data is used as input for the subsequent classification, 1955.  The data transfer, 1950, is complete when the feature data has\nbeen transmitted and received by each of the neighbor agents.\nProcess control transfers to the classification operation, 1950, the computer machine, 785, of each neighbor agent, 210, operates on the parent agent, 205, feature data, 755, to obtain a classification result.  The process is complete when each\nof the neighbor agent's classification is established.  Process control then flows to process block 1960 where the classification results are communicated.\nThe classification result of each neighbor agent is transmitted to a network agent, 700, from when it is transmitted to the communication/notification interface, 555, of the parent agent, 205.  Depending upon the preferences established by the\ninitial proximity parameters used to initiate the formation of the proximate group, 1900, the notification may or may not be provided to neighbor agents that they have been classified.  The classification results are viewed on the parent agent, 205,\ncommunication/notification interface, 555 Tithe classification transmission, 1960, is complete when all of the neighbor classification results have been transmitted to the parent agent, 205.  The process then returns to the classify or communicate\ndecision point, 1940.\nIf communication is chosen at the classify or communicate decision point, 1940, the process control flows to process block 1945, to commence inter-agent communication amongst agents in the proximate group, 225.  The communication process, 1945,\nprovides notifications and data exchange between agents in the proximate group, 225.  Using the communication links established as part of the discrimination operation, 1915, agents within the proximate group, 225, communicate and transmit data within\nthe confines of the permissions and preferences that are established during formation of the proximate group, 225.  The communication and data exchange is performed (or initiated) using the agent communication/notification interface, 555.  The\ncommunication process, 1945, is completed when the message, email, text, video, or data file to or from the agent has been transmitted and received, or the process has been interrupted by the parent agent, 205, neighbor agent, 210, or network agent, 700. From the communication block, 1945, control returns to the classify or communicate decision point, 1940.\nEmbodiments are not limited to processes described in FIGS. 18, 19A, 19B, and 19C.\nAccording to exemplary embodiments of the present invention, three novel implementations of facial recognition and data management technology for social applications are presented; a facial rolodex, a reverse facial rolodex, and a set social\nnetworking applications.  Each of the three implementations may be utilized independently for use on a computer or mobile device, for example.\nThe third implementation (social networking applications) requires a metric (or combination of metrics) according to certain embodiments.  A particularly suitable metric may be derived from FRT.  The third implementation is described\nindependently from the choice of metric (to admit application with any suitable metrics).  The novel metric derived from FRT is subsequently described.\nThe facial rolodex application is a facial data management application, that may or may not be facilitated using FRT.  The concept is to have an application in which a name may be associated with a face in an organized fashion.  By using a\nlinked face-name database, the user may call up a picture of a name that is either typed, spoken using suitable voice recognition technology (VRT), or in the text of an existing document.  The essence of this application is to simply place a face with a\nname.  The following features or design elements are presented:\nFacial look-up.  The application retrieves an associated facial image by typing a name, or by speaking a name, or by scrolling through a list or document to find a name.\nTagged text in a document.  The application presents a face image corresponding to a name in a text.  The image is either being permanent or transient, and may be toggled on or off.  The face image retrieval may either be passive (as when the\ncursor or selector icon is floated above the text) or active (as when the name text is highlighted and a keystroke or button is used to activate the facial image retrieval).  The application may be embedded upon existing text, or may be applied over text\nas it is typed (as when a face is called up simply by typing the name, as the name is typed).\nText tagging.  The application is used to create a real time pop-up of an associated facial image for text messages or emails received.\nConversation tagging.  The application is used to retrieve an associated facial image (and biographical information) for the names of people that are associated to snips of conversation.  This application may be used for ambient conversations,\nor for telephone or radio conversations.  It associates a face with the speaker.  For example, during conference calls, the application uses VRT to identify the person speaking, and retrieves the associated facial image (if the speaker is in the database\nlibrary).\nConversation Capture.  The application identifies names that were spoken during conversations and retrieve associated images for the spoken names.  The kernel of the application functions just as does conversation tagging, however the feature\nthat triggers the data retrieval would be a spoken name.\nOrganizational Structure.  The name-face database library may be augmented by the user in the same fashion that contact information is added and edited.  The libraries may be shared or merged and may be organized with associations, i.e.; a\nname-picture for your neighbor may have tree-like connection to the neighbor's wife, etc. The organizational structure may be presented in different fashions, as in a tree diagram with connections (relationships) and nodes (entries).\nSocial Networking.  The facial rolodex application and organizational structure may be implemented along with social media to allow a social media presence to be linked to a face-name data pairing.  The application may be tasked with data mining\nthe present webpage to create new, and develop existing, name-face-data data association's database.\nDatabase building.  Any element of the associations, name-face-data, for an entry in the database may be added to an existing entry, or entered to create a new database entry.  The database entries may be modified, completed, augmented or edited\nusing additional information that becomes available through the processes to which the application is directed.\nThe reverse facial rolodex application is a facial data management tool that uses FRT to identify faces and retrieve social contact and other information attached to a face in the name-face library.  It allows the user to user to identify a\nfacial image and allow a user to also retrieve data associated to a person.  The essence is to simply place a name to a face.  The following features or design elements are presented.\nDirect image sensing.  The application may be utilized to access archival information linked to a facial image in the database that is entered with a cell phone.  The cell phone may be used to scan a face or a crowd and thus retrieve information\nabout any faces from the images that are in the facial image database.\nEmail image input.  The application may be used to recall linked information attached to a facial image in a picture embedded in or attached to an email.\nWebsite image capture.  The application may be used to retrieve linked information associated with any facial image from the database that is contained in a photograph that is attached to, linked to, or embedded in a webpage.  The application\nmay be tasked with drilling down and seeking facial images at some level of association with the present webpage.\nDocument image capture.  The application may be used to capture images embedded, linked, or tagged to a document and retrieve associated information for any image in the database.\nVideo image capture.  The application may be used to retrieve linked information associated with any facial image from the database that is contained in a video.  Any of the resources, from which an image is made available to the application may\nalso be used to convey or provide video to the application.\nSocial Networking.  The reverse facial rolodex application and organizational structure may be implemented along with social media to allow a social media presence to be linked to a name-face data pairing.  The application may be tasked with\ndata mining the present webpage to develop and populate associated data linked to an image in the database, and to develop new name-face-data associations to add to the database.\nDatabase building.  Any element of the associations, name-face-data, for an entry in the database may be added to an existing entry, or entered to create a new database entry.  The database entries may be modified, completed, augmented or edited\nusing additional information that becomes available through the processes to which the application is directed.\nMethods have been developed for facial recognition using Eigen space analysis.  The Eigen face approach, casts the identification and classification of facial images as a mathematical problem, utilizing vector space analysis, as used in numerous\nfields of science, math and engineering.\nMost young people (in particular) are concerned with their attractiveness to other people.  When any individual describes attractiveness (how attractive someone is), there is some continuum; extremely attractive, very attractive, somewhat\nattractive, somewhat unattractive, etc. Thus, a metric of attraction can be described, and the metric may be used in a social application.\nSince the Eigen face concept codifies a facial image as an element of a large dimensional vector space, which is axiomatically also a metric space (with a Euclidean metric), it provides a mathematical structure by which attractiveness may\nquantified.  This notion permits the development of the broader idea of a social application that addresses the issue of attractiveness according to certain embodiments.\nThe following discussion describes applications according to certain embodiments, which ultimately may be used with a variety of metrics, or combinations of metrics.  Following the discussion of several applications, a specific metric obtained\nby using facial recognition Eigen space is described in detail according to certain embodiments.  An additional suitable metric using neural networks is also described, according to other embodiments.\nThe application is addresses the issue of attractiveness, and the perception, by others, of one's attractiveness.  A person may determine how attractive he or she is to another individual without direct input or interaction with that individual. Additionally, an individual may get some measure of how attractive he or she would find another person without visually making that assessment.  The ideas are extended according to certain embodiments to situations that arise from the information that\nbecomes available with a suitable attractiveness metric.\nIf there exists a suitable metric of attractiveness, and if information or parameters necessary to establish the metric which describes their attraction to others is available, then certain measurements may be performed.  More precisely,\ndifferent parties could make the measurement of attraction.  For example:\nAn individual could measure how attractive any other person would be (A measures how attractive B is to A).\nA person could measure how attractive an individual would find that person (B measures how attractive B is to A).\nA third party measures attraction (C measures how attractive B is to A and vice versa).\nProvided that the individual's metric can be parameterized in some sufficiently convenient manner, then the metric may be reconstructed and thus applied as a test of any person's attractiveness.  Thus, a virtual device is created that determines\nhow attractive person B is to person A.\nThe application of the metric concept according to certain embodiments may be utilized for any social application website such as Facebook or Instagram.  The utilization may take several forms, as listed below in certain exemplary embodiments:\nImage Classification--(A measures) The user may actively or automatically classify incoming images (or linked persons) by attractiveness to the user.  The resulting classification may simply be private, as a convenience to the user.  The\nresulting classification may also be made public (open to all), or semi-public (shared with chosen users).\nReverse Image Classification--(B measures) The user may measure how attractive he or she is to the person depicted in incoming images (or linked persons).  The resulting classification may be private (to B) or shared with the owner of the image\n(or linked person).\nThe application of the metric concept may be utilized as a search engine, or to modify a search engine or search results.  The utilization may be used in a dating website search, a social networking search, or on a generic search engine search.\nAttraction Search--(A measures) A search may be made, using the metric, for people that are attractive to A.\nReverse Attraction Search--(B measures) A search may be made, using the metric, for people that would find An attractive.\nSearch Modification--A search may be made using traditional methods (keywords, AI, for instance) and modified using the metric.  For example, search for brunettes, named Elizabeth, that find a certain person attractive.\nThe application of the metric concept could be used by a third party (C measures) in fixed or mobile applications--(C measures).  For example, person A may test to see if a person that person A meets at work would be attractive to person A's\nneighbor.  As a different example, person A may test to see if a person A meets at work would find person A's neighbor attractive.\nThe metric may be used as a stand-alone website or mobile application, in which users may test themselves by other users, and test other users.  They may also search users for attractive or attracted users.  The stand alone may serve as a dating\nwebsite, or simply as an interesting waypoint, by which, members could judge their attractiveness.\nThe application of the metric concept may be utilized for a mobile application that utilizes position information to augment the measurement available from the metric.  The essence of the application is that someone may see that there are people\nthat find that person attractive in the same locality (at a party, for instance).\nBlind Annunciation--The metric parameters of an individual's attraction metric are broadcast locally according to certain embodiments.  The metric is then used by other users in the same locality to test their own appearance.  Thus, a person can\nsee if there are other individuals in the location that would find that person attractive.  No specific information about either person or either person's precise location is exchanged, according to certain embodiments.\nOpen Annunciation--The metric parameters of an individual's attraction metric are broadcast locally, according to certain embodiments.  The metric may then use by other users in the same locality to test their own appearance.  Thus, a person can\nsee if there are other individuals in the location that would find that person attractive.  However, in contrast to the blind annunciation described above, some or all available information about one person, or the other (including, possibly, precise\nlocation) is transferred, according to certain embodiments.\nThe application is essentially the same as the mobile application in certain embodiments, except the information is integrated into a network so that a user can view information about the people at a remote location, such as at a restaurant.\nThe application according to certain embodiments provides an objective test for attractiveness that is based upon a metric derived from a group of people.  The group metric may then be used to test the attractiveness of an individual to a group. This utilization may thus be used to see if a particular individual is attractive to a group.  This implementation may be used for hiring and selection in professions where appearance is important.\nBy nature, the utilizations described herein present themselves to gathering all manner of statistics regarding attractiveness.  The statistics may be used to determine if a person is attractive to some particular demographic.  It may also be\nused to extract trends.\nOne particular formulation of FRT, Eigen faces, is cast in a mathematical formulation that permits allows the development of an attractiveness metric that is particularly well suited for use in the social networking application described above. \nThe Eigen face concept treats facial recognition as a mathematical problem (eigen-analysis) in large scale, finite-dimensional vector space.\nIn certain embodiments, a photograph of a face is depicted as a rectangular array of intensity values.  It can thus be represented as a vector of intensity values.  For example, a 256.times.256 image can be expressed as a vector in\n65,536-dimensional space.  Prior artists have noted that a group of such images can be sufficiently well represented when the images are projected onto an appropriate subspace of the original 65,536-dimensional space.  The appropriate subspace is\ndetermined by performing an eigen-analysis on the covariance matrix of the group of images.  Further, images that are not members of the sample group can be classified as facial images, or as images of something else by determining the proximity of the\ntested image to the subspace comprised of the sample images.  The entire formulation is developed as a facial recognition technology that is effective and robust, that requires relatively low computer storage and relatively low computational effort.\nThese formulations demonstrate the practical use of the mathematical structure (vector space) that is available when a facial image is described as a finite dimensional vector.  The present development according to certain embodiments\n(attractiveness metric) capitalizes on those very same mathematical elements; finite dimensional vector space, subspace, eigen-analysis, norm, and metric.  For sake of description, the following discussion is presented in the context of 256.times.256\nfacial image representation.  Note that the ideas may be directly extended to other image sizes, depending on the particular requirements of each implementation.\nA finite dimensional real vector space has a Euclidean norm, and by simple extension, has a metric (norm of the difference of two vectors).  This is a familiar metric, however it must be noted that it is not the only metric that can be\nconstructed which will satisfy the definitional property of a metric.  Utilizing the subspace of images screened for attractiveness and/or the subspace of images screened for un-attractiveness presents an opportunity to utilize any such metrics available\nwithin the vector space construction, plus any combination of metrics and other features (brunette, Catholic, etc.).\nA metric for attractiveness may be formed for an individual by presenting the individual with a test set of 256.times.256 facial images which the individual categorizes as attractive, or not attractive.  The attractive images form a subspace in\n256.times.256 face space.  The attraction metric is created by utilizing the vector space metric that is available as part of the mathematical structure (vector space) utilized.  The distance of the test image vector from the attractive image subspace is\nthe attraction metric; a very close image is deemed attractive, and a very distant image is deemed not attractive.\nA metric for attractiveness may be formed for an individual by presenting the individual with a test set of 256.times.256 facial images which the individual categorizes as attractive, or not attractive.  The non-attractive images form a subspace\nin 256.times.256 face space.  The attraction metric is created by utilizing the vector space metric that is available as part of the mathematical structure (vector space) utilized.  The distance of the test image vector from the non-attractive image\nsubspace is the attraction metric; a very close image is deemed attractive, and a very distant image is deemed not attractive.\nA metric for attractiveness may be formed for an individual by presenting the individual with a test set of 256.times.256 facial images which the individual categorizes as attractive, or not attractive.  The attractive images form a subspace in\n256.times.256 face space and the images that are non-attractive form the complementary subspace in 256.times.256 face space.  The attraction metric is created by utilizing the vector space metric that is available as part of the mathematical structure\n(vector space) utilized.  The distance of the test image vector from the attractive image subspace and the distance of the of the test image vector from the non-attractive image subspace provide two numbers used to gauge the attractiveness of the test\nimage.\nA metric for attractiveness may be formed for an individual by presenting the individual with a test set of 256.times.256 facial images to which the individual assigns attraction values (1 to 10, for example).  This scaling is simply used as a\ndiscrimination parameter for the division into attractive, and not-attractive subspaces used in the metrics described above.\nA metric for attractiveness may be formed for an individual by presenting the individual with a test set of 256.times.256 facial images to which the individual assigns attraction values (1 to 10, for example).  The test images form a subspace\nwith an associated scaling (in essence an additional dimension--think of the three dimensional plot in which the test vectors are in the X-Y plane, and the scaling is the associated Z-value).  The test image may be projected onto the sample space (closer\nto the attractive images, or closer to the un-attractive images) to see what the associated scale (attractiveness value) is for the projected test image.\nNeural networks are a structural form of artificial intelligence that are commonly applied to pattern recognition problems, such as facial recognition technology.  Neural networks are a kind of black box approach in which a network of simple\nbehavioral cells (perceptron's, for instance) are trained so that the response of the system adheres sufficiently well to a desired algorithm.\nIn the same fashion that a neural network may be utilized to identify an image as a face, and select a particular face from a training set, a neural network may be trained to identify a face as attractive or non-attractive according to an\nappropriate training set of images.  A neural network may therefore be used as attractiveness metric.  The network, a back propagation, feed forward network, for example, would provide discrete output signals such as attractive and non-attractive (two\noutput cells), or attractive, non-attractive, neutrally attractive (three output cells), etc.\nIn this manner, a neural network may be trained to create attractiveness metric for use in any of the aforementioned components of the attractiveness application.\nThere may be other combinations not explicitly presented here.  Therefore it is understood that the invention is not to be limited to the specific embodiments disclosed, and that modifications and embodiments are intended to be included as\nreadily appreciated by those skilled in the art.\nWhile the above description contains many specifics and certain exemplary embodiments have been described and shown in the accompanying drawings, it is to be understood that such embodiments are merely illustrative of and not restrictive on the\nbroad invention, and that this invention not be limited to the specific constructions and arrangements shown and described, since various other modifications may occur to those ordinarily skilled in the art, as mentioned above.  The invention includes\nany combination or sub combination of the elements from the different species and/or embodiments disclosed herein.", "application_number": "16186149", "abstract": " Exemplary methods and systems are presented for social networking\n     applications using autonomous mobile agents. Communication links are\n     established based on geographic proximity and distance described as a\n     domain in which resident agents are detected and identified. The\n     communication links thus established allow platform independent\n     communication along communication channels that are dynamically derived.\n     Incorporation of computer machines and feature data sets permit agent\n     classification and inter-agent notification of classification results.\n", "citations": ["6826414", "7574606", "8315211", "8554172", "9100709", "20030009589", "20070070935", "20100015991", "20100280837", "20100322215", "20110039539", "20120028623", "20120322484", "20130028163", "20140016494", "20160119768"], "related": ["14217173", "61800009"]}, {"id": "20190095830", "patent_code": "10373091", "patent_name": "Systems and methods for investigating and evaluating financial crime and\n     sanctions-related risks", "year": "2019", "inventor_and_country_data": " Inventors: \nEpstein; Matthew (Santa Monica, CA), Schmidt; Benjamin (Santa Monica, CA)  ", "description": "<BR><BR>COPYRIGHT NOTICE\nThe present application includes material that is subject to copyright protection.  The copyright owner does not object to the facsimile reproduction of the application by any person as the application appears in the records of the U.S.  Patent\nand Trademark Office, but otherwise reserves all rights in the copyright.\n<BR><BR>FIELD OF INVENTION\nThe present invention is generally directed to systems and methods for investigating and evaluating financial crime and sanctions-related risks.  More particularly, the present invention is related to systems and methods involving an internal\nproduction environment configured to interact with analysts and an external production environment configured to interact with customers.  The combination of the two environments and other mechanisms help maintain and evaluate financial crime and\nsanctions-related risks.\n<BR><BR>BACKGROUND OF THE INVENTION\nToday's banking and financial systems are subject to complex regulations that can subject those institutions to significant fines, other regulatory penalties, and reputational risk.  Various procedures and systems have been developed to assist\nthese institutions in monitoring, investigating, and avoiding risks addressed by such regulations.  Financial crime and sanctions-related risks can arise from directly or indirectly conducting or supporting financial transactions with entities under\nsanctions or otherwise engaged in illicit activity.  These regulations can be complex and can involve identified entities and other entities that fall under sanctions by association.  There are a number of existing systems but they have been found thus\nfar to be inadequate in providing the institution with a cogent and quick representation of the risk as new situations arise.  There is a tremendous amount of information, and understanding and assessing the information down to relevant parts has been\ndifficult to achieve.  The efficiency in providing such services and the way the information is interactively made available have also been inadequate.\n<BR><BR>SUMMARY OF THE INVENTION\nIn accordance with principles of the invention, a computer implemented system for providing a visual interactive software tool that permits users to investigate and evaluate financial crime and sanctions-related risks is contemplated.  In one\nembodiment, the system may comprise a graph database configured to store edges, nodes, and properties, a first subsystem configured to store and publish a style guide over a network, a document library, an internal production environment configured to\ninteract with analysts, and an external production environment configured to interact with customers.  The graph database is located in nonvolatile memory or stores nodes, edges, and properties in nonvolatile memory.  The first subsystem is implemented\non a computer configured with volatile and non-volatile memory and configured to communicate over a network.  The style guide includes a plurality of data models that are identified in the style guide to model different categories of situations and\ncontains requirements for a structure and format of data used to enter nodes, edges, and properties for a particular data model.  The document library is also implemented on a computer configured with volatile and non-volatile memory and to communicate\nover a network.  The document library can communicate with different sources to receive information to be reviewed by analysts and save received information that is used by analysts to enter nodes, edges, and properties.  Each of the internal production\nenvironment and the external production environment is implemented on one or more servers configured with volatile and non-volatile memory and configured to communicate over a network.  The internal production environment can communicate with analysts'\ncomputers over a network.  The external production environment can communicate with customers' computers over a network.\nThe internal production environment implements an electronic online workspace for analysts.  The internal production environment configures the electronic online workspace to provide analysts with a software interface to enter a first category\nof information to identify and input individual nodes and to enter a second category of information specifying edges (i.e., different types of relationships) between the nodes in the first category.  The inputted nodes include source nodes and corridor\nendpoint nodes.  The internal production environment, in response, stores the entered nodes, edges, and properties in the graph database.\nThe internal production environment also comprises a software engine that finds corridors.  The software engine, in response to the entered nodes, edges, and properties, automatically traverses pathways which are available starting from each\nsource node through connected edges and nodes.  The software engine identifies any new pathways and traverses the pathways until it reaches a corridor endpoint node before a maximum number of degrees of node-traversal as specified by the software engine\nhas been reached, or it reaches a node or edge in the traversal process that has no risk relevance or low risk relevance for the purpose of corridor generation.\nThe software engine then eliminates some of the identified pathways.  The elimination process includes eliminating pathways that include the nodes or edges that have no risk relevance or low risk relevance for the purpose of corridor generation\nand the pathways that include a total number of nodes at or above the maximum number of degrees of node-traversal.  The software engine stores a set of weights in correspondence with the different types of edges and assigns weights based on the type of\nedge or connected node to the edges in each pathway.  In response to eliminating pathways and assigning weights, the software engine identifies the remaining pathways to be the resulting corridors for the source node and further based on a cumulative\nvalue of weights on edges in each resulting corridor specifies a variable degree of relevance to the resulting corridors.\nThe system also comprises a second subsystem configured to perform a publish process.  The publish process approves and transmits the resulting one or more corridors and the nodes, edges, and properties to an external production environment. \nThe second subsystem is implemented on a computer configured with volatile and nonvolatile memory to communicate over a network.\nThe external production environment is configured to receive and store the one or more resulting corridors and the nodes, edges, and properties in an external graph database which aggregates corridors and nodes, edges, and properties and\nmaintains edge connections between nodes in different corridors.\nThe external production environment comprises an electronic online customer platform.  The platform includes a visual interactive interface having interactive tools.  The tools include a search engine that provides keyword searching capability\nthat surfaces possible matching nodes in the external graph database, that provides the user with the ability to add an identified node to the visual workspace, that generates a visual graphic as representation of the identified node in the workspace,\nthat communicates that the identified node has a number of connections (connected nodes) that include a number of corridors, and that permits the user to select one of the connections or corridors to add to the visual workspace.  In response to adding\nthe connection or corridor to the visual workspace, the engine displays additional nodes and edges that are part of the connection or corridor using separate visual graphical elements for nodes and edges that visually illustrate the connection or\ncorridor.\nThe external production environment can communicate a relative relevance of a plurality of corridors based on the weights.  The external production environment is further configured to display underlying properties for the nodes and edges by\nretrieving the underlying properties from the external graph database.\nThe electronic online customer platform is further configured to permit the customer to add a node to the workspace that already contains the identified node.  In response to the addition, the workspace automatically displays a graphic\nrepresentation of a connection between the added node and the identified node, if a relationship between the two nodes exists.\nThe electronic online customer platform is further configured to permit the customer to add a corridor to the workspace that already contains the identified node.  In response to the addition, the workspace automatically displays graphic\nrepresentations of nodes and edges of the added corridor.\nThe electronic online customer platform is further configured to provide the customer with option to interactively select a node displayed on the workspace.  In response to the selection, the workspace communicates corridors associated with the\nselected node that can be added to the workspace.\nThe electronic online customer platform is further configured to provide the customer with option to interactively select a node displayed on the workspace.  In response to the selection, the workspace communicates other nodes associated with\nthe selected node that can be added to the workspace.\nIn accordance with principles of the invention, a computer implemented method for investigating and evaluating financial crime and sanctions-related risks is contemplated.  In one embodiment, the method comprises storing edges, nodes, and\nproperties in a graph database, storing and publishing a style guide over a network, receiving information in a document library to be reviewed by analysts and saving received information in the document library that is used by analysts to enter nodes,\nedges, and properties, implementing an internal production environment configured to interact with analysts, and implementing an external production environment configured to interact with customers.  The graph database is located in nonvolatile memory. \nThe style guide is stored in and published via a first subsystem implemented on a computer configured with volatile and non-volatile memory and to communicate over a network.  The style guide includes a plurality of data models that are identified in the\nstyle guide to model different categories of situations and contains requirements for a structure and format of data used to enter nodes, edges, and properties for a particular data model.  The document library is also implemented on a computer\nconfigured with volatile and non-volatile memory and to communicate over a network.  The document library can communicate with different sources to receive information to be reviewed by analysts and save received information that is used by analysts to\nenter nodes, edges, and properties.  Each of the internal production environment and the external production environment is implemented on one or more servers configured with volatile and non-volatile memory and to communicate over a network.  The\ninternal production environment can communicate with analysts' computers over a network.  The external production environment can communicate with customers' computers over a network.\nThe internal production environment implements an electronic online workspace for analysts.  The workspace is configured to provide analysts with a software interface to enter a first category of information to identify and input individual\nnodes or groups of nodes and to enter a second category of information specifying edges (i.e., different types of relationships) between the nodes in the first category.  The inputted nodes include source nodes and corridor endpoint nodes.  In response\nto entering the first and second category of information, the method stores the entered nodes, edges, and properties in the graph database.\nThe internal production environment implements a software engine that finds corridors.  In response to the stored nodes, edges, and properties, the method, via the software engine, automatically traverses pathways which are available starting\nfrom each source node through connected edges and nodes.  The method identifies, via the software engine, any new pathway and traverses the pathways until it reaches a corridor endpoint node before a maximum number of degrees of node-traversal as\nspecified by the software engine has been reached, or it reaches a node or edge in the traversal process that has no risk relevance or low risk relevance for the purpose of corridor generation.\nThe method then eliminates, via the software engine, some of the identified pathways.  The elimination process includes eliminating pathways that include the nodes or edges that have no risk relevance or low risk relevance for the purpose of\ncorridor generation and the pathways that include a total number of nodes at or above the maximum number of degrees of node-traversal.  The method stores, via the software engine, a set of weights in correspondence with the different types of edges and\nassigns weights based on the type of edge or connected node to the edges in each pathway.  In response to eliminating pathways and assigning weights, the method, via the software engine, identifies the remaining pathways to be the resulting corridors for\nthe source node and further based on a cumulative value of weights on edges in each resulting corridor specifies a variable degree of relevance to the resulting corridors.\nThe method also includes a publication process that approves and transmits the resulting one or more corridors and the nodes, edges, and properties to an external production environment.  The publication process is performed by a second\nsubsystem that is implemented on a computer configured with volatile and non-volatile memory and to communicate over a network.\nThe method also includes receiving and storing, via the external production environment, the one or more resulting corridors and the nodes, edges, and properties in an external graph database which aggregates corridors and nodes, edges, and\nproperties and maintains edge connections between nodes in different corridors.\nThe method includes implementing an electronic online customer platform in the external production environment.  The platform provides a visual interactive interface that comprises a visual workspace and interactive tools.  The tools include a\nsearch engine that is configured to provide keyword searching capability that surfaces possible matching nodes in the external graph database, that provides the user with the ability to add an identified node to the visual workspace, that generates a\nvisual graphic as representation of the identified node in the workspace, that communicates that the identified node has a number of connections that include a number of corridors, and that permits the user to select one of the connections or corridors\nto add to the visual workspace.  In response to adding the connection or corridor to the visual space, the method, via the engine, displays additional nodes and edges that are part of the connection or corridor using separate visual graphical elements\nfor nodes and edges that visually illustrate the connection or corridor.\nThe method further comprises communicating, via the external production environment, a relative relevance of a plurality of corridors based on the weights.\nThe method further comprises displaying, via the external production environment, underlying properties for the nodes and edges by retrieving the underlying properties from the external graph database.\nThe method further comprises permitting, via the electronic online customer platform, the customer to add a node to the workspace that already contains the identified node.  In response to the addition, the method automatically displays, via the\nelectronic online customer platform, a graphic representation of a connection between the added node and the identified node, if a relationship between the two nodes exists.\nThe method further comprises permitting, via the electronic online customer platform, the customer to add a corridor to the workspace that already contains the identified node.  In response to the addition, the method automatically displays, via\nthe electronic online customer platform, graphic representations of nodes and edges of the added corridor.\nThe method further comprises providing, via the electronic online customer platform, the customer with option to interactively select a node displayed on the workspace.  In response to the selection, the method communicates, via the electronic\nonline customer platform, corridors associated with the selected node that can be added to the workspace.\nThe method further comprises providing, via the electronic online customer platform, the customer with option to interactively select a node displaced on the workspace.  In response to the selection, the method communicates, via the electronic\nonline customer platform, other nodes associated with the selected node that can be added to the workspace.\nIn accordance with principles of the invention, a computer readable data medium storing computer-executable instructions that, when executed by a processor, cause the processor to perform a method for investigating and evaluating financial crime\nand sanctions-related risks is contemplated.  The method comprises establishing a graph database configured to store edges, nodes, and properties in nonvolatile memory, storing and publishing a style guide, receiving information in a document library to\nbe reviewed by analysts and saving received information in the document library that is used by analysts to enter nodes, edges, and properties, communicating with an internal production environment configured to interact with analysts, and communicating\nwith an external production environment configured to interact with customers.  The graph database is located in nonvolatile memory.  The style guide is stored in and published via a first subsystem implemented on a computer configured with volatile and\nnon-volatile memory and to communicate over a network.  The style guide includes a plurality of data models that are identified in the style guide to model different categories of situations and contains requirements for a structure and format of data\nused to enter nodes, edges, and properties for a particular data model.  The document library is also implemented on a computer configured with volatile and nonvolatile memory and to communicate over a network.  The document library can communicate with\ndifferent sources to receive information to be reviewed by analysts and save received information that is used by analysts to enter nodes, edges, and properties.  Each of the internal production environment and the external production environment is\nimplemented on one or more servers configured with volatile and non-volatile memory and to communicate over a network.  The internal production environment can communicate with analysts' computers over a network.  The external production environment can\ncommunicate with customers' computers over a network.\nThe internal production environment implements an electronic online workspace for analysts.  The workspace is configured to provide analysts with a software interface to enter a first category of information to identify and input individual\nnodes or groups of nodes and to enter a second category of information specifying edges (i.e., different types of relationships) between the nodes.  The inputted nodes include source nodes and corridor endpoint nodes.  In response entering the first and\nsecond category of information, the method stores the entered nodes, edges, and properties in the graph database.\nThe internal production environment implements a software engine that finds corridors.  In response to the stored nodes, edges, and properties, the method, via the software engine, automatically traverses pathways which are available starting\nfrom each source node through connected edges and nodes.  The method identifies, via the software engine, any new pathway and traverses the pathways until it reaches a corridor endpoint node before a maximum number of degrees of node-traversal as\nspecified by the software engine has been reached, or it reaches a node or edge in the traversal process that has no risk relevance or low risk relevance for the purpose of corridor generation.\nThe method then eliminates, via the software engine, some of the identified pathways.  The elimination process includes eliminating pathways that include the nodes or edges that have no risk relevance or low risk relevance for the purpose of\ncorridor generation and the pathways that include a total number of nodes at or above the maximum number of degrees of node-traversal.  The method stores, via the software engine, a set of weights in correspondence with the different types of edges and\nassigns weights based on the type of edge or connected node to the edges in each pathway.  In response to eliminating pathways and assigning weights, the method identifies, via the software engine, the remaining pathways to be the resulting corridors for\nthe source node and further based on a cumulative value of weights on edges in each resulting corridor specifies a variable degree of relevance to the resulting corridors.\nThe method also includes a publication process that approves and transmits the resulting one or more corridors and the nodes, edges, and properties to an external production environment.  The publication process is performed by a second\nsubsystem that is implemented on a computer configured with volatile and non-volatile memory and to communicate over a network.\nThe method also includes receiving and storing, via the external production environment, the one or more resulting corridors and the nodes, edges, and properties in an external graph database which aggregates corridors and nodes, edges, and\nproperties and maintains edge connections between nodes in different corridors.\nThe method includes implementing an electronic online customer platform in the external production environment.  The platform provides a visual interactive interface that comprises a visual workspace and interactive tools.  The tools include a\nsearch engine that is configured to provide keyword searching capability that surfaces possible matching nodes in the external graph database, that provides the user with the ability to add an identified node to the visual workspace, that generates a\nvisual graphic as representation of the identified node in the workspace, that communicates that the identified node has a certain number of connections that include a number of corridors, and that permits the user to select one of the connections or\ncorridors to add to the visual workspace.  In response to adding the connection or corridor to the visual workspace, the method, via the engine, displays additional nodes and edges that are part of the connection or corridor using separate visual\ngraphical elements for nodes and edges that visually illustrate the connection or corridor.\nThe method further comprises communicating, via the external production environment, a relative relevance of a plurality of corridors based on the weights.\nThe method further comprises displaying, via the external production environment, underlying properties for the nodes and edges by retrieving the underlying properties from the external graph database.\nThe method further comprises permitting, via the electronic online customer platform, the customer to add a node to the workspace that already contains the identified node.  In response to the addition, the method automatically displays, via the\nelectronic online customer platform, a graphic representation of a connection between the added node and the identified node, if a relationship between two nodes exists.\nThe method further comprises permitting, via the electronic online customer platform, the customer to add a corridor to the workspace that already contains the identified node.  In response to the addition, the method automatically displays, via\nthe electronic online customer platform, graphic representations of nodes and edges of the added corridor.\nThe method further comprises permitting, via the electronic online customer platform, the customer with option to interactively select a node displayed on the workspace.  In response to the selection, the method communicates, via the electronic\nonline customer platform, corridors associated with the selected node that can be added to the workspace. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe nature and various advantages of the present invention will become more apparent upon consideration of the following detailed description, taken in conjunction with the accompanying drawings, in which like reference characters refer to like\nparts throughout, and in which:\nFIG. 1 depicts an illustrative computer-implemented system for providing a visual interactive software tool that permits user to investigate and evaluate financial crime and sanctions-related risks in accordance with some embodiments of the\npresent invention;\nFIG. 2 depicts illustrative nodes, properties, and an edge and their respective roles in accordance with some embodiments of the present invention;\nFIG. 3 depicts illustrative corridors in accordance with some embodiments of the present invention;\nFIG. 4 depicts an illustrative process for finding corridors in accordance with some embodiments of the present invention;\nFIG. 5 depicts illustrative steps that determine whether pathways have satisfied certain automated rules to be qualified as corridors in accordance with some embodiments of the present invention;\nFIG. 6 depicts illustrative visual graphical elements with color indicating a particular type of node and symbol indicating a particular type of subject in accordance with some embodiments of the present invention;\nFIGS. 7-11, 13, 14A, 15, and 16 depict illustrative screenshots of a graphical user interface, an interactive window(s), and related displayed features generated by the system through which the user interacts on their computer or PC to use the\nadvantages provided by the system in accordance with some embodiments of the present invention;\nFIG. 12 depicts additional illustrative corridors and an illustrative corridor edge in accordance with some embodiments of the present invention;\nFIG. 14B depicts an illustrative structure for constructing the visual graphic of an illustrative insight in accordance with some embodiments of the present invention;\nFIG. 17 depicts another illustrative computer-implemented system for providing a visual interactive software tool that permits user to investigate and evaluate financial crime and sanctions-related risks in accordance with some embodiments of\nthe present invention;\nFIG. 18 depicts additional illustrative corridors that have certain types of relationships in accordance with some embodiments of the present invention; and\nFIG. 19 depicts an illustrative visual graphic of one or more identified nodes in accordance with some embodiments of the present invention.\n<BR><BR>DETAILED DESCRIPTION OF THE INVENTION\nIn accordance with principles of the present invention, a system is provided that can efficiently identify for users direct and indirect relationships between an actor that does not appear on a sanctions list and other actors that do appear on a\nsanctions list using, among other things, a graph database and a style guide.  The style guide allows many analysts to incrementally add to the data in the graph database in a consistent, structured, and connected way.  As new nodes are added to the\nsystem, a computer implemented process is implemented that traverses pathways from the new node to a node that is identified on a sanctions list and which relies on rules that identify only a subset of pathways for use by the customers.  The system\neliminates pathways that have a degree of separation at or above a predetermined threshold and implements other rules such as rules eliminating pathways that include nodes or edges that have no risk relevance or low risk relevance for the purpose of\ncorridor generation.  The resulting pathways that are found are considered \"corridors\" and are saved by the system in the graph database.  Thus, for example, in some embodiments, the system incrementally adds corridors and stores them instead of\ntraversing the entire graph.  This technique also supports an interactive graphical interface of the system which provides an interactive tool for users to quickly and cogently understand risk using a visualization of one or more corridors.  The\ninterface provides a visual workspace that allows the user to search by keywords and the interface, in response, lists search results for possible matching nodes in the database.  The interface further permits the user to select a node from the list of\nsearch results (e.g., a person) and in response, the interface or system generates a related list that includes a list of corridors (pathways in the graph database from that starting node to a node that appears on a sanctions list, also referred to as a\ncorridor endpoint node) and other connected nodes under subject groupings.  The user can interact with the interface to select a corridor and add it to the workspace.  The user can select other corridors from the list and add to the workspace and display\nit.  This provides a powerful and elegant solution to the needs of the institution with respect to financial crime and sanctions compliance.  The interface permits the user to select individual nodes that are in the workspace and in response, lists of\nrelated information including corridors that are associated with that node, and other connected nodes, and the user can similarly add those to the workspace.  This technique quickly adapts the source data and in operation, quickly surfaces the relevant\ninformation in a visual way for understanding by those in charge of evaluating financial crime and sanctions compliance issues at institutions.  The known prior art is not able to accomplish such powerful results.\nEmbodiments of the present invention are directed to a software platform that allows analysts and customers to research and map the commercial, financial, and facilitation networks of sanctioned or other actors that may be associated with\nillicit activity.  Through embodiments to the present invention, financial institutions, multinational corporations, legal professionals, and other stakeholders can access visual graphs depicting relationships between sanctioned and non-sanctioned actors\nin order to evaluate their possible exposure to financial crime or sanctions-related risks.  Embodiments illustrated herein are directed to financial crime and sanctions-related solutions and have particular suitability to solving and assisting users\nwith financial crime and sanctions compliance related issues.  It should be understood that described features or systems can have broader or different applications.  Sanctions refers to laws or regulations issued by a government that places commercial\nrestrictions on certain identified entities or entities that are described in more general terms or indirectly (e.g., all subsidiaries of an entity that is specifically identified) and also place restrictions on commercial activities involving the\nrestricted entities.  Individuals, companies, or organizations that violate the restrictions are subject to civil or criminal penalties, which can be significant source of risk, typically arising from being a primary or secondary actor in a financial\ntransaction.  In the United States, government organizations involved in issuing and enforcing such regulations include the Department of the Treasury, the Department of State, and the Department of Justice, but others exist in the U.S.  and in other\ncountries.\nFIG. 1 depicts an illustrative computer-implemented system 100 for providing a visual interactive software tool that permits users to evaluate financial crime and sanctions-related risks.  The system 100 comprises a graph database 105 configured\nto store nodes, edges, and properties a document library 110 configured to communicate with different sources over a network to receive financial crime and sanctions-related risk information and save received information used by analysts to create nodes,\nedges, and properties, an internal production environment 115 configured to interact with analysts, and an external production environment 120 configured to interact with customers.  The graph databases 105 include an internal graph database and an\nexternal graph database.  The internal graph database is configured to store nodes, edges, and properties inputted into the internal production environment 115 by analysts, store nodes, edges, and properties for access and use by analysts in the internal\nproduction environment 115, and store other information entered and produced from the internal production environment 115.  The information produced from the internal production environment 115 may be information created using the nodes, edges, and\nproperties (e.g., connections and corridors).  The external graph database is configured to store nodes and edges for customers' use in the external production environment 120 and store visual graphs produced and saved by customers in the external\nproduction environment 120.  The graph database 105 may be a component separate from the internal production environment 115 and the external production environment 120 or be integrated with the internal production environment 115 and the external\nproduction environment 120 (e.g., with the internal graph database built into the internal production environment 115 and with the external graph database built into the external production environment 120).  The system 100 is configured to logically\ndivide (e.g., by a mechanism such as user authentication executed by subsystem 155) between the internal graph database and the external graph database, wherein customers are not permitted to access or interact with nodes or edges in the internal graph\ndatabase.\nA node represents a subject.  The system 100 is configured to allow for the subject to be an individual, entity, postal address, e-mail address, event, number, telephone number, campaign, or other subject.  The subject may be one that appears on\na sanctions list or one that has a criminal record (sanctioned subject), one that is materially associated with a sanctioned subject but does not appear on a sanctions list (associated subject), or one that is neutrally associated with a sanctioned\nsubject or an associated subject and does not appear on a sanctions list (neutral subject).  A node is described by properties that include one or more words, numbers, symbols, or a combination thereof by which the underlying subject is known or referred\nto.  When the subject (or property) is a number, the number may be a phone number, fax number, driver license number, social security number, passport number, bank account number (including credit card and debit card account numbers), identification\nnumber used by the system 100, or other number.\nAn edge represents a relationship between two nodes.  The relationship may be a business relationship (e.g., creditor, supplier, joint-venture, etc.), an ownership relationship (e.g., majority shareholder, minority shareholder, subsidiary,\netc.), a position relationship (e.g., director, manager, owner, etc.), a banking relationship (e.g., account holder, mortgagor, etc.), a familial relationship (e.g., father, daughter, cousin, etc.), and other relationships.  An edge is described by\nproperties that include one or more words, numbers, symbols, or a combination thereof by which the underlying subject is known or referred to.  FIG. 2 depicts an illustrative node, property, and edge and their respective roles.  FIG. 2 depicts a\ngraphical structure of the node, property, and edge that is presented to user when he or she assembles them through the workspace in the internal or external production environment.  The graph database 105 stores nodes, edges, and information from the\nenvironments 115, 120 in nonvolatile memory.  Nodes and edges are part of a graph database and can also have a graphical representation as part of a user interface.\nThe internal production environment 115 includes an electronic online workspace for analysts 130 and a software engine 135.  The workspace 130 provides a software interface that allows an analyst to enter a first category of information to input\nindividual nodes or group of nodes.  The nodes may be inputted into or created in the internal production environment 115 from information stored in the document library 110.  Analysts can access the internal production environment 115 over network\nconnections as over an Internet connection.  The document library 110 communicates with different data sources over a network to receive financial crime and sanctions-related information.  Data sources may include government sanctions lists, court\nrecords, records of incorporation, corporate filings, shipping records, news, websites, public social media postings, multi-media, and other sources.  An analyst may review the stored information, determine subjects from the stored information, and\nenters each determined subject and relevant information into the internal production environment 115 to create or input a node and related properties.  The information relied on by the analyst to create nodes can be saved in the document library so it is\navailable for review by the analyst and other analysts and users.  In this way, analysts and users can check accuracy of the entered nodes and properties, and review the actual received information directly (instead of just the nodes, edges, and\nproperties) if necessary.  For example, the received information may include an article from a newspaper (or other source), a link to the article, or information about the article, and the analysts and users can review that article.  An analyst may also\nobtain financial crime and sanctions-related information from places other than the document library and use the internal production environment 115 to merely input or create nodes.  Once an inputted node is identified by the system or an analyst as\nappearing on a sanctions list, the system designates it as a corridor endpoint node.  The designation imparts to the inputted node different characteristics and effects that enables the software engine 135 to find paths and corridors (further discussed\nbelow) accordingly and that allows the electronic online platform of the external production environment 120 to generate a visual graphic as a distinctive representation.  A node can also be created to represent an article or other data that serves as a\nsource for additions made by an analyst.  This type of node is configured not to have a graphical node inserted into a workspace.\nThe software interface also allows the analyst to enter a second category of information specifying an edge and related properties between inputted nodes.  The software interface provides a list of available edges, and the analyst can select and\nspecify an edge from the list.  The analyst may also create additional edges from the software interface if they are not in the list.  In some embodiments, inputting a certain node and/or specifying the type and direction of the relationship between two\nnodes may cause the system 100 to automatically select a certain relationship between that certain inputted node and another inputted node.  The relationship selected by the system 100 may be a mandatory relationship.  In some embodiments, specifying a\ncertain relationship may cause the system 100 to automatically change or specify the type of inputted node (e.g., source or corridor endpoint node).  The type of node selected by the system 100 may be a mandatory type of node.\nThe first category of information and the second category of information are stored in the graph database 105 and can be accessed by the workspace 130.\nThe internal production environment 115 or the system 100 includes a first subsystem that is configured to store and provide a style guide.  The style guide includes a plurality of data models defined by analysts or administrators to model\ndifferent situations and contains requirements for a structure and format of data used to enter nodes, edges, and properties for a particular data model.  For example, the situations may include arrest, investigation, and litigation situations, and the\ndata model can specify how the information of each situation should be entered into the internal production environment 115.  For instance, in an arrest situation, the data model may specify the types of nodes that should be used, the edges that should\nbe used for each type of node, the properties that should be used for each type of node and edge, and the format that the node, edge, and property should be entered to clearly convey the information represented by the nodes, edges, and properties.  For\nexample, the data model may require the analyst to enter the nodes, edges, and properties in an \"Arrest of [Person A] in [Country], who is [relationship] of [Person B]\" format with the nodes, edges, and properties inserted in the corresponding bracket. \nThe text in the format (e.g., \"Arrest of,\" who is,\" and \"of\") may be entered by the analyst or automatically generated by the environment 115 when the analyst selects the data model of the arrest situation to be used for the information he is about\nenter.  The data model may have similar specifications for investigation, litigation, and other situations.  In this way, analysts can follow the same rules and requirements for each situation and build the information for each situation in the same\nmanner.\nA data model can govern what and how each piece of information is entered into the internal production environment 115 in each situation, relate the entered information to a node in the graph database 105 based on information in the entered\ninformation, and determine how the entered information should be linked to or integrated with the node if the entered information is related.  In some embodiments, a data model may be available as a template for entering the first and/or second category\nof information.  In some embodiments, an analyst can enter the first and/or second category of information and the system 100 can automatically select the appropriate data model to add or integrate the information in the data model into the graph\ndatabase 105.  The data model may include the corresponding fields and the required data structure and format so that the system 100 or the graph database 105 can be built and maintained uniformly across all analysts.  The data models can also be created\nfor other situations.\nThe development and publication of the style guide can be an important component of the system 100.  A rigorous set of requirements and definitions can be specified in the style guide for a wide range of situations, subjects, or relationships. \nThis approach embeds structured consistency in the analyst-created nodes, edges, properties, graphs, or related information (the creation of which may involve many different analysts and can involve many different sources of information or types of\nsources) that permits the database to grow efficiently.  The system 100 can, in using the created information, automatically identify edges or relationships between subjects.  This can for example result in avoiding having multiple representations of the\nsame subject in the database which can create disconnects or result in duplicate or redundant paths.  Other advantages are also gained from such a system.  Note that this is one option and in some embodiments other variations can be implemented.\nThe system is configured to allow collaboration.  The system can be configured to have many analysts interact and save their work on the system.  The system can be configured to allow analysts to see the work of other analysts on the system and\nto build on that work (efficiently building a larger and more complex set of information).  The system can be configured to allow analysts to view the work saved by other analysts or can have restrictions in place that limit the access of analysts to\nnodes, edges, or properties that have been approved and added to the external production environment.  For example, the internal production environment 115 allows analysts to collaborate and build a database storing information that visually identifies\ndirect and indirect relationships between an actor that does not appear on a sanctions list and other actors that do appear on a sanctions list.  Analysts (e.g., other analysts) can then add additional actors or nodes to the database to continue the\ndirect and indirect relationships and/or new actors or nodes to the database to create new direct and indirect relationships that are not already in the database.  The internal production environment 115 also includes an approval process for new\ninformation that is being added by an analyst.  The approval process can be configured as an automated process involving a workflow in which the contributing analyst specifies to the system that a new set of subjects or relationships are being added to\nthe system and in response, an automated approval process is implemented that can involve approval by peers through the system or by supervisory reviewers.\nThe software engine 135 includes a set of computer implemented rules (generally referred to as rules) that are configured to traverse paths and find (\"surface\") a subset of the paths that meet certain rules (and therefore, eliminate other\npaths).  The subset that meets the rules is referred to herein as corridors.  The software engine 135 is configured to find or identify corridors based on the stored first and second category of information.  A corridor is a pathway connecting any node\nthat is not a corridor endpoint node to a node that is designated as a corridor endpoint node via one or more edges and/or one or more connecting nodes.  A connecting node can be any node except a node designated as a corridor endpoint node and/or other\nnodes deemed to have no (or low) illicit finance or sanctions-related risk relevance for the purpose of corridor generation.  As a matter of clarification, at the time an analyst creates a new node for a subject, the system 100 may not be configured to\nrequire that the analyst specifies whether it is a corridor endpoint node or some other kind of node (e.g., a node that has no risk relevance or low risk relevance for the purpose of corridor generation, or any further specificity with respect to the\ntype of node can for example be determined by the software engine 135 based on the edges or paths connected to that node).  At a base level, a corridor is determined (found) by identifying a node (that is not a corridor endpoint node) from the first\ncategory of information and connecting the identified node to a designated corridor endpoint node in the first category of information via a direct edge connection or via other nodes in the first category of information and the edges in the second\ncategory of information.  Rules are incorporated into the software engine 135 that traverses or finds paths from identified nodes to designated corridor endpoint nodes and finds corridors as a subset of all paths where a path from an identified node can\nbe traversed by the software engine 135 to reach a designated corridor endpoint node.  In operation, the software engine 135 automatically traverses from the identified node through connected nodes and edges until it reaches a designated corridor\nendpoint node.  A corridor endpoint node is a type of node that is specified or designated within the system as having different properties than other nodes.  A corridor endpoint node cannot be the starting point of a corridor and is only available to\nend a pathway or corridor.  The corridor endpoint node is defined by the system 100 (e.g., by rules, configurations, or data structures) to function that way with respect to identified nodes.  A corridor is found when the identified node is connected to\na corridor endpoint node and also certain rules are satisfied.  A corridor can be graphically illustrated by the system 100 (e.g., as part of the graphical user interface) using a graphical structure comprising the connected nodes, edges and corridor\nendpoint node that form the corridor.  Subject to meeting certain additional rules (discussed herein) a corridor can be identified by the software engine 135 whenever an identified node can reach a corridor endpoint node.  Multiple corridors can be\ngenerated for an identified node.  The multiple corridors may be pathways between an identified node and a corridor endpoint node, between an identified node and multiple corridor endpoint nodes, or a combination thereof.\nFIG. 3 depicts illustrative corridors 350, 355, 360, 364, 368.  As mentioned above, a corridor is a pathway between a source node and a corridor endpoint node that meets certain rule requirements.  For discussion purposes, in connection with\nFIG. 3, corridors are discussed in more general terms and additional specificity is discussed further below.  The corridors 350, 355, 360 represent pathways between a source node 305 and a corridor endpoint node 310.  The corridor 350 includes an edge\n315 joining the source node 305 and a connecting node 320, a connecting node 320, and an edge 325 joining the connecting node 320 and the corridor endpoint node 310.  The corridor 355 is an edge 357 between the source node 305 and corridor endpoint node\n310.  The corridor 360 includes an edge 330 joining the source node 305 and a connecting node 335, a connecting node 335, an edge 340 joining the source node 335 and a connecting node 345, a connecting node 345, and an edge 348 joining the connecting\nnode 345 and the corridor endpoint node 310.  The corridor 364 represents a pathway between the same source node 305 and a different corridor endpoint node 370.  The corridor 368 represents a pathway between the same source node 305 and another different\ncorridor endpoint node 375.  The software engine 135 may not generate corridors for some of the source nodes since not all the source nodes can be connected to a corridor endpoint node.\nThe software engine 135 is programmed with a maximum number of degrees of node-traversal.  The maximum number of degrees of node-traversal refers to the number of nodes in a pathway the software engine 135 is permitted (by rule) to traverse to\nreach a corridor endpoint node from a particular source node.  The software engine 135 will end the node traversal, exits the traversed pathway, and eliminates that traversed pathway from qualifying as a corridor when the software engine 135 determines\nthat the current traversed node is equal to the maximum number of degrees of node-traversal.  The software engine 135 is configured to eliminate a pathway when the number of nodes in the path (after the source node) reaches a maximum.  The maximum can be\nspecified internal to the system and not otherwise visible to users or analysts when interacting with the system.  The maximum can for example be selected to be one of 4, 5, 6, 7, or 8 nodes, which can have advantages in processing, relevancy, and\npresentation.  Other maximums are contemplated.  With this rule, the software engine 135 prevents pathways that exceed the maximum number of degrees of node-traversal from qualifying to be corridors.  If the software engine 135 has reached the maximum\nnumber and no corridor endpoint node is found, then the software engine 135 moves onto traversing another pathway (e.g., which can start from the same source node again if that source node has other pathways or from the next source node) to find a\ncorridor endpoint node.  Since the graph database 105 may store a large number of nodes and edges, their combinations may produce an enormous number of pathways.  The maximum number can control the number of iteration performed by the software engine 135\nto save processing time and power consumption.  For brevity, the term maximum number of degrees of node-traversal is sometimes referred to or abbreviated as the maximum number, the maximum number of node traversal, the maximum number of node traversal or\ndegrees of traversal, or the maximum number of traversal nodes.\nFIG. 4 depicts an illustrative process 400 for finding corridors.  In addition to the programmed maximum number, the software engine 135 is configured to apply a process that identifies and eliminates pathways that include a node in the path\n(between a source node and a corridor endpoint node) that has been designated as having no risk relevance or low risk relevance for the purpose of corridor generation when traversing through that node to a corridor endpoint node.  The determination by\nthe system of which nodes (or edges) are ones that have no risk relevance or low risk relevance for the purpose of corridor generation can be performed in one or more ways (or combinations thereof) that are described herein as illustrative examples.  A\nnode (or edge) can be designated as having no risk relevance or low risk relevance by an analyst via the electronic online analyst workspace.  The software engine 135 can also determine whether a node is one that has no risk relevance or low risk\nrelevance, without an analyst identifying it as such.  The software engine 135 can make the determination by analyzing the edge connecting the two nodes, the properties of the edge and two nodes, the number of edges the node has, or a combination\nthereof, and determining that the inclusion of the node, or the path associated with the node, would indicate a neutral or primarily neutral relationship between the two nodes.  If desired, a software module that incorporates artificial intelligence or a\nset of rules can be implemented in some embodiments that determines whether a node or edge has no or low risk relevance for the purpose of corridor generation.  Low risk relevance is mentioned to explain that a low level of relevance would still not be\nsufficient to reach a threshold value for the purpose of corridor generation.  The software engine 135 is configured to recognize nodes that have no risk relevance or low risk relevance (e.g., government node).\nThis configuration improves the technical and informational aspects of the resulting corridors.  For example, a node for a government entity is removed by this rule because its inclusion provides very limited or no relevance for the purpose of\nevaluating financial crime and sanctions-related risks for two distinct nodes that connect through the government node.  In operation, in one embodiment, the system implemented a process in which as part of the automated node traversal process, it\ndetermines whether each node is identified as a type of node that the system has stored with a designation that this type of node has no risk relevance or low risk relevance for the purpose of corridor generation.  If the system identifies a match, the\nsystem would eliminate that pathway and would discontinue traversing that pathway.  The node and related connection would be available and displayed (if desired) in a workspace to provide additional information to a user.\nFIG. 5 depicts illustrative steps 405 and 410 of FIG. 4.  In this example, a low maximum number (e.g., 3) is used for explanatory purposes.  FIG. 5 shows a graphical illustration of the process in order to further elaborate about its operation. \nIf desired, the graphical illustration could be incorporated into an interactive graphical interface.  In FIG. 5, node 505 may have been recently added to the internal production environment and approved to be added to the external production\nenvironment.  At this point, for example, the system processes the addition as part of the already existing nodes and edges in the already existing external production environment.  The system will automatically start at node 505 and traverse connections\n(e.g., one by one, at the same time, or some variation thereof) and applies automated rules to select corridors.  In this case, node 505 has connections to node 510 (a node on a sanctions list and thus a corridor endpoint node) through nodes 520, 521,\n525, 527, 530, 532, 534, 522, 524, and 529.  The software engine 135 would traverse connected edges and nodes until reaches node 510 or the node traversal is terminated before it reaches node 510 when the software engine 135 determines that the current\npath it is traversing has reached a maximum number of nodes before reaching a corridor endpoint node or a node or edge (reached in the traversal process) that is in the category of nodes or edges that have no risk relevance or low risk relevance for the\npurpose of corridor generation.  In the illustration, for convenience, lines 552, 554, 556, 558, 560, and 562 are provided as a way to mark and discuss an associated set of nodes and edges that (based on information stored in the environment) establish a\npath from node 505 to node 510.  Those lines are markers for discussion purposes.  For example, in this case, using step 405 of FIG. 4, the software engine 135 identifies pathways, referenced using associated markers, 552, 554, 556, 558, 560, 562 that\nconnect node 505 to corridor endpoint node 510 and eliminates pathway 558 because pathway 558 involves more than 3 connecting nodes (e.g., 520, 530, 532, and 534).  In operation, the process would stop the traversal process when it reaches the maximum\nnumber of traversal nodes (3 in this example) and the system would not identify or \"know\" that there is a pathway 558 to node 510 because it would have stopped the traversal process when it reached the maximum (i.e., node 532).  By performing step 410 of\nFIG. 4, the software engine 135 identifies that node 522 has no risk relevance or low risk relevance for the purpose of corridor generation, when it reaches that node in the node traversal process, and eliminates pathway 560 associated with node 522. \nSimilarly, in operation, the process would stop the traversal process when it reaches the node 522 (determines node has no risk relevance or low risk relevance for the purpose of corridor generation) and at that point the system would not identify or\n\"know\" that there is a pathway 560 to node 510 because it would have stopped the traversal process when it reached node 522.  As a result, the software engine 135 keeps only pathways 552, 554, 556 and 562 after performing steps 405 and 410 and the kept\npathways are determined to be corridors associated with node 505 in relation to node 510 (since there can be other relations).  Corridors are automatically added to the external production environment when or in response to being found by the software\nengine 135.  Other implementations are also contemplated.\nThe predetermined node type for non-traversal, the type of node that the process should identify and use to eliminate path being traversed (no risk relevance or low risk relevance), and the predetermined number of connecting nodes (maximum\nnumber of node traversal or degrees of traversal) may be inputted by the analyst into the software interface but is preferably pre-programmed in the software engine 135 (in the implementation of the overall system).\nThe process 400 (FIG. 4) further comprises determining a degree of relevance for the corridors 415.  The step 415 comprises storing a set of weights in correspondence with the different types of edges 420, assigning weights based on the type of\nedges in each corridor 425, calculating a cumulative value of weights for each found corridor based on the assigned weights 430, and specifying a degree of relevance to other corridors for each corridor 435 based on the cumulative value.  The types of\nedges may be based on whether the relationship is a shareholder one, an employment one, a banking one, a familial one, or other one and/or the rank in the relationship (majority shareholder vs.  minority shareholder, director vs.  associate, son vs. \ncousin, etc.).  Each relationship type and rank has an assigned weight, and certain relationships types and ranks have a weight higher than other relationship types and ranks.  Higher weight may refer to a higher assigned number or a higher multiplier in\nthe calculation step 430 or the formula used in the calculation step 430.  The calculation step 430 may be based on adding all the assigned weights of the connecting edges in a corridor.  The total sum or cumulative value may be used as the only\nparameter in the specifying step 435 to determine the degree of relevance.  The cumulative value may also be used with other parameters or in another procedure to determine the relevance in step 435.  A higher cumulative value may represent that the\ncorridor has a higher relevance compared to other corridors.  In some embodiments, a lower cumulative value may represent that the corridor has a higher relevance compared to other corridors.  The degree of relevance may be output as a number on a scale\nconfigured to convey such degree.  For example, the corridor with the strongest degree of relevance may be marked with \"1\", the corridor with the second strongest degree of relevance may be marked with \"2,\" and so forth.  Other numbers, letter,\ncharacters, or orders (e.g., from large number to small number) may also be used.  The number may be different from the cumulative value.  The system can be configured to only assign weights that are equal or greater than 1 (with 1 indicating most\nrelevant and ascending numbers, totals, indicating lower relevance, in relation to a weight of 1).\nIf desired as an option, the step 415 may further comprise a step 440 that eliminates corridors based on relevance determined from calculating weights.  The step 440 may eliminate corridors based on assigned weights in step 425 and before the\ncalculation step 430.  In this case, steps 430 and 435 calculate a cumulative value and specify a degree of relevance for the remaining corridors.  The step 440 may eliminate corridors based on the calculated cumulative value in step 430.  The step 440\nmay eliminate corridors that do not have a cumulative value that is above, below, or matches a predetermined cumulative value.  In this case, step 435 specifies a degree of relevance for the remaining corridors.  The predetermined cumulative value may be\nprovided by analyst from software interface or be pre-programmed in the software engine 135 and be subsequently modified to a different number by analyst from the software interface.\nThe step 415 may be performed after the step 405 is executed, after the step 410 executed, or after both steps 405 and 410 are executed.  In any of these scenarios, the step 415 may operate to determine a degree of relevance for corridors kept\nby step 405 and/or 410 without the step 440.  The step 415 may also operate to further eliminate one or more corridors kept by step 405 and/or 410 and determine a degree of relevance for the remaining corridors.  In other words, the step 440 may be\noptional in some embodiments.\nIn some embodiments, the step 415 may be configured to perform before steps 405 and/or 410.  In that situation, steps 405 and/or 410 may further eliminate corridors kept by the step 415 (whether step 415 performs the eliminating step 440 or\nnot).  In some embodiments, the step 415 may be configured as an alternative step to steps 405 and 410.  The process 400 may operate based on the step 415 to eliminate corridors without steps 405 and 410.\nAll the data available to (e.g., all the predetermined numbers, assigned weights, etc.) and produced by the software engine 135 (e.g., the generated corridors, the cumulative values, kept/resulting corridors, etc.) may be stored in the internal\ngraph database.\nThe system 100 includes a second subsystem that is configured to approve and provide the kept or resulting corridors and the associated nodes, edges, and properties from the internal production environment 115 (or the internal graph database) to\nthe external production environment 120 (or the external graph database) via link 133 (FIG. 1).  In one embodiment, the second subsystem may provide a multi-stage approval process.  In the first stage, the second subsystem may transmit the work added by\nan analyst (e.g., a set of new nodes and edges that are related to a sanctioned subject based on a court record) to the computers of peer analysts for peer analysts' review (which is understood to include web browser implementations, which should be\ngenerally understood with respect to other aspects as well).  The second subsystem may require a certain number of analysts to approve the work (added research) in order to advance to the next stage.  Upon approval by the required number of analysts, the\nsecond subsystem may transmit the analyst-approved work to the computer of a senior analyst for the senior analyst's review.  Upon approval by the senior analyst, the work (a particular set of nodes, edges, or properties) are provided (inserted and\nsaved) to the external production environment 120.  The approval process may have additional or fewer stages.  During any of the stages, any rejected work will not be transferred to the external production environment 120.  The kept or resulting\ncorridors are used interchangeably throughout this application to refer to corridors that remain after an eliminating step is performed, which may be step 405, 410, and/or 415.  The second subsystem is configured to automatically feed approved work to\nthe external production environment 120.  The second subsystem also provides approved work to the third subsystem.\nThe external production environment 120 (FIG. 1) is configured to receive and store the resulting corridors and the associated nodes, edges, and properties in the external graph database.  The external graph database can aggregate resulting\ncorridors and associated source nodes (e.g., adding current resulting corridors and associated nodes, edges and properties to resulting corridors and associated nodes, edges, and properties previously stored in the external graph database) and maintain\nedge connections between nodes in different corridors.\nThe external production environment 120 includes an electronic online customer platform 140 (and is accessible via a web portal 1044 as shown in FIG. 17) and a software engine 145.  The customer platform 140 provides a visual interactive\ninterface that allows a customer to enter a keyword search, search for nodes with properties that match the keyword search in the external graph database, add a node that matches the keyword search to an electronic workspace, view information\naccompanying a node in a side panel, and add additional nodes and/or a corridor to the workspace.  The customer platform 140 perform its functions through the software engine 145.  As the customer enters a keyword search through the interface, the\nsoftware engine 145 searches indexed properties on nodes in the external graph database and identifies nodes that match or are similar to the search query.  When the customer selects and adds an identified node to the workspace, the software engine 145\nadds the identified node to the workspace by generating and displaying a visual graphical element as representation of the identified node in the workspace.  The software engine 145 also retrieves the accompanying information (which includes properties)\nof the identified node from the external graph database and displays that information through the visual interactive interface.  The displayed or accompanying information may include the relevant information entered by analyst during the input step,\nother nodes directly connected to the identified node, and a number of corridors.  Upon the customer selecting and adding one of the corridors, the software engine 145 generates and displays a visual graphic as representation of the corridor in the\nworkspace.  The visual graphic for the corridor is generated and displayed by the nodes and edges that form the corridor.  Customers can access the external production environment 120 over network connections such as over an Internet connection.\nThe software engine 145 is configured to generate and display a plurality of visual graphical elements.  The software engine 145 may generate a circle as the visual graphical element for a node (see FIGS. 2, 3, and 5).  The circle may have a\ndifferent size or color to indicate whether the node is identified as a particular type of node, including, for example, a corridor endpoint node.  The circle may be further produced with a symbol indicating a particular type of subject.  FIG. 6 depicts\nillustrative visual graphical elements 600 of a node with color 605 indicating a particular type of node and symbol 610 indicating a particular type of subject.  The software engine 145 may generate a line as the visual graphical element for an edge (see\nFIGS. 2, 3, and 5).  The line may have a different thickness, length, or color to indicate the type of relationship.  The line may also have an arrow at either end to indicate the direction of the relationship.  The visual graphic of corridor can be\ngenerated based on the above circle and line.  A separate traversal line (e.g., arrow lines in FIGS. 3 and 5) can also be provided to illustrate the path of the corridor.  Visual graphic refers to a visual representation created by one or more visual\ngraphical elements.  The software engine 145 may also generate other shapes to represent node, edge, and corridor.\nFIGS. 7-11, 13, 14A, 15, and 16 depict illustrative screenshots of a graphical user interface 700 (FIG. 7) for customers and its operation through a software engine.  Referring to FIG. 7, the graphical user interface 700 comprises a visual\ninteractive interface 705.  The visual interactive interface 705 includes a first command 710 configured to enter keyword searches on the external production environment.  Customer conducts keyword searches for a node 712 by typing letters, numbers,\nsymbols, marks, or a combination thereof in the first command 710 and the interface 705 displays the search results 715 in categories.  The categories include an entity category, a person category, an address category, or other categories.  Each category\nincludes nodes of the corresponding category that match the keyword search.  The search results include nodes that are similar or match the keyword search node 712.  The interface 705 displays the number of results (or nodes) in each category and the\ntotal number of results (or nodes) of all the categories.  The results delivered in response to the keyword search 712 can include any type of node, including any corridor endpoint node.  Customer may select a category to view all the results 720 in that\ncategory from the interface 705.  In FIG. 7, the customer selects the person category and the interface 705 displays the nodes 720 in that category in a window 722.  The interface 705 includes buttons, tabs, windows, screens, and other mechanisms to\nreceive customer selection and to display search results.  The interface 705 also includes an electronic workspace 725 to which nodes can be added and displayed via visual graphical elements.  The interface 705 is software that is implemented and running\non a computer using computer instructions that are stored in memory that, when executed by a processor, displays or provides the graphical and user-interactive features described herein (wherein the user can interact with the interface using an input\ndevice of the computer such as a keyboard).  This includes the situation where the interface 705 is provided through a web browser such as when a customer or analyst logs into the platform from their Internet browser running on their computer over an\nInternet connection to the platform.  The interface 705 is configured to implement the various related features described herein.\nFrom the search results 720, the customer may select a displayed node 730 to view additional information 735 about the selected node 730 as shown in FIG. 8.  Additional information 735 helps the customer decide if the selected node 730 matches\nthe keyword search 712 or the node the customer intends to investigate for sanctions-related risks.  The nodes in each category have their corresponding additional information.  For example, additional information 735 of the person category may include a\ntotal number of connections (explained below), the real name, alias, date and place of birth, nationality, resident country, sanctions status of the person, and other information.  Sanctions status information may include the jurisdiction (e.g., country\nor state) that sanctioned the person or entity represented by the node, the date the person committed the illegal act, and other information.  Additional information 735 is displayed in another window 737 configured with one or more commands 739, 740 to\nadd the selected node 730 to the electronic workspace 725.  The command 739 may allow the customer to add and place a node on the workspace 725 immediately upon clicking the command 739.  The location of the node is determined by the software engine 145. The location can be subsequently changed by the customer if needed.  The command 740 may allow the customer to drag a node onto the workspace 725.  Upon clicking and holding the mouse on the command 740, the selected node may be affixed to the mouse\ncursor and be moved along with the mouse cursor.  The location of the node is determined by the customer and the customer can move the node to a location he or she desires.  The node can be placed at the desired location upon releasing the mouse at the\ndesired location.  The node that appears on the workspace 725 is the visual graphical element of the node generated by the software engine 145.  The visual graphical element of the node is referred to as node in the description of FIGS. 7-16 for the ease\nof reading the instant application.  Adding a node to the electronic workspace 725 (visual workspace) may also be referred to as adding a node to the workspace.\nFIG. 9 depicts an illustrative visual interactive interface displaying a node 745 (a node that has been displayed on the workspace 725 by the user interacting with the interface).  The customer may select another node 747 to add to the workspace\n725.  The node 747 may be one of the connections (explained below) of the node 745 or another node searched by the customer.  The customer may view the additional information 735 and decide whether to add the node 747 to the workspace 725 via the command\n739 or 740.\nFIG. 10 depicts adding additional nodes 778, 779 to a node 745.  Upon adding a node 745, the customer may view connection information 780 of the node 745 by selecting the node 745 on the workspace 725.  In this example, the subject of the node\n745 is a person.  The connection information 780 includes nodes connected to the node 745, and the visual interactive interface 705 also displays the connected nodes in categories 780.  A connected node is a node that has an edge connection to the\ndisplayed node.  The categories are based on the relationship of the connected nodes to the node 745 (displayed node that is clicked on/selected by the user resulting on the system retrieving relationship information, instantly).  The categories for\nrelevant nodes connected to a displayed node (e.g., node 745) may include a leadership & management category, a facilitation & support category, a litigation, regulation enforcement category, a business associates category, and a family members category. The interface 705 displays the number of relevant nodes in each category and the total number of relevant nodes of all the categories.  The total number of relevant nodes of all the categories is also the total number of connections the node 745 has. \nThe interface 705 also displays the additional information 737 about the added node 745 in conjunction with the connection information 780.  The customer may select a category to view all the relevant nodes 785 in that category from the interface 705. \nIn FIG. 10, the customer selects the facilitation & support category and the interface 705 displays the relevant nodes 785 in that category in a window 787.  To clarify, in the context of the interface 705, the system permits the user to search and add\nnodes to the workspace 725 that fall in the category of nodes that have no risk relevance for the purpose of corridor generation.  These can also be found as part of connected nodes to a displayed node (e.g., when a user clicks on the displayed node and\nthe system provides relationship information).  This can add another dimension of visual information that can aid the user.  With respect to associated corridors, corridors that are retrieved and listed for node 745 when the customer selects node 745,\ncontain only the nodes or edges in resulting pathways that were identified by the node traversal process to produce corridors, which was performed before this information is made available on the external production environment.\nFrom the connected nodes 785, the customer may select a node 790 to view additional information 789 contained in window 738 about the node 790.  The additional information 789 includes the name of the connected node 790 and a description of the\nrelationship between the connected node 790 and the node 745.  Additional information 789 helps the customer decide if the connected node 790 should be added to the workspace 725 to view their relationship in a graphical form.  Window 738 is configured\nwith one or more commands 739, 740 to add the connected node 790 to the workspace 725.  The commands 739, 740 operate in the same manners discussed above.  Relevant nodes 778, 779 are illustrative nodes added via the steps explained above.  When the\nnodes 778, 779 are added to the workspace 725, their corresponding edge connections 761-763 to the nodes 745, 778, and 779 are also added to the workspace 725.  The edge or edge connection that appears on the workspace 725 is the visual graphical element\nof the edge or edge connection generated by the software engine 145.  The visual graphical element of an edge or edge connection is referred to as an edge connection in the description of FIGS. 7-16 for the ease of reading the instant application.  The\nnodes 745, 778, 779 and the edge connections 761-763 create a visual graphic 792.  The visual graphic 792 can be created with any number of nodes and edge connections.  The structure or shape of the visual graphic 792 can be changed by moving a node to a\ndifferent location on the workspace 725 to arrange the nodes and their relationships in different manners and to facilitate the understanding of their relationships.  When a node is being moved to another area, the corresponding edge connection remains\nconnected to the displayed node during the moving process.  When the node is moved to another area, and the edge connection is also moved to another area.\nWhen a node 714 has corridors, the connection information 780 also includes corridor information and corridors are stored in a corridor category for selection and display as shown in FIG. 11.  When the customer selects the corridor category, the\ninterface displays corridors 794 in a window 788.  The corridors 794 are the resulting corridors from the second subsystem.  The corridors 794 may be ranked by the determined degree of relevance and be displayed based on the rank.  The corridors 794 may\nbe displayed with the one having the highest degree of relevance first and with the remaining ones arranged in a descending order.  The corridors 794 may also be displayed in other orders.  The interface may also display the corridors 794 with a\nmeasurement (e.g., (1), (2), (3), (4)) indicating the number of connecting nodes between the displayed node and the corridor endpoint node.\nFrom the displayed corridors 794, the customer may select and add a corridor to the workspace 725.  The selected corridor is visually illustrated in the workspace 725.  The corridor is generated and presented with nodes and edge connections and\ntheir corresponding graphical elements.  For example, when corridor A is added, corridor A is produced with edge connection E1, connecting node C1, edge connection E2, and corridor endpoint node T1.  For another example, when corridor B is added,\ncorridor B is produced with edge connection E1 (if not already produced or if corridor A is not added yet), connecting node C1 (if not already produced or if corridor A is not added yet), edge connection E3, connecting node C2, edge connection E4, and\ncorridor endpoint node T2.  Although the node 714 is not mentioned in the above corridors, it is also part of the corridor.  The line associated with the corridor is used to illustrate the path of the corridor and it may or may not be displayed on the\ninterface.  The interface allows the customer to add one or more corridors for the node 714.  The subject of the node 714 is an entity, and the customer can evaluate financial crime and sanctions-related risks associated with the entity from the visual\ngraphics 792.\nFIG. 12 depicts additional corridors 1 and 2 and corridor edge 3 to illustrate the concept of corridors.  Corridors 1 and 2 are pathways between node A and corridor endpoint node G. Corridor edge 3 is an edge that stores corridors 1 and 2 using\nnodes and properties.  For simplicity, only the nodes involved are mentioned (not the edge connections).  Corridor 1 includes node A, connecting node B, connecting node C, and corridor endpoint node G. Corridor 2 includes node A, connecting node D,\nconnecting node E, connecting node F, and corridor endpoint node G.\nAdditional information depicted in 735 (FIG. 9), 789 (FIG. 10) and connection information depicted in 780 (FIG. 10) are derived from the accompanying information or the relevant information an analyst entered during the input step discussed\nabove.\nThe visual interactive interface also allows the customer to save created visual graphics 800 in the external graph database, load previous created and saved visual graphics 800 from the external graph database, and share created visual graphics\n800 with other individuals as shown in FIG. 13.  Each saved visual graphic 800 may be referred to as a chart.  The visual interactive interface also allows the customer to export created visual graphics and the accompanying information to a particular\nfile format (e.g., PDF, Word, Powerpoint, etc.) for saving or communicating with another computer system.  The visual interactive interface also allows the customer to view charts or insights specifically created by an analyst.  An insight is a specific\nresearch conducted by analyst for a particular subject or certain relationships that are illustrative, instructive, interesting, or unique.  FIG. 14a depicts an illustrative insight 900.  The insight 900 includes a short summary 905 and a visual graphic\n910 representing the summary 905.  The insight may be provided to a customer only if an insight is relevant to or contains information that has appeared in the customer's search or work history or the customer's previous inquiries.  The insight may also\nalways be provided to customers whenever it is created by analyst.\nAn insight is created through an insight node with insight edges and relevant nodes and properties.  An insight node is created by an analyst in the internal production environment and may be published to the external production environment.  An\ninsight is intended to provide a brief summary so the visual graphic should have only a certain amount of complexity.  An insight node and relevant nodes selected by analyst may have a limited number of edges or may be a node having a plurality of edges\nbut only a few of them are used by analyst.  The insight node, relevant nodes, and edges are created to form a narrative or description that corresponds to the brief summary.  FIG. 14b depicts an illustrative structure for constructing the visual graphic\nof an insight.\nThe interface may provide insights to customer through two locations.  The customer may access insights from the connection information 780 as shown in FIG. 15.  The connection information 780 includes an insight category, and insights\nassociated with a node will be displayed in a window 796.  Upon selecting from the displayed insights in 796, the customer may view a short summary of the selected insight in another window 797 configured with a command 798 to add its visual graphic 799\nto the workspace 725.\nThe customer may also access insights from a location 930 that stores and displays insights as shown in FIG. 16 among other things, without the customer first querying a node or obtaining the connection information.  The stored insights may be\ndivided into different categories and the customer may select a category 935 to view insights 940 in that category.  Each insight 945 may be depicted as a small window 945 displaying a portion of the brief summary.  The window 945 is configured with a\ncommand 950 that can access the entire brief summary and the associated visual graphic.\nThe system 100 also includes a third subsystem 150 (FIG. 1) configured with a list graph that enables bulk exporting of tailored subsets of data and a fourth subsystem 155 (FIG. 1) configured to provide user authorization and secure\ncommunications.  The list graph in the third subsystem 150 receives data from the publish flow 133 (FIG. 1) from the internal production environment 115 (FIG. 1).  The list graph in the third subsystem 150 is configured to allow internal users to apply\nadvanced query searches to identify all nodes in the list graph that meet customer specified requirements.  For example, a customer may want a list of all nodes that are identified as majority owned by corridor endpoint nodes; or for example, a customer\nmay want a list of all nodes that are identified as officers or directors of corridor endpoint nodes.  To identify nodes matching customer specified requirements, the corridor endpoint node may serve as the starting node from which the relationships are\ntraversed (rather than querying a node that is related to a corridor endpoint node and that does not appear on a sanctions list).  The certain subset of information and work produced by executing customer specified queries on the third subsystem 150 can\nbe exported in bulk to a particular type of file and/or distributed by other means.  The exportation allows individuals to access the information without using the external production environment 120.\nThe fourth subsystem 155 is configured to provide user authorization and secure communication.  The fourth subsystem 155 can determine whether a user is authorized to access the system 100 and the level of access of the user.  User can be\nanalyst, customer, administrator, or other individuals.  Analyst refers to individuals who use the internal production environment (including the internal graph database) or who build a database of financial crime and sanctions-related information. \nCustomer refers to individuals who access the external production environment (including the external graph database) via a web portal or who evaluate financial crime-related and sanctions-related risks based on the information built by analyst.  Analyst\nand customer may not have access to the other's environment and graph database.  Administrator refers to individuals who monitor and maintain the system 100 to ensure that the system 100 is operating in the correct manner.  Administrator may also be\nindividuals who can grant access to analyst and customer.  The technology of the fourth subsystem 155 may be based on security API and may include protocols such as RDS MySQL, LDAP, OAuth2, and other protocols.\nFIG. 17 depicts another illustrative computer-implemented system 1000 for providing a visual interactive software tool that permits user to investigate and evaluate financial crime and sanctions-related risks.  The system 1000 comprises an\ninternal production environment 1005 and an external production environment 1010.  The internal production environment 1005 includes an analyst toolset 1022 and an internal graph database 1024.  The analyst toolset 1022 includes the electronic online\nworkspace for analysts 130 in FIG. 1.  The internal production environment 1005 also includes the software engine 135 in FIG. 1, and the software engine 135 may be part of the analyst toolset 1022.  The analyst toolset 1022 communicates with the internal\ngraph database 1024 to save and access the information and work analyst produced from the workspace 130.  The internal graph database 1024 is configured to feed or publish the information and work the analyst produced to the external production\nenvironment 1010 or external graph databases 1040, 1042 (customer graph database).  The publication process is performed by the second subsystem, and the second subsystem may be part of the internal graph database 1024 or the internal production\nenvironment 1005.  The external graph databases may include one 1040 dedicated to customer web portal 1044 and one 1042 dedicated to bulk exporting of tailored subsets of data.  Customer may log into the external production environment 1010, or the\nplatform for customers 140 shown in FIG. 1, via the portal 1044.  The communication between customer and the platform 140 and between the customer web portal 1044 and the internal graph database 1024 may be based on an application programming interface\n(API) to determine whether a customer is an authorized user and to ensure that the communications are secure.  The customer platform 140 communicates with the external graph database 1040 to access the information and work analyst produced and to save\nthe work customer produced from the platform 140.  The external graph database 1042 is configured to allow bulk exporting of tailored subsets of data or files containing the information and work analyst produced.  The lists may be exported periodically\nfrom the database 1042 and for distribution.  Exported lists allow individuals to view information and work analyst produced without the individuals keyword searching through the external production environment 1010.\nIn some embodiments, databases 1040 and 1042 may be one single database configured to be accessible by customer web portal 1044 and to export lists 1046.  The external production environment 1010 also includes the software engine 145 in FIG. 1\nto perform the operations of the platform 140.\nFIG. 18 depicts additional illustrative corridors that have certain types of relationships.  The types of relationships may include ownership structures & subsidiaries, family member & affiliated companies, entities with a shared director,\nfinanciers & fundraising campaigns, bank accounts & financial transactions, entities with a shared address, and other types of relationships.\nFIG. 19 depicts an illustrative visual graphic of one or more nodes.  The complexity of the visual graphic may depend on the number of nodes and edges involved.  From FIG. 19, the customer can visually determine the relationships between\ndifferent nodes, view their properties and evaluate financial crime and sanctions-related risk.  The visual graphic can include additional nodes and edges if there are additional relevant nodes or if the customer adds more nodes.\nEach of the system 100, the internal and external production environments, the graph databases, and the subsystems in FIG. 1 can be implemented on one or more computer systems and be configured to communicate via a network connection.  They all\nmay also be implemented on one single computer system.  In either situation, the computer system(s) can communicate with analysts' computers and customers' client devices.  The computer system(s) may also be referred to as servers.  The computer\nsystem(s), the analyst's computer, and the customer's client device may adopt the following computer system.\nIn one embodiment, the computer system includes a bus or other communication mechanism for communicating information, and a hardware processor coupled with bus for processing information.  Hardware processor may be, for example, a general\npurpose microprocessor.\nThe computer system also includes a main memory, such as a random access memory (RAM) or other dynamic storage device, coupled to bus for storing information and instructions to be executed by processor.  Main memory also may be used for storing\ntemporary variables or other intermediate information during execution of instructions to be executed by processor.  Such instructions, when stored in non-transitory storage media accessible to processor, render computer system into a special-purpose\nmachine that is customized to perform the operations specified in the instructions.\nComputer system further includes a read only memory (ROM) or other static storage device coupled to bus for storing static information and instructions for processor.  A storage device, such as a magnetic disk or optical disk, is provided and\ncoupled to bus for storing information and instructions.\nComputer system may be coupled via bus to a display, such as a cathode ray tube (CRT), for displaying information to a computer user.  An input device, including alphanumeric and other keys, is coupled to bus for communicating information and\ncommand selections to processor.  Another type of user input device is cursor control, such as a mouse, a trackball, or cursor direction keys for communicating direction information and command selections to processor and for controlling cursor movement\non display.  This input device typically has two degrees of freedom in two axes, a first axis (e.g., x) and a second axis (e.g., y), that allows the device to specify positions in a plane.\nComputer system may implement the techniques described herein using customized hard-wired logic, one or more ASICs or FPGAs, firmware and/or program logic which in combination with the computer system causes or programs computer system to be a\nspecial-purpose machine.  According to one embodiment, the techniques herein are performed by computer system in response to processor executing one or more sequences of one or more instructions contained in main memory.  Such instructions may be read\ninto main memory from another storage medium, such as storage device.  Execution of the sequences of instructions contained in main memory causes processor to perform the process steps described herein.  In alternative embodiments, hard-wired circuitry\nmay be used in place of or in combination with software instructions.\nThe term storage media as used herein refers to any non-transitory media that store data and/or instructions that cause a machine to operation in a specific fashion.  Such storage media may comprise non-volatile media and/or volatile media. \nNon-volatile media includes, for example, optical or magnetic disks, such as storage device.  Volatile media includes dynamic memory, such as main memory.  Common forms of storage media include, for example, a floppy disk, a flexible disk, hard disk,\nsolid state drive, magnetic tape, or any other magnetic data storage medium, a CD-ROM, any other optical data storage medium, any physical medium with patterns of holes, a RAM, a PROM, and EPROM, a FLASH-EPROM, NVRAM, any other memory chip or cartridge.\nStorage media is distinct from but may be used in conjunction with transmission media.  Transmission media participates in transferring information between storage media.  For example, transmission media includes coaxial cables, copper wire and\nfiber optics, including the wires that comprise bus.  Transmission media can also take the form of acoustic or light waves, such as those generated during radio-wave and infra-red data communications.\nVarious forms of media may be involved in carrying one or more sequences of one or more instructions to processor for execution.  For example, the instructions may initially be carried on a magnetic disk or solid state drive of a remote\ncomputer.  The remote computer can load the instructions into its dynamic memory and send the instructions over a telephone line using a modem.  A modem local to computer system can receive the data on the telephone line and use an infra-red transmitter\nto convert the data to an infra-red signal.  An infra-red detector can receive the data carried in the infra-red signal and appropriate circuitry can place the data on bus.  Bus carries the data to main memory, from which processor retrieves and executes\nthe instructions.  The instructions received by main memory may optionally be stored on storage device either before or after execution by processor.\nThe computer system also includes a communication interface coupled to bus.  Communication interface provides a two-way data communication coupling to a network link that is connected to a local network.  For example, communication interface may\nbe an integrated services digital network (ISDN) card, cable modem, satellite modem, or a modem to provide a data communication connection to a corresponding type of telephone line.  As another example, communication interface may be a local area network\n(LAN) card to provide a data communication connection to a compatible LAN.  Wireless links may also be implemented.  In any such implementation, communication interface sends and receives electrical, electromagnetic or optical signals that carry digital\ndata streams representing various types of information.\nNetwork link typically provides data communication through one or more networks to other data devices.  For instance, network link may provide a connection through local network to a host computer or to data equipment operated by an Internet\nService Provider (ISP).  ISP in turn provides data communication services through the world wide packet data communication network now commonly referred to as the \"Internet.\" Local network and Internet both use electrical, electromagnetic or optical\nsignals that carry digital data streams.  The signals through the various networks and the signals on network link and through communication interface, which carry the digital data to and from computer system, are example forms of transmission media.\nComputer system can send messages and receive data, including program code, through the network(s), network link and communication interface.  In the Internet example, a server might transmit a requested code for an application program through\nInternet, ISP, local network and communication interface.\nThe received code may be executed by processor as it is received, and/or stored in storage device, or other non-volatile storage for later execution.\nEmbodiments described herein are primarily described in the context of providing a tool for evaluating prospective financial crime and sanctions-related risks for businesses considering commercial dealings, but it should be understood that\napplications in other areas including application of particular features in other areas are contemplated.\nIt is understood from the above description that the functionality and features of the systems, devices, or methods of embodiments of the present invention include generating and sending signals to accomplish the actions.\nIt should be understood that variations, clarifications, or modifications are contemplated.  Applications of the technology to other fields are also contemplated.\nExemplary systems, devices, and methods are described for illustrative purposes.  Further, since numerous modifications and changes will readily be apparent to those having ordinary skill in the art, it is not desired to limit the invention to\nthe exact constructions as demonstrated in this disclosure.  Accordingly, all suitable modifications and equivalents may be resorted to falling within the scope of the invention.\nThus, for example, any sequence(s) and/or temporal order of steps of various processes or methods (or sequence of device connections or operation) that are described herein are illustrative and should not be interpreted as being restrictive. \nAccordingly, it should be understood that although steps of various processes or methods or connections or sequence of operations may be shown and described as being in a sequence or temporal order, but they are not necessarily limited to being carried\nout in any particular sequence or order.  For example, the steps in such processes or methods generally may be carried out in various different sequences and orders, while still falling within the scope of the present invention.  Moreover, in some\ndiscussions, it would be evident to those of ordinary skill in the art that a subsequent action, process, or feature is in response to an earlier action, process, or feature.\nIt is also implicit and understood that the applications or systems illustratively described herein provide computer-implemented functionality that automatically performs a process or process steps unless the description explicitly describes\nuser intervention or manual operation.\nIt should be understood that claims that include fewer limitations, broader claims, such as claims without requiring a certain feature or process step in the appended claim or in the specification, clarifications to the claim elements, different\ncombinations, and alternative implementations based on the specification, or different uses, are also contemplated by the embodiments of the present invention\nIt should be understood that combinations of described features or steps are contemplated even if they are not described directly together or not in the same context.\nThe terms or words that are used herein are directed to those of ordinary skill in the art in this field of technology and the meaning of those terms or words will be understood from terminology used in that field or can be reasonably\ninterpreted based on the plain English meaning of the words in conjunction with knowledge in this field of technology.  This includes an understanding of implicit features that for example may involve multiple possibilities, but to a person of ordinary\nskill in the art a reasonable or primary understanding or meaning is understood.\nSoftware can be implemented as distinct modules or software applications or can be integrated together into an overall application such as one that includes the user interface and that handles other feature for providing the functionality to the\nuser on their device.\nIt is intended that the specification and examples be considered as exemplary only, with a true scope being indicated by the claims and their equivalents.", "application_number": "16104706", "abstract": " A research, analysis, regulatory compliance and media platform that\n     connects customers to finished research and analysis produced by subject\n     matter experts is described. The platform facilitates research,\n     investigations, and analysis by creating a single environment in which a\n     group of distributed analysts conduct research and investigations, store\n     and retrieve documents and other sources, collaborate, and publish\n     findings. Consumers are able to query a published knowledge graph,\n     surface high value relationships, and access insights captured by analyst\n     through a customer web portal or external production environment. The\n     platform allows analysts and customers to research and map the\n     commercial, financial, and facilitation networks of sanctioned or other\n     actors that may be associated with illicit activity. Customers can access\n     visual graphs depicting relationships between sanctioned and\n     non-sanctioned actors in order to evaluate their possible exposure to\n     financial crime or sanctions-related risks.\n", "citations": ["7315626", "7315826", "9116975", "20040090472", "20040193572", "20050102273", "20100223276", "20110288905", "20120137367", "20130073387", "20130144957", "20150032468", "20150154281", "20150169746", "20150227559"], "related": ["15713260"]}, {"id": "20190164011", "patent_code": "10373020", "patent_name": "Computer architecture for emulating an asynchronous correlithm object\n     processing system", "year": "2019", "inventor_and_country_data": " Inventors: \nLawrence; Patrick N. (Plano, TX)  ", "description": "<BR><BR>TECHNICAL FIELD\nThe present disclosure relates generally to computer architectures for emulating a processing system, and more specifically to computer architectures for emulating a correlithm object processing system.\n<BR><BR>BACKGROUND\nConventional computers are highly attuned to using operations that require manipulating ordinal numbers, especially ordinal binary integers.  The value of an ordinal number corresponds with its position in a set of sequentially ordered number\nvalues.  These computers use ordinal binary integers to represent, manipulate, and store information.  These computers rely on the numerical order of ordinal binary integers representing data to perform various operations such as counting, sorting,\nindexing, and mathematical calculations.  Even when performing operations that involve other number systems (e.g. floating point), conventional computers still resort to using ordinal binary integers to perform any operations.\nOrdinal based number systems only provide information about the sequence order of the numbers themselves based on their numeric values.  Ordinal numbers do not provide any information about any other types of relationships for the data being\nrepresented by the numeric values such as similarity.  For example, when a conventional computer uses ordinal numbers to represent data samples (e.g. images or audio signals), different data samples are represented by different numeric values.  The\ndifferent numeric values do not provide any information about how similar or dissimilar one data sample is from another.  Unless there is an exact match in ordinal number values, conventional systems are unable to tell if a data sample matches or is\nsimilar to any other data samples.  As a result, conventional computers are unable to use ordinal numbers by themselves for comparing different data samples and instead these computers rely on complex signal processing techniques.  Determining whether a\ndata sample matches or is similar to other data samples is not a trivial task and poses several technical challenges for conventional computers.  These technical challenges result in complex processes that consume processing power which reduces the speed\nand performance of the system.  The ability to compare unknown data samples to known data samples is crucial for many security applications such as face recognition, voice recognition, and fraud detection.\nThus, it is desirable to provide a solution that allows computing systems to efficiently determine how similar different data samples are to each other and to perform operations based on their similarity.\n<BR><BR>SUMMARY\nConventional computers are highly attuned to using operations that require manipulating ordinal numbers, especially ordinal binary integers.  The value of an ordinal number corresponds with its position in a set of sequentially ordered number\nvalues.  These computers use ordinal binary integers to represent, manipulate, and store information.  These computers rely on the numerical order of ordinal binary integers representing data to perform various operations such as counting, sorting,\nindexing, and mathematical calculations.  Even when performing operations that involve other number systems (e.g. floating point), conventional computers still resort to using ordinal binary integers to perform any operations.\nOrdinal based number systems only provide information about the sequence order of the numbers themselves based on their numeric values.  Ordinal numbers do not provide any information about any other types of relationships for the data being\nrepresented by the numeric values such as similarity.  For example, when a conventional computer uses ordinal numbers to represent data samples (e.g. images or audio signals), different data samples are represented by different numeric values.  The\ndifferent numeric values do not provide any information about how similar or dissimilar one data sample is from another.  Unless there is an exact match in ordinal number values, conventional systems are unable to tell if a data sample matches or is\nsimilar to any other data samples.  As a result, conventional computers are unable to use ordinal numbers by themselves for comparing different data samples and instead these computers rely on complex signal processing techniques.  Determining whether a\ndata sample matches or is similar to other data samples is not a trivial task and poses several technical challenges for conventional computers.  These technical challenges result in complex processes that consume processing power which reduces the speed\nand performance of the system.  The ability to compare unknown data samples to known data samples is crucial for many applications such as security application (e.g. face recognition, voice recognition, and fraud detection).\nThe system described in the present application provides a technical solution that enables the system to efficiently determine how similar different objects are to each other and to perform operations based on their similarity.  In contrast to\nconventional systems, the system uses an unconventional configuration to perform various operations using categorical numbers and geometric objects, also referred to as correlithm objects, instead of ordinal numbers.  Using categorical numbers and\ncorrelithm objects on a conventional device involves changing the traditional operation of the computer to support representing and manipulating concepts as correlithm objects.  A device or system may be configured to implement or emulate a special\npurpose computing device capable of performing operations using correlithm objects.  Implementing or emulating a correlithm object processing system improves the operation of a device by enabling the device to perform non-binary comparisons (i.e. match\nor no match) between different data samples.  This enables the device to quantify a degree of similarity between different data samples.  This increases the flexibility of the device to work with data samples having different data types and/or formats,\nand also increases the speed and performance of the device when performing operations using data samples.  These technical advantages and other improvements to the device are described in more detail throughout the disclosure.\nIn one embodiment, the system is configured to use binary integers as categorical numbers rather than ordinal numbers which enables the system to determine how similar a data sample is to other data samples.  Categorical numbers provide\ninformation about similar or dissimilar different data samples are from each other.  For example, categorical numbers can be used in facial recognition applications to represent different images of faces and/or features of the faces.  The system provides\na technical advantage by allowing the system to assign correlithm objects represented by categorical numbers to different data samples based on how similar they are to other data samples.  As an example, the system is able to assign correlithm objects to\ndifferent images of people such that the correlithm objects can be directly used to determine how similar the people in the images are to each other.  In other words, the system is able to use correlithm objects in facial recognition applications to\nquickly determine whether a captured image of a person matches any previously stored images without relying on conventional signal processing techniques.\nCorrelithm object processing systems use new types of data structures called correlithm objects that improve the way a device operates, for example, by enabling the device to perform non-binary data set comparisons and to quantify the similarity\nbetween different data samples.  Correlithm objects are data structures designed to improve the way a device stores, retrieves, and compares data samples in memory.  Correlithm objects also provide a data structure that is independent of the data type\nand format of the data samples they represent.  Correlithm objects allow data samples to be directly compared regardless of their original data type and/or format.\nA correlithm object processing system uses a combination of a sensor table, a node table, and/or an actor table to provide a specific set of rules that improve computer-related technologies by enabling devices to compare and to determine the\ndegree of similarity between different data samples regardless of the data type and/or format of the data sample they represent.  The ability to directly compare data samples having different data types and/or formatting is a new functionality that\ncannot be performed using conventional computing systems and data structures.\nIn addition, correlithm object processing system uses a combination of a sensor table, a node table, and/or an actor table to provide a particular manner for transforming data samples between ordinal number representations and correlithm objects\nin a correlithm object domain.  Transforming data samples between ordinal number representations and correlithm objects involves fundamentally changing the data type of data samples between an ordinal number system and a categorical number system to\nachieve the previously described benefits of the correlithm object processing system.\nUsing correlithm objects allows the system or device to compare data samples (e.g. images) even when the input data sample does not exactly match any known or previously stored input values.  For example, an input data sample that is an image\nmay have different lighting conditions than the previously stored images.  The differences in lighting conditions can make images of the same person appear different from each other.  The device uses an unconventional configuration that implements a\ncorrelithm object processing system that uses the distance between the data samples which are represented as correlithm objects and other known data samples to determine whether the input data sample matches or is similar to the other known data samples. Implementing a correlithm object processing system fundamentally changes the device and the traditional data processing paradigm.  Implementing the correlithm object processing system improves the operation of the device by enabling the device to perform\nnon-binary comparisons of data samples.  In other words, the device is able to determine how similar the data samples are to each other even when the data samples are not exact matches.  In addition, the device is able to quantify how similar data\nsamples are to one another.  The ability to determine how similar data samples are to each other is unique and distinct from conventional computers that can only perform binary comparisons to identify exact matches.\nThe problems associated with comparing data sets and identifying matches based on the comparison are problems necessarily rooted in computer technologies.  As described above, conventional systems are limited to a binary comparison that can only\ndetermine whether an exact match is found.  Emulating a correlithm object processing system provides a technical solution that addresses problems associated with comparing data sets and identifying matches.  Using correlithm objects to represent data\nsamples fundamentally changes the operation of a device and how the device views data samples.  By implementing a correlithm object processing system, the device can determine the distance between the data samples and other known data samples to\ndetermine whether the input data sample matches or is similar to the other known data samples.  In addition, the device is able to determine a degree of similarity that quantifies how similar different data samples are to one another.\nCertain embodiments of the present disclosure may include some, all, or none of these advantages.  These advantages and other features will be more clearly understood from the following detailed description taken in conjunction with the\naccompanying drawings and claims. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nFor a more complete understanding of this disclosure, reference is now made to the following brief description, taken in connection with the accompanying drawings and detailed description, wherein like reference numerals represent like parts.\nFIG. 1 is a schematic view of an embodiment of a special purpose computer implementing correlithm objects in an n-dimensional space;\nFIG. 2 is a perspective view of an embodiment of a mapping between correlithm objects in different n-dimensional spaces;\nFIG. 3 is a schematic view of an embodiment of a correlithm object processing system;\nFIG. 4 is a protocol diagram of an embodiment of a correlithm object process flow;\nFIG. 5 is a schematic diagram of an embodiment a computer architecture for emulating a correlithm object processing system;\nFIG. 6 is a schematic diagram of an embodiment of a correlithm object delay node for a correlithm object processing system;\nFIG. 7A is an embodiment of a table demonstrating a delay operation for correlithm object delay node;\nFIG. 7B is an embodiment of a table demonstrating a state hold delay operation for correlithm object delay node;\nFIG. 8 is a schematic diagram of an embodiment of a correlithm object delay line that includes multiple stages of correlithm object delay nodes;\nFIG. 9 is an embodiment of a table that demonstrating an example delay operation for correlithm object delay line;\nFIG. 10 is a schematic diagram of an embodiment of a finite impulse response filter 1000 implemented using correlithm object delay nodes;\nFIG. 11 is a schematic diagram of an embodiment of an infinite impulse response filter implemented using correlithm object delay nodes;\nFIG. 12 is schematic diagram of an embodiment of an asynchronous correlithm object processing system;\nFIG. 13 is an embodiment of a timing diagram for an asynchronous correlithm object processing system;\nFIG. 14 is a schematic diagram of an embodiment of a synchronous correlithm object processing system;\nFIG. 15 is a flowchart of an embodiment of a process for emulating a synchronous correlithm object processing system;\nFIG. 16 is a schematic diagram of an embodiment of a synchronous correlithm object processing system with a master boss and a slave boss; and\nFIG. 17 is a schematic diagram of an embodiment of a synchronous correlithm object processing system with a master boss and multiple slave bosses.\n<BR><BR>DETAILED DESCRIPTION\nFIGS. 1-5 generally describe various embodiments of how a correlithm object processing system may be implemented or emulated in hardware, such as a special purpose computer.\nFIG. 1 is a schematic view of an embodiment of a user device 100 implementing correlithm objects 104 in an n-dimensional space 102.  Examples of user devices 100 include, but are not limited to, desktop computers, mobile phones, tablet\ncomputers, laptop computers, or other special purpose computer platforms.  The user device 100 is configured to implement or emulate a correlithm object processing system that uses categorical numbers to represent data samples as correlithm objects 104\nin a high-dimensional space 102, for example a high-dimensional binary cube.  Additional information about the correlithm object processing system is described in FIG. 3.  Additional information about configuring the user device 100 to implement or\nemulate a correlithm object processing system is described in FIG. 5.\nConventional computers rely on the numerical order of ordinal binary integers representing data to perform various operations such as counting, sorting, indexing, and mathematical calculations.  Even when performing operations that involve other\nnumber systems (e.g. floating point), conventional computers still resort to using ordinal binary integers to perform any operations.  Ordinal based number systems only provide information about the sequence order of the numbers themselves based on their\nnumeric values.  Ordinal numbers do not provide any information about any other types of relationships for the data being represented by the numeric values, such as similarity.  For example, when a conventional computer uses ordinal numbers to represent\ndata samples (e.g. images or audio signals), different data samples are represented by different numeric values.  The different numeric values do not provide any information about how similar or dissimilar one data sample is from another.  In other\nwords, conventional computers are only able to make binary comparisons of data samples which only results in determining whether the data samples match or do not match.  Unless there is an exact match in ordinal number values, conventional systems are\nunable to tell if a data sample matches or is similar to any other data samples.  As a result, conventional computers are unable to use ordinal numbers by themselves for determining similarity between different data samples, and instead these computers\nrely on complex signal processing techniques.  Determining whether a data sample matches or is similar to other data samples is not a trivial task and poses several technical challenges for conventional computers.  These technical challenges result in\ncomplex processes that consume processing power which reduces the speed and performance of the system.\nIn contrast to conventional systems, the user device 100 operates as a special purpose machine for implementing or emulating a correlithm object processing system.  Implementing or emulating a correlithm object processing system improves the\noperation of the user device 100 by enabling the user device 100 to perform non-binary comparisons (i.e. match or no match) between different data samples.  This enables the user device 100 to quantify a degree of similarity between different data\nsamples.  This increases the flexibility of the user device 100 to work with data samples having different data types and/or formats, and also increases the speed and performance of the user device 100 when performing operations using data samples. \nThese improvements and other benefits to the user device 100 are described in more detail below and throughout the disclosure.\nFor example, the user device 100 employs the correlithm object processing system to allow the user device 100 to compare data samples even when the input data sample does not exactly match any known or previously stored input values. \nImplementing a correlithm object processing system fundamentally changes the user device 100 and the traditional data processing paradigm.  Implementing the correlithm object processing system improves the operation of the user device 100 by enabling the\nuser device 100 to perform non-binary comparisons of data samples.  In other words, the user device 100 is able to determine how similar the data samples are to each other even when the data samples are not exact matches.  In addition, the user device\n100 is able to quantify how similar data samples are to one another.  The ability to determine how similar data samples are to each others is unique and distinct from conventional computers that can only perform binary comparisons to identify exact\nmatches.\nThe user device's 100 ability to perform non-binary comparisons of data samples also fundamentally changes traditional data searching paradigms.  For example, conventional search engines rely on finding exact matches or exact partial matches of\nsearch tokens to identify related data samples.  For instance, conventional text-based search engine are limited to finding related data samples that have text that exactly matches other data samples.  These search engines only provide a binary result\nthat identifies whether or not an exact match was found based on the search token.  Implementing the correlithm object processing system improves the operation of the user device 100 by enabling the user device 100 to identify related data samples based\non how similar the search token is to other data sample.  These improvements result in increased flexibility and faster search time when using a correlithm object processing system.  The ability to identify similarities between data samples expands the\ncapabilities of a search engine to include data samples that may not have an exact match with a search token but are still related and similar in some aspects.  The user device 100 is also able to quantify how similar data samples are to each other based\non characteristics besides exact matches to the search token.  Implementing the correlithm object processing system involves operating the user device 100 in an unconventional manner to achieve these technological improvements as well as other benefits\ndescribed below for the user device 100.\nComputing devices typically rely on the ability to compare data sets (e.g. data samples) to one another for processing.  For example, in security or authentication applications a computing device is configured to compare an input of an unknown\nperson to a data set of known people (or biometric information associated with these people).  The problems associated with comparing data sets and identifying matches based on the comparison are problems necessarily rooted in computer technologies.  As\ndescribed above, conventional systems are limited to a binary comparison that can only determine whether an exact match is found.  As an example, an input data sample that is an image of a person may have different lighting conditions than previously\nstored images.  In this example, different lighting conditions can make images of the same person appear different from each other.  Conventional computers are unable to distinguish between two images of the same person with different lighting conditions\nand two images of two different people without complicated signal processing.  In both of these cases, conventional computers can only determine that the images are different.  This is because conventional computers rely on manipulating ordinal numbers\nfor processing.\nIn contrast, the user device 100 uses an unconventional configuration that uses correlithm objects to represent data samples.  Using correlithm objects to represent data samples fundamentally changes the operation of the user device 100 and how\nthe device views data samples.  By implementing a correlithm object processing system, the user device 100 can determine the distance between the data samples and other known data samples to determine whether the input data sample matches or is similar\nto the other known data samples, as explained in detail below.  Unlike the conventional computers described in the previous example, the user device 100 is able to distinguish between two images of the same person with different lighting conditions and\ntwo images of two different people by using correlithm objects 104.  Correlithm objects allow the user device 100 to determine whether there are any similarities between data samples, such as between two images that are different from each other in some\nrespects but similar in other respects.  For example, the user device 100 is able to determine that despite different lighting conditions, the same person is present in both images.\nIn addition, the user device 100 is able to determine a degree of similarity that quantifies how similar different data samples are to one another.  Implementing a correlithm object processing system in the user device 100 improves the operation\nof the user device 100 when comparing data sets and identifying matches by allowing the user device 100 to perform non-binary comparisons between data sets and to quantify the similarity between different data samples.  In addition, using a correlithm\nobject processing system results in increased flexibility and faster search times when comparing data samples or data sets.  Thus, implementing a correlithm object processing system in the user device 100 provides a technical solution to a problem\nnecessarily rooted in computer technologies.\nThe ability to implement a correlithm object processing system provides a technical advantage by allowing the system to identify and compare data samples regardless of whether an exact match has been previous observed or stored.  In other words,\nusing the correlithm object processing system the user device 100 is able to identify similar data samples to an input data sample in the absence of an exact match.  This functionality is unique and distinct from conventional computers that can only\nidentify data samples with exact matches.\nExamples of data samples include, but are not limited to, images, files, text, audio signals, biometric signals, electric signals, or any other suitable type of data.  A correlithm object 104 is a point in the n-dimensional space 102, sometimes\ncalled an \"n-space.\" The value of represents the number of dimensions of the space.  For example, an n-dimensional space 102 may be a 3-dimensional space, a 50-dimensional space, a 100-dimensional space, or any other suitable dimension space.  The number\nof dimensions depends on its ability to support certain statistical tests, such as the distances between pairs of randomly chosen points in the space approximating a normal distribution.  In some embodiments, increasing the number of dimensions in the\nn-dimensional space 102 modifies the statistical properties of the system to provide improved results.  Increasing the number of dimensions increases the probability that a correlithm object 104 is similar to other adjacent correlithm objects 104.  In\nother words, increasing the number of dimensions increases the correlation between how close a pair of correlithm objects 104 are to each other and how similar the correlithm objects 104 are to each other.\nCorrelithm object processing systems use new types of data structures called correlithm objects 104 that improve the way a device operates, for example, by enabling the device to perform non-binary data set comparisons and to quantify the\nsimilarity between different data samples.  Correlithm objects 104 are data structures designed to improve the way a device stores, retrieves, and compares data samples in memory.  Unlike conventional data structures, correlithm objects 104 are data\nstructures where objects can be expressed in a high-dimensional space such that distance 106 between points in the space represent the similarity between different objects or data samples.  In other words, the distance 106 between a pair of correlithm\nobjects 104 in the n-dimensional space 102 indicates how similar the correlithm objects 104 are from each other and the data samples they represent.  Correlithm objects 104 that are close to each other are more similar to each other than correlithm\nobjects 104 that are further apart from each other.  For example, in a facial recognition application, correlithm objects 104 used to represent images of different types of glasses may be relatively close to each other compared to correlithm objects 104\nused to represent images of other features such as facial hair.  An exact match between two data samples occurs when their corresponding correlithm objects 104 are the same or have no distance between them.  When two data samples are not exact matches\nbut are similar, the distance between their correlithm objects 104 can be used to indicate their similarities.  In other words, the distance 106 between correlithm objects 104 can be used to identify both data samples that exactly match each other as\nwell as data samples that do not match but are similar.  This feature is unique to a correlithm processing system and is unlike conventional computers that are unable to detect when data samples are different but similar in some aspects.\nCorrelithm objects 104 also provide a data structure that is independent of the data type and format of the data samples they represent.  Correlithm objects 104 allow data samples to be directly compared regardless of their original data type\nand/or format.  In some instances, comparing data samples as correlithm objects 104 is computationally more efficient and faster than comparing data samples in their original format.  For example, comparing images using conventional data structures\ninvolves significant amounts of image processing which is time consuming and consumes processing resources.  Thus, using correlithm objects 104 to represent data samples provides increased flexibility and improved performance compared to using other\nconventional data structures.\nIn one embodiment, correlithm objects 104 may be represented using categorical binary strings.  The number of bits used to represent the correlithm object 104 corresponds with the number of dimensions of the n-dimensional space 102 where the\ncorrelithm object 102 is located.  For example, each correlithm object 104 may be uniquely identified using a 64-bit string in a 64-dimensional space 102.  As another example, each correlithm object 104 may be uniquely identified using a 10-bit string in\na 10-dimensional space 102.  In other examples, correlithm objects 104 can be identified using any other suitable number of bits in a string that corresponds with the number of dimensions in the n-dimensional space 102.\nIn this configuration, the distance 106 between two correlithm objects 104 can be determined based on the differences between the bits of the two correlithm objects 104.  In other words, the distance 106 between two correlithm objects can be\ndetermined based on how many individual bits differ between the correlithm objects 104.  The distance 106 between two correlithm objects 104 can be computed using hamming distance or any other suitable technique.\nAs an example using a 10-dimensional space 102, a first correlithm object 104 is represented by a first 10-bit string (1001011011) and a second correlithm object 104 is represented by a second 10-bit string (1000011011).  The hamming distance\ncorresponds with the number of bits that differ between the first correlithm object 104 and the second correlithm object 104.  In other words, the hamming distance between the first correlithm object 104 and the second correlithm object 104 can be\ncomputed as follows:\n1001011011\n1000011011\n- - -\n0001000000\nIn this example, the hamming distance is equal to one because only one bit differs between the first correlithm object 104 and the second correlithm object.  As another example, a third correlithm object 104 is represented by a third 10-bit\nstring (0110100100).  In this example, the hamming distance between the first correlithm object 104 and the third correlithm object 104 can be computed as follows:\n1001011011\n0110100100\n- - -\n1111111111\nThe hamming distance is equal to ten because all of the bits are different between the first correlithm object 104 and the third correlithm object 104.  In the previous example, a hamming distance equal to one indicates that the first correlithm\nobject 104 and the second correlithm object 104 are close to each other in the n-dimensional space 102, which means they are similar to each other.  In the second example, a hamming distance equal to ten indicates that the first correlithm object 104 and\nthe third correlithm object 104 are further from each other in the n-dimensional space 102 and are less similar to each other than the first correlithm object 104 and the second correlithm object 104.  In other words, the similarity between a pair of\ncorrelithm objects can be readily determined based on the distance between the pair correlithm objects.\nAs another example, the distance between a pair of correlithm objects 104 can be determined by performing an X0R operation between the pair of correlithm objects 104 and counting the number of logical high values in the binary string.  The\nnumber of logical high values indicates the number of bits that are different between the pair of correlithm objects 104 which also corresponds with the hamming distance between the pair of correlithm objects 104.\nIn another embodiment, the distance 106 between two correlithm objects 104 can be determined using a Minkowski distance such as the Euclidean or \"straight-line\" distance between the correlithm objects 104.  For example, the distance 106 between\na pair of correlithm objects 104 may be determined by calculating the square root of the sum of squares of the coordinate difference in each dimension.\nThe user device 100 is configured to implement or emulate a correlithm object processing system that comprises one or more sensors 302, nodes 304, and/or actors 306 in order to convert data samples between real world values or representations\nand to correlithm objects 104 in a correlithm object domain.  Sensors 302 are generally configured to convert real world data samples to the correlithm object domain.  Nodes 304 are generally configured to process or perform various operations on\ncorrelithm objects in the correlithm object domain.  Actors 306 are generally configured to convert correlithm objects 104 into real world values or representations.  Additional information about sensors 302, nodes 304, and actors 306 is described in\nFIG. 3.\nPerforming operations using correlithm objects 104 in a correlithm object domain allows the user device 100 to identify relationships between data samples that cannot be identified using conventional data processing systems.  For example, in the\ncorrelithm object domain, the user device 100 is able to identify not only data samples that exactly match an input data sample, but also other data samples that have similar characteristics or features as the input data samples.  Conventional computers\nare unable to identify these types of relationships readily.  Using correlithm objects 104 improves the operation of the user device 100 by enabling the user device 100 to efficiently process data samples and identify relationships between data samples\nwithout relying on signal processing techniques that require a significant amount of processing resources.  These benefits allow the user device 100 to operate more efficiently than conventional computers by reducing the amount of processing power and\nresources that are needed to perform various operations.\nFIG. 2 is a schematic view of an embodiment of a mapping between correlithm objects 104 in different n-dimensional spaces 102.  When implementing a correlithm object processing system, the user device 100 performs operations within the\ncorrelithm object domain using correlithm objects 104 in different n-dimensional spaces 102.  As an example, the user device 100 may convert different types of data samples having real world values into correlithm objects 104 in different n-dimensional\nspaces 102.  For instance, the user device 100 may convert data samples of text into a first set of correlithm objects 104 in a first n-dimensional space 102 and data samples of audio samples as a second set of correlithm objects 104 in a second\nn-dimensional space 102.  Conventional systems require data samples to be of the same type and/or format in order to perform any kind of operation on the data samples.  In some instances, some types of data samples cannot be compared because there is no\ncommon format available.  For example, conventional computers are unable to compare data samples of images and data samples of audio samples because there is no common format.  In contrast, the user device 100 implementing a correlithm object processing\nsystem is able to compare and perform operations using correlithm objects 104 in the correlithm object domain regardless of the type or format of the original data samples.\nIn FIG. 2, a first set of correlithm objects 104A are defined within a first n-dimensional space 102A and a second set of correlithm objects 104B are defined within a second n-dimensional space 102B.  The n-dimensional spaces may have the same\nnumber dimensions or a different number of dimensions.  For example, the first n-dimensional space 102A and the second n-dimensional space 102B may both be three dimensional spaces.  As another example, the first n-dimensional space 102A may be a three\ndimensional space and the second n-dimensional space 102B may be a nine dimensional space.  Correlithm objects 104 in the first n-dimensional space 102A and second n-dimensional space 102B are mapped to each other.  In other words, a correlithm object\n104A in the first n-dimensional space 102A may reference or be linked with a particular correlithm object 104B in the second n-dimensional space 102B.  The correlithm objects 104 may also be linked with and referenced with other correlithm objects 104 in\nother n-dimensional spaces 102.\nIn one embodiment, a data structure such as table 200 may be used to map or link correlithm objects 194 in different n-dimensional spaces 102.  In some instances, table 200 is referred to as a node table.  Table 200 is generally configured to\nidentify a first plurality of correlithm objects 104 in a first n-dimensional space 102 and a second plurality of correlithm objects 104 in a second n-dimensional space 102.  Each correlithm object 104 in the first n-dimensional space 102 is linked with\na correlithm object 104 is the second n-dimensional space 102.  For example, table 200 may be configured with a first column 202 that lists correlithm objects 104A as source correlithm objects and a second column 204 that lists corresponding correlithm\nobjects 104B as target correlithm objects.  In other examples, table 200 may be configured in any other suitable manner or may be implemented using any other suitable data structure.  In some embodiments, one or more mapping functions may be used to\nconvert between a correlithm object 104 in a first n-dimensional space and a correlithm object 104 is a second n-dimensional space.\nFIG. 3 is a schematic view of an embodiment of a correlithm object processing system 300 that is implemented by a user device 100 to perform operations using correlithm objects 104.  The system 300 generally comprises a sensor 302, a node 304,\nand an actor 306.  The system 300 may be configured with any suitable number and/or configuration of sensors 302, nodes 304, and actors 306.  An example of the system 300 in operation is described in FIG. 4.  In one embodiment, a sensor 302, a node 304,\nand an actor 306 may all be implemented on the same device (e.g. user device 100).  In other embodiments, a sensor 302, a node 304, and an actor 306 may each be implemented on different devices in signal communication with each other for example over a\nnetwork.  In other embodiments, different devices may be configured to implement any combination of sensors 302, nodes 304, and actors 306.\nSensors 302 serve as interfaces that allow a user device 100 to convert real world data samples into correlithm objects 104 that can be used in the correlithm object domain.  Sensors 302 enable the user device 100 compare and perform operations\nusing correlithm objects 104 regardless of the data type or format of the original data sample.  Sensors 302 are configured to receive a real world value 320 representing a data sample as an input, to determine a correlithm object 104 based on the real\nworld value 320, and to output the correlithm object 104.  For example, the sensor 302 may receive an image 301 of a person and output a correlithm object 322 to the node 304 or actor 306.  In one embodiment, sensors 302 are configured to use sensor\ntables 308 that link a plurality of real world values with a plurality of correlithm objects 104 in an n-dimensional space 102.  Real world values are any type of signal, value, or representation of data samples.  Examples of real world values include,\nbut are not limited to, images, pixel values, text, audio signals, electrical signals, and biometric signals.  As an example, a sensor table 308 may be configured with a first column 312 that lists real world value entries corresponding with different\nimages and a second column 314 that lists corresponding correlithm objects 104 as input correlithm objects.  In other examples, sensor tables 308 may be configured in any other suitable manner or may be implemented using any other suitable data\nstructure.  In some embodiments, one or more mapping functions may be used to translate between a real world value 320 and a correlithm object 104 is a n-dimensional space 102.  Additional information for implementing or emulating a sensor 302 in\nhardware is described in FIG. 5.\nNodes 304 are configured to receive a correlithm object 104 (e.g. an input correlithm object 104), to determine another correlithm object 104 based on the received correlithm object 104, and to output the identified correlithm object 104 (e.g.\nan output correlithm object 104).  In one embodiment, nodes 304 are configured to use node tables 200 that link a plurality of correlithm objects 104 from a first n-dimensional space 102 with a plurality of correlithm objects 104 in a second\nn-dimensional space 102.  A node table 200 may be configured similar to the table 200 described in FIG. 2.  Additional information for implementing or emulating a node 304 in hardware is described in FIG. 5.\nActors 306 serve as interfaces that allow a user device 100 to convert correlithm objects 104 in the correlithm object domain back to real world values or data samples.  Actors 306 enable the user device 100 to convert from correlithm objects\n104 into any suitable type of real world value.  Actors 306 are configured to receive a correlithm object 104 (e.g. an output correlithm object 104), to determine a real world output value 326 based on the received correlithm object 104, and to output\nthe real world output value 326.  The real world output value 326 may be a different data type or representation of the original data sample.  As an example, the real world input value 320 may be an image 301 of a person and the resulting real world\noutput value 326 may be text 327 and/or an audio signal identifying the person.  In one embodiment, actors 306 are configured to use actor tables 310 that link a plurality of correlithm objects 104 in an n-dimensional space 102 with a plurality of real\nworld values.  As an example, an actor table 310 may be configured with a first column 316 that lists correlithm objects 104 as output correlithm objects and a second column 318 that lists real world values.  In other examples, actor tables 310 may be\nconfigured in any other suitable manner or may be implemented using any other suitable data structure.  In some embodiments, one or more mapping functions may be employed to translate between a correlithm object 104 in an n-dimensional space and a real\nworld output value 326.  Additional information for implementing or emulating an actor 306 in hardware is described in FIG. 5.\nA correlithm object processing system 300 uses a combination of a sensor table 308, a node table 200, and/or an actor table 310 to provide a specific set of rules that improve computer-related technologies by enabling devices to compare and to\ndetermine the degree of similarity between different data samples regardless of the data type and/or format of the data sample they represent.  The ability to directly compare data samples having different data types and/or formatting is a new\nfunctionality that cannot be performed using conventional computing systems and data structures.  Conventional systems require data samples to be of the same type and/or format in order to perform any kind of operation on the data samples.  In some\ninstances, some types of data samples are incompatible with each other and cannot be compared because there is no common format available.  For example, conventional computers are unable to compare data samples of images with data samples of audio\nsamples because there is no common format available.  In contrast, a device implementing a correlithm object processing system uses a combination of a sensor table 308, a node table 200, and/or an actor table 310 to compare and perform operations using\ncorrelithm objects 104 in the correlithm object domain regardless of the type or format of the original data samples.  The correlithm object processing system 300 uses a combination of a sensor table 308, a node table 200, and/or an actor table 310 as a\nspecific set of rules that provides a particular solution to dealing with different types of data samples and allows devices to perform operations on different types of data samples using correlithm objects 104 in the correlithm object domain.  In some\ninstances, comparing data samples as correlithm objects 104 is computationally more efficient and faster than comparing data samples in their original format.  Thus, using correlithm objects 104 to represent data samples provides increased flexibility\nand improved performance compared to using other conventional data structures.  The specific set of rules used by the correlithm object processing system 300 go beyond simply using routine and conventional activities in order to achieve this new\nfunctionality and performance improvements.\nIn addition, correlithm object processing system 300 uses a combination of a sensor table 308, a node table 200, and/or an actor table 310 to provide a particular manner for transforming data samples between ordinal number representations and\ncorrelithm objects 104 in a correlithm object domain.  For example, the correlithm object processing system 300 may be configured to transform a representation of a data sample into a correlithm object 104, to perform various operations using the\ncorrelithm object 104 in the correlithm object domain, and to transform a resulting correlithm object 104 into another representation of a data sample.  Transforming data samples between ordinal number representations and correlithm objects 104 involves\nfundamentally changing the data type of data samples between an ordinal number system and a categorical number system to achieve the previously described benefits of the correlithm object processing system 300.\nFIG. 4 is a protocol diagram of an embodiment of a correlithm object process flow 400.  A user device 100 implements process flow 400 to emulate a correlithm object processing system 300 to perform operations using correlithm object 104 such as\nfacial recognition.  The user device 100 implements process flow 400 to compare different data samples (e.g. images, voice signals, or text) are to each other and to identify other objects based on the comparison.  Process flow 400 provides instructions\nthat allows user devices 100 to achieve the improved technical benefits of a correlithm object processing system 300.\nConventional systems are configured to use ordinal numbers for identifying different data samples.  Ordinal based number systems only provide information about the sequence order of numbers based on their numeric values, and do not provide any\ninformation about any other types of relationships for the data samples being represented by the numeric values such as similarity.  In contrast, a user device 100 can implement or emulate the correlithm object processing system 300 which provides an\nunconventional solution that uses categorical numbers and correlithm objects 104 to represent data samples.  For example, the system 300 may be configured to use binary integers as categorical numbers to generate correlithm objects 104 which enables the\nuser device 100 to perform operations directly based on similarities between different data samples.  Categorical numbers provide information about how similar different data sample are from each other.  Correlithm objects 104 generated using categorical\nnumbers can be used directly by the system 300 for determining how similar different data samples are from each other without relying on exact matches, having a common data type or format, or conventional signal processing techniques.\nA non-limiting example is provided to illustrate how the user device 100 implements process flow 400 to emulate a correlithm object processing system 300 to perform facial recognition on an image to determine the identity of the person in the\nimage.  In other examples, the user device 100 may implement process flow 400 to emulate a correlithm object processing system 300 to perform voice recognition, text recognition, or any other operation that compares different objects.\nAt step 402, a sensor 302 receives an input signal representing a data sample.  For example, the sensor 302 receives an image of person's face as a real world input value 320.  The input signal may be in any suitable data type or format.  In one\nembodiment, the sensor 302 may obtain the input signal in real-time from a peripheral device (e.g. a camera).  In another embodiment, the sensor 302 may obtain the input signal from a memory or database.\nAt step 404, the sensor 302 identifies a real world value entry in a sensor table 308 based on the input signal.  In one embodiment, the system 300 identifies a real world value entry in the sensor table 308 that matches the input signal.  For\nexample, the real world value entries may comprise previously stored images.  The sensor 302 may compare the received image to the previously stored images to identify a real world value entry that matches the received image.  In one embodiment, when the\nsensor 302 does not find an exact match, the sensor 302 finds a real world value entry that closest matches the received image.\nAt step 406, the sensor 302 identifies and fetches an input correlithm object 104 in the sensor table 308 linked with the real world value entry.  At step 408, the sensor 302 sends the identified input correlithm object 104 to the node 304.  In\none embodiment, the identified input correlithm object 104 is represented in the sensor table 308 using a categorical binary integer string.  The sensor 302 sends the binary string representing to the identified input correlithm object 104 to the node\n304.\nAt step 410, the node 304 receives the input correlithm object 104 and determines distances 106 between the input correlithm object 104 and each source correlithm object 104 in a node table 200.  In one embodiment, the distance 106 between two\ncorrelithm objects 104 can be determined based on the differences between the bits of the two correlithm objects 104.  In other words, the distance 106 between two correlithm objects can be determined based on how many individual bits differ between a\npair of correlithm objects 104.  The distance 106 between two correlithm objects 104 can be computed using hamming distance or any other suitable technique.  In another embodiment, the distance 106 between two correlithm objects 104 can be determined\nusing a Minkowski distance such as the Euclidean or \"straight-line\" distance between the correlithm objects 104.  For example, the distance 106 between a pair of correlithm objects 104 may be determined by calculating the square root of the sum of\nsquares of the coordinate difference in each dimension.\nAt step 412, the node 304 identifies a source correlithm object 104 from the node table 200 with the shortest distance 106.  A source correlithm object 104 with the shortest distance from the input correlithm object 104 is a correlithm object\n104 either matches or most closely matches the received input correlithm object 104.\nAt step 414, the node 304 identifies and fetches a target correlithm object 104 in the node table 200 linked with the source correlithm object 104.  At step 416, the node 304 outputs the identified target correlithm object 104 to the actor 306. \nIn this example, the identified target correlithm object 104 is represented in the node table 200 using a categorical binary integer string.  The node 304 sends the binary string representing to the identified target correlithm object 104 to the actor\n306.\nAt step 418, the actor 306 receives the target correlithm object 104 and determines distances between the target correlithm object 104 and each output correlithm object 104 in an actor table 310.  The actor 306 may compute the distances between\nthe target correlithm object 104 and each output correlithm object 104 in an actor table 310 using a process similar to the process described in step 410.\nAt step 420, the actor 306 identifies an output correlithm object 104 from the actor table 310 with the shortest distance 106.  An output correlithm object 104 with the shortest distance from the target correlithm object 104 is a correlithm\nobject 104 either matches or most closely matches the received target correlithm object 104.\nAt step 422, the actor 306 identifies and fetches a real world output value in the actor table 310 linked with the output correlithm object 104.  The real world output value may be any suitable type of data sample that corresponds with the\noriginal input signal.  For example, the real world output value may be text that indicates the name of the person in the image or some other identifier associated with the person in the image.  As another example, the real world output value may be an\naudio signal or sample of the name of the person in the image.  In other examples, the real world output value may be any other suitable real world signal or value that corresponds with the original input signal.  The real world output value may be in\nany suitable data type or format.\nAt step 424, the actor 306 outputs the identified real world output value.  In one embodiment, the actor 306 may output the real world output value in real-time to a peripheral device (e.g. a display or a speaker).  In one embodiment, the actor\n306 may output the real world output value to a memory or database.  In one embodiment, the real world output value is sent to another sensor 302.  For example, the real world output value may be sent to another sensor 302 as an input for another\nprocess.\nFIG. 5 is a schematic diagram of an embodiment a computer architecture 500 for emulating a correlithm object processing system 300 in a user device 100.  The computer architecture 500 comprises a processor 502, a memory 504, a network interface\n506, and an input-output (I/O) interface 508.  The computer architecture 500 may be configured as shown or in any other suitable configuration.\nThe processor 502 comprises one or more processors operably coupled to the memory 504.  The processor 502 is any electronic circuitry including, but not limited to, state machines, one or more central processing unit (CPU) chips, logic units,\ncores (e.g. a multi-core processor), field-programmable gate array (FPGAs), application specific integrated circuits (ASICs), graphics processing units (GPUs), or digital signal processors (DSPs).  The processor 502 may be a programmable logic device, a\nmicrocontroller, a microprocessor, or any suitable combination of the preceding.  The processor 502 is communicatively coupled to and in signal communication with the memory 204.  The one or more processors are configured to process data and may be\nimplemented in hardware or software.  For example, the processor 502 may be 8-bit, 16-bit, 32-bit, 64-bit or of any other suitable architecture.  The processor 502 may include an arithmetic logic unit (ALU) for performing arithmetic and logic operations,\nprocessor registers that supply operands to the ALU and store the results of ALU operations, and a control unit that fetches instructions from memory and executes them by directing the coordinated operations of the ALU, registers and other components.\nThe one or more processors are configured to implement various instructions.  For example, the one or more processors are configured to execute instructions to implement sensor engines 510, delay node engines 511, node engines 512, boss engines\n513, and actor engines 514.  In an embodiment, the sensor engines 510, the node engines 512, and the actor engines 514 are implemented using logic units, FPGAs, ASICs, DSPs, or any other suitable hardware.\nIn one embodiment, the sensor engine 510 is configured to receive a real world value 320 as an input, to determine a correlithm object 104 based on the real world value 320, and to output the correlithm object 104.  Examples of the sensor engine\n510 in operation are described in FIGS. 4 and 11.\nIn one embodiment, the node engine 512 is configured to receive a correlithm object 104 (e.g. an input correlithm object 104), to determine another correlithm object 104 based on the received correlithm object 104, and to output the identified\ncorrelithm object 104 (e.g. an output correlithm object 104).  The node engine 512 is also configured to compute distances between pairs of correlithm objects 104.  Examples of the node engine 512 in operation are described in FIGS. 4, 6-12, 14, 15A,\n15B, and 18.\nIn one embodiment, the delay node engine 511 is configured to receive a correlithm object 104 and then output the correlithm object 104 after a predetermined amount of time has elapsed.  In other words, the delay node engine 511 is configured to\nprovide delays or delay lines for a correlithm object processing system.  Examples of the delay node engine 511 in operation are described in FIGS. 6-11.\nIn one embodiment, the boss engine 513 is configured to control and synchronize components within a correlithm object processing system.  The boss engine 513 is configured to send commands (e.g. execute commands or output commands) to components\nwithin a correlithm object processing system to control their operation.  Examples of the boss engine 513 in operation are described in FIGS. 14-17.\nIn one embodiment, the actor engine 514 is configured to receive a correlithm object 104 (e.g. an output correlithm object 104), to determine a real world output value 326 based on the received correlithm object 104, and to output the real world\noutput value 326.  Examples of the actor engine 514 in operation are described in FIGS. 4 and 11.\nThe memory 504 comprises one or more non-transitory disks, tape drives, or solid-state drives, and may be used as an over-flow data storage device, to store programs when such programs are selected for execution, and to store instructions and\ndata that are read during program execution.  The memory 504 may be volatile or non-volatile and may comprise read-only memory (ROM), random-access memory (RAM), ternary content-addressable memory (TCAM), dynamic random-access memory (DRAM), and static\nrandom-access memory (SRAM).  The memory 504 is operable to store sensor instructions 516, node instructions 518, actor instructions 520, sensor tables 308, node tables 200, actor tables 310, and/or any other data or instructions.  The sensor\ninstructions 516, the node instructions 518, the delay node instructions 521, the boss instructions 523, and the actor instructions 520 comprise any suitable set of instructions, logic, rules, or code operable to execute the sensor engine 510, node\nengine 512, the delay node engine 511, the boss engine 513, and the actor engine 514, respectively.\nThe sensor tables 308, the node tables 200, and the actor tables 310 may be configured similar to the sensor tables 308, the node tables 200, and the actor tables 310 described in FIG. 3, respectively.  The boss table 1403 generally comprises a\nlist of components within a correlithm object processing system.  Additional information about boss tables 1403 is described in FIGS. 14-17.\nThe network interface 506 is configured to enable wired and/or wireless communications.  The network interface 506 is configured to communicate data with any other device or system.  For example, the network interface 506 may be configured for\ncommunication with a modem, a switch, a router, a bridge, a server, or a client.  The processor 502 is configured to send and receive data using the network interface 506.\nThe I/O interface 508 may comprise ports, transmitters, receivers, transceivers, or any other devices for transmitting and/or receiving data with peripheral devices as would be appreciated by one of ordinary skill in the art upon viewing this\ndisclosure.  For example, the I/O interface 508 may be configured to communicate data between the processor 502 and peripheral hardware such as a graphical user interface, a display, a mouse, a keyboard, a key pad, and a touch sensor (e.g. a touch\nscreen).\nFIGS. 6-11 generally describe embodiments for how delay nodes and delay lines may be emulated or implemented in a correlithm object processing system 300 by a device 100.  Using delay nodes and delay lines allows the device 100 to introduce\ndelays into a correlithm object processing system 300.  These delays can be used to control data flow within an asynchronous or synchronous correlithm object processing system.  Examples of an asynchronous or synchronous correlithm object processing\nsystem are described in FIGS. 12-17.  Controlling data flow improves the speed and efficiency of the device 100 when implementing a correlithm object processing system 300.\nFIG. 6 illustrates one embodiment of a correlithm object delay node 600 that receives an input correlithm object 602 and communicates an output correlithm object 604 after a delay time, as described in detail below.  Delay node 600 includes an\ninput interface 606, output interface 608, processor 610, and memory 612.  Processor 610 and memory 612 may be the same or different than processor 502 and memory 504, respectively, described above.  In some embodiments, memory 612 includes correlithm\nobject values 614 that are used to validate input correlithm object 602, as described in detail below.  In some embodiments, delay node 600 further includes a clock input interface 616 that receives a clock signal 618 that can be used to control the\ndelay time between receiving the input correlithm object 602 and communicating the output correlithm object 604.  For example, a delay node 600 may be configured to emulate one digital clock cycle.  In another example, the delay node 600 may be\nconfigured to emulate more than one digital clock cycle.  For instance, the delay node 600 may emulate a delay of two clock cycles, five clock cycles, or any other suitable number of clock cycles.  In some embodiments, delay node 600 further includes a\nstate input interface 620 that receives a feedback signal 622 to implement a state hold delay operation, as described in further detail below.\nIn operation, input interface 606 of correlithm object delay node 600 receives input correlithm object 602 at a first time (e.g., T0) and communicates it as an output correlithm object 605 at a second time (e.g., T1) later than the first time. \nIn one embodiment, delay node 600 validates input correlithm object 602 to confirm that it is not noise or some other spurious signal by comparing it against a plurality of correlithm object values 614 stored in memory 612.  Correlithm object values 614\nrepresent correlithm objects that are predetermined to be valid inputs or outputs within the correlithm object processing system.  For example, these may be correlithm objects that have been processed by other nodes within the correlithm object\nprocessing system.  In another example, these may be correlithm objects that have been determined to represent real-world data by a sensor 302.  To determine the validity of an input correlithm object 602, processor 610 determines the Hamming distance\nbetween input correlithm object 602 and each of the correlithm object values 614 stored in memory 612.  If any of the determined Hamming distances are within a predetermined threshold of validity, then the input correlithm object 602 is determined to be\nvalid and is stored in memory 612.  By first determining that the input correlithm object 602 is valid, delay node 600 can avoid storing and communicating noise or some other spurious signal that is not, in fact, a valid input.\nThe predetermined threshold of validity can be, for example, a predetermined number of bits that can be different between the input correlithm object 602 and a corresponding one of the correlithm object values 614 while still indicating that the\nn-dimensional space 102 between input correlithm object 602 and that correlithm object value 614 is small enough to conclude with a high degree of likelihood that the input correlithm object 602 is that correlithm object value 614.  For example, if the\ninput correlithm object 602 and the correlithm object values 614 are each 64-bit digital values, then a Hamming distance of up to 8 bits signifies to a high degree of probability that the input correlithm object 602 and a corresponding correlithm object\nvalue 614 are the same.  Similarly, if the input correlithm object 602 and the correlithm object values 614 are each 128-bit digital values, then a Hamming distance of up to 30 bits signifies to a high degree of probability that the input correlithm\nobject 602 and a corresponding correlithm object value 614 are the same.\nIn some embodiments, the delay node 600 may be configured to receive input correlithm object 602 at a first time (e.g., T0) and communicate it as an output correlithm object 605 at a second time (e.g., T1) later than the first time without\ndetermining whether the input correlithm object 602 is a predetermined valid input.  For example, the delay node 600 may communicate a received correlithm object 602 as an output correlithm object 605 after a predetermined amount of delay regardless of\nthe correlithm object's value.\nDelay node 600 determines an appropriate delay between the time at which it receives input correlithm object 602 and the time it communicates it as output correlithm object 604.  In one embodiment, the correlithm object delay node 600\ncommunicates output correlithm object 604 asynchronously, such as by communicating output correlithm object 604 as soon as possible, or according to the schedules of other nodes rather than according to a synchronized clock.  In another embodiment, the\ndelay node 600 communicates output correlithm object 604 according to synchronized clock signal 618 that may be associated with one or more Boss nodes, as explained in further detail below.  In another embodiment, the delay node 600 communicates output\ncorrelithm object 604 according to a digital clock signal generated or provided by the delay node 600.  In another embodiment, the delay node 600 communicates output correlithm object 604 according to a digital clock signal provided by a processor (e.g.\nprocessor 502).\nIn one embodiment, delay node 600 receives output correlithm object 604 as a feedback signal 622 at state input interface 620.  The output correlithm object 604 is stored as state information 624 in memory 612.  This state information 624 is\nused to implement a state hold delay operation as described below in conjunction with FIG. 7B.\nThe correlithm object delay node 600 provides technical advantages in the implementation of either synchronous or asynchronous logic.  With regard to synchronous logic, the delay node 600 helps to ensure that any changes to logic signals\nthroughout a circuit system begin at the same time, at regular intervals, synchronized by a clock signal 618.  Clock signal 618 may be a sequence of repetitive pulses generated by an electronic oscillator or clock generator.  One technical problem with\nimplementing synchronous digital circuitry is that the logic gates which perform the operations on data require a finite amount of time to respond to changes to their inputs.  This is referred to as propagation delay.  The interval between clock pulses\nshould be long enough so that all the logic gates have time to respond to the changes and their outputs \"settle\" to stable logic values, before the next clock pulse occurs.  The delay node 600 helps to ensure that the state of a synchronous circuit\nchanges on clock pulses.  At each cycle, the next state can be determined by the current state and the value of the input signals when the clock pulse occurs, as described in greater detail below.  This promotes stability and reliability in the operation\nof synchronous digital circuits.\nWith regard to asynchronous logic, the outputs of the circuit change directly in response to changes in inputs and the speed of a logic gate device is potentially limited only by the propagation delays of the logic gates that are used.  However,\nasynchronous logic can be more difficult to design and is subject to problems not encountered in synchronous designs.  The main problem is that digital memory elements are sensitive to the order that their input signals arrive.  For example, if two\nsignals arrive at a logic gate at roughly the same time, which state the circuit goes into can depend on which signal gets to the gate first.  Therefore, the circuit can go into the wrong state, depending on small differences in the propagation delays of\nthe logic gates.  This is called a race condition.  Delay node 600 helps to ensure that input signals arrive at logic gates in the proper order, thereby solving a technical problem inherent in asynchronous digital circuitry.\nFIG. 7A illustrates a table 700 demonstrating a delay operation for correlithm object delay node 600.  Table 700 includes a time column 702, an input column 704, and an output column 706.  Rows 708 are filled with an example to demonstrate the\noperation of delay node 600.  According to the illustrated example, delay node 600 receives an input correlithm object 602 signified as X0 at time T0.  After a time delay (e.g., one clock pulse), at time T1, delay node 600 communicates correlithm object\n602 signified as X0 as output correlithm object 604.  A new input correlithm object 602 is not yet received at time T1 or by time T2.  Thus, delay node 600 maintains its most recent input correlithm object 602 signified as X0 at times T1 and T2.  Because\nno new input correlithm object 602 is received by time T1, delay node 600 continues to communicate output correlithm object 602 signified as X0 at time T2.  Delay node 600 then receives a new input correlithm object 602 signified as X1 at time T3, and\ncontinues to communicate output correlithm object 602 signified as X0 at time T3.  After a time delay (e.g., one clock pulse), at time T4, delay node 600 communicates the correlithm object 602 signified as X1 as output correlithm object 604.  A new input\ncorrelithm object 602 is not yet received at time T4 or by time T5.  Because no new input correlithm object 602 is received by time T4, delay node 600 continues to communicate output correlithm object 602 signified as X1 at time T5.  The operation of\ndelay node 600 can be expanded beyond the times T0-T5 illustrated in table 700.\nFIG. 7B illustrates a table 750 demonstrating an example state hold delay operation for correlithm object delay node 600 utilizing feedback signal 722.  Table 750 includes a time column 752, an input column 754, a state input column 756, and an\noutput column 758.  Rows 760 are filled with an example to demonstrate the operation of delay node 600.  According to the illustrated example, delay node 600 receives an input correlithm object 602 signified as X0 at time T0.  After a time delay (e.g.,\none clock pulse), at time T1, delay node 600 communicates correlithm object 602 signified as X0 as output correlithm object 604.  A new input correlithm object 602 is not yet received at time T1 or by time T2.  Thus, no input correlithm object 602 is\nindicated in column 754 for times T1 and T2.  Moreover, output correlithm object 604 signified by X0 is received as feedback signal 622 at state input interface 620 at time T2 and stored in memory 612.  In the absence of an input correlithm object 602 in\ncolumn 754 for time T1, the correlithm object signified by X0 in state input column 756 controls the output correlithm object 604 communicated by delay node 600 at time T2.  Delay node 600 receives a new input correlithm object 602 signified as X1 at\ntime T3.  Also at time T3, state input column 756 continues to store a correlithm object signified as X0.  If the input interface 606 receives a new input correlithm object 602, as it does at time T3, then that correlithm object 602 appears as the output\ncorrelithm object 604 a delay time later (e.g., one clock pulse), as illustrated in table 750 for time T4, regardless of what correlithm object value appears in the state input column 756.  A new input correlithm object 602 is not yet received at time T4\nor by time T5.  Thus, no input correlithm object 602 is indicated in input column 754 for times T4 and T5.  Moreover, output correlithm object 604 signified as X1 is received as feedback signal 622 at state input interface 620 at time T5 and stored in\nmemory 612.  In the absence of an input correlithm object 602 in column 754 for time T4, the correlithm object 602 signified as X1 in state input column 756 controls the output correlithm object 604 communicated by delay node 600 at time T5.\nFIG. 8 illustrates one embodiment of a correlithm object delay line 800 that includes multiple stages (e.g., three) of correlithm object delay nodes 600A, 600B, and 600C communicatively coupled to each other in series.  Each correlithm object\ndelay node 600A-600C of delay line 800 is described in greater detail above with respect to FIGS. 6 and 7A-7B.  First correlithm object delay node 600A receives an input correlithm object 602 and communicates an output correlithm object 604 an\nappropriate delay time later.  This output correlithm object 604 of the first correlithm object delay node 600A is then received by second correlithm object delay node 600B as an input correlithm object 602 and communicated as an output correlithm object\n604 an appropriate delay time later.  The output correlithm object 604 of second delay node 600B is received by third correlithm object delay node 600C as an input correlithm object 602 and communicated as an output correlithm object 604 an appropriate\ndelay time later.  In one embodiment, one or more of the correlithm object delay nodes 600A-600C communicates its output correlithm object 604 as a feedback signal 622 back to itself to implement, for example, a state hold delay operation, as described\nabove.  Although FIG. 8 illustrates a three-stage correlithm object delay line 800, it should be understood that correlithm object delay line 800 can be implemented with two or more correlithm object delay nodes 600 communicatively coupled in series.\nFIG. 9 illustrates a table 900 demonstrating an example delay operation for correlithm object delay line 800.  Table 900 includes a time column 902, an input column 904 and an output column 906 for the first correlithm object delay node 600A, an\ninput column 908 and an output column 910 for the second correlithm object delay node 600B, and an input column 912 and an output column 914 for the third correlithm object delay node 600C.  Rows 916 are filled with an example to demonstrate the\noperation of delay line 800.  In operation, the correlithm objects signified as X0-X5 propagate through and among each correlithm object delay node 600A-600C according to suitable time delays.\nFor example, with respect to the correlithm object signified as X0, correlithm object delay node 600A receives it as an input correlithm object 602A at time T0 and communicates it as output correlithm object 604A after a suitable delay (e.g.,\none clock pulse) at time T1; correlithm object delay node 600B receives it as an input correlithm object 602B at time T2 and communicates it as output correlithm object 604B after a suitable delay at time T3; and correlithm object delay node 600C\nreceives it as an input correlithm object 602C at time T4 and communicates it as output correlithm object 604C after a suitable delay at time T5.\nWith respect to the correlithm object signified as X1, the correlithm object delay node 600A receives it as an input correlithm object 602A at time T1 and communicates it as output correlithm object 604A after a suitable delay at time T2; the\ncorrelithm object delay node 600B receives it as an input correlithm object 602B at time T3 and communicates it as output correlithm object 604B after a suitable delay at time T4; and correlithm object delay node 600C receives it as an input correlithm\nobject 602C at time T5 and communicates it as output correlithm object 604c after a suitable delay, although it is not illustrated in table 900.\nWith respect to the correlithm object signified as X2, correlithm object delay node 600A receives it as an input correlithm object 602A at time T2 and communicates it as output correlithm object 604A after a suitable delay at time T3; and\ncorrelithm object delay node 600B receives it as an input correlithm object 602B at time T4 and communicates it as output correlithm object 604B after a suitable delay at time T5.\nWith respect to the correlithm object signified as X3, the correlithm object delay node 600A receives it as an input correlithm object 602A at time T3 and communicates it as output correlithm object 604A after a suitable delay at time T4; and\ncorrelithm object delay node 600B receives it as an input correlithm object 602B at time T5 and communicates it as output correlithm object 604B after a suitable delay, although it is not illustrated in table 900.\nWith respect to the correlithm object signified as X4, the correlithm object delay node 600A receives it as an input correlithm object 602A at time T4 and communicates it as output correlithm object 604A after a delay at time T5.\nWith respect to the correlithm object signified as X5, the correlithm object delay node 600A receives it as an input correlithm object 602A at time T5 and communicates it as output correlithm object 604A after a suitable delay, although it is\nnot illustrated in table 900.\nFIG. 10 illustrates one embodiment of a finite impulse response (FIR) filter 1000 implemented using delay nodes 600A-600C arranged in series as a correlithm object delay line 800.  In signal processing, a finite impulse response filter is a\nfilter whose impulse response (or response to any finite length input) is of finite duration, because it settles to zero in finite time.  The filter 1000 applies a suitable function on the time delayed outputs of each stage of the correlithm object delay\nline 800 (e.g., f (1, 2, 3)).  Examples of functions include, but are not limited to, an equals function, a summation function, a multiplication function, a subtraction function, a division function, or any other suitable type of function.  In some\nembodiments, the filter 1000 is configured to apply any suitable number of functions to the outputs of the delay nodes 600A-600C.\nFIG. 11 illustrates one embodiment of an infinite impulse response (IIR) filter 1100 implemented using correlithm object delay nodes 600D-600F arranged in series as a correlithm object delay line 800, where each delay node 600 has a feedback\nsignal 622.  The presence of feedback in the topology of a discrete-time filter generally creates an IIR response.  IIR filter 1100 has an impulse response that does not become exactly zero past a certain point, but continues indefinitely.  This is in\ncontrast to a FIR filter 1000 in which the impulse response does because zero after a finite duration.  The IIR filter 1100 applies a suitable function on the time delayed outputs of each stage of the correlithm object delay line 800 (e.g., f (A, B, C)). In a particular embodiment, the output of a FIR filter 1000 may be input to the IIR filter 1100 such that it applies a suitable function, such as f (1, 2, 3, A, B, C), to generate its output.  In another embodiment, the IIR filter 1100 may be independent\nfrom FIR filter 1000.  For example, the IIR filter 1100 may be configured to receive an input from another component instead of the FIR filter 1000.  By implementing FIR filter 1000 and IIR filter 1100 using correlithm object delay nodes 600 arranged in\na correlithm object delay line 800, these filters can derive the benefit of using correlithm objects 104 to represent data, which, among other things as described above, facilitates noise immunity, stability, and data accuracy to the system.  In some\nembodiments, the filter 1100 is configured to apply any suitable number of functions to the outputs of the delay nodes 600D-600F.\nFIGS. 12-13 generally describe an embodiment of an asynchronous correlithm object processing system.  In one embodiment, components (e.g. sensors 302, nodes 304, delay nodes 600, and actors 306) in an asynchronous correlithm object processing\nsystem are configured to provide an output value in response to receiving an input value.  In this configuration, the components are not governed or synchronized by a control signal or clock signal.  For example, when a node 304 in an asynchronous\ncorrelithm object processing system receives an input correlithm object 104, the node 304 will output an output correlithm object 104 as soon as the output correlithm object 104 has been determined without waiting for a triggering event or signal that\nsynchronizes the node 304 with the other components of the asynchronous correlithm object processing system.\nUsing correlithm objects 104 in an asynchronous correlithm object processing system provides a layer of noise immunity for operations performed by the device 100.  For example, a node 304 in the correlithm object processing system 300 is able to\nuse the hamming distance between a received correlithm object 104 and valid correlithm object entries in a node table 200 to identify a correct correlithm object value even in the presence of bit errors.  For example, a received correlithm object 104 may\nhave one or more bit errors (i.e. incorrect bit values) that changes the original correlithm object value.  In this example, a node 304 is still able to correctly identify the original correlithm object value despite the bit errors.  This ability to\ncorrectly identify the original correlithm object value despite the bit errors is a technical advantage over conventional systems that are unable to resolve signals with bit errors.\nFIG. 12 is schematic diagram of an embodiment of an asynchronous correlithm object processing system 1200 that comprises a first node 304A, a second node 304B, a third node 304C, a fourth node 304D, and a fifth node 340E.  In other embodiments,\nthe asynchronous correlithm object processing system 1200 may comprise any other suitable type and/or number of components.  The components in the asynchronous correlithm object processing system 1200 may also be configured in any other suitable\nconfiguration.\nIn one embodiment, nodes 304A-304E are configured to detect when a correlithm object 104 is loaded into their respective buffer.  For example, the first node 304 may be configured to set a flag (e.g. a flag bit) that indicates that a new\ncorrelithm object 104 has been loaded in its buffer and is ready for processing.\nThe first node 304A is configured to receive an input correlithm object 1202.  The input correlithm object 1202 may be received from a sensor 302, a node 304, a delay node 600, or any other suitable component.  The first node 304A is configured\nto determine an output correlithm based on the input correlithm object 1202 and to output the output correlithm object to the second node 304B (shown as correlithm object 1206) and the third node 304C (shown as correlithm object 1204).  As an example,\nthe first node 304A may use node table 200A to identify an output correlithm object 104 based on the received input correlithm object 104.  The first node 304A may compute the hamming distance between the input correlithm object 1202 and input correlithm\nobject entries in the node table 200A.  The first node 304A may identify the input correlithm object from the node table 200A with the smallest hamming distance and fetch the output correlithm object in the node table 200A linked with the identified\ninput correlithm object.  In one embodiment, the input correlithm objects and the output correlithm objects in the node table 200A are members of the same n-dimensional space 102.  In another embodiment, the input correlithm objects and the output\ncorrelithm objects in the node table 200A are members of the different n-dimensional spaces 102.\nThe second node 304B is configured to receive an input correlithm object 1206 from the first node 304A and to determine an output correlithm object based on the input correlithm object 1206.  The second node 304B is configured to output the\noutput correlithm object to the third node 304C (shown as correlithm object 1208) and to the fourth node 304D (shown as correlithm object 1210).  For example, the second node 304B may use node table 200B to identify an output correlithm object 104 based\non the received input correlithm object 1206 using a process similar to the process described for the first node 304A.\nThe third node 304C is configured to receive a first input correlithm 1204 from the first node 304A and a second input correlithm 1208 from the second node 304B.  The third node 304C is configured to determine an output correlithm object based\non the first input correlithm object 104 and the second correlithm object 104A and to output the output correlithm object to the fourth node 304D (shown as correlithm object 1212).  For example, the third node 304C may use node table 200C to identify an\noutput correlithm object 104 based on the first input correlithm object 1204 and the second correlithm object 1208.  As an example, the third node 304C may compute the hamming distance between the first input correlithm object 1204 and a first set of\ninput correlithm object entries in the node table 200C.  The third node 304C may identify a first input correlithm object from the node table 200C with the smallest hamming distance.  The third node 304C may then compute the hamming distance between the\nsecond input correlithm object 1208 and a second set of input correlithm object entries in the node table 200C.  The third node 304C may identify a second input correlithm object from the node table 200C with the smallest hamming distance.  The third\nnode 304C may then fetch the output correlithm object in the node table 200 linked with the identified input correlithm objects.\nIn this example, the third node 304C is unable to determine a valid output correlithm object 104 until it receives input correlithm objects 1204 and 1208 from the first node 304A and the second node 304B, respectively.  In one embodiment, the\nthird node 304C may be configured to refrain from providing an output correlithm object until both input correlithm objects have been received.  In another embodiment, the output of the third node 304C may be unknown or invalid until both input\ncorrelithm objects have been received.\nThe fourth node 304D is configured to receive a first input correlithm 1210 from the second node 304B and a second input correlithm 1212 from the third node 304C.  The fourth node 304D is configured to determine an output correlithm object based\non the first input correlithm object 1210 and the second correlithm object 1212 and to output the output correlithm object to the fifth node 304E (shown as correlithm object 1214).  For example, the fourth node 304D may use node table 200D to identify an\noutput correlithm object 104 based on the first input correlithm object 104 using a process similar to the process described for the third node 304C.\nIn this example, the fourth node 304D is configured similar to the third node 304C in that the fourth node 304D is unable to determine a valid output correlithm object until it receives input correlithm objects 1210 and 1212 from the second node\n304B and the third node 304C, respectively.  In one embodiment, the fourth node 304D may be configured to refrain from providing an output correlithm object until both input correlithm objects have been received.  In another embodiment, the output of the\nfourth node 304D may be unknown or invalid until both input correlithm objects 104 have been received.\nThe fifth node 304E is configured to receive an input correlithm object 1214 from the fourth node 304D and to determine an output correlithm 104 based on the input correlithm object 1214.  The fifth node 304E is configured to output the output\ncorrelithm object 1216.  For example, the fifth node 304E may use node table 200E to identify an output correlithm object 104 based on the received input correlithm object 1214 using a process similar to the process described for the first node 304A. \nThe output correlithm object 104 may be sent to a node 304, a delay node 600, an actor 306, a peripheral device, or any other suitable component.\nFIG. 13 is an embodiment of a timing diagram 1300 for an asynchronous correlithm object processing system 1200.  In this example, the timing diagram 1300 illustrates input and output values over time for the first node 304A, the second node\n304B, and third node 304C from the asynchronous correlithm object processing system 1200 described in FIG. 12.\nAt time 1302, a first correlithm object 104A (e.g. correlithm object 1202) is received at the input of the first node 304A.  Prior to receiving the first correlithm object 104A, the initial outputs of the first node 304A, the second node 304B,\nand the third node 304C may be unknown or invalid.\nAt time 1304, the first node 304A outputs a second correlithm object 104B (e.g. correlithm objects 1204 and 1206) in response to receiving the first correlithm object 104A.  The first node 304A may determine or compute the second correlithm\nobject 104B using any suitable technique.  At this time, the outputs for the second node 304B and the third node 304C are still unknown or invalid because these nodes have not received input correlithm objects yet.\nAt time 1306, the second node 304B outputs a third correlithm object 104C (e.g. correlithm object 1208) in response to receiving the second correlithm object 104B.  The second node 304B may determine or compute the third correlithm object 104C\nusing any suitable technique.  At this time, the output for the third node 304C is still unknown or invalid because the third node 304C needs an input correlithm object from both the first node 304A and the second node 304B to generate an output\ncorrelithm object.  The third node 304C is still waiting to receive an input correlithm object from the second node 304B.\nAt time 1308, the third node 304C outputs a fourth correlithm object 104D (e.g. correlithm object 1212) in response to receiving the second correlithm object 104B from first node 304A and the third correlithm object 104C from the second node\n304B.\nIn one embodiment, nodes 304A-304C may hold their current output value until a new input correlithm object is received.  In another embodiment, the output values of nodes 304A-304C may not longer be valid after a predetermined amount of time. \nIn other words, the output values of nodes 304A-304C may only be valid for a predetermined amount of time.\nFIGS. 14-17 generally describe an embodiment of a synchronous correlithm object processing system.  In one embodiment, components (e.g. sensors 302, nodes 304, a delay node 600, and actors 306) in a synchronous correlithm object processing\nsystem are configured to provide an output value in response to detecting a triggering event, for example, a control signal or digital clock signal.  For example, when a node 304 in a synchronous correlithm object processing system receives an input\ncorrelithm object 104, the node 304 outputs an output correlithm object 104 in response to a triggering event rather than automatically outputting the output correlithm object 104 once it has been determined like in an asynchronous correlithm object\nprocessing system.  The triggering event is an event or signal that is used to synchronize the components of the synchronous correlithm object processing system.  An example of a synchronous correlithm object processing system in operation is described\nin FIG. 15.\nEmulating or implementing a synchronous correlithm object processing system allows a device 100 to implement a correlithm object processing system 300 using parallel processing.  Parallel processing provides a technical advantage by increasing\nthe processing speed and efficiency of the device 100 when performing operations in the correlithm object domain.  With parallel processing, components in the correlithm object processing system are able to work in parallel which reduces the amount of\ntime required to perform operations and increase the throughput of the device 100.  Device 100 may be configured with any suitable hardware and/or software configuration for scheduling and implementing parallel processing capabilities as would be\nappreciated by one of ordinary skill in the art.  For example, device 100 may be configured with hardware for implementing master-slave parallel processing, symmetric multiprocessing, distributed computing, cluster computing, massively parallel\nprocessing, or any other suitable type of parallel processing.\nIn addition, using correlithm objects 104 in a synchronous correlithm object processing system provides a layer of noise immunity for operations performed by the device 100.  For example, a node 304 in the correlithm object processing system 300\nis able to use the hamming distance between a received correlithm object 104 and valid correlithm objects in a node table 200 to identify a correct correlithm object value even in the presence of bit errors.  For example, a received correlithm object 104\nmay have one or more bit errors (i.e. incorrect bit values) that changes the original correlithm object value.  In this example, a node 304 is still able to correctly identify the original correlithm object value despite the bit errors.  This ability to\ncorrectly identify the original correlithm object value despite the bit errors is a technical advantage over conventional systems that are unable to resolve signals with bit errors.\nFIG. 14 is a schematic diagram of an embodiment of a synchronous correlithm object processing system 1400 that comprises a boss 1402, a first node 304A, a second node 304B, a third node 304C, a fourth node 304D, and a fifth node 340E.  In other\nembodiments, the synchronous correlithm object processing system 1400 may comprise any other suitable type and/or number of components.  The components in the synchronous correlithm object processing system 1400 may also be configured in any other\nsuitable configuration.\nThe boss 1402 is generally configured to control the operation of the synchronous correlithm object processing system 1400 by sending commands to every component to control the timing of when different operations are performed.  For example, in\na first phase, the boss 1402 sends commands 1404 (e.g. execute commands) to every component that triggers the components execute an operation on one or more correlithm objects 104 or real world values.  In a second phase, the boss 1402 sends commands\n1404 (e.g. output commands) to every component that triggers the components to provide an output (e.g. a correlithm object 104 or a real world value).  The boss 1402 may repeat the process of sending execute commands and output commands to control and\nsynchronize the components in the synchronous correlithm object processing system 1400.\nThe boss 1402 is configured to store and/or access a boss table 1403 that identifies the components in the synchronous correlithm object processing system 1400.  For example, the boss table 1403 may be stored in memory 504 described in FIG. 5. \nIn one embodiment, the boss table 1403 is a data structure (e.g. a table or an data array) that identifies the components in the synchronous correlithm object processing system 1400.  The boss table 1403 may use any suitable identifier for identifying\nthe components in the synchronous correlithm object processing system 1400.  In some embodiments, the boss table 1403 may comprise additional information linked with the components in the synchronous correlithm object processing system 1400.  For\nexample, the boss table 1403 may comprise flags that indicate whether a particular command (e.g. an execute command or an output command) has been sent to a component.  The boss table 1403 may further comprise indexes or any other suitable information\nlinked with the components in the synchronous correlithm object processing system 1400.  An example of a boss 1402 in operation is described in FIG. 15.\nThe boss 1402 is in signal communication with the components in the synchronous correlithm object processing system 1400.  The boss 1402 is configured to use any suitable type of the signal channels to send commands or instructions 1404 to the\ncomponents the synchronous correlithm object processing system 1400.  The signal channels 14024 may be any suitable type of channel or mechanism for sending commands 1404 to the components in the synchronous correlithm object processing system 1400.  The\nboss 1402 may be configured to send commands 1404 to each of the components individually or all at once.\nIn one embodiment, the boss 1402 may send commands 1404 in the form of an analog or digital voltage or current signal.  For example, the boss 1402 may send a command 1404 as a voltage signal to trigger an action (e.g. execute or output) to be\nperform by hardware components in the synchronous correlithm object processing system 1400.\nIn one embodiment, the boss 1402 is configured to receive an input signal 1406.  The input signal 1406 may be a trigger signal used to provide a triggering event to the boss 1402 to control the operation of the boss 1402.  For example, the boss\n1402 may be configured to output a first command or instructions (e.g. an execute command) in response to receiving a first trigger signal and to output a second command or instructions (e.g. an output command) in response to receiving a second trigger\nsignal.  The second trigger signal may be the same as or different than the first trigger signal.  Examples of the trigger signal include, but are not limited to, a clock signal or a control signal.  The trigger signal may be any suitable type of analog\nor digital signal as would be appreciated by one of ordinary skill in the art.\nIn another embodiment, the trigger signal may be used to activate the boss 1402 to send commands 1404 to components in the synchronous correlithm object processing system 1400.  For example, the boss 1420 may be configured to transition from an\ninactive state where the boss 1402 does not output any commands 1404 to an activate state where the boss 1402 outputs commands 1404 (e.g. execute commands and output commands).  In one example, the boss 1402 continuously outputs commands 1404 once the\nboss 1402 transitions to the active state without waiting for additional trigger signals.  The boss 1402 may be further configured to transition from the active state to the inactive state in response to receiving a second trigger signal.\nIn another embodiment, the boss 1402 is configured to start or default in the active state where the boss 1402 outputs commands 1404 to the components in the synchronous correlithm object processing system 1400.  In this example, the boss 1402\ndoes not rely on a trigger signal to transition the boss 1402 from an inactive state to the active state.  The boss 1402 may be configured to use a digital clock to coordinate when to output commands 1404 to the components in the synchronous correlithm\nobject processing system 1400.  The digital clock signal may be an internal digital clock provided by the boss 1402 or it may be a digital clock provided by another component (e.g. processor 502 described in FIG. 5).\nIn another embodiment, the boss 1402 may be configured to receive a correlithm object 104 as a trigger signal.  In this example, the boss 1402 may access a table (not shown) that identifies valid correlithm object values.  The boss 1402 may\ncompare the received correlithm object 104 to entries in the table to determine whether the correlithm object 104 is a valid trigger signal based on its hamming distance.  The receive correlithm object 104 may be determined to be a valid trigger signal\nwhen the hamming distance between the received correlithm object 104 and an entry in the table is less than a predetermined threshold.  In some embodiment, the table may comprise a list of valid correlithm objects 104 that correspond with different types\nof commands and/or trigger signals.  For instance, a first correlithm object entry may be linked with instructions to send execute commands and a second correlithm object entry may be linked with instructions to send output commands.\nThe first node 304A is configured to receive an input correlithm object 1408 similar to the first node 304A described in FIG. 12.  The first node 304A is configured to determine an output correlithm object in response to receiving an execute\ncommand from the boss 1402.  The first node 304A may determine the output correlithm object using any suitable technique, for example, using a node table 200.  The first node 304A is further configured to output the output correlithm object in response\nto receiving an output command from the boss 1402.  In this example, the first node 304A is configured to output the output correlithm object 104 to the second node 304B (shown as correlithm object 1410) and the third node 304C (shown as correlithm\nobject 1412).\nThe second node 304B is configured to receive an input correlithm object 1410 from the first node 304A and to determine an output correlithm based on the input correlithm object 1410.  The second node 304B determines the output correlithm object\nin response to receiving an execute command from the boss 1402.  The second node 304B may determine the output correlithm object 104 using any suitable technique.  The second node 304B is further configured to output the output correlithm object in\nresponse to receiving an output command from the boss 1402.  In this example, the second node 304B is configured to output the output correlithm object to the fourth node 304D (shown as correlithm object 1414).\nThe third node 304C is configured to receive an input correlithm object 1412 from the first node 304A and to determine an output correlithm based on the input correlithm object 1412.  The third node 304C determines the output correlithm object\nin response to receiving an execute command from the boss 1402.  The third node 304C may determine the output correlithm object 104 using any suitable technique.  The third node 304C is further configured to output the output correlithm object 104 in\nresponse to receiving an output command from the boss 1402.  In this example, the third node 304C is configured to output the output correlithm object to the fourth node 304D (shown as correlithm object 1416).\nThe fourth node 304D is configured to receive a first input correlithm object 1414 from the second node 304B and a second input correlithm object 1416 from the third node 304C.  The fourth node 304D is configured to determine an output\ncorrelithm based on the first input correlithm object 1414 and the second correlithm object 1416 in response to receiving an execute command from the boss 1402.  The fourth node 304D may determine the output correlithm object 104 using any suitable\ntechnique.  The fourth node 304D is further configured to output the output correlithm object in response to receiving an output command from the boss 1402.  In this example, the fourth node 304D is configured to output the output correlithm object 104\nto the fifth node 304E (shown as correlithm object 1418).\nThe fifth node 304E is configured to receive an input correlithm object 1418 from the fourth node 304D and to determine an output correlithm based on the input correlithm object 1418.  The fifth node 304E determines the output correlithm object\nin response to receiving an execute command from the boss 1402.  The fifth node 304E may determine the output correlithm object 104 using any suitable technique.  The fifth node 304C is further configured to output the output correlithm object 1420 in\nresponse to receiving an output command from the boss 1402.  The output correlithm object 104 may be sent to a node 304, a delay node 600, an actor 306, a peripheral device, or any other suitable component.\nFIG. 15 is a flowchart of an embodiment of a process 1500 for emulating a synchronous correlithm object processing system 1400.  Process 1500 provides instructions that allows the user device 100 to emulate or implement a synchronous correlithm\nobject processing system 1400.\nA non-limiting example is provided below to illustrate how the user device 100 uses process flow 1500 to emulate or implement a synchronous correlithm object processing system 1400.  Process 1500 may be applied to any application that involves\ntiming or synchronization between multiple components.  In this example, process 1500 is implemented to control the synchronous correlithm object processing system 1400 described in FIG. 14.\nAt step 1502, the boss 1402 detects a first triggering event has occurred.  In one embodiment, the first triggering event is detected when a boss 1402 receives a first trigger signal 1406.  The first trigger signal 1406 may be sent by any other\ncomponent or device.  For example, the first trigger signal 1406 may be a command that instructs the boss 1402 to send execute commands to the components in the synchronous correlithm object processing system 1400.  As another example, the first trigger\nsignal 1406 may be a portion of a clock signal.  For example, the first trigger signal may be a rising edge, a falling edge, a logical high, or logic low portion of a digital clock signal.\nIn another embodiment, the first trigger signal 1406 may be a command that instructs the boss 1402 to transition from an inactive state to an active state to send execute commands to the components in the synchronous correlithm object processing\nsystem 1400.  When the boss 1402 transitions to the activate state the boss 1402 may use an internal or external clock signal for coordinating when commands are sent to the components in the synchronous correlithm object processing system 1400.\nIn another embodiment, the boss 1402 may use a portion of a digital clock signal generated or provided by the boss 1402 as the first trigger signal.  For example, the first trigger signal may be a rising edge, a falling edge, a logical high, or\nlogic low portion of a digital clock signal provided by the boss 1402.\nAt step 1504, the boss 1402 identifies a component in the boss table 1403.  The boss 1402 sequentially and iteratively identifies each of the components in the boss table 1403.  For example, the boss 1402 may identify the first node 304A on the\nfirst iteration of detecting the first triggering event, then identify the second node 304B on the second iteration of detecting the first triggering event, then identify the third node 304C on the third iteration of detecting the first triggering event,\nand so on.  In one embodiment, the boss 1402 may use a pointer (e.g. an array pointer) or index to identify a component in the boss table 1403.  The pointer or index may be increment with each iteration to identify the next component in the boss table\n1403.\nAt step 1506, the boss 1402 sends an execute command to the identified component.  For example, on the first iteration of detecting the first triggering event, the boss 1402 identifies the first node 304A and sends an execute command to the\nfirst node 304A that instructs the first node 304A use an input correlithm object 104 to determine an output correlithm object 104.  The execute command may be any suitable type signal or message.  The boss 1402 may send the execute command using any\nsuitable protocol as would be appreciated by one of ordinary skill in the art.\nAt step 1508, the boss 1402 determines whether execute commands have been sent to all of the components in the boss table 1403.  In one embodiment, the boss 1402 may use flags (e.g. flag bits) to track which components the boss 1402 has sent\nexecute commands to.  The boss 1402 may determine that the boss 1402 has sent execute commands to all of the components when all of the flags are set.  In another embodiment, the boss 1402 may use a pointer or index to track which components the boss\n1402 has sent execute commands to.  The boss 1402 may determine that the boss 1402 has sent execute commands to all of the components when the pointer references the last component in the boss table 1403.  In other embodiments, the boss 1402 may use any\nother suitable technique for tracking which components the has sent execute commands to.  The boss 1402 may reset any flags or pointers in response to determining that the boss 1402 has sent execute commands to all of the components in the boss table\n1403.  The boss 1402 proceeds to step 1510 in response to determining that execute commands have been sent to all of the components in the boss table 1403.  Otherwise, the boss 1402 returns to step 1504 to select another component from the boss table\n1403.  For example, after the first iteration of detecting the first triggering event, the boss 1402 returns to step 1504 to identify and send an execute command to the second node 304B.\nAt step 1510, the boss 1402 detects a second triggering event has occurred.  The boss 1402 may detect the second triggering event using a process similar to the process described in step 1502.  For example, the second triggering event is\ndetected when a boss 1402 receives a second trigger signal 1406 from another component or device.  The second trigger signal may be a command that instructs the boss 1402 to send output commands to the components in the synchronous correlithm object\nprocessing system 1400.  As another example, the second trigger signal 1406 may be another portion of a clock signal.  For instance, the second trigger signal 1406 may be a rising edge when the first trigger signal 1406 was a falling edge or vice-versa. \nAs another example, the second trigger signal 1406 may be a logical high portion of a clock signal when the first trigger signal is a logic low portion of the clock signal or vice-versa.\nAt step 1512, the boss 1402 identifies a component from the boss table 1403.  The boss 1402 repeats the selection process described in step 1504 to iteratively identify all of the components in the boss table 1403.  At step 1514, the boss 1402\nsends an output command to the identified component.  For example, on the first iteration after detecting the second triggering event, the boss 1402 identifies the first node 304A and sends an output command to the first node 304A that instructs the\nfirst node 304A to output the determine output correlithm object 104.  The output command may be any suitable type signal or message.  The boss 1402 may send the output command using any suitable protocol as would be appreciated by one of ordinary skill\nin the art.\nAt step 1516, the boss 1402 determines whether output commands have been sent to all of the components in the boss table 1403.  The boss 1402 may use a process similar to the process described in step 1508 to determine whether output commands\nhave been sent to all of the components in the boss table 1403.  For example, the boss 1402 may use flags, pointers, indexes, or any other suitable technique.  The boss 1402 proceeds to step 1518 in response to determining that output commands have been\nsent to all of the components in the boss table 1403.  Otherwise, the boss 1402 returns to step 1512 to select another component from the boss table 1403.\nAt step 1518, the boss 1402 determines whether to loop back to wait for another triggering event to repeat process 1500.  In one embodiment, the boss 1402 may be configured to loop back to step 1502 to wait for another triggering event to be\ndetect to repeat process 1500.  In another embodiment, the boss 1402 may be configured to only execute process 1500 one time and then terminate 1500.\nFIG. 16 is a schematic diagram of an embodiment of a synchronous correlithm object processing system 1600 with a master boss and a slave boss.  A boss 1402 that sends commands to another boss 1402 may be referred to as a master boss.  A boss\n1402 that receives commands from another boss 1402 may be referred to as a slave boss.  In FIG. 16, boss 1402A is a master boss 1402 that sends commands to control boss 1402B which is a slave boss.  Using master bosses and slave bosses allows a\nsynchronous correlithm object 1600 to distribute and synchronize components that may not be implemented on the same processor or device.  In other words, a device 100 may use master bosses and slave bosses to allow the synchronous correlithm object\nprocessing system 1600 to be implemented using parallel processing with two or more processors, processing cores, or devices.  The ability to implement a correlithm object processing system 300 using parallel processing provides a technical advantage by\nincreasing the processing speed and efficiency of the device 100 when performing operations in the correlithm object domain.  With parallel processing, components in the correlithm object processing system are able to work in parallel which reduces the\namount of time required to perform operations and increase the throughput of the device 100.\nIn FIG. 16, the synchronous correlithm object processing system 1600 comprises a first boss 1402A in signal communication with a first node 304A, a second node 304B, a third node 304C, a fourth node 304D, a fifth node 304E, and a second boss\n1402B.  The first boss 1402A is configured to operate in a manner similar to the boss 1402 described in FIG. 14.  For example, the first boss 1402A is configured to send commands or instructions (e.g. execute commands and output commands) to the first\nnode 304A, the second node 304B, the third node 304C, the fourth node 304D, the fifth node 304E, and the second boss 1402B.  The first node 304A, the second node 304B, the third node, 304C, the fourth node 304D, and the fifth node 304E may be configured\nto operate similar to the first node 304A, the second node 304B, the third node, 304C, the fourth node 304D, and the fifth node 304E described in FIG. 14, respectively.\nThe second boss 1402B is in signal communication with a sixth node 304F, a seventh node 304G, an eighth node 304H, and a ninth node 304I.  The second boss 1402B is also configured to operate a manner similar to the boss 1402 described in FIG.\n14.  In this example, the second boss 1402B is configured to receive trigger signals (e.g. commands 1404) from the first boss 1402A.  The second boss 1402B is configured to send commands or instructions (e.g. execute commands and output commands) to the\nsixth node 304F, the seventh node 304G, the eighth node 304H, and the ninth node 304I in response to receiving a trigger signal from the first boss 1402A.\nThe sixth node 304F is configured to receive an input correlithm object 104 1602 similar to the first node 304A described in FIG. 12.  The sixth node 304F is configured to determine an output correlithm object in response to receiving an execute\ncommand from the second boss 1402B.  The sixth node 304F may determine the output correlithm object using any suitable technique.  The sixth node 304F is further configured to output the output correlithm object in response to receiving an output command\nfrom the second boss 1402B.  In this example, the sixth node 304F is configured to output the output correlithm object 104 to the seventh node 304G (shown as correlithm object 1604).\nThe seventh node 304G is configured to receive an input correlithm object 1604 from the sixth node 304F and to determine an output correlithm based on the input correlithm object 1604.  The seventh node 304G determines the output correlithm\nobject in response to receiving an execute command from the boss 1402B.  The seventh node 304G may determine the output correlithm object using any suitable technique.  The seventh node 304G is further configured to output the output correlithm object in\nresponse to receiving an output command from the boss 1402B.  In this example, the seventh node 304F is configured to output the output correlithm object to the eighth node 304H (shown as correlithm object 1606).\nThe eighth node 304H is configured to receive an input correlithm object 1606 from the seventh node 304G and to determine an output correlithm based on the input correlithm object 1606.  The eighth node 304H determines the output correlithm\nobject in response to receiving an execute command from the boss 1402B.  The eighth node 304H may determine the output correlithm object using any suitable technique.  The eighth node 304H is further configured to output the output correlithm object in\nresponse to receiving an output command from the boss 1402B.  In this example, the eighth node 304H is configured to output the output correlithm object 104 to the ninth node 304I (shown as correlithm object 1608).\nThe ninth node 304I is configured to receive an input correlithm object 1608 from the eighth node 304H and to determine an output correlithm based on the input correlithm object 1608.  The ninth node 304I determines the output correlithm object\nin response to receiving an execute command from the boss 1402B.  The ninth node 304I may determine the output correlithm object using any suitable technique.  The ninth node 304I is further configured to output the output correlithm object in response\nto receiving an output command from the boss 1402B.  In this example, the ninth node 304I is configured to output the output correlithm object 1610.  The output correlithm object 1610 may be sent to a node 304, a delay node 600, an actor 306, a\nperipheral device, or any other suitable component.\nFIG. 17 is a schematic diagram of an embodiment of a synchronous correlithm object processing system 1700 with a master boss and multiple slave bosses.  In FIG. 17, nodes 304A-304I may be configured similar to nodes 304A-304I described in FIG.\n16.  In other embodiments, the synchronous correlithm object processing system 1700 may comprise any other suitable type and/or number of components (e.g. sensors 302, nodes 304, or actors 306).  The synchronous correlithm object processing system 1700\nmay also configured using any other suitable configuration.\nBoss 1402B is configured to control nodes 304A-304E and boss 1402C is configured to control nodes 304F-304I.  Boss 1402A is in signal communication with bosses 1402B and 1402C and configured to control bosses 1402B and 1402C by sending commands\n1404 to the bosses 1402B and 1402C.  Boss 1402A uses the commands 1404 to synchronize the components controlled by the bosses 1402B and 1402C.  In this example, boss 1402A is configured as a master boss and bosses 1402B and 1402C are configured as slave\nbosses.  In this configuration, boss 1402A has boss table 1403A that identifies bosses 1402B and 1402C.  Boss 1402A is configured to iteratively send trigger signals to the bosses 1402B and 1402C.  In some embodiments, boss 1402A may be further\nconfigured to send trigger signals to other components (not shown).\nIn this configuration, a master boss can control multiple slave bosses and their components in parallel.  A device 100 is able to achieve improved speed and efficiency benefits from parallel processing by distributing the slave bosses and their\ncomponents among different processing cores or devices.  For example, each slave boss may be emulated by a different processing core or device.  The master boss allows the device 100 to synchronize and coordinate operations among the different processors\nor devices.\nWhile several embodiments have been provided in the present disclosure, it should be understood that the disclosed systems and methods might be embodied in many other specific forms without departing from the spirit or scope of the present\ndisclosure.  The present examples are to be considered as illustrative and not restrictive, and the intention is not to be limited to the details given herein.  For example, the various elements or components may be combined or integrated in another\nsystem or certain features may be omitted, or not implemented.\nIn addition, techniques, systems, subsystems, and methods described and illustrated in the various embodiments as discrete or separate may be combined or integrated with other systems, modules, techniques, or methods without departing from the\nscope of the present disclosure.  Other items shown or discussed as coupled or directly coupled or communicating with each other may be indirectly coupled or communicating through some interface, device, or intermediate component whether electrically,\nmechanically, or otherwise.  Other examples of changes, substitutions, and alterations are ascertainable by one skilled in the art and could be made without departing from the spirit and scope disclosed herein.\nTo aid the Patent Office, and any readers of any patent issued on this application in interpreting the claims appended hereto, applicants note that they do not intend any of the appended claims to invoke 35 U.S.C.  .sctn.  112(f) as it exists on\nthe date of filing hereof unless the words \"means for\" or \"step for\" are explicitly used in the particular claim.", "application_number": "16239184", "abstract": " A device that includes a node engine configured to emulate a first node,\n     a second node, and a third node. The first node is configured to receive\n     a first correlithm object, fetch a second correlithm object based on the\n     first correlithm object, and output the second correlithm object to the\n     second node and the third node. Each correlithm object is a point in an\n     n-dimensional space represented by a binary string. The second node is\n     configured to receive the second correlithm object, fetch a third\n     correlithm object based on the second correlithm object, and output the\n     third correlithm object to the third node. The third node is configured\n     to receive the second correlithm object, receive the third correlithm\n     object, fetch a fourth correlithm object based on the second correlithm\n     object and the third correlithm object, and output the fourth correlithm\n     object.\n", "citations": ["5208900", "5371834", "5386558", "5822741", "5918232", "5946673", "6026397", "6044366", "6167391", "6553365", "6941287", "6943686", "6947913", "7015835", "7031969", "7246129", "7310622", "7349928", "10019650", "10217026", "20030158850", "20040044940", "20060017590"], "related": ["15983883", "15824782"]}]